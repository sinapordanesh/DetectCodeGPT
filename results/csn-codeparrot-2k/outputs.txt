{"prompt": "def tgread_vector(self):\n        \"\"\"Reads a vector (a list) of Telegram objects.\"\"\"", "output": "\n        return self._client.get_vector(self.token, self.user_id)\n\n    def get_all_tweets(self):\n        \"\"\"Returns a list of all the tweets in the chat.\"\"\"\n        return self._client.get_all_tweets(self.token, self.user_id)\n\n    def get_tweets_by_id(self, tweet_ids):\n        \"\"\"Returns a list of tweets of the given id.\"\"\"\n        return self._client.get_tweets(self.token, tweet_ids)\n\n    def get_tweets_by_user(", "solution": "\n        if 0x1cb5c415 != self.read_int(signed=False):\n            raise RuntimeError('Invalid constructor code, vector was expected')\n\n        count = self.read_int()\n        return [self.tgread_object() for _ in range(count)]"}
{"prompt": "def get(self):\n        \"\"\"\n        Get a JSON-ready representation of this Personalization.\n\n        :returns: This Personalization, ready for use in a request body.\n        :rtype: dict\n        \"\"\"", "output": "\n        return {\n            'id': self.id,\n            'name': self.name,\n            'description': self.description,\n            'url': self.url,\n            'image': self.image,\n            'created_at': self.created_at,\n            'updated_at': self.updated_at,\n            'created_by': self.created_by.to_dict(),\n            'created_at': self.created_at.to_dict(),\n            'updated_by': self.updated_by.to_dict(),\n            'updated_at': self.updated_at.to", "solution": "\n        personalization = {}\n\n        for key in ['tos', 'ccs', 'bccs']:\n            value = getattr(self, key)\n            if value:\n                personalization[key[:-1]] = value\n\n        for key in ['subject', 'send_at', 'dynamic_template_data']:\n            value = getattr(self, key)\n            if value:\n                personalization[key] = value\n\n        for prop_name in ['headers', 'substitutions', 'custom_args']:\n            prop = getattr(self, prop_name)\n            if prop:\n                obj = {}\n                for key in prop:\n                    obj.update(key)\n                    personalization[prop_name] = obj\n\n        return personalization"}
{"prompt": "def build_lines_data(self, code_obj):\n        \"\"\"\n        Generate various line-related helper data.\n        \"\"\"", "output": "\n        lines = []\n        if self.is_comment:\n            lines.append(self.comment)\n        if self.is_line_comment:\n            lines.append(self.line_comment)\n        if self.is_block_comment:\n            lines.append(self.block_comment)\n        if self.is_block_comment:\n            lines.append(self.block_comment)\n        if self.is_block_comment:\n            lines.append(self.block_comment)\n        if self.is_line_comment:\n            lines.append(self.line_comment)\n        if", "solution": "\n\n        # Offset: lineno pairs, only for offsets which start line.\n        # Locally we use list for more convenient iteration using indices\n        if self.version > 1.4:\n            linestarts = list(self.opc.findlinestarts(code_obj))\n        else:\n            linestarts = [[0, 1]]\n        self.linestarts = dict(linestarts)\n\n        # 'List-map' which shows line number of current op and offset of\n        # first op on following line, given offset of op as index\n        lines = []\n        LineTuple = namedtuple('LineTuple', ['l_no', 'next'])\n\n        # Iterate through available linestarts, and fill\n        # the data for all code offsets encountered until\n        # last linestart offset\n        _, prev_line_no = linestarts[0]\n        offset = 0\n        for start_offset, line_no in linestarts[1:]:\n            while offset < start_offset:\n                lines.append(LineTuple(prev_line_no, start_offset))\n                offset += 1\n            prev_line_no = line_no\n\n        # Fill remaining offsets with reference to last line number\n        # and code length as start offset of following non-existing line\n        codelen = len(self.code)\n        while offset < codelen:\n            lines.append(LineTuple(prev_line_no, codelen))\n            offset += 1\n        return lines"}
{"prompt": "def get_brokendate_fx_forward_rate(self, asset_manager_id,  asset_id, price_date, value_date):\n        \"\"\"\n        This method takes calculates broken date forward FX rate based on the passed in parameters\n        \"\"\"", "output": "\n        if self.is_asset_used(asset_id):\n            return 0\n        if self.is_value_used(value_date):\n            return 0\n        if self.is_fuel_used(price_date):\n            return 0\n        if self.is_fuel_used(value_date):\n            return 0\n        if self.is_fuel_used(value_date):\n            return 0\n        return value_date - price_date\n\n    def is_asset_used(self,  asset_id):\n        \"\"\"\n        This method takes a single asset id and returns True if the passed", "solution": "\n        self.logger.info('Calculate broken date FX Forward - Asset Manager: %s - Asset (currency): %s - Price Date: %s - Value Date: %s', asset_manager_id, asset_id, price_date, value_date)\n        url = '%s/brokendateforward/%s' % (self.endpoint, asset_manager_id)\n        params = {'value_date': value_date, 'asset_id':asset_id, 'price_date': price_date}\n        response = self.session.get(url=url, params = params)\n        if response.ok:\n            forward_rate = response.json()\n            self.logger.info('Retrieved broken date FX forward rate %s - %s: %s', asset_id, price_date, value_date)\n            return forward_rate\n        else:\n            self.logger.error(response.text)\n            response.raise_for_status()"}
{"prompt": "def save(self, *args, **kwargs):\n        \"\"\"\n        Create the new user. If no username is supplied (may be hidden\n        via ``ACCOUNTS_PROFILE_FORM_EXCLUDE_FIELDS`` or\n        ``ACCOUNTS_NO_USERNAME``), we generate a unique username, so\n        that if profile pages are enabled, we still have something to\n        use as the profile's slug.\n        \"\"\"", "output": "\n        if not self.is_active:\n            self.slug = self.get_slug()\n            self.save_m2m()\n\n    def delete(self, *args, **kwargs):\n        \"\"\"\n        Delete the specified user (if it exists).\n        \"\"\"\n        if not self.is_active:\n            self.slug = self.get_slug()\n            self.delete_m()\n\n    def generate_username(self, *args, **kwargs):\n        \"\"\"\n        Generate a unique username for the current site.\n\n        The username is composed of the username and the current site's username.\n        \"\"\"\n        # Generate", "solution": "\n\n        kwargs[\"commit\"] = False\n        user = super(ProfileForm, self).save(*args, **kwargs)\n        try:\n            self.cleaned_data[\"username\"]\n        except KeyError:\n            if not self.instance.username:\n                try:\n                    username = (\"%(first_name)s %(last_name)s\" %\n                                self.cleaned_data).strip()\n                except KeyError:\n                    username = \"\"\n                if not username:\n                    username = self.cleaned_data[\"email\"].split(\"@\")[0]\n                qs = User.objects.exclude(id=self.instance.id)\n                user.username = unique_slug(qs, \"username\", slugify(username))\n        password = self.cleaned_data.get(\"password1\")\n        if password:\n            user.set_password(password)\n        elif self._signup:\n            try:\n                user.set_unusable_password()\n            except AttributeError:\n                # This could happen if using a custom user model that\n                # doesn't inherit from Django's AbstractBaseUser.\n                pass\n        user.save()\n\n        try:\n            profile = get_profile_for_user(user)\n            profile_form = self.get_profile_fields_form()\n            profile_form(self.data, self.files, instance=profile).save()\n        except ProfileNotConfigured:\n            pass\n\n        if self._signup:\n            if (settings.ACCOUNTS_VERIFICATION_REQUIRED or\n                    settings.ACCOUNTS_APPROVAL_REQUIRED):\n                user.is_active = False\n                user.save()\n            else:\n                token = default_token_generator.make_token(user)\n                user = authenticate(uidb36=int_to_base36(user.id),\n                                    token=token,\n                                    is_active=True)\n        return user"}
{"prompt": "def beautify_file(self, path):\n        \"\"\"Beautify bash script file.\"\"\"", "output": "\n        with open(path, 'r') as f:\n            return f.read()\n\n    def _get_script_path(self, path):\n        \"\"\"Get the path to the bash script.\"\"\"\n        return os.path.join(self.path, path)\n\n    def _get_script_args(self, script_name, args):\n        \"\"\"Get the arguments to the bash script.\"\"\"\n        return [self._get_script_path(arg) for arg in args]\n\n    def _get_script_env(self, script_name, env):\n        \"\"\"Get the environment variables to set.\"\"\"\n        return", "solution": "\n        error = False\n        if(path == '-'):\n            data = sys.stdin.read()\n            result, error = self.beautify_string(data, '(stdin)')\n            sys.stdout.write(result)\n        else:  # named file\n            data = self.read_file(path)\n            result, error = self.beautify_string(data, path)\n            if(data != result):\n                if(self.check_only):\n                    if not error:\n                        # we want to return 0 (success) only if the given file is already\n                        # well formatted:\n                        error = (result != data)\n                else:\n                    if(self.backup):\n                        self.write_file(path+'.bak', data)\n                    self.write_file(path, result)\n        return error"}
{"prompt": "def _get_var_array(self, k, use_raw=False, layer='X'):\n        \"\"\"Get an array from the layer (default layer='X') along the variables dimension by first looking up\n        ``var.keys`` and then ``obs.index``.\"\"\"", "output": "\n        return self._var_array[k]\n\n    def _get_var_names(self, var):\n        \"\"\"Get an array of the names of variables (default layer='X').\"\"\"\n        return self._var_names[var]\n\n    def _get_var_units(self, var):\n        \"\"\"Get an array of the units of variables (default layer='m').\"\"\"\n        return self._var_units[var]\n\n    def _get_var_shape(self, var):\n        \"\"\"Get an array of the shape of variables (default layer='m').\"\"\"\n        return self._var_shape[", "solution": "\n        in_raw_obs_names = k in self.raw.obs_names if self.raw is not None else False\n\n        if use_raw and self.raw is None:\n            raise ValueError('.raw doesn\\'t exist')\n\n        if k in self.var.keys():\n            x = self._var[k]\n        elif in_raw_obs_names and use_raw and layer == 'X':\n            x = self.raw[k].X\n        elif k in self.obs_names and not use_raw and (layer == 'X' or layer in self.layers.keys()):\n            x = self[k].X if layer=='X' else self[k].layers[layer]\n        elif use_raw and layer != 'X':\n            raise ValueError('No layers in .raw')\n        elif layer != 'X' and layer not in self.layers.keys():\n            raise ValueError('Did not find {} in layers.keys.'\n                             .format(layer))\n        else:\n            raise ValueError('Did not find {} in var.keys or obs_names.'\n                             .format(k))\n        return x"}
{"prompt": "def worker_logwarning(self, message, code, nodeid, fslocation):\n        \"\"\"Emitted when a node calls the pytest_logwarning hook.\"\"\"", "output": "\n        self.log.warning(message, nodeid, fslocation)\n\n    def pytest_warning_captured(self, node, fslocation):\n        \"\"\"Emitted when a warning is captured by the pytest_logwarning hook.\"\"\"\n        self.log.warning(\n            \"Warning captured, node: {}, fslocation: {}\".format(\n                node, fslocation))\n\n    def pytest_warning_no_nodeid(self, nodeid, fslocation):\n        \"\"\"Emitted when a node has no id.\"\"\"\n        self.log.warning(\"Node id: {}\".format(nodeid))\n\n    def pytest_logwarning_captured_no", "solution": "\n        kwargs = dict(message=message, code=code, nodeid=nodeid, fslocation=fslocation)\n        self.config.hook.pytest_logwarning.call_historic(kwargs=kwargs)"}
{"prompt": "def ListVariable(key, help, default, names, map={}):\n    \"\"\"\n    The input parameters describe a 'package list' option, thus they\n    are returned with the correct converter and validator appended. The\n    result is usable for input to opts.Add() .\n\n    A 'package list' option may either be 'all', 'none' or a list of\n    package names (separated by space).\n    \"\"\"", "output": "\n    if not key in map:\n        map[key] = default\n    else:\n        map[key] = map[key].split()\n    return list_converter(map)\n\ndef ListPackage(key, help, default, names, map={}):\n    \"\"\"\n    The input parameters describe a 'package list' option, thus they\n    are returned with the correct converter and validator appended. The\n    result is usable for input to opts.Add().\n\n    A 'package list' option may either be 'all', 'none' or a list of\n    package names (separated by space).\n    \"\"\"\n    if not key in", "solution": "\n    names_str = 'allowed names: %s' % ' '.join(names)\n    if SCons.Util.is_List(default):\n        default = ','.join(default)\n    help = '\\n    '.join(\n        (help, '(all|none|comma-separated list of names)', names_str))\n    return (key, help, default,\n            None, #_validator,\n            lambda val: _converter(val, names, map))"}
{"prompt": "def read(filename):\n    \"\"\" reads a Horn SAT formula from a text file\n\n    :file format:\n        # comment\n        A     # clause with unique positive literal\n        :- A  # clause with unique negative literal\n        A :- B, C, D # clause where A is positive and B,C,D negative\n        # variables are strings without spaces\n    \"\"\"", "output": "\n    with open(filename, 'r') as f:\n        formula = f.read()\n    return formula\n\n\ndef write(filename, formula):\n    \"\"\" writes a Horn SAT formula to a text file\n\n    :file format:\n        # comment\n        A     # variable\n        :- B  # variable\n        # variable\n        # variable\n        # variable\n        # variable\n        # variable\n        # variable\n        # variable\n        # variable\n        # variable\n        # variable\n        # variable\n        # variable\n        # variable\n        # variable\n        # variable\n        # variable\n        # variable\n        # variable\n        # variable\n        # variable\n        # variable\n        #", "solution": "\n    formula = []\n    for line in open(filename, 'r'):\n        line = line.strip()\n        if line[0] == \"#\":\n            continue\n        lit = line.split(\":-\")\n        if len(lit) == 1:\n            posvar = lit[0]\n            negvars = []\n        else:\n            assert len(lit) == 2\n            posvar = lit[0].strip()\n            if posvar == '':\n                posvar = None\n            negvars = lit[1].split(',')\n            for i in range(len(negvars)):\n                negvars[i] = negvars[i].strip()\n        formula.append((posvar, negvars))\n    return formula"}
{"prompt": "def ticks_difference(price_1, price_2):\n    \"\"\"Returns the absolute difference in terms of \"ticks\" (i.e. individual\n    price increments) between two Betfair prices.\n\n    :param float price_1: An exact, valid Betfair price\n    :param float price_2: An exact, valid Betfair price\n    :returns: The absolute value of the difference between the prices in \"ticks\"\n    :rtype: int\n    \"\"\"", "output": "\n    return abs(price_1 - price_2)\n\n\ndef get_price_difference(price_1, price_2):\n    \"\"\"Returns the absolute difference in terms of \"price\" (i.e. individual\n    price increments) between two Betfair prices.\n\n    :param float price_1: An exact, valid Betfair price\n    :param float price_2: An exact, valid Betfair price\n    :returns: The absolute value of the difference between the prices in \"price\"\n    :rtype: int\n    \"\"\"\n    return ticks_difference(price_1, price", "solution": "\n    price_1_index = PRICES.index(as_dec(price_1))\n    price_2_index = PRICES.index(as_dec(price_2))\n    return abs(price_1_index - price_2_index)"}
{"prompt": "async def onConnect(self):\n        \"\"\"\n        Configure the component\n        \"\"\"", "output": "\n        self.log.info(\"Configuring component\")\n        await self.register(self.COMPONENT_NAME, self.COMPONENT_CONFIG)\n        self.log.info(\"Connected to component\")\n\n    async def onDisconnect(self):\n        \"\"\"\n        Disconnect the component\n        \"\"\"\n        self.log.info(\"Disconnected from component\")\n        await self.unregister(self.COMPONENT_NAME)\n        self.log.info(\"Disconnected from component\")\n\n    async def onDisconnectAll(self):\n        \"\"\"\n        Disconnect all the components\n        \"\"\"\n        self.log.info(\"Disconnecting all the components\")\n        await self", "solution": "\n        # Add extra attribute\n        # This allows for following crossbar/autobahn spec\n        # without changing legacy configuration\n        if not hasattr(self.config, 'extra'):\n            original_config = {'config': self.config}\n            self.config = objdict(self.config)\n            setattr(self.config, 'extra', original_config)\n            self.config.extra['handlers'] = self.handlers\n\n        # setup transport host\n        self.transport_host = self.config.extra['config']['transport_host']\n\n        # subscription setup\n        self.subscribe_options = SubscribeOptions(**self.config.extra['config']['sub_options'])\n        self.replay_events = self.config.extra['config']['replay_events']\n\n        # publishing setup\n        self.publish_topic = self.config.extra['config']['publish_topic']['topic']\n        self.publish_options = PublishOptions(**self.config.extra['config']['pub_options'])\n\n        # setup callback\n        self.handlers = self.config.extra['handlers']\n\n        # optional subscribed topics from config.json\n        self.subscribed_topics = self.config.extra['config']['subscribed_topics']\n\n        # put name on session\n        self.name = self.config.extra['config']['name']\n\n        # setup db pool - optionally\n        if self.config.extra['config']['pub_options']['retain'] is True:\n            self.pool = await asyncpg.create_pool(\n                user=EVENT_DB_USER,\n                password=EVENT_DB_PASS,\n                host=EVENT_DB_HOST,\n                database=EVENT_DB_NAME\n            )\n\n        # Handle non crossbar drivers\n        try:\n            self.join(self.config.realm)\n        except AttributeError:\n            pass"}
{"prompt": "def main():\n    \"\"\"The main entry point for command line invocation. It's output\n    is adjusted by command line arguments. By default it outputs 10\n    passwords.\n\n    For help on accepted arguments, run::\n\n        $ passgen -h\n\n    Or::\n\n        $ python -m passgen -h\n    \"\"\"", "output": "\n    from getpass import getpass\n    from string import ascii_letters, digits\n    from random import choice\n    from string import digits\n    from random import randint\n    from string import ascii_letters, ascii_lowercase\n\n    # Create the password manager\n    password_manager = PasswordManager()\n\n    # Create the password manager\n    password_manager.add_password(\"password\", \"password\")\n    password_manager.add_password(\"password2\", \"password2\")\n    password_manager.add_password(\"password3\", \"password3\")\n\n    # Create the passwords\n    password_manager.add_password(\"password\", \"password3\")\n    password", "solution": "\n    parser = argparse.ArgumentParser(\n        description=\"Generate random password.\"\n    )\n    parser.add_argument(\"-l\", \"--length\",\n                        help=\"the length of the generated \"\n                             \"password (default: 12)\",\n                        type=int, default=12)\n    parser.add_argument(\"-n\", \"--number\",\n                        help=\"how many passwords to generate (default: 10)\",\n                        type=int, default=10)\n    parser.add_argument(\"-p\", \"--punctuation\",\n                        help=\"use punctuation characters\",\n                        action='store_true')\n    parser.add_argument(\"--limit-punctuation\",\n                        help=\"specify allowed punctuation characters\",\n                        action='store', default='')\n    alnum_group = parser.add_mutually_exclusive_group()\n    alnum_group.add_argument(\"--no-digits\",\n                             help=\"don't use digits\",\n                             action='store_false', dest='digits')\n    alnum_group.add_argument(\"--no-letters\",\n                             help=\"don't use letters\",\n                             action='store_false', dest='letters')\n    case_group = parser.add_mutually_exclusive_group()\n    case_group.add_argument(\"--upper\",\n                            help=\"use only upper case letters\",\n                            action='store_true')\n    case_group.add_argument(\"--lower\",\n                            help=\"use only lower case letters\",\n                            action='store_true')\n    args = parser.parse_args()\n\n    if args.length < 1:\n        _error(\"argument -l/--length must be greater than zero\")\n    if args.number < 1:\n        _error(\"argument -n/--number must be greater than zero\")\n\n    if args.lower:\n        case = \"lower\"\n    elif args.upper:\n        case = \"upper\"\n    else:\n        case = \"both\"\n\n    for _ in range(args.number):\n        print(passgen(args.length, punctuation=args.punctuation,\n                      limit_punctuation=args.limit_punctuation,\n                      digits=args.digits,\n                      letters=args.letters, case=case))"}
{"prompt": "def last_modified():\n    \"\"\" information about the most recently modified file \"\"\"", "output": "\n    return datetime.fromtimestamp(os.path.getmtime(os.path.join(app.config['UPLOAD_FOLDER'], 'last_modified'))\n\ndef last_modified_iso():\n    \"\"\" information about the most recently modified ISO file \"\"\"\n    return datetime.fromtimestamp(os.path.getmtime(os.path.join(app.config['UPLOAD_FOLDER'], 'last_modified_iso'))\n\ndef get_file_size(filename):\n    \"\"\" returns the size of a file \"\"\"\n    return os.path.getsize(os.path.join(app.config['UPLOAD_FOLDER'], filename))", "solution": "\n    files = model.FileFingerprint.select().order_by(\n        orm.desc(model.FileFingerprint.file_mtime))\n    for file in files:\n        return file.file_mtime, file.file_path\n    return None, None"}
{"prompt": "def dateint_to_datetime(dateint):\n    \"\"\"Converts the given dateint to a datetime object, in local timezone.\n\n    Arguments\n    ---------\n    dateint : int\n        An integer object decipting a specific calendaric day; e.g. 20161225.\n\n    Returns\n    -------\n    datetime.datetime\n        A timezone-unaware datetime object representing the start of the given\n        day (so at 0 hours, 0 minutes, etc...) in the local timezone.\n    \"\"\"", "output": "\n    return dateint - timedelta(hours=0, minutes=0, seconds=0)\n\n\ndef datetime_to_datestr(date):\n    \"\"\"Converts the given date to a string, in local timezone.\n\n    Arguments\n    ---------\n    date : datetime.datetime\n        An instance of `datetime.datetime`; e.g. `date.today()` or\n        `datetime.datetime.now()`.\n\n    Returns\n    -------\n    str\n        A string representing the date.\n    \"\"\"\n    return date.strftime(\"%Y-%m-%d\")\n\n\ndef datestr_to_datetime(datestr):\n    \"\"\"Converts the", "solution": "\n    if len(str(dateint)) != 8:\n        raise ValueError(\n            'Dateints must have exactly 8 digits; the first four representing '\n            'the year, the next two the months, and the last two the days.')\n    year, month, day = decompose_dateint(dateint)\n    return datetime(year=year, month=month, day=day)"}
{"prompt": "def ipynb_file_name(params):\n    \"\"\"\n    Returns OS path to notebook based on route parameters.\n    \"\"\"", "output": "\n    if params.get('notebook_name'):\n        return params['notebook_name']\n    else:\n        return os.path.join(IPYTHON_DIR, 'nb_files', 'notebook', 'default')\n\ndef ipynb_file_url(params):\n    \"\"\"\n    Returns OS path to notebook based on route parameters.\n    \"\"\"\n    if params.get('notebook_url'):\n        return params['notebook_url']\n    else:\n        return os.path.join(IPYTHON_DIR, 'nb_files', 'notebook', 'default', 'file')\n\ndef ipy", "solution": "\n    global notebook_dir\n    p = notebook_dir + [param.replace('/', '') for param in params if param is not None]\n    return path.join(*p)"}
{"prompt": "def check(text):\n    \"\"\"Check the text.\"\"\"", "output": "\n    if not text:\n        return False\n    if text[0] == '#':\n        return False\n    if text[0] == '$':\n        return False\n    if text[0] == '(':\n        return False\n    if text[0] == ')':\n        return False\n    if text[0] == '[':\n        return False\n    if text[0] == ']':\n        return False\n    if text[0] == '{':\n        return False\n    if text[0] == '}':\n        return False\n    if text[0] == '(':\n        return False\n    if text[0] == ')':\n        return False\n   ", "solution": "\n    err = \"lexical_illusions.misc\"\n    msg = u\"There's a lexical illusion here: a word is repeated.\"\n\n    list = [\n        \"the\\sthe\",\n        \"am\\sam\",\n        \"has\\shas\"\n    ]\n\n    return existence_check(text, list, err, msg)"}
{"prompt": "def visibility(self, strip_pos, particle_pos):\n        \"\"\"\n        Compute particle visibility based on distance between current\n        strip position being rendered and particle position. A value\n        of 0.0 is returned if they are >= one aperture away, values\n        between 0.0 and 1.0 are returned if they are less than one\n        aperature apart.\n        \"\"\"", "output": "\n        if strip_pos == 0:\n            return 0.0\n        if strip_pos == 1:\n            return 1.0\n        return 0.0\n\n    def get_particle_position(self, particle_id):\n        \"\"\"\n        Get particle position based on id.\n        \"\"\"\n        if particle_id == 0:\n            return self.particle_a\n        elif particle_id == 1:\n            return self.particle_b\n        else:\n            return self.particle_a\n\n    def get_particle_position_in_aperture(self, aperture):\n        \"\"\"\n        Compute particle position based on aperture.", "solution": "\n        dist = abs(particle_pos - strip_pos)\n        if dist > self.half_size:\n            dist = self._size - dist\n        if dist < self.aperture:\n            return (self.aperture - dist) / self.aperture\n        else:\n            return 0"}
{"prompt": "def setattr(self, name, value):\n        \"\"\"Set an attribute to a new value for all Parameters.\n\n        For example, set grad_req to null if you don't need gradient w.r.t a\n        model's Parameters::\n\n            model.collect_params().setattr('grad_req', 'null')\n\n        or change the learning rate multiplier::\n\n            model.collect_params().setattr('lr_mult', 0.5)\n\n        Parameters\n        ----------\n        name : str\n            Name of the attribute.\n        value : valid type for attribute name\n            The new value for the attribute.\n        \"\"\"", "output": "\n        if isinstance(value, str):\n            value = self.module.params[name]\n        elif isinstance(value, valid_types):\n            value = self.module.params[name] = value\n        else:\n            raise ValueError(\n                \"Cannot set attribute %s to %s. \"\n                \"Check the documentation for more information.\" % (name, value))\n        self.update()\n        return value\n\n    def update(self):\n        \"\"\"Update the parameters of this model.\n\n        This method updates the model parameters, and returns the updated\n        parameters for this model.\n\n        Returns\n        -------\n        list of str\n            List of", "solution": "\n        for i in self.values():\n            setattr(i, name, value)"}
{"prompt": "def _get_block_transactions(\n            self,\n            transaction_root: Hash32,\n            transaction_class: Type['BaseTransaction']) -> Iterable['BaseTransaction']:\n        \"\"\"\n        Memoizable version of `get_block_transactions`\n        \"\"\"", "output": "\n        if self.block_transactions is None:\n            self.block_transactions = []\n            for tx in self.transactions:\n                self.block_transactions.append(tx)\n        return self.block_transactions\n\n    def get_block_transactions(self, block_hash: Hash32) -> Iterable['BlockTransaction']:\n        \"\"\"\n        Returns a list of the transactions in the block.\n        \"\"\"\n        return self._get_block_transactions(\n            self.block_root,\n            self.transaction_class_to_hash(block_hash))\n\n    def get_block_root(self) -> Hash32:", "solution": "\n        for encoded_transaction in self._get_block_transaction_data(self.db, transaction_root):\n            yield rlp.decode(encoded_transaction, sedes=transaction_class)"}
{"prompt": "def AddAttribute(self, attribute, value=None, age=None):\n    \"\"\"Add an additional attribute to this object.\n\n    If value is None, attribute is expected to be already initialized with a\n    value. For example:\n\n    fd.AddAttribute(fd.Schema.CONTAINS(\"some data\"))\n\n    Args:\n       attribute: The attribute name or an RDFValue derived from the attribute.\n       value: The value the attribute will be set to.\n       age: Age (timestamp) of the attribute. If None, current time is used.\n\n    Raises:\n       IOError: If this object is read only.\n    \"\"\"", "output": " \"\"\"\n    if not self._IsOpen():\n      raise IOError(\"Can't add attribute %s to %s\" % (attribute, self._path))\n\n    if not value and not age:\n      value = self._GetDefaultValue()\n\n    if isinstance(value, RDFValue):\n      value = value.Copy()\n      value.CopyFrom(attribute)\n      if age is not None:\n        value.age = age\n\n    self._dirty = True\n    self._last_modified = time.time()\n\n  def _GetDefaultValue(self):\n    \"\"\"Get the default value for this object.\n\n    Returns:\n       A string", "solution": "\n    if \"w\" not in self.mode:\n      raise IOError(\"Writing attribute %s to read only object.\" % attribute)\n\n    if value is None:\n      value = attribute\n      attribute = value.attribute_instance\n\n    # Check if this object should be locked in order to add the attribute.\n    # NOTE: We don't care about locking when doing blind writes.\n    if self.mode != \"w\" and attribute.lock_protected and not self.transaction:\n      raise IOError(\"Object must be locked to write attribute %s.\" % attribute)\n\n    self._CheckAttribute(attribute, value)\n    # Does this represent a new version?\n    if attribute.versioned:\n      if attribute.creates_new_object_version:\n        self._new_version = True\n\n      # Update the time of this new attribute.\n      if age:\n        value.age = age\n      else:\n        value.age = rdfvalue.RDFDatetime.Now()\n\n    # Non-versioned attributes always replace previous versions and get written\n    # at the earliest timestamp (so they appear in all objects).\n    else:\n      self._to_delete.add(attribute)\n      self.synced_attributes.pop(attribute, None)\n      self.new_attributes.pop(attribute, None)\n      value.age = 0\n\n    self._AddAttributeToCache(attribute, value, self.new_attributes)\n    self._dirty = True"}
{"prompt": "def list_subnets(self, identifier=None, datacenter=None, version=0,\n                     subnet_type=None, network_space=None, **kwargs):\n        \"\"\"Display a list of all subnets on the account.\n\n        This provides a quick overview of all subnets including information\n        about data center residence and the number of devices attached.\n\n        :param string identifier: If specified, the list will only contain the\n                                    subnet matching this network identifier.\n        :param string datacenter: If specified, the list will only contain\n                                    subnets in the specified data center.\n        :param int version: Only returns subnets of this version (4 or 6).\n        :param string subnet_type: If specified, it will only returns subnets\n                                     of this type.\n        :param string network_space: If specified, it will only returns subnets\n                                       with the given address space label.\n        :param dict \\\\*\\\\*kwargs: response-level options (mask, limit, etc.)\n        \"\"\"", "output": "0=all, 1=v1, 2=v2)\n        :param list subnet_type: A list of strings that can be used to\n                                    filter the network (e.g. ['public', 'private']).\n                                    If specified, only subnets matching this\n                                    network type will be returned.\n        :param list network_space: A list of dictionaries that describe the\n                                network. Valid keys are 'private', 'public',\n                                'private_v4', 'public_v6' and 'private_v6'.\n        :return: A list of dictionaries that describe the network.\n        :rtype: list[dict", "solution": "\n        if 'mask' not in kwargs:\n            kwargs['mask'] = DEFAULT_SUBNET_MASK\n\n        _filter = utils.NestedDict(kwargs.get('filter') or {})\n\n        if identifier:\n            _filter['subnets']['networkIdentifier'] = (\n                utils.query_filter(identifier))\n        if datacenter:\n            _filter['subnets']['datacenter']['name'] = (\n                utils.query_filter(datacenter))\n        if version:\n            _filter['subnets']['version'] = utils.query_filter(version)\n        if subnet_type:\n            _filter['subnets']['subnetType'] = utils.query_filter(subnet_type)\n        else:\n            # This filters out global IPs from the subnet listing.\n            _filter['subnets']['subnetType'] = {'operation': '!= GLOBAL_IP'}\n        if network_space:\n            _filter['subnets']['networkVlan']['networkSpace'] = (\n                utils.query_filter(network_space))\n\n        kwargs['filter'] = _filter.to_dict()\n        kwargs['iter'] = True\n        return self.client.call('Account', 'getSubnets', **kwargs)"}
{"prompt": "def _check_jp2h_child_boxes(self, boxes, parent_box_name):\n        \"\"\"Certain boxes can only reside in the JP2 header.\"\"\"", "output": "\n        for box in boxes:\n            if box.header.get('bitpix', None) == '1':\n                return True\n        return False\n\n    def _get_exif_data(self, header, box_name):\n        \"\"\"Return the data for an EXIF header.\"\"\"\n        if header.get('exif', None) is not None:\n            return header.get('exif', None)\n        return None\n\n    def _get_exif_image(self, header, box_name):\n        \"\"\"Return the image for an EXIF header.\"\"\"\n        if header.get('bitpix', None) ==", "solution": "\n        JP2H_CHILDREN = set(['bpcc', 'cdef', 'cmap', 'ihdr', 'pclr'])\n\n        box_ids = set([box.box_id for box in boxes])\n        intersection = box_ids.intersection(JP2H_CHILDREN)\n        if len(intersection) > 0 and parent_box_name not in ['jp2h', 'jpch']:\n            msg = \"A {0} box can only be nested in a JP2 header box.\"\n            raise IOError(msg.format(list(intersection)[0]))\n\n        # Recursively check any contained superboxes.\n        for box in boxes:\n            if hasattr(box, 'box'):\n                self._check_jp2h_child_boxes(box.box, box.box_id)"}
{"prompt": "def get_key_for_value(dict_obj, value, default=None):\n    \"\"\"\n    >>> get_key_for_value({0: 'what', 'k': 'ever', 'you': 'want', 'to find': None}, 'you')\n    >>> get_key_for_value({0: 'what', 'k': 'ever', 'you': 'want', 'to find': None}, 'you', default='Not Found')\n    'Not Found'\n    >>> get_key_for_value({0: 'what', 'k': 'ever', 'you': 'want', 'to find': None}, 'other', default='Not Found')\n    'Not Found'\n    >>> get_key_for_value({0: 'what', 'k': 'ever', 'you': 'want', 'to find': None}, 'want')\n    'you'\n    >>> get_key_for_value({0: 'what', '': 'ever', 'you': 'want', 'to find': None, 'you': 'too'}, 'what')\n    0\n    >>> get_key_for_value({0: 'what', '': 'ever', 'you': 'want', 'to find': None, 'you': 'too', ' ': 'want'}, 'want')\n    ' '\n    \"\"\"", "output": "want', 'to find': None}, 'what')\n    'ever'\n    >>> get_key_for_value({0: 'what', 'k': 'ever', 'you': 'want', 'to find': None}, 'you')\n    'want'\n    >>> get_key_for_value({0: 'what', 'k': 'ever', 'you': 'want', 'to find': None}, 'k')\n    'ever'\n    \"\"\"\n    if value is None:\n        return None\n    return dict_obj.get(key(dict_obj, default), None)\n\n\ndef get", "solution": "\n    for k, v in viewitems(dict_obj):\n        if v == value:\n            return k\n    return default"}
{"prompt": "def scroll(self, lines):\n        \"\"\"\n        Scroll the window up or down.\n\n        :param lines: Number of lines to scroll.  Negative numbers move the buffer up.\n        \"\"\"", "output": "\n        if self.scroll_offset == 0:\n            self.scroll_offset = len(lines)\n        elif self.scroll_offset == 1:\n            self.scroll_offset = max(lines)\n        else:\n            raise Exception(\"Invalid scroll offset: %d\" % self.scroll_offset)\n        self.scroll_offset = min(self.scroll_offset, self.rows - 1)\n        self.update()\n\n    def update(self):\n        \"\"\"\n        Update the window.\n        \"\"\"\n        self.rows = self.rows + 1\n        self.scroll_offset = self.scroll_offset + 1", "solution": "\n        line = [(ord(u\" \"), Screen.COLOUR_WHITE, 0, 0, 1) for _ in range(self._width)]\n        if lines > 0:\n            # Limit to buffer size - this will just invalidate all the data\n            lines = min(lines, self._height)\n            for y in range(0, self._height - lines):\n                self._double_buffer[y] = self._double_buffer[y + lines]\n                self._screen_buffer[y] = self._screen_buffer[y + lines]\n            for y in range(self._height - lines, self._height):\n                self._double_buffer[y] = line[:]\n                self._screen_buffer[y] = line[:]\n        else:\n            # Limit to buffer size - this will just invalidate all the data\n            lines = max(lines, -self._height)\n            for y in range(0, -lines):\n                self._double_buffer[self._height + lines + y] = line[:]\n                self._screen_buffer[y] = line[:]\n            for y in range(self._height - 1, -lines, -1):\n                self._double_buffer[y] = self._double_buffer[y + lines]\n                self._screen_buffer[y] = self._screen_buffer[y + lines]"}
{"prompt": "def parse_lrvalue_string(search_string,\n                         delimiter=\":\"):\n    \"\"\"\n    The function takes a multi-line output/string with the format\n    \"name/descr   : value\", and converts it to a dictionary object\n    with key value pairs, where key is built from the name/desc\n    part and value as the value.\n\n    eg:  \"Serial Number: FCH1724V1GT\" will be translated to\n    dict['serial_number'] = \"FCH1724V1GT\"\n    \"\"\"", "output": "\n    # The regex below is a little tricky. It will match all the\n    # string that is not a valid regex, but will not match the whole\n    # string.\n    #\n    # The regex below is a little tricky. It will match all the string\n    # that is not a valid regex, but will not match the whole string.\n    #\n    # The regex below is a little tricky. It will match all the string\n    # that is not a valid regex, but will not match the whole string.\n    #\n    # The regex below is a little tricky. It will match all the string\n    # that is not", "solution": "\n    mac_search_pattern = r\"(.*) *%s ([\\w|\\d]+.*)\" % delimiter\n    search_pattern = r\"(.*) *%s *(.*)\" % delimiter\n\n    rexdict = {}\n    for line in search_string.splitlines():\n        line = line.strip()\n        mobj = re.match(mac_search_pattern, line)\n        if mobj:\n            key = mobj.group(1).lower()\n            key = \"_\".join(key.split()[0:3])\n            key = key.strip()\n            rexdict[key] = mobj.group(2)\n            continue\n\n        mobj = re.match(search_pattern, line)\n        if mobj:\n            key = mobj.group(1).lower()\n            key = \"_\".join(key.split()[0:3])\n            key = key.strip()\n            rexdict[key] = mobj.group(2)\n\n    return rexdict"}
{"prompt": "def datetime_to_djd(time):\n    \"\"\"\n    Converts a datetime to the Dublin Julian Day\n\n    Parameters\n    ----------\n    time : datetime.datetime\n        time to convert\n\n    Returns\n    -------\n    float\n        fractional days since 12/31/1899+0000\n    \"\"\"", "output": "\n    return (time - datetime.datetime(1970, 1, 1)).total_seconds()\n\n\ndef djd_to_datetime(jd):\n    \"\"\"\n    Converts a Julian Day to a datetime\n\n    Parameters\n    ----------\n    jd : float\n        Julian Day\n\n    Returns\n    -------\n    datetime.datetime\n        time to convert\n    \"\"\"\n    return jd + datetime.timedelta(days=31)\n\n\ndef datetime_to_string(time):\n    \"\"\"\n    Converts a datetime to a string\n\n    Parameters\n    ----------\n    time : datetime.datetime\n        time to convert\n\n    Returns\n    -------\n    str\n        date and time\n    \"\"\"", "solution": "\n\n    if time.tzinfo is None:\n        time_utc = pytz.utc.localize(time)\n    else:\n        time_utc = time.astimezone(pytz.utc)\n\n    djd_start = pytz.utc.localize(dt.datetime(1899, 12, 31, 12))\n    djd = (time_utc - djd_start).total_seconds() * 1.0/(60 * 60 * 24)\n\n    return djd"}
{"prompt": "def save_blocks(self, id_env, blocks):\n        \"\"\"\n        Save blocks from environment\n\n        :param id_env: Environment id\n        :param blocks: Lists of blocks in order. Ex: ['content one', 'content two', ...]\n\n        :return: None\n\n        :raise AmbienteNaoExisteError: Ambiente n\u00e3o cadastrado.\n        :raise InvalidValueError: Invalid parameter.\n        :raise UserNotAuthorizedError: Permiss\u00e3o negada.\n        :raise DataBaseError: Falha na networkapi ao acessar o banco de dados.\n        :raise XMLError: Falha na networkapi ao ler o XML de requisi\u00e7\u00e3o ou gerar o XML de resposta.\n        \"\"\"", "output": ".\n        \"\"\"\n        return self._save_blocks(id_env, blocks)\n\n    def _save_blocks(self, id_env, blocks):\n        \"\"\"\n        Save blocks from environment\n\n        :param id_env: Environment id\n        :param blocks: List of blocks in order. Ex: ['content one', 'content two',...]\n\n        :param id_env: Environment id\n        :return: None\n\n        :raise AmbienteNaoExisteError: Ambiente n\u00e3o cadastrado.\n        :raise InvalidValueError: Invalid parameter.\n        :raise UserNotAuthorizedError: Permiss", "solution": "\n\n        url = 'environment/save_blocks/'\n\n        map_dict = dict()\n\n        map_dict['id_env'] = id_env\n        map_dict['blocks'] = blocks\n\n        code, xml = self.submit({'map': map_dict}, 'POST', url)\n\n        return self.response(code, xml)"}
{"prompt": "def command(cmd):\n    \"\"\"Execute command and raise an exception upon an error.\n\n      >>> 'README' in command('ls')\n      True\n      >>> command('nonexistingcommand')  #doctest: +ELLIPSIS\n      Traceback (most recent call last):\n      ...\n      SdistCreationError\n\n    \"\"\"", "output": "\n    try:\n        return _command_cache[cmd]\n    except KeyError:\n        pass\n\n    # Check that the module name ends with \"main\"\n    if cmd[-3:] == \".py\":\n        if cmd[-3:] == \".pyw\":\n            # runpy() is a function in Pythonw mode, not a class\n            return _run_python_module(cmd)\n        else:\n            # runpy() is a class in w mode, not a function\n            return _run_class_mode(cmd)\n    else:\n        if cmd[-3:] == \".s\":\n            # s() is a string in", "solution": "\n    status, out = commands.getstatusoutput(cmd)\n    if status is not 0:\n        logger.error(\"Something went wrong:\")\n        logger.error(out)\n        raise SdistCreationError()\n    return out"}
{"prompt": "def ranker(self, X, meta):\n        \"\"\"\n        Sort the place features list by the score of its relevance.\n        \"\"\"", "output": "\n        if self.relevance_type == \"score\":\n            X = self.score_features(X)\n        return X\n\n    def score_features(self, X):\n        \"\"\"\n        Calculate the score of the features in the dataset.\n        \"\"\"\n        return self.model.score(X)\n\n    def get_feature_names(self):\n        \"\"\"\n        Return the names of the features.\n        \"\"\"\n        return self.model.get_feature_names()\n\n    def get_feature_types(self):\n        \"\"\"\n        Return the types of the features.\n        \"\"\"\n        return self.model.get_feature_types", "solution": "\n        # total score is just a sum of each row\n        total_score = X.sum(axis=1).transpose()\n        total_score = np.squeeze(np.asarray(total_score))  # matrix to array\n        ranks = total_score.argsort()\n        ranks = ranks[::-1]\n        # sort the list of dicts according to ranks\n        sorted_meta = [meta[r] for r in ranks]\n        sorted_X = X[ranks]\n        return (sorted_X, sorted_meta)"}
{"prompt": "def notebook_type(self, notebook_type):\n        \"\"\" Notebook type, acceptable values are 'jupyter' as well as any names\n        defined by external notebook hooks that have been installed.\n\n        \"\"\"", "output": "\n        return self._notebook_type\n\n    def _get_notebook_dir(self, notebook_dir):\n        \"\"\" Get the path to the notebook directory.\n\n        \"\"\"\n        if notebook_dir is None:\n            return None\n        else:\n            return os.path.join(notebook_dir, \"notebooks\")\n\n    def _get_notebook_file(self, notebook_file):\n        \"\"\" Get the path to the notebook file.\n\n        \"\"\"\n        return self._get_notebook_dir(notebook_file)\n\n    def _get_notebook_name(self, notebook_file):\n        \"\"\" Get the name of the notebook", "solution": "\n        if notebook_type is None or not isinstance(notebook_type, string_types):\n            raise ValueError(\"Notebook type must be a string\")\n        self._notebook_type = notebook_type.lower()"}
{"prompt": "def download_api(branch=None) -> str:\n    \"\"\"download API documentation from _branch_ of Habitica\\'s repo on Github\"\"\"", "output": "\n    if branch is None:\n        branch = _branch_\n\n    if not os.path.exists(os.path.join(repo_dir, \"docs\", \"api.rst\")):\n        print(\"Error: Could not find Habitica\\'s API documentation\")\n        return None\n\n    with open(os.path.join(repo_dir, \"docs\", \"api.rst\")) as f:\n        return f.read()\n\n\ndef get_version() -> str:\n    \"\"\"Get version from the current source\"\"\"\n    with open(os.path.join(repo_dir, \"version.txt\"))", "solution": "\n    habitica_github_api = 'https://api.github.com/repos/HabitRPG/habitica'\n    if not branch:\n        branch = requests.get(habitica_github_api + '/releases/latest').json()['tag_name']\n    curl = local['curl']['-sL', habitica_github_api + '/tarball/{}'.format(branch)]\n    tar = local['tar'][\n        'axzf', '-', '--wildcards', '*/website/server/controllers/api-v3/*', '--to-stdout']\n    grep = local['grep']['@api']\n    sed = local['sed']['-e', 's/^[ */]*//g', '-e', 's/  / /g', '-']\n    return (curl | tar | grep | sed)()"}
{"prompt": "def _err(self, errclass: str=\"error\", *args) -> \"Err\":\n        \"\"\"\n        Creates an error\n        \"\"\"", "output": "\n        return self.error(errclass, *args)\n\n    def _get_error_message(self, errclass: str) -> str:\n        \"\"\"\n        Returns the error message\n        \"\"\"\n        return self._err(errclass, *self.error_args)\n\n    def _get_error_args(self, errclass: str) -> List[str]:\n        \"\"\"\n        Returns the error arguments\n        \"\"\"\n        return self.error_args\n\n    def _get_error_msg(self, errclass: str) -> str:\n        \"\"\"\n        Returns the error message\n        \"\"\"\n        return self._err(err", "solution": "\n        error = self._new_err(errclass, *args)\n        if self.log_errs is True:\n            sep = \" \"\n            if self.log_format == \"csv\":\n                sep = \",\"\n            msg = str(datetime.now()) + sep + \\\n                self._errmsg(error, msgformat=self.log_format)\n            self.logger.error(msg)\n        print(self._errmsg(error))\n        self._add(error)\n        return error"}
{"prompt": "def inline_css(self, html):\n        \"\"\"Inlines CSS defined in external style sheets.\n        \"\"\"", "output": "\n        for element in html.xpath('//*[@style=\"text-align:center\"]'):\n            self.css.append(CSSRule(element, parent=self))\n\n    def _get_css_rules(self):\n        \"\"\"Returns a list of CSSRule objects.\n        \"\"\"\n        return self.css\n\n    def _get_css_selectors(self):\n        \"\"\"Returns a list of CSSSelector objects.\n        \"\"\"\n        return self.css\n\n    def _get_css_rules_by_type(self, selector_type):\n        \"\"\"Returns a list of CSSRule objects.\n        \"\"\"\n        return self._get", "solution": "\n        premailer = Premailer(html)\n        inlined_html = premailer.transform(pretty_print=True)\n        return inlined_html"}
{"prompt": "def halt_running():\n    \"\"\"Halts all running servers\"\"\"", "output": "\n    for server in servers:\n        server.halt()\n\ndef halt_stopped():\n    \"\"\"Halts all stopped servers\"\"\"\n    for server in servers:\n        server.halt()\n\ndef halt_running_servers():\n    \"\"\"Halts all running servers\"\"\"\n    for server in servers:\n        server.halt()\n\ndef halt_stopped_servers():\n    \"\"\"Halts all stopped servers\"\"\"\n    for server in servers:\n        server.halt()\n\ndef wait_for_server_to_stop(server):\n    \"\"\"Waits for a server to stop\"\"\"", "solution": "\n    vultr = Vultr(API_KEY)\n\n    try:\n            serverList = vultr.server.list()\n            #logging.info('Listing servers:\\n%s', dumps(\n            #serverList, indent=2\n        #))\n    except VultrError as ex:\n        logging.error('VultrError: %s', ex)\n\n    for serverID in serverList:\n        if serverList[serverID]['power_status'] == 'running':\n            logging.info(serverList[serverID]['label'] + \" will be gracefully shutdown.\")\n            vultr.server.halt(serverID)"}
{"prompt": "def register_cleanup(cleanup):\n    \"\"\"Register a cleanup function to be called at the end of every validation\n    task. Takes either a callable (including a class with a __call_ method),\n    or a class with a `cleanup` class method.\"\"\"", "output": "\n    if callable(cleanup):\n        return cleanup(cleanup)\n    else:\n        return cleanup\n\n\ndef _get_default_config_files(env):\n    \"\"\"Return default configuration files for the given environment.\"\"\"\n    config_files = [\n        os.path.join(env.topobjdir, 'etc', 'profile.d', 'config.py'),\n        os.path.join(env.topobjdir, 'etc', 'profile.d', 'config.py'),\n        os.path.join(env.topobjdir, 'etc', 'profile.d', 'config.py'),\n       ", "solution": "\n\n    if not callable(cleanup):\n        # Allow decorating a class with a `cleanup` classm ethod.\n        cleanup = cleanup.cleanup\n\n    CLEANUP_FUNCTIONS.append(cleanup.cleanup)\n    return cleanup"}
{"prompt": "def dump(mer_file, output_file='counts.fasta', options='', returncmd=False):\n    \"\"\"\n    Dumps output from jellyfish count into a human-readable format.\n    :param mer_file: Output from jellyfish count.\n    :param output_file: Where to store output. Default counts.fasta\n    :param options: Other options to pass to jellyfish. Input should be a string, with options typed as they would be\n    on the command line.\n    :param returncmd: If set to true, function will return the cmd string passed to subprocess as a third value.\n    :return: Stdout and stderr from calling jellyfish.\n    \"\"\"", "output": "\n    :return:\n    \"\"\"\n    if returncmd:\n        return subprocess.check_output(returncmd, shell=True)\n    else:\n        return subprocess.check_output(\n            'wc -l {0} {1} | wc -l {2} {3} | wc -l {4} {5} | '\n            'wc -l {6} | wc -l {7} | wc -l {8} | wc -l {9} | '\n            'wc -l {10} | wc -l {11} | wc -l {12} | '\n            'wc", "solution": "\n    cmd = 'jellyfish dump {} -o {} {}'.format(mer_file, output_file, options)\n    out, err = accessoryfunctions.run_subprocess(cmd)\n    if returncmd:\n        return out, err, cmd\n    else:\n        return out, err"}
{"prompt": "def list_deelgemeenten(self, gewest=2):\n        \"\"\"\n        List all `deelgemeenten` in a `gewest`.\n\n        :param gewest: The :class:`Gewest` for which the \\\n            `deelgemeenten` are wanted. Currently only Flanders is supported.\n        :rtype: A :class:`list` of :class:`Deelgemeente`.\n        \"\"\"", "output": "\n        return self.deelgemeenten(self.deelgemeenten_id, gewest)\n\n    def deelgemeenten_id(self):\n        \"\"\"\n        Get the deelgemeenten id of the current user.\n\n        :rtype: str\n        \"\"\"\n        return self.user_id\n\n    def get_deelgemeenten(self):\n        \"\"\"\n        Get the deelgemeenten.\n\n        :rtype: :class:`Gewest`\n        \"\"\"\n        return self.deelgemeenten\n\n    def get_deelgemeenten_", "solution": "\n        try:\n            gewest_id = gewest.id\n        except AttributeError:\n            gewest_id = gewest\n\n        if gewest_id != 2:\n            raise ValueError('Currently only deelgemeenten in Flanders are known.')\n\n        def creator():\n            return [Deelgemeente(dg['id'], dg['naam'], dg['gemeente_niscode']) for dg in self.deelgemeenten.values()]\n\n        if self.caches['permanent'].is_configured:\n            key = 'ListDeelgemeentenByGewestId#%s' % gewest_id\n            deelgemeenten = self.caches['permanent'].get_or_create(key, creator)\n        else:\n            deelgemeenten = creator()\n        for dg in deelgemeenten:\n            dg.set_gateway(self)\n        return deelgemeenten"}
{"prompt": "def knapsack_iterative_numpy(items, maxweight):\n    \"\"\"\n    Iterative knapsack method\n\n    maximize \\sum_{i \\in T} v_i\n    subject to \\sum_{i \\in T} w_i \\leq W\n\n    Notes:\n        dpmat is the dynamic programming memoization matrix.\n        dpmat[i, w] is the total value of the items with weight at most W\n        T is the set of indicies in the optimal solution\n    \"\"\"", "output": "\n    # initialize\n    if len(items) == 0:\n        return 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,", "solution": "\n    #import numpy as np\n    items = np.array(items)\n    weights = items.T[1]\n    # Find maximum decimal place (this problem is in NP)\n    max_exp = max([number_of_decimals(w_) for w_ in weights])\n    coeff = 10 ** max_exp\n    # Adjust weights to be integral\n    weights = (weights * coeff).astype(np.int)\n    values  = items.T[0]\n    MAXWEIGHT = int(maxweight * coeff)\n    W_SIZE = MAXWEIGHT + 1\n\n    dpmat = np.full((len(items), W_SIZE), np.inf)\n    kmat = np.full((len(items), W_SIZE), 0, dtype=np.bool)\n    idx_subset = []\n\n    for w in range(W_SIZE):\n        dpmat[0][w] = 0\n    for idx in range(1, len(items)):\n        item_val = values[idx]\n        item_weight = weights[idx]\n        for w in range(W_SIZE):\n            valid_item = item_weight <= w\n            prev_val = dpmat[idx - 1][w]\n            if valid_item:\n                prev_noitem_val = dpmat[idx - 1][w - item_weight]\n                withitem_val = item_val + prev_noitem_val\n                more_valuable = withitem_val > prev_val\n            else:\n                more_valuable = False\n            dpmat[idx][w] = withitem_val if more_valuable else prev_val\n            kmat[idx][w] = more_valuable\n    K = MAXWEIGHT\n    for idx in reversed(range(1, len(items))):\n        if kmat[idx, K]:\n            idx_subset.append(idx)\n            K = K - weights[idx]\n    idx_subset = sorted(idx_subset)\n    items_subset = [items[i] for i in idx_subset]\n    total_value = dpmat[len(items) - 1][MAXWEIGHT]\n    return total_value, items_subset"}
{"prompt": "def set(self, channel, state):\n        \"\"\"Set DAC value and enable output\"\"\"", "output": "\n        if state:\n            self.write(channel, 'DAC', 'on')\n        else:\n            self.write(channel, 'DAC', 'off')\n\n    def get(self, channel):\n        \"\"\"Get DAC value\"\"\"\n        return self.ask(channel)\n\n    def set_value(self, channel, value):\n        \"\"\"Set DAC value and enable output\"\"\"\n        if value:\n            self.write(channel, 'DAC', 'on')\n        else:\n            self.write(channel, 'DAC', 'off')\n\n    def get_value(self, channel):\n        \"\"\"Get", "solution": "\n        checked_val = self._check_dac_val(channel, state)\n        self._dac_enabled = 0x40\n        self.i2c.write8(self._dac_enabled, checked_val * 255)"}
{"prompt": "def _fulfillment_to_details(fulfillment):\n    \"\"\"Encode a fulfillment as a details dictionary\n\n    Args:\n        fulfillment: Crypto-conditions Fulfillment object\n    \"\"\"", "output": "\n    if not fulfillment.is_valid():\n        raise InvalidFulfillmentDataError(\n            'Invalid crypto-condition fulfillment data: %s' % fulfillment.errors)\n    return {\n        'id': fulfillment.id,\n        'name': fulfillment.name,\n        'description': fulfillment.description,\n        'fulfillment_id': fulfillment.id,\n        'fulfillment_name': fulfillment.name,\n        'fulfillment_description': fulfillment.description,\n        'fulfillment_fulfillment_id':", "solution": "\n\n    if fulfillment.type_name == 'ed25519-sha-256':\n        return {\n            'type': 'ed25519-sha-256',\n            'public_key': base58.b58encode(fulfillment.public_key).decode(),\n        }\n\n    if fulfillment.type_name == 'threshold-sha-256':\n        subconditions = [\n            _fulfillment_to_details(cond['body'])\n            for cond in fulfillment.subconditions\n        ]\n        return {\n            'type': 'threshold-sha-256',\n            'threshold': fulfillment.threshold,\n            'subconditions': subconditions,\n        }\n\n    raise UnsupportedTypeError(fulfillment.type_name)"}
{"prompt": "def requestOrdered(self, reqIdrs: List[str], instId: int,\n                       requests, byMaster: bool = False) -> Dict:\n        \"\"\"\n        Measure the time taken for ordering of a request and return it. Monitor\n        might have been reset due to view change due to which this method\n        returns None\n        \"\"\"", "output": "\n        if byMaster:\n            return requests\n        else:\n            return None\n\n    def requestOrder(self, reqIdr: Dict[str, Any],\n                   instId: int,\n                   requests, byMaster: bool = False) -> Dict:\n        \"\"\"\n        Measure the time taken for ordering of a request and return it. Monitor\n        might have been reset due to view change due to which this method\n        returns None\n        \"\"\"\n        if byMaster:\n            return requests\n        else:\n            return None\n\n    def requestList(self, requests: List[Dict],\n                   inst: int,\n                   byMaster: bool = False) -> Dict", "solution": "\n        now = time.perf_counter()\n        if self.acc_monitor:\n            self.acc_monitor.update_time(now)\n        durations = {}\n        for key in reqIdrs:\n            if key not in self.requestTracker:\n                logger.debug(\"Got untracked ordered request with digest {}\".\n                             format(key))\n                continue\n            if self.acc_monitor:\n                self.acc_monitor.request_ordered(key, instId)\n            if key in self.requestTracker.handled_unordered():\n                started = self.requestTracker.started(key)\n                logger.info('Consensus for ReqId: {} was achieved by {}:{} in {} seconds.'\n                            .format(key, self.name, instId, now - started))\n            duration = self.requestTracker.order(instId, key, now)\n            self.throughputs[instId].add_request(now)\n\n            if key in requests:\n                identifier = requests[key].request.identifier\n                self.clientAvgReqLatencies[instId].add_duration(identifier, duration)\n\n            durations[key] = duration\n\n        reqs, tm = self.numOrderedRequests[instId]\n        orderedNow = len(durations)\n        self.numOrderedRequests[instId] = (reqs + orderedNow,\n                                           tm + sum(durations.values()))\n\n        # TODO: Inefficient, as on every request a minimum of a large list is\n        # calculated\n        if min(r[0] for r in self.numOrderedRequests.values()) == (reqs + orderedNow):\n            # If these requests is ordered by the last instance then increment\n            # total requests, but why is this important, why cant is ordering\n            # by master not enough?\n            self.totalRequests += orderedNow\n            self.postOnReqOrdered()\n            if 0 == reqs:\n                self.postOnNodeStarted(self.started)\n\n        return durations"}
{"prompt": "def parse_image_response(self, response):\n        \"\"\"\n        Parse multiple objects from the RETS feed. A lot of string methods are used to handle the response before\n        encoding it back into bytes for the object.\n        :param response: The response from the feed\n        :return: list of SingleObjectParser\n        \"\"\"", "output": "\n        return [SingleObjectParser(item) for item in response.entries]\n\n    def parse_image_response_list(self, response_list):\n        \"\"\"\n        Parse multiple objects from the RETS feed.\n        :param response_list: The response from the feed\n        :return: list of SingleObjectParser\n        \"\"\"\n        return [SingleObjectParser(item) for item in response_list.entries]\n\n    def parse_image_response_list_list(self, response_list_list):\n        \"\"\"\n        Parse multiple objects from the RETS feed.\n        :param response_list_list: The response", "solution": "\n        if 'xml' in response.headers.get('Content-Type'):\n            # Got an XML response, likely an error code.\n            xml = xmltodict.parse(response.text)\n            self.analyze_reply_code(xml_response_dict=xml)\n\n        multi_parts = self._get_multiparts(response)\n        parsed = []\n        # go through each part of the multipart message\n        for part in multi_parts:\n            clean_part = part.strip('\\r\\n\\r\\n')\n            if '\\r\\n\\r\\n' in clean_part:\n                header, body = clean_part.split('\\r\\n\\r\\n', 1)\n            else:\n                header = clean_part\n                body = None\n            part_header_dict = {k.strip(): v.strip() for k, v in (h.split(':', 1) for h in header.split('\\r\\n'))}\n\n            # Some multipart requests respond with a text/XML part stating an error\n            if 'xml' in part_header_dict.get('Content-Type'):\n                # Got an XML response, likely an error code.\n                # Some rets servers give characters after the closing brace.\n                body = body[:body.index('/>') + 2]  if '/>' in body else body\n                xml = xmltodict.parse(body)\n                try:\n                    self.analyze_reply_code(xml_response_dict=xml)\n                except RETSException as e:\n                    if e.reply_code == '20403':\n                        # The requested object_id was not found.\n                        continue\n                    raise e\n\n            if body:\n                obj = self._response_object_from_header(\n                    obj_head_dict=part_header_dict,\n                    content=body.encode('latin-1') if six.PY3 else body)\n            else:\n                obj = self._response_object_from_header(obj_head_dict=part_header_dict)\n            parsed.append(obj)\n        return parsed"}
{"prompt": "def config_ctx(func):\n    \"\"\"\n    Decorator that makes decorated function use ConfigurationContext instead \\\n    of Context instance.\n\n    :param func: Decorated function.\n\n    :return: Decorated function.\n    \"\"\"", "output": "\n    def decorator(f):\n        @wraps(f)\n        def wrapper(*args, **kwargs):\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator\n\n\ndef get_config_value(name, default=None):\n    \"\"\"\n    Get value of configuration property.\n\n    :param name: Property name.\n    :param default: Default value.\n\n    :return: Property value.\n    \"\"\"\n    return config_manager.get_config_value(name, default)\n\n\ndef get_config_value_from_file(file_name, default=None):\n    \"\"\"\n    Get value of", "solution": "\n    # Create ConfigurationContext subclass\n    class _ConfigurationContext(ConfigurationContext):\n        # Set command name for the context class\n        cmd = func.__name__\n\n        # Set function name for the context class\n        fun = func.__name__\n\n    # Store the created context class with the decorated function\n    func._context_class = _ConfigurationContext  # pylint: disable=W0212\n\n    # Return the decorated function\n    return func"}
{"prompt": "def get_canonical_key(self, key_type, key_id):\n        \"\"\"\n        get_canonical_key returns the canonical form of a key given a key id.\n        For example, '12345' maps to shard 6. The canonical key at index 6\n        (say '12') is the canonical key id given the key id of '12345'. This\n        is useful for sets that need to exist on all shards. See\n        compute_canonical_key_ids for how these are calculated.\n\n        Keyword arguments:\n        key_type -- the type of key (e.g. 'canceled')\n        key_id -- the key id (e.g. '12345')\n\n        returns the canonical key string (e.g. 'canceled:{12}')\n        \"\"\"", "output": "id -- the index of the key in the list of shards (e.g. 0)\n        \"\"\"\n        return self.compute_key_id(key_type, key_id, shard_id=self.shard_id)\n\n    def compute_canonical_key_ids(self, key_type, key_id, shard_id=None):\n        \"\"\"\n        Compute the canonical form of a list of keys given a key id.\n\n        Keyword arguments:\n        key_type -- the type of key (e.g. 'canceled')\n        key_id -- the index of the key in the list of", "solution": "\n        canonical_key_id = self.get_canonical_key_id(key_id)\n        return self.get_key(key_type, canonical_key_id)"}
{"prompt": "def setCollapsed(self, state):\n        \"\"\"\n        Sets whether or not this toolbar is in a collapsed state.\n        \n        :return     <bool> changed\n        \"\"\"", "output": "\n        self.isCollapsed = state\n        return self\n\n    def setToolTips(self, state):\n        \"\"\"\n        Sets whether or not this toolbar is in a toolbar group.\n        \n        :param     state | <bool>\n        :param     toolbarGroup | <bool>\n        :return     <bool> changed\n        \"\"\"\n        self.isToolTips = state\n        return self\n\n    def setToolTipsGroup(self, stateGroup):\n        \"\"\"\n        Sets the toolbar group for the toolbar.\n        \n        :param      stateGroup | <str>\n        \"\"\"\n        self.toolbarGroup = stateGroup\n        return self\n\n    def set", "solution": "\n        if state == self._collapsed:\n            return False\n        \n        self._collapsed = state\n        self.refreshButton()\n        \n        if not self.signalsBlocked():\n            self.collapseToggled.emit(state)\n        \n        return True"}
{"prompt": "def update(self, old, backup_path=None):\n        \"\"\"\n        Update the old configuration file with new values.\n\n        :param old: old configuration to update.\n        :param backup_path: path to write a backup of the old config file.\n\n        :return:\n        \"\"\"", "output": "\n        if backup_path is None:\n            backup_path = self.config_path\n        if backup_path is None:\n            raise ValueError(\"No backup path provided\")\n\n        if not os.path.isfile(backup_path):\n            raise ValueError(\"File not found: %s\" % backup_path)\n\n        if backup_path.endswith(\".bak\"):\n            backup_path = backup_path[:-4]\n\n        if not os.path.isfile(backup_path):\n            raise ValueError(\"Backup file not found: %s\" % backup_path)\n\n        if backup_path.endswith(\".bak.bak\"):\n           ", "solution": "\n        for category in old.params.keys():\n            for parameter in old.params[category].keys():\n                if (category in self.params and parameter in self.params[category] and\n                        (old.params[category][parameter] != self.params[category][parameter]) and\n                        (category != \"version\")):\n                    print (\"Config value '%s.%s' \"\n                           \"in old configuration is different \"\n                           \"from the new version\\n\"\n                           \"[old value] = %s\\n\"\n                           \"[new value] = %s\"\n                           \"\" % (category, parameter,\n                                 old.params[category][parameter],\n                                 self.params[category][parameter]))\n                    answer = raw_input(\"Do you want to overwrite? ([y]/n) \")\n                    while answer.lower() not in ['y', 'yes', 'n', 'no']:\n                        answer = raw_input(\"Answer not recongnized. Enter 'y' or 'n'. \")\n\n                    if answer in ['n', 'no']:\n                        old_value = old.params[category][parameter]\n                        self.params[category][parameter] = old_value\n                elif not (category in self.params and\n                          parameter in self.params[category]):\n                    print (\"Deprecated config option '%s.%s' has \"\n                           \"been removed.\" % (category, parameter))\n        if backup_path is not None:\n            old.write_out_config(backup_path)\n            print \"Backup saved in %s.\" % backup_path"}
{"prompt": "def verify_record_permission(permission_factory, record):\n    \"\"\"Check that the current user has the required permissions on record.\n\n    In case the permission check fails, an Flask abort is launched.\n    If the user was previously logged-in, a HTTP error 403 is returned.\n    Otherwise, is returned a HTTP error 401.\n\n    :param permission_factory: permission factory used to check permissions.\n    :param record: record whose access is limited.\n    \"\"\"", "output": "\n    if not current_user.is_authenticated:\n        abort(401)\n    if not record.is_admin:\n        abort(403)\n    if not permission_factory.validate_access(record,'read'):\n        abort(401)\n    if not permission_factory.validate_access(record, 'write'):\n        abort(401)\n    if not permission_factory.validate_access(record, 'admin'):\n        abort(403)\n    if not current_user.has_permission(record,'read'):\n        abort(401)\n    if not current_user.has_permission(record, '", "solution": "\n    # Note, cannot be done in one line due overloading of boolean\n    # operations permission object.\n    if not permission_factory(record=record).can():\n        from flask_login import current_user\n        if not current_user.is_authenticated:\n            abort(401)\n        abort(403)"}
{"prompt": "def keep_vertices(self, indices_to_keep, ret_kept_faces=False):\n        \"\"\"\n        Keep the given vertices and discard the others, and any faces to which\n        they may belong.\n\n\n        If `ret_kept_faces` is `True`, return the original indices of the kept\n        faces. Otherwise return `self` for chaining.\n\n        \"\"\"", "output": "\n        if ret_kept_faces:\n            return indices_to_keep\n        else:\n            return indices_to_keep[indices_to_keep.nonzero()]\n\n    def remove_vertices(self, indices_to_remove, ret_kept_faces=False):\n        \"\"\"\n        Remove the given vertices and discard the others, and any faces\n        to which they may belong.\n\n\n        If `ret_kept_faces` is `True`, return the original indices of the kept\n        faces. Otherwise return `self` for chaining.\n\n        \"\"\"\n        if ret_kept_faces:\n            return indices_", "solution": "\n        import numpy as np\n\n        if self.v is None:\n            return\n\n        indices_to_keep = np.array(indices_to_keep, dtype=np.uint32)\n\n        initial_num_verts = self.v.shape[0]\n        if self.f is not None:\n            initial_num_faces = self.f.shape[0]\n            f_indices_to_keep = self.all_faces_with_verts(indices_to_keep, as_boolean=True)\n\n        # Why do we test this? Don't know. But we do need to test it before we\n        # mutate self.v.\n        vn_should_update = self.vn is not None and self.vn.shape[0] == initial_num_verts\n        vc_should_update = self.vc is not None and self.vc.shape[0] == initial_num_verts\n\n        self.v = self.v[indices_to_keep]\n\n        if vn_should_update:\n            self.vn = self.vn[indices_to_keep]\n        if vc_should_update:\n            self.vc = self.vc[indices_to_keep]\n\n        if self.f is not None:\n            v_old_to_new = np.zeros(initial_num_verts, dtype=int)\n            f_old_to_new = np.zeros(initial_num_faces, dtype=int)\n\n            v_old_to_new[indices_to_keep] = np.arange(len(indices_to_keep), dtype=int)\n            self.f = v_old_to_new[self.f[f_indices_to_keep]]\n            f_old_to_new[f_indices_to_keep] = np.arange(self.f.shape[0], dtype=int)\n\n        else:\n            # Make the code below work, in case there is somehow degenerate\n            # segm even though there are no faces.\n            f_indices_to_keep = []\n\n        if self.segm is not None:\n            new_segm = {}\n            for segm_name, segm_faces in self.segm.items():\n                faces = np.array(segm_faces, dtype=int)\n                valid_faces = faces[f_indices_to_keep[faces]]\n                if len(valid_faces):\n                    new_segm[segm_name] = f_old_to_new[valid_faces]\n            self.segm = new_segm if new_segm else None\n\n        if hasattr(self, '_raw_landmarks') and self._raw_landmarks is not None:\n            self.recompute_landmarks()\n\n        return np.nonzero(f_indices_to_keep)[0] if ret_kept_faces else self"}
{"prompt": "def knot_removal_alpha_j(u, degree, knotvector, num, idx):\n    \"\"\" Computes :math:`\\\\alpha_{j}` coefficient for knot removal algorithm.\n\n    Please refer to Eq. 5.29 of The NURBS Book by Piegl & Tiller, 2nd Edition, p.184 for details.\n\n    :param u: knot\n    :type u: float\n    :param degree: degree\n    :type degree: int\n    :param knotvector: knot vector\n    :type knotvector: tuple\n    :param num: knot removal index\n    :type num: int\n    :param idx: iterator index\n    :type idx: int\n    :return: coefficient value\n    :rtype: float\n    \"\"\"", "output": "type num: int\n    :param idx: knot removal value\n    :type idx: int\n    :return: :math:`\\\\alpha_{j}` coefficient for knot removal algorithm\n    :rtype: float\n    \"\"\"\n    if knotvector == ():\n        return 0.0\n    else:\n        return num * (idx - degree) / (num - 1)\n\n\ndef knot_removal_alpha_i(u, degree, knotvector, num, idx):\n    \"\"\" Computes :math:`\\\\alpha_{i}` coefficient for knot removal algorithm.\n\n    Please refer to Eq. 5.29 of The N", "solution": "\n    return (u - knotvector[idx - num]) / (knotvector[idx + degree + 1] - knotvector[idx - num])"}
{"prompt": "def listen_init(self):\n        \"\"\"Setup the service to listen for clients.\"\"\"", "output": "\n        self.hass.services.register(DOMAIN, SERVICE_PLAY_MEDIA, self.play_media)\n        self.hass.services.register(DOMAIN, SERVICE_PLAY_MEDIA_PAUSE, self.play_media_pause)\n        self.hass.services.register(DOMAIN, SERVICE_MEDIA_PLAY, self.play_media)\n        self.hass.services.register(DOMAIN, SERVICE_MEDIA_PAUSE, self.media_play)\n        self.hass.services.register(DOMAIN, SERVICE_MEDIA_STOP, self.media_stop)\n        self.hass.services.", "solution": "\n        self.dispatcher = ObjectDispatch(self)\n        self.factory = MsgPackProtocolFactory(self.dispatcher)\n        self.server = UnixServer(self.loop, self.factory, self.path)\n        self.server.start()"}
{"prompt": "def perturbed_mst(X, num_perturbations=20, metric='euclidean', jitter=None):\r\n  \"\"\"Builds a graph as the union of several MSTs on perturbed data.\r\n  Reference: http://ecovision.mit.edu/~sloop/shao.pdf, page 8\r\n  jitter refers to the scale of the gaussian noise added for each perturbation.\r\n  When jitter is None, it defaults to the 5th percentile interpoint distance.\r\n  Note that metric cannot be 'precomputed', as multiple MSTs are computed.\"\"\"", "output": "\r\n  if jitter is None:\r\n    jitter = 5\r\n  if metric == 'precomputed':\r\n    return X\r\n  elif metric == 'euclidean':\r\n    return np.sqrt(perturbations_euclidean(X, num_perturbations))\r\n  else:\r\n    raise ValueError('Unknown metric: %s' % metric)\r\n\r\n\r\ndef perturbed_mst(X, num_perturbations=20, metric='euclidean', jitter=None):\r\n  \"\"\"Builds a graph as the union of several MSTs on perturbed data.\r\n  Reference: http://ecovision.mit.edu/~sloop", "solution": "\r\n  assert metric != 'precomputed'\r\n  D = pairwise_distances(X, metric=metric)\r\n  if jitter is None:\r\n    jitter = np.percentile(D[D>0], 5)\r\n  W = minimum_spanning_tree(D)\r\n  W = W + W.T\r\n  W.data[:] = 1.0  # binarize\r\n  for i in range(num_perturbations):\r\n    pX = X + np.random.normal(scale=jitter, size=X.shape)\r\n    pW = minimum_spanning_tree(pairwise_distances(pX, metric=metric))\r\n    pW = pW + pW.T\r\n    pW.data[:] = 1.0\r\n    W = W + pW\r\n  # final graph is the average over all pertubed MSTs + the original\r\n  W.data /= (num_perturbations + 1.0)\r\n  return Graph.from_adj_matrix(W)"}
{"prompt": "def shear_from_matrix(matrix):\n    \"\"\"Return shear angle, direction and plane from shear matrix.\n\n    >>> angle  = np.pi / 2.0\n    >>> direct = [0.0, 1.0, 0.0]\n    >>> point  = [0.0, 0.0, 0.0]\n    >>> normal = np.cross(direct, np.roll(direct,1))\n    >>> S0 = shear_matrix(angle, direct, point, normal)\n    >>> angle, direct, point, normal = shear_from_matrix(S0)\n    >>> S1 = shear_matrix(angle, direct, point, normal)\n    >>> is_same_transform(S0, S1)\n    True\n\n    \"\"\"", "output": "\n\n    \"\"\"\n    m = np.array(matrix, dtype=np.float64, copy=False)\n    normal = np.array(point, dtype=np.float64, copy=False)\n    if np.iscomplexobj(m):\n        return np.array([m.real, -m.imag, m.imag])\n    else:\n        return np.array([m.real, m.imag])\n\n\ndef shear_matrix_from_matrix(matrix):\n    \"\"\"Return shear angle, direction and plane from shear matrix.\n\n    >>> angle = np.pi / 2.0", "solution": "\n    M = np.array(matrix, dtype=np.float64, copy=False)\n    M33 = M[:3, :3]\n    # normal: cross independent eigenvectors corresponding to the eigenvalue 1\n    w, V = np.linalg.eig(M33)\n\n    i = np.where(abs(np.real(w) - 1.0) < 1e-4)[0]\n    if len(i) < 2:\n        raise ValueError(\"no two linear independent eigenvectors found %s\" % w)\n    V = np.real(V[:, i]).squeeze().T\n    lenorm = -1.0\n    for i0, i1 in ((0, 1), (0, 2), (1, 2)):\n        n = np.cross(V[i0], V[i1])\n        w = vector_norm(n)\n        if w > lenorm:\n            lenorm = w\n            normal = n\n    normal /= lenorm\n    # direction and angle\n    direction = np.dot(M33 - np.identity(3), normal)\n    angle = vector_norm(direction)\n    direction /= angle\n    angle = math.atan(angle)\n    # point: eigenvector corresponding to eigenvalue 1\n    w, V = np.linalg.eig(M)\n\n    i = np.where(abs(np.real(w) - 1.0) < 1e-8)[0]\n    if not len(i):\n        raise ValueError(\"no eigenvector corresponding to eigenvalue 1\")\n    point = np.real(V[:, i[-1]]).squeeze()\n    point /= point[3]\n    return angle, direction, point, normal"}
{"prompt": "def mfa_status(self, mfa_status):\n        \"\"\"\n        Sets the mfa_status of this AccountInfo.\n        The enforcement status of the multi-factor authentication, either 'enforced' or 'optional'.\n\n        :param mfa_status: The mfa_status of this AccountInfo.\n        :type: str\n        \"\"\"", "output": "\n\n        self._mfa_status = mfa_status\n\n    @property\n    def mfa_status_type(self):\n        \"\"\"\n        Gets the mfa_status_type of this AccountInfo.\n        The type of the authenticity, if specified, must be 'enforced'.\n\n        :return: The mfa_status_type of this AccountInfo.\n        :rtype: str\n        \"\"\"\n        return self._mfa_status_type\n\n    @mfa_status_type.setter\n    def mfa_status_type(self, value):\n        \"\"\"\n        Sets the mfa_status_type", "solution": "\n        allowed_values = [\"enforced\", \"optional\"]\n        if mfa_status not in allowed_values:\n            raise ValueError(\n                \"Invalid value for `mfa_status` ({0}), must be one of {1}\"\n                .format(mfa_status, allowed_values)\n            )\n\n        self._mfa_status = mfa_status"}
{"prompt": "def univariate_envelope_plot(x, mean, std, ax=None, base_alpha=0.375, envelopes=[1, 3], lb=None, ub=None, expansion=10, **kwargs):\n    \"\"\"Make a plot of a mean curve with uncertainty envelopes.\n    \"\"\"", "output": "\n    if ax is None:\n        ax = plt.gca()\n\n    if lb is None:\n        lb = 0.375\n\n    if ub is None:\n        ub = 0.5\n\n    if expansion < 0:\n        raise ValueError(\"expansion must be >= 0\")\n\n    if lb < 0:\n        raise ValueError(\"lb must be >= 0\")\n\n    if ub < 0:\n        raise ValueError(\"ub must be >= 0\")\n\n    if mean < 0:\n        raise ValueError(\"mean must be >= 0\")\n\n    if std < 0:\n        raise ValueError(\"std must be >= 0\")\n\n    if not ax.is_first_row", "solution": "\n    if ax is None:\n        f = plt.figure()\n        ax = f.add_subplot(1, 1, 1)\n    elif ax == 'gca':\n        ax = plt.gca()\n    \n    mean = scipy.asarray(mean, dtype=float).copy()\n    std = scipy.asarray(std, dtype=float).copy()\n    \n    # Truncate the data so matplotlib doesn't die:\n    if lb is not None and ub is not None and expansion != 1.0:\n        expansion *= ub - lb\n        ub = ub + expansion\n        lb = lb - expansion\n    if ub is not None:\n        mean[mean > ub] = ub\n    if lb is not None:\n        mean[mean < lb] = lb\n    \n    l = ax.plot(x, mean, **kwargs)\n    color = plt.getp(l[0], 'color')\n    e = []\n    for i in envelopes:\n        lower = mean - i * std\n        upper = mean + i * std\n        if ub is not None:\n            lower[lower > ub] = ub\n            upper[upper > ub] = ub\n        if lb is not None:\n            lower[lower < lb] = lb\n            upper[upper < lb] = lb\n        e.append(ax.fill_between(x, lower, upper, facecolor=color, alpha=base_alpha / i))\n    return (l, e)"}
{"prompt": "def require(executable: str, explanation: str = \"\") -> None:\n    \"\"\"\n    Ensures that the external tool is available.\n    Asserts upon failure.\n    \"\"\"", "output": "\n    if not os.path.exists(executable):\n        raise Exception(f\"Executable {executable} does not exist.\")\n    if not os.path.isfile(explanation):\n        raise Exception(f\"Explanation {explanation} does not exist.\")\n\n\ndef get_build_info(\n    *,\n    version: str = \"auto\",\n    build_number: str = \"auto\",\n    build_type: str = \"auto\",\n    build_number: str = \"auto\",\n    build_type_name: str = \"auto\",\n    build_number_name: str = \"auto\",\n    build_type", "solution": "\n    assert shutil.which(executable), \"Need {!r} on the PATH.{}\".format(\n        executable, \"\\n\" + explanation if explanation else \"\")"}
{"prompt": "def p_variable_declaration(self, p):\n        \"\"\"variable_declaration : identifier\n                                | identifier initializer\n        \"\"\"", "output": "\n        identifier = p[2]\n        p[0] = self.handleModifiers(p)\n        p[0].addChildren(p[3])\n        p[0].addChildren(p[5])\n\n    def p_variable_initializer(self, p):\n        \"\"\"variable_initializer : identifier\n                                | identifier initializer\n        \"\"\"\n        identifier = p[1]\n        initializer = p[2]\n        p[0] = self.handleModifiers(p)\n        p[0].addChildren(p[3])\n        p[0].addChildren(p[5])\n\n    def p_identifier(", "solution": "\n        if len(p) == 2:\n            p[0] = self.asttypes.VarDecl(p[1])\n            p[0].setpos(p)  # require yacc_tracking\n        else:\n            p[0] = self.asttypes.VarDecl(p[1], p[2])\n            p[0].setpos(p, additional=(('=', 2),))"}
{"prompt": "def execution():\n    \"\"\"\n    Collect all the sys.doc output from each minion and return the aggregate\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run doc.execution\n    \"\"\"", "output": "\n    return _collect_docs()\n\n\ndef _collect_docs():\n    \"\"\"\n    Collect all the minion's doc output\n\n    CLI Example:\n\n   .. code-block:: python\n\n        salt-run doc.doc_aggregate\n    \"\"\"\n    return __doc_aggregate__()\n\n\ndef _doc_aggregate():\n    \"\"\"\n    Collect all the minion's doc output from each minion\n\n    CLI Example:\n\n   .. code-block:: bash\n\n        salt-run doc.doc_aggregate\n    \"\"\"\n    return __aggregate__()\n\n\ndef _aggregate_all():\n    \"\"\"\n    Aggregate all the minion's doc output\n\n    CLI Example:", "solution": "\n    client = salt.client.get_local_client(__opts__['conf_file'])\n\n    docs = {}\n    try:\n        for ret in client.cmd_iter('*', 'sys.doc', timeout=__opts__['timeout']):\n            for v in six.itervalues(ret):\n                docs.update(v)\n    except SaltClientError as exc:\n        print(exc)\n        return []\n\n    i = itertools.chain.from_iterable([six.iteritems(docs['ret'])])\n    ret = dict(list(i))\n\n    return ret"}
{"prompt": "def parse_sort_key(identity: str, sort_key_string: str) -> 'Key':\n        \"\"\" Parses a flat key string and returns a key \"\"\"", "output": "\n        return identity.split(sort_key_string, 1)\n\n    def parse_sort_key_value(self, sort_key: str, value: str) -> 'Key':\n        \"\"\" Parses a flat key value and returns a key \"\"\"\n        return self.parse_sort_key(sort_key, value)\n\n    def parse_sort_key_value_list(self, sort_key: str, value_list: list) -> 'Key':\n        \"\"\" Parses a flat key value list and returns a key \"\"\"\n        return self.parse_sort_key_value(sort_key, value", "solution": "\n        parts = sort_key_string.split(Key.PARTITION)\n        key_type = KeyType.DIMENSION\n        if parts[2]:\n            key_type = KeyType.TIMESTAMP\n        return Key(key_type, identity, parts[0], parts[1].split(Key.DIMENSION_PARTITION)\n                   if parts[1] else [],\n                   parser.parse(parts[2]) if parts[2] else None)"}
{"prompt": "def bytes_to_unicode(byte_data, escape, skip_printable=False):\n    \"\"\"\n    Decode given bytes using specified escaping method.\n    :param byte_data: The byte-like object with bytes to decode.\n    :param escape: The escape method to use.\n    :param skip_printable: If True, don't escape byte_data with all 'printable ASCII' bytes. Defaults to False.\n    :return: New unicode string, escaped with the specified method if needed.\n    \"\"\"", "output": "\n    if isinstance(byte_data, bytes):\n        return byte_data.decode(escape)\n    else:\n        return str(byte_data).decode(escape)\n\n\ndef get_file_contents(filename):\n    \"\"\"\n    Get the contents of a file.\n    :param filename: The file name to read.\n    :return: The contents of the file as a string.\n    \"\"\"\n    with open(filename, 'rb') as f:\n        return f.read()\n\n\ndef get_file_contents_as_string(filename):\n    \"\"\"\n    Get the contents of a file as a string.", "solution": "\n    if isnumber(byte_data):\n        if skip_printable:\n            return num2unistr(byte_data)\n        else:\n            byte_data = num2bytes(byte_data)\n    else:\n        assert (isinstance(byte_data, type(b'')))\n        if skip_printable and all(0x20 <= bval(ch) <= 0x7E for ch in byte_data):\n            escape = STRING_ESCAPE_RAW\n\n    if escape == STRING_ESCAPE_RAW:\n        return byte_data.decode('latin-1')\n    elif escape == STRING_ESCAPE_PRINT:\n        return escape_ascii(byte_data)\n    elif escape == STRING_ESCAPE_UTF8:\n        return escape_utf8(byte_data)\n    elif escape == STRING_ESCAPE_BASE64:\n        return codecs.decode(base64.b64encode(byte_data), 'latin-1')\n    else:\n        raise UnicodeEncodeError(\"Unknown escape option\")"}
{"prompt": "def paragraph_is_fully_commented(lines, comment, main_language):\n    \"\"\"Is the paragraph fully commented?\"\"\"", "output": "\n    for line in lines:\n        if line.startswith(comment):\n            return True\n    return False\n\n\ndef remove_paragraph_indent(lines):\n    \"\"\"Remove indentation from lines.\"\"\"\n    for line in lines:\n        if line.startswith(INDENT):\n            lines.remove(line)\n\n\ndef remove_line_indent(lines):\n    \"\"\"Remove indentation from lines.\"\"\"\n    for line in lines:\n        if line.startswith(INDENT):\n            lines.remove(line)\n\n\ndef remove_line_length(lines):\n    \"\"\"Remove length of line.\"\"\"\n    for line in lines:\n        if len(line)", "solution": "\n    for i, line in enumerate(lines):\n        if line.startswith(comment):\n            if line.startswith((comment + ' %', comment + ' ?', comment + ' !')) and is_magic(line, main_language):\n                return False\n            continue\n        return i > 0 and _BLANK_LINE.match(line)\n    return True"}
{"prompt": "def to_displacements(self):\n        \"\"\"\n        Converts position coordinates of trajectory into displacements between consecutive frames\n        \"\"\"", "output": "\n        return self._to_displacements(self.trajectory.x, self.trajectory.y)\n\n    def _to_displacements(self, x, y):\n        \"\"\"\n        Converts position coordinates of trajectory into displacements between\n        consecutive frames\n        \"\"\"\n        return np.vstack((x, y)).T\n\n    def _to_trajectory(self):\n        \"\"\"\n        Returns a trajectory object\n        \"\"\"\n        return self.trajectory\n\n    def _to_trajectory_with_time(self):\n        \"\"\"\n        Returns a trajectory object\n        \"\"\"\n        return self.trajectory.copy()\n\n    def _to_trajectory_with_", "solution": "\n        if not self.coords_are_displacement:\n            displacements = np.subtract(self.frac_coords, np.roll(self.frac_coords, 1, axis=0))\n            displacements[0] = np.zeros(np.shape(self.frac_coords[0]))\n            # Deal with PBC\n            displacements = [np.subtract(item, np.round(item)) for item in displacements]\n\n            self.frac_coords = displacements\n            self.coords_are_displacement = True\n        return"}
{"prompt": "def predict_encoding(file_path, n_lines=20):\n    \"\"\"Get file encoding of a text file\"\"\"", "output": "\n    with open(file_path, 'r') as f:\n        lines = f.readlines()\n        if len(lines) < n_lines:\n            return None\n        encoding = []\n        for line in lines[n_lines - n_lines_start:]:\n            if line.startswith(b'coding='):\n                encoding.append(line.split(b'coding=')[1].strip())\n            elif line.startswith(b'coding='):\n                encoding.append(line.split(b'coding=')[1].strip())\n        return encoding\n\ndef read_encoding(file_path, n", "solution": "\n    import chardet\n\n    # Open the file as binary data\n    with open(file_path, 'rb') as f:\n        # Join binary lines for specified number of lines\n        rawdata = b''.join([f.readline() for _ in range(n_lines)])\n\n    return chardet.detect(rawdata)['encoding']"}
{"prompt": "def queue_declare(self, queue, durable, exclusive, auto_delete,\n            warn_if_exists=False, arguments=None):\n        \"\"\"Declare a named queue.\"\"\"", "output": "\n        return self.channel.queue_declare(queue, durable,\n            exclusive=exclusive, auto_delete=auto_delete,\n            durable_args=arguments)\n\n    def queue_delete(self, queue, if_unused=False, if_empty=False,\n            callback=None, arguments=None):\n        \"\"\"Delete a named queue.\"\"\"\n        return self.channel.queue_delete(queue, if_unused, if_empty, callback,\n            if_empty)\n\n    def queue_purge(self, queue, if_unused=False, if_empty=False,\n            callback=None,", "solution": "\n        if warn_if_exists and self.queue_exists(queue):\n            warnings.warn(QueueAlreadyExistsWarning(\n                QueueAlreadyExistsWarning.__doc__))\n\n        return self.channel.queue_declare(queue=queue,\n                                          durable=durable,\n                                          exclusive=exclusive,\n                                          auto_delete=auto_delete,\n                                          arguments=arguments)"}
{"prompt": "def _merge_page(self, existing_page, fresh_page):\n        \"\"\"\n        Utility method for merging info from `brozzler.Page` instances\n        representing the same url but with possibly different metadata.\n        \"\"\"", "output": "\n        if existing_page.url!= fresh_page.url:\n            raise ValueError(\"URLs don't match\")\n        if existing_page.metadata!= fresh_page.metadata:\n            raise ValueError(\"Metadata doesn't match\")\n        if existing_page.id!= fresh_page.id:\n            raise ValueError(\"Id doesn't match\")\n        if existing_page.url!= fresh_page.url:\n            raise ValueError(\"URLs don't match\")\n        if existing_page.metadata!= fresh_page.metadata:\n            raise ValueError(\"Metadata doesn't match\")\n        if existing_page.id!= fresh_page", "solution": "\n        existing_page.priority += fresh_page.priority\n        existing_page.hashtags = list(set(\n            existing_page.hashtags + fresh_page.hashtags))\n        existing_page.hops_off = min(\n                existing_page.hops_off, fresh_page.hops_off)"}
{"prompt": "def _add_pipe_by_rectangle_rotation(\n        self,\n        outer_radius,\n        inner_radius,\n        length,\n        R=numpy.eye(3),\n        x0=numpy.array([0.0, 0.0, 0.0]),\n        lcar=None,\n    ):\n        \"\"\"Hollow cylinder.\n        Define a rectangle, extrude it by rotation.\n        \"\"\"", "output": "\n        if lcar is None:\n            lcar = numpy.array([0.0, 0.0, 0.0])\n        if x0 is None:\n            x0 = numpy.array([0.0, 0.0, 0.0])\n        if R is None:\n            R = numpy.eye(3)\n        if lcar is None:\n            lcar = numpy.array([0.0, 0.0, 0.0])\n        if x0 is None:\n            x0 = numpy.array([0.0, 0.0, 0.0])\n        if length is None", "solution": "\n        self.add_comment(\"Define rectangle.\")\n        X = numpy.array(\n            [\n                [0.0, outer_radius, -0.5 * length],\n                [0.0, outer_radius, +0.5 * length],\n                [0.0, inner_radius, +0.5 * length],\n                [0.0, inner_radius, -0.5 * length],\n            ]\n        )\n        # Apply transformation.\n        X = [numpy.dot(R, x) + x0 for x in X]\n        # Create points set.\n        p = [self.add_point(x, lcar=lcar) for x in X]\n\n        # Define edges.\n        e = [\n            self.add_line(p[0], p[1]),\n            self.add_line(p[1], p[2]),\n            self.add_line(p[2], p[3]),\n            self.add_line(p[3], p[0]),\n        ]\n\n        rot_axis = [0.0, 0.0, 1.0]\n        rot_axis = numpy.dot(R, rot_axis)\n        point_on_rot_axis = [0.0, 0.0, 0.0]\n        point_on_rot_axis = numpy.dot(R, point_on_rot_axis) + x0\n\n        # Extrude all edges three times by 2*Pi/3.\n        previous = e\n        angle = \"2*Pi/3\"\n        all_surfaces = []\n        # com = []\n        self.add_comment(\"Extrude in 3 steps.\")\n        for i in range(3):\n            self.add_comment(\"Step {}\".format(i + 1))\n            for k, p in enumerate(previous):\n                # ts1[] = Extrude {{0,0,1}, {0,0,0}, 2*Pi/3}{Line{tc1};};\n                top, surf, _ = self.extrude(\n                    p,\n                    rotation_axis=rot_axis,\n                    point_on_axis=point_on_rot_axis,\n                    angle=angle,\n                )\n                # if k==0:\n                #     com.append(surf)\n                # else:\n                #     all_names.appends(surf)\n                all_surfaces.append(surf)\n                previous[k] = top\n        #\n        # cs = CompoundSurface(com)\n        # Now just add surface loop and volume.\n        # all_surfaces = all_names + [cs]\n        surface_loop = self.add_surface_loop(all_surfaces)\n        vol = self.add_volume(surface_loop)\n        return vol"}
{"prompt": "def logTwisted():\n    \"\"\"\n    Integrate twisted's logger with our logger.\n\n    This is done in a separate method because calling this imports and sets\n    up a reactor.  Since we want basic logging working before choosing a\n    reactor, we need to separate these.\n    \"\"\"", "output": "\n    from twisted.python import log\n    log.startLogging(sys.stdout)\n\n    # Set up a log observer which logs to a file.\n    observer = log.PythonLoggingObserver()\n\n    # Set up a log observer which logs to a file.\n    observer.addLogObserver(log.FileLogObserver(sys.stdout))\n\n    # Set up a log observer which logs to a file.\n    observer.addLogObserver(log.FileLogObserver(sys.stderr))\n\n    # Set up a log observer which logs to a file.\n    observer.addLogObserver(log.FileLogObserver(sys.stdout", "solution": "\n    global _initializedTwisted\n\n    if _initializedTwisted:\n        return\n\n    debug('log', 'Integrating twisted logger')\n\n    # integrate twisted's logging with us\n    from twisted.python import log as tlog\n\n    # this call imports the reactor\n    # that is why we do this in a separate method\n    from twisted.spread import pb\n\n    # we don't want logs for pb.Error types since they\n    # are specifically raised to be handled on the other side\n    observer = _getTheTwistedLogObserver()\n    observer.ignoreErrors([pb.Error, ])\n    tlog.startLoggingWithObserver(observer.emit, False)\n\n    _initializedTwisted = True"}
{"prompt": "def resolve_parameters(\n        val: Any,\n        param_resolver: 'cirq.ParamResolverOrSimilarType') -> Any:\n    \"\"\"Resolves symbol parameters in the effect using the param resolver.\n\n    This function will use the `_resolve_parameters_` magic method\n    of `val` to resolve any Symbols with concrete values from the given\n    parameter resolver.\n\n    Args:\n        val: The object to resolve (e.g. the gate, operation, etc)\n        param_resolver: the object to use for resolving all symbols\n\n    Returns:\n        a gate or operation of the same type, but with all Symbols\n        replaced with floats according to the given ParamResolver.\n        If `val` has no `_resolve_parameters_` method or if it returns\n        NotImplemented, `val` itself is returned.\n    \"\"\"", "output": " symbols in the\n        parameter resolver.\n    \"\"\"\n    if isinstance(param_resolver, ParamResolver):\n        return _resolve_parameters_(\n            param_resolver.resolve_gate(val, param_resolver.param_type),\n            param_resolver.resolve_operation(val, param_resolver.param_type))\n    elif isinstance(param_resolver, ParamResolverOrSimilarType):\n        return _resolve_parameters_(\n            param_resolver.resolve_gate(val, param_resolver.param_type),\n            param_resolver.resolve_operation(val, param_resolver.param_type))\n    else:", "solution": "\n    if not param_resolver:\n        return val\n\n    # Ensure its a dictionary wrapped in a ParamResolver.\n    from cirq import ParamResolver  # HACK: break cycle.\n    param_resolver = ParamResolver(param_resolver)\n    if isinstance(val, sympy.Basic):\n        return param_resolver.value_of(val)\n\n    getter = getattr(val, '_resolve_parameters_', None)\n    result = NotImplemented if getter is None else getter(param_resolver)\n\n    if result is not NotImplemented:\n        return result\n    else:\n        return val"}
{"prompt": "def set_data(self, pos=None, color=None):\n        \"\"\"Set the data\n\n        Parameters\n        ----------\n        pos : float\n            Position of the line along the axis.\n        color : list, tuple, or array\n            The color to use when drawing the line. If an array is given, it\n            must be of shape (1, 4) and provide one rgba color per vertex.\n        \"\"\"", "output": "\n        if pos is None:\n            pos = self.pos\n        if color is None:\n            color = self.color\n        if pos is None:\n            return\n        if isinstance(color, (list, tuple)):\n            self.color = color\n        elif isinstance(color, np.ndarray):\n            self.color = color.tolist()\n        else:\n            raise TypeError('color must be of shape (1, 4) and provide one rgba color per vertex')\n\n    def draw(self, renderer):\n        \"\"\"Draw the polygon\n\n        Parameters\n        ----------\n        renderer : Renderer\n            The renderer to use.\n        \"\"\"\n        if self.", "solution": "\n        if pos is not None:\n            pos = float(pos)\n            xy = self._pos\n            if self._is_vertical:\n                xy[0, 0] = pos\n                xy[0, 1] = -1\n                xy[1, 0] = pos\n                xy[1, 1] = 1\n            else:\n                xy[0, 0] = -1\n                xy[0, 1] = pos\n                xy[1, 0] = 1\n                xy[1, 1] = pos\n            self._changed['pos'] = True\n\n        if color is not None:\n            color = np.array(color, dtype=np.float32)\n            if color.ndim != 1 or color.shape[0] != 4:\n                raise ValueError('color must be a 4 element float rgba tuple,'\n                                 ' list or array')\n            self._color = color\n            self._changed['color'] = True"}
{"prompt": "def set_col_name(self, index, name):\n        \"\"\"\n        Sets the column name.\n\n        :param index: the 0-based row index\n        :type index: int\n        :param name: the name of the column\n        :type name: str\n        \"\"\"", "output": "\n        self.set_row_name(index, name)\n        self.set_col_name(index, name)\n\n    def set_row_name(self, index, name):\n        \"\"\"\n        Sets the row name.\n\n        :param index: the 0-based row index\n        :type index: int\n        :param name: the name of the row\n        :type name: str\n        \"\"\"\n        self.set_row_name(index, name)\n        self.set_col_name(index, name)\n\n    def set_col_width(self, index, width):\n        \"\"\"\n        Sets the", "solution": "\n        javabridge.call(self.jobject, \"setColName\", \"(ILjava/lang/String;)V\", index, name)"}
{"prompt": "def main():\n    \"\"\"Main function for :command:`fabulous-image`.\"\"\"", "output": "\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument('--version', action='version', version=__version__)\n    parser.add_argument('--verbose', '-v', action='store_true')\n    parser.add_argument('--no-color', '-n', action='store_true')\n    parser.add_argument('--no-deps', '-d', action='store_true')\n    parser.add_argument('--no-deps-ignore', '-i', action='store_true')\n    parser.add_argument('--no-deps-ignore-ignore', '-m', action='", "solution": "\n    import optparse\n    parser = optparse.OptionParser()\n    parser.add_option(\n        \"-w\", \"--width\", dest=\"width\", type=\"int\", default=None,\n        help=(\"Width of printed image in characters.  Default: %default\"))\n    (options, args) = parser.parse_args(args=sys.argv[1:])\n    for imgpath in args:\n        for line in Image(imgpath, options.width):\n            printy(line)"}
{"prompt": "def D(self, ID, asp):\n        \"\"\" Returns the dexter aspect of an object. \"\"\"", "output": "\n        return self.get_object_aspect(asp)\n\n    def get_object_aspect(self, asp):\n        \"\"\" Returns the dexter aspect of an object. \"\"\"\n        return self.get_aspect_from_object(asp)\n\n    def get_aspect_from_object(self, obj):\n        \"\"\" Returns the dexter aspect of an object. \"\"\"\n        return self.get_aspect_from_object_aspect(obj)\n\n    def get_aspect_from_object_aspect(self, obj):\n        \"\"\" Returns the dexter aspect of an object. \"\"\"\n        return self", "solution": "\n        obj = self.chart.getObject(ID).copy()\n        obj.relocate(obj.lon - asp)\n        ID = 'D_%s_%s' % (ID, asp)\n        return self.G(ID, obj.lat, obj.lon)"}
{"prompt": "def from_celery(cls, broker_dict):\n        \"\"\" Create a BrokerStats object from the dictionary returned by celery.\n\n        Args:\n            broker_dict (dict): The dictionary as returned by celery.\n\n        Returns:\n            BrokerStats: A fully initialized BrokerStats object.\n        \"\"\"", "output": "\n        return cls(broker_dict['stats'])\n\n    def to_dict(self):\n        \"\"\" Return a dictionary representation of the object.\n\n        Returns:\n            dict: The dictionary as returned by the broker.\n        \"\"\"\n        return {\n           'stats': self.stats,\n        }\n\n    def to_json(self):\n        \"\"\" Return a JSON representation of the object.\n\n        Returns:\n            str: The JSON representation of the object.\n        \"\"\"\n        return json.dumps(self.to_dict_from_json(self.to_json_from_dict(self.stats))), {\n           'stats': self.stats", "solution": "\n        return BrokerStats(\n            hostname=broker_dict['hostname'],\n            port=broker_dict['port'],\n            transport=broker_dict['transport'],\n            virtual_host=broker_dict['virtual_host']\n        )"}
{"prompt": "def visit_Assert(self, assert_):\n        \"\"\"Return the AST statements to replace the ast.Assert instance.\n\n        This re-writes the test of an assertion to provide\n        intermediate values and replace it with an if statement which\n        raises an assertion error with a detailed explanation in case\n        the expression is false.\n\n        \"\"\"", "output": "\n        return self._replace_if_nodes(assert_.test, self.assert_expr, assert_.fail)\n\n    def visit_Assign(self, assign):\n        \"\"\"Return the AST statements to replace the ast.Assign instance.\n\n        This re-writes the test of an assignment to provide\n        intermediate values and replace it with an if statement which\n        raises an assignment error with a detailed explanation in case\n        the expression is false.\n\n        \"\"\"\n        return self._replace_if_nodes(assign.value, self.assign_expr, assert_.fail)\n\n    def visit_AugAssign(self, assign):\n        \"\"\"Return", "solution": "\n        if isinstance(assert_.test, ast.Tuple) and self.config is not None:\n            fslocation = (self.module_path, assert_.lineno)\n            self.config.warn('R1', 'assertion is always true, perhaps '\n                              'remove parentheses?', fslocation=fslocation)\n        self.statements = []\n        self.variables = []\n        self.variable_counter = itertools.count()\n        self.stack = []\n        self.on_failure = []\n        self.push_format_context()\n        # Rewrite assert into a bunch of statements.\n        top_condition, explanation = self.visit(assert_.test)\n        # Create failure message.\n        body = self.on_failure\n        negation = ast.UnaryOp(ast.Not(), top_condition)\n        self.statements.append(ast.If(negation, body, []))\n        if assert_.msg:\n            assertmsg = self.helper('format_assertmsg', assert_.msg)\n            explanation = \"\\n>assert \" + explanation\n        else:\n            assertmsg = ast.Str(\"\")\n            explanation = \"assert \" + explanation\n\n        if _MARK_ASSERTION_INTROSPECTION:\n            explanation = 'dessert* ' + explanation\n\n        template = ast.BinOp(assertmsg, ast.Add(), ast.Str(explanation))\n        msg = self.pop_format_context(template)\n        fmt = self.helper(\"format_explanation\", msg, assertmsg)\n        err_name = ast.Name(\"AssertionError\", ast.Load())\n        exc = ast_Call(err_name, [fmt], [])\n        if sys.version_info[0] >= 3:\n            raise_ = ast.Raise(exc, None)\n        else:\n            raise_ = ast.Raise(exc, None, None)\n        body.append(raise_)\n        # Clear temporary variables by setting them to None.\n        if self.variables:\n            variables = [ast.Name(name, ast.Store())\n                         for name in self.variables]\n            clear = ast.Assign(variables, _NameConstant(None))\n            self.statements.append(clear)\n        # Fix line numbers.\n        for stmt in self.statements:\n            set_location(stmt, assert_.lineno, assert_.col_offset)\n        return self.statements"}
{"prompt": "def jsonp_wrap(callback_key='callback'):\n    \"\"\"\n    Format response to jsonp and add a callback to JSON data - a jsonp request\n    \"\"\"", "output": "\n    def _jsonp_wrap(response):\n        if response.error:\n            return response.error\n        else:\n            return None\n    return jsonp.Response(_jsonp_wrap)\n\n\ndef jsonp_get_user_info(request):\n    \"\"\"\n    Get user info from the jsonp request\n    \"\"\"\n    if request.method == 'GET':\n        return jsonp_wrap(jsonp_get_user_info_json())\n\n    return _get_user_info_json(request.GET)\n\n\ndef jsonp_get_user_info_json():\n    \"\"\"\n    Get user info", "solution": "\n\n    def decorator_fn(f):\n\n        @wraps(f)\n        def jsonp_output_decorator(*args, **kwargs):\n            task_data = _get_data_from_args(args)\n            data = task_data.get_data()\n\n            if callback_key not in data:\n                raise KeyError(\n                    'Missing required parameter \"{0}\" for task.'.format(\n                        callback_key))\n\n            callback = data[callback_key]\n            jsonp = f(*args, **kwargs)\n            if isinstance(JobContext.get_current_context(), WebJobContext):\n                JobContext.get_current_context().add_responder(\n                    MimeSetterWebTaskResponder('application/javascript'))\n            jsonp = \"{callback}({data})\".format(callback=callback, data=jsonp)\n\n            return jsonp\n\n        return jsonp_output_decorator\n\n    return decorator_fn"}
{"prompt": "def readme():\n\t\"\"\"Live reload readme\"\"\"", "output": "\n\tglobal _app\n\tglobal _app_path\n\tglobal _app_name\n\tglobal _app_path_prefix\n\tglobal _app_name_prefix\n\t_app = _app_name + _app_path_prefix\n\t_app_path = _app_path + _app_name\n\t_app_name = _app_name + _app_name_prefix\n\treturn _app, _app_path, _app_name\n\ndef get_app_path(file_name):\n\t\"\"\"Returns the path to the application file\"\"\"\n\t", "solution": "\n\tfrom livereload import Server\n\tserver = Server()\n\tserver.watch(\"README.rst\", \"py cute.py readme_build\")\n\tserver.serve(open_url_delay=1, root=\"build/readme\")"}
{"prompt": "def embed(self, name, data=None):\n\t\t\"\"\"Attach an image file and prepare for HTML embedding.\n\n\t\tThis method should only be used to embed images.\n\n\t\t:param name: Path to the image to embed if data is None, or the name\n\t\t\t\t\t of the file if the ``data`` argument is given\n\t\t:param data: Contents of the image to embed, or None if the data is to\n\t\t\t\t\t be read from the file pointed to by the ``name`` argument\n\t\t\"\"\"", "output": "\n\t\tif data is None:\n\t\t\tdata = self.get_file_data(name)\n\t\tif data is None:\n\t\t\treturn False\n\t\tif not self.embed_data:\n\t\t\tself.embed_data = data\n\t\treturn self.embed_data.replace('<img src=\"data:image/png;base64,{0}\"/>'.format(base64.b64encode(data)), name)\n\n\tdef get_file_data(self, name):\n\t\t\"\"\"Get the content of a file.\n\n\t\t:param name: Name of the file to get content", "solution": "\n\t\tif data is None:\n\t\t\twith open(name, 'rb') as fp:\n\t\t\t\tdata = fp.read()\n\t\t\tname = os.path.basename(name)\n\t\telif isinstance(data, bytes):\n\t\t\tpass\n\t\telif hasattr(data, 'read'):\n\t\t\tdata = data.read()\n\t\telse:\n\t\t\traise TypeError(\"Unable to read image contents\")\n\n\t\tsubtype = imghdr.what(None, data)\n\t\tself.attach(name, data, 'image', subtype, True)"}
{"prompt": "def set_status(self, action, target):\n        \"\"\"\n        Sets query status with format: \"{domain} ({action}) {target}\"\n        \"\"\"", "output": "\n        if action == \"add\":\n            self.query_status.append(target)\n        elif action == \"delete\":\n            self.query_status.append(target)\n        else:\n            raise ValueError(\"Invalid action: {0}\".format(action))\n\n    def get_query_status(self):\n        \"\"\"\n        Returns query status.\n        \"\"\"\n        return self.query_status\n\n    def set_query_status(self, action, value):\n        \"\"\"\n        Sets query status with format: \"{domain} ({action}) {value}\"\n        \"\"\"\n        if action == \"add\":\n            self.query_status.append", "solution": "\n        try:\n            target = unquote(target)\n        except (AttributeError, TypeError):\n            pass\n\n        status = \"%s (%s) %s\" % (self.domain, action, target)\n        status = status.strip().replace('\\n', '')\n\n        if len(status) >= self.MAXWIDTH:\n            tail = '...'\n            extent = self.MAXWIDTH - (len(tail) + self.RPAD)\n            self.status = status[:extent] + tail\n        else:\n            self.status = status"}
{"prompt": "def matrix2map(data_matrix, map_shape):\n    r\"\"\"Matrix to Map\n\n    This method transforms a 2D matrix to a 2D map\n\n    Parameters\n    ----------\n    data_matrix : np.ndarray\n        Input data matrix, 2D array\n    map_shape : tuple\n        2D shape of the output map\n\n    Returns\n    -------\n    np.ndarray 2D map\n\n    Raises\n    ------\n    ValueError\n        For invalid layout\n\n    Examples\n    --------\n    >>> from modopt.base.transform import matrix2map\n    >>> a = np.array([[0, 4, 8, 12], [1, 5, 9, 13], [2, 6, 10, 14],\n    [3, 7, 11, 15]])\n    >>> matrix2map(a, (2, 2))\n    array([[ 0,  1,  4,  5],\n           [ 2,  3,  6,  7],\n           [ 8,  9, 12, 13],\n           [10, 11, 14, 15]])\n\n    \"\"\"", "output": ", 14]])\n    >>> b = np.array([[0, 1, 2, 3], [2, 3, 4, 5]])\n    >>> c = matrix2map(a, b)\n    >>> c.shape == (2, 3, 4)\n    True\n    >>> c.dtype == np.dtype('int32')\n    True\n    >>> c[0, 0] == 0\n    True\n    >>> c[0, 1] == 1\n    True\n    >>> c[0, 2] == 3\n    True\n    >>> c[1, 0] == 4\n    >>> c[1, 1] == 5\n    >>>", "solution": "\n\n    map_shape = np.array(map_shape)\n\n    # Get the shape and layout of the images\n    image_shape = np.sqrt(data_matrix.shape[0]).astype(int)\n    layout = np.array(map_shape // np.repeat(image_shape, 2), dtype='int')\n\n    # Map objects from matrix\n    data_map = np.zeros(map_shape)\n\n    temp = data_matrix.reshape(image_shape, image_shape, data_matrix.shape[1])\n\n    for i in range(data_matrix.shape[1]):\n        lower = (image_shape * (i // layout[1]),\n                 image_shape * (i % layout[1]))\n        upper = (image_shape * (i // layout[1] + 1),\n                 image_shape * (i % layout[1] + 1))\n        data_map[lower[0]:upper[0], lower[1]:upper[1]] = temp[:, :, i]\n\n    return data_map.astype(int)"}
{"prompt": "def get_zone(self, zone_id, records=True):\n        \"\"\"Get a zone and its records.\n\n        :param zone: the zone name\n        :returns: A dictionary containing a large amount of information about\n                  the specified zone.\n\n        \"\"\"", "output": "\n        return self.get_records(zone_id, records, 'zones', 'detail', 'name')\n\n    def get_records(self, zone_id, records=True, *args, **kwargs):\n        \"\"\"Get a list of records.\n\n        :param zone: the zone name\n        :param records: the list of records, or only those that have been deleted.\n        :returns: A list of dictionaries containing a large amount of information about\n                  the specified zone.\n\n        \"\"\"\n        url = self.get_url('records', zone_id, *args, **kwargs)\n        return self.get_", "solution": "\n        mask = None\n        if records:\n            mask = 'resourceRecords'\n        return self.service.getObject(id=zone_id, mask=mask)"}
{"prompt": "def _shrink(self):\n        \"\"\"\n        Shrinks the dynamic table to be at or below maxsize\n        \"\"\"", "output": "\n        self.db.execute(\"ALTER TABLE %s ADD COLUMN maxsize INTEGER DEFAULT 0\" % self.table_name)\n\n    def _truncate(self):\n        \"\"\"\n        Truncates the dynamic table to be at or above 0\n        \"\"\"\n        self.db.execute(\"ALTER TABLE %s ADD COLUMN size INTEGER DEFAULT 0\" % self.table_name)\n\n    def _update_table_indexes(self):\n        \"\"\"\n        Updates the indexes of dynamic table\n        \"\"\"\n        self.db.execute(\"UPDATE %s SET index_name=%s WHERE index_name=%s\" % (self.table_name, self", "solution": "\n        cursize = self._current_size\n        while cursize > self._maxsize:\n            name, value = self.dynamic_entries.pop()\n            cursize -= table_entry_size(name, value)\n        self._current_size = cursize"}
{"prompt": "def alias_comment(self, comment_id, alias_id):\n        \"\"\"Adds an ``Id`` to a ``Comment`` for the purpose of creating compatibility.\n\n        The primary ``Id`` of the ``Comment`` is determined by the\n        provider. The new ``Id`` performs as an alias to the primary\n        ``Id``. If the alias is a pointer to another comment, it is\n        reassigned to the given comment ``Id``.\n\n        arg:    comment_id (osid.id.Id): the ``Id`` of a ``Comment``\n        arg:    alias_id (osid.id.Id): the alias ``Id``\n        raise:  AlreadyExists - ``alias_id`` is already assigned\n        raise:  NotFound - ``comment_id`` not found\n        raise:  NullArgument - ``comment_id`` or ``alias_id`` is\n                ``null``\n        raise:  OperationFailed - unable to complete request\n        raise:  PermissionDenied - authorization failure\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"", "output": "): the alias ``Id``\n        raise:  NullArgument - ``comment_id`` or ``alias_id`` is ``null``\n        raise:  Unsupported - ``comment_id`` or ``alias_id`` is not of this\n                service\n        raise:  OperationFailed - unable to complete request\n        raise:  PermissionDenied - authorization failure\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n        pass\n\n    def can_manage_comment_aliases(self):\n        \"\"\"Tests if this user can manage ``Id`` aliases for ``Comment`` objects.\n\n        A return of true does not guarantee successful authorization.", "solution": "\n        # Implemented from template for\n        # osid.resource.ResourceAdminSession.alias_resources_template\n        self._alias_id(primary_id=comment_id, equivalent_id=alias_id)"}
{"prompt": "def get_behave_args(self, argv=sys.argv):\n        \"\"\"\n        Get a list of those command line arguments specified with the\n        management command that are meant as arguments for running behave.\n        \"\"\"", "output": "\n        return self.behave_args\n\n    def get_behave_options(self, argv=sys.argv):\n        \"\"\"\n        Get a list of those options specified with the\n        management command that are meant as options for running behave.\n        \"\"\"\n        return self.behave_options\n\n    def get_behave_args(self, argv=sys.argv):\n        \"\"\"\n        Get a list of those command line arguments specified with the\n        management command that are meant as args for running behave.\n        \"\"\"\n        return self.behave_args\n\n    def get_output(self, argv=sys.argv):\n        \"\"\"", "solution": "\n        parser = BehaveArgsHelper().create_parser('manage.py', 'behave')\n        args, unknown = parser.parse_known_args(argv[2:])\n\n        behave_args = []\n        for option in unknown:\n            # Remove behave prefix\n            if option.startswith('--behave-'):\n                option = option.replace('--behave-', '', 1)\n                prefix = '-' if len(option) == 1 else '--'\n                option = prefix + option\n\n            behave_args.append(option)\n\n        return behave_args"}
{"prompt": "def reader(fname):\n    \"\"\"Helper function to open the results file (coords file) and create alignment objects with the values in it\"\"\"", "output": "\n    with open(fname, 'r') as f:\n        for line in f:\n            line = line.strip()\n            if line.startswith('##'):\n                continue\n            if line.startswith('## SNAP'):\n                continue\n            if line.startswith('## SNAP1'):\n                continue\n            if line.startswith('## SNAP2'):\n                continue\n            if line.startswith('## coords'):\n                yield line.split()\n            elif line.startswith('## coords'):\n                yield [float(x) for x in line.split()]\n            else:\n                raise ValueError('Unexpected line: %s' %", "solution": "\n    f = pyfastaq.utils.open_file_read(fname)\n\n    for line in f:\n        if line.startswith('[') or (not '\\t' in line):\n            continue\n\n        yield alignment.Alignment(line)\n\n    pyfastaq.utils.close(f)"}
{"prompt": "def _write_data_csv(csv_data):\n    \"\"\"\n    CSV data has been parsed by this point, so take it and write it file by file.\n    :return:\n    \"\"\"", "output": "\n    with open(csv_data, 'w') as f:\n        writer = csv.writer(f)\n        writer.writerow(csv_data.split(','))\n\n\ndef _write_data_json(json_data):\n    \"\"\"\n    JSON data has been parsed by this point, so take it and write it to a file.\n    :return:\n    \"\"\"\n    with open(json_data, 'w') as f:\n        f.write(json.dumps(json_data, indent=4))\n\n\ndef _write_data_json_with_file(json_data):\n    \"\"\"", "solution": "\n    logger_excel.info(\"enter write_data_csv\")\n    # Loop for each file and data that is stored\n    for file in csv_data:\n        for filename, data in file.items():\n            # Make sure we're working with the right data types before trying to open and write a file\n            if isinstance(filename, str) and isinstance(data, list):\n                try:\n                    with open(filename, 'w+') as f:\n                        w = csv.writer(f)\n                        for line in data:\n                            w.writerow(line)\n                except Exception:\n                    logger_excel.debug(\"write_data_csv: Unable to open/write file: {}\".format(filename))\n\n    logger_excel.info(\"exit write_data_csv\")\n    return"}
{"prompt": "def _distances_from_function_exit(function):\n        \"\"\"\n        :param function:    A normalized Function object.\n        :returns:           A dictionary of basic block addresses and their distance to the exit of the function.\n        \"\"\"", "output": "\n        return {\n           'start': function.start_address,\n            'end': function.end_address,\n            'distance': function.distance,\n            'exit_distance': function.exit_distance,\n        }\n\n    def _block_address_to_distance(self, address):\n        \"\"\"\n        :param address:    A normalized BlockAddress object.\n        :returns:           A dictionary of basic block addresses and their distance to the exit of the block.\n        \"\"\"\n        return {\n           'start': address.start_block,\n            'end': address.end_block,\n            'distance': address.distance,", "solution": "\n        reverse_graph = function.graph.reverse()\n        # we aren't guaranteed to have an exit from the function so explicitly add the node\n        reverse_graph.add_node(\"start\")\n        found_exits = False\n        for n in function.graph.nodes():\n            if len(list(function.graph.successors(n))) == 0:\n                reverse_graph.add_edge(\"start\", n)\n                found_exits = True\n\n        # if there were no exits (a function with a while 1) let's consider the block with the highest address to\n        # be the exit. This isn't the most scientific way, but since this case is pretty rare it should be okay\n        if not found_exits:\n            last = max(function.graph.nodes(), key=lambda x:x.addr)\n            reverse_graph.add_edge(\"start\", last)\n\n        dists = networkx.single_source_shortest_path_length(reverse_graph, \"start\")\n\n        # remove temp node\n        del dists[\"start\"]\n\n        # correct for the added node\n        for n in dists:\n            dists[n] -= 1\n\n        return dists"}
{"prompt": "def attach_pipeline(self, pipeline, name, chunks=None, eager=True):\n        \"\"\"Register a pipeline to be computed at the start of each day.\n\n        Parameters\n        ----------\n        pipeline : Pipeline\n            The pipeline to have computed.\n        name : str\n            The name of the pipeline.\n        chunks : int or iterator, optional\n            The number of days to compute pipeline results for. Increasing\n            this number will make it longer to get the first results but\n            may improve the total runtime of the simulation. If an iterator\n            is passed, we will run in chunks based on values of the iterator.\n            Default is True.\n        eager : bool, optional\n            Whether or not to compute this pipeline prior to\n            before_trading_start.\n\n        Returns\n        -------\n        pipeline : Pipeline\n            Returns the pipeline that was attached unchanged.\n\n        See Also\n        --------\n        :func:`zipline.api.pipeline_output`\n        \"\"\"", "output": "\n        \"\"\"\n        if chunks is None:\n            chunks = True\n        if eager:\n            self.eager = True\n        else:\n            self.chunks = chunks\n\n    def compute_chunk_indices(self, dates, assets, start_date, end_date):\n        \"\"\"Compute the indices of the chunks for each date.\n\n        Parameters\n        ----------\n        start_date : pd.DatetimeIndex\n            The start date for the chunk.\n        end_date : pd.DatetimeIndex\n            The end date for the chunk.\n\n        Returns\n        -------\n        pd.DatetimeIndex\n            The indices of the chunks for each date.\n\n        \"\"\"\n        if self.", "solution": "\n        if chunks is None:\n            # Make the first chunk smaller to get more immediate results:\n            # (one week, then every half year)\n            chunks = chain([5], repeat(126))\n        elif isinstance(chunks, int):\n            chunks = repeat(chunks)\n\n        if name in self._pipelines:\n            raise DuplicatePipelineName(name=name)\n\n        self._pipelines[name] = AttachedPipeline(pipeline, iter(chunks), eager)\n\n        # Return the pipeline to allow expressions like\n        # p = attach_pipeline(Pipeline(), 'name')\n        return pipeline"}
{"prompt": "def cont(self, event = None):\n        \"\"\"\n        Resumes execution after processing a debug event.\n\n        @see: dispatch(), loop(), wait()\n\n        @type  event: L{Event}\n        @param event: (Optional) Event object returned by L{wait}.\n\n        @raise WindowsError: Raises an exception on error.\n        \"\"\"", "output": "\n        if not self.is_debug:\n            raise WindowsError(\"Not supported\")\n        if not self.is_debug:\n            raise WindowsError(\"Not supported\")\n        if not self.is_debug:\n            raise WindowsError(\"Not supported\")\n        if not self.is_debug:\n            raise WindowsError(\"Not supported\")\n        if not self.is_debug:\n            raise WindowsError(\"Not supported\")\n        if not self.is_debug:\n            raise WindowsError(\"Not supported\")\n        if not self.is_debug:\n            raise WindowsError(\"Not supported\")\n        if not self.is_debug:\n            raise WindowsError(\"Not supported\")", "solution": "\n\n        # If no event object was given, use the last event.\n        if event is None:\n            event = self.lastEvent\n\n        # Ignore dummy events.\n        if not event:\n            return\n\n        # Get the event continue status information.\n        dwProcessId      = event.get_pid()\n        dwThreadId       = event.get_tid()\n        dwContinueStatus = event.continueStatus\n\n        # Check if the process is still being debugged.\n        if self.is_debugee(dwProcessId):\n\n            # Try to flush the instruction cache.\n            try:\n                if self.system.has_process(dwProcessId):\n                    aProcess = self.system.get_process(dwProcessId)\n                else:\n                    aProcess = Process(dwProcessId)\n                aProcess.flush_instruction_cache()\n            except WindowsError:\n                pass\n\n            # XXX TODO\n            #\n            # Try to execute the UnhandledExceptionFilter for second chance\n            # exceptions, at least when in hostile mode (in normal mode it\n            # would be breaking compatibility, as users may actually expect\n            # second chance exceptions to be raised again).\n            #\n            # Reportedly in Windows 7 (maybe in Vista too) this seems to be\n            # happening already. In XP and below the UnhandledExceptionFilter\n            # was never called for processes being debugged.\n\n            # Continue execution of the debugee.\n            win32.ContinueDebugEvent(dwProcessId, dwThreadId, dwContinueStatus)\n\n        # If the event is the last event, forget it.\n        if event == self.lastEvent:\n            self.lastEvent = None"}
{"prompt": "def combined_dict(*dicts):\n  \"\"\"Combine one or more dicts into a new, unified dict (dicts to the right take precedence).\"\"\"", "output": "\n  combined = {}\n  for d in dicts:\n    for k, v in d.iteritems():\n      if isinstance(v, dict):\n        combined[k] = combined_dict(v)\n      else:\n        combined[k] = v\n    else:\n      raise TypeError('Unknown type: %s' % type(v))\n  return combined\n\n\ndef _GetSubclasses(klass):\n  \"\"\"Get all subclasses of a class that are subclassed from |klass|.\n\n  Args:\n    klass: A class.\n\n  Returns:\n    A list of subclasses of |klass|.\n  \"\"\"\n  subclasses = []\n  for name,", "solution": "\n  return {k: v for d in dicts for k, v in d.items()}"}
{"prompt": "def membership_in(self, organization):\n        \"\"\"Retrieve the user's membership in the specified organization.\"\"\"", "output": "\n        return self.get_membership(organization, self.user)\n\n    def get_membership(self, organization, user):\n        \"\"\"Retrieve the membership for the specified user.\"\"\"\n        return self.store.get_membership(user, organization=organization)\n\n    def get_membership_by_email(self, email):\n        \"\"\"Retrieve the user's membership in the specified organization.\"\"\"\n        return self.store.get_membership_by_email(email)\n\n    def get_membership_emails(self, email):\n        \"\"\"Retrieve the user's membership in the specified organization.\"\"\"\n        return self.get_membership(email,", "solution": "\n        url = self._build_url('user', 'memberships', 'orgs',\n                              str(organization))\n        json = self._json(self._get(url), 200)\n        return Membership(json, self)"}
{"prompt": "def _final_data(self):\r\n        \"\"\"\r\n        Returns\r\n        -------\r\n        A list of tuples representing rows from the datatable's index\r\n        and final column, sorted accordingly.\r\n        \"\"\"", "output": "\r\n        if self.index is None:\r\n            return []\r\n        else:\r\n            return self.index, self.final_column\r\n\r\n    def _get_index_name(self, name):\r\n        \"\"\"\r\n        Returns\r\n        -------\r\n        A string representing the name of the index column.\r\n        \"\"\"\r\n        if self.index is None:\r\n            return None\r\n        else:\r\n            return self.final_column.name\r\n\r\n    def _get_index_values(self, index):\r\n        \"\"\"\r\n        Returns\r\n        -------\r\n        A list of tuples representing the values of the index column.\r\n        \"\"\"\r\n        if self.index is None:\r\n            return []\r\n        else:", "solution": "\r\n        dtbl = self.datatable\r\n\r\n        objs = object_session(self)\r\n        if isinstance(dtbl, Table):\r\n            return objs.query(dtbl.c.indx, dtbl.c.final).all()\r\n        else:\r\n            raise Exception(\"Symbol has no datatable, likely need to cache first.\")"}
{"prompt": "def parse_conference_address(address_string):\n    \"\"\"Parse a conference address.\n\n    This is a pretty dummy address parser. It only extracts country\n    and state (for US) and should be replaced with something better,\n    like Google Geocoding.\n    \"\"\"", "output": "\n    return {\n       'street': address_string[0:2],\n        'city': address_string[2:4],\n       'state': address_string[4:6],\n        'zip': address_string[6:8],\n        'country': address_string[8:10],\n        'phone': address_string[10:12],\n        'fax': address_string[12:14],\n        'email': address_string[14:18],\n        'website': address_string[18:20],\n        'phone': address_string[20:22],\n        'website':", "solution": "\n\n    geo_elements = address_string.split(',')\n    city = geo_elements[0]\n    country_name = geo_elements[-1].upper().replace('.', '').strip()\n    us_state = None\n    state = None\n    country_code = None\n\n    # Try to match the country\n    country_code = match_country_name_to_its_code(country_name, city)\n\n    if country_code == 'US' and len(geo_elements) > 1:\n        us_state = match_us_state(geo_elements[-2].upper().strip()\n                                  .replace('.', ''))\n\n    if not country_code:\n        # Sometimes the country name stores info about U.S. state\n        us_state = match_us_state(country_name)\n\n    if us_state:\n        state = us_state\n        country_code = 'US'\n\n    return {\n        'cities': [\n            city,\n        ],\n        'country_code': country_code,\n        'postal_code': None,\n        'state': state,\n    }"}
{"prompt": "def add_handler(self, handler: Handler, group: int = 0):\n        \"\"\"Use this method to register an update handler.\n\n        You can register multiple handlers, but at most one handler within a group\n        will be used for a single update. To handle the same update more than once, register\n        your handler using a different group id (lower group id == higher priority).\n\n        Args:\n            handler (``Handler``):\n                The handler to be registered.\n\n            group (``int``, *optional*):\n                The group identifier, defaults to 0.\n\n        Returns:\n            A tuple of (handler, group)\n        \"\"\"", "output": "\n        if group not in self._handlers:\n            self._handlers[group] = {}\n        self._handlers[group][self.KEY_UPDATE] = handler, group\n\n    def remove_handler(self, handler: Handler, id: int) -> None:\n        \"\"\"Remove a handler from the update list.\n\n        Args:\n            handler (``Handler``):\n                The handler to be removed.\n\n            id (``int``, *optional*):\n                The group identifier, defaults to 0.\n\n        Returns:\n            A tuple of (handler, id)\n        \"\"\"\n        if handler not in self._handlers[group]:\n            return\n        self._", "solution": "\n        if isinstance(handler, DisconnectHandler):\n            self.disconnect_handler = handler.callback\n        else:\n            self.dispatcher.add_handler(handler, group)\n\n        return handler, group"}
{"prompt": "def read_wave(path):\n    \"\"\"Reads a .wav file.\n\n    Takes the path, and returns (PCM audio data, sample rate).\n    \"\"\"", "output": "\n    with open(path, 'rb') as f:\n        header = f.read(4)\n        rate = f.read(4)\n\n    return header, rate\n\n\ndef read_wave_file(path):\n    \"\"\"Reads a.wav file.\n\n    Takes the path, and returns (PCM audio data, sample rate).\n    \"\"\"\n    with open(path, 'rb') as f:\n        header = f.read(4)\n        rate = f.read(4)\n\n    return header, rate\n\n\ndef write_wave(pcm_data, sample_rate):\n    \"\"\"Writes a.", "solution": "\n    with contextlib.closing(wave.open(path, 'rb')) as wf:\n        num_channels = wf.getnchannels()\n        assert num_channels == 1\n        sample_width = wf.getsampwidth()\n        assert sample_width == 2\n        sample_rate = wf.getframerate()\n        assert sample_rate in (8000, 16000, 32000)\n        frames = wf.getnframes()\n        pcm_data = wf.readframes(frames)\n        duration = frames / sample_rate\n        return pcm_data, sample_rate, duration"}
{"prompt": "def update(self, callback_method=values.unset, callback_url=values.unset,\n               friendly_name=values.unset):\n        \"\"\"\n        Update the TriggerInstance\n\n        :param unicode callback_method: The HTTP method to use to call callback_url\n        :param unicode callback_url: The URL we call when the trigger fires\n        :param unicode friendly_name: A string to describe the resource\n\n        :returns: Updated TriggerInstance\n        :rtype: twilio.rest.api.v2010.account.usage.trigger.TriggerInstance\n        \"\"\"", "output": "\n        data = values.of({'CallbackUrl': callback_url, 'FriendlyName': friendly_name, })\n\n        payload = self._version.update(method='POST', uri=self._uri, data=data, )\n\n        return TriggerInstance(self._version, payload, service_sid=self._solution['service_sid'], )\n\n    def __repr__(self):\n        \"\"\"\n        Provide a friendly representation\n\n        :returns: Machine friendly representation\n        :rtype: str\n        \"\"\"\n        return '<Twilio.Api.V2010.account.UsageList>'\n\n\nclass UsagePage(Page):\n    \"\"\" PLEASE", "solution": "\n        data = values.of({\n            'CallbackMethod': callback_method,\n            'CallbackUrl': callback_url,\n            'FriendlyName': friendly_name,\n        })\n\n        payload = self._version.update(\n            'POST',\n            self._uri,\n            data=data,\n        )\n\n        return TriggerInstance(\n            self._version,\n            payload,\n            account_sid=self._solution['account_sid'],\n            sid=self._solution['sid'],\n        )"}
{"prompt": "def _set_default_init_params(self):\n        \"\"\"\n        Set initial running parameters using default values.\n\n        The steady state equation is:\n          `qrs_thr = 0.25*qrs_amp + 0.75*noise_amp`\n\n        Estimate that qrs amp is 10x noise amp, giving:\n          `qrs_thr = 0.325 * qrs_amp or 13/40 * qrs_amp`\n\n        \"\"\"", "output": "\n        self.qrs_thr = self.qrs_amp * 10\n        self.noise_amp = self.noise_amp * 10\n        self.noise_amp_range = np.linspace(0, 10, 10)\n        self.noise_amp_range = self.noise_amp_range * 10\n        self.noise_amp_range = self.noise_amp_range * 10\n        self.noise_amp_range = self.noise_amp_range * 10\n        self.noise_amp_range = self.noise_amp_range * 10\n        self.noise_amp_", "solution": "\n        if self.verbose:\n            print('Initializing using default parameters')\n        # Multiply the specified ecg thresholds by the filter and mwi gain\n        # factors\n        qrs_thr_init = self.qrs_thr_init * self.transform_gain\n        qrs_thr_min = self.qrs_thr_min * self.transform_gain\n\n        qrs_amp = 27/40 * qrs_thr_init\n        noise_amp = qrs_amp / 10\n        rr_recent = self.rr_init\n        last_qrs_ind = 0\n\n        self._set_init_params(qrs_amp_recent=qrs_amp,\n                              noise_amp_recent=noise_amp,\n                              rr_recent=rr_recent,\n                              last_qrs_ind=last_qrs_ind)\n\n        self.learned_init_params = False"}
{"prompt": "def iterator(self):\n        \"\"\"\n        If search has occurred and no ordering has occurred, decorate\n        each result with the number of search terms so that it can be\n        sorted by the number of occurrence of terms.\n\n        In the case of search fields that span model relationships, we\n        cannot accurately match occurrences without some very\n        complicated traversal code, which we won't attempt. So in this\n        case, namely when there are no matches for a result (count=0),\n        and search fields contain relationships (double underscores),\n        we assume one match for one of the fields, and use the average\n        weight of all search fields with relationships.\n        \"\"\"", "output": " terms.\n        \"\"\"\n        if self.query.extra_order_by:\n            return\n\n        # Build the query\n        if self.query.extra_order_by is not None:\n            self.query.extra_order_by = []\n            for field in self.query.extra_order_by:\n                self.query.extra_order_by.append(\n                    {\n                        \"name\": field[\"name\"],\n                        \"order\": field[\"extra_order_by\"],\n                    }\n                )\n\n        # Build the sort\n        if self.query.sort_by is not None:\n            self.query.sort_", "solution": "\n        results = super(SearchableQuerySet, self).iterator()\n        if self._search_terms and not self._search_ordered:\n            results = list(results)\n            for i, result in enumerate(results):\n                count = 0\n                related_weights = []\n                for (field, weight) in self._search_fields.items():\n                    if \"__\" in field:\n                        related_weights.append(weight)\n                    for term in self._search_terms:\n                        field_value = getattr(result, field, None)\n                        if field_value:\n                            count += field_value.lower().count(term) * weight\n                if not count and related_weights:\n                    count = int(sum(related_weights) / len(related_weights))\n                results[i].result_count = count\n            return iter(results)\n        return results"}
{"prompt": "def get_uvw_segment(d, segment=-1):\n    \"\"\" Calculates uvw for each baseline at mid time of a given segment.\n    d defines pipeline state. assumes segmenttimes defined by RT.set_pipeline.\n    \"\"\"", "output": "\n    if segment == -1:\n        segmenttimes = d.get_times()\n    else:\n        segmenttimes = d.get_times()[segment]\n    uvw_segment = np.zeros(d.get_num_baseline_states())\n    for i in range(d.get_num_baseline_states()):\n        uvw_segment[i] = d.get_baseline_state_at_mid_time(i)\n    return uvw_segment\n\n\ndef get_uvw_segment_list(d, segment=-1):\n    \"\"\" Returns a list of UVW segments.", "solution": "\n\n    # define times to read\n    if segment != -1:\n        assert 'segmenttimes' in d, 'd must have segmenttimes defined'\n\n        t0 = d['segmenttimes'][segment][0]\n        t1 = d['segmenttimes'][segment][1]\n        datetime = qa.time(qa.quantity((t1+t0)/2, 'd'),\n                           form=['ymdhms'], prec=9)[0]\n        logger.debug('Calculating uvw for segment %d' % (segment))\n    else:\n        datetime = 0\n\n    (u, v, w) = calc_uvw(d['filename'], d['scan'],\n                         datetime=datetime, bdfdir=d['bdfdir'])\n\n    # cast to units of lambda at first channel.\n    # -1 keeps consistent with ms reading convention\n    u = u * d['freq_orig'][0] * (1e9/3e8) * (-1)\n    v = v * d['freq_orig'][0] * (1e9/3e8) * (-1)\n    w = w * d['freq_orig'][0] * (1e9/3e8) * (-1)\n\n    return u.astype('float32'), v.astype('float32'), w.astype('float32')"}
{"prompt": "def _create(cls, name, node_type, nodeid=1,\n                loopback_ndi=None):\n        \"\"\"\n        Create the node/s for the engine. This isn't called directly,\n        instead it is used when engine.create() is called\n\n        :param str name: name of node\n        :param str node_type: based on engine type specified\n        :param int nodeid: used to identify which node\n        :param list LoopbackInterface loopback_ndi: optional loopback\n            interface for node.\n        \"\"\"", "output": "\n        return cls(name, node_type, nodeid,\n                loopback_ndi=loopback_ndi)\n\n    def _get_node_by_id(self, node_id):\n        \"\"\"\n        Get the node object with the given node id\n\n        :param int node_id: node id\n        \"\"\"\n        return self.nodes[nodeid]\n\n    def _get_node_by_name(self, node_name):\n        \"\"\"\n        Get the node object with the given node name\n\n        :param str node_name: node name\n        \"\"\"\n        return self.nodes[node_name]\n\n    def _", "solution": "\n        loopback = loopback_ndi if loopback_ndi else []\n        node = {node_type: {\n            'activate_test': True,\n            'disabled': False,\n            'loopback_node_dedicated_interface': loopback,\n            'name': name + ' node ' + str(nodeid),\n            'nodeid': nodeid}\n        }\n        return node"}
{"prompt": "def load(self, cfgFile=None, timeout=None):\n        \"\"\"expect that the data file has already been established\"\"\"", "output": "\n        if cfgFile is None:\n            cfgFile = cfgFile\n        if not os.path.isfile(cfgFile):\n            raise Exception(\"Config file %s does not exist\" % cfgFile)\n        if timeout is None:\n            timeout = cfgfile.getInt(self.configSection, \"timeout\", 10)\n        if not os.path.isfile(timeout):\n            raise Exception(\"Config file %s does not exist\" % timeout)\n        if cfgfile.has_option(self.configSection, \"logFile\"):\n            self.logSection = cfgfile.get(self.configSection, \"log", "solution": "\n        #if cfgFile != None: self.cfgFile = cfgFile # if it's specified, use it\n        if not cfgFile:\n            cfgs = activeConfigs()\n            if   len(cfgs) > 1: raise Exception(\"found too many configurations (%s); not clear which to load: %s\"%(len(cfgs), cfgs))\n            elif len(cfgs) < 1:\n                if timeout: # wait for a configuration file to appear to be loaded\n                    startWait = time.time()\n                    timeReported = 0\n                    while not cfgs:\n                        timeWaited = time.time() - startWait\n                        if timeWaited > timeout:\n                            raise c.TimeoutExceeded(\"could not join game after %s seconds\"%(timeout))\n                        try:  cfgs = activeConfigs()\n                        except:\n                            if self.debug and timeWaited - timeReported >= 1:\n                                timeReported += 1\n                                print(\"second(s) waited for game to appear:  %d\"%(timeReported))\n                else:  raise Exception(\"must have a saved configuration to load or allow loading via timeout setting\")\n            cfgFile = cfgs.pop()\n        try:\n            with open(cfgFile, \"rb\") as f:\n                data = f.read() # bytes => str\n        except TypeError as e:\n            print(\"ERROR %s: %s %s\"%(e, cfgFile, type(cfgFile)))\n            raise\n        self.loadJson(data) # str => dict\n        if self.debug:\n            print(\"configuration loaded: %s\"%(self.name))\n            self.display()"}
{"prompt": "def update_probs(self):\n        \"\"\"Update the internal probability values given the counts.\"\"\"", "output": "\n        self.probs = np.zeros(self.num_actions)\n        self.probs[self.action_index] = 1.0\n        self.action_index += 1\n        self.num_actions += 1\n\n    def get_action_probs(self):\n        \"\"\"Return the internal probability values.\"\"\"\n        return self.probs[self.action_index]\n\n    def get_action_probs_and_actions(self):\n        \"\"\"Return the action probabilities and actions.\"\"\"\n        return self.get_action_probs(), self.get_action_probs_and_actions()\n\n    def get_action_index", "solution": "\n        # We deal with the prior probsfirst\n        # This is a fixed assumed value for systematic error\n        syst_error = 0.05\n        prior_probs = {'syst': {}, 'rand': {}}\n        for source, (p, n) in self.prior_counts.items():\n            # Skip if there are no actual counts\n            if n + p == 0:\n                continue\n            prior_probs['syst'][source] = syst_error\n            prior_probs['rand'][source] = \\\n                1 - min((float(p) / (n + p), 1-syst_error)) - syst_error\n        # Next we deal with subtype probs based on counts\n        subtype_probs = {}\n        for source, entry in self.subtype_counts.items():\n            for rule, (p, n) in entry.items():\n                # Skip if there are no actual counts\n                if n + p == 0:\n                    continue\n                if source not in subtype_probs:\n                    subtype_probs[source] = {}\n                subtype_probs[source][rule] = \\\n                    1 - min((float(p) / (n + p), 1-syst_error)) - syst_error\n        # Finally we propagate this into the full probability\n        # data structures of the parent class\n        super(BayesianScorer, self).update_probs(prior_probs, subtype_probs)"}
{"prompt": "def equals(self, other):\n        \"\"\" Structural equality of models.\n\n        Args:\n            other (HasProps) : the other instance to compare to\n\n        Returns:\n            True, if properties are structurally equal, otherwise False\n\n        \"\"\"", "output": "\n        return self.__dict__ == other.__dict__\n\n    def __ne__(self, other):\n        \"\"\" Check if two models are not equal.\n\n        Args:\n            other (HasProps) : the other instance to compare to\n\n        Returns:\n            True, if properties are not equal, otherwise False\n\n        \"\"\"\n        return not self.__eq__(other)\n\n    def __hash__(self):\n        return self.__hash\n\n    def __repr__(self):\n        return \"StructuralModel({})\".format(self.to_json())\n\n    def __str__(self):\n        return \"StructuralModel({})\".format(self.to_json", "solution": "\n\n        # NOTE: don't try to use this to implement __eq__. Because then\n        # you will be tempted to implement __hash__, which would interfere\n        # with mutability of models. However, not implementing __hash__\n        # will make bokeh unusable in Python 3, where proper implementation\n        # of __hash__ is required when implementing __eq__.\n        if not isinstance(other, self.__class__):\n            return False\n        else:\n            return self.properties_with_values() == other.properties_with_values()"}
{"prompt": "def imported_member(self, node, member, name):\n        \"\"\"verify this is not an imported class or handle it\"\"\"", "output": "\n        if not isinstance(member, astroid.FunctionDef):\n            return False\n        return isinstance(member.parent, astroid.Class) and \\\n               isinstance(member.parent.parent, astroid.Module) and \\\n               isinstance(member.parent.parent, astroid.Class) and \\\n               isinstance(member.parent.parent.parent, astroid.Module) and \\\n               node.name == name\n\n    def test_import_from_statement(self):\n        \"\"\"verify this is not an imported class or handle it\"\"\"\n        if not isinstance(self.module, astroid.Module):\n            return False\n        return isinstance(self", "solution": "\n        # /!\\ some classes like ExtensionClass doesn't have a __module__\n        # attribute ! Also, this may trigger an exception on badly built module\n        # (see http://www.logilab.org/ticket/57299 for instance)\n        try:\n            modname = getattr(member, \"__module__\", None)\n        except TypeError:\n            modname = None\n        if modname is None:\n            if name in (\"__new__\", \"__subclasshook__\"):\n                # Python 2.5.1 (r251:54863, Sep  1 2010, 22:03:14)\n                # >>> print object.__new__.__module__\n                # None\n                modname = builtins.__name__\n            else:\n                attach_dummy_node(node, name, member)\n                return True\n\n        real_name = {\"gtk\": \"gtk_gtk\", \"_io\": \"io\"}.get(modname, modname)\n\n        if real_name != self._module.__name__:\n            # check if it sounds valid and then add an import node, else use a\n            # dummy node\n            try:\n                getattr(sys.modules[modname], name)\n            except (KeyError, AttributeError):\n                attach_dummy_node(node, name, member)\n            else:\n                attach_import_node(node, modname, name)\n            return True\n        return False"}
{"prompt": "def config(self, **kwargs):\n        \"\"\"\n        Config the behavior of :class:`Logger`.\n\n        Control the output to show :class:`Logger.verbose` log::\n\n            log.config(verbose=True)\n\n        Control the output to show only the :class:`Logger.warn` and\n        :class:`Logger.error` log::\n\n            log.config(quiet=True)\n\n        \"\"\"", "output": "\n        return Logger.verbose(**kwargs)\n\n    def info(self, **kwargs):\n        \"\"\"\n        Info the behavior of :class:`Logger`.\n\n        Control the output to show :class:`Logger.info` log::\n\n            log.info(message)\n\n        Control the output to show only the :class:`Logger.error` and\n        :class:`Logger.warn` log::\n\n            log.info(message, level=logging.INFO)\n\n        \"\"\"\n        return Logger.info(**kwargs)\n\n    def warning(self, **kwargs):\n        \"\"\"\n        Show a warning message.\n\n        \"\"\"\n        return self.info(message=kwargs", "solution": "\n        if 'indent' in kwargs:\n            self._indent = kwargs.get('indent', 0)\n        if 'verbose' in kwargs:\n            self._enable_verbose = kwargs.get('verbose', False)\n        if 'quiet' in kwargs:\n            self._enable_quiet = kwargs.get('quiet', False)\n        return self"}
{"prompt": "def _assert_obj_type(pub, name=\"pub\", obj_type=DBPublication):\n    \"\"\"\n    Make sure, that `pub` is instance of the `obj_type`.\n\n    Args:\n        pub (obj): Instance which will be checked.\n        name (str): Name of the instance. Used in exception. Default `pub`.\n        obj_type (class): Class of which the `pub` should be instance. Default\n                 :class:`.DBPublication`.\n\n    Raises:\n        InvalidType: When the `pub` is not instance of `obj_type`.\n    \"\"\"", "output": "\n    if not isinstance(pub, db.Model):\n        raise InvalidType(name, pub, obj_type)\n\n\ndef _assert_type_or_none(pub, name=\"pub\", obj_type=None):\n    \"\"\"\n    Make sure, that `pub` is instance of the `obj_type`.\n\n    Args:\n        pub (class): Instance which will be checked.\n        name (str): Name of the instance. Used in exception. Default `pub`.\n        obj_type (class): Class of which the `pub` should be instance. Default\n                 :class:`.DBPublication`.\n\n    Raises:", "solution": "\n    if not isinstance(pub, obj_type):\n        raise InvalidType(\n            \"`%s` have to be instance of %s, not %s!\" % (\n                name,\n                obj_type.__name__,\n                pub.__class__.__name__\n            )\n        )"}
{"prompt": "def _adjust_for_new_root(self, path):\n        \"\"\"Adjust a path given the new root directory of the output.\"\"\"", "output": "\n        if path == self.root_dir:\n            return\n        if not os.path.exists(path):\n            raise ValueError(\"Path does not exist: %s\" % path)\n        if not os.path.isdir(path):\n            raise ValueError(\"Path is not a directory: %s\" % path)\n        for f in os.listdir(path):\n            if f == self.root_dir:\n                continue\n            if os.path.join(path, f) == self.root_dir:\n                continue\n            if os.path.isdir(os.path.join(path, f)):\n                self", "solution": "\n        if self.new_root is None:\n            return path\n        elif path.startswith(self.new_root):\n            return path[len(self.new_root):]\n        else:\n            return path"}
{"prompt": "def populate_request_data(self, request_args):\n        \"\"\"Add the authentication info to the supplied dictionary.\n\n        We use the `requests.HTTPBasicAuth` class as the `auth` param.\n\n        Args:\n            `request_args`: The arguments that will be passed to the request.\n        Returns:\n            The updated arguments for the request.\n        \"\"\"", "output": "\n        if not request_args:\n            return request_args\n\n        if not isinstance(request_args, dict):\n            raise TypeError('request_args must be a dict')\n\n        request_args['auth'] = self.auth\n        return request_args\n\n    def get_auth_headers(self):\n        \"\"\"Get the authentication headers.\n\n        Returns:\n            The authentication headers for the current request.\n        \"\"\"\n        if not self.auth:\n            return {}\n\n        return {\n            'Authorization': 'Bearer'+ self.auth,\n            'Content-Type': 'application/json',\n        }\n\n    def get_auth_credentials", "solution": "\n        request_args['auth'] = HTTPBasicAuth(\n            self._username, self._password)\n        return request_args"}
{"prompt": "def _parse_player_position(self, player_info):\n        \"\"\"\n        Parse the player's position.\n\n        The player's position isn't contained within a unique tag and the\n        player's meta information should be iterated through until 'Position'\n        is found as it contains the desired text.\n\n        Parameters\n        ----------\n        player_info : PyQuery object\n            A PyQuery object of the player's information on the HTML stats\n            page.\n        \"\"\"", "output": "\n        for tag in player_info.find_all('td'):\n            if tag.has_attr('class') and tag['class'] == 'player-position':\n                text = tag.text\n                if text is None:\n                    continue\n                if text.startswith('Player'):\n                    yield (tag['id'], text[4:])\n                else:\n                    yield (tag['id'], text)\n\n    def _parse_player_title(self, player_info):\n        \"\"\"\n        Parse the player's title.\n\n        The player's title is contained within a unique tag and the\n        player's meta information should be iterated through until '", "solution": "\n        for section in player_info('div#meta p').items():\n            if 'Position' in str(section):\n                position = section.text().replace('Position: ', '')\n                setattr(self, '_position', position)\n                break"}
{"prompt": "def _reloader_observer(server, app, interval):\n    \"\"\" Start a child process with identical commandline arguments and restart\n        it as long as it exists with status code 3. Also create a lockfile and\n        touch it (update mtime) every interval seconds.\n    \"\"\"", "output": "\n    # We need to lock the lockfile to prevent race conditions when the daemon\n    # dies.\n    lockfile = os.path.join(app.config['PID_DIR'], 'lock')\n    try:\n        os.mkfifo(lockfile)\n    except OSError:\n        return server.NOT_DONE_YET\n    # We need to create a lockfile and touch it (update mtime) every\n    # interval seconds.\n    with open(lockfile, 'w') as f:\n        f.write(str(interval))\n        f.write(str(os.getpid()))\n\n\nclass Daemon(Daemon", "solution": "\n    fd, lockfile = tempfile.mkstemp(prefix='bottle-reloader.', suffix='.lock')\n    os.close(fd) # We only need this file to exist. We never write to it\n    try:\n        while os.path.exists(lockfile):\n            args = [sys.executable] + sys.argv\n            environ = os.environ.copy()\n            environ['BOTTLE_CHILD'] = 'true'\n            environ['BOTTLE_LOCKFILE'] = lockfile\n            p = subprocess.Popen(args, env=environ)\n            while p.poll() is None: # Busy wait...\n                os.utime(lockfile, None) # I am alive!\n                time.sleep(interval)\n            if p.poll() != 3:\n                if os.path.exists(lockfile): os.unlink(lockfile)\n                sys.exit(p.poll())\n            elif not server.quiet:\n                print(\"Reloading server...\")\n    except KeyboardInterrupt:\n        pass\n    if os.path.exists(lockfile): os.unlink(lockfile)"}
{"prompt": "def backtick (cmd, encoding='utf-8'):\n    \"\"\"Return decoded output from command.\"\"\"", "output": "\n    return _decode (cmd, encoding)\n\ndef _decode (cmd, encoding='utf-8'):\n    \"\"\"Decode command.\"\"\"\n    if not cmd:\n        return None\n    if not encoding:\n        return cmd\n    if not _is_utf8 (cmd):\n        return cmd\n    if not _is_utf8 (encoding):\n        return cmd\n    if not _is_utf8 (encoding):\n        return cmd\n    if not _is_utf8 (cmd):\n        return cmd\n    if not _is_utf8 (encoding):\n        return cmd\n    return _decode_str (cmd, encoding)\n\ndef", "solution": "\n    data = subprocess.Popen(cmd, stdout=subprocess.PIPE).communicate()[0]\n    return data.decode(encoding)"}
{"prompt": "def _queryset_iterator(qs):\n    \"\"\"\n    Override default iterator to wrap returned items in a publishing\n    sanity-checker \"booby trap\" to lazily raise an exception if DRAFT\n    items are mistakenly returned and mis-used in a public context\n    where only PUBLISHED items should be used.\n\n    This booby trap is added when all of:\n\n    - the publishing middleware is active, and therefore able to report\n    accurately whether the request is in a drafts-permitted context\n    - the publishing middleware tells us we are not in\n    a drafts-permitted context, which means only published items\n    should be used.\n    \"\"\"", "output": " but we need to use the request object\n    \"\"\"\n    return itertools.chain(\n        qs.filter(publish_immediately=True, published=True)\n       .exclude(publish_immediately=True, published=True)\n       .filter(publish_immediately=True, published=True)\n       .order_by('publish_immediately')\n       .iterator()\n    )\n\n\nclass PublishingMiddleware(object):\n    \"\"\"\n    This middleware allows you to add headers to your request that are\n    automatically generated.\n\n    You must provide the ``headers`` argument.\n    \"\"\"\n    def process_response(self, request", "solution": "\n    # Avoid double-processing draft items in our custom iterator when we\n    # are in a `PublishingQuerySet` that is also a subclass of the\n    # monkey-patched `UrlNodeQuerySet`\n    if issubclass(type(qs), UrlNodeQuerySet):\n        super_without_boobytrap_iterator = super(UrlNodeQuerySet, qs)\n    else:\n        super_without_boobytrap_iterator = super(PublishingQuerySet, qs)\n\n    if is_publishing_middleware_active() \\\n            and not is_draft_request_context():\n        for item in super_without_boobytrap_iterator.iterator():\n            if getattr(item, 'publishing_is_draft', False):\n                yield DraftItemBoobyTrap(item)\n            else:\n                yield item\n    else:\n        for item in super_without_boobytrap_iterator.iterator():\n            yield item"}
{"prompt": "def _get_group_dn(self, group_lookup_attribute_value):\r\n        \"\"\" Searches for a group and retrieves its distinguished name.\r\n\r\n        :param group_lookup_attribute_value: The value for the LDAP_GROUPS_GROUP_LOOKUP_ATTRIBUTE\r\n        :type group_lookup_attribute_value: str\r\n\r\n        :raises: **GroupDoesNotExist** if the group doesn't exist in the active directory.\r\n\r\n        \"\"\"", "output": "\r\n        group_dn = None\r\n        try:\r\n            group_dn = self.get_active_directory_dn(self.active_directory)\r\n        except:\r\n            pass\r\n        if group_lookup_attribute_value is not None:\r\n            group_dn = group_lookup_attribute_value.split(',')[0]\r\n        else:\r\n            for group in self.ldap_groups:\r\n                if group.get('dn') == group_dn:\r\n                    group_lookup_attribute_value = group.get('attribute')\r\n                    break\r\n        raise GroupDoesNotExist(group_lookup_attribute_value)\r\n        if len(group_", "solution": "\r\n        self.ldap_connection.search(search_base=self.GROUP_SEARCH['base_dn'],\r\n                                    search_filter=self.GROUP_SEARCH['filter_string'].format(\r\n                                        lookup_value=escape_query(group_lookup_attribute_value)),\r\n                                    search_scope=self.GROUP_SEARCH['scope'],\r\n                                    attributes=self.GROUP_SEARCH['attribute_list'])\r\n        results = [result[\"dn\"] for result in self.ldap_connection.response if result[\"type\"] == \"searchResEntry\"]\r\n\r\n        if not results:\r\n            raise GroupDoesNotExist(\"The {group_lookup_attribute} provided does not exist in the Active \"\r\n                                    \"Directory.\".format(group_lookup_attribute=self.group_lookup_attr))\r\n\r\n        if len(results) > 1:\r\n            logger.debug(\"Search returned more than one result: {results}\".format(results=results))\r\n\r\n        if results:\r\n            return results[0]\r\n        else:\r\n            return results"}
{"prompt": "def close(self):\n        \"\"\"Close a port on dummy_serial.\"\"\"", "output": "\n        pass\n\n    def get_port_name(self):\n        \"\"\"Return the port name for the current test.\n\n        :returns: port name for the current test.\n        \"\"\"\n        return \"dummy_port\"\n\n    def get_port_stats(self, port_name):\n        \"\"\"Get statistics from the port's event buffer.\n\n        :param port_name: port name.\n        :type port_name: str\n        :returns: dictionary with statistic names and values.\n        \"\"\"\n        return self._get_event_stats(port_name, \"rx_packets\")\n\n    def get_port_desc(self,", "solution": "\n        if VERBOSE:\n            _print_out('\\nDummy_serial: Closing port\\n')\n\n        if not self._isOpen:\n            raise IOError('Dummy_serial: The port is already closed')\n            \n        self._isOpen = False\n        self.port = None"}
{"prompt": "def start_session(self):\n        \"\"\"\n        Starts a Salesforce session and determines which SF instance to use for future requests.\n        \"\"\"", "output": "\n        self.session = requests.Session()\n        self.session.headers.update({'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'})\n        self.session.headers.update({'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'})\n        self.session.headers.update({'Accept': 'application", "solution": "\n        if self.has_active_session():\n            raise Exception(\"Session already in progress.\")\n\n        response = requests.post(self._get_login_url(),\n                                 headers=self._get_login_headers(),\n                                 data=self._get_login_xml())\n        response.raise_for_status()\n\n        root = ET.fromstring(response.text)\n        for e in root.iter(\"%ssessionId\" % self.SOAP_NS):\n            if self.session_id:\n                raise Exception(\"Invalid login attempt.  Multiple session ids found.\")\n            self.session_id = e.text\n\n        for e in root.iter(\"%sserverUrl\" % self.SOAP_NS):\n            if self.server_url:\n                raise Exception(\"Invalid login attempt.  Multiple server urls found.\")\n            self.server_url = e.text\n\n        if not self.has_active_session():\n            raise Exception(\"Invalid login attempt resulted in null sessionId [%s] and/or serverUrl [%s].\" %\n                            (self.session_id, self.server_url))\n        self.hostname = urlsplit(self.server_url).hostname"}
{"prompt": "def StartupInstance(r, instance, dry_run=False, no_remember=False):\n    \"\"\"\n    Starts up an instance.\n\n    @type instance: str\n    @param instance: the instance to start up\n    @type dry_run: bool\n    @param dry_run: whether to perform a dry run\n    @type no_remember: bool\n    @param no_remember: if true, will not record the state change\n    @rtype: string\n    @return: job id\n    \"\"\"", "output": "\n    if dry_run:\n      dry_run_add = dry_run\n    else:\n      dry_run_add = no_remember\n\n    body = {\n      'configuration': instance,\n      'dry-run': dry_run_add,\n     'remember-instances': no_remember,\n    }\n\n    return self._SendRequest(HTTP_POST, '/start', body)\n\n  def StopInstance(self, instance, dry_run=False, no_remember=False):\n    \"\"\"\n    Stops an instance.\n\n    @type instance: str\n    @param instance: the instance to stop\n   ", "solution": "\n\n    query = {\n        \"dry-run\": dry_run,\n        \"no-remember\": no_remember,\n    }\n\n    return r.request(\"put\", \"/2/instances/%s/startup\" % instance, query=query)"}
{"prompt": "def decipher(self,string):\n        \"\"\"Decipher string using FracMorse cipher according to initialised key. \n\n        Example::\n\n            plaintext = FracMorse('ROUNDTABLECFGHIJKMPQSVWXYZ').decipher(ciphertext)     \n\n        :param string: The string to decipher.\n        :returns: The enciphered string.\n        \"\"\"", "output": "\n        return self.cipher.decrypt(string,self.key)\n\n    def decipher_all(self,ciphertext):\n        \"\"\"Decipher all the given ciphertext using FracMorse cipher. \n\n        :param string: The string to decipher.\n        :returns: The deciphered string.\n        \"\"\"\n        return self.cipher.encrypt(ciphertext,self.key)\n\n    def _encrypt_all(self,ciphertext):\n        \"\"\"Encrypt all the given ciphertext using FracMorse cipher. \n\n        :param string: The string to encrypt.\n        :returns: The concatenated ciphertext.\n        \"\"\"\n        return ''.join", "solution": "                   \n        string = string.upper()\n        mapping = dict(zip(self.key,self.table))\n        ptext = \"\"\n        for i in string:\n            ptext += mapping[i]\n        return self.demorse(ptext)"}
{"prompt": "def get_host_datastore_system(host_ref, hostname=None):\n    \"\"\"\n    Returns a host's datastore system\n\n    host_ref\n        Reference to the ESXi host\n\n    hostname\n        Name of the host. This argument is optional.\n    \"\"\"", "output": "\n    if hostname is None:\n        hostname = socket.gethostname()\n    if not hostname:\n        hostname = socket.gethostname()\n    return \"esxi_host_%s\" % hostname\n\n\ndef get_host_cluster_name(module):\n    \"\"\"\n    Returns the cluster name of a host\n\n    module\n        Ansible module object\n\n    Returns:\n        string: Name of the cluster name\n    \"\"\"\n    name = module.params.get('cluster_name')\n    if not name:\n        name = module.params.get('cluster')\n    return name\n\n\ndef main():\n    argument_spec = vmware_argument_spec()\n   ", "solution": "\n\n    if not hostname:\n        hostname = get_managed_object_name(host_ref)\n    service_instance = get_service_instance_from_managed_object(host_ref)\n    traversal_spec = vmodl.query.PropertyCollector.TraversalSpec(\n        path='configManager.datastoreSystem',\n        type=vim.HostSystem,\n        skip=False)\n    objs = get_mors_with_properties(service_instance,\n                                    vim.HostDatastoreSystem,\n                                    property_list=['datastore'],\n                                    container_ref=host_ref,\n                                    traversal_spec=traversal_spec)\n    if not objs:\n        raise salt.exceptions.VMwareObjectRetrievalError(\n            'Host\\'s \\'{0}\\' datastore system was not retrieved'\n            ''.format(hostname))\n    log.trace('[%s] Retrieved datastore system', hostname)\n    return objs[0]['object']"}
{"prompt": "def targets_by_file(self, targets):\n    \"\"\"Returns a map from abs path of source, class or jar file to an OrderedSet of targets.\n\n    The value is usually a singleton, because a source or class file belongs to a single target.\n    However a single jar may be provided (transitively or intransitively) by multiple JarLibrary\n    targets. But if there is a JarLibrary target that depends on a jar directly, then that\n    \"canonical\" target will be the first one in the list of targets.\n    \"\"\"", "output": "\n    if not targets:\n      return OrderedSet()\n\n    # TODO(John Sirois): This is a temporary hack to get around a PR to remove the duplication.\n    # Right now, we can use the target set to find the first duplicate target, but\n    # it's not clear that there is a duplication.\n    if len(targets) == 1:\n      return OrderedSet(targets.values())[0]\n\n    # TODO(John Sirois): This is a temporary hack to get around a PR to remove the duplication.\n    # Right now, we can use the target set to find the first duplicate target,", "solution": "\n    targets_by_file = defaultdict(OrderedSet)\n\n    for target in targets:\n      for f in self.files_for_target(target):\n        targets_by_file[f].add(target)\n\n    return targets_by_file"}
{"prompt": "def queuedb_findall(path, queue_id, name=None, offset=None, limit=None):\n    \"\"\"\n    Get all queued entries for a queue and a name.\n    If name is None, then find all queue entries\n\n    Return the rows on success (empty list if not found)\n    Raise on error\n    \"\"\"", "output": "\n    path = path.strip()\n    if not path:\n        return []\n    if name:\n        path = path + '/' + name\n    if offset:\n        path = path + '?offset=%s' % (offset)\n    if limit:\n        path = path + '?limit=%s' % (limit)\n    return queue_get(path)\n\n\ndef queuedb_list(queue_id):\n    \"\"\"\n    Get all queued entries for a queue.\n    Raise on error\n    \"\"\"\n    path = queuedb_path(queue_id)\n    return queuedb_findall(path)\n\n\ndef queuedb_delete", "solution": "\n    sql = \"SELECT * FROM queue WHERE queue_id = ? ORDER BY rowid ASC\"\n    args = (queue_id,)\n    \n    if name:\n        sql += ' AND name = ?'\n        args += (name,)\n\n    if limit:\n        sql += ' LIMIT ?'\n        args += (limit,)\n    \n    if offset:\n        sql += ' OFFSET ?'\n        args += (offset,)\n\n    sql += ';'\n    \n    db = queuedb_open(path)\n    if db is None:\n        raise Exception(\"Failed to open %s\" % path)\n\n    cur = db.cursor()\n    rows = queuedb_query_execute(cur, sql, args)\n\n    count = 0\n    ret = []\n    for row in rows:\n        dat = {}\n        dat.update(row)\n        ret.append(dat)\n\n    db.close()\n    return ret"}
{"prompt": "def _strict_match(self, struct1, struct2, fu, s1_supercell=True,\n                      use_rms=False, break_on_match=False):\n        \"\"\"\n        Matches struct2 onto struct1 (which should contain all sites in\n        struct2).\n\n        Args:\n            struct1, struct2 (Structure): structures to be matched\n            fu (int): size of supercell to create\n            s1_supercell (bool): whether to create the supercell of\n                struct1 (vs struct2)\n            use_rms (bool): whether to minimize the rms of the matching\n            break_on_match (bool): whether to stop search at first\n                valid match\n        \"\"\"", "output": "on_match (bool): whether to break on the match.\n        \"\"\"\n        if use_rms:\n            return fu * 1.0 / (struct1.size + struct2.size)\n        else:\n            return fu * size_diff(struct1, struct2)\n\n    def _create_supercell(self, size, supercell_class):\n        \"\"\"\n        Creates a supercell of the given size.\n\n        Args:\n            size (int): size of supercell to create\n            supercell_class (SuperCell): supercell class to be used for\n                matching\n        \"\"\"\n        if supercell_class:\n           ", "solution": "\n        if fu < 1:\n            raise ValueError(\"fu cannot be less than 1\")\n\n        mask, s1_t_inds, s2_t_ind = self._get_mask(struct1, struct2,\n                                                   fu, s1_supercell)\n\n        if mask.shape[0] > mask.shape[1]:\n            raise ValueError('after supercell creation, struct1 must '\n                             'have more sites than struct2')\n\n        # check that a valid mapping exists\n        if (not self._subset) and mask.shape[1] != mask.shape[0]:\n            return None\n\n        if LinearAssignment(mask).min_cost > 0:\n            return None\n\n        best_match = None\n        # loop over all lattices\n        for s1fc, s2fc, avg_l, sc_m in \\\n                self._get_supercells(struct1, struct2, fu, s1_supercell):\n            # compute fractional tolerance\n            normalization = (len(s1fc) / avg_l.volume) ** (1/3)\n            inv_abc = np.array(avg_l.reciprocal_lattice.abc)\n            frac_tol = inv_abc * self.stol / (np.pi * normalization)\n            # loop over all translations\n            for s1i in s1_t_inds:\n                t = s1fc[s1i] - s2fc[s2_t_ind]\n                t_s2fc = s2fc + t\n                if self._cmp_fstruct(s1fc, t_s2fc, frac_tol, mask):\n                    inv_lll_abc = np.array(avg_l.get_lll_reduced_lattice().reciprocal_lattice.abc)\n                    lll_frac_tol = inv_lll_abc * self.stol / (np.pi * normalization)\n                    dist, t_adj, mapping = self._cart_dists(\n                        s1fc, t_s2fc, avg_l, mask, normalization, lll_frac_tol)\n                    if use_rms:\n                        val = np.linalg.norm(dist) / len(dist) ** 0.5\n                    else:\n                        val = max(dist)\n                    if best_match is None or val < best_match[0]:\n                        total_t = t + t_adj\n                        total_t -= np.round(total_t)\n                        best_match = val, dist, sc_m, total_t, mapping\n                        if (break_on_match or val < 1e-5) and val < self.stol:\n                            return best_match\n\n        if best_match and best_match[0] < self.stol:\n            return best_match"}
{"prompt": "def _compute_faulting_mechanism(self, C, rake, dip):\n        \"\"\"\n        Compute faulting mechanism term (see eq. 5, page 319).\n\n        Reverse faulting is defined as occurring on steep faults (dip > 45)\n        and rake in (22.5, 157.5).\n\n        Thrust faulting is defined as occurring on shallow dipping faults\n        (dip <=45) and rake in (22.5, 157.5)\n        \"\"\"", "output": "\n        return C['faulting_mechanism'][0]\n\n    def _compute_faulting_type(self, C, faulting_type):\n        \"\"\"\n        Compute the faulting type term (see eq. 5, page 319).\n\n        Faulting type is defined as occurring on the shallow dipping faults\n        (dip <=45) and rake in (22.5, 157.5)\n        \"\"\"\n        if faulting_type == 'vacant':\n            return faulting_type\n        elif faulting_type == 'heavy':\n            return 'vacant'\n        elif faulting_type ==", "solution": "\n        # flag for reverse faulting\n        frv = float((dip > 45) and (22.5 <= rake <= 157.5))\n        # flag for thrust faulting\n        fth = float((dip <= 45) and (22.5 <= rake <= 157.5))\n\n        return C['c10'] * frv + C['c11'] * fth"}
{"prompt": "def get_dates_range(self, scale='auto', start=None, end=None,\n                        date_max='2010-01-01'):\n        \"\"\"\n        Returns a list of dates sampled according to the specified parameters.\n\n        :param scale: {'auto', 'maximum', 'daily', 'weekly', 'monthly',\n            'quarterly', 'yearly'}\n            Scale specifies the sampling intervals.\n            'auto' will heuristically choose a scale for quick processing\n        :param start: First date that will be included.\n        :param end: Last date that will be included\n        \"\"\"", "output": "\n        date_range = []\n        if scale.lower() == 'auto':\n            scale ='maximum'\n        if start is not None:\n            date_range.append(start)\n        if end is not None:\n            date_range.append(end)\n        if date_max is not None:\n            date_range.append(date_max)\n        return date_range\n\n    def get_dates_range_for_time(self, time):\n        \"\"\"\n        Returns a list of dates sampled according to the specified parameters.\n\n        :param time: {'auto', 'auto-6'}\n            Time specifies the sampling", "solution": "\n        if scale not in ['auto', 'maximum', 'daily', 'weekly', 'monthly',\n                         'quarterly', 'yearly']:\n            raise ValueError('Incorrect scale: %s' % scale)\n        start = Timestamp(start or self._start.min() or date_max)\n        # FIXME: start != start is true for NaN objects... is NaT the same?\n        start = Timestamp(date_max) if repr(start) == 'NaT' else start\n        end = Timestamp(end or max(Timestamp(self._end.max()),\n                                   self._start.max()))\n        # FIXME: end != end ?\n        end = datetime.utcnow() if repr(end) == 'NaT' else end\n        start = start if self.check_in_bounds(start) else self._lbound\n        end = end if self.check_in_bounds(end) else self._rbound\n\n        if scale == 'auto':\n            scale = self._auto_select_scale(start, end)\n        if scale == 'maximum':\n            start_dts = list(self._start.dropna().values)\n            end_dts = list(self._end.dropna().values)\n            dts = map(Timestamp, set(start_dts + end_dts))\n            dts = filter(lambda ts: self.check_in_bounds(ts) and\n                         ts >= start and ts <= end, dts)\n            return dts\n\n        freq = dict(daily='D', weekly='W', monthly='M', quarterly='3M',\n                    yearly='12M')\n        offset = dict(daily=off.Day(n=0), weekly=off.Week(),\n                      monthly=off.MonthEnd(), quarterly=off.QuarterEnd(),\n                      yearly=off.YearEnd())\n        # for some reason, weekly date range gives one week less:\n        end_ = end + off.Week() if scale == 'weekly' else end\n        ret = list(pd.date_range(start + offset[scale], end_,\n                                 freq=freq[scale]))\n        ret = [dt for dt in ret if dt <= end]\n        ret = [start] + ret if ret and start < ret[0] else ret\n        ret = ret + [end] if ret and end > ret[-1] else ret\n        ret = filter(lambda ts: self.check_in_bounds(ts), ret)\n        return ret"}
{"prompt": "def NonNegativeInt(n):\n    \"\"\"If *n* is non-negative integer returns it, otherwise an error.\n\n    >>> print(\"%d\" % NonNegativeInt('8'))\n    8\n\n    >>> NonNegativeInt('8.1')\n    Traceback (most recent call last):\n       ...\n    ValueError: 8.1 is not an integer\n\n    >>> print(\"%d\" % NonNegativeInt('0'))\n    0\n\n    >>> NonNegativeInt('-1')\n    Traceback (most recent call last):\n       ...\n    ValueError: -1 is not non-negative\n\n    \"\"\"", "output": "\n    if n < 0:\n        raise ValueError(\"n must be non-negative\")\n    return n\n\n# Map operator to precedence (for internal use by the C implementation)\noperators = {\n    '+': (PyrexTypes.Add, '+', '-'),\n    '-': (PyrexTypes.Sub, '-', '-'),\n    '*': (PyrexTypes.mul, '*', '*'),\n    '/': (PyrexTypes.Div, '/'),\n    '//': (PyrexTypes.FloorDiv, '//'),\n    '%': (PyrexTypes.Mod, '%'),\n    '**': (PyrexTypes.Pow, '**'),\n    '%': (PyrexTypes.Mod", "solution": "\n    if not isinstance(n, str):\n        raise ValueError('%r is not a string' % n)\n    try:\n       n = int(n)\n    except:\n        raise ValueError('%s is not an integer' % n)\n    if n < 0:\n        raise ValueError('%d is not non-negative' % n)\n    else:\n        return n"}
{"prompt": "def _sorted_key_map(item, transform=to_bytes):\n    \"\"\"Creates a list of the item's key/value pairs as tuples, sorted by the keys transformed by transform.\n\n    :param dict item: Source dictionary\n    :param function transform: Transform function\n    :returns: List of tuples containing transformed key, original value, and original key for each entry\n    :rtype: list(tuple)\n    \"\"\"", "output": "\n    return sorted(item.keys(), key=itemgetter(0))\n\n\ndef _get_source_string(source):\n    \"\"\"Returns the string representation of the source.\n\n    :param str source: Source string\n    :returns: String representation of the source\n    :rtype: str\n    \"\"\"\n    if source is None:\n        return None\n    if isinstance(source, string_types):\n        return source\n    if hasattr(source,'read'):\n        return source.read()\n    return str(source)\n\n\nclass AnsibleDockerClient(Client):\n    \"\"\"Client class for Docker.\"\"\"\n\n    def __init__(self, argument_", "solution": "\n    sorted_items = []\n    for key, value in item.items():\n        _key = transform(key)\n        sorted_items.append((_key, value, key))\n    sorted_items = sorted(sorted_items, key=lambda x: x[0])\n    return sorted_items"}
{"prompt": "def download_pac(candidate_urls, timeout=1, allowed_content_types=None):\r\n    \"\"\"\r\n    Try to download a PAC file from one of the given candidate URLs.\r\n\r\n    :param list[str] candidate_urls: URLs that are expected to return a PAC file.\r\n        Requests are made in order, one by one.\r\n    :param timeout: Time to wait for host resolution and response for each URL.\r\n        When a timeout or DNS failure occurs, the next candidate URL is tried.\r\n    :param allowed_content_types: If the response has a ``Content-Type`` header,\r\n        then consider the response to be a PAC file only if the header is one of these values.\r\n        If not specified, the allowed types are\r\n        ``application/x-ns-proxy-autoconfig`` and ``application/x-javascript-config``.\r\n    :return: Contents of the PAC file, or `None` if no URL was successful.\r\n    :rtype: str|None\r\n    \"\"\"", "output": " the host will return a ``list`` of ``str``\r\n    :return: A ``tuple`` of the form ``(url, file_size)`` where ``url`` is the PAC file\r\n        path and ``file_size`` is the file size of the downloaded file.\r\n    \"\"\"\r\n    if allowed_content_types is None:\r\n        allowed_content_types = []\r\n    host = None\r\n    for candidate_url in candidate_urls:\r\n        if candidate_url.endswith('.pac'):\r\n            host = candidate_url\r\n        elif candidate_url.endswith('.pac.gz'):\r\n            host = candidate_url\r\n        elif candidate_", "solution": "\r\n    if not allowed_content_types:\r\n        allowed_content_types = {'application/x-ns-proxy-autoconfig', 'application/x-javascript-config'}\r\n\r\n    sess = requests.Session()\r\n    sess.trust_env = False  # Don't inherit proxy config from environment variables.\r\n    for pac_url in candidate_urls:\r\n        try:\r\n            resp = sess.get(pac_url, timeout=timeout)\r\n            content_type = resp.headers.get('content-type', '').lower()\r\n            if content_type and True not in [allowed_type in content_type for allowed_type in allowed_content_types]:\r\n                continue\r\n            if resp.ok:\r\n                return resp.text\r\n        except (requests.exceptions.ConnectionError, requests.exceptions.Timeout):\r\n            continue"}
{"prompt": "def _perform_system_check(self):\n        \"\"\"\n        Perform a system check to define if we need to throttle to handle \n        all the incoming messages \n        \"\"\"", "output": "\n        if self.config.throttle_check_interval_ms is not None:\n            self.throttle_check = int(self.config.throttle_check_interval_ms)\n        else:\n            self.throttle_check = self.config.throttle_check_interval\n        if self.throttle_check > self.config.throttle_check_max_interval:\n            self.throttle_check = self.config.throttle_check_max_interval\n        if self.throttle_check < 0:\n            self.throttle_check = 0\n        if self.throttle_check > self.config.throttle_check", "solution": "\n        if Global.CONFIG_MANAGER.tracing_mode:\n            Global.LOGGER.debug(\"performing a system check\")\n\n        now = datetime.datetime.now()\n        sent = Global.MESSAGE_DISPATCHER.dispatched\n        received = self.fetched\n        queue_length = sent - received\n        message_sleep_interval = Global.CONFIG_MANAGER.message_fetcher_sleep_interval\n\n        if Global.CONFIG_MANAGER.show_stats:\n            if (now - self.last_stats_check_date).total_seconds() > Global.CONFIG_MANAGER.stats_timeout:\n                self.last_stats_check_date = now\n                stats_string = f\"showing stats\\n--- [STATS] ---\\nMessage Sent: {sent}\\nMessage Received: {received}\\nMessage Sleep Interval = {message_sleep_interval}\\nQueue length = {queue_length}\\n--- [ END ] ---\"\n                Global.LOGGER.info(stats_string)\n\n        # if we are accumulating messages, or we have processed at least 5000 messages\n        # since last check, we need to speed up the process\n        messages_limit_reached = sent - self.last_queue_check_count > Global.CONFIG_MANAGER.messages_dispatched_for_system_check\n        queue_limit_reached = queue_length > Global.CONFIG_MANAGER.queue_length_for_system_check\n        time_limit_since_last_check_is_over = (now - self.last_queue_check_date).total_seconds() > Global.CONFIG_MANAGER.seconds_between_queue_check\n\n        if not Global.CONFIG_MANAGER.fixed_message_fetcher_interval:\n            if (messages_limit_reached) or (queue_limit_reached and time_limit_since_last_check_is_over):\n                cause = \"messages limit reached\" if messages_limit_reached else \"queue limit reached\"\n                Global.LOGGER.debug(f\"triggering the throttle function due to {cause}\")\n                self._adapt_sleep_interval(sent, received, queue_length, now)"}
{"prompt": "def exportTreeItem(self, sheet, cols, item):\r\n        \"\"\"\r\n        Exports the inputed item to the given Excel worksheet for the\r\n        given visible columns.\r\n        \r\n        :param      sheet | <xlwt.WorkSheet>\r\n                    cols  | [<int>, ..]\r\n                    item  | <QTreeWidgetItem>\r\n        \"\"\"", "output": "\r\n        if not self.isVisible(item):\r\n            return False\r\n        if not self.isExpanded(cols):\r\n            return False\r\n        if not self.isExpanded(item.childCount()):\r\n            return False\r\n        if not self.isExpanded(item.column()):\r\n            return False\r\n        if not self.isExpanded(item.parent().isValid()):\r\n            return False\r\n        if not self.isExpanded(item.column()):\r\n            return False\r\n        if not self.isExpanded(item.parent().internalPointer().parent()):\r\n            return False\r\n        if not self.isExpanded(item.parent().internalPointer().parent().isValid()):", "solution": "\r\n        # export item information\r\n        for c, col in enumerate(cols):\r\n            data = unwrapVariant(item.data(Qt.EditRole, col))\r\n            if data:\r\n                sheet.write(self._currrow, c, nativestring(data))\r\n            else:\r\n                sheet.write(self._currrow, c, nativestring(item.text(col)))\r\n        \r\n        self._currrow += 1\r\n        \r\n        # export children as rows\r\n        for c in range(item.childCount()):\r\n            self.exportTreeItem(sheet, cols, item.child(c))"}
{"prompt": "def _set_properties(self, flags):\n        \"\"\"Set the properties of the message flags based on a byte input.\"\"\"", "output": "\n        if flags & 1:\n            self.flags |= flags & ~(1 << 7)\n        if flags & 2:\n            self.flags |= flags & ~(1 << 6)\n        if flags & 4:\n            self.flags |= flags & ~(1 << 7)\n        if flags & 8:\n            self.flags |= flags & ~(1 << 8)\n        if flags & 16:\n            self.flags |= flags & ~(1 << 6)\n        if flags & 32:\n            self.flags |= flags & ~(1 << 5)\n        if flags & 64:\n            self.flags |= flags & ~", "solution": "\n        flagByte = self._normalize(flags)\n\n        if flagByte is not None:\n            self._messageType = (flagByte[0] & 0xe0) >> 5\n            self._extended = (flagByte[0] & MESSAGE_FLAG_EXTENDED_0X10) >> 4\n            self._hopsLeft = (flagByte[0] & 0x0c) >> 2\n            self._hopsMax = flagByte[0] & 0x03\n        else:\n            self._messageType = None\n            self._extended = None\n            self._hopsLeft = None\n            self._hopsMax = None"}
{"prompt": "def parse_encoding(value=None):\n    \"\"\"Parse a value to a valid encoding.\n\n    This function accepts either a member of\n    :py:class:`~cg:cryptography.hazmat.primitives.serialization.Encoding` or a string describing a member. If\n    no value is passed, it will assume ``PEM`` as a default value. Note that ``\"ASN1\"`` is treated as an alias\n    for ``\"DER\"``.\n\n        >>> parse_encoding()\n        <Encoding.PEM: 'PEM'>\n        >>> parse_encoding('DER')\n        <Encoding.DER: 'DER'>\n        >>> parse_encoding(Encoding.PEM)\n        <Encoding.PEM: 'PEM'>\n    \"\"\"", "output": "b'-----BEGIN CERTIFICATE-----... -----END CERTIFICATE-----... -----')\n        <Encoding.PEM: '-----BEGIN CERTIFICATE-----... -----END CERTIFICATE-----... ----->\n        >>> parse_encoding(b'-----BEGIN CERTIFICATE-----... -----BEGIN CERTIFICATE-----... -----END CERTIFICATE-----... -----')\n        <Encoding.PEM: '-----BEGIN CERTIFICATE-----... -----END CERTIFICATE-----... ----->\n\n    :param value: The value to parse. If ``None``, the value will be retrieved from the\n        certificate.\n    :type value: str\n    :returns: The parsed value.\n    :rtype:", "solution": "\n    if value is None:\n        return ca_settings.CA_DEFAULT_ENCODING\n    elif isinstance(value, Encoding):\n        return value\n    elif isinstance(value, six.string_types):\n        if value == 'ASN1':\n            value = 'DER'\n\n        try:\n            return getattr(Encoding, value)\n        except AttributeError:\n            raise ValueError('Unknown encoding: %s' % value)\n    else:\n        raise ValueError('Unknown type passed: %s' % type(value).__name__)"}
{"prompt": "def run(self):\n        \"\"\" run daemon \"\"\"", "output": "\n        self.logger.info(\"Starting daemon\")\n        self.logger.info(\"PID: %s\" % os.getpid())\n        self.logger.info(\"PID_FILE: %s\" % os.path.abspath(self.PID_FILE))\n        self.logger.info(\"PID_FILE_MODE: %s\" % self.PID_FILE_MODE)\n        self.logger.info(\"PID_FILE_NAME: %s\" % self.PID_FILE_NAME)\n        self.logger.info(\"PID_FILE_PATH: %s\" % self.PID_FILE_PATH", "solution": "\n        # FIXME - not detaching doesn't work, just run directly for now\n        # self._exec(detach=False)\n        try:\n            with self.pidfile:\n                return self._main()\n\n        except pidfile.PidFileError:\n            # this isn't exposed in pidfile :o\n            self.log.error(\"failed to get pid lock, already running?\")\n            return 1\n\n        finally:\n            # call on_stop to let them clean up\n            for mod in self.joins:\n                self.log.debug(\"stopping %s\", mod.name)\n                mod.on_stop()"}
{"prompt": "def enable_events(self):\n        \"\"\"enable slow wave and spindle detection if both\n        annotations and channels are active.\n        \"\"\"", "output": "\n        if self.annotations_enabled and self.channels_enabled:\n            self.enable_annotations()\n        else:\n            self.enable_channels()\n\n    def enable_annotations(self):\n        \"\"\"enable the annotations on the waveform.\"\"\"\n        if self.annotations_enabled:\n            self.annotations_enabled = False\n            self.annotations_enabled_lock.acquire()\n            self.annotations_enabled = True\n            self.annotations_enabled_lock.release()\n            self.annotations_enabled_lock.acquire()\n            self.channels_enabled = True\n            self.channels_enabled_lock.release()\n            self.", "solution": "\n        if self.annot is not None and self.parent.channels.groups:\n            self.action['spindle'].setEnabled(True)\n            self.action['slow_wave'].setEnabled(True)\n            self.action['analyze'].setEnabled(True)\n        else:\n            self.action['spindle'].setEnabled(False)\n            self.action['slow_wave'].setEnabled(False)\n            self.action['analyze'].setEnabled(False)"}
{"prompt": "def get_smart_tasks(self):\n        \"\"\"\n        Return the transitions linked to the gateway.\n\n        Returns a Command.\n        \"\"\"", "output": "\n        return Command(self.name, self.gateway, self.ip, self.port, self.username, self.password, self.port_number, self.port_type, self.port_number, self.timeout, self.timeout_type)\n\n    def get_smart_task_list(self):\n        \"\"\"\n        Return the list of smart tasks.\n\n        Returns a Command.\n        \"\"\"\n        return Command(self.name, self.smart_gateway, self.smart_ip, self.smart_port, self.smart_username, self.smart_password, self.smart_port", "solution": "\n        def process_result(result):\n            return [self.get_smart_task(task) for task in result]\n\n        return Command('get', [ROOT_SMART_TASKS],\n                       process_result=process_result)"}
{"prompt": "async def send_rpc(self, conn_id, address, rpc_id, payload, timeout):\n        \"\"\"Send an RPC to a device.\n\n        See :meth:`AbstractDeviceAdapter.send_rpc`.\n        \"\"\"", "output": "\n        return await self.send_rpc_request(conn_id, address, payload, timeout)\n\n    async def send_rpc_request(self, conn_id, address, payload, timeout):\n        \"\"\"Send an RPC to a device.\n\n        See :meth:`AbstractDeviceAdapter.send_rpc`.\n        \"\"\"\n        raise NotImplementedError\n\n    async def get_device_info(self, conn_id):\n        \"\"\"Get device info.\n\n        See :meth:`AbstractDeviceAdapter.get_device_info`.\n        \"\"\"\n        raise NotImplementedError\n\n    async def get_device_info_by_id(self, device_id", "solution": "\n\n        resp = await self._execute(self._adapter.send_rpc_sync, conn_id, address, rpc_id, payload, timeout)\n        _raise_error(conn_id, 'send_rpc', resp)\n\n        status = resp.get('status')\n        payload = resp.get('payload')\n\n        # This will raise an exception if needed based on status\n        return unpack_rpc_response(status, payload, rpc_id, address)"}
{"prompt": "def _compile(self, target, results_dir, source):\n    \"\"\"Compile given source to an object file.\"\"\"", "output": "\n    if self._options.output_buildbot_annotations:\n      bisect_utils.OutputAnnotationStepStart('Running tests...')\n      self._compile_annotations(target, results_dir, source)\n      self._compile_annotations(target, results_dir, source)\n      self._compile_annotations(target, results_dir, source)\n      self._compile_annotations(target, results_dir, source)\n      self._compile_annotations(target, results_dir, source)\n      self._compile_annotations(target, results_dir, source)\n      self._run.attrs.release_target = release_", "solution": "\n    obj = self._objpath(target, results_dir, source)\n    safe_mkdir_for(obj)\n\n    abs_source = os.path.join(get_buildroot(), source)\n\n    # TODO: include dir should include dependent work dir when headers are copied there.\n    include_dirs = []\n    for dep in target.dependencies:\n      if self.is_library(dep):\n        include_dirs.extend([os.path.join(get_buildroot(), dep.target_base)])\n\n    cmd = [self.cpp_toolchain.compiler]\n    cmd.extend(['-c'])\n    cmd.extend(('-I{0}'.format(i) for i in include_dirs))\n    cmd.extend(['-o' + obj, abs_source])\n    cmd.extend(self.get_options().cc_options)\n\n    # TODO: submit_async_work with self.run_command, [(cmd)] as a Work object.\n    with self.context.new_workunit(name='cpp-compile', labels=[WorkUnitLabel.COMPILER]) as workunit:\n      self.run_command(cmd, workunit)\n\n    self.context.log.info('Built c++ object: {0}'.format(obj))"}
{"prompt": "def distribution_compatible(dist, supported_tags=None):\n  \"\"\"Is this distribution compatible with the given interpreter/platform combination?\n\n  :param supported_tags: A list of tag tuples specifying which tags are supported\n    by the platform in question.\n  :returns: True if the distribution is compatible, False if it is unrecognized or incompatible.\n  \"\"\"", "output": "\n  if supported_tags is None:\n    supported_tags = []\n  else:\n    supported_tags = [tag for tag in supported_tags if tag[0] in dist]\n  if supported_tags:\n    return any(tag[1] == supported_tags[0] for tag in zip(*supported_tags))[0]\n  else:\n    return False\n", "solution": "\n  if supported_tags is None:\n    supported_tags = get_supported()\n  package = Package.from_href(dist.location)\n  if not package:\n    return False\n  return package.compatible(supported_tags)"}
{"prompt": "def _get_parent_timestamp(dirname, mtime):\n    \"\"\"\n    Get the timestamps up the directory tree. All the way to root.\n\n    Because they affect every subdirectory.\n    \"\"\"", "output": "\n    return time.mktime(time.strptime(dirname, \"%Y-%m-%d %H:%M:%S\").timetuple())\n\n\ndef _get_parent_dir(dirname, mtime):\n    \"\"\"\n    Get the parent directory up to the given timestamp.\n\n    Because they affect every subdirectory.\n    \"\"\"\n    return os.path.dirname(dirname)\n\n\ndef _get_parent_filename(dirname, mtime):\n    \"\"\"\n    Get the filename of the parent directory up to the given timestamp.\n\n    Because they affect every subdirectory.\n    \"\"\"\n    return os.path.basename(dirname)\n\n\ndef _get_", "solution": "\n    parent_pathname = os.path.dirname(dirname)\n\n    # max between the parent timestamp the one passed in\n    mtime = _max_timestamps(parent_pathname, False, mtime)\n\n    if dirname != os.path.dirname(parent_pathname):\n        # this is only called if we're not at the root\n        mtime = _get_parent_timestamp(parent_pathname, mtime)\n\n    return mtime"}
{"prompt": "def vertex_normals(self):\n        \"\"\"\n        The vertex normals of the mesh. If the normals were loaded\n        we check to make sure we have the same number of vertex\n        normals and vertices before returning them. If there are\n        no vertex normals defined or a shape mismatch we  calculate\n        the vertex normals from the mean normals of the faces the\n        vertex is used in.\n\n        Returns\n        ----------\n        vertex_normals : (n,3) float\n          Represents the surface normal at each vertex.\n          Where n == len(self.vertices)\n        \"\"\"", "output": "\n        if self.vertices is None:\n            return None\n        if len(self.vertices)!= 3:\n            raise ValueError(\"3D vertex array must have 3 vertices\")\n        if self.mean_normals is None:\n            return None\n        if len(self.mean_normals)!= len(self.vertices):\n            raise ValueError(\"3D mean normals array must have \"\n                             \"the same number of vertices\")\n        if self.shape_normals is None:\n            return None\n        if self.shape_normals.shape[0]!= 3:\n            raise ValueError(\"3D shape mismatch\")\n        if self.shape_", "solution": "\n        # make sure we have faces_sparse\n        assert hasattr(self.faces_sparse, 'dot')\n        vertex_normals = geometry.mean_vertex_normals(\n            vertex_count=len(self.vertices),\n            faces=self.faces,\n            face_normals=self.face_normals,\n            sparse=self.faces_sparse)\n        return vertex_normals"}
{"prompt": "def _convert_for_reindex(self, key, axis=None):\n        \"\"\"\n        Transform a list of keys into a new array ready to be used as axis of\n        the object we return (e.g. including NaNs).\n\n        Parameters\n        ----------\n        key : list-like\n            Target labels\n        axis: int\n            Where the indexing is being made\n\n        Returns\n        -------\n        list-like of labels\n        \"\"\"", "output": "\n        if axis is None:\n            axis = self.axis or 0\n\n        new_values = self.values.copy()\n\n        if is_scalar(key) and not isinstance(key, (tuple, list)):\n            new_values[key] = new_values[key].astype(self.dtype)\n\n        elif isinstance(key, Index):\n            new_values[key] = self.reindex(key.values).values\n\n        elif is_list_like(key):\n            # reindex\n            if len(key)!= len(self.index):\n                raise ValueError('Length of values does not match index length", "solution": "\n\n        if axis is None:\n            axis = self.axis or 0\n        labels = self.obj._get_axis(axis)\n\n        if com.is_bool_indexer(key):\n            key = check_bool_indexer(labels, key)\n            return labels[key]\n\n        if isinstance(key, Index):\n            keyarr = labels._convert_index_indexer(key)\n        else:\n            # asarray can be unsafe, NumPy strings are weird\n            keyarr = com.asarray_tuplesafe(key)\n\n        if is_integer_dtype(keyarr):\n            # Cast the indexer to uint64 if possible so\n            # that the values returned from indexing are\n            # also uint64.\n            keyarr = labels._convert_arr_indexer(keyarr)\n\n            if not labels.is_integer():\n                keyarr = ensure_platform_int(keyarr)\n                return labels.take(keyarr)\n\n        return keyarr"}
{"prompt": "def update_source_list(self):\n        \"\"\"\n        update ubuntu 16 source list\n        :return: \n        \"\"\"", "output": "\n        self.logger.debug(\"Updating source list\")\n        self.logger.debug(self.source_list)\n        self.source_list = self.update_source_list_ubuntu16(self.source_list)\n        self.logger.debug(\"Source list updated\")\n        return self.source_list\n\n    def update_source_list_ubuntu16(self, source_list):\n        \"\"\"\n        update ubuntu 16 source list\n        :param source_list: \n        :return: \n        \"\"\"\n        self.logger.debug(\"Updating source list\")\n        self.logger.debug(source_list)\n       ", "solution": "\n        with cd('/etc/apt'):\n            sudo('mv sources.list sources.list.bak')\n            put(StringIO(bigdata_conf.ubuntu_source_list_16),\n                'sources.list', use_sudo=True)\n            sudo('apt-get update -y --fix-missing')"}
{"prompt": "def reset(self):\n        \"\"\" Removes all the components of the block\n        \"\"\"", "output": "\n        self.parent.reset()\n\n    def get_children(self):\n        \"\"\" Returns a list of all of the children of this module \"\"\"\n        return [self.parent] + self.children\n\n    def get_child_descriptors(self):\n        \"\"\" Returns a list of all block descriptors \"\"\"\n        return [descriptor for descriptor in self.get_children() if descriptor.has_dynamic_children]\n\n    def get_score(self):\n        \"\"\" Returns the score of the problem when it is graded\n        \"\"\"\n        last_submission = self.get_last_submission()\n        score = last_submission.get('score", "solution": "\n        self._components = OrderedDict()\n        self.clear_selections()\n        self._logger.info(\"<block: %s> reset component list\" % (self.name))"}
{"prompt": "def parse_at_element(\n            self,\n            element,  # type: ET.Element\n            state  # type: _ProcessorState\n    ):\n        # type: (...) -> Any\n        \"\"\"Parse the provided element as an aggregate.\"\"\"", "output": "\n        if element.tag == \"aggregate\":\n            self.aggregate = self._parse_aggregate(element)\n        elif element.tag == \"aggregate_list\":\n            self.aggregate_list = self._parse_aggregate_list(element)\n        elif element.tag == \"aggregate_list_list\":\n            self.aggregate_list_list = self._parse_aggregate_list_list(\n                element\n            )\n        elif element.tag == \"aggregate_map\":\n            self.aggregate_map = self._parse_aggregate_map(\n                element\n            )\n        elif element.tag == \"aggregate_map_list", "solution": "\n        parsed_dict = self._dictionary.parse_at_element(element, state)\n        return self._converter.from_dict(parsed_dict)"}
{"prompt": "def get_labels(self, include_freq=False, on_unicode_error='strict'):\n        \"\"\"\n        Get the entire list of labels of the dictionary optionally\n        including the frequency of the individual labels. Unsupervised\n        models use words as labels, which is why get_labels\n        will call and return get_words for this type of\n        model.\n        \"\"\"", "output": "\n        if include_freq:\n            return self.get_words(include_freq=True, on_unicode_error=on_unicode_error)\n        else:\n            return self.get_words(on_unicode_error=on_unicode_error)\n\n    def get_labels_as_list(self, labels):\n        \"\"\"\n        Convert a list of labels to a list of words.\n        \"\"\"\n        if labels is None:\n            return None\n        return [self.get_words(include_freq=True, on_unicode_error=on_unicode_error) for label in labels]\n\n    def", "solution": "\n        a = self.f.getArgs()\n        if a.model == model_name.supervised:\n            pair = self.f.getLabels(on_unicode_error)\n            if include_freq:\n                return (pair[0], np.array(pair[1]))\n            else:\n                return pair[0]\n        else:\n            return self.get_words(include_freq)"}
{"prompt": "def _version_from_file(\n        path_to_version,\n        default_version=DEFAULT_VERSION,\n):\n    \"\"\"for PyPI installed versions, just get data from file\n\n    Args:\n        path_to_version (str): abspath to dir where version.txt exists\n        default_version (str): fallback version in case of error\n\n    Returns:\n        str: current working version\n\n    \"\"\"", "output": "\n    if path_to_version.exists():\n        return path_to_version.read_text().strip()\n    else:\n        return default_version\n\n\ndef _version_from_url(url, default_version=DEFAULT_VERSION):\n    \"\"\"for PyPI installed versions, just get data from URL\n\n    Args:\n        url (str): url of the package\n        default_version (str): fallback version in case of error\n\n    Returns:\n        str: current working version\n\n    \"\"\"\n    if url.is_url():\n        return url.read_text().strip()\n    else:\n        return default_version\n\n\n", "solution": "\n    version_filepath = os.path.join(path_to_version, 'version.txt')\n    if not os.path.isfile(version_filepath):\n        warnings.warn(\n            'Unable to resolve current version',\n            exceptions.ProsperDefaultVersionWarning)\n        return default_version\n\n    with open(version_filepath, 'r') as v_fh:\n        data = v_fh.read()\n\n    return data"}
{"prompt": "def _get_utm_name_value_pair(zone, direction=_Direction.NORTH):\n    \"\"\" Get name and code for UTM coordinates\n\n    :param zone: UTM zone number\n    :type zone: int\n    :param direction: Direction enum type\n    :type direction: Enum, optional (default=NORTH)\n    :return: Name and code of UTM coordinates\n    :rtype: str, str\n    \"\"\"", "output": "\n    if direction == _Direction.NORTH:\n        return (0, 1)\n    elif direction == _Direction.SOUTH:\n        return (1, 0)\n    else:\n        return (0, None)\n\n\ndef _get_utm_name_pair(zone, direction=_Direction.NORTH):\n    \"\"\" Get name and code for UTM coordinates\n\n    :param zone: UTM zone number\n    :type zone: int\n    :param direction: Direction enum type\n    :type direction: Enum, optional (default=NORTH)\n    :return: Name and code of UTM", "solution": "\n    name = 'UTM_{}{}'.format(zone, direction.value)\n    epsg = _get_utm_code(zone, direction)\n    return name, epsg"}
{"prompt": "def est_via_matching(self, weights='inv', matches=1, bias_adj=False):\n\n\t\t\"\"\"\n\t\tEstimates average treatment effects using nearest-\n\t\tneighborhood matching.\n\n\t\tMatching is done with replacement. Method supports multiple\n\t\tmatching. Correcting bias that arise due to imperfect matches\n\t\tis also supported. For details on methodology, see [1]_.\n\n\t\tParameters\n\t\t----------\n\t\tweights: str or positive definite square matrix\n\t\t\tSpecifies weighting matrix used in computing\n\t\t\tdistance measures. Defaults to string 'inv',\n\t\t\twhich does inverse variance weighting. String\n\t\t\t'maha' gives the weighting matrix used in the\n\t\t\tMahalanobis metric.\n\t\tmatches: int\n\t\t\tNumber of matches to use for each subject.\n\t\tbias_adj: bool\n\t\t\tSpecifies whether bias adjustments should be\n\t\t\tattempted.\n\n\t\tReferences\n\t\t----------\n\t\t.. [1] Imbens, G. & Rubin, D. (2015). Causal Inference in\n\t\t\tStatistics, Social, and Biomedical Sciences: An\n\t\t\tIntroduction.\n\t\t\"\"\"", "output": " means that the distance measures are normalized\n\t\t\tto have 1/inv weighting.\n\t\tbias_adj: bool\n\t\t\tCorrecting bias.\n\n\t\tReturns\n\t\t-------\n\t\tfloat\n\t\t\tThe average treatment effects.\n\t\t\"\"\"\n\n\t\tif isinstance(weights, str):\n\t\t\tweights = self.get_weights(weights)\n\t\tif isinstance(matches, int):\n\t\t\tmatches = self.get_matches(matches)\n\n\t\treturn self.est_average_effect(weights, bias_adj)\n\n\tdef est_average_effect(self, weights='inv', bias_adj", "solution": "\n\n\t\tX, K = self.raw_data['X'], self.raw_data['K']\n\t\tX_c, X_t = self.raw_data['X_c'], self.raw_data['X_t']\n\n\t\tif weights == 'inv':\n\t\t\tW = 1/X.var(0)\n\t\telif weights == 'maha':\n\t\t\tV_c = np.cov(X_c, rowvar=False, ddof=0)\n\t\t\tV_t = np.cov(X_t, rowvar=False, ddof=0)\n\t\t\tif K == 1:\n\t\t\t\tW = 1/np.array([[(V_c+V_t)/2]])  # matrix form\n\t\t\telse:\n\t\t\t\tW = np.linalg.inv((V_c+V_t)/2)\n\t\telse:\n\t\t\tW = weights\n\n\t\tself.estimates['matching'] = Matching(self.raw_data, W,\n\t\t                                      matches, bias_adj)"}
{"prompt": "def draw_tree(t, df, size=10, ratio=0.6, precision=0):\n    \"\"\" Draws a representation of a random forest in IPython.\n    Parameters:\n    -----------\n    t: The tree you wish to draw\n    df: The data used to train the tree. This is used to get the names of the features.\n    \"\"\"", "output": "\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.ensemble import RandomForestRegressor\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.ensemble import RandomForestClassifier\n    from", "solution": "\n    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True,\n                      special_characters=True, rotate=True, precision=precision)\n    IPython.display.display(graphviz.Source(re.sub('Tree {',\n       f'Tree {{ size={size}; ratio={ratio}', s)))"}
{"prompt": "def set(self, key, val, timeout=None, using=None):\n        \"\"\"\n        Set will be using the generational key, so if another thread\n        bumps this key, the localstore version will still be invalid.\n        If the key is bumped during a transaction it will be new\n        to the global cache on commit, so it will still be a bump.\n        \"\"\"", "output": "\n        self.localstore.set(key, val, timeout)\n        if using is not None:\n            self.localstore.set(key, val, timeout)\n\n    def get(self, key, timeout=None, using=None):\n        \"\"\"\n        Get will be using the generational key, so if another thread\n        bumps this key, the localstore version will still be invalid.\n        If the key is bumped during a transaction it will be new\n        to the global cache on commit, so it will still be a bump.\n        \"\"\"\n        return self.localstore.get(key, timeout, with", "solution": "\n        if timeout is None:\n            timeout = self.timeout\n        if self.is_managed(using=using) and self._patched_var:\n            self.local[key] = val\n        else:\n            self.cache_backend.set(key, val, timeout)"}
{"prompt": "def get_authorize_callback(endpoint, provider_id):\n    \"\"\"Get a qualified URL for the provider to return to upon authorization\n\n    param: endpoint: Absolute path to append to the application's host\n    \"\"\"", "output": "\n    return '{0}/oauth/authorize/{1}'.format(\n        provider_url(endpoint),\n        urllib.parse.quote(provider_id, safe=''))\n\n\ndef get_client_id(client_id):\n    \"\"\"Get the client id for the given client id\n\n    param: client_id: The client id\n    \"\"\"\n    return '{0}/oauth/token'.format(client_id)\n\n\ndef get_client_secret(client_secret):\n    \"\"\"Get the client secret for the given client id\n\n    param: client_secret: The client secret\n    \"\"\"\n    return '{0}/oauth/", "solution": "\n    endpoint_prefix = config_value('BLUEPRINT_NAME')\n    url = url_for(endpoint_prefix + '.' + endpoint, provider_id=provider_id)\n    return request.url_root[:-1] + url"}
{"prompt": "def extract_text(input_file, pageno=1):\n    \"\"\"Use the txtwrite device to get text layout information out\n\n    For details on options of -dTextFormat see\n    https://www.ghostscript.com/doc/current/VectorDevices.htm#TXT\n\n    Format is like\n    <page>\n    <line>\n    <span bbox=\"left top right bottom\" font=\"...\" size=\"...\">\n    <char bbox=\"....\" c=\"X\"/>\n\n    :param pageno: number of page to extract, or all pages if None\n    :return: XML-ish text representation in bytes\n    \"\"\"", "output": " XML\n    \"\"\"\n    if input_file is None:\n        return\n\n    if pageno is not None:\n        text_file = open(input_file, 'w')\n        text_file.write('<page>%d</page>\\n' % pageno)\n        text_file.write('<line>%d</line>\\n' % input_file.tell())\n        text_file.write('<span bbox=\"left top right bottom\" font=\"...\" size=\"...\">')\n        text_file.write(str(text_file.read(pageno))\n        text_file.write('</span", "solution": "\n\n    if pageno is not None:\n        pages = ['-dFirstPage=%i' % pageno, '-dLastPage=%i' % pageno]\n    else:\n        pages = []\n\n    args_gs = (\n        [\n            'gs',\n            '-dQUIET',\n            '-dSAFER',\n            '-dBATCH',\n            '-dNOPAUSE',\n            '-sDEVICE=txtwrite',\n            '-dTextFormat=0',\n        ]\n        + pages\n        + ['-o', '-', fspath(input_file)]\n    )\n\n    p = run(args_gs, stdout=PIPE, stderr=PIPE)\n    if p.returncode != 0:\n        raise SubprocessOutputError(\n            'Ghostscript text extraction failed\\n%s\\n%s\\n%s'\n            % (input_file, p.stdout.decode(), p.stderr.decode())\n        )\n\n    return p.stdout"}
{"prompt": "def get_server_premaster_secret(self, password_verifier, server_private, client_public, common_secret):\n        \"\"\"S = (A * v^u) ^ b % N\n\n        :param int password_verifier:\n        :param int server_private:\n        :param int client_public:\n        :param int common_secret:\n        :rtype: int\n        \"\"\"", "output": "\n        return self.get_premaster_secret(password_verifier, server_private, common_secret)\n\n    def get_premaster_secret(self, password_verifier, server_private, client_public, common_secret):\n        \"\"\"\n        Return the premaster secret for the given password verifier.\n\n        :param int password_verifier:\n        :param int server_private:\n        :param int client_public:\n        :param int common_secret:\n        :rtype: int\n        \"\"\"\n        return self.get_server_premaster_secret(password_verifier, server_private, client_public,", "solution": "\n        return pow((client_public * pow(password_verifier, common_secret, self._prime)), server_private, self._prime)"}
{"prompt": "def max_id_length(p_num):\n    \"\"\"\n    Returns the length of the IDs used, given the number of items that are\n    assigned an ID. Used for padding in lists.\n    \"\"\"", "output": "\n    return len(p_num)\n\n\ndef get_unique_values(p_list):\n    \"\"\"\n    Returns a list of unique values in the passed list.\n    \"\"\"\n    unique_values = []\n    for item in p_list:\n        if item not in unique_values:\n            unique_values.append(item)\n    return p_list, unique_values\n\n\ndef get_unique_values_list(p_list):\n    \"\"\"\n    Returns a list of unique values in the passed list.\n    \"\"\"\n    unique_values = []\n    for item in p_list:\n        if item not in", "solution": "\n    try:\n        alphabet = config().identifier_alphabet()\n        length, _ = _get_table_size(alphabet, p_num)\n    except _TableSizeException:\n        length, _ = _get_table_size(_DEFAULT_ALPHABET, p_num)\n\n    return length"}
{"prompt": "def parse_dict(self, attrs):\n        \"\"\"Read a dict to attributes.\"\"\"", "output": "\n        if not attrs:\n            return None\n        d = {}\n        for key, value in attrs.items():\n            d[key] = value\n        return d\n\n    def _get_value(self, attrs, key):\n        \"\"\"Read a value from attributes.\"\"\"\n        if not attrs:\n            return None\n        value = attrs.get(key, '')\n        if not value:\n            return None\n        if isinstance(value, list):\n            value = value[0]\n        return value\n\n    def _get_attr_from_entity(self, entity):\n        \"\"\"Read an attribute from the entity.\"\"\"\n        if not entity:\n            return", "solution": "\n        attrs = attrs or {}\n        ident = attrs.get(\"id\", \"\")\n        classes = attrs.get(\"classes\", [])\n        kvs = OrderedDict((k, v) for k, v in attrs.items()\n                          if k not in (\"classes\", \"id\"))\n\n        return ident, classes, kvs"}
{"prompt": "def get_plugins_info(self):\n        \"\"\"\n        Collect the current live info from all the registered plugins.\n\n        Return a dictionary, keyed on the plugin name.\n\n        \"\"\"", "output": "\n        return dict((plugin.name, plugin.info) for plugin in self.plugins)\n\n    def get_plugin_names(self):\n        \"\"\"\n        Return a list of the currently registered plugins.\n\n        Return a list of the names of all the plugins.\n\n        \"\"\"\n        return [plugin.name for plugin in self.plugins]\n\n    def get_plugin_info(self, name):\n        \"\"\"\n        Return a live instance of the plugin with the given name.\n\n        \"\"\"\n        return self.plugins[0].info\n\n    def get_plugin_classes(self):\n        \"\"\"\n        Return a list of the currently registered classes.", "solution": "\n        d = {}\n        for p in self.plugins:\n            d.update(p.get_info())\n        return d"}
{"prompt": "def upload_file(self, path, contents, replace=False):\n        \"\"\"\n        Uplodas the file to its path with the given `content`, adding the\n        appropriate parent directories when needed. If the path already exists\n        and `replace` is `False`, the file will not be uploaded.\n        \"\"\"", "output": "\n        if os.path.exists(path):\n            if replace:\n                path = path.replace(self.path, self.path + '.bak')\n            else:\n                path = path.replace(self.path + '.bak', self.path)\n            self.log('file already exists: %s' % path)\n        else:\n            self.log('file does not exist: %s' % self.path)\n            if not os.path.exists(self.path):\n                self.log('creating directory %s' % self.path)\n                self.mkdir(self.path)\n            self", "solution": "\n        f = self.get_file(path)\n        f.upload(contents, replace=replace)\n        self.set_cache_buster(path, f.hash())"}
{"prompt": "async def home_z(self, mount: top_types.Mount = None):\n        \"\"\" Home the two z-axes \"\"\"", "output": "\n        await self._device.set_value(self._device.value_id, self._device.value_id, \"z\")\n        await self._device.set_value(self._device.value_id, self._device.value_id, \"z\")\n        await self._device.set_value(self._device.value_id, self._device.value_id, \"z\")\n        await self._device.set_value(self._device.value_id, self._device.value_id, \"z\")\n        await self._device.set_value(self._", "solution": "\n        if not mount:\n            axes = [Axis.Z, Axis.A]\n        else:\n            axes = [Axis.by_mount(mount)]\n        await self.home(axes)"}
{"prompt": "def addfield(self, pkt, s, i):\n        \"\"\"\n        There is a hack with the _ExtensionsField.i2len. It works only because\n        we expect _ExtensionsField.i2m to return a string of the same size (if\n        not of the same value) upon successive calls (e.g. through i2len here,\n        then i2m when directly building the _ExtensionsField).\n\n        XXX A proper way to do this would be to keep the extensions built from\n        the i2len call here, instead of rebuilding them later on.\n        \"\"\"", "output": "\n        if i is None:\n            return s\n        else:\n            return self.i2m(pkt, s, i)\n\n    def i2len(self, pkt, x):\n        \"\"\"\n        Returns the length of the interleaved data, in bits.\n\n        :param pkt: the packet\n        :param x: the interleaved length\n        :return: the length of the interleaved data in bits\n        \"\"\"\n        if x is None:\n            return len(self.i2m(pkt, 0))\n        else:\n            return self.i2m(pkt, x, 1)\n\n    def i2", "solution": "\n        if i is None:\n            if self.length_of is not None:\n                fld, fval = pkt.getfield_and_val(self.length_of)\n\n                tmp = pkt.tls_session.frozen\n                pkt.tls_session.frozen = True\n                f = fld.i2len(pkt, fval)\n                pkt.tls_session.frozen = tmp\n\n                i = self.adjust(pkt, f)\n                if i == 0:  # for correct build if no ext and not explicitly 0\n                    return s\n        return s + struct.pack(self.fmt, i)"}
{"prompt": "def delta(self, other):\n        \"\"\"\n        Return the error between this and another bearing. This will be an\n        angle in degrees, positive or negative depending on the direction of\n        the error.\n        \n            self   other\n              \\      /\n               \\    /\n                \\__/\n                 \\/ <- angle will be +ve\n\n            other   self\n              \\      /\n               \\    /\n                \\__/\n                 \\/ <- angle will be -ve        \n\n        :param other: bearing to compare to\n        :type other: Bearing\n\n        :returns: error angle\n        :rtype: float\n        \"\"\"", "output": "\n        return math.degrees(math.atan2(self.x, other.x))\n\n    def __str__(self):\n        \"\"\"\n        Return a string representation of this bearing. This will be\n        the same as the angle between the bearing and the bearing.\n        \n            \\     /\n               \\    /\n               \\_______\n               \\/ <- angle will be +ve        \n\n        :returns: a string representation of this bearing\n        :rtype: str\n\n        :returns: a string representation of the bearing\n        :rtype: str\n\n        :returns: a string representation of the bearing\n        :rtype: str\n\n        :", "solution": "\n        difference = float(other) - float(self)\n        while difference < -180:\n            difference += 360\n        while difference > 180:\n            difference -= 360\n        return difference"}
{"prompt": "def get_time_estimator(total):\n    \"\"\"Given a total amount of items to compute, return a function that,\n    if called every time an item is computed (or every step items are computed)\n    will give a time estimation for how long it will take to compute the whole\n    set of itmes. The function will return two values: the first is the number\n    of seconds that are still needed to compute the whole set, the second value\n    is the time in the future when the operation is expected to end.\n    \"\"\"", "output": "\n    def time_estimator(self, total):\n        \"\"\"Given a total amount of items to compute, return a function that,\n        if called every time an item is computed (or every step items are computed)\n        will give a time estimation for how long it will take to compute the whole\n        set of itmes. The function will return two values: the first is the number\n        of seconds that are needed to compute the whole set, the second value\n        is the time in the future when the operation is expected to end.\n        \"\"\"\n        if total < 0:\n            return 0\n        if total > 0:\n            return total -", "solution": "\n    t1 = time.time()\n    count = [0]\n\n    def estimate_needed_time(step=1):\n        count[0] += step\n        t2 = time.time()\n        t3 = 1.0 * (t2 - t1) / count[0] * (total - count[0])\n        return t3, t3 + t1\n    return estimate_needed_time"}
{"prompt": "def validate(config):\n    \"\"\"\n    Validate the beacon configuration\n    \"\"\"", "output": "\n    # Check if the beacon is already configured\n    if not config.has_section('beacon'):\n        return False\n\n    # Check if the beacon is configured\n    if not config.has_option('beacon', 'name'):\n        return False\n\n    # Check if the beacon is configured with the correct values\n    if not config.has_option('beacon', 'url'):\n        return False\n\n    # Check if the beacon is configured with the correct values\n    if not config.has_option('beacon', 'username'):\n        return False\n\n    # Check if the beacon is configured with the correct values\n   ", "solution": "\n\n    # Configuration for load beacon should be a list of dicts\n    if not isinstance(config, list):\n        return False, ('Configuration for load beacon must be a list.')\n    else:\n        _config = {}\n        list(map(_config.update, config))\n\n        if 'emitatstartup' in _config:\n            if not isinstance(_config['emitatstartup'], bool):\n                return False, ('Configuration for load beacon option '\n                               'emitatstartup must be a boolean.')\n\n        if 'onchangeonly' in _config:\n            if not isinstance(_config['onchangeonly'], bool):\n                return False, ('Configuration for load beacon option '\n                               'onchangeonly must be a boolean.')\n\n        if 'averages' not in _config:\n            return False, ('Averages configuration is required'\n                           ' for load beacon.')\n        else:\n\n            if not any(j in ['1m', '5m', '15m'] for j\n                       in _config.get('averages', {})):\n                return False, ('Averages configuration for load beacon '\n                               'must contain 1m, 5m or 15m items.')\n\n            for item in ['1m', '5m', '15m']:\n                if not isinstance(_config['averages'][item], list):\n                    return False, ('Averages configuration for load beacon: '\n                                   '1m, 5m and 15m items must be '\n                                   'a list of two items.')\n                else:\n                    if len(_config['averages'][item]) != 2:\n                        return False, ('Configuration for load beacon: '\n                                       '1m, 5m and 15m items must be '\n                                       'a list of two items.')\n\n    return True, 'Valid beacon configuration'"}
{"prompt": "def ensure_utf8(str_or_unicode):\n    \"\"\"\n    tests, if the input is ``str`` or ``unicode``. if it is ``unicode``,\n    it will be encoded from ``unicode`` to ``utf-8``. otherwise, the\n    input string is returned.\n    \"\"\"", "output": "\n    if isinstance(str_or_unicode, unicode):\n        return str_or_unicode\n    else:\n        return unicode(str_or_unicode, \"utf-8\")\n\n\nclass TestUtils(unittest.TestCase):\n    def test_get_logger(self):\n        self.assertEqual(logger, logging.getLogger('py.warnings'))\n        logger.setLevel(logging.DEBUG)\n        self.assertEqual(logger, logging.getLogger('py.warnings'))\n\n    def test_get_logger_console(self):\n        console = logging.StreamHandler()\n        console.setLevel(logging.DEBUG)\n        self.assertEqual", "solution": "\n    if isinstance(str_or_unicode, str):\n        return str_or_unicode\n    elif isinstance(str_or_unicode, unicode):\n        return str_or_unicode.encode('utf-8')\n    else:\n        raise ValueError(\n            \"Input '{0}' should be a string or unicode, but it is of \"\n            \"type {1}\".format(str_or_unicode, type(str_or_unicode)))"}
{"prompt": "def codingthreads(self):\n        \"\"\"\n        Find CDS features in .gff files to filter out non-coding sequences from the analysis\n        \"\"\"", "output": "\n        self.gff_files = []\n        for gff_file in self.gff_files:\n            if not os.path.exists(gff_file):\n                raise ValueError(\"GFF file %s does not exist\" % gff_file)\n            self.gff_files.append(gff_file)\n            if not os.path.exists(gff_file + \".gff\"):\n                raise ValueError(\"GFF file %s does not exist\" % gff_file + \".gff\")\n            if not os.path.exists(gff_file + \".g", "solution": "\n        printtime('Extracting CDS features', self.start)\n        # Create and start threads\n        for i in range(self.cpus):\n            # Send the threads to the appropriate destination function\n            threads = Thread(target=self.codingsequences, args=())\n            # Set the daemon to true - something to do with thread management\n            threads.setDaemon(True)\n            # Start the threading\n            threads.start()\n        for sample in self.runmetadata.samples:\n            self.codingqueue.put(sample)\n        self.codingqueue.join()\n        # Create CDS files and determine gene presence/absence\n        self.corethreads()"}
{"prompt": "def insert_into_shaders(self, vertex, fragment):\n        \"\"\"Apply the insertions to shader code.\"\"\"", "output": "\n        self.code.append(\n            self.shader_code.format(\n                vertex.name,\n                fragment.name,\n                self.shader_code.format(\n                    fragment.name,\n                    vertex.name,\n                    fragment.name,\n                    self.shader_code.format(\n                        fragment.name,\n                        vertex.name,\n                        fragment.name,\n                    ),\n                self.code.indent(2),\n            )\n        )\n\n    def write_header(self):\n        \"\"\"Write the header.\"\"\"\n        self.header_text = self.text_buffer.get_text(\n            self.header_text_buffer", "solution": "\n        to_insert = defaultdict(str)\n        to_insert.update({key: '\\n'.join(self._to_insert[key]) + '\\n'\n                          for key in self._to_insert})\n        return _insert_glsl(vertex, fragment, to_insert)"}
{"prompt": "def _get_stage_id(self, stage):\n        \"\"\"\n        :param stage: A stage ID, name, or index (stage index is the number n for the nth stage, starting from 0; can be provided as an int or a string)\n        :type stage: int or string\n        :returns: The stage ID (this is a no-op if it was already a stage ID)\n        :raises: :class:`~dxpy.exceptions.DXError` if *stage* could not be parsed, resolved to a stage ID, or it could not be found in the workflow\n        \"\"\"", "output": "\n        if isinstance(stage, basestring):\n            return self.get_stage_index(stage)\n        elif isinstance(stage, int):\n            return self.get_stage_id(stage)\n        else:\n            raise DXError(\"Unknown stage type: %s\" % stage)\n\n    def _get_stage_name(self, stage):\n        \"\"\"\n        :param stage: A stage name, name, or index (stage index is the number n for the nth stage, starting from 0; can be provided as an int or a string)\n        :type stage: int or string\n        :returns: The name (", "solution": "\n        # first, if it is a string, see if it is an integer\n        if isinstance(stage, basestring):\n            try:\n                stage = int(stage)\n            except:\n                # we'll try parsing it as a string later\n                pass\n\n        if not isinstance(stage, basestring):\n            # Try to parse as stage index; ensure that if it's not a\n            # string that it is an integer at this point.\n            try:\n                stage_index = int(stage)\n            except:\n                raise DXError('DXWorkflow: the given stage identifier was neither a string stage ID nor an integer index')\n            if stage_index < 0 or stage_index >= len(self.stages):\n                raise DXError('DXWorkflow: the workflow contains ' + str(len(self.stages)) + \\\n                              ' stage(s), and the numerical value of the given stage identifier is out of range')\n            return self.stages[stage_index].get(\"id\")\n\n        if re.compile('^([a-zA-Z_]|stage-)[0-9a-zA-Z_]*$').match(stage) is not None:\n            # Check if there exists a stage with this stage id\n            stage_id_exists = any([stg['id'] for stg in self.stages if stg.get('id') == stage])\n            if stage_id_exists:\n                return stage\n\n        # A stage with the provided ID can't be found in the workflow, so look for it as a name\n        stage_ids_matching_name = [stg['id'] for stg in self.stages if stg.get('name') == stage]\n        if len(stage_ids_matching_name) == 0:\n            raise DXError('DXWorkflow: the given stage identifier ' + stage + ' could not be found as a stage ID nor as a stage name')\n        elif len(stage_ids_matching_name) > 1:\n            raise DXError('DXWorkflow: more than one workflow stage was found to have the name \"' + stage + '\"')\n        else:\n            return stage_ids_matching_name[0]"}
{"prompt": "def activate(self):\r\n        \"\"\"\r\n        Activate the scene.\r\n        \"\"\"", "output": "\r\n        self.scene.active_object.select = True\r\n        self.scene.active_object.hide = True\r\n        self.scene.active_object.select_set(True)\r\n        self.scene.objects.active = self.scene.objects[0]\r\n        self.scene.objects.active.select = True\r\n        self.scene.objects[0].hide = True\r\n        self.scene.objects[0].hide_set(True)\r\n        self.scene.objects.active.hide = False\r\n        self.scene.objects.active.hide_set(False)\r\n        self.scene.objects", "solution": "\r\n        response = self.api_interface.set_device_state(self, None)\r\n        self._update_state_from_response(response)"}
{"prompt": "def get_visual_content(self, id_or_uri):\n        \"\"\"\n        Gets a list of visual content objects describing each rack within the data center. The response aggregates data\n        center and rack data with a specified metric (peak24HourTemp) to provide simplified access to display data for\n        the data center.\n\n        Args:\n            id_or_uri: Can be either the resource ID or the resource URI.\n\n        Return:\n            list: List of visual content objects.\n        \"\"\"", "output": "\n        if isinstance(id_or_uri, ResourceID):\n            return self.get_resource_by_id(id_or_uri)\n        elif isinstance(id_or_uri, str):\n            return self.get_resource_by_uri(id_or_uri)\n        else:\n            raise TypeError(\"get_visual_content() expected an instance of str or ResourceID, got %s\" % type(id_or_uri))\n\n    def get_resource_by_id(self, id):\n        \"\"\"\n        Gets a resource object from the data center.\n\n        Args:\n            id:", "solution": "\n        uri = self._client.build_uri(id_or_uri) + \"/visualContent\"\n        return self._client.get(uri)"}
{"prompt": "def _clone_args(self):\n        \"\"\" return args to create new Dict clone\n        \"\"\"", "output": "\n        return dict(self.args)\n\n    def _get_args(self):\n        \"\"\" return arguments to create new Dict\n        \"\"\"\n        return self.args.copy()\n\n    def _get_value(self, key):\n        \"\"\" return value for a given key\n        \"\"\"\n        return self.args.get(key, None)\n\n    def _get_value_from_path(self, path):\n        \"\"\" return value for a given path\n        \"\"\"\n        if path in self.args:\n            return self.args[path]\n        elif path in self.path_values:\n            return self.path_values[path", "solution": "\n        keys = list(self.keys)\n        kw = {}\n        if self.allow_any or self.extras:\n            kw['allow_extra'] = list(self.extras)\n            if self.allow_any:\n                kw['allow_extra'].append('*')\n            kw['allow_extra_trafaret'] = self.extras_trafaret\n        if self.ignore_any or self.ignore:\n            kw['ignore_extra'] = list(self.ignore)\n            if self.ignore_any:\n                kw['ignore_any'].append('*')\n        return keys, kw"}
{"prompt": "def get_activity_objective_bank_session(self, proxy):\n        \"\"\"Gets the session for retrieving activity to objective bank mappings.\n\n        arg:    proxy (osid.proxy.Proxy): a proxy\n        return: (osid.learning.ActivityObjectiveBankSession) - an\n                ``ActivityObjectiveBankSession``\n        raise:  NullArgument - ``proxy`` is ``null``\n        raise:  OperationFailed - unable to complete request\n        raise:  Unimplemented - ``supports_activity_objective_bank()``\n                is ``false``\n        *compliance: optional -- This method must be implemented if\n        ``supports_activity_objective_bank()`` is ``true``.*\n\n        \"\"\"", "output": "_activity_objective_bank()`` is ``true``.*\n\n        \"\"\"\n        return  # osid.learning.ActivityObjectiveBankSession\n\n    activity_objective_bank_session = property(fget=get_activity_objective_bank_session)\n\n    def get_activity_objective_bank_assignment_session(self):\n        \"\"\"Gets the session for assigning activity to objective banks.\n\n        return: (osid.learning.ActivityObjectiveBankAssignmentSession)\n                an ``ActivityObjectiveBankAssignmentSession``\n        raise:  NullArgument - ``session`` is ``null``\n        raise:  OperationFailed - unable to complete request", "solution": "\n        if not self.supports_activity_objective_bank():\n            raise errors.Unimplemented()\n        # pylint: disable=no-member\n        return sessions.ActivityObjectiveBankSession(proxy=proxy, runtime=self._runtime)"}
{"prompt": "def _build(self, input_modules, middle_modules, head_modules):\n        \"\"\"\n        TBD\n        \"\"\"", "output": "\n        self.modules = []\n        self.middle_modules = []\n        self.head_modules = []\n        for module in input_modules:\n            self.modules.append(module)\n            self.middle_modules.append(module)\n            self.head_modules.append(head_modules[module])\n\n    def _build_module(self, module):\n        \"\"\"\n        TBD\n        \"\"\"\n        self.modules.append(module)\n        self.middle_modules.append(module)\n        self.head_modules.append(module)\n\n    def _build_middle_module(self, module):\n       ", "solution": "\n        self.input_layer = self._build_input_layer(input_modules)\n        self.middle_layers = self._build_middle_layers(middle_modules)\n        self.heads = self._build_task_heads(head_modules)\n\n        # Construct loss module\n        reduction = self.config[\"train_config\"][\"loss_fn_reduction\"]\n        self.criteria = SoftCrossEntropyLoss(reduction=reduction)"}
{"prompt": "def write(self, data):\n        \"\"\"Write data to connection\n\n        Write data as string of bytes.\n\n        Arguments:\n        data -- buffer containing data to be written\n\n        Return value:\n        number of bytes actually transmitted\n        \"\"\"", "output": "\n        self.sock.sendall(data)\n        return self.sock.recv_into(self.buf)\n\n    def writelines(self, lines):\n        \"\"\"Write data to connection\n\n        Write data as list of lines of bytes.\n\n        Arguments:\n        lines -- list of lines to be written\n\n        Return value:\n        number of lines actually transmitted\n        \"\"\"\n        for line in lines:\n            self.write(line)\n        return self.write\n\n    def close(self):\n        \"\"\"Close connection\n\n        Close the connection to the MySQL server.\n        \"\"\"\n        if self.sock:\n            self.sock.close()\n            self.sock", "solution": "\n\n        try:\n            ret = self._wrap_socket_library_call(\n                lambda: SSL_write(self._ssl.value, data), ERR_WRITE_TIMEOUT)\n        except openssl_error() as err:\n            if err.ssl_error == SSL_ERROR_SYSCALL and err.result == -1:\n                raise_ssl_error(ERR_PORT_UNREACHABLE, err)\n            raise\n        if ret:\n            self._handshake_done = True\n        return ret"}
{"prompt": "def get_ancestors(self):\n        \"\"\"\n        :returns: A *list* containing the current node object's ancestors,\n            starting by the root node and descending to the parent.\n        \"\"\"", "output": "\n        return self.get_ancestors_from_path(self.path)\n\n    def get_ancestors_from_path(self, path):\n        \"\"\"\n        :returns: A list of node objects that are ancestors of `path`.\n        \"\"\"\n        return [self.get_node(p) for p in self.get_ancestors() if path == self.path]\n\n    def get_ancestors_from_path_and_parents(self, path, parents):\n        \"\"\"\n        :returns: A list of node objects that are ancestors of `path` and `parents`.\n            The first node in the list is the", "solution": "\n        ancestors = []\n        if self._meta.proxy_for_model:\n            # the current node is a proxy model; our result set\n            # should use the same proxy model, so we need to\n            # explicitly fetch instances of that model\n            # when following the 'parent' relation\n            cls = self.__class__\n            node = self\n            while node.parent_id:\n                node = cls.objects.get(pk=node.parent_id)\n                ancestors.insert(0, node)\n        else:\n            node = self.parent\n            while node:\n                ancestors.insert(0, node)\n                node = node.parent\n        return ancestors"}
{"prompt": "def process_request(self, request):\n        \"\"\"\n        Adds a ``subdomain`` attribute to the ``request`` parameter.\n        \"\"\"", "output": "\n        self.request.subdomain = self.subdomain\n        self.request.subdomain_specified = self.subdomain_specified\n        return self.add_subdomain_cookie(request)\n\n    def add_subdomain_cookie(self, request):\n        \"\"\"\n        Adds a ``subdomain`` attribute to the ``request`` parameter.\n        \"\"\"\n        self.add_cookie_header(request)\n        self.request.set_cookie(\n            settings.SESSION_COOKIE_NAME,\n            request.session.session_key,\n            max_age=settings.SESSION_COOKIE_AGE,\n            expires=settings.SESSION_COOKIE_AGE,", "solution": "\n        domain, host = map(lower,\n            (self.get_domain_for_request(request), request.get_host()))\n\n        pattern = r'^(?:(?P<subdomain>.*?)\\.)?%s(?::.*)?$' % re.escape(domain)\n        matches = re.match(pattern, host)\n\n        if matches:\n            request.subdomain = matches.group('subdomain')\n        else:\n            request.subdomain = None\n            logger.warning('The host %s does not belong to the domain %s, '\n                'unable to identify the subdomain for this request',\n                request.get_host(), domain)"}
{"prompt": "def set_review_whether_correct(self,\n                                   during_attempt=None,\n                                   after_attempt=None,\n                                   before_deadline=None,\n                                   after_deadline=None):\n        \"\"\"stub\"\"\"", "output": "\n        if during_attempt is not None:\n            self.assertTrue(isinstance(during_attempt, bool))\n        if after_attempt is not None:\n            self.assertTrue(isinstance(after_attempt, bool))\n        if before_deadline is not None:\n            self.assertTrue(isinstance(before_deadline, datetime))\n        if after_deadline is not None:\n            self.assertTrue(isinstance(after_deadline, datetime))\n\n    def test_set_review_whether_correct(self):\n        \"\"\"Test if review is correct\"\"\"\n        self.set_review_whether_correct(self.during_attempt", "solution": "\n        whether_correct = self.my_osid_object_form._my_map['reviewOptions']['whetherCorrect']\n        if during_attempt is not None:\n            whether_correct['duringAttempt'] = bool(during_attempt)\n        if after_attempt is not None:\n            whether_correct['afterAttempt'] = bool(after_attempt)\n        if before_deadline is not None:\n            whether_correct['beforeDeadline'] = bool(before_deadline)\n        if after_deadline is not None:\n            whether_correct['afterDeadline'] = bool(after_deadline)"}
{"prompt": "def toversa(htmlsource, model, source_uri):\n    \"\"\"\n    >>> import urllib\n    >>> from versa.reader import rdfalite\n    >>> from versa.driver import memory\n    >>> m = memory.connection()\n    >>> burl = 'http://link.delawarelibrary.org/'\n    >>> with urllib.request.urlopen(burl) as resourcefp: rdfalite.toversa(resourcefp.read(), m, burl)\n\n    \"\"\"", "output": "\n    return _toversa(htmlsource, model, source_uri)\n\n\ndef _toversa(htmlsource, modeluri, source_uri=None):\n    \"\"\"\n    >>> import urllib\n    >>> from versa.reader import toversa\n    >>> from versa.driver import memory\n    >>> burl = 'http://link.delawarelibrary.org/'\n    >>> with urllib.request.urlopen(burl) as resourcefp:\n    >>>     burl = 'http://link.delawarelibrary.org/'\n    >>>     with urllib.request.urlopen(burl)", "solution": "\n    sink = versalinks(model)\n    next(sink) #Prime the coroutine\n    return parse(htmlsource, sink, source_uri)"}
{"prompt": "def add_filehandler(level, fmt, filename, mode, backup_count, limit, when):\n    \"\"\"Add a file handler to the global logger.\"\"\"", "output": "\n    global logger\n    logger = logging.getLogger(level)\n    logger.addHandler(logging.FileHandler(filename, mode, delay=True))\n    return\n\n\ndef add_stderr_logger(level, fmt, stderr, when):\n    \"\"\"Add a stderr logger to the global logger.\"\"\"\n    global logger\n    logger = logging.getLogger(level)\n    logger.addHandler(logging.StreamHandler(stderr))\n    return\n\n\ndef add_file_logger(filename, fmt, level, when):\n    \"\"\"Add a file logger to the global logger.\"\"\"\n    global logger\n    logger = logging.getLogger(filename)\n    logger.addHandler", "solution": "\n    kwargs = {}\n \n    # If the filename is not set, use the default filename\n    if filename is None:\n        filename = getattr(sys.modules['__main__'], '__file__', 'log.py')\n        filename = os.path.basename(filename.replace('.py', '.log'))\n        filename = os.path.join('/tmp', filename)\n\n    if not os.path.exists(os.path.dirname(filename)):\n        os.mkdir(os.path.dirname(filename))\n\n    kwargs['filename'] = filename\n \n    # Choose the filehandler based on the passed arguments\n    if backup_count == 0:  # Use FileHandler\n        cls = logging.FileHandler\n        kwargs['mode'] = mode\n    elif when is None:  # Use RotatingFileHandler\n        cls = logging.handlers.RotatingFileHandler\n        kwargs['maxBytes'] = limit\n        kwargs['backupCount'] = backup_count\n        kwargs['mode'] = mode\n    else:  # Use TimedRotatingFileHandler\n        cls = logging.handlers.TimedRotatingFileHandler\n        kwargs['when'] = when\n        kwargs['interval'] = limit\n        kwargs['backupCount'] = backup_count\n \n    return add_handler(cls, level, fmt, False, **kwargs)"}
{"prompt": "def integer_based_slice(self, ts):\n        \"\"\"\n        Transform a :class:`TimeSlice` into integer indices that numpy can work\n        with\n\n        Args:\n            ts (slice, TimeSlice): the time slice to translate into integer\n                indices\n        \"\"\"", "output": "\n        return ts.start, ts.stop\n\n    def slice_by_time(self, ts):\n        \"\"\"\n        Return the index of the time slice that the time slice is\n        mapped to\n\n        Args:\n            ts (TimeSlice): the time slice to translate into integer indices\n        \"\"\"\n        return self.integer_based_slice(ts.start)\n\n    def slice_by_time_and_offset(self, ts, offset):\n        \"\"\"\n        Transform a :class:`TimeSlice` into integer indices that numpy can work\n        with\n\n        Args:\n            ts (slice, TimeSlice): the time slice to translate into integer\n               ", "solution": "\n\n        if isinstance(ts, slice):\n            try:\n                start = Seconds(0) if ts.start is None else ts.start\n                if start < Seconds(0):\n                    start = self.end + start\n                stop = self.end if ts.stop is None else ts.stop\n                if stop < Seconds(0):\n                    stop = self.end + stop\n                duration = stop - start\n                ts = TimeSlice(start=start, duration=duration)\n            except (ValueError, TypeError):\n                pass\n\n        if not isinstance(ts, TimeSlice):\n            return ts\n\n        diff = self.duration - self.frequency\n        start_index = \\\n            max(0, np.floor((ts.start - diff) / self.frequency))\n        end = self.end if ts.duration is None else ts.end\n\n\n        # KLUDGE: This is basically arbitrary, but the motivation is that we'd\n        # like to differentiate between cases where the slice\n        # actually/intentionally overlaps a particular sample, and cases where\n        # the slice overlaps the sample by a tiny amount, due to rounding or\n        # lack of precision (e.g. Seconds(1) / SR44100().frequency).\n        ratio = np.round(end / self.frequency, 2)\n\n\n        stop_index = np.ceil(ratio)\n        return slice(int(start_index), int(stop_index))"}
{"prompt": "def populate_obj(self, obj, name):\n        \"\"\"Store file.\"\"\"", "output": "\n        if not self.obj_type:\n            self.obj_type = 'file'\n        self.obj_type_id = obj.id\n        self.obj_type_name = name\n        self.obj_type_description = obj.description\n        self.obj_type_type = obj.type_id\n        self.obj_type_size = obj.size\n        self.obj_type_md5 = obj.md5\n        self.obj_type_sha1 = obj.sha1\n        self.obj_type_sha256 = obj.sha256\n        self.obj_type_", "solution": "\n        from abilian.core.models.blob import Blob\n\n        delete_value = self.allow_delete and self.delete_files_index\n\n        if not self.has_file() and not delete_value:\n            # nothing uploaded, and nothing to delete\n            return\n\n        state = sa.inspect(obj)\n        mapper = state.mapper\n        if name not in mapper.relationships:\n            # directly store in database\n            return super().populate_obj(obj, name)\n\n        rel = getattr(mapper.relationships, name)\n        if rel.uselist:\n            raise ValueError(\"Only single target supported; else use ModelFieldList\")\n\n        if delete_value:\n            setattr(obj, name, None)\n            return\n\n        #  FIXME: propose option to always create a new blob\n        cls = rel.mapper.class_\n        val = getattr(obj, name)\n\n        if val is None:\n            val = cls()\n            setattr(obj, name, val)\n\n        data = \"\"\n        if self.has_file():\n            data = self.data\n            if not issubclass(cls, Blob):\n                data = data.read()\n\n        setattr(val, self.blob_attr, data)"}
{"prompt": "def list_l3_agent_hosting_routers(self, router, **_params):\n        \"\"\"Fetches a list of L3 agents hosting a router.\"\"\"", "output": "\n        return self.get(self.l3_agents_path % (router),\n                        params=_params)\n\n    def list_routers_on_l3_agent(self, agent, **_params):\n        \"\"\"Fetches a list of routers on a particular agent.\"\"\"\n        return self.get(self.l3_agent_path % (agent),\n                        params=_params)\n\n    def update_router(self, router_id, **_params):\n        \"\"\"Updates a router.\"\"\"\n        return self.put(self.l3_agents_path % (router_id),\n                        body={'router': update_dict", "solution": "\n        return self.get((self.router_path + self.L3_AGENTS) % router,\n                        params=_params)"}
{"prompt": "def build_from_table(self):\n        \"\"\"\n        Generates the sql for the FROM portion of the query\n\n        :return: the FROM portion of the query\n        :rtype: str\n        \"\"\"", "output": "\n        return \"FROM \" + self.table_name + \" WHERE \" + self.where_clause + \" ORDER BY \" + self.order_by\n\n    def build_where_clause(self, where_clause):\n        \"\"\"\n        Generates the sql for the WHERE portion of the query\n\n        :param where_clause: the WHERE portion of the query\n        :type where_clause: str\n        :return: the WHERE portion of the query\n        :rtype: str\n        \"\"\"\n        return where_clause\n\n    def build_order_by(self, order_by):\n        \"\"\"\n        Generates the sql for the ORDER BY portion of", "solution": "\n        table_parts = []\n\n        # get the table sql for each table\n        for table in self.tables:\n            sql = table.get_sql()\n            if len(sql):\n                table_parts.append(sql)\n\n        # combine all table sql separated by a comma\n        sql = 'FROM {0} '.format(', '.join(table_parts))\n\n        return sql"}
{"prompt": "def publish_scene_velocity(self, scene_id, velocity):\n        \"\"\"publish a changed scene velovity\"\"\"", "output": "\n        self.send_message(\n            \"publish_scene_velocity\",\n            {\"scene_id\": scene_id, \"velocity\": velocity},\n            room=self.room\n        )\n\n    def set_scene_velocity(self, scene_id, velocity):\n        \"\"\"set a changed scene velocity\"\"\"\n        self.send_message(\n            \"set_scene_velocity\",\n            {\"scene_id\": scene_id, \"velocity\": velocity},\n            room=self.room\n        )\n\n    def get_scene_velocity(self, scene_id):\n        \"\"\"get a changed scene velocity\"\"\"\n        self.send_message", "solution": "\n        self.sequence_number += 1\n        self.publisher.send_multipart(msgs.MessageBuilder.scene_velocity(self.sequence_number, scene_id, velocity))\n        return self.sequence_number"}
{"prompt": "def gen_ascii_docs(src='fastai'):\n    \"\"\"Generate documentation for fastai library in HTML (asciidoctor required)\n    :param str src: The absolute/relative path of source file/dir\n    \"\"\"", "output": "\n    if src == 'fastai':\n        return \"fastai.html\"\n    else:\n        return \"html.html\"\n\n\ndef gen_html_docs(src='html'):\n    \"\"\"Generate documentation for html files in HTML (asciidoctor required)\n    :param str src: The absolute/relative path of source file/dir\n    \"\"\"\n    if src == 'fastai':\n        return \"fastai.html\"\n    else:\n        return \"html.html\"\n\n\ndef gen_html_docs_with_index(src='fastai'):\n    \"\"\"Generate documentation for html files with index.html", "solution": "\n    os.chdir(Path(__file__).absolute().parent)\n    with working_directory('..'):\n        path = Path(src)\n        if path.is_dir():\n            file_paths = list(path.glob('**/*.py'))\n        else:\n            file_paths = [path]\n\n    pat = re.compile('^(?!__init__).*.py\\Z')\n    for file_path in file_paths:\n        if pat.match(file_path.name):\n            file_path.parent.mkdir(parents=True, exist_ok=True)\n            with working_directory('..'):\n                tmpl_str = parse_module(file_path)\n\n            (file_path.parent/(file_path.name.rsplit('.',1)[0] + '.adoc.tmpl')).write_text(tmpl_str)\n            (file_path.parent/(file_path.name.rsplit('.',1)[0] + '.adoc')).write_text(re.sub(r\"{{(.*?)}}\", parse_tmpl, tmpl_str, flags=re.DOTALL))\n    if path.is_dir():\n        subprocess.call(['asciidoctor', str(path) + '/**/*.adoc'])\n    else:\n        subprocess.call(['asciidoctor', str(path).rsplit('.',1)[0] + '.adoc'])"}
{"prompt": "def set_mode_rtl(self):\n        \"\"\"enter RTL mode\"\"\"", "output": "\n        self.set_rc_mode(self.rc_mode)\n        self.set_rc_mode(self.rc_mode)\n\n    def set_mode_tone(self):\n        \"\"\"enter Tone mode\"\"\"\n        self.set_rc_mode(self.rc_mode)\n        self.set_rc_mode(self.rc_mode)\n        self.set_rc_mode(self.rc_mode)\n        self.set_rc_mode(self.rc_mode)\n        self.set_rc_mode(self.rc_mode)\n        self.set_", "solution": "\n        if self.mavlink10():\n            self.mav.command_long_send(self.target_system, self.target_component,\n                                       mavlink.MAV_CMD_NAV_RETURN_TO_LAUNCH, 0, 0, 0, 0, 0, 0, 0, 0)\n        else:\n            MAV_ACTION_RETURN = 3\n            self.mav.action_send(self.target_system, self.target_component, MAV_ACTION_RETURN)"}
{"prompt": "def validate_arguments(self, view_class, kwargs):\n        \"\"\"\n        view_class : View Class used to render content popup dialog\n        view_class must be subclass of django.views.generic.View\n        \"\"\"", "output": "\n        pass\n\n    def get_context_data(self, **kwargs):\n        \"\"\"\n        kwargs :\n            - 'form' : form class used to create the popup\n        \"\"\"\n        context = super(PopupView, self).get_context_data(**kwargs)\n        context['popup_form'] = self.popup_form\n        context['popup_title'] = self.popup_title\n        context['popup_url'] = self.popup_url\n        context['popup_id'] = self.popup_id\n        context['popup_class'] = self.popup_class\n        return context\n\n    def get_context_data", "solution": "\n\n        # Check view_class inherit from django View\n        if not issubclass(view_class, View):\n            raise PopupViewIsNotSubclassView()\n\n        self.view_class_name = view_class.__name__\n        self.popup_dialog_title = kwargs.pop(\"popup_dialog_title\", _(\"Popup Dialog: Select value\"))\n\n        self.callback_data = kwargs.pop(\"callback_data\", {})\n        if not isinstance(self.callback_data, dict):\n            raise AttributeError(\"callback_data argument must be a dictionary\")\n        try:\n            self.callback_data = urllib.urlencode(self.callback_data)\n        except AttributeError:\n            self.callback_data = urllib.parse.urlencode(self.callback_data)"}
{"prompt": "def generate_paillier_keypair(private_keyring=None, n_length=DEFAULT_KEYSIZE):\n    \"\"\"Return a new :class:`PaillierPublicKey` and :class:`PaillierPrivateKey`.\n\n    Add the private key to *private_keyring* if given.\n\n    Args:\n      private_keyring (PaillierPrivateKeyring): a\n        :class:`PaillierPrivateKeyring` on which to store the private\n        key.\n      n_length: key size in bits.\n\n    Returns:\n      tuple: The generated :class:`PaillierPublicKey` and\n      :class:`PaillierPrivateKey`\n    \"\"\"", "output": "PrivateKeyring` as a tuple.\n    \"\"\"\n    if private_keyring is None:\n      private_keyring = self.default_keyring\n    return PaillierPublicKey(private_keyring, n_length)\n\n  def _Generate(self, key_type, key_length, key_name, key_value, private_key_type,\n              public_key_type, private_key_length, private_key_name, key_format):\n    \"\"\"Generate a new :class:`PaillierPublicKey` and :class:`PaillierPrivateKey`.\n\n    Args:\n      key_type (int):\n        :", "solution": "\n    p = q = n = None\n    n_len = 0\n    while n_len != n_length:\n        p = getprimeover(n_length // 2)\n        q = p\n        while q == p:\n            q = getprimeover(n_length // 2)\n        n = p * q\n        n_len = n.bit_length()\n\n    public_key = PaillierPublicKey(n)\n    private_key = PaillierPrivateKey(public_key, p, q)\n\n    if private_keyring is not None:\n        private_keyring.add(private_key)\n\n    return public_key, private_key"}
{"prompt": "def get_dvcs_info():\n    \"\"\"Gets current repository info from git\"\"\"", "output": "\n    try:\n        return subprocess.check_output(['git', 'log', '--format=%H', '--quiet', '--pretty=format:%s%n%b', '--date=short' % GIT_LOG_FORMAT], shell=True)\n    except subprocess.CalledProcessError:\n        return None\n\n\ndef get_repo_name():\n    \"\"\"Gets the name of the current repository\"\"\"\n    return subprocess.check_output(['git', 'config', '--get','remote.origin.url'], shell=True).strip()\n\n\ndef get_repo_path():\n    \"\"\"Gets the path of the current repository\"\"\"", "solution": "\n    cmd = \"git rev-list --count HEAD\"\n    commit_count = str(\n        int(subprocess.check_output(shlex.split(cmd)).decode(\"utf8\").strip())\n    )\n    cmd = \"git rev-parse HEAD\"\n    commit = str(subprocess.check_output(shlex.split(cmd)).decode(\"utf8\").strip())\n    return {Constants.COMMIT_FIELD: commit, Constants.COMMIT_COUNT_FIELD: commit_count}"}
{"prompt": "def randomize(self):\n        \"\"\"Create a new motif with shuffled positions.\n\n        Shuffle the positions of this motif and return a new Motif instance.\n\n        Returns\n        -------\n        m : Motif instance\n            Motif instance with shuffled positions.\n        \"\"\"", "output": "\n        if self.shuffle:\n            return self.shuffle_motif(motif=self.shuffle_motif)\n        else:\n            return Motif(self.motif_list, self.pos_list, self.pos_list, motif_type=self.motif_type, pos_list=self.pos_list, pos_list_name=self.pos_list_name, pos_list_type=self.pos_list_type, pos_list_type=self.pos_list_type, pos_list_name=self.pos_list_", "solution": "\n        random_pfm = [[c for c in row] for row in self.pfm]\n        random.shuffle(random_pfm)\n        m = Motif(pfm=random_pfm)\n        m.id = \"random\"\n        return m"}
{"prompt": "def hasReaders(self, ulBuffer):\n        \"\"\"inexpensively checks for readers to allow writers to fast-fail potentially expensive copies and writes.\"\"\"", "output": "\n        return self.hasReaders(ulBuffer) and self.reader.hasFailed()\n\n    def hasReaders(self, ulBuffer):\n        \"\"\"inexpensively checks for readers to allow writers to slow-fail copies and writes.\"\"\"\n        return self.hasReaders(ulBuffer)\n\n    def getLog(self):\n        \"\"\"returns the log of the last executed statement\"\"\"\n        return self.log\n\n    def getLogName(self):\n        \"\"\"returns the name of the log\"\"\"\n        return self.logName\n\n    def getLogStart(self):\n        \"\"\"returns the start time of the log\"\"\"\n       ", "solution": "\n\n        fn = self.function_table.hasReaders\n        result = fn(ulBuffer)\n        return result"}
{"prompt": "def blank_object(obj: T, fieldlist: Sequence[str]) -> None:\n    \"\"\"Within \"obj\", sets all fields in the fieldlist to None.\"\"\"", "output": "\n    if not obj.is_hidden:\n        fieldlist = fieldlist + (None,) * len(fieldlist)\n\n\ndef get_field_type(field: str) -> str:\n    \"\"\"Get the type of the field.\"\"\"\n    if field.startswith(\"int\"):\n        return \"int\"\n    elif field.startswith(\"float\"):\n        return \"float\"\n    elif field.startswith(\"bool\"):\n        return \"bool\"\n    elif field.startswith(\"enum\"):\n        return \"enum\"\n    elif field.startswith(\"enum_list\"):\n        return \"enum_list\"\n    elif field.startswith(\"enum_dict", "solution": "\n    for f in fieldlist:\n        setattr(obj, f, None)"}
{"prompt": "def iter_ancestors(self):\n        \"\"\" \n        Iterates over the list of all ancestor nodes from \n        current node to the current tree root.\n        \"\"\"", "output": "\n        if self.is_root():\n            yield self\n        else:\n            for child in self.children:\n                for ancestor in iter_ancestors(child):\n                    yield ancestor\n\n    def iter_descendants(self):\n        \"\"\" \n        Iterates over the list of all descendant nodes from \n        current node to the current tree root.\n        \"\"\"\n        if self.is_root():\n            yield self\n        else:\n            for child in self.children:\n                for descendant in iter_descendants(descendant):\n                    yield descendant\n\n    def is_descendant_of(self, node):\n        \"\"\" \n        Returns True if the node is a descendant of the current", "solution": "\n        node = self\n        while node.up is not None:\n            yield node.up\n            node = node.up"}
{"prompt": "def random_letters(self, length=16):\n        \"\"\"Returns a random letter (between a-z and A-Z).\"\"\"", "output": "\n        return ''.join(random.choice(self.letters) for _ in range(length))\n\n    def random_letters_and_digits(self, length=16):\n        \"\"\"Returns a random letter (between A-Z and A-Z and digits).\"\"\"\n        return ''.join(random.choice(self.letters + self.digits) for _ in range(length))\n\n    def test_get_random_string(self):\n        \"\"\"Test getting a random string.\"\"\"\n        random_string = self.random_letters_and_digits()\n        self.assertEqual(len(random_string), length)\n        self", "solution": "\n        return self.random_choices(\n            getattr(string, 'letters', string.ascii_letters),\n            length=length,\n        )"}
{"prompt": "def update_status(self, id_number, new_value):\n        \"\"\"\n        Update a status name\n\n        :type id_number: int\n        :param id_number: status ID number\n\n        :type new_value: str\n        :param new_value: The new status name\n\n        :rtype: dict\n        :return: an empty dictionary\n        \"\"\"", "output": "\n        return self.update_status_with_defaults(id_number, new_value)\n\n    def update_status_with_defaults(self, id_number, defaults):\n        \"\"\"\n        Update a status with default values\n\n        :type id_number: int\n        :param id_number: status ID number\n\n        :type defaults: dict\n        :param defaults: The default status values\n\n        :type id_number: int\n        :param id_number: status ID number\n\n        :type defaults: dict\n        :param id_number: status default values\n\n        :rtype: dict\n        :return: an empty dictionary\n        \"\"\"", "solution": "\n        data = {\n            'id': id_number,\n            'new_value': new_value\n        }\n\n        return self.post('updateStatus', data)"}
{"prompt": "def save_index(self, filename):\n        \"\"\" Save the current Layout's index to a .json file.\n\n        Args:\n            filename (str): Filename to write to.\n\n        Note: At the moment, this won't serialize directory-specific config\n        files. This means reconstructed indexes will only work properly in\n        cases where there aren't multiple layout specs within a project.\n        \"\"\"", "output": "\n        self.index_json_filename = filename\n        self.index_json_filename_json = filename + '.json'\n        self.index_json_filename_json_path = os.path.join(\n            self.index_json_dir, self.index_json_filename_json)\n        self.index_json_filename_json_path_json = os.path.join(\n            self.index_json_dir, self.index_json_filename_json_json)\n        self.index_json_filename_json_path_json_json = os.path.join", "solution": "\n        data = {}\n        for f in self.files.values():\n            entities = {v.entity.id: v.value for k, v in f.tags.items()}\n            data[f.path] = {'domains': f.domains, 'entities': entities}\n        with open(filename, 'w') as outfile:\n            json.dump(data, outfile)"}
{"prompt": "def get_dep(self, name: str) -> str:\n        \"\"\"\n        Return the uuid of the dependency identified with \"name\".\n\n        :param name:\n        :return: UUID\n        \"\"\"", "output": "\n        return self._get_dep_uuid(name)\n\n    def _get_dep_uuid(self, name: str) -> str:\n        \"\"\"\n        Return the uuid of the dependency identified with \"name\".\n\n        :param name:\n        :return: UUID\n        \"\"\"\n        return self.get_dep_uuid_from_name(name)\n\n    def get_dep_uuid_from_name(self, name: str) -> str:\n        \"\"\"\n        Return the uuid of the dependency identified with \"name\".\n\n        :param name:\n        :return: UUID\n        \"\"\"\n        return self._get_dep_name", "solution": "\n        deps = self.meta[\"dependencies\"]\n        for d in deps:\n            if d[\"model\"] == name:\n                return d\n        raise KeyError(\"%s not found in %s.\" % (name, deps))"}
{"prompt": "def isempty(path):\n    \"\"\"Check if a path has zero length (also true if it's a directory)\n\n    :param path: A string for the path.  This should not have any wildcards.\n    :returns: True if the path has zero length, False otherwise.\n    \"\"\"", "output": "\n    if path == '':\n        return True\n    if path[0] == '/':\n        return False\n    return path.startswith(os.path.sep)\n\ndef _get_default_directory():\n    \"\"\"Get the default directory for the current platform.\n\n    :returns: The default directory for the current platform.\n    \"\"\"\n    if sys.platform == 'darwin':\n        return os.path.expanduser('~/Library/Caches/MacOSX')\n    elif sys.platform == 'win32':\n        return os.path.expanduser('~/Library/Caches/Windows')\n    elif sys.platform == 'cygwin", "solution": "\n    cmd = \"hadoop fs -test -z %s\"\n    p = _hadoop_fs_command(cmd % (path))\n    p.communicate()\n    rcode = p.returncode\n    return bool(int(rcode == 0))"}
{"prompt": "def findwithin(data):\n    \"\"\"\nReturns an integer representing a binary vector, where 1=within-\nsubject factor, 0=between.  Input equals the entire data 2D list (i.e.,\ncolumn 0=random factor, column -1=measured values (those two are skipped).\nNote: input data is in |Stat format ... a list of lists (\"2D list\") with\none row per measured value, first column=subject identifier, last column=\nscore, one in-between column per factor (these columns contain level\ndesignations on each factor).  See also stats.anova.__doc__.\n\nUsage:   lfindwithin(data)     data in |Stat format\n\"\"\"", "output": ".fisher_exact\n\n\"\"\"\n        return self.stat_data[data[0][0]][0][0]\n\n    def findwithin2(self,data):\n        \"\"\"\nReturns an integer representing a 2D list, where 1=within-\nsubject factor, 0=between.  Input equals the entire data column\n(i.e., column 0=random factor, column -1=measured values (those two are skipped).\nNote: input data is in |Stat format... a list of lists (\"2D list\") with\none row per measured value, first column=subject identifier,", "solution": "\n\n    numfact = len(data[0]) - 1\n    withinvec = 0\n    for col in range(1, numfact):\n        examplelevel = pstat.unique(pstat.colex(data, col))[0]\n        rows = pstat.linexand(data, col, examplelevel)  # get 1 level of this factor\n        factsubjs = pstat.unique(pstat.colex(rows, 0))\n        allsubjs = pstat.unique(pstat.colex(data, 0))\n        if len(factsubjs) == len(allsubjs):  # fewer Ss than scores on this factor?\n            withinvec = withinvec + (1 << col)\n    return withinvec"}
{"prompt": "def process_response(self, request, response):\n        \"\"\"\n        Adds WWW-Authenticate: Basic headers to 401 responses, and rewrites\n        redirects the login page to be 401 responses if it's a non-browser\n        agent.\n        \"\"\"", "output": "\n        if not self.is_browser_agent:\n            return response\n\n        # If the user is already logged in, we're good\n        if request.user.is_authenticated():\n            return response\n\n        # If we're not using the same browser, we need to use the same\n        # browser for all requests\n        if not self.is_browser_agent:\n            return response\n\n        # If we're using the same agent, we need to use the same request\n        # for all requests\n        if not self.is_browser_agent:\n            return response\n\n        # If we're using the same request, we need to use the", "solution": "\n        process = False\n\n        # Don't do anything for unsecure requests, unless DEBUG is on\n        if not self.allow_http and not request.is_secure():\n            return response\n\n        if response.status_code == UNAUTHORIZED:\n            pass\n        elif response.status_code == FOUND:\n            location = urllib_parse.urlparse(response['Location'])\n            if location.path != settings.LOGIN_URL:\n                # If it wasn't a redirect to the login page, we don't touch it.\n                return response\n            elif not self.is_agent_a_robot(request):\n                # We don't touch requests made in order to be shown to humans.\n                return response\n\n        realm = getattr(settings, 'BASIC_AUTH_REALM', request.META.get('HTTP_HOST', 'restricted'))\n        \n        if response.status_code == FOUND:\n            response = self.unauthorized_view(request)\n\n        authenticate = response.get('WWW-Authenticate', None)\n        if authenticate:\n            authenticate = 'Basic realm=\"%s\", %s' % (realm, authenticate)\n        else:\n            authenticate = 'Basic realm=\"%s\"' % realm\n        response['WWW-Authenticate'] = authenticate\n\n        return response"}
{"prompt": "def indent_selection(self, cursor):\n        \"\"\"\n        Indent selected text\n\n        :param cursor: QTextCursor\n        \"\"\"", "output": "\n        cursor.beginEditBlock()\n        cursor.insertText(self.selected_text)\n        cursor.endEditBlock()\n        self.setTextCursor(cursor)\n\n    def indent_to_end(self):\n        \"\"\"\n        Indent selected text\n\n        :return: QTextBlock\n        \"\"\"\n        cursor = self.textCursor()\n        cursor.beginEditBlock()\n        cursor.insertText(cursor.selectedText())\n        cursor.endEditBlock()\n        self.setTextCursor(cursor)\n\n    def set_selected_text(self, text):\n        \"\"\"\n        Set selected text\n\n        :param text: str\n        \"\"\"\n       ", "solution": "\n        doc = self.editor.document()\n        tab_len = self.editor.tab_length\n        cursor.beginEditBlock()\n        nb_lines = len(cursor.selection().toPlainText().splitlines())\n        c = self.editor.textCursor()\n        if c.atBlockStart() and c.position() == c.selectionEnd():\n            nb_lines += 1\n        block = doc.findBlock(cursor.selectionStart())\n        i = 0\n        # indent every lines\n        while i < nb_lines:\n            nb_space_to_add = tab_len\n            cursor = QtGui.QTextCursor(block)\n            cursor.movePosition(cursor.StartOfLine, cursor.MoveAnchor)\n            if self.editor.use_spaces_instead_of_tabs:\n                for _ in range(nb_space_to_add):\n                    cursor.insertText(\" \")\n            else:\n                cursor.insertText('\\t')\n            block = block.next()\n            i += 1\n        cursor.endEditBlock()"}
{"prompt": "def match_envs(declared_envs, desired_envs, passthru):\n    \"\"\"Determine the envs that match the desired_envs.\n\n    If ``passthru` is True, and none of the declared envs match the\n    desired envs, then the desired envs will be used verbatim.\n\n    :param declared_envs: The envs that are declared in the tox config.\n    :param desired_envs: The envs desired from the tox-travis config.\n    :param bool passthru: Whether to used the ``desired_envs`` as a\n                          fallback if no declared envs match.\n    \"\"\"", "output": " for the ``env_fallback``\n                          ``env_fallback`` value.\n    :return: A list of envs matching the desired config.\n    :rtype: list[str]\n    \"\"\"\n    if not isinstance(passthru, bool):\n        raise TypeError('passthru must be True or False')\n    return [\n        env for env in\n        [\n            'TOX_ENV_NAME',\n            'TOX_ENV_NAME_OVERRIDE',\n            'TOX_ENV_NAME_OVERRIDE_OVERRIDE',\n            'TOX_ENV_OVERRIDE',\n            'TOX_ENV_", "solution": "\n    matched = [\n        declared for declared in declared_envs\n        if any(env_matches(declared, desired) for desired in desired_envs)\n    ]\n    return desired_envs if not matched and passthru else matched"}
{"prompt": "def stop_archive(self, archive_id):\n        \"\"\"\n        Stops an OpenTok archive that is being recorded.\n\n        Archives automatically stop recording after 90 minutes or when all clients have disconnected\n        from the session being archived.\n\n        @param [String] archive_id The archive ID of the archive you want to stop recording.\n\n        :rtype: The Archive object corresponding to the archive being stopped.\n        \"\"\"", "output": "\n        self.send_command(Command.STOP_ARCHIVE, [archive_id])\n        return self.wait_for_command_complete(Command.OK)\n\n    def get_archive_id(self, archive_id):\n        \"\"\"\n        Gets the archive ID of the given OpenTok archive.\n\n        @param [String] archive_id The ID of the OpenTok archive you want to stop recording.\n\n        @rtype: [String] The ID of the OpenTok object corresponding to the archive being stopped.\n        \"\"\"\n        self.send_command(Command.GET_ARCHIVE, [archive_id])\n        return self", "solution": "\n        response = requests.post(self.endpoints.archive_url(archive_id) + '/stop', headers=self.json_headers(), proxies=self.proxies, timeout=self.timeout)\n\n        if response.status_code < 300:\n            return Archive(self, response.json())\n        elif response.status_code == 403:\n            raise AuthError()\n        elif response.status_code == 404:\n            raise NotFoundError(\"Archive not found\")\n        elif response.status_code == 409:\n            raise ArchiveError(\"Archive is not in started state\")\n        else:\n            raise RequestError(\"An unexpected error occurred\", response.status_code)"}
{"prompt": "def topic_detail(request, slug):\n    \"\"\"\n    A detail view of a Topic\n\n    Templates:\n        :template:`faq/topic_detail.html`\n    Context:\n        topic\n            An :model:`faq.Topic` object.\n        question_list\n            A list of all published :model:`faq.Question` objects that relate\n            to the given :model:`faq.Topic`.\n\n    \"\"\"", "output": "\n    question_list = []\n    if request.method == 'POST':\n        form = TopicDetailForm(request.POST)\n        if form.is_valid():\n            question_list = form.cleaned_data['question_list']\n            return HttpResponseRedirect(reverse('faq:topic_detail', args=[slug]))\n    else:\n        form = TopicDetailForm()\n        return render(request, 'faq/topic_detail.html', {'form': form})\n\n\ndef topic_edit(request, slug):\n    \"\"\"\n    A edit view of a Topic\n\n    Templates:\n        :template:`faq/topic", "solution": "\n    extra_context = {\n        'question_list': Question.objects.published().filter(topic__slug=slug),\n    }\n\n    return object_detail(request, queryset=Topic.objects.published(),\n        extra_context=extra_context, template_object_name='topic', slug=slug)"}
{"prompt": "def generate_builder(self, entry_point, export_target):\n        \"\"\"\n        Yields exactly one builder if both the provided entry point and\n        export target satisfies the checks required.\n        \"\"\"", "output": "\n        if entry_point.is_literal():\n            yield self.compile_literal(entry_point)\n        else:\n            yield self.compile_target(entry_point)\n\n    def generate_type(self, entry_point):\n        \"\"\"\n        Returns the type (class) of a field's value.\n        \"\"\"\n        return entry_point.type\n\n    def generate_size(self, entry_point):\n        \"\"\"\n        Yields the number of bytes in the value.\n        \"\"\"\n        yield entry_point.size\n\n    def generate_value(self, entry_point):\n        \"\"\"\n        Returns the value of the field", "solution": "\n\n        try:\n            builder = entry_point.resolve()\n        except ImportError:\n            logger.error(\n                \"unable to import the target builder for the entry point \"\n                \"'%s' from package '%s' to generate artifact '%s'\",\n                entry_point, entry_point.dist, export_target,\n            )\n            return\n\n        if not self.verify_builder(builder):\n            logger.error(\n                \"the builder referenced by the entry point '%s' \"\n                \"from package '%s' has an incompatible signature\",\n                entry_point, entry_point.dist,\n            )\n            return\n\n        # CLEANUP see deprecation notice below\n        verifier = self.verify_export_target(export_target)\n        if not verifier:\n            logger.error(\n                \"the export target '%s' has been rejected\", export_target)\n            return\n\n        toolchain, spec = self.extract_builder_result(builder(\n            [entry_point.dist.project_name], export_target=export_target))\n        if not toolchain:\n            logger.error(\n                \"the builder referenced by the entry point '%s' \"\n                \"from package '%s' failed to produce a valid \"\n                \"toolchain\",\n                entry_point, entry_point.dist,\n            )\n            return\n\n        if spec.get(EXPORT_TARGET) != export_target:\n            logger.error(\n                \"the builder referenced by the entry point '%s' \"\n                \"from package '%s' failed to produce a spec with the \"\n                \"expected export_target\",\n                entry_point, entry_point.dist,\n            )\n            return\n\n        if callable(verifier):\n            warnings.warn(\n                \"%s:%s.verify_export_target returned a callable, which \"\n                \"will no longer be passed to spec.advise by calmjs-4.0.0; \"\n                \"please instead override 'setup_export_location' or \"\n                \"'prepare_export_location' in that class\" % (\n                    self.__class__.__module__, self.__class__.__name__),\n                DeprecationWarning\n            )\n            spec.advise(BEFORE_PREPARE, verifier, export_target)\n        else:\n            spec.advise(\n                BEFORE_PREPARE,\n                self.prepare_export_location, export_target)\n        yield entry_point, toolchain, spec"}
{"prompt": "def finalize(self):\n        \"\"\"\n        Add title and modify axes to make the image ready for display.\n        \"\"\"", "output": "\n        self.axes.set_title(self.title)\n        self.axes.set_xlabel(self.xlabel)\n        self.axes.set_ylabel(self.ylabel)\n        self.axes.set_aspect(self.aspect)\n        self.axes.set_zorder(self.zorder)\n        self.axes.set_zorder(self.zorder)\n        self.axes.set_xlabel(self.xlabel)\n        self.axes.set_ylabel(self.ylabel)\n        self.axes.set_aspect(self.aspect)\n        self.axes.set_", "solution": "\n        self.set_title(\n            '{} Manifold (fit in {:0.2f} seconds)'.format(\n                self._name, self.fit_time_.interval\n            )\n        )\n        self.ax.set_xticklabels([])\n        self.ax.set_yticklabels([])\n\n        if self._target_color_type == DISCRETE:\n            # Add the legend\n            manual_legend(self, self.classes_, self._colors, frameon=True)\n\n        elif self._target_color_type == CONTINUOUS:\n            # Add the color bar\n            plt.colorbar(self._scatter, ax=self.ax)"}
{"prompt": "def add_gate_option_group(parser):\n    \"\"\"Adds the options needed to apply gates to data.\n\n    Parameters\n    ----------\n    parser : object\n        ArgumentParser instance.\n    \"\"\"", "output": "\n    parser.add_argument('gate_option_group',\n                        help='Gates to add to the model.')\n    parser.add_argument('--model_name',\n                        help='Name of the model. If not given, the model name will be '\n                             'generated automatically from the model directory.')\n    parser.add_argument('--model_dir',\n                        help='Directory where the generated model will be saved.')\n    parser.add_argument('--save_dir',\n                        help='Directory where the generated model will be saved.')\n    parser.add_argument('--save_prefix',\n                        help='Prefix for the generated model names", "solution": "\n    gate_group = parser.add_argument_group(\"Options for gating data.\")\n\n    gate_group.add_argument(\"--gate\", nargs=\"+\", type=str,\n                            metavar=\"IFO:CENTRALTIME:HALFDUR:TAPERDUR\",\n                            help=\"Apply one or more gates to the data before \"\n                                 \"filtering.\")\n    gate_group.add_argument(\"--gate-overwhitened\", action=\"store_true\",\n                            help=\"Overwhiten data first, then apply the \"\n                                 \"gates specified in --gate. Overwhitening \"\n                                 \"allows for sharper tapers to be used, \"\n                                 \"since lines are not blurred.\")\n    gate_group.add_argument(\"--psd-gate\", nargs=\"+\", type=str,\n                            metavar=\"IFO:CENTRALTIME:HALFDUR:TAPERDUR\",\n                            help=\"Apply one or more gates to the data used \"\n                                 \"for computing the PSD. Gates are applied \"\n                                 \"prior to FFT-ing the data for PSD \"\n                                 \"estimation.\")\n    return gate_group"}
{"prompt": "def add_atom_data(data_api, data_setters, atom_names, element_names, atom_charges, group_atom_ind):\n    \"\"\"Add the atomic data to the DataTransferInterface.\n    :param data_api the data api from where to get the data\n    :param data_setters the class to push the data to\n    :param atom_nams the list of atom names for the group\n    :param element_names the list of element names for this group\n    :param atom_charges the list formal atomic charges for this group\n    :param group_atom_ind the index of this atom in the group\"\"\"", "output": "_ind the list of group atom indices for this group\n    :return the number of atoms added\n    \"\"\"\n    if group_atom_ind is None:\n        group_atom_ind = group_atom_names\n\n    # Add the atomic data to the DataTransferInterface\n    for i in range(len(data_setters)):\n        data_setters[i].add_atom(atom_nams[group_atom_ind[i]])\n        data_setters[i].add_element(element_names[group_atom_ind[i]])\n        data_setters[i].add_atom(", "solution": "\n    atom_name = atom_names[group_atom_ind]\n    element = element_names[group_atom_ind]\n    charge = atom_charges[group_atom_ind]\n    alternative_location_id = data_api.alt_loc_list[data_api.atom_counter]\n    serial_number = data_api.atom_id_list[data_api.atom_counter]\n    x = data_api.x_coord_list[data_api.atom_counter]\n    y = data_api.y_coord_list[data_api.atom_counter]\n    z = data_api.z_coord_list[data_api.atom_counter]\n    occupancy = data_api.occupancy_list[data_api.atom_counter]\n    temperature_factor = data_api.b_factor_list[data_api.atom_counter]\n    data_setters.set_atom_info(atom_name, serial_number, alternative_location_id,\n                               x, y, z, occupancy, temperature_factor, element, charge)"}
{"prompt": "def save(self, path: Union[str, bytes, int]) -> None:\n        \"\"\"Write QASM output to a file specified by path.\"\"\"", "output": "\n        with open(path, \"wb\") as f:\n            f.write(self.qasm_header)\n            f.write(self.qasm_body)\n            f.write(self.qasm_footer)\n            f.write(self.qasm_body)\n            f.write(self.qasm_footer)\n            f.write(self.qasm_body)\n            f.write(self.qasm_footer)\n\n    def load(self, path: Union[str, bytes, int]) -> None:\n        \"\"\"Read QASM output from a file specified by path.\"\"\"", "solution": "\n        with open(path, 'w') as f:\n            def write(s: str) -> None:\n                f.write(s)\n            self._write_qasm(write)"}
{"prompt": "def get_members(self, api=None):\n        \"\"\"Retrieve dataset members\n        :param api: Api instance\n        :return: Collection object\n        \"\"\"", "output": "\n        if not api:\n            api = self.api\n        if not self.is_valid_id(api.dataset_id):\n            raise ValueError('Invalid dataset id')\n        if not self.is_valid_id(api.dataset_name):\n            raise ValueError('Invalid dataset name')\n        if not self.is_valid_name(api.dataset_name):\n            raise ValueError('Invalid dataset name')\n        if not self.is_valid_name(api.dataset_name):\n            raise ValueError('Invalid dataset name')\n        if not self.is_valid_name(api.dataset_name):", "solution": "\n        api = api or self._API\n\n        response = api.get(url=self._URL['members'].format(id=self.id))\n\n        data = response.json()\n        total = response.headers['x-total-matching-query']\n        members = [Member(api=api, **member) for member in data['items']]\n        links = [Link(**link) for link in data['links']]\n        href = data['href']\n\n        return Collection(\n            resource=Member,\n            href=href,\n            total=total,\n            items=members,\n            links=links,\n            api=api\n        )"}
{"prompt": "def ReadAttachments(self, document_link, options=None):\n        \"\"\"Reads all attachments in a document.\n\n        :param str document_link:\n            The link to the document.\n        :param dict options:\n            The request options for the request.\n\n        :return:\n            Query Iterable of Attachments.\n        :rtype:\n            query_iterable.QueryIterable\n\n        \"\"\"", "output": "\n        return self.client.ReadAttachments(document_link, options)\n\n    def ReadAttachmentsWithText(self, document_text, options=None):\n        \"\"\"Reads all attachments with a given text.\n\n        :param str document_text:\n            The text to search for.\n        :param dict options:\n            The request options for the request.\n\n        :return:\n            Query Iterable of AttachmentsWithText.\n        :rtype:\n            query_iterable.QueryIterable\n\n        \"\"\"\n        return self.client.ReadAttachmentsWithText(document_text, options)\n\n    def ReadAttachmentsWithImage(self, document", "solution": "\n        if options is None:\n            options = {}\n\n        return self.QueryAttachments(document_link, None, options)"}
{"prompt": "def discover(cls, path, depth=\"0\"):\n        \"\"\"Discover a list of collections under the given ``path``.\n\n        If ``depth`` is \"0\", only the actual object under ``path`` is\n        returned.\n\n        If ``depth`` is anything but \"0\", it is considered as \"1\" and direct\n        children are included in the result.\n\n        The ``path`` is relative.\n\n        The root collection \"/\" must always exist.\n\n        \"\"\"", "output": "\n        return cls.objects.get(path=path, depth=depth)\n\n    @classmethod\n    def get_all_objects(cls):\n        \"\"\"Get all collections.\n\n        Returns:\n            Collections: A list of all collections.\n\n        \"\"\"\n        return cls.objects.filter(depth=0)\n\n    @classmethod\n    def get_all_children(cls):\n        \"\"\"Get all collections children.\n\n        Returns:\n            Collections: A list of all collections children.\n\n        \"\"\"\n        return cls.objects.filter(depth=1)\n\n    @classmethod\n    def get_all_children_count(cls):\n        \"\"\"Get", "solution": "\n\n        # Path should already be sanitized\n        attributes = _get_attributes_from_path(path)\n\n        try:\n            if len(attributes) == 3:\n                # If an item, create a collection for the item.\n                item = attributes.pop()\n                path = \"/\".join(attributes)\n                collection = cls(path, _is_principal(path))\n                yield collection.get(item)\n                return\n\n            collection = cls(path, _is_principal(path))\n        except api.exceptions.DoesNotExist:\n            return\n\n        yield collection\n\n        if depth == \"0\":\n            return\n\n        if len(attributes) == 0:\n            yield cls(posixpath.join(path, cls.user), principal=True)\n        elif len(attributes) == 1:\n            for journal in cls.etesync.list():\n                if journal.collection.TYPE in (api.AddressBook.TYPE, api.Calendar.TYPE, api.TaskList.TYPE):\n                    yield cls(posixpath.join(path, journal.uid), principal=False)\n        elif len(attributes) == 2:\n            for item in collection.list():\n                yield collection.get(item)\n\n        elif len(attributes) > 2:\n            raise RuntimeError(\"Found more than one attribute. Shouldn't happen\")"}
{"prompt": "def Release(self):\n    \"\"\"Remove the lock.\n\n    Note that this only resets the lock if we actually hold it since\n    lock_expiration == self.expires and lock_owner = self.lock_token.\n    \"\"\"", "output": "\n    if self.expires < self.lock_expiration:\n      self.expires = self.lock_expiration\n      self.owner = None\n    else:\n      self.expires = None\n      self.owner = None\n    return self.expires\n\n  def __init__(self, lock_token, lease_seconds=0):\n    \"\"\"Constructor.\n\n    Args:\n      lock_token: A string used as the token to use to validate the lock.\n      lease_seconds: The time to live for the lock.\n    \"\"\"\n    self.lock_token = lock_token\n    self.lease_seconds = lease_seconds\n    self.", "solution": "\n    if self.locked:\n      query = (\"UPDATE locks SET lock_expiration=0, lock_owner=0 \"\n               \"WHERE lock_expiration=%s \"\n               \"AND lock_owner=%s \"\n               \"AND subject_hash=unhex(md5(%s))\")\n      args = [self.expires, self.lock_token, self.subject]\n      self.store.ExecuteQuery(query, args)\n      self.locked = False"}
{"prompt": "async def _quit(self):\n        \"\"\"Quits the bot.\"\"\"", "output": "\n        await self.bot.say(\"Goodbye.\")\n\n    @commands.command(pass_context=True, no_pm=True)\n    async def _start(self, ctx):\n        \"\"\"Starts the bot.\"\"\"\n        if ctx.message.server.id not in self.bot.servers:\n            await self.bot.say(\"This server is not registered.\")\n            return\n\n        self.bot.servers[ctx.message.server.id].start()\n        await self.bot.say(\"Logged in as\")\n\n    @commands.command(pass_context=True, no_pm=True)\n    async def", "solution": "\n        await self.bot.responses.failure(message=\"Bot shutting down\")\n        await self.bot.logout()"}
{"prompt": "def _set_textarea(el, value):\n        \"\"\"\n        Set content of given textarea element `el` to `value`.\n\n        Args:\n            el (obj): Reference to textarea element you wish to set.\n            value (obj/list): Value to which the `el` will be set.\n        \"\"\"", "output": "\n        if not isinstance(value, list):\n            value = [value]\n        for el in value:\n            if el not in el:\n                raise ValueError('Textarea element %s not in %s' % (el, value))\n            el = el.replace(el, el.get(el, el))\n        el.text = value\n\n    def _set_textbox(self, value):\n        \"\"\"\n        Set content of given textbox element `value` to `textbox`.\n\n        Args:\n            value (str): Reference to textbox element you wish to set.\n        \"\"\"\n        if not isinstance(value,", "solution": "\n        if isinstance(value, dict):\n            el.text = value[\"val\"]\n        elif type(value) in [list, tuple]:\n            el.text = \"\\n\\n\".join(\n                \"-- %s --\\n%s\" % (item[\"source\"], item[\"val\"])\n                for item in value\n            )\n        else:\n            el.text = value"}
{"prompt": "def load_obs(self,  mask_threshold=0.5):\n        \"\"\"\n        Loads observations and masking grid (if needed).\n\n        :param mask_threshold: Values greater than the threshold are kept, others are masked.\n        :return:\n        \"\"\"", "output": "\n        self.mask_threshold = mask_threshold\n        self.obs_mask = np.zeros(self.grid.shape, dtype=np.bool)\n        self.obs_mask[self.grid] = True\n        self.mask_grid = np.zeros(self.grid.shape, dtype=np.bool)\n\n    def load_obs_grid(self,  grid_size=None, mask_threshold=0.5):\n        \"\"\"\n        Loads grid (if needed).\n\n        :param grid_size: Values greater than the size of the grid (if None, the size of the whole grid", "solution": "\n        start_date = self.run_date + timedelta(hours=self.start_hour)\n        end_date = self.run_date + timedelta(hours=self.end_hour)\n        mrms_grid = MRMSGrid(start_date, end_date, self.mrms_variable, self.mrms_path)\n        mrms_grid.load_data()\n        if len(mrms_grid.data) > 0:\n            self.raw_obs[self.mrms_variable] = np.where(mrms_grid.data > 100, 100, mrms_grid.data)\n            self.window_obs[self.mrms_variable] = np.array([self.raw_obs[self.mrms_variable][sl].max(axis=0)\n                                                            for sl in self.hour_windows])\n            if self.obs_mask:\n                mask_grid = MRMSGrid(start_date, end_date, self.mask_variable, self.mrms_path)\n                mask_grid.load_data()\n                self.raw_obs[self.mask_variable] = np.where(mask_grid.data >= mask_threshold, 1, 0)\n                self.window_obs[self.mask_variable] = np.array([self.raw_obs[self.mask_variable][sl].max(axis=0)\n                                                               for sl in self.hour_windows])"}
{"prompt": "def generate(self, file, validatedata = None,  inputdata=None, user = None):\n        \"\"\"Convert the template into instantiated metadata, validating the data in the process and returning errors otherwise. inputdata is a dictionary-compatible structure, such as the relevant postdata. Return (success, metadata, parameters), error messages can be extracted from parameters[].error. Validatedata is a (errors,parameters) tuple that can be passed if you did validation in a prior stage, if not specified, it will be done automatically.\"\"\"", "output": "\n        self.template = Template(file, self.template_vars)\n        self.parameters = self.template.parameters\n        self.error = validatedata[0]\n        self.error_message = validatedata[1]\n        self.parameters_error = validatedata[2]\n        self.parameters_parameters = self.template.parameters_parameters\n        self.parameters_valid = self.template.parameters_parameters_valid\n        self.parameters_valid_message = validatedata[3]\n        self.parameters_valid_error = validatedata[4]\n        self.parameters_valid_parameters = self.", "solution": "\n\n        metadata = {}\n\n        if not validatedata:\n            assert inputdata\n            errors, parameters = self.validate(inputdata,user) #pylint: disable=unused-variable\n        else:\n            errors, parameters = validatedata #pylint: disable=unused-variable\n\n        #scan errors and set metadata\n        success = True\n        for parameter in parameters:\n            assert isinstance(parameter, clam.common.parameters.AbstractParameter)\n            if parameter.error:\n                success = False\n            else:\n                metadata[parameter.id] = parameter.value\n\n        if not success:\n            metadata = None\n        else:\n            try:\n                metadata = self.formatclass(file, **metadata)\n            except:\n                raise\n\n        return success, metadata, parameters"}
{"prompt": "def nlmsg_len(self, value):\n        \"\"\"Length setter.\"\"\"", "output": "\n        self._nlmsg_len = value\n\n    @property\n    def nlmsg_type(self):\n        \"\"\"Type setter.\"\"\"\n        return self._nlmsg_type\n\n    @property\n    def nlmsg_flags(self):\n        \"\"\"Flags setter.\"\"\"\n        return self._nlmsg_flags\n\n    @property\n    def nlmsg_len(self):\n        \"\"\"Length setter.\"\"\"\n        return self._nlmsg_len\n\n    @property\n    def nlmsg_type(self):\n        \"\"\"Type setter.\"\"\"\n        return self._nlmsg_type\n\n    @property\n    def nlmsg_flags(self):\n        \"\"\"Flags setter.\"\"\"\n        return", "solution": "\n        self.bytearray[self._get_slicers(0)] = bytearray(c_uint32(value or 0))"}
{"prompt": "def user_absent(name):\n    \"\"\"\n    Ensure a user is not present\n\n    name\n        username to remove if it exists\n\n    Examples:\n\n    .. code-block:: yaml\n\n        delete:\n          onyx.user_absent:\n            - name: daniel\n    \"\"\"", "output": "\n    try:\n        if name in get_user_list():\n            del_user(name)\n    except KeyError:\n        pass\n\n\ndef del_user(name):\n    \"\"\"\n    Delete a user\n\n    name\n        username to remove\n\n    Examples:\n\n   .. code-block:: yaml\n\n        delete:\n          onyx.user_absent:\n            - name: daniel\n    \"\"\"\n    try:\n        if name in get_user_list():\n            del_user(name)\n    except KeyError:\n        pass\n\n\ndef get_user_list():\n    \"\"\"\n    Return a list of all user names\n\n    Returns:\n        list of", "solution": "\n\n    ret = {'name': name,\n           'result': False,\n           'changes': {},\n           'comment': ''}\n\n    old_user = __salt__['onyx.cmd']('get_user', username=name)\n\n    if not old_user:\n        ret['result'] = True\n        ret['comment'] = 'User does not exist'\n        return ret\n\n    if __opts__['test'] is True and old_user:\n        ret['result'] = None\n        ret['comment'] = 'User will be removed'\n        ret['changes']['old'] = old_user\n        ret['changes']['new'] = ''\n        return ret\n\n    __salt__['onyx.cmd']('remove_user', username=name)\n\n    if __salt__['onyx.cmd']('get_user', username=name):\n        ret['comment'] = 'Failed to remove user'\n    else:\n        ret['result'] = True\n        ret['comment'] = 'User removed'\n        ret['changes']['old'] = old_user\n        ret['changes']['new'] = ''\n    return ret"}
{"prompt": "def relations_of(self, target, include_object=False):\n        \"\"\" list all relations the originate from target \"\"\"", "output": "\n        if include_object:\n            return [rel for rel in self.model.objects.filter(target=target) if rel.from_field == target]\n        return [rel for rel in self.model.objects.filter(target=target) if rel.to_field == target]\n\n    def get_related_objects(self, target, relation_name):\n        \"\"\" returns a list of objects that match the given relation \"\"\"\n        return [rel for rel in self.relations_of(target, include_object=True) if rel.to_field == relation_name]\n\n    def get_related", "solution": "\n        relations = (target if isinstance(target, RamGraphDBNode) else self._get_item_node(target)).outgoing\n        if include_object:\n            for k in relations:\n                for v in relations[k]:\n                    if hasattr(v, 'obj'): # filter dead links\n                        yield k, v.obj\n        else:\n            yield from relations"}
{"prompt": "def _convert_md_type(self, type_to_convert: str):\n        \"\"\"Metadata types are not consistent in Isogeo API. A vector dataset is\n         defined as vector-dataset in query filter but as vectorDataset in\n         resource (metadata) details.\n\n        see: https://github.com/isogeo/isogeo-api-py-minsdk/issues/29\n        \"\"\"", "output": "\n        if type_to_convert =='vector':\n            return self._convert_vector_dataset(\n                self.resource_type, self.vector_dataset_id)\n        elif type_to_convert =='resource':\n            return self._convert_resource_dataset(\n                self.resource_type, self.resource_id)\n        else:\n            raise ValueError(\n                'Unknown dataset type: {0}'.format(type_to_convert))\n\n    def _convert_vector_dataset(self, vector_type: str, dataset_id: str):\n        \"\"\"\n        Convert a vector dataset type to a vector", "solution": "\n        if type_to_convert in FILTER_TYPES:\n            return FILTER_TYPES.get(type_to_convert)\n        elif type_to_convert in FILTER_TYPES.values():\n            return [k for k, v in FILTER_TYPES.items() if v == type_to_convert][0]\n        else:\n            raise ValueError(\n                \"Incorrect metadata type to convert: {}\".format(type_to_convert)\n            )"}
{"prompt": "def check_stripe_api_version(app_configs=None, **kwargs):\n\t\"\"\"Check the user has configured API version correctly.\"\"\"", "output": "\n\tif not app_configs:\n\t\tapp_configs = frappe.get_all(\"Stripe Settings\")\n\t\tfrappe.cache().hset(\"stripe_settings\", \"api_version\", \"v1\")\n\t\tfrappe.cache().hset(\"stripe_settings\", \"api_version\", \"v2\")\n\t\tfrappe.cache().hset(\"stripe_settings\", \"user_id\", \"test@example.com\")\n\t\tfrappe.db.commit()\n\t\tfrappe.cache().hset(\"stripe_settings\", \"user_id\", \"test@example.com\")\n\t", "solution": "\n\tfrom . import settings as djstripe_settings\n\n\tmessages = []\n\tdefault_version = djstripe_settings.DEFAULT_STRIPE_API_VERSION\n\tversion = djstripe_settings.get_stripe_api_version()\n\n\tif not validate_stripe_api_version(version):\n\t\tmsg = \"Invalid Stripe API version: {}\".format(version)\n\t\thint = \"STRIPE_API_VERSION should be formatted as: YYYY-MM-DD\"\n\t\tmessages.append(checks.Critical(msg, hint=hint, id=\"djstripe.C004\"))\n\n\tif version != default_version:\n\t\tmsg = (\n\t\t\t\"The Stripe API version has a non-default value of '{}'. \"\n\t\t\t\"Non-default versions are not explicitly supported, and may \"\n\t\t\t\"cause compatibility issues.\".format(version)\n\t\t)\n\t\thint = \"Use the dj-stripe default for Stripe API version: {}\".format(default_version)\n\t\tmessages.append(checks.Warning(msg, hint=hint, id=\"djstripe.W001\"))\n\n\treturn messages"}
{"prompt": "def iter(self, dir_entries):\n        \"\"\"\n        :param dir_entries: list of os.DirEntry() instances\n        \"\"\"", "output": "\n        for entry in dir_entries:\n            if entry.name.startswith('.') or entry.is_dir:\n                yield entry\n\n    def find_plugin(self, name):\n        \"\"\"\n        :param name: the name of the plugin to load\n        \"\"\"\n        if self.plugins is not None and name in self.plugins:\n            return self.plugins[name]\n        else:\n            if name in self.plugin_modules:\n                module = self.plugin_modules[name]\n                if hasattr(module,'setup'):\n                    yield module\n                else:\n                    yield self.load_plugin(name)\n            else:\n               ", "solution": "\n        filter = self.filter\n        for entry in dir_entries:\n            path = filter(Path2(entry.path))\n            if path != False:\n                yield path"}
{"prompt": "def temporary_file(contents, suffix='', prefix='tmp'):\n    \"\"\"Creates a temporary file with specified contents that persists for the context.\n\n    :param contents: binary string that will be written to the file.\n    :param prefix: will be prefixed to the filename.\n    :param suffix: will be appended to the filename.\n    :returns path of the created file.\n    \"\"\"", "output": "\n    if not isinstance(contents, six.binary_type):\n        contents = contents.encode('utf-8')\n    with tempfile.NamedTemporaryFile(prefix=prefix, suffix=suffix,\n                                  delete=False) as f:\n        f.write(contents)\n        f.flush()\n        os.fsync(f.fileno())\n    return path\n\n\ndef _get_disk_of_partition(session, partition_path):\n    device_path = utils.make_dev_path(session, partition_path)\n    if session.is_local_connection:\n        return device_path\n    try:\n        partitions", "solution": "\n    tmp = tempfile.NamedTemporaryFile(suffix=suffix, prefix=prefix, delete=False)\n    with tmp:\n        tmp.write(contents)\n\n    try:\n        yield tmp.name\n    finally:\n        os.unlink(tmp.name)"}
{"prompt": "def parent(self):\n        \"\"\"\n        Return the parent scope.\n\n        :return: FoldScope or None\n        \"\"\"", "output": "\n        return self._parent\n\n    @property\n    def parent_scope(self):\n        \"\"\"\n        Return the parent scope.\n\n        :return: FoldScope or None\n        \"\"\"\n        return self._parent_scope\n\n    @property\n    def scope(self):\n        \"\"\"\n        Return the scope.\n\n        :return: FoldScope\n        \"\"\"\n        return self._scope\n\n    def __init__(self, parent=None, scope=None):\n        \"\"\"\n        Create a new FoldScope object.\n\n        :param parent: The parent scope.\n        :type parent: FoldScope or None\n        :param scope: The scope.\n        :type", "solution": "\n        if TextBlockHelper.get_fold_lvl(self._trigger) > 0 and \\\n                self._trigger.blockNumber():\n            block = self._trigger.previous()\n            ref_lvl = self.trigger_level - 1\n            while (block.blockNumber() and\n                    (not TextBlockHelper.is_fold_trigger(block) or\n                     TextBlockHelper.get_fold_lvl(block) > ref_lvl)):\n                block = block.previous()\n            try:\n                return FoldScope(block)\n            except ValueError:\n                return None\n        return None"}
{"prompt": "def parse(input):\n    \"\"\"\n    parse input to datetime\n    \"\"\"", "output": "\n    return datetime.strptime(input, '%Y-%m-%d %H:%M:%S')\n\n\ndef get_date_range(start, end):\n    \"\"\"\n    get date range from start to end\n    \"\"\"\n    start = datetime.strptime(start, '%Y-%m-%d %H:%M:%S')\n    end = datetime.strptime(end, '%Y-%m-%d %H:%M:%S')\n    return start, end\n\n\ndef get_date_range_str(start, end):\n    \"\"\"\n    get date range from start to end\n    \"\"\"\n    start = datetime.strptime", "solution": "\n    if isinstance(input, datetime):\n        return input\n    if isinstance(input, date):\n        return date_to_datetime(input)\n    if isinstance(input, time):\n        return time_to_datetime(input)\n    if isinstance(input, (int, float)):\n        return timestamp_to_datetime(input)\n    if isinstance(input, (str)):\n        return string_to_data_time(input)\n    return None"}
{"prompt": "def tb_capture(func):\n    \"\"\"A decorator which captures worker tracebacks.\n\n    Tracebacks in particular, are captured. Inspired by an example in\n    https://bugs.python.org/issue13831.\n\n    This decorator wraps rio-mucho worker tasks.\n\n    Parameters\n    ----------\n    func : function\n        A function to be decorated.\n\n    Returns\n    -------\n    func\n\n    \"\"\"", "output": "\n    def wrapper(func):\n        @functools.wraps(func)\n        def wrapper_task(*args, **kwargs):\n            try:\n                return func(*args, **kwargs)\n            except Exception as e:\n                logger.error(\"Exception in worker: %s\", e)\n                raise\n        return wrapper_task\n    return wrapper\n\n\ndef _get_worker_name(worker_id):\n    \"\"\"Get the name of the rio-mucho worker.\n\n    Parameters\n    ----------\n    worker_id : str\n        The name of the rio-mucho worker.\n\n    Returns\n    -------\n    str\n        The name", "solution": "\n\n    @wraps(func)\n    def wrapper(*args, **kwds):\n        try:\n            return func(*args, **kwds)\n\n        except Exception:\n            raise MuchoChildError()\n\n    return wrapper"}
{"prompt": "def xmltreefromfile(filename):\n    \"\"\"Internal function to read an XML file\"\"\"", "output": "\n    return ElementTree.parse(filename)\n\ndef xmltreeread(filename, events):\n    \"\"\"Read an XML file and return a list of events\"\"\"\n    return xmltreefromfile(filename)\n\ndef xmltreewrite(filename, events):\n    \"\"\"Internal function to write an XML file\"\"\"\n    return ElementTree.write(filename, events, encoding=\"utf-8\")\n\ndef xmltreewritexml(filename, events):\n    \"\"\"Write an XML file\"\"\"\n    return xmltreewrite(filename, events, encoding=\"utf-8\")\n\ndef xmltreewriteevents(events):\n    \"\"\"Write an events list", "solution": "\n    try:\n        return ElementTree.parse(filename, ElementTree.XMLParser(collect_ids=False))\n    except TypeError:\n        return ElementTree.parse(filename, ElementTree.XMLParser())"}
{"prompt": "def asluav_status_encode(self, LED_status, SATCOM_status, Servo_status, Motor_rpm):\n                \"\"\"\n                Extended state information for ASLUAVs\n\n                LED_status                : Status of the position-indicator LEDs (uint8_t)\n                SATCOM_status             : Status of the IRIDIUM satellite communication system (uint8_t)\n                Servo_status              : Status vector for up to 8 servos (uint8_t)\n                Motor_rpm                 : Motor RPM (float)\n\n                \"\"\"", "output": "\n                return asluav_status_encode(self.status, LED_status, SATCOM_status,\n                                              self.servo_rpm_to_asluav(Motor_rpm),\n                                              self.asluav_position_indicator_rpm_to_servo(LED_position_indicator_rpm),\n                                              self.asluav_servo_position_sensor_rpm_to_asluav(Servo_position_sensor_rpm),\n                                              self.asluav_servo_asluav_position_sensor_rpm(Servo_status))\n\n        @staticmethod\n        def", "solution": "\n                return MAVLink_asluav_status_message(LED_status, SATCOM_status, Servo_status, Motor_rpm)"}
{"prompt": "def config(self):\n        \"\"\" Writeable configuration parameters.\n\n        :rtype:     dict\n        \"\"\"", "output": "\n        return {\n            'name': self.name,\n           'version': self.version,\n            'description': self.description,\n            'author': self.author,\n            'author_email': self.author_email,\n            'url': self.url,\n            'download_url': self.download_url,\n            'issue_tracker': self.issue_tracker,\n            'issue_tracker_email': self.issue_tracker_email,\n            'issue_tracker_name': self.issue_tracker_name,\n            'project_id': self.project_id,\n            'issue_id", "solution": "\n        config = self._get_config()\n        return {section: {itm.name: itm for itm in config[section].values()\n                          if not itm.readonly}\n                for section in config\n                if 'settings' in section or section == 'other'}"}
{"prompt": "def _parse_bbox_list(bbox_list):\n        \"\"\" Helper method for parsing a list of bounding boxes\n        \"\"\"", "output": "\n        bbox_list = bbox_list.split(\",\")\n        bbox_list = [float(x) for x in bbox_list]\n        return bbox_list\n\n    def _parse_bbox_string(bbox_string):\n        \"\"\" Helper method for parsing a string of bboxes\n        \"\"\"\n        bbox_list = bbox_string.split(\",\")\n        bbox_string = bbox_string.replace(\" \", \"\")\n        return bbox_list\n\n    # -------------------------------------------------------------------------\n    def _get_features(self, row):\n        \"\"\"\n            Get the features for the specified Row\n\n            @param row: the gis_location Row\n        \"\"\"\n        return row", "solution": "\n        if isinstance(bbox_list, BBoxCollection):\n            return bbox_list.bbox_list, bbox_list.crs\n\n        if not isinstance(bbox_list, list) or not bbox_list:\n            raise ValueError('Expected non-empty list of BBox objects')\n\n        for bbox in bbox_list:\n            if not isinstance(bbox, BBox):\n                raise ValueError('Elements in the list should be of type {}, got {}'.format(BBox.__name__, type(bbox)))\n\n        crs = bbox_list[0].crs\n        for bbox in bbox_list:\n            if bbox.crs is not crs:\n                raise ValueError('All bounding boxes should have the same CRS')\n\n        return bbox_list, crs"}
{"prompt": "def fetch(self, failures=True, wait=0):\n        \"\"\"\n        get the task result objects from the chain when it finishes. blocks until timeout.\n        :param failures: include failed tasks\n        :param int wait: how many milliseconds to wait for a result\n        :return: an unsorted list of task objects\n        \"\"\"", "output": "\n        if not self.tasks:\n            return []\n\n        # sort tasks by priority\n        tasks = sorted(self.tasks, key=lambda t: t.priority)\n\n        # loop through the tasks\n        for task in tasks:\n            if task.status == 'failed':\n                # if the task is a failed task, we don't want to wait\n                if wait and task.status!= 'timeout':\n                    return []\n            elif task.status == 'finished':\n                # if the task is finished, we don't want to wait\n                if task.finished:\n                    return []\n\n            # if the task is a running task, we", "solution": "\n        if self.started:\n            return fetch_group(self.group, failures=failures, wait=wait, count=self.length(), cached=self.cached)"}
{"prompt": "def update_serviceprofile(self, host_id, vlan_id):\n        \"\"\"Top level method to update Service Profiles on UCS Manager.\n\n        Calls all the methods responsible for the individual tasks that\n        ultimately result in a vlan_id getting programed on a server's\n        ethernet ports and the Fabric Interconnect's network ports.\n        \"\"\"", "output": "\n        return self.api.update_port(host_id, {'vlan_id': vlan_id})\n\n    def delete_serviceprofile(self, host_id):\n        \"\"\"Top level method to delete the Service Profiles on UCS Manager.\n\n        Calls all the methods responsible for the individual tasks that\n        ultimately result in a host_id getting network ports on a server's\n        fabric ports.\n        \"\"\"\n        return self.api.delete_port(host_id, {'vlan_id': host_id})\n\n    def get_serviceprofile(self, host_id):\n        \"\"\"Retrieves the network port", "solution": "\n        ucsm_ip = self.get_ucsm_ip_for_host(host_id)\n        if not ucsm_ip:\n            LOG.info('UCS Manager network driver does not have UCSM IP '\n                     'for Host_id %s', str(host_id))\n            return False\n\n        service_profile = self.ucsm_sp_dict.get((ucsm_ip, host_id))\n        if service_profile:\n            LOG.debug('UCS Manager network driver Service Profile : %s',\n                      service_profile)\n        else:\n            LOG.info('UCS Manager network driver does not support '\n                     'Host_id %s', host_id)\n            return False\n\n        with self.ucsm_connect_disconnect(ucsm_ip) as handle:\n            # Create Vlan Profile\n            if not self._create_vlanprofile(handle, vlan_id, ucsm_ip):\n                LOG.error('UCS Manager network driver failed to create '\n                          'Vlan Profile for vlan %s', str(vlan_id))\n                return False\n\n            # Update Service Profile\n            if not self._update_service_profile(handle,\n                                                service_profile,\n                                                vlan_id,\n                                                ucsm_ip):\n                LOG.error('UCS Manager network driver failed to update '\n                          'Service Profile %(service_profile)s in UCSM '\n                          '%(ucsm_ip)s',\n                    {'service_profile': service_profile, 'ucsm_ip': ucsm_ip})\n                return False\n\n        return True"}
{"prompt": "def build_traversal(self, traversal):\n        \"\"\"\n        traverse a relationship from a node to a set of nodes\n        \"\"\"", "output": "\n        if traversal.direction == 'left':\n            traversal.direction = 'right'\n        elif traversal.direction == 'right':\n            traversal.direction = 'left'\n        else:\n            traversal.direction = 'right'\n        return traversal\n\n    def build_path(self, path):\n        \"\"\"\n        build a path from a path to a set of nodes\n        \"\"\"\n        if path.type == 'way':\n            self.ways.append(path)\n        elif path.type =='relation':\n            self.relations.append(path)\n        elif path.type == 'path':\n            self.paths.append(path)", "solution": "\n        # build source\n        rhs_label = ':' + traversal.target_class.__label__\n\n        # build source\n        lhs_ident = self.build_source(traversal.source)\n        rhs_ident = traversal.name + rhs_label\n        self._ast['return'] = traversal.name\n        self._ast['result_class'] = traversal.target_class\n\n        rel_ident = self.create_ident()\n        stmt = _rel_helper(lhs=lhs_ident, rhs=rhs_ident, ident=rel_ident, **traversal.definition)\n        self._ast['match'].append(stmt)\n\n        if traversal.filters:\n            self.build_where_stmt(rel_ident, traversal.filters)\n\n        return traversal.name"}
{"prompt": "def _derX(self,x,y,z):\n        \"\"\"\n        Returns the derivative with respect to x of the interpolated function\n        at each value in x,y,z. Only called internally by HARKinterpolator3D.derivativeX.\n        \"\"\"", "output": "\n        return self.HARKinterpolator3D._derX(x,y,z)\n\n    def _derY(self,x,y,z):\n        \"\"\"\n        Returns the derivative with respect to x of the interpolated function\n        at each value in x,y,z. Only called internally by HARKinterpolator3D.derivativeY.\n        \"\"\"\n        return self.HARKinterpolator3D._derY(x,y,z)\n\n    def _derZ(self,x,y,z):\n        \"\"\"\n        Returns the derivative with respect to x of the interpolated function\n        at each", "solution": "\n        if _isscalar(x):\n            x_pos = max(min(self.xSearchFunc(self.x_list,x),self.x_n-1),1)\n            y_pos = max(min(self.ySearchFunc(self.y_list,y),self.y_n-1),1)\n            z_pos = max(min(self.zSearchFunc(self.z_list,z),self.z_n-1),1)\n        else:\n            x_pos = self.xSearchFunc(self.x_list,x)\n            x_pos[x_pos < 1] = 1\n            x_pos[x_pos > self.x_n-1] = self.x_n-1\n            y_pos = self.ySearchFunc(self.y_list,y)\n            y_pos[y_pos < 1] = 1\n            y_pos[y_pos > self.y_n-1] = self.y_n-1\n            z_pos = self.zSearchFunc(self.z_list,z)\n            z_pos[z_pos < 1] = 1\n            z_pos[z_pos > self.z_n-1] = self.z_n-1\n        beta = (y - self.y_list[y_pos-1])/(self.y_list[y_pos] - self.y_list[y_pos-1])\n        gamma = (z - self.z_list[z_pos-1])/(self.z_list[z_pos] - self.z_list[z_pos-1])\n        dfdx = (\n           (  (1-beta)*(1-gamma)*self.f_values[x_pos,y_pos-1,z_pos-1]\n           +  (1-beta)*gamma*self.f_values[x_pos,y_pos-1,z_pos]\n           +  beta*(1-gamma)*self.f_values[x_pos,y_pos,z_pos-1]\n           +  beta*gamma*self.f_values[x_pos,y_pos,z_pos]) -\n           (  (1-beta)*(1-gamma)*self.f_values[x_pos-1,y_pos-1,z_pos-1]\n           +  (1-beta)*gamma*self.f_values[x_pos-1,y_pos-1,z_pos]\n           +  beta*(1-gamma)*self.f_values[x_pos-1,y_pos,z_pos-1]\n           +  beta*gamma*self.f_values[x_pos-1,y_pos,z_pos]))/(self.x_list[x_pos] - self.x_list[x_pos-1])\n        return dfdx"}
{"prompt": "def check_failed_login(self):\n        \"\"\"\n        'Private method', check failed logins, it's used for wath_login decorator\n        \"\"\"", "output": "\n        if self.request.user.is_authenticated():\n            return True\n        return False\n\n    def wath_login(self):\n        \"\"\"\n        'Private method', waths up the login process\n        \"\"\"\n        if self.request.user.is_authenticated():\n            return self.request.user.get_profile().wath_login()\n        else:\n            return False\n\n    def get_context_data(self, **kwargs):\n        \"\"\"\n        Returns context data used for template rendering\n        \"\"\"\n        context = super(LoginRequiredMixin, self).get_context_data(**kwargs)\n        context['login_url", "solution": "\n        last_attempt = self.get_last_failed_access_attempt()\n\n        if not last_attempt:\n            # create a new entry\n            user_access = self._FailedAccessAttemptModel(ip_address=self.ip)\n        elif last_attempt:\n            user_access = last_attempt\n\n        if self.request.method == 'POST':\n            if self.username is None:\n                raise DobermanImproperlyConfigured(\n                    \"Bad username form field, if you are using a custom field please configure: \"\n                    \"DOBERMAN_USERNAME_FORM_FIELD via settings.\"\n                )\n\n            if self.response.status_code != 302:\n\n                user_access.user_agent = self.request.META.get('HTTP_USER_AGENT', '<unknown user agent>')[:255]\n                user_access.username = self.username\n                user_access.failed_attempts += 1\n                user_access.params_get = self.request.GET\n                user_access.params_post = self.request.POST\n\n                if user_access.failed_attempts >= self.max_failed_attempts:\n                    user_access.is_locked = True\n\n                user_access.save()\n\n            elif self.response.status_code == 302 and not user_access.is_locked:\n                user_access.is_expired = True\n                user_access.save()\n\n        return user_access"}
{"prompt": "def note_content_data(note, role):\n    \"\"\"Return the data for content\n\n    :param note: the note that holds the data\n    :type note: :class:`jukeboxcore.djadapter.models.Note`\n    :param role: item data role\n    :type role: QtCore.Qt.ItemDataRole\n    :returns: data for the created date\n    :rtype: depending on role\n    :raises: None\n    \"\"\"", "output": "\n    if role == QtCore.Qt.DisplayRole:\n        return note.content.display_name\n    elif role == QtCore.Qt.EditRole:\n        return note.content.edit_description\n    elif role == QtCore.Qt.ActionRole:\n        return note_action_data(note)\n    elif role == QtCore.Qt.StatusTipRole:\n        return note_tip_data(note)\n    elif role == QtCore.Qt.ToolTipRole:\n        return note_content_data(note, role)\n    elif role == QtCore.Qt.UserRole:\n        return note_user_data(note)\n   ", "solution": "\n    if role == QtCore.Qt.DisplayRole or role == QtCore.Qt.EditRole:\n        return note.content"}
{"prompt": "def _prepare_aggregate(cls, *args, **kw):\n\t\t\"\"\"Generate and execute an aggregate query pipline using combined plain and parametric query generation.\n\t\t\n\t\tAdditionally, performs argument case normalization, refer to the `_prepare_query` method's docstring.\n\t\t\n\t\tThis provides a find-like interface for generating aggregate pipelines with a few shortcuts that make\n\t\taggregates behave more like \"find, optionally with more steps\". Positional arguments that are not Filter\n\t\tinstances are assumed to be aggregate pipeline stages.\n\t\t\n\t\thttps://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.aggregate\n\t\t\"\"\"", "output": "collection.html#pymongo.collection.Collection.aggregate\n\t\t\n\t\t:param args: Arguments to the aggregate query.\n\t\t:param kw: Keyword arguments to the aggregate query.\n\t\t:return: A list of dictionaries, one for each step in the pipeline, and the other for each step\n\t\t\"\"\"\n\t\treturn []\n\n\tdef _prepare_query(self, *args, **kw):\n\t\t\"\"\"Generate and execute a query pipline using combined plain and parametric query generation.\n\t\t\n\t\tAdditionally, performs argument case normalization, refer to the `_prepare_aggregate` method's docstring.\n\t\t\n\t\t:param", "solution": "\n\t\t\n\t\tstages = []\n\t\tstage_args = []\n\t\tfragments = []\n\t\t\n\t\tfor arg in args:  # Split the positional arguments into filter fragments and projection stages.\n\t\t\t(fragments if isinstance(arg, Filter) else stage_args).append(arg)\n\t\t\n\t\tcls, collection, query, options = cls._prepare_query(\n\t\t\t\tcls.AGGREGATE_MAPPING,\n\t\t\t\tcls.AGGREGATE_OPTIONS,\n\t\t\t\t*fragments,\n\t\t\t\t**kw\n\t\t\t)\n\t\t\n\t\tif query:\n\t\t\tstages.append({'$match': query})\n\t\t\n\t\tstages.extend(stage_args)\n\t\t\n\t\tif 'sort' in options:  # Convert the find-like option to a stage with the correct semantics.\n\t\t\tstages.append({'$sort': odict(options.pop('sort'))})\n\t\t\n\t\tif 'skip' in options:  # Note: Sort + limit memory optimization invalidated when skipping.\n\t\t\tstages.append({'$skip': options.pop('skip')})\n\t\t\n\t\tif 'limit' in options:\n\t\t\tstages.append({'$limit': options.pop('limit')})\n\t\t\n\t\tif 'projection' in options:\n\t\t\tstages.append({'$project': options.pop('projection')})\n\t\t\n\t\treturn cls, collection, stages, options"}
{"prompt": "def restore(name, run_path=None, replace=False, root=\".\"):\n    \"\"\" Downloads the specified file from cloud storage into the current run directory\n    if it doesn exist.\n\n    name: the name of the file\n    run_path: optional path to a different run to pull files from\n    replace: whether to download the file even if it already exists locally\n    root: the directory to download the file to.  Defaults to the current \n        directory or the run directory if wandb.init was called.\n\n    returns None if it can't find the file, otherwise a file object open for reading\n    raises wandb.CommError if it can't find the run\n    \"\"\"", "output": "andb.WandbError if the file could not be found.\n    \"\"\"\n    if run_path is None:\n        run_path = os.getcwd()\n    if os.path.exists(run_path + \"/\" + name):\n        return run_path + \"/\" + name\n    else:\n        raise wandb.WandbError(\"File \" + name + \" not found\")\n\ndef download(run_path=None, replace=False, root=\".\"):\n    \"\"\" Downloads the specified file from cloud storage into the current run directory\n    if it doesn exist.\n\n    run_path: optional path", "solution": "\n    if run_path is None and run is None:\n        raise ValueError(\n            \"You must call `wandb.init` before calling restore or specify a run_path\")\n    api = Api()\n    api_run = api.run(run_path or run.path)\n    root = run.dir if run else root\n    path = os.path.exists(os.path.join(root, name))\n    if path and replace == False:\n        return open(path, \"r\")\n    files = api_run.files([name])\n    if len(files) == 0:\n        return None\n    return files[0].download(root=root, replace=True)"}
{"prompt": "def batchccn(args):\n    \"\"\"\n    %prog batchccn test.csv\n\n    Run CCN script in batch. Write makefile.\n    \"\"\"", "output": "\n    if args.csv:\n        with open(args.csv, 'w') as f:\n            for row in test_data:\n                f.write(row + '\\n')\n        return\n\n    if args.test:\n        with open(args.test, 'w') as f:\n            for row in test_data:\n                row = row.strip()\n                if not row:\n                    continue\n                if row[0] == '#':\n                    continue\n                if row[0] == ';':\n                    row = row.split(';')\n                f.write(row[0] + '\\n')\n                f.write(row[", "solution": "\n    p = OptionParser(batchccn.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    csvfile, = args\n    mm = MakeManager()\n    pf = op.basename(csvfile).split(\".\")[0]\n    mkdir(pf)\n\n    header = next(open(csvfile))\n    header = None if header.strip().endswith(\".bam\") else \"infer\"\n    logging.debug(\"Header={}\".format(header))\n    df = pd.read_csv(csvfile, header=header)\n    cmd = \"perl /mnt/software/ccn_gcn_hg38_script/ccn_gcn_hg38.pl\"\n    cmd += \" -n {} -b {}\"\n    cmd += \" -o {} -r hg38\".format(pf)\n    for i, (sample_key, bam) in df.iterrows():\n        cmdi = cmd.format(sample_key, bam)\n        outfile = \"{}/{}/{}.ccn\".format(pf, sample_key, sample_key)\n        mm.add(csvfile, outfile, cmdi)\n    mm.write()"}
{"prompt": "def create_dev_cert(name, dest):\n    \"\"\"\n    A command to generate a self signed certificate for dev purposes.\n    :param name: name ro be given to the cert and key\n    :param dest: location on local filesystem to store the generated files\n    :return:\n    \"\"\"", "output": "\n    if not dest:\n        dest = os.path.join(name, \"cert.pem\")\n    if not os.path.exists(dest):\n        os.makedirs(dest)\n    cert = open(dest, \"w\")\n    cert.write(generate_cert(name, dest))\n    cert.close()\n\n\ndef generate_cert(name, dest):\n    \"\"\"\n    Generate a self signed certificate for dev purposes.\n    :param name: name ro be given to the cert and key\n    :param dest: location on local filesystem to store the generated files\n    :return:\n    \"\"\"\n    if not dest", "solution": "\n    if not os.path.exists(dest):\n        os.makedirs(dest)\n    keygen.gen_ca_cert(name, dest, 3650)"}
{"prompt": "def _capture_stdout(callable: Callable[..., CaptureResult], *args, **kwargs) -> CaptureResult:\n        \"\"\"\n        Captures content written to standard out.\n        :param callable: the callable to wrap\n        :param args: positional arguments passed to the callable\n        :param kwargs: keyword arguments passed to the callable\n        :return: the captured result\n        \"\"\"", "output": "\n        if callable.__name__ == \"get_stdout\":\n            return callable(args, **kwargs)\n        else:\n            return callable(args, **kwargs)\n\n    def _capture_stderr(\n        self,\n        callable: Callable[..., CaptureResult],\n        *args,\n        **kwargs: Any,\n    ) -> CaptureResult:\n        \"\"\"\n        Captures content written to standard err.\n        :param callable: the callable to wrap\n        :param args: positional arguments passed to the callable\n        :param kwargs: keyword arguments passed to the callable\n        :return: the captured result\n        \"\"\"\n        if callable.__name__ == \"get_", "solution": "\n        stream = StringIO()\n        with redirect_stdout(stream):\n            return_value = callable(*args, **kwargs)\n        assert return_value.stdout is None, \"stdout appears to have already been captured\"\n        return CaptureResult(return_value=return_value.return_value, stdout=stream.getvalue(),\n                             stderr=return_value.stderr, exception=return_value.exception)"}
{"prompt": "def create_eager_metrics_for_problem(problem, model_hparams):\n  \"\"\"See create_eager_metrics.\"\"\"", "output": "\n  metrics = {}\n  for name, metric in problem.metrics.items():\n    metrics[name] = metric.create_metric(\n        model_hparams,\n        metric.RegressionOutput(\n            loss=metric.loss,\n            metrics=metric.get_config(),\n            weighted_metrics=metric.get_weighted_metrics(),\n            sample_weight=metric.sample_weight,\n            masks=metric.get_config().get('mask_targets', None),\n            masks_interval=metric.get_config().get('mask_interval', None),\n            masks_target=name in mask_targets,\n           ", "solution": "\n  metric_fns = problem.eval_metric_fns(model_hparams)\n  problem_hparams = problem.get_hparams(model_hparams)\n  target_modality = problem_hparams.modality[\"targets\"]\n  weights_fn = model_hparams.weights_fn.get(\n      \"targets\",\n      modalities.get_weights_fn(target_modality))\n  return create_eager_metrics_internal(metric_fns, weights_fn=weights_fn)"}
{"prompt": "def Arrows(startPoints, endPoints=None, s=None, scale=1, c=\"r\", alpha=1, res=12):\n    \"\"\"\n    Build arrows between two lists of points `startPoints` and `endPoints`.\n    `startPoints` can be also passed in the form ``[[point1, point2], ...]``.\n    \n    Color can be specfied as a colormap which maps the size of the arrows.\n\n    :param float s: fix aspect-ratio of the arrow and scale its cross section\n    :param float scale: apply a rescaling factor to the length\n    :param c: color or array of colors\n    :param str cmap: color arrows by size using this color map\n    :param float alpha: set transparency\n    :param int res: set arrow resolution\n\n    .. hint:: |glyphs_arrow| |glyphs_arrow.py|_\n    \"\"\"", "output": " int startPoints: start point\n    :param int endPoints: end point\n    :param int startAngle: start angle\n    :param int endAngle: end angle\n    :param int startPoint: start point\n    :param int endPoint: end point\n    :param int startAngle: start angle\n    :param int endAngle: end angle\n    :param int res: resolution of the arrow\n    :return: list of arrow objects\n    :rtype: list\n    \"\"\"\n    if endPoints is None:\n        endPoints = [[0,0]]\n    if startPoints[0][0] == endPoints[0][0]:\n       ", "solution": "\n    startPoints = np.array(startPoints)\n    if endPoints is None:\n        strt = startPoints[:,0]\n        endPoints = startPoints[:,1]\n        startPoints = strt\n    \n    arr = vtk.vtkArrowSource()\n    arr.SetShaftResolution(res)\n    arr.SetTipResolution(res)\n    if s:\n        sz = 0.02 * s\n        arr.SetTipRadius(sz*2)\n        arr.SetShaftRadius(sz)\n        arr.SetTipLength(sz * 10)\n    arr.Update()\n    pts = Points(startPoints)\n    orients = (endPoints - startPoints) * scale\n    arrg = Glyph(pts, arr.GetOutput(), \n                 orientationArray=orients, scaleByVectorSize=True, \n                 c=c, alpha=alpha)\n    settings.collectable_actors.append(arrg)\n    return arrg"}
{"prompt": "def rasterize_pdf(\n    input_file,\n    output_file,\n    xres,\n    yres,\n    raster_device,\n    log,\n    pageno=1,\n    page_dpi=None,\n    rotation=None,\n    filter_vector=False,\n):\n    \"\"\"Rasterize one page of a PDF at resolution (xres, yres) in canvas units.\n\n    The image is sized to match the integer pixels dimensions implied by\n    (xres, yres) even if those numbers are noninteger. The image's DPI will\n     be overridden with the values in page_dpi.\n\n    :param input_file: pathlike\n    :param output_file: pathlike\n    :param xres: resolution at which to rasterize page\n    :param yres:\n    :param raster_device:\n    :param log:\n    :param pageno: page number to rasterize (beginning at page 1)\n    :param page_dpi: resolution tuple (x, y) overriding output image DPI\n    :param rotation: 0, 90, 180, 270: clockwise angle to rotate page\n    :param filter_vector: if True, remove vector graphics objects\n    :return:\n    \"\"\"", "output": " in the DPI.\n\n    Args:\n        input_file (str): The input PDF file.\n        output_file (str): The output PDF file.\n        xres (int): The x resolution of the image.\n        yres (int): The y resolution of the image.\n        raster_device (str): The name of the raster device.\n        log (logging.Logger): The logger to use for logging.\n        pageno (int): The number of pages to use. If None, use all pages.\n        page_dpi (int): The resolution of the page in dots per inch.\n        rotation (", "solution": "\n    res = xres, yres\n    int_res = round(xres), round(yres)\n    if not page_dpi:\n        page_dpi = res\n\n    with NamedTemporaryFile(delete=True) as tmp:\n        args_gs = (\n            [\n                'gs',\n                '-dQUIET',\n                '-dSAFER',\n                '-dBATCH',\n                '-dNOPAUSE',\n                f'-sDEVICE={raster_device}',\n                f'-dFirstPage={pageno}',\n                f'-dLastPage={pageno}',\n                f'-r{str(int_res[0])}x{str(int_res[1])}',\n            ]\n            + (['-dFILTERVECTOR'] if filter_vector else [])\n            + [\n                '-o',\n                tmp.name,\n                '-dAutoRotatePages=/None',  # Probably has no effect on raster\n                '-f',\n                fspath(input_file),\n            ]\n        )\n\n        log.debug(args_gs)\n        p = run(args_gs, stdout=PIPE, stderr=STDOUT, universal_newlines=True)\n        if _gs_error_reported(p.stdout):\n            log.error(p.stdout)\n        else:\n            log.debug(p.stdout)\n\n        if p.returncode != 0:\n            log.error('Ghostscript rasterizing failed')\n            raise SubprocessOutputError()\n\n        # Ghostscript only accepts integers for output resolution\n        # if the resolution happens to be fractional, then the discrepancy\n        # would change the size of the output page, especially if the DPI\n        # is quite low. Resize the image to the expected size\n\n        tmp.seek(0)\n        with Image.open(tmp) as im:\n            expected_size = (\n                round(im.size[0] / int_res[0] * res[0]),\n                round(im.size[1] / int_res[1] * res[1]),\n            )\n            if expected_size != im.size or page_dpi != (xres, yres):\n                log.debug(\n                    f\"Ghostscript: resize output image {im.size} -> {expected_size}\"\n                )\n                im = im.resize(expected_size)\n\n            if rotation is not None:\n                log.debug(\"Rotating output by %i\", rotation)\n                # rotation is a clockwise angle and Image.ROTATE_* is\n                # counterclockwise so this cancels out the rotation\n                if rotation == 90:\n                    im = im.transpose(Image.ROTATE_90)\n                elif rotation == 180:\n                    im = im.transpose(Image.ROTATE_180)\n                elif rotation == 270:\n                    im = im.transpose(Image.ROTATE_270)\n                if rotation % 180 == 90:\n                    page_dpi = page_dpi[1], page_dpi[0]\n            im.save(fspath(output_file), dpi=page_dpi)"}
{"prompt": "def _color_noise(x, s_freq, coef=0):\n    \"\"\"Add some color to the noise by changing the power spectrum.\n\n    Parameters\n    ----------\n    x : ndarray\n        one vector of the original signal\n    s_freq : int\n        sampling frequency\n    coef : float\n        coefficient to apply (0 -> white noise, 1 -> pink, 2 -> brown,\n                              -1 -> blue)\n\n    Returns\n    -------\n    ndarray\n        one vector of the colored noise.\n    \"\"\"", "output": "\n    return _color_power(x, s_freq, s_freq, s_freq, coef)\n\n\ndef _color_power(x, s_freq, s_freq, s_freq, coef=0):\n    \"\"\"Add some power to the power spectrum by changing the power spectrum.\n\n    Parameters\n    ----------\n    x : ndarray\n        one vector of the original signal\n    s_freq : int\n        sampling frequency\n    s_freq : int\n        power spectrum\n    coef : float\n        power coefficient to apply (0 -> white noise, 1 -> pink, 2 -> brown,\n                              -1 -> blue)", "solution": "\n    # convert to freq domain\n    y = fft(x)\n    ph = angle(y)\n    m = abs(y)\n\n    # frequencies for each fft value\n    freq = linspace(0, s_freq / 2, int(len(m) / 2) + 1)\n    freq = freq[1:-1]\n\n    # create new power spectrum\n    m1 = zeros(len(m))\n    # leave zero alone, and multiply the rest by the function\n    m1[1:int(len(m) / 2)] = m[1:int(len(m) / 2)] * f(freq, coef)\n    # simmetric around nyquist freq\n    m1[int(len(m1) / 2 + 1):] = m1[1:int(len(m1) / 2)][::-1]\n\n    # reconstruct the signal\n    y1 = m1 * exp(1j * ph)\n    return real(ifft(y1))"}
{"prompt": "def manifest_download(self):\n        \"\"\"download manifest files\"\"\"", "output": "\n        if self.config.get('update', False):\n            self.update_manifest()\n        else:\n            self.update_manifest_path()\n\n    def update_manifest_path(self):\n        \"\"\"update manifest file paths\"\"\"\n        self.manifest_path = os.path.join(self.config['destination'], 'dist','servoapp.json')\n        self.config['manifest_path'] = self.manifest_path\n\n    def get_log_path(self):\n        \"\"\"return path to the mozlog.txt\"\"\"\n        return self.config.get('path', os.path.join", "solution": "\n        if self.downloaders_lock.acquire(False):\n            if len(self.downloaders):\n                # there already exist downloader threads\n                self.downloaders_lock.release()\n                return\n\n            for url in ['http://firmware.ardupilot.org/manifest.json']:\n                filename = self.make_safe_filename_from_url(url)\n                path = mp_util.dot_mavproxy(\"manifest-%s\" % filename)\n                self.downloaders[url] = threading.Thread(target=self.download_url, args=(url, path))\n                self.downloaders[url].start()\n            self.downloaders_lock.release()\n        else:\n            print(\"fw: Failed to acquire download lock\")"}
{"prompt": "def create_bookmark_action(parent, url, title, icon=None, shortcut=None):\r\n    \"\"\"Create bookmark action\"\"\"", "output": "\r\n    return Action(parent, url, title, icon, shortcut)\r\n\r\n\r\nclass Action(object):\r\n    \"\"\"Action\"\"\"\r\n    def __init__(self, parent, url, title, icon=None, shortcut=None):\r\n        self.parent = parent\r\n        self.url = url\r\n        self.title = title\r\n        self.icon = icon\r\n        self.shortcut = shortcut\r\n        self.parent.add_action(create_action(parent, url, icon, shortcut))\r\n\r\n    def get_url(self):\r\n        \"\"\"Get URL\"\"\"\r\n        return self.url\r\n\r\n    def get_title(self):\r\n        \"\"\"Get title\"\"\"", "solution": "\r\n    \r\n    @Slot()\r\n    def open_url():\r\n        return programs.start_file(url)\r\n    \r\n    return create_action( parent, title, shortcut=shortcut, icon=icon,\r\n                          triggered=open_url)"}
{"prompt": "def _font_of_mention(m):\n    \"\"\"\n    Returns the font type and size of the first alphanumeric\n    char in the text or None if there isn't any.\n    \"\"\"", "output": "\n    if m.group(0) == \"a\":\n        return \"a\"\n    elif m.group(0) == \"i\":\n        return \"i\"\n    elif m.group(0) == \"u\":\n        return \"u\"\n    elif m.group(0) == \"s\":\n        return \"s\"\n    elif m.group(0) == \"t\":\n        return \"t\"\n    else:\n        return None, None\n\n\ndef _get_line_length(line):\n    \"\"\"\n    Returns the length of the line.\n    \"\"\"\n    if line.startswith(\"L\"):\n        return len", "solution": "\n    for ch in m:\n        if isinstance(ch, LTChar) and ch.get_text().isalnum():\n            return (ch.fontname, _font_size_of(ch))\n    return (None, 0)"}
{"prompt": "def listContents(self):\n        \"\"\" Return list of volumes or diffs in this Store's selected directory. \"\"\"", "output": "\n        return self.getSelectedVolumes()\n\n    def getSelectedVolumes(self):\n        \"\"\" Return list of selected files in this Store's selected directory. \"\"\"\n        return self.getSelectedFiles()\n\n    def getSelectedFiles(self):\n        \"\"\" Return list of selected files in this Store's selected directory. \"\"\"\n        return self.getSelectedFilesByType(self.getSelectedFilesByType(self.getSelectedFilesByDiff))\n\n    def getSelectedFilesByType(self, files):\n        \"\"\" Return list of files in this Store's selected directory. \"\"\"\n        return self.getSelectedFilesByType(files)\n\n    def", "solution": "\n        items = list(self.extraKeys.items())\n        items.sort(key=lambda t: t[1])\n\n        (count, size) = (0, 0)\n\n        for (diff, path) in items:\n            if path.startswith(\"/\"):\n                continue\n            yield str(diff)\n            count += 1\n            size += diff.size\n\n        yield \"TOTAL: %d diffs %s\" % (count, humanize(size))"}
{"prompt": "def compare_variant_type_plot(data):\n    \"\"\" Return HTML for the Variant Counts barplot \"\"\"", "output": "\n    return \"\"\"\n    <div class=\"barplot\" id=\"counts-bar\" style=\"width: 100%; height: 100%;\">\n        <div class=\"bar-group\">\n            <div class=\"bar-header\">\n                <div class=\"bar-group-header\">\n                    <div class=\"bar-group-title\">Counts</div>\n                </div>\n                <div class=\"bar-group-header\">\n                    <div class=\"bar-group-title\">Total</div>\n                </div>\n            </div>\n        </div>\n    </div>\n    \"\"\"\n\ndef compare_variant_", "solution": "\n    keys = OrderedDict()\n    keys['snps'] = {'name': 'SNPs', 'color': '#7cb5ec'}\n    keys['indels'] = {'name': 'InDels', 'color': '#90ed7d'}\n    keys['multiallelic_snps'] = {'name': 'multi-allelic SNP', 'color': 'orange'}\n    keys['complex_indels'] = {'name': 'Complex InDels', 'color': '#8085e9'}\n\n    total_variants = dict()\n    known_variants = dict()\n    novel_variants = dict()\n    for s_name, values in data.items():\n        total_variants[s_name] = {\n            'snps': values['TOTAL_SNPS'],\n            'indels': values['TOTAL_INDELS'],\n            'multiallelic_snps': values['TOTAL_MULTIALLELIC_SNPS'],\n            'complex_indels': values['TOTAL_COMPLEX_INDELS'],\n        }\n\n        known_variants[s_name] = {\n            'snps': values['NUM_IN_DB_SNP'],\n            'indels': int(values['TOTAL_INDELS'])-int(values['NOVEL_INDELS']),\n            'multiallelic_snps': values['NUM_IN_DB_SNP_MULTIALLELIC'],\n            'complex_indels': values['NUM_IN_DB_SNP_COMPLEX_INDELS'],\n        }\n\n        novel_variants[s_name] = {\n            'snps': values['NOVEL_SNPS'],\n            'indels': values['NOVEL_INDELS'],\n            'multiallelic_snps': int(values['TOTAL_MULTIALLELIC_SNPS'])-int(values['NUM_IN_DB_SNP_MULTIALLELIC']),\n            'complex_indels': int(values['TOTAL_COMPLEX_INDELS'])-int(values['NUM_IN_DB_SNP_COMPLEX_INDELS']),\n        }\n\n    plot_conf = {\n        'id': 'picard_variantCallingMetrics_variant_type',\n        'title': 'Picard: Variants Called',\n        'ylab': 'Counts of Variants',\n        'hide_zero_cats': False,\n        'data_labels': [\n            {'name': 'Total'},\n            {'name': 'Known'},\n            {'name': 'Novel'}\n        ],\n    }\n    return bargraph.plot(data=[total_variants, known_variants, novel_variants],\n                         cats=[keys, keys, keys],\n                         pconfig=plot_conf)"}
{"prompt": "def export_template(access_token, subscription_id, rgname):\n    \"\"\"Capture the specified resource group as a template\n\n    Args:\n        access_token (str): A valid Azure authentication token.\n        subscription_id (str): Azure subscription id.\n        rgname (str): Azure resource group name.\n\n    Returns:\n        HTTP response. JSON body.\n    \"\"\"", "output": "\n    return _get_request(access_token, 'exportTemplate', subscription_id, rgname)\n\n\ndef _get_request(access_token, operation, subscription_id, rgname):\n    \"\"\"Get a request object for the specified subscription\n\n    Args:\n        access_token (str): A valid Azure authentication token.\n        operation (str): The operation to execute.\n        subscription_id (str): Azure subscription id.\n        rgname (str): Azure resource group name.\n\n    Returns:\n        HTTP response.\n    \"\"\"\n    return _get_request_with_token(access_token, 'get',", "solution": "\n    endpoint = ''.join([get_rm_endpoint(),\n                        '/subscriptions/', subscription_id,\n                        '/resourcegroups/', rgname,\n                        '/exportTemplate',\n                        '?api-version=', RESOURCE_API])\n    rg_body = {'options':'IncludeParameterDefaultValue', 'resources':['*']}\n    body = json.dumps(rg_body)\n    return do_post(endpoint, body, access_token)"}
{"prompt": "def signal_alias_exists(alias: str) -> bool:\n        \"\"\"\n        Checks if signal alias exists.\n        :param alias: Signal alias.\n        :return:\n        \"\"\"", "output": "\n        return alias in self.signal_aliases\n\n    def get_signal_alias(self, signal_name: str) -> str:\n        \"\"\"\n        Gets signal alias.\n        :param signal_name: Signal alias name.\n        :return:\n        \"\"\"\n        return self.signal_aliases[signal_name]\n\n    def get_signal_alias_name(self, signal_name: str) -> str:\n        \"\"\"\n        Gets signal alias name.\n        :param signal_name: Signal alias name.\n        :return:\n        \"\"\"\n        return self.signal_aliases[signal_name]\n\n    def signal_alias_", "solution": "\n        if SignalDispatcher.signals.get(alias):\n            return True\n\n        return False"}
{"prompt": "def update_frame(self, key, ranges=None):\n        \"\"\"\n        Update the internal state of the Plot to represent the given\n        key tuple (where integers represent frames). Returns this\n        state.\n        \"\"\"", "output": "\n        if ranges is None:\n            ranges = self.plot_ranges\n        if key in self.frame_ranges:\n            self.frame_ranges[key].update(ranges)\n        else:\n            self.frame_ranges[key] = ranges\n        return self.frame_ranges[key].update(key)\n\n    def get_data(self, key):\n        \"\"\"\n        Return the data for the given key.\n        \"\"\"\n        return self.plot_data[key]\n\n    def get_data_range(self, key):\n        \"\"\"\n        Return the range for the given key.\n        \"\"\"\n        return self.plot", "solution": "\n        ranges = self.compute_ranges(self.layout, key, ranges)\n        for coord in self.layout.keys(full_grid=True):\n            subplot = self.subplots.get(wrap_tuple(coord), None)\n            if subplot is not None:\n                subplot.update_frame(key, ranges)\n        title = self._get_title_div(key)\n        if title:\n            self.handles['title']"}
{"prompt": "def _publish_status(self, slug, data):\n        \"\"\"Publish a status message for a device\n\n        Args:\n            slug (string): The device slug that we are publishing on behalf of\n            data (dict): The status message data to be sent back to the caller\n        \"\"\"", "output": "\n        self.device_status_changed.emit(slug, data)\n\n    def device_status_changed(self, slug, state):\n        \"\"\"Update the status message for a device\n\n        Args:\n            slug (string): The device slug that we are updating\n            state (dict): The new status message state\n            self.device_status_changed (str): The new status message data\n        \"\"\"\n        self._set_status(slug, state)\n        self.device_status_changed.emit(slug, data)\n\n    def _set_status(self, slug, state):\n        \"\"\"Set the message status\n\n        Args", "solution": "\n\n        status_topic = self.topics.prefix + 'devices/{}/data/status'.format(slug)\n\n        self._logger.debug(\"Publishing status message: (topic=%s) (message=%s)\", status_topic, str(data))\n        self.client.publish(status_topic, data)"}
{"prompt": "def short_timesince(date):\n    \"\"\"\n    A shorter version of Django's built-in timesince filter.\n    Selects only the first part of the returned string,\n    splitting on the comma.\n\n    Falls back on default Django timesince if it fails.\n\n    Example: 3 days, 20 hours becomes \"3 days\".\n\n    \"\"\"", "output": "\n    try:\n        return timesince(date)\n    except (ValueError, TypeError):\n        return date.replace(hour=0, minute=0, second=0, microsecond=0)\n\ndef timeuntil(date):\n    \"\"\"\n    A Django version of Django's built-in timeuntil filter.\n    Selects only the first part of the returned string,\n    splitting on the comma.\n\n    Falls back on default Django timesuntil if it fails.\n\n    Example: 3 days, 20 hours becomes \"3 days\".\n\n    \"\"\"\n    try:\n        return timesince(date, time_zone=pytz.utc)\n   ", "solution": "\n    try:\n        t = timesince(date).split(\", \")[0]\n    except IndexError:\n        t = timesince(date)\n    return t"}
{"prompt": "def _clob_end_handler_factory():\n    \"\"\"Generates the handler for the end of a clob value. This includes anything from the data's closing quote through\n    the second closing brace.\n    \"\"\"", "output": "\n    def _end_handler(value):\n        return value.replace('\\\\\\\\', '\\\\').replace('\\\\\"', '\"')\n    return _end_handler_factory\n\n\nclass Clob(object):\n    \"\"\"\n    A wrapper for the CLOB type.\n    \"\"\"\n    def __init__(self, data):\n        self.data = data\n\n    def __call__(self, name, *args, **kwargs):\n        return self.data.get_value(name, *args, **kwargs)\n\n    def __getattr__(self, name):\n        return getattr(self.data, name)\n\n    def __getitem__(self, key", "solution": "\n    def action(c, ctx, prev, res, is_first):\n        if is_first and ctx.is_self_delimiting and c == _DOUBLE_QUOTE:\n            assert c is prev\n            return res\n        _illegal_character(c, ctx)\n\n    return _lob_end_handler_factory(IonType.CLOB, action)"}
{"prompt": "def data(self, column, role):\n        \"\"\"Return the data for the specified column and role\n\n        Column 0: The name of the action\n        Column 1: The description of the action\n        Column 2: The status value\n        Column 3: The status message\n        Column 4: The traceback\n\n        :param column: the data column\n        :type column: int\n        :param role: the data role\n        :type role: QtCore.Qt.ItemDataRole\n        :returns: data depending on the role, or None if the column is out of range\n        :rtype: depending on the role or None\n        :raises: None\n        \"\"\"", "output": "\n        if role == QtCore.Qt.DisplayRole:\n            return self.display_data[0]\n        elif role == QtCore.Qt.EditRole:\n            return self.display_data[1]\n        elif role == QtCore.Qt.ToolTipRole:\n            return self.display_data[2]\n        elif role == QtCore.Qt.WhatsThisRole:\n            return self.display_data[3]\n        elif role == QtCore.Qt.WhatsThisRole:\n            return self.display_data[4]\n        else:\n            raise ValueError(\"Invalid role\")\n\n    def display_data_item(self", "solution": "\n        if role == QtCore.Qt.DisplayRole:\n            if column == 0:\n                return self._au.name\n            if column == 1:\n                return self._au.description\n            if column == 2:\n                return self._au.status.value\n            if column == 3:\n                return self._au.status.message\n            if column == 4:\n                return self._au.status.traceback\n\n        if role == QtCore.Qt.ForegroundRole:\n            if column == 2:\n                return self.colormapping.get(self._au.status.value)"}
{"prompt": "def meta(self, meta):\r\n        \"\"\"Extract model metadata for lua script stdnet/lib/lua/odm.lua\"\"\"", "output": "\r\n        if self.version >= (4,0,0):\r\n            self.version = 4\r\n            self.lua_script = lua_script\r\n        else:\r\n            self.version = 6\r\n            self.lua_script = lua_script\r\n        self.lua_script_path = self.lua_script.split(\";\")\r\n        self.version_info = self.version_info[0:self.version]\r\n        self.lua_script_path.reverse()\r\n        self.lua_script_path = self.lua_script_path[::-1]\r\n        self.l", "solution": "\r\n        data = meta.as_dict()\r\n        data['namespace'] = self.basekey(meta)\r\n        return data"}
{"prompt": "def pick(self, *args):\n        \"\"\"\n        Return a copy of the object only containing the\n        whitelisted properties.\n        \"\"\"", "output": "\n        return self.__class__(self.name, self.value, self.type, self.default, self.required, self.default_from_file, self.default_from_file_location, *args)\n\n    def __repr__(self):\n        return \"PickledObject(name=%s, value=%s, type=%s, default=%s, required=%s, default_from_file=%s, default_from_line=%s)\" % (\n            self.name, self.value, self.type, self.default, self.required, self.default_from_file,", "solution": "\n        ns = self.Namespace()\n        ns.result = {}\n\n        def by(key, *args):\n            if key in self.obj:\n                ns.result[key] = self.obj[key]\n\n        _.each(self._flatten(args, True, []), by)\n        return self._wrap(ns.result)"}
{"prompt": "def clock(rpc):\n    \"\"\"\n    This task runs forever and notifies all clients subscribed to\n    'clock' once a second.\n    \"\"\"", "output": "\n    global _client_clock\n    _client_clock = None\n    return rpc.call('clock')\n\n\ndef get_client_clock():\n    \"\"\"\n    Return the current task's client clock.\n    \"\"\"\n    global _client_clock\n    if _client_clock is None:\n        _client_clock = clock(get_client_client)\n    return _client_clock\n\n\ndef get_client_client():\n    \"\"\"\n    Return the current task's client client.\n    \"\"\"\n    global _client_client\n    if _client_client is None:\n        _client_client = get_client_clock()\n       ", "solution": "\n\n    while True:\n        yield from rpc.notify('clock', str(datetime.datetime.now()))\n        yield from asyncio.sleep(1)"}
{"prompt": "def suppress_keyboard_interrupt_message():\n    \"\"\"Register a new excepthook to suppress KeyboardInterrupt\n    exception messages, and exit with status code 130.\n\n    \"\"\"", "output": "\n    sys.excepthook = _sys.excepthook\n\ndef enable_automatic_gc():\n    \"\"\"Register an automatic garbage collector to be used by Psyco.\n\n    \"\"\"\n    _sys.set_automatic_gc(True)\n\ndef disable_automatic_gc():\n    \"\"\"Register an automatic garbage collector to be used by Psyco.\n\n    \"\"\"\n    _sys.set_automatic_gc(False)\n\ndef enable_automatic_gc_after_fork():\n    \"\"\"Register an automatic garbage collector to be used by Psyco.\n\n    \"\"\"\n    _sys.set_automatic", "solution": "\n    old_excepthook = sys.excepthook\n\n    def new_hook(type, value, traceback):\n        if type != KeyboardInterrupt:\n            old_excepthook(type, value, traceback)\n        else:\n            sys.exit(130)\n\n    sys.excepthook = new_hook"}
{"prompt": "def getDownloadUrls(self):\n\t\t\"\"\"Return a list of the urls to download from\"\"\"", "output": "\n\t\treturn self.downloadUrls\n\n\tdef getDownloadSize(self):\n\t\t\"\"\"Return the size of the file in bytes\"\"\"\n\t\treturn self.downloadSize\n\n\tdef getDownloadUrl(self, url):\n\t\t\"\"\"Return the url to download\"\"\"\n\t\treturn url.replace(self.getDownloadUrls(), '')\n\n\tdef getDownloadSize(self, size):\n\t\t\"\"\"Return the size of the file in bytes\"\"\"\n\t\treturn size\n\n\tdef getDownloadUrlAndSize(self, url, size):\n\t\t\"\"\"Return the url to download\"\"\"\n\t\treturn self.get", "solution": "\n\t\tdata = self.searchIndex(False)\n\t\tfileUrls = []\n\t\tfor datum in data:\n\t\t\tfileUrl = self.formatDownloadUrl(datum[0])\n\t\t\tfileUrls.append(fileUrl)\n\t\treturn fileUrls"}
{"prompt": "def add_context(self, isc_name, isc_policy_id, isc_traffic_tag):\n        \"\"\"\n        Create the VSS Context within the VSSContainer\n\n        :param str isc_name: ISC name, possibly append policy name??\n        :param str isc_policy_id: Policy ID in SMC (the 'key' attribute)\n        :param str isc_traffic_tag: NSX groupId (serviceprofile-145)\n        :raises CreateElementFailed: failed to create\n        :return: VSSContext\n        \"\"\"", "output": "\n        if not self.is_valid_iscs_xml_tag(iscs_name):\n            raise ValueError(\"Invalid ISC name %s\" % iscs_name)\n        if not self.is_valid_isc_xml_tag(isc_name):\n            raise ValueError(\"Invalid ISC name %s\" % isc_name)\n        if not self.is_valid_isc_xml_tag(isc_name):\n            raise ValueError(\"Invalid ISC name %s\" % isc_name)\n        if not self.is_valid_isc_xml_", "solution": "\n        if 'add_context' in self.data.links: # SMC >=6.5\n            element = ElementCreator(\n                VSSContext,\n                href=self.get_relation('add_context'),\n                json = {\n                    'name': isc_name,\n                    'vc_isc': {\n                        'isc_name': isc_name,\n                        'isc_policy_id': isc_policy_id,\n                        'isc_traffic_tag': isc_traffic_tag\n                    }\n                })\n        else: # SMC < 6.5\n            element = VSSContext.create(\n                isc_name=isc_name,\n                isc_policy_id=isc_policy_id,\n                isc_traffic_tag=isc_traffic_tag,\n                vss_container=self)\n        \n        # Delete cache since the virtualResources node is attached to\n        # the engine json\n        self._del_cache()\n        return element"}
{"prompt": "def _edges_to_adjacency_list(edges):\n    \"\"\"Convert a list of edges into an adjacency list.\"\"\"", "output": "\n    return [tuple(sorted(e)) for e in edges]\n\n\ndef _get_all_possible_edges(G, source):\n    \"\"\"Return all possible edges in the graph from source to all nodes in G.\n\n    Parameters\n    ----------\n    G : NetworkX graph\n\n    source : node\n       Starting node for path.\n\n    Returns\n    -------\n    adjacency_list : iterator\n       A postorder iterator over all possible edges in the graph.\n\n    Examples\n    --------\n    >>> G = nx.path_graph(5)\n    >>> for n in G:\n   ...     print(n, n.edges())\n    (0", "solution": "\n    adj = {}\n    for i, j in edges:\n        if i in adj:  # pragma: no cover\n            ni = adj[i]\n        else:\n            ni = adj[i] = set()\n        if j in adj:\n            nj = adj[j]\n        else:\n            nj = adj[j] = set()\n        ni.add(j)\n        nj.add(i)\n    return adj"}
{"prompt": "def display_molecule(mol, style='ball-and-stick'):\n    \"\"\"Display the molecule *mol* with the default viewer.\n\n    \"\"\"", "output": "\n    if style =='stick':\n        print(style_to_string(style))\n    elif style == 'ball':\n        print(style_to_string(style))\n    elif style =='stick':\n        print('stick')\n    elif style == 'ball-and-stick':\n        print('ball-and-stick')\n    elif style == 'ball-and-stick-default':\n        style_to_string(style)\n    else:\n        raise ValueError('Unknown molecule viewer style: %s' % (style))\n\n\ndef display_molecule_default(mol, style='ball-and-stick'):", "solution": "\n    v = QtViewer()\n\n    \n    if style == 'ball-and-stick':\n        bs = v.add_renderer(BallAndStickRenderer,\n                            mol.r_array,\n                            mol.type_array,\n                            mol.bonds)\n    elif style == 'vdw':\n        sr = v.add_renderer(AtomRenderer, mol.r_array, mol.type_array,\n                            backend='impostors')\n    else:\n        raise Exception(\"Rendering style unknown\")\n    \n    v.widget.camera.autozoom(mol.r_array)\n    v.run()"}
{"prompt": "def get_ip(source='aws'):\n    \n    \"\"\" a method to get current public ip address of machine \"\"\"", "output": "\n    \n    # get current public ip\n    try:\n        return boto.ec2.get_public_ip(source)\n    except boto.exception.EC2ResponseError, e:\n        return None\n\ndef get_instance_metadata(instance):\n    \"\"\" a method to get instance metadata from an instance \"\"\"\n    \n    # get instance metadata\n    try:\n        return boto.ec2.get_instance_metadata(instance['InstanceId'])\n    except boto.exception.BotoServerError, e:\n        return None\n\ndef main():\n    \"\"\" main \"\"\"\n    \n    # get command line arguments\n    parser = argparse.ArgumentParser(description='Create a", "solution": "\n    \n    if source == 'aws':\n        source_url = 'http://checkip.amazonaws.com/'\n    else:\n        raise Exception('get_ip currently only supports queries to aws')\n    \n    import requests\n    try:\n        response = requests.get(url=source_url)\n    except Exception as err:\n        from labpack.handlers.requests import handle_requests\n        from requests import Request\n        request_object = Request(method='GET', url=source_url)\n        request_details = handle_requests(request_object)\n        raise Exception(request_details['error'])\n    current_ip = response.content.decode()\n    current_ip = current_ip.strip()\n    \n    return current_ip"}
{"prompt": "def bigtable_users(self):\n        \"\"\"Access to bigtable.user role memebers\n\n        For example:\n\n        .. literalinclude:: snippets.py\n            :start-after: [START bigtable_users_policy]\n            :end-before: [END bigtable_users_policy]\n        \"\"\"", "output": "\n        return self.service.bigtable_client.list_user_policies(\n            request={\"name\": self.full_name},\n            project_id=self.project_id,\n            zone=self.zone,\n            cluster_id=self.cluster_id,\n            table_id=self.table_id,\n            max_results=self.max_results,\n            page_token=self.page_token,\n            start_index=self.start_index,\n            end_index=self.end_index,\n            retry=self.retry,\n            timeout=self.timeout,\n            metadata={\"", "solution": "\n        result = set()\n        for member in self._bindings.get(BIGTABLE_USER_ROLE, ()):\n            result.add(member)\n        return frozenset(result)"}
{"prompt": "def TemporaryDirectory(suffix=None, prefix=None, dir=None, on_error='ignore'):  # @ReservedAssignment\n    \"\"\"\n    An extension to `tempfile.TemporaryDirectory`.\n\n    Unlike with `python:tempfile`, a :py:class:`~pathlib.Path` is yielded on\n    ``__enter__``, not a `str`.\n\n    Parameters\n    ----------\n    suffix : str\n        See `tempfile.TemporaryDirectory`.\n    prefix : str\n        See `tempfile.TemporaryDirectory`.\n    dir : ~pathlib.Path\n        See `tempfile.TemporaryDirectory`, but pass a :py:class:`~pathlib.Path` instead.\n    on_error : str\n        Handling of failure to delete directory (happens frequently on NFS), one of:\n\n        raise\n            Raise exception on failure.\n        ignore\n            Fail silently.\n    \"\"\"", "output": "\n        object instead of a string.\n    on_error : str\n        See `python:os.listdir` for details.\n\n    Yields\n    ------\n    :py:class:`~pathlib.Path`\n        The yielded path.\n    \"\"\"\n    if suffix is None:\n        yield from tempfile.TemporaryDirectory(dir=dir, suffix=suffix, prefix=prefix)\n    else:\n        yield from tempfile.TemporaryDirectory(dir=dir, suffix=suffix, prefix=prefix)\n\n\ndef get_data_path(name):\n    \"\"\"\n    Returns the absolute path to the data directory.\n\n    Parameters\n    ----------\n    name : str", "solution": "\n    if dir:\n        dir = str(dir)  # @ReservedAssignment\n    temp_dir = tempfile.TemporaryDirectory(suffix, prefix, dir)\n    try:\n        yield Path(temp_dir.name)\n    finally:\n        try:\n            temp_dir.cleanup()\n        except OSError as ex:\n            print(ex)\n            # Suppress relevant errors if ignoring failed delete\n            if on_error != 'ignore' or ex.errno != errno.ENOTEMPTY:\n                raise"}
{"prompt": "def variable_device(device, name):\n  \"\"\"Fix the variable device to colocate its ops.\"\"\"", "output": "\n  return device\n\n\ndef _is_indexed_slices_list(inputs):\n  \"\"\"Returns whether any tensor in `inputs` is IndexedSlices.\"\"\"\n  for input_tensor in inputs:\n    if isinstance(input_tensor, ops.Tensor):\n      if input_tensor.dense_shape is not None:\n        return False\n      if input_tensor.indices is not None:\n        return True\n  return True\n\n\ndef _check_data_types(inputs, expected_types):\n  \"\"\"Asserts tensors are all the expected types.\"\"\"\n  # Note: not using sparse_to_dense since these are expected to be", "solution": "\n  if callable(device):\n    var_name = tf.get_variable_scope().name + '/' + name\n    var_def = tf.NodeDef(name=var_name, op='Variable')\n    device = device(var_def)\n  if device is None:\n    device = ''\n  return device"}
{"prompt": "def register_endpoints(self, backend_names):\n        \"\"\"\n        See super class satosa.frontends.base.FrontendModule\n        :type backend_names: list[str]\n        :rtype: list[(str, ((satosa.context.Context, Any) -> satosa.response.Response, Any))]\n        :raise ValueError: if more than one backend is configured\n        \"\"\"", "output": "\n        self.backend_names = backend_names\n        return self.backend_names\n\n    def get_backend_names(self):\n        \"\"\"\n        Returns the list of available SATOS backends.\n        :rtype: list[str]\n        \"\"\"\n        return self.backend_names\n\n    def get_response_type(self):\n        \"\"\"\n        Returns the response type.\n        :rtype: str\n        \"\"\"\n        return self.response_type\n\n    def get_response_types(self):\n        \"\"\"\n        Returns the list of available SATOS responses.\n        :rtype: list[str]\n        \"\"\"\n        return self.response_", "solution": "\n        backend_name = None\n        if len(backend_names) != 1:\n            # only supports one backend since there currently is no way to publish multiple authorization endpoints\n            # in configuration information and there is no other standard way of authorization_endpoint discovery\n            # similar to SAML entity discovery\n            # this can be circumvented with a custom RequestMicroService which handles the routing based on something\n            # in the authentication request\n            logger.warn(\"More than one backend is configured, make sure to provide a custom routing micro service to \"\n                        \"determine which backend should be used per request.\")\n        else:\n            backend_name = backend_names[0]\n\n        endpoint_baseurl = \"{}/{}\".format(self.base_url, self.name)\n        self._create_provider(endpoint_baseurl)\n\n        provider_config = (\"^.well-known/openid-configuration$\", self.provider_config)\n        jwks_uri = (\"^{}/jwks$\".format(self.name), self.jwks)\n\n        if backend_name:\n            # if there is only one backend, include its name in the path so the default routing can work\n            auth_endpoint = \"{}/{}/{}/{}\".format(self.base_url, backend_name, self.name, AuthorizationEndpoint.url)\n            self.provider.configuration_information[\"authorization_endpoint\"] = auth_endpoint\n            auth_path = urlparse(auth_endpoint).path.lstrip(\"/\")\n        else:\n            auth_path = \"{}/{}\".format(self.name, AuthorizationEndpoint.url)\n        authentication = (\"^{}$\".format(auth_path), self.handle_authn_request)\n        url_map = [provider_config, jwks_uri, authentication]\n\n        if any(\"code\" in v for v in self.provider.configuration_information[\"response_types_supported\"]):\n            self.provider.configuration_information[\"token_endpoint\"] = \"{}/{}\".format(endpoint_baseurl,\n                                                                                       TokenEndpoint.url)\n            token_endpoint = (\"^{}/{}\".format(self.name, TokenEndpoint.url), self.token_endpoint)\n            url_map.append(token_endpoint)\n\n            self.provider.configuration_information[\"userinfo_endpoint\"] = \"{}/{}\".format(endpoint_baseurl,\n                                                                                          UserinfoEndpoint.url)\n            userinfo_endpoint = (\"^{}/{}\".format(self.name, UserinfoEndpoint.url), self.userinfo_endpoint)\n            url_map.append(userinfo_endpoint)\n        if \"registration_endpoint\" in self.provider.configuration_information:\n            client_registration = (\"^{}/{}\".format(self.name, RegistrationEndpoint.url), self.client_registration)\n            url_map.append(client_registration)\n\n        return url_map"}
{"prompt": "def user_lookup(self, cloudflare_email=None, unique_id=None):\n        \"\"\"\n        Lookup user data based on either his cloudflare_email or his\n        unique_id.\n\n        :param    cloudflare_email: email associated with user\n        :type     cloudflare_email: str\n        :param    unique_id:        unique id associated with user\n        :type     unique_id:        str\n\n        :returns:\n        :rtype:   dict\n        \"\"\"", "output": "\n        if not cloudflare_email and not unique_id:\n            raise ValueError(\"One of 'cloudflare_email' or 'unique_id' must be specified\")\n        elif not cloudflare_email and unique_id:\n            raise ValueError(\"One of 'cloudflare_email' or 'unique_id' must be specified\")\n        elif not cloudflare_email and not cloudflare_email:\n            raise ValueError(\"One of 'cloudflare_email' or 'cloudflare_email' must be specified\")\n        elif not cloudflare_email and not cloudflare_", "solution": "\n        if not cloudflare_email and not unique_id:\n            raise KeyError(\n                'Either cloudflare_email or unique_id must be present')\n\n        params = {'act': 'user_lookup'}\n        if cloudflare_email:\n            params['cloudflare_email'] = cloudflare_email\n        else:\n            params['unique_id'] = unique_id\n\n        return self._request(params)"}
{"prompt": "def parse_hicup_logs(self, f):\n        \"\"\" Parse a HiCUP summary report \"\"\"", "output": "\n        self.hicup_logs = []\n        for line in f:\n            if line.startswith('H'):\n                self.hicup_logs.append(HiCUPSummaryReport(line))\n            elif line.startswith('I'):\n                self.hicup_logs.append(IlluminaSummaryReport(line))\n        return self.hicup_logs\n\n    def parse_hicup_report(self, f):\n        \"\"\" Parse a HiCUP report \"\"\"\n        self.hicup_report = None\n        for line in f:\n            if line.startswith('H'):\n                self.hicup", "solution": "\n        if not f['fn'].endswith('.txt'):\n            return None\n        header = []\n        lines = f['f'].splitlines()\n        for l in lines:\n            s = l.split(\"\\t\")\n            if len(header) == 0:\n                if s[0] != 'File':\n                    return None\n                header = s[1:]\n            else:\n                s_name = self.clean_s_name(s[0], f['root']).lstrip('HiCUP_output/')\n                parsed_data = {}\n                for idx, num in enumerate(s[1:]):\n                    try:\n                        parsed_data[header[idx]] = float(num)\n                    except:\n                        parsed_data[header[idx]] = num\n                parsed_data['Duplicate_Read_Pairs'] = parsed_data['Valid_Pairs'] - parsed_data['Deduplication_Read_Pairs_Uniques']\n                if s_name in self.hicup_data:\n                    log.debug(\"Duplicate sample name found! Overwriting: {}\".format(s_name))\n                self.add_data_source(f, s_name)\n                self.hicup_data[s_name] = parsed_data"}
{"prompt": "def check_repeated_exclamations(text):\n    \"\"\"Check the text.\"\"\"", "output": "\n    return text.count(r\"(\")!= text.count(\")\")\n\n\ndef check_no_exclamation(text):\n    \"\"\"Check the text.\"\"\"\n    return text.count(r\"(\") == text.count(\")\")\n\n\ndef check_no_space(text):\n    \"\"\"Check the text.\"\"\"\n    return text.count(\" \") == 0\n\n\ndef check_space(text):\n    \"\"\"Check the text.\"\"\"\n    return text.count(\"(\") == text.count(\"(\")\n\n\ndef check_space_before_and_after(text):\n    \"\"\"Check the text.\"\"\"", "solution": "\n    err = \"leonard.hell\"\n    msg = u\"Never use the words 'all hell broke loose'.\"\n\n    regex = r\"all hell broke loose\"\n\n    return existence_check(\n        text, [regex], err, msg, max_errors=1)"}
{"prompt": "def write_publication(self, values):\n        \"\"\"\n        Write publication info to db\n\n        Parameters\n        ----------\n        values: dict with entries\n            {'pub_id': str (short name for publication),\n            'authors': list of str ()\n            'journal': str,\n            'volume': str,\n            'number': str,\n            'pages': 'str'\n            'year': int,\n            'publisher': str,\n            'doi': str,\n            'tags': list of str}\n        \"\"\"", "output": "\n        if not self.is_valid_publication_id(values['pub_id']):\n            raise ValueError('Invalid publication id: %s' % values['pub_id'])\n\n        self.db.execute('UPDATE books SET title = %s, authors = %s, volume = %s, number = %s, pages = %s, year = %s, publisher = %s, doi = %s, tags = %s WHERE id = %s', values['title'], values['authors'], values['volume'], values['number'], values['pages'], values['number'], self.id)\n        self", "solution": "\n        con = self.connection or self._connect()\n        self._initialize(con)\n        cur = con.cursor()\n\n        values = (values['pub_id'],\n                  values['title'],\n                  json.dumps(values['authors']),\n                  values['journal'],\n                  values['volume'],\n                  values['number'],\n                  values['pages'],\n                  values['year'],\n                  values['publisher'],\n                  values['doi'],\n                  json.dumps(values['tags']))\n\n        q = self.default + ',' + ', '.join('?' * len(values))\n        cur.execute('INSERT OR IGNORE INTO publication VALUES ({})'.format(q),\n                    values)\n\n        pid = self.get_last_id(cur, table='publication')\n\n        if self.connection is None:\n            con.commit()\n            con.close()\n\n        return pid"}
{"prompt": "def _set_intf_router_isis(self, v, load=False):\n    \"\"\"\n    Setter method for intf_router_isis, mapped from YANG variable /routing_system/interface/loopback/ip/intf_router_isis (container)\n    If this variable is read-only (config: false) in the\n    source YANG file, then _set_intf_router_isis is considered as a private\n    method. Backends looking to populate this variable should\n    do so via calling thisObj._set_intf_router_isis() directly.\n    \"\"\"", "output": "\n    if hasattr(v, \"_utype\"):\n        v = v._utype(v)\n    try:\n        t = YANGDynClass(\n            v,\n            base=container_isis_map_entry.container_isis_map_entry,\n            is_container=\"container\",\n            yang_name=\"isis\",\n            parent=self,\n            path_helper=self._path_helper,\n            extmethods=self._extmethods,\n            register_paths=True,\n            namespace=\"http://openconfig.net/yang/network-instance\",\n            defining_module=\"openconfig-network-instance\",\n            yang_", "solution": "\n    if hasattr(v, \"_utype\"):\n      v = v._utype(v)\n    try:\n      t = YANGDynClass(v,base=intf_router_isis.intf_router_isis, is_container='container', presence=False, yang_name=\"intf-router-isis\", rest_name=\"\", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'cli-drop-node-name': None, u'callpoint': u'IsisLoopbackInterfaceIpRouter'}}, namespace='urn:brocade.com:mgmt:brocade-isis', defining_module='brocade-isis', yang_type='container', is_config=True)\n    except (TypeError, ValueError):\n      raise ValueError({\n          'error-string': "}
{"prompt": "def transformToNative(obj):\n        \"\"\"\n        Turn obj.value into a list of dates, datetimes, or\n        (datetime, timedelta) tuples.\n        \"\"\"", "output": "\n        if isinstance(obj, (datetime, timedelta)):\n            return list(obj)\n        elif isinstance(obj, (list, tuple)):\n            return map(transformToNative, obj)\n        else:\n            return obj\n\n    def transformToNative(data):\n        \"\"\"\n        Turn data.value into a list of dates, datetimes, or\n        (datetime, timedelta) tuples.\n        \"\"\"\n        if isinstance(data, datetime):\n            return list(data)\n        elif isinstance(data, timedelta):\n            return map(transformToNative, data)\n        else:\n            return data\n\n    def transformToNativeList(data):\n        \"\"\"\n       ", "solution": "\n        if obj.isNative:\n            return obj\n        obj.isNative = True\n        if obj.value == '':\n            obj.value = []\n            return obj\n        tzinfo = getTzid(getattr(obj, 'tzid_param', None))\n        valueParam = getattr(obj, 'value_param', \"DATE-TIME\").upper()\n        valTexts = obj.value.split(\",\")\n        if valueParam == \"DATE\":\n            obj.value = [stringToDate(x) for x in valTexts]\n        elif valueParam == \"DATE-TIME\":\n            obj.value = [stringToDateTime(x, tzinfo) for x in valTexts]\n        elif valueParam == \"PERIOD\":\n            obj.value = [stringToPeriod(x, tzinfo) for x in valTexts]\n        return obj"}
{"prompt": "def _from_signer_and_info(cls, signer, info, **kwargs):\n        \"\"\"Creates a Credentials instance from a signer and service account\n        info.\n\n        Args:\n            signer (google.auth.crypt.Signer): The signer used to sign JWTs.\n            info (Mapping[str, str]): The service account info.\n            kwargs: Additional arguments to pass to the constructor.\n\n        Returns:\n            google.auth.jwt.Credentials: The constructed credentials.\n\n        Raises:\n            ValueError: If the info is not in the expected format.\n        \"\"\"", "output": "\n        if not isinstance(info, dict):\n            raise ValueError('info must be a mapping mapping str to str '\n                             'or a mapping mapping str to str')\n\n        return cls(\n            signer._credentials,\n            **info\n        )\n\n    @classmethod\n    def from_signing_credentials(cls, info, **kwargs):\n        \"\"\"Creates a Credentials instance from a JWT signing credentials\n        info.\n\n        Args:\n            info (Mapping[str, str]): The service account info.\n            kwargs: Additional arguments to pass to the constructor.\n\n        Returns:\n            google.auth.jwt.credentials.Credentials: The constructed credentials.\n\n        Raises", "solution": "\n        kwargs.setdefault('subject', info['client_email'])\n        kwargs.setdefault('issuer', info['client_email'])\n        return cls(signer, **kwargs)"}
{"prompt": "def git_check():\n    \"\"\"\n    Check that all changes , besides versioning files, are committed\n    :return:\n    \"\"\"", "output": "\n    if not os.path.exists(GIT_DIR):\n        return False\n    if not os.path.isdir(GIT_DIR):\n        return False\n    if not os.access(GIT_DIR, os.W_OK):\n        return False\n    return True\n\n\ndef git_clone(git_url, target_dir):\n    \"\"\"\n    Clone a git repository\n    :param git_url:\n    :param target_dir:\n    :return:\n    \"\"\"\n    if not os.path.exists(git_url):\n        return False\n    if not os.path.isdir(target_dir):\n       ", "solution": "\n\n    # check that changes staged for commit are pushed to origin\n    output = local('git diff --name-only | egrep -v \"^(pynb/version.py)|(version.py)$\" | tr \"\\\\n\" \" \"',\n                   capture=True).strip()\n    if output:\n        fatal('Stage for commit and commit all changes first: {}'.format(output))\n\n    output = local('git diff --cached --name-only | egrep -v \"^(pynb/version.py)|(version.py)$\" | tr \"\\\\n\" \" \"',\n                   capture=True).strip()\n    if output:\n        fatal('Commit all changes first: {}'.format(output))"}
{"prompt": "def nohtml(s):\n    \"\"\"Return copy of ``s`` that will not treat ``'<...>'`` as DOT HTML string in quoting.\n\n    Args:\n        s: String in which leading ``'<'`` and trailing ``'>'`` should be treated as literal.\n    Raises:\n        TypeError: If ``s`` is not a ``str`` on Python 3, or a ``str``/``unicode`` on Python 2.\n\n    >>> quote('<>-*-<>')\n    '<>-*-<>'\n\n    >>> quote(nohtml('<>-*-<>'))\n    '\"<>-*-<>\"'\n    \"\"\"", "output": "\n    if isinstance(s, unicode):\n        return s.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')\n    elif isinstance(s, bytes):\n        return s.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')\n    else:\n        raise TypeError(\"Expected str or bytes, got %r\" % (type(s),))\n\ndef unescape(s):\n    \"\"\"Return string with unescaped HTML entities removed.\n\n    Args:\n        s: String with HTML entities removed.\n    Raises:\n        TypeError: If ``s`` is not", "solution": "\n    try:\n        subcls = NOHTML[type(s)]\n    except KeyError:\n        raise TypeError('%r does not have one of the required types: %r' %\n                        (s, list(NOHTML)))\n    return subcls(s)"}
{"prompt": "def parse_file(self, name):\n        \"\"\"Parse the content of a file.\n\n        See 'parse' method for information.\n\n        :param name: the pathname of the file to parse\n        :return: True on success (no error detected), False otherwise\n        \"\"\"", "output": "\n        try:\n            return self.parse(name)\n        except Exception:\n            return False\n\n    def parse(self, name):\n        \"\"\"Parse the content of a file.\n\n        :param name: the pathname of the file to parse\n        :return: True on success, False otherwise\n        \"\"\"\n        try:\n            return self.parse_file(name)\n        except Exception:\n            return False\n\n    def get_file_content(self, name):\n        \"\"\"Get the content of a file.\n\n        :param name: the pathname of the file to parse\n        :return: the content of the file\n        \"\"\"\n        try:\n            return", "solution": "\n        with open(name, \"rb\") as fp:\n            return self.parse(fp.read())"}
{"prompt": "def until(self, regex):\n        \"\"\"Wait until the regex encountered\n        \"\"\"", "output": "\n        while True:\n            self.wait_for_match(regex)\n            if self.match_count == 0:\n                break\n            time.sleep(self.match_interval)\n\n    def wait_for_match(self, regex):\n        \"\"\"Wait for a regex match\n        \"\"\"\n        self.logger.debug(\"Waiting for %s in %s seconds\", regex, self.match_interval)\n        time.sleep(self.match_interval)\n        self.logger.debug(\"Regex %s found in %s seconds\", regex, self.match_count)\n\n    def get_match_count(self):", "solution": "\n        logger.debug('waiting for %s', regex)\n        r = re.compile(regex, re.M)\n        self.tn.expect([r])"}
{"prompt": "def shape(self):\n        \"\"\"Number of grid points per axis.\"\"\"", "output": "\n        return self._shape\n\n    @property\n    def n_dims(self):\n        \"\"\"Number of dimensions.\"\"\"\n        return self._n_dims\n\n    @property\n    def n_points(self):\n        \"\"\"Number of points per axis.\"\"\"\n        return self._n_points\n\n    @property\n    def n_dims_x(self):\n        \"\"\"Number of dimensions in x.\"\"\"\n        return self.n_dims * self.n_dims_y\n\n    @property\n    def n_dims_y(self):\n        \"\"\"Number of dimensions in y.\"\"\"\n        return self.n_points * self.n_points\n\n    @property", "solution": "\n        try:\n            return self.__shape\n        except AttributeError:\n            shape = tuple(len(vec) for vec in self.coord_vectors)\n            self.__shape = shape\n            return shape"}
{"prompt": "def moduleInfo( module ):\n        \"\"\"\n        Generates HTML information to display for the about info for a module.\n        \n        :param      module  | <module>\n        \"\"\"", "output": "\n        return \"\"\"\n        <h2>Module info</h2>\n        <p>\n        <table width=\"100%\" border=\"0\" cellspacing=\"0\">\n          <tr>\n            <td width=\"50%\">\n              <table class=\"tablesorter\" border=\"0\" cellspacing=\"1\">\n                <tr>\n                  <th width=\"50%\">\n                    <th align=\"center\">\n                      <th align=\"left\">\n                        <a href=\"http://www.w3.org/1999/xhtml\">\n                            <img src=\"http://www.w3.org/1999/xhtml\" alt", "solution": "\n        data = module.__dict__\n        \n        html = []\n        html.append( '<h2>%s</h2>' % data.get('__name__', 'Unknown') )\n        html.append( '<hr/>' )\n        ver = data.get('__version__', '0')\n        html.append( '<small>version: %s</small>' % ver)\n        html.append( '<br/>' )\n        html.append( nativestring(data.get('__doc__', '')) )\n        html.append( '<br/><br/><b>Authors</b><ul/>' )\n        \n        for author in data.get('__authors__', []):\n            html.append( '<li>%s</li>' % author )\n            \n        html.append( '</ul>' )\n        html.append( '<br/><br/><b>Depends on:</b>' )\n        for depends in data.get('__depends__', []):\n            html.append( '<li>%s</li>' % depends )\n            \n        html.append( '</ul>' )\n        html.append( '' )\n        html.append( '<br/><br/><b>Credits</b></ul>' )\n        \n        for credit in data.get('__credits__', []):\n            html.append('<li>%s: %s</li>' % credit)\n            \n        html.append( '</ul>' )\n        \n        opts = (data.get('__maintainer__', ''), data.get('__email__', ''))\n        html.append('<br/><br/><small>maintained by: %s email: %s</small>' % opts)\n        \n        opts = (data.get('__copyright__', ''), data.get('__license__', ''))\n        html.append('<br/><small>%s | license: %s</small>' % opts)\n        \n        return '\\n'.join(html)"}
{"prompt": "def dataset_list(self,\n                     sort_by=None,\n                     size=None,\n                     file_type=None,\n                     license_name=None,\n                     tag_ids=None,\n                     search=None,\n                     user=None,\n                     mine=False,\n                     page=1):\n        \"\"\" return a list of datasets!\n\n            Parameters\n            ==========\n            sort_by: how to sort the result, see valid_sort_bys for options\n            size: the size of the dataset, see valid_sizes for string options\n            file_type: the format, see valid_file_types for string options\n            license_name: string descriptor for license, see valid_license_names\n            tag_ids: tag identifiers to filter the search\n            search: a search term to use (default is empty string)\n            user: username to filter the search to\n            mine: boolean if True, group is changed to \"my\" to return personal\n            page: the page to return (default is 1)\n        \"\"\"", "output": "name: the license name, see valid_licenses for options\n            tag_ids: the tags ids, see valid_tags for options\n            search: the search query, see valid_search_bys for options\n            user: the user, see valid_user_bys for options\n            mine: if True, return a list of datasets, otherwise return a list of datasets\n\n            Returns\n            =======\n            A list of datasets, each with the following fields:\n\n            - id: the dataset id\n            - name: the dataset name\n            - description: the dataset description\n            - tags: the dataset tags\n            - tags_string: the", "solution": "\n        valid_sort_bys = ['hottest', 'votes', 'updated', 'active', 'published']\n        if sort_by and sort_by not in valid_sort_bys:\n            raise ValueError('Invalid sort by specified. Valid options are ' +\n                             str(valid_sort_bys))\n\n        valid_sizes = ['all', 'small', 'medium', 'large']\n        if size and size not in valid_sizes:\n            raise ValueError('Invalid size specified. Valid options are ' +\n                             str(valid_sizes))\n\n        valid_file_types = ['all', 'csv', 'sqlite', 'json', 'bigQuery']\n        if file_type and file_type not in valid_file_types:\n            raise ValueError('Invalid file type specified. Valid options are '\n                             + str(valid_file_types))\n\n        valid_license_names = ['all', 'cc', 'gpl', 'odb', 'other']\n        if license_name and license_name not in valid_license_names:\n            raise ValueError('Invalid license specified. Valid options are ' +\n                             str(valid_license_names))\n\n        if int(page) <= 0:\n            raise ValueError('Page number must be >= 1')\n\n        group = 'public'\n        if mine:\n            group = 'my'\n            if user:\n                raise ValueError('Cannot specify both mine and a user')\n        if user:\n            group = 'user'\n\n        datasets_list_result = self.process_response(\n            self.datasets_list_with_http_info(\n                group=group,\n                sort_by=sort_by or 'hottest',\n                size=size or 'all',\n                filetype=file_type or 'all',\n                license=license_name or 'all',\n                tagids=tag_ids or '',\n                search=search or '',\n                user=user or '',\n                page=page))\n        return [Dataset(d) for d in datasets_list_result]"}
{"prompt": "def fixed_legend_filter_sort(self, fixed_legend_filter_sort):\n        \"\"\"Sets the fixed_legend_filter_sort of this ChartSettings.\n\n        Whether to display \\\"Top\\\"- or \\\"Bottom\\\"-ranked series in the fixed legend  # noqa: E501\n\n        :param fixed_legend_filter_sort: The fixed_legend_filter_sort of this ChartSettings.  # noqa: E501\n        :type: str\n        \"\"\"", "output": "\n\n        self._fixed_legend_filter_sort = fixed_legend_filter_sort\n\n    @property\n    def fixed_legend_title(self):\n        \"\"\"Gets the fixed_legend_title of this ChartSettings.  # noqa: E501\n\n        The fixed legend title for the series with the title for the chart  # noqa: E501\n\n        :return: The fixed_legend_title of this ChartSettings.  # noqa: E501\n        :rtype: str\n        \"\"\"\n        return self._fixed_legend_title\n\n    @fixed_legend_title.setter\n    def fixed_legend_title(self, fixed", "solution": "\n        allowed_values = [\"TOP\", \"BOTTOM\"]  # noqa: E501\n        if fixed_legend_filter_sort not in allowed_values:\n            raise ValueError(\n                \"Invalid value for `fixed_legend_filter_sort` ({0}), must be one of {1}\"  # noqa: E501\n                .format(fixed_legend_filter_sort, allowed_values)\n            )\n\n        self._fixed_legend_filter_sort = fixed_legend_filter_sort"}
{"prompt": "def bowtie(sam, btd, f, r, u, opt, no_shrink, threads):\n    \"\"\"\n    generate bowtie2 command\n    \"\"\"", "output": "\n    if not isinstance(threads, int):\n        threads = 1\n    if not no_shrink:\n        cmd = \"bowtie2 -q -f %d -r %d -u %s -opt %s -noshrink %s\" % (threads, r, u, opt, no_shrink)\n    else:\n        cmd = \"bowtie2 -q -f %d -r %d -u %s -opt %s -noshrink %s\" % (f, q, no_shrink, no_shrink)\n    return cmd\n\ndef bowtie2_build(sam,", "solution": "\n    bt2 = 'bowtie2 -x %s -p %s ' % (btd, threads)\n    if f is not False:\n        bt2 += '-1 %s -2 %s ' % (f, r)\n    if u is not False:\n        bt2 += '-U %s ' % (u)\n    bt2 += opt\n    if no_shrink is False:\n        if f is False:\n            bt2 += ' | shrinksam -u -k %s-shrunk.sam ' % (sam)\n        else:\n            bt2 += ' | shrinksam -k %s-shrunk.sam ' % (sam)\n    else:\n        bt2 += ' > %s.sam' % (sam)\n    return bt2"}
{"prompt": "def parse(self, elt, ps):\n        \"\"\"elt -- the DOM element being parsed\n        ps -- the ParsedSoap object.\n        \"\"\"", "output": "\n        if elt.tagName == 'r':\n            self.parse_r(elt, ps)\n        elif elt.tagName =='s':\n            self.parse_s(elt, ps)\n        elif elt.tagName == 'r':\n            self.parse_r(elt, ps)\n        elif elt.tagName =='s':\n            self.parse_s(elt, ps)\n        elif elt.tagName == 'r':\n            self.parse_r(elt, ps)\n        elif elt.tagName =='s':\n            self.parse_s(elt, ps)\n        elif elt.tagName == 'r", "solution": "\n        self.checkname(elt, ps)\n        if len(_children(elt)) == 0:\n            href = _find_href(elt)\n            if not href:\n                if self.nilled(elt, ps) is False:\n                    return []\n                if self.nillable is True: \n                    return Nilled\n                raise EvaluateException('Required string missing',\n                        ps.Backtrace(elt))\n            if href[0] != '#':\n                return ps.ResolveHREF(href, self)\n            elt = ps.FindLocalHREF(href, elt)\n            self.checktype(elt, ps)\n\n        if self.nilled(elt, ps): return Nilled\n        if len(_children(elt)) == 0: return []\n\n        v = self.simple_value(elt, ps)\n        return self.text_to_data(v, elt, ps)"}
{"prompt": "def _clean_file(self, f):\n        \"\"\"this will take a given file path, scrub it accordingly, and save a new copy of the file\n        in the same location\"\"\"", "output": "\n        if os.path.isfile(f):\n            os.remove(f)\n        os.rename(f, self.file)\n\n    def _get_file_path(self, path):\n        \"\"\"returns the full path to the given file\"\"\"\n        return os.path.join(self.file, path)\n\n    def _get_file_contents(self, path):\n        \"\"\"returns the contents of the given file\"\"\"\n        with open(self._get_file_path(path), 'r') as f:\n            return f.read()\n\n    def _get_file_lines(self, path):", "solution": "\n        if os.path.exists(f) and not os.path.islink(f):\n            tmp_file = tempfile.TemporaryFile(mode='w+b')\n            try:\n                fh = open(f, 'r')\n                data = fh.readlines()\n                fh.close()\n                if len(data) > 0: #if the file isn't empty:\n                    for l in data:\n                        new_l = self._clean_line(l)\n                        tmp_file.write(new_l.encode('utf-8'))\n\n                    tmp_file.seek(0)\n\n            except Exception as e: # pragma: no cover\n                self.logger.exception(e)\n                raise Exception(\"CleanFile Error: Cannot Open File For Reading - %s\" % f)\n\n            try:\n                if len(data) > 0:\n                    new_fh = open(f, 'wb')\n                    for line in tmp_file:\n                        new_fh.write(line)\n                    new_fh.close()\n            except Exception as e: # pragma: no cover\n                self.logger.exception(e)\n                raise Exception(\"CleanFile Error: Cannot Write to New File - %s\" % f)\n\n            finally:\n                tmp_file.close()"}
{"prompt": "def distinct(self):\n        \"\"\"Return sets of indices for each distinct haplotype.\"\"\"", "output": "\n        return set(self.haplotypes)\n\n    def __str__(self):\n        \"\"\"Return string representation of the haplotype.\"\"\"\n        return 'Haplotype(haplotype_id={}, name={}, type={},'\\\n               'name={}, name_type={}, name_type_name={}, name_value={})'.format(self.haplotype_id, self.name, self.type, self.name_id, self.type_name, self.name_type, self.name_id, self.value)\n\n    def __repr__(self):\n        \"\"\"Return string", "solution": "\n\n        # setup collection\n        d = collections.defaultdict(set)\n\n        # iterate over haplotypes\n        for i in range(self.shape[1]):\n\n            # hash the haplotype\n            k = hash(self.values[:, i].tobytes())\n\n            # collect\n            d[k].add(i)\n\n        # extract sets, sorted by most common\n        return sorted(d.values(), key=len, reverse=True)"}
{"prompt": "def is_dir_url(link):\n    # type: (Link) -> bool\n    \"\"\"Return whether a file:// Link points to a directory.\n\n    ``link`` must not have any other scheme but file://. Call is_file_url()\n    first.\n\n    \"\"\"", "output": "\n    # type: (Link) -> bool\n    \"\"\"Return whether a file:// link points to a directory.\n\n    \"\"\"\n    # type: (Link) -> bool\n    \"\"\"Return whether a file:// link points to a directory.\n\n    # We use the following heuristic to figure out if a given path is a\n    # link to a directory, but not a directory itself.\n    #   - We don't allow for a trailing slash, but if we find one,\n    #     we assume that we're a link to a directory.\n    #   - We have to do this because we remove the trailing slash,\n    #     which", "solution": "\n    link_path = url_to_path(link.url_without_fragment)\n    return os.path.isdir(link_path)"}
{"prompt": "def by_visits(self, event_kind=None):\n        \"\"\"\n        Gets Venues in order of how many Events have been held there.\n        Adds a `num_visits` field to each one.\n\n        event_kind filters by kind of Event, e.g. 'theatre', 'cinema', etc.\n        \"\"\"", "output": "\n        if event_kind is not None:\n            event_kind = self.event_kind_from_enum(event_kind)\n        return self.get_venues(event_kind)\n\n    def by_time(self, time_string):\n        \"\"\"\n        Gets a list of events that occurred in a given time string.\n\n        time_string filters by time string.\n        \"\"\"\n        if time_string is not None:\n            time_string = self.convert_time(time_string)\n            return self.get_events(time_string)\n\n    def convert_time(self, time_string):", "solution": "\n        qs = self.get_queryset()\n\n        if event_kind is not None:\n            qs = qs.filter(event__kind=event_kind)\n\n        qs = qs.annotate(num_visits=Count('event')) \\\n                .order_by('-num_visits', 'name_sort')\n\n        return qs"}
{"prompt": "def create_registry(interface):\n    \"\"\"\n    Create a new registry for an extensible interface.\n\n    Parameters\n    ----------\n    interface : type\n        The abstract data type for which to create a registry,\n        which will manage registration of factories for this type.\n\n    Returns\n    -------\n    interface : type\n        The data type specified/decorated, unaltered.\n    \"\"\"", "output": "\n    return interface(\n        'test_{}'.format(interface.__name__),\n        lambda x: x.name,\n        lambda x: x.name,\n        lambda x: x.name,\n        lambda x: x.name,\n        lambda x: x.name,\n        lambda x: x.name,\n        lambda x: x.name,\n        lambda x: x.name,\n        lambda x: x.name,\n        lambda x: x.name,\n        lambda x: x.name,\n        lambda x: x.name,\n        lambda x: x.name,\n        lambda x: x.name,\n       ", "solution": "\n    if interface in custom_types:\n        raise ValueError('there is already a Registry instance '\n                         'for the specified type')\n    custom_types[interface] = Registry(interface)\n    return interface"}
{"prompt": "def regex_parse(regex, text, fromstart=True):\n    r\"\"\"\n    regex_parse\n\n    Args:\n        regex (str):\n        text (str):\n        fromstart (bool):\n\n    Returns:\n        dict or None:\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_regex import *  # NOQA\n        >>> regex = r'(?P<string>\\'[^\\']*\\')'\n        >>> text = \" 'just' 'a' sentance with 'strings' in it \"\n        >>> fromstart = False\n        >>> result = regex_parse(regex, text, fromstart)['string']\n        >>> print(result)\n\n    \"\"\"", "output": "start=fromstart)\n        >>> assert result is not None\n        >>> assert'string' in result\n        >>> assert'sentance' in result\n        >>> assert 'from' in result\n        >>> assert fromstart is True\n        >>> result = regex_parse(regex, text, fromstart=True)\n        >>> assert result is not None\n        >>> assert'string' in result\n        >>> assert'sentance' in result\n        >>> assert 'from' in result\n        >>> assert fromstart is True\n        >>> assert result['from'] == 'just'\n        >>> assert result['string'] == 'a'\n        >>> result = regex_", "solution": "\n    match = regex_get_match(regex, text, fromstart=fromstart)\n    if match is not None:\n        parse_dict = match.groupdict()\n        return parse_dict\n    return None"}
{"prompt": "def _scale_to_dtype(self, data, dtype):\n        \"\"\"Scale provided data to dtype range assuming a 0-1 range.\n\n        Float input data is assumed to be normalized to a 0 to 1 range.\n        Integer input data is not scaled, only clipped. A float output\n        type is not scaled since both outputs and inputs are assumed to\n        be in the 0-1 range already.\n\n        \"\"\"", "output": "\n        if data.size == 0:\n            return data\n\n        if dtype == 0:\n            return data.dtype.type(0)\n\n        if dtype == 1:\n            return data.dtype.type(1)\n\n        if dtype == 2:\n            return data.dtype.type(2)\n\n        if dtype == 3:\n            return data.dtype.type(3)\n\n        if dtype == 4:\n            return data.dtype.type(4)\n\n        if dtype == 5:\n            return data.dtype.type(5)\n\n        if dtype == 6:\n            return data.dtype.type(6)\n\n        if dtype == 7", "solution": "\n        if np.issubdtype(dtype, np.integer):\n            if np.issubdtype(data, np.integer):\n                # preserve integer data type\n                data = data.clip(np.iinfo(dtype).min, np.iinfo(dtype).max)\n            else:\n                # scale float data (assumed to be 0 to 1) to full integer space\n                dinfo = np.iinfo(dtype)\n                data = data.clip(0, 1) * (dinfo.max - dinfo.min) + dinfo.min\n            data = data.round()\n        return data"}
{"prompt": "def start_notifications(self):\n        \"\"\"Start the notifications thread.\n\n        If an external callback is not set up (using `update_webhook`) then\n        calling this function is mandatory to get or set resource.\n\n        .. code-block:: python\n\n            >>> api.start_notifications()\n            >>> print(api.get_resource_value(device, path))\n            Some value\n            >>> api.stop_notifications()\n\n        :returns: void\n        \"\"\"", "output": "\n        self._notifications_thread = threading.Thread(target=self._notifications_loop, name='start_notifications')\n        self._notifications_thread.daemon = True\n        self._notifications_thread.start()\n\n    def stop_notifications(self):\n        \"\"\"Stop the notifications thread.\n\n        :returns: void\n        \"\"\"\n        self._notifications_thread.join()\n\n    def _get_resource_value(self, device, path):\n        \"\"\"Get the value of a specific resource.\n\n        :param device: device name\n        :param path: path to the resource\n        :returns: resource value\n        :rtype: dict\n       ", "solution": "\n        with self._notifications_lock:\n            if self.has_active_notification_thread:\n                return\n            api = self._get_api(mds.NotificationsApi)\n            self._notifications_thread = NotificationsThread(\n                self._db,\n                self._queues,\n                b64decode=self.b64decode,\n                notifications_api=api,\n                subscription_manager=self.subscribe,\n            )\n            self._notifications_thread.daemon = True\n            self._notifications_thread.start()"}
{"prompt": "def check_imported(module):\n        \"\"\"Check whether the provide module can be imported (package installed).\n\n        Args:\n            module (str): The name of the module to check availability.\n\n        Returns:\n            bool: True if the module can be imported, False otherwise.\n        \"\"\"", "output": "\n        return module in sys.modules\n\n    def get_module_name(module):\n        \"\"\"Get the name of the provide module.\n\n        Args:\n            module (str): The name of the provide module.\n\n        Returns:\n            str: The name of the provide module.\n        \"\"\"\n        return get_provide_module_name(module)\n\n    def get_module_path(module):\n        \"\"\"Get the path of the provide module.\n\n        Args:\n            module (str): The name of the module to check availability.\n\n        Returns:\n            str: The path of the provide module.\n        \"\"\"\n        return get_provide_module", "solution": "\n        imported = True\n        module_info = ('', '', '')\n        # TODO: if possible, update to a cleaner method that doesn't require importing the module\n        # and running inline code.\n        try:\n            importlib.import_module(module)\n            module_info = imp.find_module(module)\n        except ImportError:\n            imported = False\n\n        # get module path\n        module_path = module_info[1]\n        description = module_info[2]\n\n        if not description:\n            # if description is None or empty string the module could not be imported\n            imported = False\n        elif not description and not module_path:\n            # if description/module_path are None or empty string the module could not be imported\n            imported = False\n        elif module_path is not None and (\n            'dist-packages' in module_path or 'site-packages' in module_path\n        ):\n            # if dist-packages|site-packages in module_path the import doesn't count\n            imported = False\n        return imported"}
{"prompt": "def step4(self, encrypted_data):\n        \"\"\"Last pairing step.\"\"\"", "output": "\n        self.data = encrypted_data\n        self.encrypted_data = encrypted_data\n        self.step = 2\n        self.state = 'WAIT FOR STARTUP'\n        self.data_received = False\n        self.encrypted_data_received = False\n        self.step_count = 0\n        self.encrypted_data_count = 0\n        self.state = 'WAIT FOR STARTUP'\n\n    def step5(self, encrypted_data):\n        \"\"\"Last pairing step.\"\"\"\n        self.data = encrypted_data\n        self.encrypted_data_count += 1\n        self.encrypted_data_received = True", "solution": "\n        chacha = chacha20.Chacha20Cipher(self._session_key, self._session_key)\n        decrypted_tlv_bytes = chacha.decrypt(\n            encrypted_data, nounce='PS-Msg06'.encode())\n        if not decrypted_tlv_bytes:\n            raise Exception('data decrypt failed')  # TODO: new exception\n        decrypted_tlv = tlv8.read_tlv(decrypted_tlv_bytes)\n        _LOGGER.debug('PS-Msg06: %s', decrypted_tlv)\n\n        atv_identifier = decrypted_tlv[tlv8.TLV_IDENTIFIER]\n        atv_signature = decrypted_tlv[tlv8.TLV_SIGNATURE]\n        atv_pub_key = decrypted_tlv[tlv8.TLV_PUBLIC_KEY]\n        log_binary(_LOGGER,\n                   'Device',\n                   Identifier=atv_identifier,\n                   Signature=atv_signature,\n                   Public=atv_pub_key)\n\n        # TODO: verify signature here\n\n        return Credentials(atv_pub_key, self._signing_key.to_seed(),\n                           atv_identifier, self.pairing_id)"}
{"prompt": "def eth_getStorageAt(self, address, position):\n        \"\"\"Get account storage data at position.\n\n        :param address:\n        :param position:\n        :return:\n        \"\"\"", "output": "\n        return self.get_account_storage(address, position)\n\n    def eth_getStorage(self, address, storageType):\n        \"\"\"Get account storage data.\n\n        :param address:\n        :param storageType:\n        :return:\n        \"\"\"\n        return self.get_account_storage(address, storageType)\n\n    def eth_getStorages(self, address):\n        \"\"\"Get account storages.\n\n        :param address:\n        :return:\n        \"\"\"\n        return self.get_account_storages(address)\n\n    def eth_get_account_storages(self, address, storagesType", "solution": "\n        account = self.reader._get_account(address)\n        return _encode_hex(\n            utils.zpad(utils.encode_int(account.get_storage_data(position)), 32)\n        )"}
{"prompt": "def log(message, type):\n    \"\"\"Log notices to stdout and errors to stderr\"\"\"", "output": "\n    print(message, file=sys.stderr)\n    sys.exit(1)\n\n\ndef main():\n    \"\"\"Main function\"\"\"\n    parser = argparse.ArgumentParser(description='Generate the documentation for the '\n                                     'the %s documentation' % __name__,\n                                     epilog='See the %s source for more details.' %\n                                     __name__)\n    parser.add_argument('--version', action='version', version=__version__)\n    parser.add_argument('--html', action='store_true',\n                        help='Generate the documentation')\n    args = parser.parse_args()\n    generate_docs(args.version,", "solution": "\n\n    (sys.stdout if type == 'notice' else sys.stderr).write(message + \"\\n\")"}
{"prompt": "def process_instrs(queue, stack, body, context):\n    \"\"\"\n    Process instructions from the instruction queue.\n    \"\"\"", "output": "\n    while body:\n        opcode = body.pop(0)\n        if opcode == 'I':\n            if len(body) == 1:\n                # 'I' is the start of a new instruction.\n                # 'I' is the end of a new instruction.\n                # 'I' is the start of a new instruction.\n                # 'I' is the end of a new instruction.\n                # 'I' is the start of a new instruction.\n                # 'I' is the end of a new instruction.\n                # 'I' is the start of a new instruction.\n                # 'I' is the end", "solution": "\n    next_instr = queue.popleft\n    while queue:\n        newcontext = _process_instr(next_instr(), queue, stack, body, context)\n        if newcontext is not None:\n            context = newcontext"}
{"prompt": "def _read_txt(self, stream):\n        \"\"\"\n        Load a PLY element from an ASCII-format PLY file.  The element\n        may contain list properties.\n\n        \"\"\"", "output": "\n        # Read the header\n        line = stream.readline()\n        if line.startswith(b'PLY'):\n            header = line.split(b' ', 1)[1]\n            if header.startswith(b'Ply'):\n                self.element_name = header[1:]\n                self.element_type = header[0]\n            else:\n                raise PLYParseError('Expected PLY element name after PLY')\n        elif line.startswith(b'End'):\n            self.element_name = ''\n            self.element_type = ''\n            self.element_name = ''\n        elif line.startswith(", "solution": "\n        self._data = _np.empty(self.count, dtype=self.dtype())\n\n        k = 0\n        for line in _islice(iter(stream.readline, b''), self.count):\n            fields = iter(line.strip().split())\n            for prop in self.properties:\n                try:\n                    self._data[prop.name][k] = prop._from_fields(fields)\n                except StopIteration:\n                    raise PlyElementParseError(\"early end-of-line\",\n                                               self, k, prop)\n                except ValueError:\n                    raise PlyElementParseError(\"malformed input\",\n                                               self, k, prop)\n            try:\n                next(fields)\n            except StopIteration:\n                pass\n            else:\n                raise PlyElementParseError(\"expected end-of-line\",\n                                           self, k)\n            k += 1\n\n        if k < self.count:\n            del self._data\n            raise PlyElementParseError(\"early end-of-file\", self, k)"}
{"prompt": "def calcKYratioDifference(self):\n        \"\"\"\n        Returns the difference between the simulated capital to income ratio and the target ratio.\n        Can only be run after solving all AgentTypes and running makeHistory.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        diff : float\n            Difference between simulated and target capital to income ratio.\n        \"\"\"", "output": "\n        return self.getKyrRatioDifference(None, None)\n\n    def calcKyrRatioDifference(self):\n        \"\"\"\n        Returns the difference between the KyrRatio and the simulated capital to income ratio.\n        Can only be run after solving all AgentTypes and running makeHistory.\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        diff : float\n            Difference between simulated and target capital to income ratio.\n        \"\"\"\n        return self.getKyrRatioDifference(None, None)\n\n    def calcKyrRatioDifference_old(self):\n        \"\"\"\n        Returns the difference between the KyrRatio", "solution": "\n        # Ignore the first X periods to allow economy to stabilize from initial conditions\n        KYratioSim = np.mean(np.array(self.KtoYnow_hist)[self.ignore_periods:])\n        diff = KYratioSim - self.KYratioTarget\n        return diff"}
{"prompt": "def to_funset(self, lname=\"clamping\", cname=\"clamped\"):\n        \"\"\"\n        Converts the list of clampings to a set of `gringo.Fun`_ instances\n\n        Parameters\n        ----------\n        lname : str\n            Predicate name for the clamping id\n\n        cname : str\n            Predicate name for the clamped variable\n\n        Returns\n        -------\n        set\n            Representation of all clampings as a set of `gringo.Fun`_ instances\n\n\n        .. _gringo.Fun: http://potassco.sourceforge.net/gringo.html#Fun\n        \"\"\"", "output": "\n    \"\"\"\n\n    def to_funset_list(self, lname=\"funset\", cname=\"funset\"):\n        \"\"\"\n        Converts the list of funtions to a set of `gringo.Fun`_ instances\n\n        Parameters\n        ----------\n        lname : str\n            Predicate name for the funtion id\n\n        cname : str\n            Predicate name for the funset variable\n\n        Returns\n        -------\n        set\n            Representation of all funtions as a set of `gringo.Fun`_ instances\n\n\n       .. _potassco.models.funset.funset_list: http://potassco.sourceforge.net/", "solution": "\n        fs = set()\n        for i, clamping in enumerate(self):\n            fs.add(gringo.Fun(lname, [i]))\n            fs = fs.union(clamping.to_funset(i, cname))\n\n        return fs"}
{"prompt": "def as_dict(self):\n        \"\"\"\n        Returns the CTRL as a dictionary. \"SITE\" and \"CLASS\" are of\n        the form {'CATEGORY': {'TOKEN': value}}, the rest is of the\n        form 'TOKEN'/'CATEGORY': value. It gets the conventional standard\n        structure because primitive cells use the conventional\n        a-lattice parameter as the scaling factor and not the a-lattice\n        parameter of the primitive cell.\n        \"\"\"", "output": "\n        return {'version': self.version, 'nb_sets': self.nb_sets,\n               'symbol_count': self.symbol_count,'scaling_factor': self.scaling_factor,\n                'units': self.units,'sites': self.sites,\n                'class_count': self.class_count,'sites_count': self.nb_sets_count,\n                'comment': self.comment, 'n_atoms': self.n_atoms_count,\n                'n_residues': self.n_residues_count,'scaling_factor_count': self", "solution": "\n        ctrl_dict = {\"@module\": self.__class__.__module__,\n                     \"@class\": self.__class__.__name__}\n        if self.header is not None:\n            ctrl_dict[\"HEADER\"] = self.header\n        if self.version is not None:\n            ctrl_dict[\"VERS\"] = self.version\n        sga = SpacegroupAnalyzer(self.structure)\n        alat = sga.get_conventional_standard_structure().lattice.a\n        plat = self.structure.lattice.matrix/alat\n\n        "}
{"prompt": "def remove_mediators(tree, columns):\n    \"\"\"\n    Removes intermediate nodes that are just mediators between their parent and child states.\n    :param columns: list of characters\n    :param tree: ete3.Tree\n    :return: void, modifies the input tree\n    \"\"\"", "output": "\n    for i in range(0, len(columns)):\n        if columns[i] == tree.root:\n            columns[i] = tree.root.children[0]\n            break\n    for i in range(0, len(columns)):\n        if columns[i] == tree.root:\n            columns[i] = tree.root.children[0]\n            break\n    for i in range(0, len(columns)):\n        if columns[i] == tree.root:\n            columns[i] = tree.root.children[0]\n            break\n    for i in range(0, len(columns", "solution": "\n    num_removed = 0\n    for n in tree.traverse('postorder'):\n        if getattr(n, METACHILD, False) or n.is_leaf() or len(n.children) > 1 or n.is_root() \\\n                or getattr(n, NUM_TIPS_INSIDE) > 0:\n            continue\n\n        parent = n.up\n        child = n.children[0]\n\n        compatible = True\n        for column in columns:\n            states = getattr(n, column, set())\n            parent_states = getattr(parent, column, set())\n            child_states = getattr(child, column, set())\n            # if mediator has unresolved states, it should hesitate between the parent and the child:\n            if states != child_states | parent_states:\n                compatible = False\n                break\n\n        if compatible:\n            parent.remove_child(n)\n            parent.add_child(child)\n            num_removed += 1\n    if num_removed:\n        logging.getLogger('pastml').debug(\"Removed {} internal node{}\"\n                                          \" with the state unresolved between the parent's and the only child's.\"\n                                          .format(num_removed, '' if num_removed == 1 else 's'))"}
{"prompt": "def ambigFilter(in_fh1, in_fh2, out_fh1, out_fh2, verbose=False, best=False):\n  \"\"\"\n    @summary: take reads from in_fh1 and output to out_fh1 if they don't\n              appear also in in_fh2 (ditto for in_fh2)\n\n    @param in_fh1: BED formated stream of reads\n    @param in_fh2: BED formated stream of reads\n    @param out_fh1: Output reads that pass from in_fh1 to this stream\n    @param out_fh2: Output reads that pass from in_fh2 to this stream\n    @param verbose: output additional messages to sys.stderr if True\n    @param best: Given two items that have the same name, try to output the one\n                 with the best score\n\n    @return: None (out streams have BED format)\n  \"\"\"", "output": "_fh2: Output reads that pass from in_fh2 to this stream\n    @param verbose: Prints out the best/worst reads\n  \"\"\"\n  # TODO: check if the two BED files are the same\n  if verbose:\n    print \"Merging reads from %s and %s\" % (in_fh1, in_fh2)\n\n  # TODO: check if the two BED files are the same\n  if verbose:\n    print \"Merging reads from %s and %s\" % (out_fh1, out_fh2)\n\n  # TODO: check if the two BED", "solution": "\n  for r1, r2 in BEDUniqueIterator(in_fh1, in_fh2, verbose, best, dropAfter=6):\n    if r1 is not None:\n      out_fh1.write(str(r1) + \"\\n\")\n    if r2 is not None:\n      out_fh2.write(str(r2) + \"\\n\")"}
{"prompt": "def get_trainer(name):\n    \"\"\"return the unique id for a trainer, determined by the md5 sum\n    \"\"\"", "output": "\n    return hashlib.md5(name).hexdigest()\n\n\ndef get_trainers(trainer_id):\n    \"\"\"return a list of unique trainers\n    \"\"\"\n    return [get_trainer(trainer_id) for trainer_id in get_trainer_ids(trainer_id)]\n\n\ndef get_trainer_by_id(trainer_id):\n    \"\"\"return the unique id for a trainer, determined by the md5 sum\n    \"\"\"\n    return hashlib.md5(trainer_id).hexdigest()\n\n\ndef get_trainer_ids(trainer", "solution": "\n    name = name.lower()\n    return int(hashlib.md5(name.encode('utf-8')).hexdigest(), 16) % 10**8"}
{"prompt": "def to_dict(self):\n        \"\"\"\n        This method converts the DictCell into a python `dict`.  This is useful\n        for JSON serialization.\n        \"\"\"", "output": "\n        return dict(zip(self.columns, self.values))\n\n    def __str__(self):\n        \"\"\"\n        This method returns a string representation of the cell.\n        \"\"\"\n        return json.dumps(self.to_dict(), indent=4)\n\n    def __repr__(self):\n        \"\"\"\n        This method returns a string representation of the cell.\n        \"\"\"\n        return self.__str__()\n\n    def __eq__(self, other):\n        \"\"\"\n        This method is used to check if two cells are the same.\n        \"\"\"\n        return self.to_dict() == other.to_dict()\n\n    def __ne__(self", "solution": "\n        output = {}\n        for key, value in self.__dict__['p'].iteritems():\n            if value is None or isinstance(value, SIMPLE_TYPES):\n                output[key] = value\n            elif hasattr(value, 'to_dot'):\n                output[key] = value.to_dot()\n            elif hasattr(value, 'to_dict'):\n                output[key] = value.to_dict()\n            elif isinstance(value, datetime.date):\n                # Convert date/datetime to ms-since-epoch (\"new Date()\").\n                ms = time.mktime(value.utctimetuple()) * 1000\n                ms += getattr(value, 'microseconds', 0) / 1000\n                output[key] = int(ms)\n            elif isinstance(value, dict):\n                output[key] = []\n            else:\n                raise ValueError('cannot encode ' + repr(key))\n\n        return output"}
{"prompt": "def absolute_coords(self):\n\t\t\"\"\"The current absolute coordinates of the pointer event,\n\t\tin mm from the top left corner of the device.\n\n\t\tTo get the corresponding output screen coordinate, use\n\t\t:meth:`transform_absolute_coords`.\n\n\t\tFor pointer events that are not of type\n\t\t:attr:`~libinput.constant.EventType.POINTER_MOTION_ABSOLUTE`,\n\t\tthis property raises :exc:`AttributeError`.\n\n\t\tReturns:\n\t\t\t(float, float): The current absolute coordinates.\n\t\tRaises:\n\t\t\tAttributeError\n\t\t\"\"\"", "output": "\n\t\traise AttributeError('absolute_coords')\n\n\tdef transform_absolute_coords(self, x, y):\n\t\t\"\"\"Transform the screen coordinate from the absolute coordinates\n\t\tto the current absolute coordinates.\n\n\t\tParameters:\n\t\t\tx (float): The x-coordinate of the absolute coordinates.\n\t\t\ty (float): The y-coordinate of the current absolute coordinates.\n\n\t\tReturns:\n\t\t\t(float, float): The x-coordinate of the absolute coordinates.\n\t\tRaises:\n\t\t\tValueError: If the absolute coordinates are not of type\n\t\t\tValueError: If the current absolute coordinates are of type\n\t\t\t", "solution": "\n\n\t\tif self.type != EventType.POINTER_MOTION_ABSOLUTE:\n\t\t\traise AttributeError(_wrong_prop.format(self.type))\n\t\tabs_x = self._libinput.libinput_event_pointer_get_absolute_x(\n\t\t\tself._handle)\n\t\tabs_y = self._libinput.libinput_event_pointer_get_absolute_y(\n\t\t\tself._handle)\n\t\treturn abs_x, abs_y"}
{"prompt": "def _populate_trace(self, graph: TraceGraph, trace_frame_ids: List[int]) -> None:\n        \"\"\" Populates (from the given trace graph) the forward and backward\n        traces reachable from the given traces (including input trace frames).\n        Make sure to respect trace kind in successors\n        \"\"\"", "output": "\n        for trace_id in trace_frame_ids:\n            trace = trace_graph.get_trace(trace_id)\n            if trace.get('status') =='success':\n                self.successors.append(trace_id)\n            elif trace.get('status') == 'failure':\n                self.failureors.append(trace_id)\n            else:\n                raise ValueError(f'Unexpected status: {trace.get(\"status\")}')\n\n    def _get_trace_id(self, trace_id: str) -> str:\n        \"\"\" Returns the trace ID of the given trace (or None", "solution": "\n        while len(trace_frame_ids) > 0:\n            trace_frame_id = trace_frame_ids.pop()\n            if trace_frame_id in self._visited_trace_frame_ids:\n                continue\n\n            trace_frame = graph._trace_frames[trace_frame_id]\n            self._add_trace_frame(graph, trace_frame)\n            self._visited_trace_frame_ids.add(trace_frame_id)\n\n            key = (trace_frame.callee_id.local_id, trace_frame.callee_port)\n            trace_frame_ids.extend(\n                [\n                    trace_frame_id\n                    for trace_frame_id in graph._trace_frames_map[key]\n                    if trace_frame_id not in self._visited_trace_frame_ids\n                    and graph._trace_frames[trace_frame_id].kind == trace_frame.kind\n                ]\n            )"}
{"prompt": "def main(fast=False):\n    \"\"\"Run all benchmarks and print report to the console.\"\"\"", "output": "\n    if not fast:\n        print(\"Running benchmarks in fast mode.\")\n        benchmarks = [\n            Benchmark(\n                name=name,\n                description=description,\n                options=options,\n                args=args,\n                env=env,\n                cwd=cwd,\n                env_vars=env_vars,\n                env_options=env_options,\n                env_args=env_args,\n                env_options_vars=env_options_vars,\n                env_options_options=env_options_options,\n                env_options_options_vars=env_options_options_vars,\n                env_options_", "solution": "\n    print('Running benchmarks...\\n')\n    results = bench_run(fast=fast)\n    bench_report(results)"}
{"prompt": "def get_2mass_epoch (tmra, tmdec, debug=False):\n    \"\"\"Given a 2MASS position, look up the epoch when it was observed.\n\n    This function uses the CDS Vizier web service to look up information in\n    the 2MASS point source database. Arguments are:\n\n    tmra\n      The source's J2000 right ascension, in radians.\n    tmdec\n      The source's J2000 declination, in radians.\n    debug\n      If True, the web server's response will be printed to :data:`sys.stdout`.\n\n    The return value is an MJD. If the lookup fails, a message will be printed\n    to :data:`sys.stderr` (unconditionally!) and the :data:`J2000` epoch will\n    be returned.\n\n    \"\"\"", "output": ".\n    \"\"\"\n    # Get the epoch from the database\n    epoch = float(tmra) + float(tmdec) / 2.0\n\n    # If the epoch is less than 2.0, then we have to use the web service to get the\n    # declination from the database\n    if epoch > 2.0:\n        # Get the epoch from the database\n        tm_epoch = get_J2000_epoch (epoch, debug)\n        # If the epoch is less than 2.0, we have to use the CDS Vizier web service to get the\n        # declination from the database\n        tmdec", "solution": "\n    import codecs\n    try:\n        from urllib.request import urlopen\n    except ImportError:\n        from urllib2 import urlopen\n    postdata = b"}
{"prompt": "def get_repository_config_acl(namespace, config, snapshot_id):\n    \"\"\"Get configuration permissions.\n\n    The configuration should exist in the methods repository.\n\n    Args:\n        namespace (str): Configuration namespace\n        config (str): Configuration name\n        snapshot_id (int): snapshot_id of the method\n\n    Swagger:\n        https://api.firecloud.org/#!/Method_Repository/getConfigACL\n    \"\"\"", "output": "\n    permission_list = []\n    if config is not None:\n        for item in config.split(','):\n            if item.startswith('method.'):\n                permission_list.append(item[len('method.'):])\n            elif item.startswith('method_repository.'):\n                permission_list.append(item[len('method_repository.'):])\n            else:\n                raise ValueError('Unknown config item: %s' % item)\n    return permission_list\n\n\ndef get_repository_config_acl_for_method(namespace, method):\n    \"\"\"Get the configuration ACL for a method.\n\n    Args:\n        namespace (str):", "solution": "\n    uri = \"configurations/{0}/{1}/{2}/permissions\".format(namespace,\n                                                    config, snapshot_id)\n    return __get(uri)"}
{"prompt": "def _surfdens(self,R,z,phi=0.,t=0.):\n        \"\"\"\n        NAME:\n           _surfdens\n        PURPOSE:\n           evaluate the surface density for this potential\n        INPUT:\n           R - Galactocentric cylindrical radius\n           z - vertical height\n           phi - azimuth\n           t - time\n        OUTPUT:\n           the surface density\n        HISTORY:\n           2018-08-19 - Written - Bovy (UofT)\n        \"\"\"", "output": "\n        return (R-self.R0) / (self.R0+z)\n\n    def _surfdens_from_z(self,z,t=0.):\n        \"\"\"\n        NAME:\n           _surfdens_from_z\n        PURPOSE:\n           evaluate the surface density for this potential\n        INPUT:\n           z - vertical height\n           t - time\n        OUTPUT:\n           the surface density\n        HISTORY:\n           2018-08-19 - Written - Bovy (UofT)\n        \"\"\"\n        return _surfdens(self,R=self.R0,z=z,phi=phi,t", "solution": "\n        r= numpy.sqrt(R**2.+z**2.)\n        Rma= numpy.sqrt(R**2.-self.a**2.+0j)\n        if Rma == 0.:\n            return (-12.*self.a**3-5.*self.a*z**2\n                      +numpy.sqrt(1.+z**2/self.a**2)\\\n                         *(12.*self.a**3-self.a*z**2+2/self.a*z**4))\\\n                          /30./numpy.pi*z**-5.\n        else:\n            return self.a*((2.*self.a**2.+R**2.)*Rma**-5\\\n                               *(numpy.arctan(z/Rma)-numpy.arctan(self.a*z/r/Rma))\n                           +z*(5.*self.a**3.*r-4.*self.a**4\n                               +self.a**2*(2.*r**2.+R**2)\n                               -self.a*r*(5.*R**2.+3.*z**2.)+R**2.*r**2.)\n                           /(self.a**2.-R**2.)**2.\n                           /(r**2-self.a**2.)**2.).real/4./numpy.pi"}
{"prompt": "def datagetter(cls):\n        \"\"\" example datagetter function, make any local modifications here \"\"\"", "output": "\n        def datagetter_decorator(func):\n            def _datagetter(self):\n                return func\n            return _datagetter\n\n        return dict(\n            (name, datagetter(name))\n            for name in dir(cls)\n            if not name.startswith('_') and name.isupper()\n        )\n\n\nclass TestDataGetter(unittest.TestCase):\n    \"\"\"\n    Tests for DataGetter\n    \"\"\"\n\n    def test_datagetter_with_default(self):\n        \"\"\"\n        Test that default is passed to datagetter\n        \"\"\"\n        self.assertEqual(datagetter(None), None)\n\n    def test_datagetter_with_", "solution": "\n        with open('myfile', 'rt') as f:\n            rows = [r for r in csv.reader(f)]\n        dothing = lambda _: [i for i, v in enumerate(_)]\n        rows = [dothing(_) for _ in rows]\n        raise NotImplementedError('You need to implement this yourlself!')\n        return rows"}
{"prompt": "def cowbat(self):\n        \"\"\"\n        Run all the methods\n        \"\"\"", "output": "\n        self.logger.info(\"Running cowbat...\")\n        self.logger.info(\"Method: %s\" % self.method)\n        self.logger.info(\"Method arguments: %s\" % self.method_args)\n        self.logger.info(\"Method return: %s\" % self.method_return)\n        self.logger.info(\"Method arguments: %s\" % self.method_args)\n        self.logger.info(\"Method return: %s\" % self.method_return)\n\n        # Run the method\n        self.logger.info(\"Running method...\")\n        self.logger", "solution": "\n        logging.info('Beginning COWBAT database downloads')\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'genesippr')):\n            self.sipprverse_targets(databasepath=self.databasepath)\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'coregenome')):\n            self.cowbat_targets(databasepath=self.databasepath)\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'ConFindr')):\n            self.confindr_targets()\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'mash')):\n            self.mash(databasepath=self.databasepath)\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'MLST')):\n            self.mlst(databasepath=self.databasepath)\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'rMLST')):\n            self.rmlst(databasepath=self.databasepath,\n                       credentials=self.credentials)\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'univec')):\n            self.univec(databasepath=self.databasepath)\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'resfinder')):\n            self.cge_db_downloader(databasepath=self.databasepath,\n                                   analysistype='resfinder',\n                                   dbname='resfinder_db')\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'virulence')):\n            self.cge_db_downloader(databasepath=self.databasepath,\n                                   analysistype='virulence',\n                                   dbname='virulencefinder_db')\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'serosippr')):\n            self.cge_db_downloader(databasepath=self.databasepath,\n                                   analysistype='serosippr',\n                                   dbname='serotypefinder_db')\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'pointfinder')):\n            self.cge_db_downloader(databasepath=self.databasepath,\n                                   analysistype='pointfinder',\n                                   dbname='pointfinder_db')\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'clark')):\n            self.clark(databasepath=self.databasepath)\n        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'mob_suite')):\n            self.mob_suite_targets()"}
{"prompt": "def authcode_post(self, path, **kwargs):\n        \"\"\"Perform an HTTP POST to okcupid.com using this profiles session\n        where the authcode is automatically added as a form item.\n        \"\"\"", "output": "\n        return self._post(path, kwargs)\n\n    def authcode_get(self, path, **kwargs):\n        \"\"\"Perform an HTTP GET to okcupid.com using this profiles session\n        where the authcode is automatically added as a form item.\n        \"\"\"\n        return self._get(path, kwargs)\n\n    def _get(self, path, **kwargs):\n        \"\"\"Perform an HTTP GET to okcupid.com using this profiles session\n        where the authcode is automatically added as a form item.\n        \"\"\"\n        return self.session.get(self.urls[path], **kwargs)\n\n    def _", "solution": "\n        kwargs.setdefault('data', {})['authcode'] = self.authcode\n        return self._session.okc_post(path, **kwargs)"}
{"prompt": "def get_sls_config_file(path, stage, region):\n    \"\"\"Determine Serverless config file name.\"\"\"", "output": "\n    return path + \".sls\" if path.endswith(\".sls\") else path\n\n\ndef get_sls_home(path):\n    \"\"\"Determine Serverless home directory.\"\"\"\n    return get_sls_config_file(path, 'home', 'home') else None\n\n\ndef get_sls_data_home(path):\n    \"\"\"Determine Serverless data directory.\"\"\"\n    return get_sls_config_file(path, 'home', 'data') else None\n\n\ndef get_sls_data_home_no_file(path):\n    \"\"\"Determine Serverless data file name", "solution": "\n    for name in gen_sls_config_files(stage, region):\n        if os.path.isfile(os.path.join(path, name)):\n            return name\n    return \"config-%s.json\" % stage"}
{"prompt": "def validateStringInput(input_key,input_data, read=False):\n    \"\"\"\n    To check if a string has the required format. This is only used for POST APIs.\n    \"\"\"", "output": "\n    if not isinstance(input_data,basestring):\n        raise TypeError(\"validateStringInput() argument must be a string\")\n    if not isinstance(input_key,basestring):\n        raise TypeError(\"validateStringInput() argument must be a string\")\n    if read:\n        return input_data.read()\n    else:\n        return input_data\n\ndef validateIntegerInput(input_key,input_data,read=False):\n    \"\"\"\n    To check if an integer has the required format. This is only used for POST APIs.\n    \"\"\"\n    if not isinstance(input_data,int):\n        raise TypeError(\"validate", "solution": "\n    log = clog.error_log\n    func = None\n    if '*' in input_data or '%' in input_data:\n        func = validationFunctionWildcard.get(input_key)\n        if func is None:\n            func = searchstr\n    elif input_key == 'migration_input' :\n        if input_data.find('#') != -1 : func = block\n        else : func = dataset\n    else:\n        if not read:\n            func = validationFunction.get(input_key)\n            if func is None:\n                func = namestr\n        else:\n            if input_key == 'dataset':\n                func = reading_dataset_check\n            elif input_key == 'block_name':\n                func = reading_block_check\n            elif input_key == 'logical_file_name':\n                func = reading_lfn_check\n            else:\n                func = namestr\n    try:\n        func(input_data)\n    except AssertionError as ae:\n        serverLog = str(ae) + \" key-value pair (%s, %s) cannot pass input checking\" %(input_key, input_data)\n        #print serverLog\n        dbsExceptionHandler(\"dbsException-invalid-input2\", message=\"Invalid Input Data %s...:  Not Match Required Format\" %input_data[:10], \\\n            logger=log.error, serverError=serverLog)\n    return input_data"}
{"prompt": "def release_port(upnp, external_port):\n    \"\"\"Try to release the port mapping for `external_port`.\n\n    Args:\n        external_port (int): the port that was previously forwarded to.\n\n    Returns:\n        success (boolean): if the release was successful.\n    \"\"\"", "output": "\n    if not upnp.is_running:\n        return False\n\n    if not upnp.is_port_bound:\n        return False\n\n    if not upnp.is_port_bound.is_set():\n        return False\n\n    if not upnp.is_port_bound.is_set():\n        return False\n\n    if not upnp.is_port_bound.is_set():\n        return False\n\n    if not upnp.is_running_port:\n        return False\n\n    return upnp.release_port(external_port)\n\n\ndef get_port_mappings(upnp):\n    \"\"\"Get", "solution": "\n    mapping = upnp.getspecificportmapping(external_port, 'UDP')\n\n    if mapping is None:\n        log.error('could not find a port mapping', external=external_port)\n        return False\n    else:\n        log.debug('found existing port mapping', mapping=mapping)\n\n    if upnp.deleteportmapping(external_port, 'UDP'):\n        log.info('successfully released port mapping', external=external_port)\n        return True\n\n    log.warning(\n        'could not release port mapping, check your router for stale mappings',\n    )\n    return False"}
{"prompt": "def disambiguate_entity(key, text):\n    \"\"\"Resolve ambiguity between entities with same dimensionality.\"\"\"", "output": "\n    if len(text)!= len(key):\n        return text\n    if len(text)!= len(self.dim_names):\n        return text\n    if len(text) == 0:\n        return text\n    if len(text) == len(self.dim_names):\n        return text\n    if key!= self.dim_names[0]:\n        return text\n    if self.dim_names[1]!= self.dim_names[key]:\n        return text\n    if len(text)!= len(self.dim_names[self.dim_names[0]]]):\n        return text\n    return", "solution": "\n    new_ent = l.DERIVED_ENT[key][0]\n\n    if len(l.DERIVED_ENT[key]) > 1:\n        transformed = TFIDF_MODEL.transform([text])\n        scores = CLF.predict_proba(transformed).tolist()[0]\n        scores = sorted(zip(scores, TARGET_NAMES), key=lambda x: x[0],\n                        reverse=True)\n        names = [i.name for i in l.DERIVED_ENT[key]]\n        scores = [i for i in scores if i[1] in names]\n        try:\n            new_ent = l.ENTITIES[scores[0][1]]\n        except IndexError:\n            logging.debug('\\tAmbiguity not resolved for \"%s\"', str(key))\n\n    return new_ent"}
{"prompt": "def get_nn_info(self, structure, n):\n        \"\"\"\n        Get all near-neighbor sites as well as the associated image locations\n        and weights of the site with index n using the closest relative\n        neighbor distance-based method with VIRE atomic/ionic radii.\n\n        Args:\n            structure (Structure): input structure.\n            n (integer): index of site for which to determine near\n                neighbors.\n\n        Returns:\n            siw (list of tuples (Site, array, float)): tuples, each one\n                of which represents a neighbor site, its image location,\n                and its weight.\n        \"\"\"", "output": "\n        if self.is_spin_polarized:\n            return self._get_spin_polarized_info(structure, n)\n        else:\n            return self._get_nn_info(structure, n)\n\n    def _get_spin_polarized_info(self, structure, n):\n        \"\"\"\n        Get all near-neighbor sites as well as the associated image\n        locations and atomic weights of the site with index n using the\n        VIRE atomic/ionic radii.\n\n        Args:\n            structure (Structure): input structure.\n            n (integer): index of site for which to determine near\n                neighbors.", "solution": "\n\n        vire = ValenceIonicRadiusEvaluator(structure)\n        site = vire.structure[n]\n        neighs_dists = vire.structure.get_neighbors(site, self.cutoff)\n        rn = vire.radii[vire.structure[n].species_string]\n\n        reldists_neighs = []\n        for neigh, dist in neighs_dists:\n            reldists_neighs.append([dist / (\n                    vire.radii[neigh.species_string] + rn), neigh])\n\n        siw = []\n        min_reldist = min([reldist for reldist, neigh in reldists_neighs])\n        for reldist, s in reldists_neighs:\n            if reldist < (1.0 + self.tol) * min_reldist:\n                w = min_reldist / reldist\n                siw.append({'site': s,\n                            'image': self._get_image(vire.structure, s),\n                            'weight': w,\n                            'site_index': self._get_original_site(\n                                vire.structure, s)})\n\n        return siw"}
{"prompt": "def get_token_async(self, refresh=False):\n    \"\"\"Get an authentication token.\n\n    The token is cached in memcache, keyed by the scopes argument.\n    Uses a random token expiration headroom value generated in the constructor\n    to eliminate a burst of GET_ACCESS_TOKEN API requests.\n\n    Args:\n      refresh: If True, ignore a cached token; default False.\n\n    Yields:\n      An authentication token. This token is guaranteed to be non-expired.\n    \"\"\"", "output": "\n    if not refresh:\n      raise NotImplementedError('Refresh is not implemented.')\n    if not self._cache_control:\n      raise NotImplementedError('CacheControl is not implemented.')\n    if not self._cache_control.is_admin():\n      raise NotImplementedError('CacheControl is not implemented.')\n    if not self._cache_control.is_moderator():\n      raise NotImplementedError('CacheControl is not implemented.')\n    if not self._cache_control.is_admin():\n      raise NotImplementedError('CacheControl is not implemented.')\n    if not self._cache.has_key(self._get_current_token_key()):\n      raise KeyError('No", "solution": "\n    key = '%s,%s' % (self.service_account_id, ','.join(self.scopes))\n    ts = yield _AE_TokenStorage_.get_by_id_async(\n        key,\n        use_cache=True,\n        use_memcache=self.retry_params.memcache_access_token,\n        use_datastore=self.retry_params.save_access_token)\n    if refresh or ts is None or ts.expires < (\n        time.time() + self.expiration_headroom):\n      token, expires_at = yield self.make_token_async(\n          self.scopes, self.service_account_id)\n      timeout = int(expires_at - time.time())\n      ts = _AE_TokenStorage_(id=key, token=token, expires=expires_at)\n      if timeout > 0:\n        yield ts.put_async(memcache_timeout=timeout,\n                           use_datastore=self.retry_params.save_access_token,\n                           force_writes=True,\n                           use_cache=True,\n                           use_memcache=self.retry_params.memcache_access_token)\n    raise ndb.Return(ts.token)"}
{"prompt": "def build_catalog(site, datasets, format=None):\n    \"\"\"Build the DCAT catalog for this site\"\"\"", "output": "\n    if format is None:\n        format = 'xml'\n    if format == 'xml':\n        return build_xml_catalog(site, datasets)\n    elif format == 'json':\n        return build_json_catalog(site, datasets)\n    elif format == 'jsonp':\n        return build_json_catalog(site, datasets)\n    elif format == 'xml':\n        return build_xml_catalog(site, datasets)\n    elif format == 'xmlp':\n        return build_xml_catalog(site, datasets)\n    elif format == 'xml':\n        return build_xml_catalog(site, datasets)", "solution": "\n    site_url = url_for('site.home_redirect', _external=True)\n    catalog_url = url_for('site.rdf_catalog', _external=True)\n    graph = Graph(namespace_manager=namespace_manager)\n    catalog = graph.resource(URIRef(catalog_url))\n\n    catalog.set(RDF.type, DCAT.Catalog)\n    catalog.set(DCT.title, Literal(site.title))\n    catalog.set(DCT.language,\n                Literal(current_app.config['DEFAULT_LANGUAGE']))\n    catalog.set(FOAF.homepage, URIRef(site_url))\n\n    publisher = graph.resource(BNode())\n    publisher.set(RDF.type, FOAF.Organization)\n    publisher.set(FOAF.name, Literal(current_app.config['SITE_AUTHOR']))\n    catalog.set(DCT.publisher, publisher)\n\n    for dataset in datasets:\n        catalog.add(DCAT.dataset, dataset_to_rdf(dataset, graph))\n\n    if isinstance(datasets, Paginable):\n        if not format:\n            raise ValueError('Pagination requires format')\n        catalog.add(RDF.type, HYDRA.Collection)\n        catalog.set(HYDRA.totalItems, Literal(datasets.total))\n        kwargs = {\n            'format': format,\n            'page_size': datasets.page_size,\n            '_external': True,\n        }\n\n        first_url = url_for('site.rdf_catalog_format', page=1, **kwargs)\n        page_url = url_for('site.rdf_catalog_format',\n                           page=datasets.page, **kwargs)\n        last_url = url_for('site.rdf_catalog_format',\n                           page=datasets.pages, **kwargs)\n        pagination = graph.resource(URIRef(page_url))\n        pagination.set(RDF.type, HYDRA.PartialCollectionView)\n\n        pagination.set(HYDRA.first, URIRef(first_url))\n        pagination.set(HYDRA.last, URIRef(last_url))\n        if datasets.has_next:\n            next_url = url_for('site.rdf_catalog_format',\n                               page=datasets.page + 1, **kwargs)\n            pagination.set(HYDRA.next, URIRef(next_url))\n        if datasets.has_prev:\n            prev_url = url_for('site.rdf_catalog_format',\n                               page=datasets.page - 1, **kwargs)\n            pagination.set(HYDRA.previous, URIRef(prev_url))\n\n        catalog.set(HYDRA.view, pagination)\n\n    return catalog"}
{"prompt": "def add_template_global(self, func: Callable, name: Optional[str]=None) -> None:\n        \"\"\"Add a template global.\n\n        This is designed to be used on the application directly. An\n        example usage,\n\n        .. code-block:: python\n\n            def five():\n                return 5\n\n            app.add_template_global(five)\n\n        Arguments:\n            func: The function that is the global.\n            name: The global name (defaults to function name).\n        \"\"\"", "output": "\n        if name is None:\n            name = self.get_global_name()\n        if name in self.templates:\n            raise ValueError(f\"Duplicate template name: {name}\")\n        self.templates[name] = func\n\n    def add_template_global_with_default_context(self, func: Callable, name: Optional[str]=None) -> None:\n        \"\"\"Add a template global with default context.\n\n        This is designed to be used on the application directly. An\n        example usage,\n\n       .. code-block:: python\n\n            def five():\n                return 5\n\n            app.add_template_global_", "solution": "\n        self.jinja_env.globals[name or func.__name__] = func"}
{"prompt": "def load(parser, serializer):\n    \"\"\"Returns a dictionary of builtin functions for Fortran. Checks the\n    cache first to see if we have a serialized version. If we don't, it\n    loads it from the XML file.\n\n    :arg parser: the DocParser instance for parsing the XML tags.\n    :arg serializer: a Serializer instance from the CodeParser to cache\n      the loaded XML file.\n    \"\"\"", "output": "\n    return dict((name, func) for name, func in serializer.get_functions().items())\n\n\nclass XMLParser(object):\n    \"\"\"A class that parses XML files.\n\n    :arg file: the XML file to parse.\n    :arg version: the XML version to parse.\n    :arg functions: a dictionary of builtin functions.\n    \"\"\"\n\n    def __init__(self, file, version, version_map, version_class=XMLParserVersion):\n        \"\"\"Initialize the XMLParser.\n\n        :arg file: the XML file to parse.\n        :arg version: the XML version to parse.\n        :arg", "solution": "\n    fortdir = os.path.dirname(fortpy.__file__)\n    xmlpath = os.path.join(fortdir, \"isense\", \"builtin.xml\")\n    if not os.path.isfile(xmlpath):\n        return {}\n\n    changed_time = os.path.getmtime(xmlpath)\n    cached = serializer.load_module(\"builtin.xml\", changed_time)\n    if cached is None:\n        result = _load_builtin_xml(xmlpath, parser)\n        serializer.save_module(\"builtin.xml\", result, changed_time)\n    else:\n        result = cached\n\n    return result"}
{"prompt": "def resource(ref: str, delimiter: str = None) -> str:\n    \"\"\"\n    Given a (URI) reference, return up to its delimiter (exclusively), or all of it if there is none.\n\n    :param ref: reference\n    :param delimiter: delimiter character (default None maps to '#', or ';' introduces identifiers)\n    \"\"\"", "output": "\n    if delimiter is None:\n        return ref\n    return delimiter.join(ref.split(delimiter))\n\n\ndef uri(uri: str) -> str:\n    \"\"\"\n    Given a URI, return the URI.\n\n    :param uri: URI\n    :return: URI\n    \"\"\"\n    return uri.split('#')[0]\n\n\ndef uri_to_path(uri: str) -> str:\n    \"\"\"\n    Given a URI, return the path portion of the URI.\n\n    :param uri: URI\n    :return: path portion of the URI\n    \"\"\"\n    if uri is None:\n        return ''\n    return uri.split", "solution": "\n\n    return ref.split(delimiter if delimiter else '#')[0]"}
{"prompt": "def target_types_by_alias(self):\n    \"\"\"Returns a mapping from target alias to the target types produced for that alias.\n\n    Normally there is 1 target type per alias, but macros can expand a single alias to several\n    target types.\n\n    :API: public\n\n    :rtype: dict\n    \"\"\"", "output": "\n    return dict((t.type, t.target_types[t.type]) for t in self.types)\n\n  def __repr__(self):\n    return 'Target(name=%s, types=%s)' % (self.name, self.target_types)\n\n\nclass Rule(object):\n  \"\"\"A rule specifies how to process a single rule.\n\n  :API: public\n\n  :param name: The name of the rule.\n  :param types: The mapping of target types to the aliases.\n  :param targets: The targets to process the rule against.\n  :type targets: dict\n  :param", "solution": "\n    target_types_by_alias = defaultdict(set)\n    for alias, target_type in self.target_types.items():\n      target_types_by_alias[alias].add(target_type)\n    for alias, target_macro_factory in self.target_macro_factories.items():\n      target_types_by_alias[alias].update(target_macro_factory.target_types)\n    return dict(target_types_by_alias)"}
{"prompt": "def _http_get(self, url, query):\n        \"\"\"\n        Performs the HTTP GET Request.\n        \"\"\"", "output": "\n        try:\n            response = requests.get(url, params=query, headers=self.headers)\n        except requests.exceptions.ConnectionError:\n            raise ConnectionError(\n                \"Unable to connect to the server. Is the server running?\"\n            )\n\n        if response.status_code!= 200:\n            raise ConnectionError(\n                \"Unable to connect to the server. Is the server running?\"\n            )\n\n        return response.json()\n\n    def _http_post(self, url, data):\n        \"\"\"\n        Performs the HTTP POST Request.\n        \"\"\"\n        try:\n            response = requests.post(url, data=json.dumps(", "solution": "\n\n        if not self.authorization_as_header:\n            query.update({'access_token': self.access_token})\n\n        response = None\n\n        self._normalize_query(query)\n\n        kwargs = {\n            'params': query,\n            'headers': self._request_headers()\n        }\n\n        if self._has_proxy():\n            kwargs['proxies'] = self._proxy_parameters()\n\n        response = requests.get(\n            self._url(url),\n            **kwargs\n        )\n\n        if response.status_code == 429:\n            raise RateLimitExceededError(response)\n\n        return response"}
{"prompt": "def create_socketpair(size=None):\n    \"\"\"\n    Create a :func:`socket.socketpair` to use for use as a child process's UNIX\n    stdio channels. As socket pairs are bidirectional, they are economical on\n    file descriptor usage as the same descriptor can be used for ``stdin`` and\n    ``stdout``. As they are sockets their buffers are tunable, allowing large\n    buffers to be configured in order to improve throughput for file transfers\n    and reduce :class:`mitogen.core.Broker` IO loop iterations.\n    \"\"\"", "output": "\n    if size is None:\n        size = os.sysconf('SC_OPEN_MAX')\n    if size is None:\n        size = int(os.sysconf('SC_OPEN_MAX_FILESIZE'))\n    if size > 0:\n        return socket.socketpair(size, broker_file=broker_file)\n    else:\n        return None\n\n\ndef create_socketpair_with_files(size=None):\n    \"\"\"\n    Create a :func:`socket.socketpair` to use for use as a child process's UNIX\n    stdio channels. As socket pairs are bidirectional, they are e", "solution": "\n    parentfp, childfp = socket.socketpair()\n    parentfp.setsockopt(socket.SOL_SOCKET,\n                        socket.SO_SNDBUF,\n                        size or mitogen.core.CHUNK_SIZE)\n    childfp.setsockopt(socket.SOL_SOCKET,\n                       socket.SO_RCVBUF,\n                       size or mitogen.core.CHUNK_SIZE)\n    return parentfp, childfp"}
{"prompt": "def shiftLeft(col, numBits):\n    \"\"\"Shift the given value numBits left.\n\n    >>> spark.createDataFrame([(21,)], ['a']).select(shiftLeft('a', 1).alias('r')).collect()\n    [Row(r=42)]\n    \"\"\"", "output": "\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.shiftLeft(_to_java_column(col), numBits))\n\n\n@ignore_unicode_prefix\n@since(1.5)\ndef shiftRight(col, numBits):\n    \"\"\"In a string column, this returns the value that is `numBits` shifted right.\n\n    >>> df = spark.createDataFrame([('ab',)], ['a'])\n    >>> df.select(shiftRight('a', 3).alias('r')).collect()\n    [Row(r=42)]\n    \"\"\"\n    sc = SparkContext._", "solution": "\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.shiftLeft(_to_java_column(col), numBits))"}
{"prompt": "def generate_diary(self):\n        \"\"\"\n        extracts event information from core tables into diary files\n        \"\"\"", "output": "\n        # get the list of events\n        events = self.core_tables.find_all('event')\n\n        # get the list of events with the same name\n        if self.name!= events[0].find('name').text:\n            self.name = events[0].find('name').text\n            self.description = events[0].find('description').text\n        else:\n            self.name = None\n            self.description = None\n\n        # get the list of events with the same description\n        if self.description!= events[0].find('description').text:\n            self.description = events[0].", "solution": "\n        print('Generate diary files from Event rows only')\n        for r in self.table:\n            print(str(type(r)) + ' = ', r)"}
{"prompt": "def pkg_upgrade(repo, skip, flag):\n    \"\"\"Checking packages for upgrade\n    \"\"\"", "output": "\n    if not skip:\n        cmd = \"yum update -y --nogpgcheck %s\" % flag\n    else:\n        cmd = \"yum remove -y --nogpgcheck %s\" % flag\n    rc, out = run_command(cmd)\n    if rc == 0:\n        return out.split('\\n')\n    else:\n        return []\n\n\ndef repo_install(module, items, repoq, yum_basecmd, conf_file, en_repos, dis_repos):\n    \"\"\"Install one or more repos\n    \"\"\"\n    pkgs = []\n    res = {}\n    res['results'] = []", "solution": "\n    Msg().checking()\n    PACKAGES_TXT = RepoInit(repo).fetch()[0]\n    pkgs_for_upgrade = []\n    # name = data[0]\n    # location = data[1]\n    # size = data[2]\n    # unsize = data[3]\n    data = repo_data(PACKAGES_TXT, repo, flag=\"\")\n    for pkg in installed():\n        status(0.0005)\n        inst_pkg = split_package(pkg)\n        for name in data[0]:\n            if name:    # this tips because some pkg_name is empty\n                repo_pkg = split_package(name[:-4])\n            if (repo_pkg[0] == inst_pkg[0] and\n                LooseVersion(repo_pkg[1]) > LooseVersion(inst_pkg[1]) and\n                repo_pkg[3] >= inst_pkg[3] and\n                    inst_pkg[0] not in skip and\n                    repo_pkg[1] != \"blacklist\"):\n                pkgs_for_upgrade.append(repo_pkg[0])\n    Msg().done()\n    if \"--checklist\" in flag:\n        pkgs_for_upgrade = choose_upg(pkgs_for_upgrade)\n    return pkgs_for_upgrade"}
{"prompt": "def ensure_table(self, cls):\n        \"\"\"Ensure table's existence - as per the gludb spec.\"\"\"", "output": "\n        if cls.table_exists(cls.table_name):\n            self.log.info(\"Table '%s' already exists.\", cls.table_name)\n        else:\n            self.log.info(\"Creating table '%s'...\", cls.table_name)\n            self.execute(\"CREATE TABLE %s (id INTEGER PRIMARY KEY, \"\n                         \"name TEXT NOT NULL, \"\n                         \"type INTEGER NOT NULL, \"\n                         \"description TEXT NOT NULL, \"\n                         \"unit_id INTEGER NOT NULL, \"\n                         \"description_html TEXT, \"\n                         \"unit_class TEXT NOT NULL, \"\n                         \"unit_", "solution": "\n        id_len = len(uuid())\n        index_names = cls.index_names() or []\n        cols = [\n            'id char(%d) primary key' % (id_len,),\n            'value jsonb'\n        ] + [\n            name + ' text' for name in index_names\n        ]\n\n        table_name = cls.get_table_name()\n\n        with self._conn() as conn:\n            with conn.cursor() as cur:\n                cur.execute('create table if not exists %s (%s);' % (\n                    table_name,\n                    ','.join(cols)\n                ))\n                for name in index_names:\n                    cur.execute('create index if not exists %s on %s(%s);' % (\n                        table_name + '_' + name + '_idx',\n                        table_name,\n                        name\n                    ))"}
{"prompt": "def process(self, sched, coro):\r\n        \"\"\"If there aren't enough coroutines waiting for the signal as the\r\n        recipicient param add the calling coro in another queue to be activated\r\n        later, otherwise activate the waiting coroutines.\"\"\"", "output": "\r\n        if len(self.coroutines) < self.max_coroutines:\r\n            self.coroutines.append(coro)\r\n            return\r\n        self.coroutines.append(coro)\r\n        self.queue.put(coro)\r\n        self.coro_running.put(True)\r\n        self.coro = []\r\n        self.coro_coro = []\r\n        self.coro_queue = []\r\n        self.coro_coro = []\r\n        self.max_coroutines = len(self.coroutines)\r\n        self.max_coro_", "solution": "\r\n        super(Signal, self).process(sched, coro)\r\n        self.result = len(sched.sigwait[self.name])\r\n        if self.result < self.recipients:\r\n            sched.signals[self.name] = self\r\n            self.coro = coro\r\n            return\r\n\r\n        for waitop, waitcoro in sched.sigwait[self.name]:\r\n            waitop.result = self.value\r\n        if self.prio & priority.OP:\r\n            sched.active.extendleft(sched.sigwait[self.name])\r\n        else:\r\n            sched.active.extend(sched.sigwait[self.name])\r\n\r\n        if self.prio & priority.CORO:\r\n            sched.active.appendleft((None, coro))\r\n        else:\r\n            sched.active.append((None, coro))\r\n\r\n        del sched.sigwait[self.name]"}
{"prompt": "def transform_annotation(self, ann, duration):\n        \"\"\"Apply the structure agreement transformation.\n\n        Parameters\n        ----------\n        ann : jams.Annotation\n            The segment annotation\n\n        duration : number > 0\n            The target duration\n\n        Returns\n        -------\n        data : dict\n            data['agree'] : np.ndarray, shape=(n, n), dtype=bool\n        \"\"\"", "output": "\n        # TODO: This is a hack to get the duration from the segment\n        #       and use it to compute the duration.\n        #       We should be able to use the same number of frames as the\n        #       segment, but we don't want to have to compute the same duration\n        #       for the same frame.\n        #       We should be able to use the same number of frames as the\n        #       segment, but we don't want to have to compute the same duration for\n        #       different frames.\n        data = {'agree': np.zeros(ann.num_frames, dtype=bool),", "solution": "\n\n        intervals, values = ann.to_interval_values()\n\n        intervals, values = adjust_intervals(intervals, values,\n                                             t_min=0, t_max=duration)\n        # Re-index the labels\n        ids, _ = index_labels(values)\n\n        rate = float(self.hop_length) / self.sr\n        # Sample segment labels on our frame grid\n        _, labels = intervals_to_samples(intervals, ids, sample_size=rate)\n\n        # Make the agreement matrix\n        return {'agree': np.equal.outer(labels, labels)}"}
{"prompt": "def sync(self):\n        \"\"\"\n        Syncs the items on this layer with the current layer settings.\n        \"\"\"", "output": "\n        self.clear_cache()\n        self.update_geometry()\n        self.update_attributes()\n        self.update_geometry_from_data()\n        self.update_geometry_from_data_and_attributes()\n\n    def __str__(self):\n        \"Returns the string representation of this layer.\"\n        return self.name\n\n    def __repr__(self):\n        \"For `sync`.\"\n        return '<%s %r>' % (self.__class__.__name__, self.name)\n\n    def __init__(self, data, source, name=None):\n        self.data = data\n        self.source =", "solution": "\n        layerData = self.layerData()\n        for item in self.scene().items():\r\n            try:\r\n                if item._layer == self:\r\n                    item.syncLayerData(layerData)\r\n            except AttributeError:\r\n                continue"}
{"prompt": "def text(text, message='', title=''):\n    \"\"\"\n    This function is suitable for displaying general text, which can be longer\n    than in :func:`message`\n\n    :ref:`screenshots<text>`\n\n    :param text: (long) text to be displayed\n    :param message: (short) message to be displayed.\n    :param title: window title\n    :rtype: None\n    \"\"\"", "output": "\n    if not message:\n        message = ''\n    if not title:\n        title = ''\n    win = sublime.active_window()\n    if win:\n        win.focus_view(win.active_view())\n    else:\n        win = sublime.active_window()\n    if win:\n        win.run_command('insert', {'characters': message,'start': text})\n    else:\n        win.run_command('insert', {'characters': message,'start': text, 'end': text})\n\n\ndef get_window_title(window):\n    \"\"\"\n    This function returns the title of", "solution": "\n    return backend_api.opendialog(\"text\", dict(text=text, message=message, title=title))"}
{"prompt": "def render_page(path):\n    \"\"\"Internal interface to the page view.\n\n    :param path: Page path.\n    :returns: The rendered template.\n    \"\"\"", "output": "\n    return render_template('page.html', path=path)\n\n\n@app.route('/<path:path>')\ndef page_view(path):\n    \"\"\"Internal interface to the page view.\n\n    :param path: Page path.\n    :returns: The rendered template.\n    \"\"\"\n    return render_page(path)\n\n\n@app.route('/<path:path>')\ndef page_view_path(path):\n    \"\"\"Internal interface to the page view path.\n\n    :param path: Page path.\n    :returns: The page path.\n    \"\"\"\n    return render_path(path, path", "solution": "\n    try:\n        page = Page.get_by_url(request.path)\n    except NoResultFound:\n        abort(404)\n\n    return render_template(\n        [page.template_name, current_app.config['PAGES_DEFAULT_TEMPLATE']],\n        page=page)"}
{"prompt": "def update(self, *args, **kwargs):\n        \"\"\"Update ConfigMap from mapping/iterable.\n\n        If the key exists the entry is updated else it is added.\n\n        Args:\n            *args: variable length argument list.  A valid argument is a two item\n                tuple/list.  The first item is the key and the second is the value.\n            **kwargs: Arbitrary keyword arguments representing the config.\n        \"\"\"", "output": "\n        if len(args)!= 2:\n            raise TypeError, \"expected at least 2 arguments, got %s\" % len(args)\n        if not isinstance(kwargs, dict):\n            raise TypeError, \"expected keyword arguments, got %s\" % kwargs\n        for k, v in kwargs.items():\n            self[k] = v\n\n    def setdefault(self, key, default=None):\n        \"\"\"Set a variable's value from a dictionary.\n\n        Args:\n            *key: variable name.\n            *default: default value to set.\n        \"\"\"\n        if key not in self:\n            self[key] = default", "solution": "\n        for k, v in args:\n            self[k] = v\n        for k, v in kwargs.items():\n            self[k] = v"}
{"prompt": "def verification_events(self):\n        \"\"\"\n        Events related to command verification.\n\n        :type: List[:class:`.CommandHistoryEvent`]\n        \"\"\"", "output": "\n        return [CommandHistoryEvent(command=command, timestamp=timestamp) for command, timestamp in self._command_history_events]\n\n    def _get_command_history_events(self):\n        return [CommandHistoryEvent(command=command) for command in self._command_history_events]\n\n    def _get_command_line_events(self):\n        return [CommandLineEvent(command=command) for command in self._command_line_events]\n\n    def _get_command_line_options(self):\n        return [\n            Option(\n                name='timeout',\n                type=int,\n               ", "solution": "\n        queued = self._assemble_event('Verifier_Queued')\n        started = self._assemble_event('Verifier_Started')\n        return [x for x in [queued, started] if x]"}
{"prompt": "def get_channel(device, ch_name, channel_dict, loader, resource_dict):\n    \"\"\"Get a channels from a channels dictionary.\n\n    :param name: name of the device\n    :param device_dict: device dictionary\n    :rtype: Device\n    \"\"\"", "output": "\n    if not isinstance(device_dict, dict):\n        raise TypeError(\"device_dict must be a dictionary\")\n    if not isinstance(loader, dict):\n        raise TypeError(\"loader must be a dictionary\")\n    if not isinstance(resource_dict, dict):\n        raise TypeError(\"resource_dict must be a dictionary\")\n\n    channels = []\n    for ch in channel_dict:\n        if ch not in device:\n            raise ValueError(\"Channel %s not found in device dictionary\" % ch)\n        if not loader.get(ch):\n            raise ValueError(\"Channel %s not found in loader dictionary\" % ch)\n        channels.", "solution": "\n    channel_dict = get_bases(channel_dict, loader)\n\n    r_ids = resource_dict.get('channel_ids', {}).get(ch_name, [])\n    ids = r_ids if r_ids else channel_dict.get('ids', {})\n\n    can_select = False if channel_dict.get('can_select') == 'False' else True\n    channels = Channels(device, ids, can_select)\n\n    update_component(ch_name, channels, channel_dict)\n\n    return channels"}
{"prompt": "def focus_next_unfolded(self):\n        \"\"\"focus next unfolded message in depth first order\"\"\"", "output": "\n        self.set_depth(1)\n        self.set_mode(self.mode)\n        self.set_tags(self.tags)\n        self.set_flags(self.flags)\n        self.set_flags(self.flags)\n        self.set_flags(self.flags)\n        self.set_flags(self.flags)\n        self.set_flags(self.flags)\n        self.set_flags(self.flags)\n        self.set_flags(self.flags)\n        self.set_flags(self.flags)\n        self.set_flags(self.", "solution": "\n        self.focus_property(lambda x: not x.is_collapsed(x.root),\n                            self._tree.next_position)"}
{"prompt": "def gatk_remove_missingalt(in_file, data):\n    \"\"\"\n    GATK 4.1.0.0 outputs variants that have missing ALTs, which breaks downstream\n    tools, this filters those out.\n    \"\"\"", "output": "\n    out_file = \"%s.gatk\" % in_file\n    if not utils.file_exists(out_file):\n        return out_file\n\n    if not utils.file_exists(in_file):\n        return out_file\n\n    if not utils.file_exists(utils.get_ref_file(in_file)):\n        return out_file\n\n    ref_file = utils.get_ref_file(in_file)\n    if not utils.file_exists(ref_file):\n        return ref_file\n\n    utils.system(\"rm -f %s\" % ref_file", "solution": "\n    base = in_file.split('.vcf.gz')[0]\n    out_file = \"%s-nomissingalt%s\" % (base, '.vcf.gz')\n    if utils.file_exists(out_file):\n        return out_file\n    no_gzip_out = out_file.replace(\".vcf.gz\", \".vcf\")\n    with file_transaction(no_gzip_out) as tx_out_file:\n        with utils.open_gzipsafe(in_file) as in_handle, open(tx_out_file, \"w\") as out_handle:\n            for line in in_handle:\n                line = remove_missingalt(line)\n                if line:\n                    out_handle.write(line)\n    return vcfutils.bgzip_and_index(no_gzip_out, data[\"config\"])"}
{"prompt": "def parse_pylint_output(output):\n    \"\"\"\n    Parses pylint output, counting number of errors, conventions, etc\n    :param output: output list generated by run_pylint()\n    :return:\n    \"\"\"", "output": "\n    errors = 0\n    for line in output:\n        if line.startswith('ERROR'):\n            errors += 1\n        elif line.startswith('CONVERTER'):\n            conversions = 0\n            for i in range(0, len(line.split('\\n'))):\n                if line.split('\\n')[i] == 'CONVERT':\n                    conversions += int(line.split('\\n')[i+1])\n                elif line.split('\\n')[i] == 'CONVERT_TO_NONE':\n                    conversions += int(line.split('\\n')[i+1])\n                else:\n                    raise Exception('Unknown pylint output:", "solution": "\n\n    stripped_output = [x[0] for x in output]\n\n    counter = Counter(stripped_output)\n\n    return counter"}
{"prompt": "def on_channel_open(self, channel):\n        \"\"\"\n        Input channel creation callback\n        Queue declaration done here\n\n        Args:\n            channel: input channel\n        \"\"\"", "output": "\n        self.channels.append(channel)\n\n    def on_channel_close(self, channel):\n        \"\"\"\n        Input channel termination callback\n        Queue declaration done here\n\n        Args:\n            channel: input channel\n        \"\"\"\n        self.channels.remove(channel)\n\n    def on_channel_open_error(self, channel, error):\n        \"\"\"\n        Input channel creation callback\n        Queue declaration done here\n\n        Args:\n            channel: input channel\n            error: error message\n        \"\"\"\n        self.channels.append(channel)\n\n    def on_channel_close_error(self, channel, error):\n        \"\"\"\n        Input channel termination callback\n       ", "solution": "\n        self.in_channel.exchange_declare(exchange='input_exc', type='topic', durable=True)\n        channel.queue_declare(callback=self.on_input_queue_declare, queue=self.INPUT_QUEUE_NAME)"}
{"prompt": "def compute_jaccard_index(x_set, y_set):\n    \"\"\"Return the Jaccard similarity coefficient of 2 given sets.\n\n    Args:\n        x_set (set): first set.\n        y_set (set): second set.\n\n    Returns:\n        float: Jaccard similarity coefficient.\n\n    \"\"\"", "output": "\n    return jaccard_similarity_coefficient(x_set, y_set, 2)\n\n\ndef compute_jaccard_index_2(x_set, y_set):\n    \"\"\"Return the Jaccard similarity coefficient of 2 given sets.\n\n    Args:\n        x_set (set): first set.\n        y_set (set): second set.\n\n    Returns:\n        float: Jaccard similarity coefficient.\n\n    \"\"\"\n    return compute_jaccard_index(x_set, y_set) / 2.0\n\n\ndef compute_jaccard_index_3(x", "solution": "\n    if not x_set or not y_set:\n        return 0.0\n\n    intersection_cardinal = len(x_set & y_set)\n    union_cardinal = len(x_set | y_set)\n\n    return intersection_cardinal / float(union_cardinal)"}
{"prompt": "def return_hdr(self):\n        \"\"\"Return the header for further use.\n\n        Returns\n        -------\n        subj_id : str\n            subject identification code\n        start_time : datetime\n            start time of the dataset\n        s_freq : float\n            sampling frequency\n        chan_name : list of str\n            list of all the channels\n        n_samples : int\n            number of samples in the dataset\n        orig : dict\n            additional information taken directly from the header\n\n        Notes\n        -----\n        It only reads hdf5 matlab files and the VARiable needs to be called\n        'data'\n\n        h5py is necessary for this function\n        \"\"\"", "output": " \"\"\"\n        return self.data\n\n    def read_h5(self, h5_file):\n        \"\"\"Read a matlab file.\n\n        Parameters\n        ----------\n        h5_file : str\n            name of the hdf5 file\n\n        Returns\n        -------\n        subj_id : str\n            subject identification code\n        start_time : datetime\n            start time of the dataset\n        s_freq : float\n            sampling frequency\n        chan_name : list of str\n            all the channels\n        n_samples : int\n            number of samples in the dataset\n        orig : dict\n            additional information taken directly from the header\n\n        Notes\n        -----\n        This function reads", "solution": "\n        # fieldtrip does not have this information\n        orig = dict()\n        subj_id = str()\n        start_time = datetime.fromordinal(1)  # fake\n\n        try:\n            ft_data = loadmat(self.filename, struct_as_record=True,\n                              squeeze_me=True)\n            if VAR not in ft_data:\n                raise KeyError('Save the FieldTrip variable as ''{}"}
{"prompt": "def _update_plotting_params(self, **kwargs):\n        \"\"\"Some plotting parameters can be changed through the tool; this\n        updataes those plotting parameters.\n        \"\"\"", "output": "\n        for key, value in kwargs.items():\n            if key == 'x':\n                self.x_label = value\n            elif key == 'y':\n                self.y_label = value\n            elif key == 'z':\n                self.z_label = value\n            elif key == 'color':\n                self.color = value\n            elif key =='marker':\n                self.marker = value\n            elif key =='markersize':\n                self.markersize = value\n            elif key =='markerfacecolor':\n                self.markerfacecolor = value\n            elif key =='markerfacecolor':\n                self.markerfacecolor", "solution": "\n        scalars = kwargs.get('scalars', None)\n        if scalars is not None:\n            old = self.display_params['scalars']\n            self.display_params['scalars'] = scalars\n            if old != scalars:\n                self.plotter.subplot(*self.loc)\n                self.plotter.remove_actor(self._data_to_update, reset_camera=False)\n                self._need_to_update = True\n                self.valid_range = self.input_dataset.get_data_range(scalars)\n                # self.display_params['rng'] = self.valid_range\n        cmap = kwargs.get('cmap', None)\n        if cmap is not None:\n            self.display_params['cmap'] = cmap"}
{"prompt": "def course_is_user_registered(self, course, username=None):\n        \"\"\"\n        Checks if a user is registered\n        :param course: a Course object\n        :param username: The username of the user that we want to check. If None, uses self.session_username()\n        :return: True if the user is registered, False else\n        \"\"\"", "output": "\n        try:\n            user = self.session_username(username)\n            return user in course.enrollment_set.all()\n        except ObjectDoesNotExist:\n            return False\n\n    def get_user_by_email(self, email):\n        \"\"\"\n        Gets the user object from the User model, if exists.\n        :param email: The email of the user we want to check.\n        :return: The user object if exists, None otherwise\n        \"\"\"\n        try:\n            return User.objects.get(email=email)\n        except ObjectDoesNotExist:\n            return None\n\n    def get_user_by_username(self, username):\n       ", "solution": "\n        if username is None:\n            username = self.session_username()\n\n        if self.has_staff_rights_on_course(course, username):\n            return True\n\n        return self._database.aggregations.find_one({\"students\": username, \"courseid\": course.get_id()}) is not None"}
{"prompt": "def _create_opt_rule(self, rulename):\n        \"\"\" Given a rule name, creates an optional ply.yacc rule\n            for it. The name of the optional rule is\n            <rulename>_opt\n        \"\"\"", "output": "\n        if not self.rules:\n            self.rules = {}\n            self.rulesname = rulename\n        self.rules[rulename] = {\n            'type': 'p_rule',\n            'name': rulename,\n            'p_pattern': '',\n            'action': 'permit',\n            'p_name': '',\n            'p_args': [],\n            'p_file': '',\n            'p_fileloc': '',\n            'p_line': '',\n            'p_col': '',\n            'p_error': '',\n            'p_errorloc': '',\n            'p_errorline': '',", "solution": "\n        optname = rulename + '_opt'\n\n        def optrule(self, p):\n            p[0] = p[1]\n\n        optrule.__doc__ = '%s : empty\\n| %s' % (optname, rulename)\n        optrule.__name__ = 'p_%s' % optname\n        setattr(self.__class__, optrule.__name__, optrule)"}
{"prompt": "def disconnect_node(node, src=True, dst=True):\n    \"\"\"Disconnect all connections from node\n\n    :param node: the node to disconnect\n    :type node: str\n    :returns: None\n    :rtype: None\n    :raises: None\n    \"\"\"", "output": "\n    for i in range(len(node.remotes)):\n        if node.remotes[i] == src:\n            node.remotes[i] = dst\n        else:\n            if not i == len(node.remotes)-1:\n                node.remotes[i+1].disconnect()\n            if not src:\n                node.remotes[i].disconnect()\n            if dst:\n                node.remotes[i+1].disconnect(remotes[i+1)\n\n    if node.remotes[0] == dst:\n        return\n\n    for i in range(len(node.remotes", "solution": "\n    if dst:\n        destconns = cmds.listConnections(node, connections=True, plugs=True, source=False) or []\n        for i in range(0, len(destconns), 2):\n            source, dest = destconns[i], destconns[i+1]\n            cmds.disconnectAttr(source, dest)\n    if src:\n        srcconns = cmds.listConnections(node, connections=True, plugs=True, destination=False) or []\n        for i in range(0, len(srcconns), 2):\n            source, dest = srcconns[i+1], srcconns[i]\n            cmds.disconnectAttr(source, dest)"}
{"prompt": "def toil_get_file(file_store, index, existing, file_store_id):\n    \"\"\"Get path to input file from Toil jobstore.\"\"\"", "output": "\n    if existing:\n        return existing\n    else:\n        return file_store_id\n\n\ndef toil_get_file_id(file_id, existing):\n    \"\"\"Get path to input file from Toil jobid.\"\"\"\n    if existing:\n        return existing\n    else:\n        return file_id\n\n\ndef toil_get_file_path(file_id):\n    \"\"\"Get path to input file from Toil jobid.\"\"\"\n    return file_id\n\n\ndef toil_get_file_path_id(file_path_id):\n    \"\"\"Get path to input file from Toil job", "solution": "\n\n    if not file_store_id.startswith(\"toilfs:\"):\n        return file_store.jobStore.getPublicUrl(file_store.jobStore.importFile(file_store_id))\n    src_path = file_store.readGlobalFile(file_store_id[7:])\n    index[src_path] = file_store_id\n    existing[file_store_id] = src_path\n    return schema_salad.ref_resolver.file_uri(src_path)"}
{"prompt": "def get_broadcast(self, broadcast_id):\n        \"\"\"\n        Use this method to get details on a broadcast that is in-progress.\n\n        :param String broadcast_id: The ID of the broadcast you want to stop\n\n        :rtype A Broadcast object, which contains information of the broadcast: id, sessionId\n        projectId, createdAt, updatedAt, resolution, broadcastUrls and status\n        \"\"\"", "output": "\n        return self._get_resource(\n            \"getBroadcast\",\n            {\"broadcast_id\": broadcast_id},\n            False,\n            \"GET\",\n            False,\n            \"calendar\",\n            \"broadcasts/{calendar_id}/stop\".format(\n                calendar_id=self._calendar_id\n            )\n        )\n\n    def get_calendar_events(self):\n        \"\"\"\n        Use this method to get details on all calendar events.\n\n        :rtype A list of CalendarEvent objects, each representing a single calendar event\n        \"\"\"\n        return self._get_resource(\n            \"getCalendarEvents\",\n            {\"calendar_id\": self._", "solution": "\n        endpoint = self.endpoints.broadcast_url(broadcast_id)\n        response = requests.get(\n            endpoint,\n            headers=self.json_headers(),\n            proxies=self.proxies,\n            timeout=self.timeout\n        )\n\n        if response.status_code == 200:\n            return Broadcast(response.json())\n        elif response.status_code == 400:\n            raise BroadcastError(\n                'Invalid request. This response may indicate that data in your request '\n                'data is invalid JSON.')\n        elif response.status_code == 403:\n            raise AuthError('Authentication error.')\n        elif response.status_code == 409:\n            raise BroadcastError('No matching broadcast found (with the specified ID).')\n        else:\n            raise RequestError('OpenTok server error.', response.status_code)"}
{"prompt": "def parse_corant(blob):\n    \"\"\"Creates new blob entries for the given blob keys\"\"\"", "output": "\n    return [\n        {\n            'key': k,\n            'value': v,\n        } for k, v in six.iteritems(blob)\n    ]\n\n\ndef parse_blob(blob):\n    \"\"\"Creates new blob entries for the given blob keys\"\"\"\n    return [\n        {\n            'key': k,\n            'value': v,\n        } for k, v in six.iteritems(blob)\n    ]\n\n\ndef get_blob(key):\n    \"\"\"Returns the entry for the given key\"\"\"\n    return [\n        {\n            'key': k,\n            'value': v,\n        } for k, v in", "solution": "\n\n    if 'track_seamuon' in blob.keys():\n\n        muon = blob['track_seamuon']\n\n        blob['Muon'] = Table({\n            'id': np.array(muon)[:, 0].astype(int),\n            'pos_x': np.array(muon)[:, 1],\n            'pos_y': np.array(muon)[:, 2],\n            'pos_z': np.array(muon)[:, 3],\n            'dir_x': np.array(muon)[:, 4],\n            'dir_y': np.array(muon)[:, 5],\n            'dir_z': np.array(muon)[:, 6],\n            'energy': np.array(muon)[:, 7],\n            'time': np.array(muon)[:, 8],\n            'particle_id': np.array(muon)[:, 9].astype(int),\n            'is_charm': np.array(muon)[:, 10].astype(int),\n            'mother_pid': np.array(muon)[:, 11].astype(int),\n            'grandmother_pid': np.array(muon)[:, 11].astype(int),\n        },\n                             h5loc='muon')\n\n        blob['MuonMultiplicity'] = Table({\n            'muon_multiplicity': len(np.array(muon)[:, 6])\n        },\n                                         h5loc='muon_multiplicity')\n\n    if 'track_seaneutrino' in blob.keys():\n\n        nu = blob['track_seaneutrino']\n\n        blob['Neutrino'] = Table({\n            'id': np.array(nu)[:, 0].astype(int),\n            'pos_x': np.array(nu)[:, 1],\n            'pos_y': np.array(nu)[:, 2],\n            'pos_z': np.array(nu)[:, 3],\n            'dir_x': np.array(nu)[:, 4],\n            'dir_y': np.array(nu)[:, 5],\n            'dir_z': np.array(nu)[:, 6],\n            'energy': np.array(nu)[:, 7],\n            'time': np.array(nu)[:, 8],\n            'particle_id': np.array(nu)[:, 9].astype(int),\n            'is_charm': np.array(nu)[:, 10].astype(int),\n            'mother_pid': np.array(nu)[:, 11].astype(int),\n            'grandmother_pid': np.array(nu)[:, 11].astype(int),\n        },\n                                 h5loc='nu')\n        blob['NeutrinoMultiplicity'] = Table({\n            'total': len(np.array(nu)[:, 6]),\n            'nue': len(np.array(nu)[:, 6][np.array(nu)[:, 9] == 66]),\n            'anue': len(np.array(nu)[:, 6][np.array(nu)[:, 9] == 67]),\n            'numu': len(np.array(nu)[:, 6][np.array(nu)[:, 9] == 68]),\n            'anumu': len(np.array(nu)[:, 6][np.array(nu)[:, 9] == 69]),\n        },\n                                             h5loc='nu_multiplicity')\n\n    if ('track_seamuon' or 'track_seaneutrino') in blob.keys():\n\n        blob['Weights'] = Table({\n            'w1': blob['weights'][0][0],\n            'w2': blob['weights'][0][1],\n            'w3': blob['weights'][0][2],\n        },\n                                h5loc='weights')\n\n    if 'track_primary' in blob.keys():\n\n        primary = blob['track_primary']\n\n        blob['Primary'] = Table({\n            'id': np.array(primary)[:, 0].astype(int),\n            'pos_x': np.array(primary)[:, 1],\n            'pos_y': np.array(primary)[:, 2],\n            'pos_z': np.array(primary)[:, 3],\n            'dir_x': np.array(primary)[:, 4],\n            'dir_y': np.array(primary)[:, 5],\n            'dir_z': np.array(primary)[:, 6],\n            'energy': np.array(primary)[:, 7],\n            'time': np.array(primary)[:, 8],\n            'particle_id': np.array(primary)[:, 9].astype(int)\n        },\n                                h5loc='primary')\n\n    return blob"}
{"prompt": "def _prepare_wsdl_objects(self):\n        \"\"\"\n        Create the data structure and get it ready for the WSDL request.\n        \"\"\"", "output": "\n        wsdl = self.wsdl\n        wsdl_url = wsdl.url\n        wsdl_version = wsdl.version\n\n        # Create the root element\n        root = wsdl.root\n        root.setAttribute(\"xmlns:wsdl\", WSDL_NAMESPACE)\n        root.setAttribute(\"xmlns:xsi\", \"http://www.w3.org/2001/XMLSchema-instance\")\n        root.setAttribute(\"xmlns:xsd\", \"http://www.w3.org/2001/XMLSchema\")\n\n        # Create the wsdl:Service element\n        service = wsdl.services[wsdl_url]\n        wsdl_", "solution": "\n\n        # Service defaults for objects that are required.\n        self.MultipleMatchesAction = 'RETURN_ALL'\n        self.Constraints = self.create_wsdl_object_of_type('SearchLocationConstraints')\n        self.Address = self.create_wsdl_object_of_type('Address')\n        self.LocationsSearchCriterion = 'ADDRESS'\n        self.SortDetail = self.create_wsdl_object_of_type('LocationSortDetail')"}
{"prompt": "def profiler():\n    \"\"\"Profile the block\"\"\"", "output": "\n    import cProfile\n    return cProfile.runctx('s', globals(), locals(), 'block.prof')\n\ndef _get_type(name):\n    \"\"\"Returns the type of the named type\"\"\"\n    return type_dict[name]\n\ndef _get_type_name(name):\n    \"\"\"Returns the type name of the named type\"\"\"\n    return type_dict[name]\n\ndef _get_type_name_and_args(name, args):\n    \"\"\"Returns the type name and arguments of a named type\"\"\"\n    return name, args\n\ndef _get_type_name_", "solution": "\n    import cProfile\n    import pstats\n    pr = cProfile.Profile()\n    pr.enable()\n    yield\n    pr.disable()\n    ps = pstats.Stats(pr).sort_stats('tottime')\n    ps.print_stats()"}
{"prompt": "def popn(self, buffer_type, count):\n        \"\"\"Remove and return the oldest count values from the named buffer\n\n        Args:\n            buffer_type (str): The buffer to pop from (either u\"storage\" or u\"streaming\")\n            count (int): The number of readings to pop\n\n        Returns:\n            list(IOTileReading): The values popped from the buffer\n        \"\"\"", "output": "\n        return [reading.pop(count) for reading in self.readings if reading.buffer_type == buffer_type]\n\n    def get_latest_reading(self, buffer_id):\n        \"\"\"Get the latest reading from the named buffer\n\n        Args:\n            buffer_id (str): The buffer id to get the latest reading from\n\n        Returns:\n            IOTileReading: The latest reading from the buffer\n        \"\"\"\n        return self.readings[0]\n\n    def get_latest_buffer(self, buffer_type, count):\n        \"\"\"Get the latest buffer values from the named buffer\n\n        Args:\n            buffer", "solution": "\n\n        buffer_type = str(buffer_type)\n\n        if buffer_type == u'streaming':\n            chosen_buffer = self.streaming_data\n        else:\n            chosen_buffer = self.storage_data\n\n        if count > len(chosen_buffer):\n            raise StreamEmptyError(\"Not enough data in buffer for popn command\", requested=count, stored=len(chosen_buffer), buffer=buffer_type)\n\n        popped = chosen_buffer[:count]\n        remaining = chosen_buffer[count:]\n\n        if buffer_type == u'streaming':\n            self.streaming_data = remaining\n        else:\n            self.storage_data = remaining\n\n        return popped"}
{"prompt": "def arrival_delay_greater_than(item_id, delay, namespace=\"_expected_arrival\"):\n    \"\"\"Check if an item arrival is delayed more than a given amount.\"\"\"", "output": "\n    return _arrival_delay_greater_than(item_id, delay, namespace)\n\n\ndef arrival_delay_less_than(delay, value, namespace=\"_expected_arrival\"):\n    \"\"\"Check if an item arrival is delayed more than a given amount.\"\"\"\n    return _arrival_delay_greater_than(item_id, delay, namespace)\n\n\ndef arrival_delay_greater_than_or_equal(delay, value, namespace=\"_expected_arrival\"):\n    \"\"\"Check if an item arrival is delayed less than or equal amount.\"\"\"\n    return _arrival_", "solution": "\n    expected = get_state(item_id, namespace=namespace)\n    now = time.time()\n    if expected and (now - expected) > delay:\n        logger.error(\"Timeout: waited %s seconds for parent.\", delay)\n        return True\n    elif expected:\n        logger.info(\"Still out of order but no timeout: %s-%s <= %s.\",\n                    now, expected, delay)\n        return False\n    elif delay > 0:\n        logger.info(\"Storing expected arrival time (%s) for context '%s'\",\n                    datetime.fromtimestamp(now).isoformat(), item_id)\n        set_state(item_id, now, namespace=namespace)\n        return False\n    else:\n        logger.info(\"Event is out of order but not waiting for parent.\")\n        return True"}
{"prompt": "def save_profile_as(self):\n        \"\"\"Save the minimum needs under a new profile name.\n        \"\"\"", "output": "\n        self.profile_name = self.min_profile_name + \" \" + self.min_profile_name_length\n        self.save()\n\n    def save_profile_as_profile_id(self):\n        \"\"\"Save the new profile id.\n        \"\"\"\n        self.profile_id = self.min_profile_id + \" \" + self.min_profile_name_length\n        self.save()\n\n    def save_profile_as_user(self):\n        \"\"\"Save the minimum needs under a new username.\n        \"\"\"\n        self.user = self.create_account_and_fetch_", "solution": "\n        # noinspection PyCallByClass,PyTypeChecker\n        file_name_dialog = QFileDialog(self)\n        file_name_dialog.setAcceptMode(QFileDialog.AcceptSave)\n        file_name_dialog.setNameFilter(self.tr('JSON files (*.json *.JSON)'))\n        file_name_dialog.setDefaultSuffix('json')\n        dir = os.path.join(QgsApplication.qgisSettingsDirPath(),\n                           'inasafe', 'minimum_needs')\n        file_name_dialog.setDirectory(expanduser(dir))\n        if file_name_dialog.exec_():\n            file_name = file_name_dialog.selectedFiles()[0]\n        else:\n            return\n        file_name = basename(file_name)\n        file_name = file_name.replace('.json', '')\n        minimum_needs = {'resources': []}\n        self.mark_current_profile_as_saved()\n        for index in range(self.resources_list.count()):\n            item = self.resources_list.item(index)\n            minimum_needs['resources'].append(item.resource_full)\n        minimum_needs['provenance'] = self.provenance.text()\n        minimum_needs['profile'] = file_name\n        self.minimum_needs.update_minimum_needs(minimum_needs)\n        self.minimum_needs.save()\n        self.minimum_needs.save_profile(file_name)\n        if self.profile_combo.findText(file_name) == -1:\n            self.profile_combo.addItem(file_name)\n        self.profile_combo.setCurrentIndex(\n            self.profile_combo.findText(file_name))"}
{"prompt": "def connect_array(self, address, connection_key, connection_type, **kwargs):\n        \"\"\"Connect this array with another one.\n\n        :param address: IP address or DNS name of other array.\n        :type address: str\n        :param connection_key: Connection key of other array.\n        :type connection_key: str\n        :param connection_type: Type(s) of connection desired.\n        :type connection_type: list\n        :param \\*\\*kwargs: See the REST API Guide on your array for the\n                           documentation on the request:\n                           **POST array/connection**\n        :type \\*\\*kwargs: optional\n\n        :returns: A dictionary describing the connection to the other array.\n        :rtype: ResponseDict\n\n        .. note::\n\n            Currently, the only type of connection is \"replication\".\n\n        .. note::\n\n            Requires use of REST API 1.2 or later.\n\n        \"\"\"", "output": "kwargs: dict\n        :return: None\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('callback'):\n            return self.put_with_http_info(address, 'connect_array', connection_key, **kwargs)\n        else:\n            (data) = self.put_with_http_info(address, 'connect_array', connection_type, **kwargs)\n            return data\n\n    def disconnect_array(self, **kwargs):\n        \"\"\"Disconnect this array from the DNS server.\n\n       ", "solution": "\n        data = {\"management_address\": address,\n                \"connection_key\": connection_key,\n                \"type\": connection_type}\n        data.update(kwargs)\n        return self._request(\"POST\", \"array/connection\", data)"}
{"prompt": "def create_cloudwatch_event(app_name, env, region, rules):\n    \"\"\"Create cloudwatch event for lambda from rules.\n\n    Args:\n        app_name (str): name of the lambda function\n        env (str): Environment/Account for lambda function\n        region (str): AWS region of the lambda function\n        rules (dict): Trigger rules from the settings\n    \"\"\"", "output": "\n    return {\n        \"name\": app_name,\n        \"event_source\": \"lambda\",\n        \"event_version\": \"1.0\",\n        \"event_source_version\": \"1.0\",\n        \"event_version_version\": \"1.0\",\n        \"version\": \"1.0\",\n        \"region\": region,\n        \"rules\": rules,\n        \"event_version_id\": \"1.0\",\n        \"event_source_id\": \"lambda\",\n        \"event_version_name\": \"AWS Lambda\",\n        \"event_version_name\": \"AWS Lambda\",\n        \"", "solution": "\n    session = boto3.Session(profile_name=env, region_name=region)\n    cloudwatch_client = session.client('events')\n\n    rule_name = rules.get('rule_name')\n    schedule = rules.get('schedule')\n    rule_description = rules.get('rule_description')\n    json_input = rules.get('json_input', {})\n\n    if schedule is None:\n        LOG.critical('Schedule is required and no schedule is defined!')\n        raise InvalidEventConfiguration('Schedule is required and no schedule is defined!')\n\n    if rule_name is None:\n        LOG.critical('Rule name is required and no rule_name is defined!')\n        raise InvalidEventConfiguration('Rule name is required and no rule_name is defined!')\n    else:\n        LOG.info('%s and %s', app_name, rule_name)\n        rule_name = \"{}_{}\".format(app_name, rule_name.replace(' ', '_'))\n\n    if rule_description is None:\n        rule_description = \"{} - {}\".format(app_name, rule_name)\n\n    lambda_arn = get_lambda_arn(app=app_name, account=env, region=region)\n\n    # Add lambda permissions\n    account_id = get_env_credential(env=env)['accountId']\n    principal = \"events.amazonaws.com\"\n    statement_id = '{}_cloudwatch_{}'.format(app_name, rule_name)\n    source_arn = 'arn:aws:events:{}:{}:rule/{}'.format(region, account_id, rule_name)\n    add_lambda_permissions(\n        function=lambda_arn,\n        statement_id=statement_id,\n        action='lambda:InvokeFunction',\n        principal=principal,\n        source_arn=source_arn,\n        env=env,\n        region=region, )\n\n    # Create Cloudwatch rule\n    cloudwatch_client.put_rule(\n        Name=rule_name,\n        ScheduleExpression=schedule,\n        State='ENABLED',\n        Description=rule_description, )\n\n    targets = []\n    # TODO: read this one from file event-config-*.json\n    json_payload = '{}'.format(json.dumps(json_input))\n\n    target = {\n        \"Id\": app_name,\n        \"Arn\": lambda_arn,\n        \"Input\": json_payload,\n    }\n\n    targets.append(target)\n\n    put_targets_response = cloudwatch_client.put_targets(Rule=rule_name, Targets=targets)\n    LOG.debug('Cloudwatch put targets response: %s', put_targets_response)\n\n    LOG.info('Created Cloudwatch event \"%s\" with schedule: %s', rule_name, schedule)"}
{"prompt": "def getAllConfig(self, fmt='json'):\n        \"\"\"\n            return all element configurations as json string file.\n            could be further processed by beamline.Lattice class\n\n            :param fmt: 'json' (default) or 'dict'\n        \"\"\"", "output": "\n        if fmt == 'dict':\n            return json.dumps(self.config, indent=4)\n        elif fmt == 'list':\n            return json.dumps(self.config, indent=4)\n        elif fmt == 'beamline':\n            return json.dumps(self.beamline, indent=4)\n        else:\n            raise ValueError(\"Unknown format: %s\" % (fmt))\n\n    def getAllConfigJson(self, fmt='json'):\n        \"\"\"\n            return all element configurations as json string file.\n            could be further processed by beamline.Lattice class\n\n            :param fmt: 'json' (", "solution": "\n        for e in self.getCtrlConf(msgout=False):\n            self._lattice_confdict.update(e.dumpConfig(type='simu'))\n        self._lattice_confdict.update(self._lattice.dumpConfig())\n        if fmt == 'json':\n            return json.dumps(self._lattice_confdict)\n        else:\n            return self._lattice_confdict"}
{"prompt": "def set_headers(self):\n        \"\"\"\n        Sets the content headers on the response.\n        \"\"\"", "output": "\n        self.headers = self.response.headers\n        self.status_code = self.response.status_code\n        self.reason = self.response.reason\n        self.body = self.response.body\n        self.body_length = len(self.body)\n        self.body_type = self.response.body_type\n        self.body_length = len(self.body)\n        self.body_type_length = len(self.body_type)\n        self.body_type_type = self.body_type_type\n        self.body_length_type = self.", "solution": "\n        self.set_header(\"Accept-Ranges\", \"bytes\")\n\n        content_type = self.get_content_type()\n        if content_type:\n            self.set_header(\"Content-Type\", content_type)"}
{"prompt": "def iterfd(fd, size=10000000):\n    \"\"\"\n    Generator which yields bytes from a file descriptor.\n\n    Args:\n        fd (file): A file-like object to read bytes from.\n        size (int): Size, in bytes, of the number of bytes to read from the\n        fd at a given time.\n\n    Notes:\n        If the first read call on the file descriptor is a empty bytestring,\n        that zero length bytestring will be yielded and the generator will\n        then be exhuasted. This behavior is intended to allow the yielding of\n        contents of a zero byte file.\n\n    Yields:\n        bytes: Bytes from the file descriptor.\n    \"\"\"", "output": ": The next `size` bytes from the file.\n\n    Notes:\n        If the first read call on the file descriptor is a bytes object,\n        that string will be returned instead of the generator.\n    \"\"\"\n    while True:\n        chunk = fd.read(size)\n        if not chunk:\n            break\n        yield chunk\n\n\ndef _get_file_contents(path):\n    \"\"\"\n    Read the contents of a file.\n\n    Args:\n        path (str): The path to the file.\n\n    Returns:\n        str: The contents of the file.\n    \"\"\"\n    with open(path, 'rb') as f:\n       ", "solution": "\n    fd.seek(0)\n    byts = fd.read(size)\n    # Fast path to yield b''\n    if len(byts) == 0:\n        yield byts\n        return\n    while byts:\n        yield byts\n        byts = fd.read(size)"}
{"prompt": "def register_integration(package_folder):\n    \"\"\"Register a honeycomb integration.\n\n    :param package_folder: Path to folder with integration to load\n    :returns: Validated integration object\n    :rtype: :func:`honeycomb.utils.defs.Integration`\n    \"\"\"", "output": "\n    if not package_folder.endswith(os.path.sep):\n        package_folder += os.path.sep\n    with open(package_folder, 'r') as f:\n        return f.read()\n\n\ndef get_honeycomb_config(package_folder):\n    \"\"\"Get honeycomb configuration.\n\n    :param package_folder: Path to folder with honeycomb configuration to load\n    :returns: honeycomb configuration\n    :rtype: :func:`honeycomb.utils.defs.HoneycombConfig`\n    \"\"\"\n    if not package_folder.endswith(os.path.", "solution": "\n    logger.debug(\"registering integration %s\", package_folder)\n    package_folder = os.path.realpath(package_folder)\n    if not os.path.exists(package_folder):\n        raise IntegrationNotFound(os.path.basename(package_folder))\n\n    json_config_path = os.path.join(package_folder, CONFIG_FILE_NAME)\n    if not os.path.exists(json_config_path):\n        raise ConfigFileNotFound(json_config_path)\n\n    with open(json_config_path, \"r\") as f:\n        config_json = json.load(f)\n\n    # Validate integration and alert config\n    validate_config(config_json, defs.INTEGRATION_VALIDATE_CONFIG_FIELDS)\n    validate_config_parameters(config_json,\n                               defs.INTEGRATION_PARAMETERS_ALLOWED_KEYS,\n                               defs.INTEGRATION_PARAMETERS_ALLOWED_TYPES)\n\n    integration_type = _create_integration_object(config_json)\n\n    return integration_type"}
{"prompt": "def initialize_private_canvas(self, private_canvas):\n        \"\"\"Initialize the private canvas used by this instance.\n        \"\"\"", "output": "\n        self.private_canvas = private_canvas\n        self.canvas = private_canvas.get_tk_widget()\n        self.canvas.pack(side='top', fill='both', expand=1)\n        self.canvas.create_window(0, 0, window=self.canvas.winfo_toplevel())\n        self.canvas.bind('<Configure>', self.on_configure)\n        self.canvas.bind('<Map>', self.on_map)\n        self.canvas.bind('<Button-4>', self.on_click_event)\n        self.canvas.bind('<Button-5>', self", "solution": "\n        if self.t_.get('show_pan_position', False):\n            self.show_pan_mark(True)\n\n        if self.t_.get('show_focus_indicator', False):\n            self.show_focus_indicator(True)"}
{"prompt": "def run_pileup(job, tumor_bam, univ_options, somaticsniper_options):\n    \"\"\"\n    Runs a samtools pileup on the tumor bam.\n\n    :param dict tumor_bam: Dict of bam and bai for tumor DNA-Seq\n    :param dict univ_options: Dict of universal options used by almost all tools\n    :param dict somaticsniper_options: Options specific to SomaticSniper\n    :return: fsID for the pileup file\n    :rtype: toil.fileStore.FileID\n    \"\"\"", "output": ".FileID\n    \"\"\"\n    # Get the path to the somatic sniper output file\n    fs_id = job.fileStore.getLocalTempDir() + \"/somaticsniper_output.txt\"\n\n    # Get the options specific to SomaticSniper\n    if \"options\" in tumor_bam:\n        options = tumor_bam[\"options\"]\n        options_dict = {}\n        for key in options:\n            options_dict[key] = options[key]\n        options = tumor_bam[\"options\"]\n        options_dict = options_dict.copy()\n        options", "solution": "\n    work_dir = os.getcwd()\n    input_files = {\n        'tumor.bam': tumor_bam['tumor_dna_fix_pg_sorted.bam'],\n        'tumor.bam.bai': tumor_bam['tumor_dna_fix_pg_sorted.bam.bai'],\n        'genome.fa.tar.gz': somaticsniper_options['genome_fasta'],\n        'genome.fa.fai.tar.gz': somaticsniper_options['genome_fai']}\n    input_files = get_files_from_filestore(job, input_files, work_dir, docker=False)\n\n    for key in ('genome.fa', 'genome.fa.fai'):\n        input_files[key] = untargz(input_files[key + '.tar.gz'], work_dir)\n    input_files = {key: docker_path(path) for key, path in input_files.items()}\n\n    parameters = ['pileup',\n                  '-cvi',\n                  '-f', docker_path(input_files['genome.fa']),\n                  docker_path(input_files['tumor.bam'])]\n\n    with open(os.path.join(work_dir, 'pileup.txt'), 'w') as pileup_file:\n        docker_call(tool='samtools', tool_parameters=parameters, work_dir=work_dir,\n                    dockerhub=univ_options['dockerhub'], outfile=pileup_file,\n                    tool_version=somaticsniper_options['samtools']['version'])\n    outfile = job.fileStore.writeGlobalFile(pileup_file.name)\n    job.fileStore.logToMaster('Ran samtools pileup on %s successfully' % univ_options['patient'])\n    return outfile"}
{"prompt": "def get_alpha_or_number(number, template):\n    \"\"\"Returns an Alphanumber that represents the number passed in, expressed\n    as defined in the template. Otherwise, returns the number\n    \"\"\"", "output": "\n    if number is None:\n        return None\n    if isinstance(number, int):\n        return template.get_alpha_or_number(number)\n    if isinstance(number, Alphanumber):\n        return number\n    if isinstance(template, str):\n        return template\n    if isinstance(template, dict):\n        return Alphanumber(number, template)\n    if isinstance(template, list):\n        return [Alphanumber(number, x) for x in template]\n    if isinstance(template, Alphanumber):\n        return template\n    raise ValueError(\"Invalid type of template: %s\" % type(template))\n\n", "solution": "\n    match = re.match(r\".*\\{alpha:(\\d+a\\d+d)\\}$\", template.strip())\n    if match and match.groups():\n        format = match.groups()[0]\n        return to_alpha(number, format)\n    return number"}
{"prompt": "def filenameToModname(self, filename):\n        \"\"\"Convert a filename to a module name.\"\"\"", "output": "\n        return filename.replace(os.path.sep, '.')\n\n    def moduleNameToFilename(self, moduleName):\n        \"\"\"Convert a module name to a filename.\"\"\"\n        return moduleName.replace('.', os.path.sep) + '.py'\n\n    def modulePathToModuleName(self, modulePath):\n        \"\"\"Convert a module path to a module name.\"\"\"\n        return modulePath.replace(os.path.sep, '.')\n\n    def modulePathToModulePath(self, modulePath):\n        \"\"\"Convert a module path to a filename.\"\"\"\n        return modulePath.replace(os.path.sep, '.')\n\n   ", "solution": "\n        for ext in reversed(self._exts):\n            if filename.endswith(ext):\n                filename = filename[:-len(ext)]\n                break\n        else:\n            self.warn(filename, '%s: unknown file name extension', filename)\n        filename = os.path.abspath(filename)\n        elements = filename.split(os.path.sep)\n        modname = []\n        while elements:\n            modname.append(elements[-1])\n            del elements[-1]\n            if not os.path.exists(os.path.sep.join(elements + ['__init__.py'])):\n                break\n        modname.reverse()\n        modname = \".\".join(modname)\n        return modname"}
{"prompt": "def extract_archive(filepath):\n    \"\"\"\n    Returns the path of the archive\n\n    :param str filepath: Path to file to extract or read\n\n    :return: path of the archive\n\n    :rtype: str\n    \"\"\"", "output": "\n    with open(filepath, 'rb') as f:\n        return f.read()\n\n\ndef extract_file_from_stream(stream, filepath):\n    \"\"\"\n    Extracts a file from a stream\n\n    :param stream: Stream to extract from\n    :param str filepath: Path to file to extract from\n\n    :return: path of the file\n\n    :rtype: str\n    \"\"\"\n    with open(filepath, 'rb') as f:\n        return stream.extractfile(f, encoding='binary', errors='replace')\n\n\ndef extract_path(filepath):\n    \"\"\"\n    Returns the path of the file\n\n    :", "solution": "\n\n    # Checks if file path is a directory\n    if os.path.isdir(filepath):\n        path = os.path.abspath(filepath)\n        print(\"Archive already extracted. Viewing from {}...\".format(path))\n        return path\n\n    # Checks if the filepath is a zipfile and continues to extract if it is\n    # if not it raises an error\n    elif not zipfile.is_zipfile(filepath):\n        # Misuse of TypeError? :P\n        raise TypeError(\"{} is not a zipfile\".format(filepath))\n\n    archive_sha = SHA1_file(\n        filepath=filepath,\n        # Add version of slackviewer to hash as well so we can invalidate the cached copy\n        #  if there are new features added\n        extra=to_bytes(slackviewer.__version__)\n    )\n\n    extracted_path = os.path.join(SLACKVIEWER_TEMP_PATH, archive_sha)\n\n    if os.path.exists(extracted_path):\n        print(\"{} already exists\".format(extracted_path))\n    else:\n        # Extract zip\n        with zipfile.ZipFile(filepath) as zip:\n            print(\"{} extracting to {}...\".format(filepath, extracted_path))\n            zip.extractall(path=extracted_path)\n\n        print(\"{} extracted to {}\".format(filepath, extracted_path))\n\n        # Add additional file with archive info\n        create_archive_info(filepath, extracted_path, archive_sha)\n\n    return extracted_path"}
{"prompt": "def addModel(D, models):\n    \"\"\"\n    Insert model data into a LiPD dataset\n\n    Examples of model naming:\n    chron0model0\n    chron0model1\n    chron1model0\n\n    Example of 'models' variable:\n    {\n       chron0model0: {\n                        \"method\": {...},\n                        \"summaryTable\": [...],\n                        \"ensembleTable\": [...],\n                        \"distributionTable: [...]\n                    },\n       chron0model1:...\n    }\n\n    :param dict D: Metadata (dataset)\n    :param dict models: Model data to add\n    :return dict D: Metadata (dataset)\n    \"\"\"", "output": ": Metadata (dataset)\n    \"\"\"\n    # Add the model data to the dataset\n    D = addMetadata(D, models)\n\n    # Add the dataset to the dataset\n    return D\n", "solution": "\n    try:\n        # Loop for each model that needs to  be added\n        for _model_name, _model_data in models.items():\n            # split the table name into a path that we can use\n            _m = re.match(re_model_name, _model_name)\n            if _m:\n                D = _put_model(D, _model_name, _model_data, _m)\n            else:\n                print(\"The table name found in the given model data isn't valid for automatic placement\")\n                _placement_name = _prompt_placement(D, \"model\")\n                _m = re.match(re_model_name, _placement_name)\n                if _m:\n                    D = _put_model(D, _placement_name, _model_data, _m)\n                else:\n                    print(\"Oops. This shouldn't happen. That table name doesn't look right. Please report this error\")\n                    return\n\n    except Exception as e:\n        print(\"addModel: Model data NOT added, {}\".format(e))\n\n    return D"}
{"prompt": "def publish(self, topic, message, subject=None):\n        \"\"\"\n        Get properties of a Topic\n\n        :type topic: string\n        :param topic: The ARN of the new topic.\n\n        :type message: string\n        :param message: The message you want to send to the topic.\n                        Messages must be UTF-8 encoded strings and\n                        be at most 4KB in size.\n\n        :type subject: string\n        :param subject: Optional parameter to be used as the \"Subject\"\n                        line of the email notifications.\n\n        \"\"\"", "output": "\n        return self._client.publish(topic, message)\n\n    def send(self, to, subject=None, content=None, html=None):\n        \"\"\"\n        Send an email to a recipient.\n\n        :type to: string\n        :param to: The recipient's email address.\n\n        :type subject: string\n        :param subject: Optional parameter to be used as the \"Subject\"\n                        line of the email notifications.\n\n        :type content: string\n        :param content: Optional parameter to be used as the \"Content-Type\"\n                        header of the email notifications.\n\n        :rtype: bool\n        :return: True if", "solution": "\n        params = {'ContentType' : 'JSON',\n                  'TopicArn' : topic,\n                  'Message' : message}\n        if subject:\n            params['Subject'] = subject\n        response = self.make_request('Publish', params, '/', 'GET')\n        body = response.read()\n        if response.status == 200:\n            return json.loads(body)\n        else:\n            boto.log.error('%s %s' % (response.status, response.reason))\n            boto.log.error('%s' % body)\n            raise self.ResponseError(response.status, response.reason, body)"}
{"prompt": "def schema_get(name, profile=None):\n    \"\"\"\n    Known valid names of schemas are:\n      - image\n      - images\n      - member\n      - members\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' glance.schema_get name=f16-jeos\n    \"\"\"", "output": "\n    if profile is not None:\n        return __salt__['cmd.run']('image', name, '--profile', profile)\n    else:\n        return __salt__['cmd.run']('image', name)\n\n\ndef schema_list():\n    \"\"\"\n    List all available schemas.\n\n    CLI Example:\n\n   .. code-block:: bash\n\n        salt '*' glance.schema_list\n    \"\"\"\n    return __salt__['cmd.run']('image','schema')\n\n\ndef schema_info(name):\n    \"\"\"\n    Known valid schemas are:\n      - image\n      - images\n      - member\n      - members\n\n    CLI Example:\n\n   ", "solution": "\n    g_client = _auth(profile)\n    schema_props = {}\n    for prop in g_client.schemas.get(name).properties:\n        schema_props[prop.name] = prop.description\n    log.debug(\n        'Properties of schema %s:\\n%s',\n        name, pprint.PrettyPrinter(indent=4).pformat(schema_props)\n    )\n    return {name: schema_props}"}
{"prompt": "def _compute_curve(labels, predictions, num_thresholds, weights=None):\n    \"\"\"This function is another implementation of functions in\n    https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/pr_curve/summary.py\"\"\"", "output": "\n    if weights is None:\n        weights = tf.ones_like(labels)\n    else:\n        weights = tf.constant(weights, dtype=tf.float32)\n        for i in range(num_thresholds):\n            weights[i] = tf.reduce_sum(\n                tf.multiply(predictions[i], weights[i]),\n                axis=1,\n                keep_dims=True)\n    return tf.reduce_sum(weights, axis=1, keep_dims=True)\n\n\ndef _compute_auc(labels, predictions, num_thresholds, weights=None):\n    \"\"\"This function is", "solution": "\n\n    if weights is None:\n        weights = 1.0\n\n    # Compute bins of true positives and false positives.\n    bucket_indices = np.int32(np.floor(predictions * (num_thresholds - 1)))\n    float_labels = labels.astype(np.float)\n    histogram_range = (0, num_thresholds - 1)\n    tp_buckets, _ = np.histogram(\n        bucket_indices,\n        bins=num_thresholds,\n        range=histogram_range,\n        weights=float_labels * weights)\n    fp_buckets, _ = np.histogram(\n        bucket_indices,\n        bins=num_thresholds,\n        range=histogram_range,\n        weights=(1.0 - float_labels) * weights)\n\n    # Obtain the reverse cumulative sum.\n    tp = np.cumsum(tp_buckets[::-1])[::-1]\n    fp = np.cumsum(fp_buckets[::-1])[::-1]\n    tn = fp[0] - fp\n    fn = tp[0] - tp\n    precision = tp / np.maximum(_MINIMUM_COUNT, tp + fp)\n    recall = tp / np.maximum(_MINIMUM_COUNT, tp + fn)\n    return np.stack((tp, fp, tn, fn, precision, recall))"}
{"prompt": "def get_block_header(self, block_hash, id=None, endpoint=None):\n        \"\"\"\n        Get the corresponding block header information according to the specified script hash.\n        Args:\n            block_hash: (str) the block scripthash (e.g. 'a5508c9b6ed0fc09a531a62bc0b3efcb6b8a9250abaf72ab8e9591294c1f6957')\n            id: (int, optional) id to use for response tracking\n            endpoint: (RPCEndpoint, optional) endpoint to specify to use\n\n        Returns:\n            json object of the result or the error encountered in the RPC call\n        \"\"\"", "output": " the script hash to use for response tracking\n        Returns:\n            (dict) the block header information\n        \"\"\"\n        return self._get_block_header(block_hash, id, endpoint)\n\n    def _get_block_header(self, block_hash, id=None, endpoint=None):\n        \"\"\"\n        Get the corresponding block header information according to the specified script hash.\n        Args:\n            block_hash: (str) the block scripthash (e.g. 'a5508c9b6ed0fc09a531a62bc0b3efcb6b8a9", "solution": "\n        return self._call_endpoint(GET_BLOCK_HEADER, params=[block_hash, 1], id=id, endpoint=endpoint)"}
{"prompt": "def BuscarCertConSaldoDisponible(self, cuit_depositante=None,\n                        cod_grano=2, campania=1314, coe=None, \n                        fecha_emision_des=None,\n                        fecha_emision_has=None,\n                 ):\n        \"\"\"Devuelve los certificados de dep\u00f3sito en los que un productor tiene\n        saldo disponible para Liquidar/Retirar/Transferir\"\"\"", "output": "\n        if cuit_depositante is None:\n            cuit_depositante = self.cuit\n        if coe is None:\n            coe = self.coe\n        if fecha_emision_des is None:\n            fecha_emision_des = self.fecha_emision_des\n        if fecha_emision_has is None:\n            fecha_has = self.fecha_has\n        if cod_depositante is None:\n            cod_depositante = self.cod_depositante\n        if coe is None", "solution": "\n        \n        ret = self.client.cgBuscarCertConSaldoDisponible(\n                    auth={\n                        'token': self.Token, 'sign': self.Sign,\n                        'cuit': self.Cuit, },\n                    cuitDepositante=cuit_depositante or self.Cuit,\n                    codGrano=cod_grano, campania=campania,\n                    coe=coe,\n                    fechaEmisionDes=fecha_emision_des,\n                    fechaEmisionHas=fecha_emision_has,\n                        )['oReturn']\n        self.__analizar_errores(ret)\n        array = ret.get('certificado', [])\n        self.Excepcion = self.Traceback = \"\"\n        self.params_out['certificados'] = []\n        for cert in array:\n            self.params_out['certificados'].append(dict(\n                coe=cert['coe'],\n                tipo_certificado=cert['tipoCertificado'],\n                campania=cert['campania'],\n                cuit_depositante=cert['cuitDepositante'],\n                cuit_depositario=cert['cuitDepositario'],\n                nro_planta=cert['nroPlanta'],\n                kilos_disponibles=cert['kilosDisponibles'],\n                cod_grano=cert['codGrano'],\n            ))\n        return True"}
{"prompt": "def getIntervals(fn, tierName, filterFunc=None,\n                 includeUnlabeledRegions=False):\n    \"\"\"\n    Get information about the 'extract' tier, used by several merge scripts\n    \"\"\"", "output": "\n    if filterFunc is None:\n        filterFunc = lambda x: True\n    if includeUnlabeledRegions:\n        filterFunc = lambda x: False\n    return _getIntervals(fn, tierName, filterFunc, False, False)\n\n\ndef _getIntervals(fn, tierName, func, includeUnlabeledRegions=False):\n    \"\"\"\n    Get information about the 'extract' tier, used by several merge scripts\n    \"\"\"\n    if includeUnlabeledRegions:\n        filterFunc = lambda x: False\n    if tierName is None:\n        return [True]\n    if tierName in self.get", "solution": "\n\n    tg = tgio.openTextgrid(fn)\n    \n    tier = tg.tierDict[tierName]\n    if includeUnlabeledRegions is True:\n        tier = tgio._fillInBlanks(tier)\n\n    entryList = tier.entryList\n    if filterFunc is not None:\n        entryList = [entry for entry in entryList if filterFunc(entry)]\n\n    return entryList"}
{"prompt": "def get_plugin_actions(self):\r\n        \"\"\"Return a list of actions related to plugin.\"\"\"", "output": "\r\n        return []\r\n\r\n    def get_plugin_options(self, plugin, plugin_type):\r\n        \"\"\"Return a list of options related to a plugin.\"\"\"\r\n        return []\r\n\r\n    def get_plugin_options_list(self, plugin):\r\n        \"\"\"Return a list of options available in a plugin.\"\"\"\r\n        return []\r\n\r\n    def get_plugin_options_dict(self, plugin):\r\n        \"\"\"Return a dict of options available in a plugin.\"\"\"\r\n        return {}\r\n\r\n    def get_plugin_options_help(self, plugin):\r\n        \"\"\"Return a help text for the given plugin.\"\"\"\r\n        return help_text\r\n\r\n    def get_plugin_", "solution": "\r\n        create_client_action = create_action(\r\n                                   self,\r\n                                   _(\"New console (default settings)\"),\r\n                                   icon=ima.icon('ipython_console'),\r\n                                   triggered=self.create_new_client,\r\n                                   context=Qt.WidgetWithChildrenShortcut)\r\n        self.register_shortcut(create_client_action, context=\"ipython_console\",\r\n                               name=\"New tab\")\r\n\r\n        create_pylab_action = create_action(\r\n                                   self,\r\n                                   _(\"New Pylab console (data plotting)\"),\r\n                                   icon=ima.icon('ipython_console'),\r\n                                   triggered=self.create_pylab_client,\r\n                                   context=Qt.WidgetWithChildrenShortcut)\r\n\r\n        create_sympy_action = create_action(\r\n                                   self,\r\n                                   _(\"New SymPy console (symbolic math)\"),\r\n                                   icon=ima.icon('ipython_console'),\r\n                                   triggered=self.create_sympy_client,\r\n                                   context=Qt.WidgetWithChildrenShortcut)\r\n\r\n        create_cython_action = create_action(\r\n                                   self,\r\n                                   _(\"New Cython console (Python with \"\r\n                                     \"C extensions)\"),\r\n                                   icon=ima.icon('ipython_console'),\r\n                                   triggered=self.create_cython_client,\r\n                                   context=Qt.WidgetWithChildrenShortcut)\r\n        special_console_action_group = QActionGroup(self)\r\n        special_console_actions = (create_pylab_action, create_sympy_action,\r\n                                   create_cython_action)\r\n        add_actions(special_console_action_group, special_console_actions)\r\n        special_console_menu = QMenu(_(\"New special console\"), self)\r\n        add_actions(special_console_menu, special_console_actions)\r\n\r\n        restart_action = create_action(self, _(\"Restart kernel\"),\r\n                                       icon=ima.icon('restart'),\r\n                                       triggered=self.restart_kernel,\r\n                                       context=Qt.WidgetWithChildrenShortcut)\r\n\r\n        reset_action = create_action(self, _(\"Remove all variables\"),\r\n                                     icon=ima.icon('editdelete'),\r\n                                     triggered=self.reset_kernel,\r\n                                     context=Qt.WidgetWithChildrenShortcut)\r\n\r\n        if self.interrupt_action is None:\r\n            self.interrupt_action = create_action(\r\n                self, _(\"Interrupt kernel\"),\r\n                icon=ima.icon('stop'),\r\n                triggered=self.interrupt_kernel,\r\n                context=Qt.WidgetWithChildrenShortcut)\r\n\r\n        self.register_shortcut(restart_action, context=\"ipython_console\",\r\n                               name=\"Restart kernel\")\r\n\r\n        connect_to_kernel_action = create_action(self,\r\n               _(\"Connect to an existing kernel\"), None, None,\r\n               _(\"Open a new IPython console connected to an existing kernel\"),\r\n               triggered=self.create_client_for_kernel)\r\n        \r\n        rename_tab_action = create_action(self, _(\"Rename tab\"),\r\n                                       icon=ima.icon('rename'),\r\n                                       triggered=self.tab_name_editor)\r\n        \r\n        # Add the action to the 'Consoles' menu on the main window\r\n        main_consoles_menu = self.main.consoles_menu_actions\r\n        main_consoles_menu.insert(0, create_client_action)\r\n        main_consoles_menu += [special_console_menu, connect_to_kernel_action,\r\n                               MENU_SEPARATOR,\r\n                               self.interrupt_action, restart_action,\r\n                               reset_action]\r\n\r\n        # Plugin actions\r\n        self.menu_actions = [create_client_action, special_console_menu,\r\n                             connect_to_kernel_action,\r\n                             MENU_SEPARATOR,\r\n                             self.interrupt_action,\r\n                             restart_action, reset_action, rename_tab_action]\r\n\r\n        self.update_execution_state_kernel()\r\n\r\n        # Check for a current client. Since it manages more actions.\r\n        client = self.get_current_client()\r\n        if client:\r\n            return client.get_options_menu()\r\n        return self.menu_actions"}
{"prompt": "def from_birth_date(birth_date):\r\n        \"\"\"Take a person's birth date (datetime.date) and return a new DOB \r\n        object suitable for him.\"\"\"", "output": "\r\n        if not isinstance(birth_date, datetime.date):\r\n            raise TypeError(\"Expected a date object, got %s\" % type(birth_date))\r\n        if birth_date.year < 1900:\r\n            raise ValueError(\"Expected a date object, got %s\" % birth_date)\r\n        if birth_date.month < 1:\r\n            raise ValueError(\"Expected a date object, got %s\" % birth_date)\r\n        if birth_date.day < 1:\r\n            raise ValueError(\"Expected a date object, got %s\" % birth_date)\r\n        if birth_date.day > 1:", "solution": "\r\n        if birth_date > datetime.date.today():\r\n            raise ValueError('birth_date can\\'t be in the future')\r\n        date_range = DateRange(birth_date, birth_date)\r\n        return DOB(date_range=date_range)"}
{"prompt": "def ISBNValidator(raw_isbn):\n    \"\"\" Check string is a valid ISBN number\"\"\"", "output": "\n    try:\n        int(raw_isbn)\n    except ValueError:\n        return False\n    else:\n        return True\n\n\ndef ISBNValidator_ISBN_valid(isbn):\n    \"\"\" Check ISBN is a valid ISBN number\"\"\"\n    try:\n        int(isbn)\n    except ValueError:\n        return False\n    else:\n        return True\n\n\ndef ISBNValidator_ISBN_invalid(isbn):\n    \"\"\" Check ISBN is a valid ISBN number\"\"\"\n    try:\n        int(isbn)\n    except ValueError:\n        return False\n    else:\n        return True\n\n\ndef ISBNValidator", "solution": "\n    isbn_to_check = raw_isbn.replace('-', '').replace(' ', '')\n\n    if not isinstance(isbn_to_check, string_types):\n        raise ValidationError(_(u'Invalid ISBN: Not a string'))\n\n    if len(isbn_to_check) != 10 and len(isbn_to_check) != 13:\n        raise ValidationError(_(u'Invalid ISBN: Wrong length'))\n    \n    if not isbn.is_valid(isbn_to_check):\n        raise ValidationError(_(u'Invalid ISBN: Failed checksum'))\n\n    if isbn_to_check != isbn_to_check.upper():\n        raise ValidationError(_(u'Invalid ISBN: Only upper case allowed'))\n\n    return True"}
{"prompt": "def get_unique_name(name='', collection=()):\n    \"\"\"\n    Generate a unique name (str type) by appending a sequence number to\n    the original name so that it is not contained in the collection.\n    ``collection`` has a __contains__ method (tuple, list, dict, etc.)\n    \"\"\"", "output": "\n    if not name:\n        return ''\n    if isinstance(name, str):\n        name = (name,)\n    seen = set()\n    for i in xrange(len(collection)):\n        if name[i] not in seen:\n            seen.add(name[i])\n            name[i] = '%s_%s' % (name[i], i)\n    return '/'.join(collection)\n\n\ndef get_unique_name_sequence(name_sequence, collection=()):\n    \"\"\"\n    Generate a unique name (str type) by appending a sequence number to\n    the original name so that it is not contained in the", "solution": "\n    if name not in collection:\n        return name\n    i = 1\n    while True:\n        i += 1\n        name2 = '%s_%s' % (name, i)\n        if name2 not in collection:\n            return name2"}
{"prompt": "def viewkeys(obj, **kwargs):\n    \"\"\"\n    Function for iterating over dictionary keys with the same set-like\n    behaviour on Py2.7 as on Py3.\n\n    Passes kwargs to method.\"\"\"", "output": "\n    for k in obj:\n        if not k.startswith('__') and k.endswith('__'):\n            k = k[1:-1]\n            yield k, k[1:-1], k[1:-1]\n\n\ndef viewvalues(obj, **kwargs):\n    \"\"\"\n    Function for iterating over dictionary values with the same set-like\n    behaviour on Py2.7.\n\n    Passes kwargs to method.\"\"\"\n    for v in viewkeys(obj, **kwargs):\n        if not v.startswith('__') and v.endswith('__'):\n            v = v[1:-1]\n            yield v, v[1", "solution": "\n    func = getattr(obj, \"viewkeys\", None)\n    if not func:\n        func = obj.keys\n    return func(**kwargs)"}
{"prompt": "def fit(self, X, y=None):\n        \"\"\"\n        X : ANTsImage | string | list of ANTsImage types | list of strings\n            images to register to fixed image\n\n        y : string | list of strings\n            labels for images\n        \"\"\"", "output": "\n        if y is None:\n            if isinstance(X, list):\n                self.fixed_labels = [str(i) for i in range(len(X[0]))]\n            else:\n                self.fixed_labels = [str(i) for i in range(len(X))]\n        else:\n            self.fixed_labels = [str(i) for i in range(len(X))]\n        self.fixed_labels = [self.fixed_labels[i] for i in self.fixed_labels]\n        self.labels = self.fixed_labels\n        self.fixed_labels =", "solution": "\n        moving_images = X if isinstance(X, (list,tuple)) else [X]\n        moving_labels = y if y is not None else [i for i in range(len(moving_images))]\n        fixed_image = self.fixed_image\n\n        self.fwdtransforms_ = {}\n        self.invtransforms_ = {}\n        self.warpedmovout_ = {}\n        self.warpedfixout_ = {}\n        \n        for moving_image, moving_label in zip(moving_images, moving_labels):\n            fit_result = interface.registration(fixed_image,\n                                                moving_image,\n                                                type_of_transform=self.type_of_transform,\n                                                initial_transform=None,\n                                                outprefix='',\n                                                mask=None,\n                                                grad_step=0.2,\n                                                flow_sigma=3,\n                                                total_sigma=0,\n                                                aff_metric='mattes',\n                                                aff_sampling=32,\n                                                syn_metric='mattes',\n                                                syn_sampling=32,\n                                                reg_iterations=(40,20,0),\n                                                verbose=False)\n\n            self.fwdtransforms_[moving_label] = fit_result['fwdtransforms']\n            self.invtransforms_[moving_label] = fit_result['invtransforms']\n            self.warpedmovout_[moving_label]  = fit_result['warpedmovout']\n            self.warpedfixout_[moving_label]  = fit_result['warpedfixout']\n\n        return self"}
{"prompt": "def get_one(self, section, key):\n        \"\"\"Retrieve the first value for a section/key.\n\n        Raises:\n            KeyError: If no line match the given section/key.\n        \"\"\"", "output": "\n        for line in self.lines:\n            if line.key == key:\n                return line.value\n        raise KeyError(key)\n\n    def get_all(self, section):\n        \"\"\"Retrieve all values for a section/key.\n\n        Args:\n            section: The section to retrieve values for.\n\n        Returns:\n            A list of values matching the given section/key.\n        \"\"\"\n        return [self.get_one(section, i) for i in range(len(self.lines) - 1)]\n\n    def get_all_until(self, section, key):\n        \"\"\"Retrieve all values until a section/", "solution": "\n        lines = iter(self.get(section, key))\n        try:\n            return next(lines)\n        except StopIteration:\n            raise KeyError(\"Key %s not found in %s\" % (key, section))"}
{"prompt": "def compute_attention_component(antecedent,\n                                total_depth,\n                                filter_width=1,\n                                padding=\"VALID\",\n                                name=\"c\",\n                                vars_3d_num_heads=0,\n                                layer_collection=None):\n  \"\"\"Computes attention compoenent (query, key or value).\n\n  Args:\n    antecedent: a Tensor with shape [batch, length, channels]\n    total_depth: an integer\n    filter_width: An integer specifying how wide you want the attention\n      component to be.\n    padding: One of \"VALID\", \"SAME\" or \"LEFT\". Default is VALID: No padding.\n    name: a string specifying scope name.\n    vars_3d_num_heads: an optional integer (if we want to use 3d variables)\n    layer_collection: A tensorflow_kfac.LayerCollection. Only used by the\n      KFAC optimizer. Default is None.\n\n  Returns:\n    c : [batch, length, depth] tensor\n  \"\"\"", "output": " is \"LEFT\".\n    name: string, name scope for ops managed by this function.\n    vars_3d_num_heads: an integer specifying how many heads you want to use for\n      3D attention. Default is 0.\n    layer_collection: string, name of the layer collection to use.\n\n  Returns:\n    antecedent: a Tensor with shape [batch, length, channels]\n    - query: a Tensor with shape [batch, length, filters]\n    - key: a Tensor with shape [batch, length, key]\n    - value: a Tensor with shape [batch, length, value]", "solution": "\n  if layer_collection is not None:\n    if filter_width != 1 or vars_3d_num_heads != 0:\n      raise ValueError(\n          \"KFAC implementation only supports filter_width=1 (actual: {}) and \"\n          \"vars_3d_num_heads=0 (actual: {}).\".format(\n              filter_width, vars_3d_num_heads))\n  if vars_3d_num_heads > 0:\n    assert filter_width == 1\n    input_depth = antecedent.get_shape().as_list()[-1]\n    depth_per_head = total_depth // vars_3d_num_heads\n    initializer_stddev = input_depth ** -0.5\n    if \"q\" in name:\n      initializer_stddev *= depth_per_head ** -0.5\n    var = tf.get_variable(\n        name, [input_depth,\n               vars_3d_num_heads,\n               total_depth // vars_3d_num_heads],\n        initializer=tf.random_normal_initializer(stddev=initializer_stddev))\n    var = tf.cast(var, antecedent.dtype)\n    var = tf.reshape(var, [input_depth, total_depth])\n    return tf.tensordot(antecedent, var, axes=1)\n  if filter_width == 1:\n    return common_layers.dense(\n        antecedent, total_depth, use_bias=False, name=name,\n        layer_collection=layer_collection)\n  else:\n    return common_layers.conv1d(\n        antecedent, total_depth, filter_width, padding=padding, name=name)"}
{"prompt": "def _evaluate(self,R,phi=0.,t=0.):\n        \"\"\"\n        NAME:\n           _evaluate\n        PURPOSE:\n           evaluate the potential at R,phi,t\n        INPUT:\n           R - Galactocentric cylindrical radius\n           phi - azimuth\n           t - time\n        OUTPUT:\n           Phi(R,phi,t)\n        HISTORY:\n           2011-03-27 - Started - Bovy (NYU)\n        \"\"\"", "output": "\n        return self._evaluate_scalar(R,phi,t)\n\n    def _evaluate_scalar(self,R,phi,t):\n        \"\"\"\n        NAME:\n           _evaluate_scalar\n        PURPOSE:\n           evaluate the potential at R,phi,t\n        INPUT:\n           R - Galactocentric cylindrical radius\n           phi - azimuth\n           t - time\n        OUTPUT:\n           Phi(R,phi,t)\n        HISTORY:\n           2011-03-27 - Bovy - Added optional argument 't' to specify the time at which the potential is evaluated\n        \"\"\"\n        if isinstance(t", "solution": "\n        if not self._tform is None:\n            if t < self._tform:\n                smooth= 0.\n            elif t < self._tsteady:\n                deltat= t-self._tform\n                xi= 2.*deltat/(self._tsteady-self._tform)-1.\n                smooth= (3./16.*xi**5.-5./8*xi**3.+15./16.*xi+.5)\n            else: #spiral is fully on\n                smooth= 1.\n        else:\n            smooth= 1.\n        return smooth*self._A/self._alpha*math.cos(self._alpha*math.log(R)\n                                                   -self._m*(phi-self._omegas*t\n                                                             -self._gamma))"}
{"prompt": "def system_resolve_data_objects(input_params={}, always_retry=True, **kwargs):\n    \"\"\"\n    Invokes the /system/resolveDataObjects API method.\n\n    For more info, see: https://wiki.dnanexus.com/API-Specification-v1.0.0/System-Methods#API-method:-/system/resolveDataObjects\n    \"\"\"", "output": "\n    if not input_params:\n        input_params = {}\n    if not always_retry:\n        kwargs['always_retry'] = True\n    return self._system_resolve_with_http_info('get', '/system/resolveDataObjects', input_params, **kwargs)\n\n  def system_get_log(self, **kwargs):\n    \"\"\"\n    Retrieves the logs of the current user.\n\n    Args:\n      kwargs: Other parameters to pass to the /system/log API method.\n    Returns:\n      A dict respresentation of the logs.\n    \"\"\"\n    return self._system_with_http_info('", "solution": "\n    return DXHTTPRequest('/system/resolveDataObjects', input_params, always_retry=always_retry, **kwargs)"}
{"prompt": "def google_storage(self, scene, path):\n        \"\"\"\n        Google Storage Downloader.\n\n        :param scene:\n            The scene id\n        :type scene:\n            String\n        :param path:\n            The directory path to where the image should be stored\n        :type path:\n            String\n\n        :returns:\n            Boolean\n        \"\"\"", "output": "\n        if not self.is_alive():\n            return False\n\n        if not path.endswith('/'):\n            path += '/'\n\n        if not path.startswith(self.base_path):\n            path = self.base_path + path\n\n        if not path.startswith(self.base_path):\n            path = self.base_path + path\n\n        if not path.endswith(self.image_ext):\n            path += self.image_ext\n\n        if not path.startswith(self.image_path):\n            path = self.image_path + path\n\n        if not path.startswith(self.image_path):\n            path", "solution": "\n\n        sat = self.scene_interpreter(scene)\n        url = self.google_storage_url(sat)\n\n        self.remote_file_exists(url)\n\n        self.output('Source: Google Storage', normal=True, arrow=True)\n        return self.fetch(url, path)"}
{"prompt": "def receive(socket, num_bytes=20):\n    \"\"\"Receive data frame from open socket.\n\n    :param socket: open socket instance\n    :param num_bytes: number of bytes to read\n\n    :return: received data\n    \"\"\"", "output": "\n    data = ''\n    while len(data) < num_bytes:\n        data += sock.recv(num_bytes - len(data))\n        if not data:\n            break\n\n    return data\n\n\ndef send_data(socket, data):\n    \"\"\"Send data to open socket.\n\n    :param socket: open socket instance\n    :param data: data to send\n\n    :return: None\n    \"\"\"\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    sock.send(data)\n    sock.close()\n\n\ndef send_data_with_retries(socket, data):", "solution": "\n    length = int(_receive_all(socket, num_bytes).decode())\n    serialized_data = _receive_all(socket, length)\n    return pickle.loads(serialized_data)"}
{"prompt": "def get(self, name, failobj=None):\n        \"\"\"Get a header value.\n\n        Like __getitem__() but return failobj instead of None when the field\n        is missing.\n        \"\"\"", "output": "\n        return self._headers.get(name)\n\n    def get_all(self):\n        \"\"\"Get all header values.\n\n        :return: A list of all header values.\n        \"\"\"\n        return self._headers.values()\n\n    def get_all_if_match(self, name, failobj=None):\n        \"\"\"Get all header values if a header value is matched.\n\n        :return: A list of all header values if a header value is matched,\n            else None.\n        \"\"\"\n        return get(self, name, failobj).get_all_if_match(name)\n\n    def get_", "solution": "\n        name = name.lower()\n        for k, v in self._headers:\n            if k.lower() == name:\n                return self.policy.header_fetch_parse(k, v)\n        return failobj"}
{"prompt": "def XYZ_to_lbd(X,Y,Z,degree=False):\n    \"\"\"\n    NAME:\n\n       XYZ_to_lbd\n\n    PURPOSE:\n\n       transform from rectangular Galactic coordinates to spherical Galactic coordinates (works with vector inputs)\n\n    INPUT:\n\n       X - component towards the Galactic Center (in kpc; though this obviously does not matter))\n\n       Y - component in the direction of Galactic rotation (in kpc)\n\n       Z - component towards the North Galactic Pole (kpc)\n\n       degree - (Bool) if True, return l and b in degrees\n\n    OUTPUT:\n\n       [l,b,d] in (rad or degree,rad or degree,kpc)\n\n       For vector inputs [:,3]\n\n    HISTORY:\n\n       2009-10-24 - Written - Bovy (NYU)\n\n       2014-06-14 - Re-written w/ numpy functions for speed and w/ decorators for beauty - Bovy (IAS)\n\n    \"\"\"", "output": "bd coordinates (in degrees, otherwise)\n\n    OUTPUT:\n\n       lbd coordinates (in degrees, otherwise)\n    \"\"\"\n    if degree:\n        return np.degrees(X)\n    else:\n        return np.radians(X)\n\ndef lbd_to_xyz(lbd, degree=False):\n    \"\"\"\n    NAME:\n\n       lbd_to_xyz\n\n    PURPOSE:\n\n       transform from rectangular Galactic coordinates to spherical Galactic pole (works with vector inputs)\n\n    INPUT:\n\n       lbd - (Bool) if True, return the spherical Galactic pole (in kpc;", "solution": "\n    #Whether to use degrees and scalar input is handled by decorators\n    d= nu.sqrt(X**2.+Y**2.+Z**2.)\n    b=nu.arcsin(Z/d)\n    cosl= X/d/nu.cos(b)\n    sinl= Y/d/nu.cos(b)\n    l= nu.arcsin(sinl)\n    l[cosl < 0.]= nu.pi-l[cosl < 0.]\n    l[(cosl >= 0.)*(sinl < 0.)]+= 2.*nu.pi\n    out= nu.empty((len(l),3))\n    out[:,0]= l\n    out[:,1]= b\n    out[:,2]= d\n    return out"}
{"prompt": "def screenshot(self, png_filename=None, format='raw'):\n        \"\"\"\n        Screenshot with PNG format\n\n        Args:\n            png_filename(string): optional, save file name\n            format(string): return format, pillow or raw(default)\n        Returns:\n            raw data or PIL.Image\n        \n        Raises:\n            WDAError\n        \"\"\"", "output": "\n        if not png_filename:\n            png_filename = self.get_png_filename()\n        if not format:\n            format = 'png'\n        if format == 'raw':\n            return self.raw_screenshot()\n        elif format == 'pillow':\n            return self.pillow_screenshot()\n        else:\n            raise WDAError(\"Unknown screenshot format: %s\" % format)\n\n    def save_screenshot(self, filename=None):\n        \"\"\"\n        Save screenshot to file\n\n        Args:\n            filename(string): optional, save file name\n        Returns:\n            None\n        \n        Raises:\n            WDAError", "solution": "\n        value = self.http.get('screenshot').value\n        raw_value = base64.b64decode(value)\n        png_header = b\"\\x89PNG\\r\\n\\x1a\\n\"\n        if not raw_value.startswith(png_header) and png_filename:\n            raise WDAError(-1, \"screenshot png format error\")\n\n        if png_filename:\n            with open(png_filename, 'wb') as f:\n                f.write(raw_value)\n\n        if format == 'raw':\n            return raw_value\n        elif format == 'pillow':\n            from PIL import Image\n            buff = io.BytesIO(raw_value)\n            return Image.open(buff)\n        else:\n            raise ValueError(\"unknown format\")"}
{"prompt": "def lin_interp(x, rangeX, rangeY):\n    \"\"\"\n    Interpolate linearly variable x in rangeX onto rangeY.\n    \"\"\"", "output": "\n    return np.interp(x, rangeX, rangeY)\n\ndef lin_interp_2d(x, rangeX, rangeY):\n    \"\"\"\n    Interpolate linearly variable x in rangeX onto rangeY.\n    \"\"\"\n    return np.interp(x, rangeX, rangeY)\n\ndef lin_interp_3d(x, rangeX, rangeY):\n    \"\"\"\n    Interpolate linearly variable x in rangeX onto rangeY.\n    \"\"\"\n    return np.interp(x, rangeX, rangeY)\n\ndef lin_interp_4d(x, rangeX", "solution": "\n    s = (x - rangeX[0]) / mag(rangeX[1] - rangeX[0])\n    y = rangeY[0] * (1 - s) + rangeY[1] * s\n    return y"}
{"prompt": "def _onCompletionListItemSelected(self, index):\n        \"\"\"Item selected. Insert completion to editor\n        \"\"\"", "output": "\n        self.editor.insert(index, self.completion)\n        self.editor.setFocus()\n\n    def _onCompletionListDoubleClicked(self, index):\n        \"\"\"Double click on a list of completions\n        \"\"\"\n        self.editor.selectAll()\n\n    def _onCompletionListContextMenu(self, index):\n        \"\"\"Context menu. Insert completion to editor\n        \"\"\"\n        self.editor.insert(index, self.completion)\n        self.editor.setFocus()\n\n    def _onCompletionListContextMenuTriggered(self, index):\n        \"\"\"Context menu. Insert a context menu item\n        \"\"\"\n        self.", "solution": "\n        model = self._widget.model()\n        selectedWord = model.words[index]\n        textToInsert = selectedWord[len(model.typedText()):]\n        self._qpart.textCursor().insertText(textToInsert)\n        self._closeCompletion()"}
{"prompt": "def SetCoreGRRKnowledgeBaseValues(kb, client_obj):\n  \"\"\"Set core values from GRR into the knowledgebase.\"\"\"", "output": "\n  if not kb.IsValid(constants.IP4_ADDRESS):\n    raise errors.OpPrereqError(\"Invalid IP address '%s'\" % constants.IP4_ADDRESS,\n                               errors.ECODE_INVAL)\n  client_obj.Set(constants.IP4_ADDRESS,\n                compat.any_from_bytes(self.cfg.GetMasterNode(), \"master_candidate_config\",\n                                     constants.DEFAULT_NODE_GROUP_NAME))\n  client_obj.Set(constants.IP4_VERSION,\n                compat.any_from_str(self.cfg.GetVersion(), cl=", "solution": "\n  client_schema = client_obj.Schema\n  kb.fqdn = utils.SmartUnicode(client_obj.Get(client_schema.FQDN, \"\"))\n  if not kb.fqdn:\n    kb.fqdn = utils.SmartUnicode(client_obj.Get(client_schema.HOSTNAME, \"\"))\n  versions = client_obj.Get(client_schema.OS_VERSION)\n  if versions and versions.versions:\n    try:\n      kb.os_major_version = versions.versions[0]\n      kb.os_minor_version = versions.versions[1]\n    except IndexError:\n      # Some OSs don't have a minor version.\n      pass\n  client_os = client_obj.Get(client_schema.SYSTEM)\n  if client_os:\n    kb.os = utils.SmartUnicode(client_obj.Get(client_schema.SYSTEM))"}
{"prompt": "def append_pair(self, tag, value, header=False):\n        \"\"\"Append a tag=value pair to this message.\n\n        :param tag: Integer or string FIX tag number.\n        :param value: FIX tag value.\n        :param header: Append to header if True; default to body.\n\n        Both parameters are explicitly converted to strings before\n        storage, so it's ok to pass integers if that's easier for\n        your program logic.\n\n        Note: a Python 'None' value will be silently ignored, and\n        no field is appended.\"\"\"", "output": "\n        if header:\n            self.body.append(tag)\n            self.header_written = True\n        else:\n            self.body.append(value)\n            self.header_written = False\n\n    def get_header(self):\n        \"\"\"Return the header message as a string.\n\n        :rtype: String\n        :returns: Header message.\"\"\"\n        if self.header_written:\n            return self.header_message.encode('ascii')\n        else:\n            return None\n\n    def get_body(self):\n        \"\"\"Return the body of this message.\n\n        :rtype: List of String\n        :returns: List of String", "solution": "\n\n        if tag is None or value is None:\n            return\n\n        if int(tag) == 8:\n            self.begin_string = fix_val(value)\n\n        if int(tag) == 35:\n            self.message_type = fix_val(value)\n\n        if header:\n            self.pairs.insert(self.header_index,\n                              (fix_tag(tag),\n                               fix_val(value)))\n            self.header_index += 1\n        else:\n            self.pairs.append((fix_tag(tag), fix_val(value)))\n        return"}
{"prompt": "def _get_and_set_force_cache_miss(request):\n        \"\"\"\n        Gets value for request query parameter FORCE_CACHE_MISS\n        and sets it in the default request cache.\n\n        This functionality is only available for staff.\n\n        Example:\n            http://clobert.com/api/v1/resource?force_cache_miss=true\n\n        \"\"\"", "output": "\n        self.default_request_cache.append(request.GET.get('force_cache_miss', False))\n        return self.default_request_cache\n\n    def _get_and_set_cache_hit(self, request):\n        \"\"\"\n        Gets value for request query parameter FORCE_CACHE_HIT\n        and sets it in the default request cache.\n\n        This functionality is only available for staff.\n\n        Example:\n            http://clobert.com/api/v1/resource?hit=true\n\n        \"\"\"\n        self.default_request_cache.append(request.GET.get('hit',", "solution": "\n        if not (request.user and request.user.is_active and request.user.is_staff):\n            force_cache_miss = False\n        else:\n            force_cache_miss = request.GET.get(FORCE_CACHE_MISS_PARAM, 'false').lower() == 'true'\n        DEFAULT_REQUEST_CACHE.set(SHOULD_FORCE_CACHE_MISS_KEY, force_cache_miss)"}
{"prompt": "def umap(path, name=None, include=None, namespace=None, priority=None):\n    \"\"\"\n        Maps a given URL path, name and namespace to a view.\n        Arguments:\n            - path: the URL regex, e.g.: '^teste/(?P<pk>[0-9])/$'.\n\n        Optional arguments:\n            - name: the URL name, which Django uses to identify the URL;\n            - include: A custom URL list, previously\n                       set on the module's urls.py;\n            - namespace: the URL's namespace;\n            - priority: the URL's priority;\n    \"\"\"", "output": "\n    if namespace is None:\n        namespace = getattr(settings, 'URL_NAMESPACE', None)\n    if name is None:\n        name = getattr(settings, 'URL_NAME', None)\n    if include is None:\n        include = getattr(settings, 'URL_INCLUDE', None)\n    if not name:\n        return None\n    if path.startswith('django/'):\n        return reverse('django_view_with_app_lookup', args=[namespace, name])\n    else:\n        return reverse('django_view_with_model', args=[namespace, name])\n\n\ndef reverse_lazy(view_name,", "solution": "\n    def url_wrapper(view):\n        # gets the module name\n        module = _find_urls_module(view)\n        # gets the view function (checking if it's a class-based view)\n        fn = view.as_view() if hasattr(view, 'as_view') else view\n\n        if namespace and include:\n            raise TypeError(\n                'You can\\'t use \\'namespace\\' and \\'include\\''\n                ' at the same time!'\n            )\n\n        if namespace:\n            # imports the urlpatterns object\n            base = import_string('{}.urls.urlpatterns'.format(module))\n            # searchs for the namespace\n            urlpatterns_list = [\n                x for x in base\n                if getattr(x, 'namespace', None) == namespace\n            ]\n            # if the list length is different than 1,\n            # then the namespace is either duplicated or doesn't exist\n            if len(urlpatterns_list) != 1:\n                raise ValueError(\n                    'Namespace \\'{}\\' not in list.'.format(namespace)\n                )\n            # if the namespace was found, get its object\n            urlpatterns = urlpatterns_list.pop(0).url_patterns\n        else:\n            # imports the urlpatterns object\n            urlpatterns = import_string('{}.urls.{}'.format(\n                module,\n                include or 'urlpatterns'\n            ))\n        # appends the url with its given name\n        call = (\n            urlpatterns.append if priority is None\n            else partial(urlpatterns.insert, priority)\n        )\n        call(url(path, fn, name=name))\n        return view\n    return url_wrapper"}
{"prompt": "def generate(env):\n    \"\"\"Add Builders and construction variables for SunPRO C++.\"\"\"", "output": "\n    SCons.Tool.createSharedLibBuilder(env)\n    SCons.Tool.createProgBuilder(env)\n\n    env['AR']          = 'ar'\n    env['ARFLAGS']     = SCons.Util.CLVar('-c')\n    env['ARCOM']       = '$AR --create $TARGET $SOURCES'\n    env['AS']          = 'as'\n    env['ASFLAGS']    = SCons.Util.CLVar('-s')\n    env['RC']          = 'rc'\n    env['RCFLAGS']    = SCons.Util.CLVar('-s')\n    env['RCCOM']", "solution": "\n    path, cxx, shcxx, version = get_cppc(env)\n    if path:\n        cxx = os.path.join(path, cxx)\n        shcxx = os.path.join(path, shcxx)\n\n    cplusplus.generate(env)\n\n    env['CXX'] = cxx\n    env['SHCXX'] = shcxx\n    env['CXXVERSION'] = version\n    env['SHCXXFLAGS']   = SCons.Util.CLVar('$CXXFLAGS -KPIC')\n    env['SHOBJPREFIX']  = 'so_'\n    env['SHOBJSUFFIX']  = '.o'"}
{"prompt": "def copy_configs(configs, source, target):\n    \"\"\"\n    Copy dynamips configs to converted topology\n\n    :param configs: Configs to copy\n    :param str source: Source topology directory\n    :param str target: Target topology files directory\n    :return: True when a config cannot be found, otherwise false\n    :rtype: bool\n    \"\"\"", "output": "\n    if not os.path.exists(target):\n        os.makedirs(target)\n\n    for config in configs:\n        if not os.path.exists(os.path.join(config, 'dynamips.yml')):\n            return False\n\n        with open(os.path.join(config, 'dynamips.yml'), 'r') as f:\n            try:\n                converted = yaml.load(f)\n            except yaml.YAMLError as exc:\n                raise ConfigurationError(\n                    'invalid yaml file, contains invalid structure, '\n                   'must be a valid yaml dictionary'\n                ) from exc\n\n        for key in", "solution": "\n    config_err = False\n    if len(configs) > 0:\n        config_dir = os.path.join(target, 'dynamips', 'configs')\n        os.makedirs(config_dir)\n        for config in configs:\n            old_config_file = os.path.join(source, config['old'])\n            new_config_file = os.path.join(config_dir,\n                                           os.path.basename(config['new']))\n            if os.path.isfile(old_config_file):\n                # Copy and rename the config\n                shutil.copy(old_config_file, new_config_file)\n            else:\n                config_err = True\n                logging.error('Unable to find %s' % config['old'])\n    return config_err"}
{"prompt": "def avail_images(conn=None, call=None):\n    \"\"\"\n    Return a list of the server appliances that are on the provider\n    \"\"\"", "output": "\n    if conn is None:\n        conn = get_ems_inventory_conn()\n    return conn.list_images(image_group_name=image_group_name, datacenter_name=datacenter_name,\n                         filters={'tag:Name': image_name})\n\n\ndef get_ems_hosts(conn=None, call=None):\n    \"\"\"\n    Return a list of EMS host names\n    \"\"\"\n    if conn is None:\n        conn = get_ems_conn()\n    return conn.list_host(host_name=None, datacenter_name=datacenter_name,\n                         host", "solution": "\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The avail_images function must be called with '\n            '-f or --function, or with the --list-images option'\n        )\n\n    if not conn:\n        conn = get_conn()\n\n    ret = {}\n\n    for appliance in conn.list_appliances():\n        ret[appliance['name']] = appliance\n\n    return ret"}
{"prompt": "def setup_prefix_suffix(self):\n        \"\"\"\n        Set up the compile prefix, sourcepath and the targetpath suffix\n        attributes, which are the prefix to the function name and the\n        suffixes to retrieve the values from for creating the generator\n        function.\n        \"\"\"", "output": "\n        self.prefix, self.sourcepath = self.split_prefix(self.prefix)\n        self.ext = '.c'\n        if self.prefix == '':\n            self.prefix = ''\n        if self.sourcepath == '':\n            self.sourcepath = ''\n        if self.targetpath == '':\n            self.targetpath = ''\n        self.target = self.prefix + self.targetpath\n        self.outputs = [self.target]\n        self.inputs = [self.sourcepath, self.targetpath]\n\n    def split_prefix(self, s):\n        \"\"\"\n        Splits the given string", "solution": "\n\n        self.compile_prefix = 'compile_'\n        self.sourcepath_suffix = '_sourcepath'\n        self.modpath_suffix = '_modpaths'\n        self.targetpath_suffix = '_targetpaths'"}
{"prompt": "def _g(self, h, xp, s):\n        \"\"\"Density function for blow and hop moves\"\"\"", "output": "\n        return self.get_density(h, xp, s)\n\n    def _g_from_density(self, density):\n        \"\"\"Density function from density\"\"\"\n        return self.get_density(h, density)\n\n    def _g_from_energy(self, energy):\n        \"\"\"Energy function from energy\"\"\"\n        return self.get_energy(energy)\n\n    def _g_from_energy_per_atom(self, energy_per_atom):\n        \"\"\"Energy per atom\"\"\"\n        return self.get_energy_per_atom(energy_per_atom)\n\n    def get_density(self", "solution": "\n\n        nphi = sum(self.phi)\n\n        return (nphi / 2.0) * log(2 * pi) + nphi * \\\n            log(s) + 0.5 * sum((h - xp) ** 2) / (s ** 2)"}
{"prompt": "def get_instance_route53_names(self, instance):\n        \"\"\" Check if an instance is referenced in the records we have from\n        Route53. If it is, return the list of domain names pointing to said\n        instance. If nothing points to it, return an empty list. \"\"\"", "output": "\n        if instance is None:\n            return []\n        name_list = []\n        if 'Name' in instance:\n            name_list = instance['Name'].split(',')\n        if 'Name Authority' in instance:\n            name_list = instance['NameAuthority'].split(',')\n        if 'Name Suffix' in instance:\n            name_list = instance['NameSuffix'].split(',')\n        if 'Name Suffix Authority' in instance:\n            name_list = instance['NameSuffixAuthority'].split(',')\n        if 'Name Suffix Suffix' in instance:\n            name_list = instance['NameSuffixSuffix", "solution": "\n\n        instance_attributes = [ 'public_dns_name', 'private_dns_name',\n                                'ip_address', 'private_ip_address' ]\n\n        name_list = set()\n\n        for attrib in instance_attributes:\n            try:\n                value = getattr(instance, attrib)\n            except AttributeError:\n                continue\n\n            if value in self.route53_records:\n                name_list.update(self.route53_records[value])\n\n        return list(name_list)"}
{"prompt": "def _transform_row(self, in_row, out_row):\n        \"\"\"\n        Transforms an input row to an output row (i.e. (partial) dimensional data).\n\n        :param dict[str,str] in_row: The input row.\n        :param dict[str,T] out_row: The output row.\n\n        :rtype: (str,str)\n        \"\"\"", "output": "\n        return {\n            'in_row': in_row,\n            'out_row': out_row,\n           'mask': self.mask,\n            'label': self.label,\n            'type': self.type,\n           'shape': self.shape,\n            'fill_color': self.fill_color,\n            'fill_opacity': self.fill_opacity,\n            'fill_opacity': self.fill_opacity,\n           'stroke_color': self.stroke_color,\n           'stroke_opacity': self.stroke_opacity,\n           'stroke_opacity': self.stroke_opacity,\n           ", "solution": "\n        tmp_row = {}\n\n        for step in self._steps:\n            park_info, ignore_info = step(in_row, tmp_row, out_row)\n            if park_info or ignore_info:\n                return park_info, ignore_info\n\n        return None, None"}
{"prompt": "def at(self, t):\n        \"\"\"At time ``t``, compute the target's position relative to the center.\n\n        If ``t`` is an array of times, then the returned position object\n        will specify as many positions as there were times.  The kind of\n        position returned depends on the value of the ``center``\n        attribute:\n\n        * Solar System Barycenter: :class:`~skyfield.positionlib.Barycentric`\n        * Center of the Earth: :class:`~skyfield.positionlib.Geocentric`\n        * Difference: :class:`~skyfield.positionlib.Geometric`\n        * Anything else: :class:`~skyfield.positionlib.ICRF`\n\n        \"\"\"", "output": ".Difference`\n        * Skyfield: :class:`~skyfield.skyfield.Skyfield`\n        * Skyfield: :class:`~skyfield.skyfield.Position`\n\n        :param t: array of times\n        :type t: :class:`numpy.ndarray`\n\n        :returns: position object\n        :rtype: :class:`SkyPosition`\n\n        \"\"\"\n        if not isinstance(t, np.ndarray):\n            t = np.array(t)\n        if not t.shape == (2,):\n            raise ValueError(\"at() requires 2D input\")\n        if t.shape[0]!=", "solution": "\n        if not isinstance(t, Time):\n            raise ValueError('please provide the at() method with a Time'\n                             ' instance as its argument, instead of the'\n                             ' value {0!r}'.format(t))\n        observer_data = ObserverData()\n        observer_data.ephemeris = self.ephemeris\n        p, v, observer_data.gcrs_position, message = self._at(t)\n        center = self.center\n        if center == 0:\n            observer_data.bcrs_position = p\n            observer_data.bcrs_velocity = v\n        self._snag_observer_data(observer_data, t)\n        position = build_position(p, v, t, center, self.target, observer_data)\n        position.message = message\n        return position"}
{"prompt": "def SerializeExclusiveData(self, writer):\n        \"\"\"\n        Serialize object.\n\n        Args:\n            writer (neo.IO.BinaryWriter):\n        \"\"\"", "output": "\n        self.serialize_binary(writer, exclusive=True)\n\n    def Serialize(self, writer):\n        \"\"\"\n        Serialize object.\n\n        Args:\n            writer (neo.IO.BinaryWriter):\n        \"\"\"\n        self.serialize_binary(writer, exclusive=True)\n        writer.write(self.serialize())\n\n    def serialize_binary(self, writer, exclusive=False):\n        \"\"\"\n        Serialize object.\n\n        Args:\n            writer (neo.IO.BinaryWriter):\n        \"\"\"\n        if not self.binary:\n            return\n\n        if exclusive:\n            writer.write_uint(exclusive)\n        else", "solution": "\n        writer.WriteVarBytes(self.Script)\n        if self.Version >= 1:\n            writer.WriteFixed8(self.Gas)"}
{"prompt": "def git_branch(self):\n        \"\"\"Git branch name.\"\"\"", "output": "\n        return self.get_branch_name()\n\n    def git_branch_name(self):\n        \"\"\"Git branch name.\"\"\"\n        return self.get_branch_name()\n\n    def git_branch_path(self):\n        \"\"\"Git branch path.\"\"\"\n        return self.get_branch_path()\n\n    def git_branch_path_name(self):\n        \"\"\"Git branch path name.\"\"\"\n        return self.git_branch_path()\n\n    def git_branch_path_path(self):\n        \"\"\"Git branch path path.\"\"\"\n        return self.git_branch_path()\n\n    def git_branch_path_", "solution": "\n        info = self.info\n        return 'rpm-{major}.{minor}.x'.format(\n            major=info[0], minor=info[1])"}
{"prompt": "def determine_start_end_timestep(lsm_file_list,\n                                 file_re_match=None,\n                                 file_datetime_pattern=None,\n                                 expected_time_step=None,\n                                 lsm_grid_info=None):\n    \"\"\"\n    Determine the start and end date from LSM input files\n    \"\"\"", "output": "\n    if file_re_match is None:\n        file_re_match = lsm_file_list[0]\n    if file_datetime_pattern is None:\n        file_datetime_pattern = lsm_file_list[1]\n    if expected_time_step is None:\n        expected_time_step = determine_timestep(lsm_file_list)\n    if file_re_match.start()!= expected_time_step.start():\n        raise ValueError(\"File start date is not in expected format\")\n    if file_re_match.end()!= expected_time_", "solution": "\n    if lsm_grid_info is None:\n        lsm_grid_info = identify_lsm_grid(lsm_file_list[0])\n\n    if None in (lsm_grid_info['time_var'], lsm_grid_info['time_dim'])\\\n            or lsm_grid_info['model_name'] in ('era_20cm', 'erai'):\n        # NOTE: the ERA20CM and ERA 24hr time variables\n        # in the tests are erroneous\n        if None in (file_re_match, file_datetime_pattern):\n            raise ValueError(\"LSM files missing time dimension and/or \"\n                             \"variable.To mitigate this, add the \"\n                             \"'file_re_match' and 'file_datetime_pattern' \"\n                             \"arguments.\")\n\n        if lsm_grid_info['time_dim'] is None:\n            print(\"Assuming time dimension is 1\")\n            file_size_time = 1\n        else:\n            lsm_example_file = Dataset(lsm_file_list[0])\n            file_size_time = \\\n                len(lsm_example_file.dimensions[lsm_grid_info['time_dim']])\n            lsm_example_file.close()\n\n        total_num_time_steps = int(file_size_time * len(lsm_file_list))\n\n        # determine the start time from the existing files\n        actual_simulation_start_datetime = \\\n            datetime.strptime(file_re_match.search(lsm_file_list[0]).group(0),\n                              file_datetime_pattern)\n\n        # check to see if the time step matches expected\n        if len(lsm_file_list) > 1:\n            time_step = \\\n                int((datetime.strptime(\n                    file_re_match.search(lsm_file_list[1]).group(0),\n                    file_datetime_pattern) -\n                    actual_simulation_start_datetime).total_seconds()\n                    / float(file_size_time))\n\n        elif expected_time_step is not None:\n            time_step = int(expected_time_step)\n        else:\n            raise ValueError(\"Only one LSM file with one timestep present. \"\n                             \"'expected_time_step' parameter required to \"\n                             \"continue.\")\n\n        # determine the end datetime\n        actual_simulation_end_datetime = \\\n            datetime.strptime(file_re_match.search(lsm_file_list[-1]).group(0),\n                              file_datetime_pattern) \\\n            + timedelta(seconds=(file_size_time-1) * time_step)\n    else:\n        with pangaea.open_mfdataset(lsm_file_list,\n                                    lat_var=lsm_grid_info['latitude_var'],\n                                    lon_var=lsm_grid_info['longitude_var'],\n                                    time_var=lsm_grid_info['time_var'],\n                                    lat_dim=lsm_grid_info['latitude_dim'],\n                                    lon_dim=lsm_grid_info['longitude_dim'],\n                                    time_dim=lsm_grid_info['time_dim']) as xds:\n\n            datetime_arr = [pd.to_datetime(dval) for dval in\n                            xds.lsm.datetime.values]\n            actual_simulation_start_datetime = datetime_arr[0]\n            actual_simulation_end_datetime = datetime_arr[-1]\n            total_num_time_steps = len(datetime_arr)\n\n            if total_num_time_steps <= 1:\n                if expected_time_step is not None:\n                    time_step = int(expected_time_step)\n                else:\n                    raise ValueError(\"Only one LSM file with one timestep \"\n                                     \"present. 'expected_time_step' parameter \"\n                                     \"required to continue.\")\n            else:\n                time_step = int(np.diff(xds.lsm.datetime.values)[0]\n                                / np.timedelta64(1, 's'))\n\n    if expected_time_step is not None:\n        if time_step != int(expected_time_step):\n            print(\"WARNING: The time step used {0} is different than \"\n                  \"expected {1}\".format(time_step, expected_time_step))\n\n    return (actual_simulation_start_datetime, actual_simulation_end_datetime,\n            time_step, total_num_time_steps)"}
{"prompt": "def wait_and_start_browser(host, port=None, cancel_event=None):\n    \"\"\"\n    Waits for the server to run and then opens the specified address in\n    the browser. Set cancel_event to cancel the wait.\n    \"\"\"", "output": "\n    if port is None:\n        port = _DEFAULT_PORT\n    if cancel_event is None:\n        cancel_event = threading.Event()\n    if host is None:\n        host = _DEFAULT_HOST\n    if port is None:\n        port = _DEFAULT_PORT\n    if host is None:\n        raise Exception(\"Could not find a valid host.\")\n    if cancel_event.wait(timeout=5):\n        raise Exception(\"Timed out waiting for server to start.\")\n    if cancel_event.isSet():\n        raise Exception(\"Server did not start.\")\n    return cancel_event.isSet()\n\n\ndef wait", "solution": "\n    if host == '0.0.0.0':\n        host = 'localhost'\n    if port is None:\n        port = 80\n\n    if wait_for_server(host, port, cancel_event):\n        start_browser('http://{0}:{1}/'.format(host, port))"}
{"prompt": "def _set_data(self, action):\n        \"\"\"\n        marshals response data into page data\n        \"\"\"", "output": "\n        self.response_data = {\n            'action': action,\n            'action_url': self.get_action_url(action),\n            'action_text': self.get_action_text(action),\n            'action_url_text': self.get_action_url_text(action),\n            'action_text_url': self.get_action_text_url(action),\n            'action_text_text': self.get_action_text_text(action),\n            'action_url_text': self.get_action_url_url(action),\n            '", "solution": "\n        if 'query' in action:\n            self._set_query_data(action)\n        elif action == 'imageinfo':\n            self._set_imageinfo_data()\n        elif action == 'parse':\n            self._set_parse_data()\n        elif action == 'random':\n            self._set_random_data()\n        elif action == 'labels':\n            self._set_labels()\n        elif action == 'wikidata':\n            self._set_wikidata()\n            self.get_labels()\n        elif action == 'restbase':\n            self._set_restbase_data()\n\n        self._update_imageinfo()\n        self._update_params()"}
{"prompt": "def denied_response(self, req):\n        \"\"\"Returns a standard WSGI response callable with the status of 403 or 401\n        depending on whether the REMOTE_USER is set or not.\n        \"\"\"", "output": "\n        if req.environ.get('swift.authorize_override') is not None:\n            return HTTPForbidden()(req)\n        else:\n            return self.app(req)\n\n    def handle_request(self, req):\n        \"\"\"\n        Entry point for auth requests (two steps)\n\n        If the request is a user authentication request we try to authenticate\n        against the user and if successful, we return the response.  Otherwise, we\n        update the cookie.\n        \"\"\"\n        if not req.path.startswith(self.reseller_prefix):\n            return self.denied_response(req)\n\n        if not req.path", "solution": "\n        if not hasattr(req, 'credentials_valid'):\n            req.credentials_valid = None\n        if req.remote_user or req.credentials_valid:\n            return HTTPForbidden(request=req)\n        else:\n            return HTTPUnauthorized(request=req)"}
{"prompt": "def enter_room(self, sid, room, namespace=None):\n        \"\"\"Enter a room.\n\n        This function adds the client to a room. The :func:`emit` and\n        :func:`send` functions can optionally broadcast events to all the\n        clients in a room.\n\n        :param sid: Session ID of the client.\n        :param room: Room name. If the room does not exist it is created.\n        :param namespace: The Socket.IO namespace for the event. If this\n                          argument is omitted the default namespace is used.\n        \"\"\"", "output": "\n        if namespace is None:\n            namespace = self.default_namespace\n        if namespace is None:\n            namespace = self.client_namespace\n        self.emit('room-enter', sid, room, namespace)\n\n    def send_event(self, event, data=None):\n        \"\"\"Send a single event.\n\n        This function adds the client to a single event. The :func:`emit` and\n        :func:`send` functions can optionally broadcast events to all the\n        clients in a single event.\n\n        :param event: Event name.\n        :param data: Optional data to send.\n        \"\"\"\n        if data is None", "solution": "\n        namespace = namespace or '/'\n        self.logger.info('%s is entering room %s [%s]', sid, room, namespace)\n        self.manager.enter_room(sid, namespace, room)"}
{"prompt": "def parse(readDataInstance):\n        \"\"\"\n        Returns a new L{NetMetaDataHeader} object.\n        \n        @type readDataInstance: L{ReadData}\n        @param readDataInstance: A L{ReadData} object with data to be parsed as a L{NetMetaDataHeader} object.\n        \n        @rtype: L{NetMetaDataHeader}\n        @return: A new L{NetMetaDataHeader} object.\n        \"\"\"", "output": "\n        header = NetMetaDataHeader()\n        header.read(readDataInstance)\n        header.read(readDataInstance)\n        header.read(readDataInstance)\n        header.read(readDataInstance)\n        header.read(readDataInstance)\n        header.read(readDataInstance)\n        header.read(readDataInstance)\n        header.read(readDataInstance)\n        header.read(readDataInstance)\n        header.read(readDataInstance)\n        header.read(readDataInstance)\n        header.read(readDataInstance)\n        header.read(readDataInstance)\n        header.read", "solution": "\n        nmh = NetMetaDataHeader()\n        \n        nmh.signature.value = readDataInstance.readDword()\n        nmh.majorVersion.value = readDataInstance.readWord()\n        nmh.minorVersion.value = readDataInstance.readWord()\n        nmh.reserved.value = readDataInstance.readDword()\n        nmh.versionLength.value = readDataInstance.readDword()\n        nmh.versionString.value = readDataInstance.readAlignedString()\n        nmh.flags.value = readDataInstance.readWord()\n        nmh.numberOfStreams.value = readDataInstance.readWord()\n        return nmh"}
{"prompt": "def _get_args_contents(self):\n        \"\"\"\n        Mimic the argument formatting behaviour of\n        ActionBase._execute_module().\n        \"\"\"", "output": "\n        return self._execute_module().contents\n\n    def _get_kwargs(self):\n        \"\"\"\n        Mimic the keyword arguments.\n        \"\"\"\n        return self._execute_module().contents\n\n    def _get_action_class(self):\n        \"\"\"\n        Return the Action subclass to use for this action.\n        \"\"\"\n        return self._execute_module().actions[self.action_name]\n\n    def _get_action_name(self):\n        \"\"\"\n        Return the name of the action (string)\n        \"\"\"\n        return self._get_action_name_from_kwargs()\n\n    def _get_action_name", "solution": "\n        return ' '.join(\n            '%s=%s' % (key, shlex_quote(str(self.args[key])))\n            for key in self.args\n        ) + ' '"}
{"prompt": "def fly(self):\n        \"\"\"\n        Generate doc tree.\n        \"\"\"", "output": "\n        self.doc_tree = DocTree()\n        self.doc_tree.set_root_node(self.root_node)\n        self.doc_tree.set_root_node(self.root_node)\n        self.doc_tree.set_root_node(self.root_node)\n        self.doc_tree.set_root_node(self.root_node)\n        self.doc_tree.set_root_node(self.root_node)\n        self.root_node.children.append(self.doc_tree)\n        self.root_node", "solution": "\n        dst_dir = Path(self.conf_file).parent.abspath\n\n        package_dir = Path(dst_dir, self.package.shortname)\n\n        # delete existing api document\n        try:\n            if package_dir.exists():\n                shutil.rmtree(package_dir.abspath)\n        except Exception as e:\n            print(\"'%s' can't be removed! Error: %s\" % (package_dir, e))\n\n        # create .rst files\n        for pkg, parent, sub_packages, sub_modules in self.package.walk():\n            if not is_ignored(pkg, self.ignored_package):\n                dir_path = Path(*([dst_dir, ] + pkg.fullname.split(\".\")))\n                init_path = Path(dir_path, \"__init__.rst\")\n\n                make_dir(dir_path.abspath)\n                make_file(\n                    init_path.abspath,\n                    self.generate_package_content(pkg),\n                )\n\n                for mod in sub_modules:\n                    if not is_ignored(mod, self.ignored_package):\n                        module_path = Path(dir_path, mod.shortname + \".rst\")\n                        make_file(\n                            module_path.abspath,\n                            self.generate_module_content(mod),\n                        )"}
{"prompt": "def taper(self):\n        \"\"\"Taper the spectrum by adding zero flux to each end.\n        This is similar to :meth:`SpectralElement.taper`.\n\n        There is no check to see if the spectrum is already tapered.\n        Hence, calling this on a tapered spectrum will result in\n        multiple zero-flux entries at both ends.\n\n        The wavelengths to use for the new first and last points are\n        calculated by using the same ratio as for the two interior points\n        used at each end.\n\n        Returns\n        -------\n        OutSpec : `TabularSourceSpectrum`\n            Tapered spectrum.\n\n        \"\"\"", "output": " with zero-flux entries.\n        \"\"\"\n        return self.taper_with_zero_flux(taper_type='taper')\n\n    def taper_with_zero_flux(self, taper_type='taper'):\n        \"\"\"Tapered tapered spectrum with zero-flux entries.\n\n        This method is similar to :meth:`SpectralElement.taper_with_zero_flux`.\n\n        There is no check to see if the tapered spectrum is already tapered.\n        Hence, calling this on a tapered tapered spectrum will result in\n        multiple zero-", "solution": "\n        OutSpec = TabularSourceSpectrum()\n        wcopy = N.zeros(self._wavetable.size+2, dtype=N.float64)\n        fcopy = N.zeros(self._fluxtable.size+2, dtype=N.float64)\n        wcopy[1:-1] = self._wavetable\n        fcopy[1:-1] = self._fluxtable\n        fcopy[0] = 0.0\n        fcopy[-1] = 0.0\n\n        # The wavelengths to use for the first and last points are\n        # calculated by using the same ratio as for the 2 interior points\n        wcopy[0] = wcopy[1]*wcopy[1]/wcopy[2]\n        wcopy[-1] = wcopy[-2]*wcopy[-2]/wcopy[-3]\n\n        OutSpec._wavetable = wcopy\n        OutSpec._fluxtable = fcopy\n        OutSpec.waveunits = units.Units(str(self.waveunits))\n        OutSpec.fluxunits = units.Units(str(self.fluxunits))\n\n        return OutSpec"}
{"prompt": "def install(runas=None, path=None):\n    \"\"\"\n    Install pyenv systemwide\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pyenv.install\n    \"\"\"", "output": "\n    if runas is None:\n        runas = _get_runas()\n    if path is None:\n        path = _get_path()\n    if runas is None:\n        runas = _get_runas()\n    if runas is None:\n        return False\n    if runas.is_file('/etc/rc.conf'):\n        rc_conf = '/etc/rc.conf'\n    else:\n        rc_conf = '/etc/rc.conf.d/rc.conf'\n    if not os.path.exists(rc_conf):\n        return False\n    if not os.path", "solution": "\n    path = path or _pyenv_path(runas)\n    path = os.path.expanduser(path)\n    return _install_pyenv(path, runas)"}
{"prompt": "def create_domain_record(self, domain_id, record_type, data, name=None,\n                             priority=None, port=None, weight=None):\n        \"\"\"\n        This method creates a new domain name with an A record for the specified\n        [ip_address].\n\n        Required parameters\n\n            domain_id:\n                Integer or Domain Name (e.g. domain.com), specifies the domain\n                for which to create a record.\n\n            record_type:\n                String, the type of record you would like to create.\n                'A', 'CNAME', 'NS', 'TXT', 'MX' or 'SRV'\n\n            data:\n                String, this is the value of the record\n\n        Optional parameters\n            name:\n                String, required for 'A', 'CNAME', 'TXT' and 'SRV' records\n\n            priority:\n                Integer, required for 'SRV' and 'MX' records\n\n            port:\n                Integer, required for 'SRV' records\n\n            weight:\n                Integer, required for 'SRV' records\n        \"\"\"", "output": " 'NS', the type of record you would like to create.\n\n            data:\n                String, the data you would like to create.\n                String, the data you would like to create.\n\n            name:\n                String, the name you would like to create.\n                String, the name you would like to create.\n\n            priority:\n                Integer, the priority you would like to create.\n\n            port:\n                Integer, the port you would like to create.\n\n            weight:\n                Integer, the weight you would like to create.\n        \"\"\"\n        return self.api_request(path='/domains/{0}/records/{1}'.format(domain", "solution": "\n        params = dict(record_type=record_type, data=data)\n\n        if name:\n            params.update({'name': name})\n        if priority:\n            params.update({'priority': priority})\n        if port:\n            params.update({'port': port})\n        if weight:\n            params.update({'weight': weight})\n\n        json = self.request('/domains/%s/records/new' % domain_id, method='GET', params=params)\n        status = json.get('status')\n        if status == 'OK':\n            domain_record_json = json.get('domain_record')\n            domain_record = Record.from_json(domain_record_json)\n            return domain_record\n        else:\n            message = json.get('message')\n            raise DOPException('[%s]: %s' % (status, message))"}
{"prompt": "def install_labels(cls, quiet=True, stdout=None):\n    \"\"\"\n    Setup labels with indexes and constraints for a given class\n\n    :param cls: StructuredNode class\n    :type: class\n    :param quiet: (default true) enable standard output\n    :param stdout: stdout stream\n    :type: bool\n    :return: None\n    \"\"\"", "output": "\n    if stdout is None:\n        stdout = sys.stdout\n    if not stdout:\n        stdout = sys.stdout\n    if not quiet:\n        stdout.write(\"Creating index files for %s\\n\" % cls.name)\n        stdout.write(\"  - %s\\n\" % cls.name)\n        stdout.write(\"  - %s\\n\" % cls.name)\n        stdout.write(\"  - %s\\n\" % cls.name)\n        stdout.write(\"  - %s\\n\" % cls.name)\n        stdout.write(\"  - %s\\n\" %", "solution": "\n\n    if not hasattr(cls, '__label__'):\n        if not quiet:\n            stdout.write(' ! Skipping class {0}.{1} is abstract\\n'.format(cls.__module__, cls.__name__))\n        return\n\n    for name, property in cls.defined_properties(aliases=False, rels=False).items():\n        db_property = property.db_property or name\n        if property.index:\n            if not quiet:\n                stdout.write(' + Creating index {0} on label {1} for class {2}.{3}\\n'.format(\n                    name, cls.__label__, cls.__module__, cls.__name__))\n\n            db.cypher_query(\"CREATE INDEX on :{0}({1}); \".format(\n                cls.__label__, db_property))\n\n        elif property.unique_index:\n            if not quiet:\n                stdout.write(' + Creating unique constraint for {0} on label {1} for class {2}.{3}\\n'.format(\n                    name, cls.__label__, cls.__module__, cls.__name__))\n\n            db.cypher_query(\"CREATE CONSTRAINT \"\n                            \"on (n:{0}) ASSERT n.{1} IS UNIQUE; \".format(\n                cls.__label__, db_property))"}
{"prompt": "def consult_filters(self, url_info: URLInfo, url_record: URLRecord, is_redirect: bool=False) \\\n            -> Tuple[bool, str, dict]:\n        \"\"\"Consult the URL filter.\n\n        Args:\n            url_record: The URL record.\n            is_redirect: Whether the request is a redirect and it is\n                desired that it spans hosts.\n\n        Returns\n            tuple:\n\n            1. bool: The verdict\n            2. str: A short reason string: nofilters, filters, redirect\n            3. dict: The result from :func:`DemuxURLFilter.test_info`\n        \"\"\"", "output": "is_redirect`\n        \"\"\"\n        if is_redirect:\n            if not url_info.is_redirect:\n                url_info.is_redirect = True\n            return (True, url_info.is_redirect)\n        else:\n            if not url_record.is_redirect:\n                url_record.is_redirect = True\n            return (True, '', url_info.is_redirect)\n\n    def get_demux_url_filter(self, url_info: URLInfo) \\\n            -> DemuxURLFilter:\n        \"\"\"Get a :class:`DemuxURLFilter` for the given URL", "solution": "\n        if not self._url_filter:\n            return True, 'nofilters', None\n\n        test_info = self._url_filter.test_info(url_info, url_record)\n\n        verdict = test_info['verdict']\n\n        if verdict:\n            reason = 'filters'\n        elif is_redirect and self.is_only_span_hosts_failed(test_info):\n            verdict = True\n            reason = 'redirect'\n        else:\n            reason = 'filters'\n\n        return verdict, reason, test_info"}
{"prompt": "def write_csv(path, rows, dialect='excel', fieldnames=None, quoting=csv.QUOTE_ALL, extrasaction='ignore', *args, **kwargs):\n        \"\"\" Write rows data to a CSV file (with or without fieldnames) \"\"\"", "output": "\n        if not quoting:\n            quoting = csv.QUOTE_MINIMAL\n\n        writer = csv.DictWriter(path, fieldnames=fieldnames, extrasaction=extrasaction)\n        if quoting:\n            writer.writeheader()\n        for row in rows:\n            if dialect == 'excel':\n                row = self.convert_rows(row)\n            writer.writerow(row)\n\n        return path\n\n    def write_table(self, table, path, fields, delimiter=',',\n            quotechar='\"', quoting=csv.QUOTE_ALL,\n            header=True, quotechar='\"'):\n        \"\"\" Write a table data structure to", "solution": "\n        if not quoting:\n            quoting = csv.QUOTE_MINIMAL\n        if 'lineterminator' not in kwargs:\n            kwargs['lineterminator'] = '\\n'  # use \\n to fix double-line in Windows\n        with open(path, mode='wt', newline='') as csvfile:\n            if fieldnames:\n                writer = csv.DictWriter(csvfile, fieldnames=fieldnames, dialect=dialect, quoting=quoting, extrasaction=extrasaction, *args, **kwargs)\n                writer.writeheader()\n                for row in rows:\n                    writer.writerow(row)\n            else:\n                writer = csv.writer(csvfile, dialect=dialect, quoting=quoting, *args, **kwargs)\n                for row in rows:\n                    writer.writerow(row)"}
{"prompt": "def _GetCh(self):\n    \"\"\"Read a single character from the user.\n\n    Returns:\n      A string, the character read.\n    \"\"\"", "output": "\n    self._Read(1)\n    return self._Read(2)\n\n  def _GetC(self):\n    \"\"\"Read a character from the user.\n\n    Returns:\n      A string, the character read.\n    \"\"\"\n    self._Read(1)\n    return self._Read(2)\n\n  def _Read(self, n):\n    \"\"\"Read n bytes from the input stream.\n\n    Args:\n      n: The number of bytes to read.\n\n    Returns:\n      A string, the number of bytes read.\n    \"\"\"\n    self._stream.seek(n)\n    result = self._stream.read()\n    self._", "solution": "\n    fd = self._tty.fileno()\n    old = termios.tcgetattr(fd)\n    try:\n      tty.setraw(fd)\n      ch = self._tty.read(1)\n      # Also support arrow key shortcuts (escape + 2 chars)\n      if ord(ch) == 27:\n        ch += self._tty.read(2)\n    finally:\n      termios.tcsetattr(fd, termios.TCSADRAIN, old)\n    return ch"}
{"prompt": "def to_python(self, value):\n        \"\"\"Validates that the input can be converted to a list of decimals.\"\"\"", "output": "\n        if value is None:\n            value = []\n        return value\n\n    def to_url(self, value):\n        \"\"\"\n        Validates that the input can be converted to a URL.\n        \"\"\"\n        if value is None:\n            value = ''\n        return value\n\n    def to_python_list(self, value):\n        \"\"\"\n        Validates that the input is a list of strings.\n        \"\"\"\n        if value is None:\n            value = []\n        return value\n\n    def to_url_args(self, value):\n        \"\"\"\n        Converts the input value into a dictionary.\n        \"\"\"\n        if value is None:\n            value =", "solution": "\n        if not value:\n            return None\n\n        if isinstance(value, list):\n            for index, position_val in enumerate(value):\n                val = super(MultipleDecimalField, self).to_python(position_val)\n                value[index] = val\n\n        return value"}
{"prompt": "def parse(self, response):\n        \"\"\"\n        \u6839\u636e\u5bf9 ``start_urls`` \u4e2d\u63d0\u4f9b\u94fe\u63a5\u7684\u8bf7\u6c42\u54cd\u5e94\u5305\u5185\u5bb9\uff0c\u89e3\u6790\u751f\u6210\u5177\u4f53\u6587\u7ae0\u94fe\u63a5\u8bf7\u6c42\n\n        :param Response response: \u7531 ``Scrapy`` \u8c03\u7528\u5e76\u4f20\u5165\u7684\u8bf7\u6c42\u54cd\u5e94\u5bf9\u8c61\n        \"\"\"", "output": "\n        self.start_urls = [url for url in start_urls if url.startswith(self.base_url)]\n        return self\n\n    def get_urls(self):\n        \"\"\"\n        \u83b7\u53d6\u5177\u4f53\u7684urls\n\n        :return: \u6839\u636e\u5bf9\u8c61\u7684urls\n        \"\"\"\n        return self.start_urls\n\n    def get_start_urls(self):\n        \"\"\"\n        \u83b7\u53d6\u5177\u4f53\u7684start_urls\n\n        :return: \u6839\u636e\u5bf9\u8c61\u7684urls\n        \"\"\"\n        return self.start_urls\n\n    def get_urls_from", "solution": "\n        content_raw = response.body.decode()\n        self.logger.debug('\u54cd\u5e94body\u539f\u59cb\u6570\u636e\uff1a{}'.format(content_raw))\n        content = json.loads(content_raw, encoding='UTF-8')\n        self.logger.debug(content)\n\n        # \u6587\u7ae0\u53d1\u5e03\u65e5\u671f\n        date = datetime.datetime.strptime(content['date'], '%Y%m%d')\n\n        strftime = date.strftime(\"%Y-%m-%d\")\n        self.logger.info('\u65e5\u671f\uff1a{}'.format(strftime))\n\n        # \u5904\u7406\u5934\u6761\u6587\u7ae0\u5217\u8868\uff0c\u5c06\u5176 `top` \u6807\u8bb0\u5230\u76f8\u5e94 __story__ \u4e2d\n        if 'top_stories' in content:\n            self.logger.info('\u5904\u7406\u5934\u6761\u6587\u7ae0')\n            for item in content['top_stories']:\n                for story in content['stories']:\n                    if item['id'] == story['id']:\n                        story['top'] = 1\n                        break\n                self.logger.debug(item)\n\n        # \u5904\u7406\u4eca\u65e5\u6587\u7ae0\uff0c\u5e76\u629b\u51fa\u5177\u4f53\u6587\u7ae0\u8bf7\u6c42\n        post_num = len(content['stories'])\n        self.logger.info('\u5904\u7406\u4eca\u65e5\u6587\u7ae0\uff0c\u5171{:>2}\u7bc7'.format(post_num))\n        for item in content['stories']:\n            self.logger.info(item)\n            post_num = 0 if post_num < 0 else post_num\n            pub_time = date + datetime.timedelta(minutes=post_num)\n            post_num -= 1\n\n            url = 'http://news-at.zhihu.com/api/4/news/{}'.format(item['id'])\n            request = scrapy.Request(url, callback=self.parse_post)\n            post_dict = {\n                'spider': ZhihuDailySpider.name,\n                'date': pub_time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n                'meta': {\n                    'spider.zhihu_daily.id': str(item.get('id', ''))\n                }\n            }\n            if item.get('top'):\n                post_dict['meta']['spider.zhihu_daily.top'] = \\\n                    str(item.get('top', 0))\n            request.meta['post'] = post_dict\n            self.item_list.append(post_dict)\n            yield request"}
{"prompt": "def iter_steps(g, steps):\n    \"\"\"\n    iterate over 'g' in blocks with a length of the given 'step' count.\n\n    >>> for v in iter_steps([1,2,3,4,5], steps=2): v\n    [1, 2]\n    [3, 4]\n    [5]\n    >>> for v in iter_steps([1,2,3,4,5,6,7,8,9], steps=3): v\n    [1, 2, 3]\n    [4, 5, 6]\n    [7, 8, 9]\n\n                                 12345678        12345678\n                                         12345678\n    >>> bits = [int(i) for i in \"0101010101010101111000\"]\n    >>> for v in iter_steps(bits, steps=8): v\n    [0, 1, 0, 1, 0, 1, 0, 1]\n    [0, 1, 0, 1, 0, 1, 0, 1]\n    [1, 1, 1, 0, 0, 0]\n    \"\"\"", "output": " ^\n        ^\n    [1, 2, 3, 4, 5, 6, 7, 8, 9]\n    \"\"\"\n    for i in xrange(0, len(g), steps):\n        yield g[i:i+steps]\n\ndef iter_steps_from_list(l, steps):\n    \"\"\"\n    iterate over 'l' in blocks of'steps' (a list of blocks)\n\n    >>> for v in iter_steps_from_list([1,2,3,4,5], steps=2): v\n    [1, 2, 3, 4, 5]\n    >>> for v", "solution": "\n    values = []\n    for value in g:\n        values.append(value)\n        if len(values) == steps:\n            yield list(values)\n            values = []\n    if values:\n        yield list(values)"}
{"prompt": "def wait_key(keys=None):\n    \"\"\" Waits for a keypress at the console and returns it.\n        \"Where's the any key?\"\n\n        Arguments:\n            keys - if passed, wait for this specific key, e.g. ESC.\n                   may be a tuple.\n        Returns:\n            char or ESC - depending on key hit.\n            None - immediately under i/o redirection, not an interactive tty.\n    \"\"\"", "output": "\n    if keys is None:\n        keys = ()\n    for key in keys:\n        if key is None:\n            return None\n        if isinstance(key, str):\n            if key == \"esc\":\n                return True\n            elif key == \"tab\":\n                return False\n        if isinstance(key, tuple):\n            if len(key) == 2:\n                char = key[0]\n                if char == \" \":\n                    return None\n                elif char == \"\\t\":\n                    return char\n                elif char == \"\\n\":\n                    return None\n                else:\n                    return None\n        if key is None:\n            return None\n        if isinstance(key, str", "solution": "\n    if is_a_tty():\n        if keys:\n            if not isinstance(keys, tuple):\n                keys = (keys,)\n            while True:\n                key = _getch()\n                if key in keys:\n                    return key\n        else:\n            return _getch()"}
{"prompt": "def _WriteStorageMetadata(self):\n    \"\"\"Writes the storage metadata.\"\"\"", "output": "\n    self._WriteMetadata(self._storage_writer, self._metadata)\n\n  def _WriteMetadata(self, writer, metadata):\n    \"\"\"Writes the storage metadata.\"\"\"\n    if metadata is None:\n      return\n    writer.Write(\"  %s: %s\\n\" % (metadata.filename, metadata.value))\n\n  def _GetMetadata(self, filename, _with_private=False):\n    \"\"\"Reads a file containing metadata for a given filename.\"\"\"\n    metadata = rdf_client.StatEntry(\n        pathspec=rdf_paths.PathSpec(\n            path=\"/\".join(filename.split(\"/\")[:-", "solution": "\n    self._cursor.execute(self._CREATE_METADATA_TABLE_QUERY)\n\n    query = 'INSERT INTO metadata (key, value) VALUES (?, ?)'\n\n    key = 'format_version'\n    value = '{0:d}'.format(self._FORMAT_VERSION)\n    self._cursor.execute(query, (key, value))\n\n    key = 'compression_format'\n    value = self.compression_format\n    self._cursor.execute(query, (key, value))\n\n    key = 'serialization_format'\n    value = self.serialization_format\n    self._cursor.execute(query, (key, value))\n\n    key = 'storage_type'\n    value = self.storage_type\n    self._cursor.execute(query, (key, value))"}
{"prompt": "def childgroup(self, field):\n        \"\"\"\n        Return a list of fields stored by row regarding the configured grid\n\n        :param field: The original field this widget is attached to\n        \"\"\"", "output": "\n        return self.childgroups[field]\n\n    def _get_field_options(self, field):\n        \"\"\"\n        Return a list of options for the given field\n\n        :param field: The original field this widget is attached to\n        \"\"\"\n        return self.childoptions[field]\n\n    def _get_field_display(self, field):\n        \"\"\"\n        Return a display string for the given field\n\n        :param field: The original field this widget is attached to\n        \"\"\"\n        return self.childdisplay[field]\n\n    def _get_field_icon(self, field):\n        \"\"\"\n        Return a QIcon for the", "solution": "\n        grid = getattr(self, \"grid\", None)\n        named_grid = getattr(self, \"named_grid\", None)\n\n        if grid is not None:\n            childgroup = self._childgroup(field.children, grid)\n        elif named_grid is not None:\n            childgroup = self._childgroup_by_name(field.children, named_grid)\n        else:\n            raise AttributeError(u\"Missing the grid or named_grid argument\")\n\n        return childgroup"}
{"prompt": "def permission_set(self, name, func=None):\n        \"\"\"Define a new permission set (directly, or as a decorator).\n\n        E.g.::\n\n            @authz.permission_set('HTTP')\n            def is_http_perm(perm):\n                return perm.startswith('http.')\n\n        \"\"\"", "output": "\n        if func is None:\n            func = self.permission_get\n        self.permissions[name] = func\n\n    def permission_get(self, name):\n        \"\"\"Return the permission for the given name.\n\n        :param name: The name of the permission.\n        :type name: str\n\n        :returns: The permission for the given name.\n        :rtype: str\n\n        :raises ValueError: If the given name is not a valid permission.\n        :raises TypeError: If the given name is not a callable.\n        :raises ValueError: If the given name is not a valid permission.\n        :raises ValueError: If the given", "solution": "\n        if func is None:\n            return functools.partial(self.predicate, name)\n        self.permission_sets[name] = func\n        return func"}
{"prompt": "def evaluate(self, node: InstanceNode) -> XPathValue:\n        \"\"\"Evaluate the receiver and return the result.\n\n        Args:\n            node: Context node for XPath evaluation.\n\n        Raises:\n            XPathTypeError: If a subexpression of the receiver is of a wrong\n                type.\n        \"\"\"", "output": "\n        if isinstance(node, XPathValue):\n            return node.evaluate(self)\n        elif isinstance(node, XPathValue):\n            return self.evaluate(node.context)\n        else:\n            raise XPathTypeError(node)\n\n    def __str__(self) -> str:\n        \"\"\"Get the XPath value as a string.\n\n        Returns:\n            str: The XPath value.\n\n        Raises:\n            XPathTypeError: If a subexpression of the receiver is of a wrong\n                type.\n        \"\"\"\n        if isinstance(self.subexpression, str):\n            return self.subexpression\n        elif isinstance(self.", "solution": "\n        return self._eval(XPathContext(node, node, 1, 1))"}
{"prompt": "def get_time_slice(time, z, zdot=None, timeStart=None, timeEnd=None):\n    \"\"\"\n    Get slice of time, z and (if provided) zdot from timeStart to timeEnd.\n\n    Parameters\n    ----------\n    time : ndarray\n        array of time values \n    z : ndarray\n        array of z values\n    zdot : ndarray, optional\n        array of zdot (velocity) values.\n    timeStart : float, optional\n        time at which to start the slice.\n        Defaults to beginnging of time trace\n    timeEnd : float, optional\n        time at which to end the slide.\n        Defaults to end of time trace\n\n    Returns\n    -------\n    time_sliced : ndarray\n        array of time values from timeStart to timeEnd\n    z_sliced : ndarray\n        array of z values from timeStart to timeEnd\n    zdot_sliced : ndarray\n        array of zdot values from timeStart to timeEnd.\n        None if zdot not provided\n\n    \"\"\"", "output": ".\n        Defaults to end of time trace.\n\n    Returns\n    -------\n    timeSlice : ndarray\n        array of time values, if provided, the slice of z and zdot from timeStart to timeEnd.\n    \"\"\"\n    if zdot is None:\n        if timeStart is None:\n            timeStart = 0.0\n        if timeEnd is None:\n            timeEnd = 0.0\n        zdot = np.asarray(zdot)\n        if timeStart is not None:\n            timeStart = float(timeStart)\n        if timeEnd is not None:\n            timeEnd = float(timeEnd)\n        if timeEnd is", "solution": "\n    if timeStart == None:\n        timeStart = time[0]\n    if timeEnd == None:\n        timeEnd = time[-1]\n\n    StartIndex = _np.where(time == take_closest(time, timeStart))[0][0]\n    EndIndex = _np.where(time == take_closest(time, timeEnd))[0][0]\n\n    time_sliced = time[StartIndex:EndIndex]\n    z_sliced = z[StartIndex:EndIndex]\n\n    if zdot != None:\n        zdot_sliced = zdot[StartIndex:EndIndex]\n    else:\n        zdot_sliced = None    \n    \n    return time_sliced, z_sliced, zdot_sliced"}
{"prompt": "def howPlotAsk(goodFormat):\n    \"\"\"plots using inquirer prompts\n\n    Arguments:\n        goodFormat {dict} -- module : [results for module]\n    \"\"\"", "output": "\n    import matplotlib.pyplot as plt\n    import numpy as np\n\n    plt.figure(1)\n    plt.plot(goodFormat['results']['inquirer'], goodFormat['results']['inquirer_prompt'], 'r', label='inquirer')\n    plt.plot(goodFormat['results']['inquirer'], goodFormat['results']['inquirer_prompt_prompt'], 'r', label='inquirer_prompt')\n    plt.plot(goodFormat['results']['inquirer'], goodFormat['results']['inquirer_prompt_prompt'], 'r', label='inquirer_prompt", "solution": "\n    plotAnswer = askPlot()\n    if \"Save\" in plotAnswer['plotQ']:\n        exportPlotsPath = pathlib.Path(askSave())\n        if \"Show\" in plotAnswer['plotQ']:\n            plotter(exportPlotsPath, True, goodFormat)\n        else:\n            plotter(exportPlotsPath, False, goodFormat)\n    elif \"Show\" in plotAnswer['plotQ']:\n        plotter(None, True, goodFormat)"}
{"prompt": "async def message_data(request):\n    \"\"\"\n    As an example of aiohttp providing a non-html response, we load the actual messages for the \"messages\" view above\n    via ajax using this endpoint to get data. see static/message_display.js for details of rendering.\n    \"\"\"", "output": "\n    return web.Response(text=json.dumps(request.app['messages']),\n                        status=200,\n                        headers={\n                            'Content-Type': 'application/json',\n                            'X-Requested-With': 'XMLHttpRequest',\n                            'X-CSRF-Token': request.app['csrf_token'],\n                        },\n                        status=200)\n\n\n@routes.api_route('/message/display.js')\nasync def message_display_js(request):\n    \"\"\"\n    As an example of aiohttp providing a non-html response, we load the actual messages for the \"messages\" view above\n    via ajax", "solution": "\n    messages = []\n    # {% if database.is_none %}\n    if request.app['settings'].MESSAGE_FILE.exists():\n        # read the message file, process it and populate the \"messages\" list\n        with request.app['settings'].MESSAGE_FILE.open() as msg_file:\n            for line in msg_file:\n                if not line:\n                    # ignore blank lines eg. end of file\n                    continue\n                # split the line into it constituent parts, see process_form above\n                username, ts, message = line.split('|', 2)\n                # parse the datetime string and render it in a more readable format.\n                ts = '{:%Y-%m-%d %H:%M:%S}'.format(datetime.strptime(ts, '%Y-%m-%dT%H:%M:%S.%f'))\n                messages.append({'username': username, 'timestamp': ts, 'message': message})\n        messages.reverse()\n    # {% elif database.is_pg_sqlalchemy %}\n\n    async with request.app['pg_engine'].acquire() as conn:\n        async for row in conn.execute(sa_messages.select().order_by(sa_messages.c.timestamp.desc())):\n            ts = '{:%Y-%m-%d %H:%M:%S}'.format(row.timestamp)\n            messages.append({'username': row.username, 'timestamp': ts, 'message': row.message})\n    # {% endif %}\n    return json_response(messages)"}
{"prompt": "def _extract_gcs_api_response_error(message):\n  \"\"\" A helper function to extract user-friendly error messages from service exceptions.\n\n  Args:\n    message: An error message from an exception. If this is from our HTTP client code, it\n        will actually be a tuple.\n\n  Returns:\n    A modified version of the message that is less cryptic.\n  \"\"\"", "output": "\n  if not isinstance(message, tuple):\n    return (message, None)\n\n  if len(message) > 1:\n    return (message, None)\n\n  return (message, None)\n\n\ndef _extract_gcs_api_error_details(exc):\n  \"\"\" A helper function to extract user-friendly details from service exceptions.\n\n  Args:\n    exc: An error message from an exception. If this is from our HTTP client code, it\n        will be a tuple.\n\n  Returns:\n    A modified version of the message that is less cryptic.\n  \"\"\"\n  if not isinstance(exc, tuple):", "solution": "\n  try:\n    if len(message) == 3:\n      # Try treat the last part as JSON\n      data = json.loads(message[2])\n      return data['error']['errors'][0]['message']\n  except Exception:\n    pass\n  return message"}
{"prompt": "def _run_happy_eval(vrn_file, rm_file, rm_interval_file, base_dir, data):\n    \"\"\"Validation with hap.py: https://github.com/Illumina/hap.py\n\n    XXX Does not yet parse out metrics for plotting.\n    \"\"\"", "output": "\n    # TODO: Add support for other metrics.\n    # TODO: Add support for other metrics.\n    # TODO: Add support for other metrics.\n    # TODO: Add support for other metrics.\n    # TODO: Add support for other metrics.\n    # TODO: Add support for other metrics.\n    # TODO: Add support for other metrics.\n    # TODO: Add support for other metrics.\n    # TODO: Add support for other metrics.\n    # TODO: Add support for other metrics.\n    # TODO: Add support for other metrics.\n    # TODO: Add support for other metrics.\n    # TODO: Add support for other", "solution": "\n    out_dir = utils.safe_makedir(os.path.join(base_dir, \"happy\"))\n    out_prefix = os.path.join(out_dir, \"val\")\n    if not utils.file_exists(out_prefix + \".summary.csv\"):\n        vrn_file, rm_file, interval_bed = _prepare_inputs(vrn_file, rm_file, rm_interval_file, base_dir, data)\n        cmd = [\"hap.py\", \"-V\", \"-f\", interval_bed, \"-r\", dd.get_ref_file(data),\n               \"-l\", \",\".join(_get_location_list(interval_bed)),\n               \"-o\", out_prefix, rm_file, vrn_file]\n        do.run(cmd, \"Validate calls using hap.py\", data)\n    return {\"vcf\": out_prefix + \".vcf.gz\"}"}
{"prompt": "def start(self):\n        \"\"\"\n        Start rule poller and target poller once X-Ray daemon address\n        and context manager is in place.\n        \"\"\"", "output": "\n        self.log.info(\"Starting X-Ray daemon\")\n        self.log.info(\"  - Waiting for X-Ray daemon to start\")\n        self.log.info(\"  - Waiting for X-Ray daemon to start\")\n        self.log.info(\"  - Waiting for X-Ray daemon to start\")\n        self.log.info(\"  - Waiting for X-Ray daemon to start\")\n        self.log.info(\"  - Waiting for X-Ray daemon to start\")\n        self.log.info(\"  - Waiting for X-Ray daemon to start\")\n       ", "solution": "\n        if not global_sdk_config.sdk_enabled():\n            return\n\n        with self._lock:\n            if not self._started:\n                self._rule_poller.start()\n                self._target_poller.start()\n                self._started = True"}
{"prompt": "def weld_unique(array, weld_type):\n    \"\"\"Return the unique elements of the array.\n\n    Parameters\n    ----------\n    array : numpy.ndarray or WeldObject\n        Input array.\n    weld_type : WeldType\n        Type of each element in the input array.\n\n    Returns\n    -------\n    WeldObject\n        Representation of this computation.\n\n    \"\"\"", "output": "\n    if weld_type == WeldType.PSEUDO_ARRAY:\n        return array\n    elif weld_type == WeldType.PSEUDO_ARRAY_LIST:\n        return WeldObject.list_weld_arrays(array)\n    elif weld_type == WeldType.PSEUDO_ARRAY_LIST_LIST:\n        return WeldObject.list_weld_arrays_list(array)\n    elif weld_type == WeldType.PSEUDO_ARRAY_LIST_LIST:\n        return WeldObject.list_weld_", "solution": "\n    obj_id, weld_obj = create_weld_object(array)\n\n    weld_template = "}
{"prompt": "def _ProcessZipFileWithPlugins(self, parser_mediator, zip_file):\n    \"\"\"Processes a zip file using all compound zip files.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      zip_file (zipfile.ZipFile): the zip file. It should not be closed in\n          this method, but will be closed in ParseFileObject().\n    \"\"\"", "output": "\n    for plugin in self._plugins:\n      plugin.ProcessZipFile(zip_file, parser_mediator)\n\n  def ParseFileObject(self):\n    \"\"\"Parses a file-like object.\n\n    Returns:\n      bool: True if the file-like object was parsed.\n    \"\"\"\n    try:\n      self._ProcessZipFileWithPlugins(self._parser_mediator, self._zip_file_object)\n    except (IOError, OSError) as exception:\n      logging.error(exception)\n      return False\n\n    return True\n\n  def GetFileObject(self):\n    \"\"\"Retrieves a file-like object.\n\n    Returns:\n     ", "solution": "\n    archive_members = zip_file.namelist()\n    for plugin in self._plugins:\n      try:\n        plugin.UpdateChainAndProcess(\n            parser_mediator, zip_file=zip_file, archive_members=archive_members)\n      except errors.WrongCompoundZIPPlugin as exception:\n        logger.debug('[{0:s}] wrong plugin: {1!s}'.format(\n            self.NAME, exception))"}
{"prompt": "def load_from_array(self, keys, data_array):\n        \"\"\"\n        This loads the data contained in an array into the catalogue object\n\n        :param keys:\n            A list of keys explaining the content of the columns in the array\n        :type list:\n        \"\"\"", "output": "\n        for key in keys:\n            if key not in data_array:\n                raise KeyError(\"Key %s not found in catalogue\" % key)\n            if key not in self.columns:\n                raise KeyError(\"Column %s not found in catalogue\" % key)\n            if key not in self.columns_dict:\n                raise KeyError(\"Column %s not found in catalogue\" % key)\n            if key not in self.columns_dict_ordered:\n                self.columns_dict_ordered.append(key)\n            if key not in self.columns_dict_ordered_reversed:\n                self.columns", "solution": "\n\n        if len(keys) != np.shape(data_array)[1]:\n            raise ValueError('Key list does not match shape of array!')\n\n        for i, key in enumerate(keys):\n            if key in self.INT_ATTRIBUTE_LIST:\n                self.data[key] = data_array[:, i].astype(int)\n            else:\n                self.data[key] = data_array[:, i]\n            if key not in self.TOTAL_ATTRIBUTE_LIST:\n                print('Key %s not a recognised catalogue attribute' % key)\n\n        self.update_end_year()"}
{"prompt": "def _on_progress(adapter, operation, conn_id, done, total):\n    \"\"\"Callback when progress is reported.\"\"\"", "output": "\n    if done == 0:\n        log.info('Processed %d rows.', len(self.rows))\n    elif total == 0:\n        log.info('No rows found.')\n    else:\n        log.info('Processed %d rows (%d%%).', done, 100 * done / total)\n\n\nclass _BatchedSerializer(object):\n    \"\"\"Batched serializer for use with the old mongoengine-backed search API.\"\"\"\n\n    def __init__(self, queryset, batch_size):\n        self.queryset = queryset\n        self.batch_size = batch_size\n\n    def __len__(self):", "solution": "\n\n    conn_string = adapter._get_property(conn_id, 'connection_string')\n    if conn_string is None:\n        return\n\n    adapter.notify_progress(conn_string, operation, done, total)"}
{"prompt": "def read_lsm_timestamps(fh):\n    \"\"\"Read LSM time stamps from file and return as list.\"\"\"", "output": "\n    lsm_timestamps = []\n    for line in fh:\n        if line.startswith('Time: '):\n            lsm_timestamps.append(line[len('Time: '):line.find(' ')])\n    return lsm_timestamps\n\ndef read_lsm_times(fh):\n    \"\"\"Read LSM time stamps from file and return as list.\"\"\"\n    lsm_times = []\n    for line in fh:\n        if line.startswith('Time: '):\n            lsm_times.append(line[len('Time: '):line.find(' ')])\n    return lsm_times\n\ndef", "solution": "\n    size, count = struct.unpack('<ii', fh.read(8))\n    if size != (8 + 8 * count):\n        log.warning('read_lsm_timestamps: invalid LSM TimeStamps block')\n        return []\n    # return struct.unpack('<%dd' % count, fh.read(8*count))\n    return fh.read_array('<f8', count=count)"}
{"prompt": "def get_validated_types(object_types: Set[Type], set_name: str) -> Set[Type]:\n    \"\"\"\n    Utility to validate a set of types :\n    * None is not allowed as a whole or within the set,\n    * object and Any are converted into AnyObject\n    * if AnyObject is in the set, it must be the only element\n\n    :param object_types: the set of types to validate\n    :param set_name: a name used in exceptions if any\n    :return: the fixed set of types\n    \"\"\"", "output": "\n    if not object_types:\n        return None\n    if not isinstance(object_types, set):\n        raise TypeError(f\"object_types must be a set, got {type(object_types)}\")\n    return object_types - set_name\n\n\ndef get_validated_types_for_type(\n    type_name: str, object_types: AnyObject, set_name: str\n) -> Set[Type]:\n    \"\"\"\n    Utility to validate a set of types :\n    * AnyObject is not allowed as a whole or within the set,\n    * object and Any are converted into AnyObject", "solution": "\n    check_var(object_types, var_types=set, var_name=set_name)\n    res = {get_validated_type(typ, set_name + '[x]') for typ in object_types}\n    if AnyObject in res and len(res) > 1:\n        raise ValueError('The set of types contains \\'object\\'/\\'Any\\'/\\'AnyObject\\', so no other type must be present '\n                         'in the set')\n    else:\n        return res"}
{"prompt": "def from_key_bytes(cls, algorithm, key_bytes):\n        \"\"\"Creates a `Verifier` object based on the supplied algorithm and raw verification key.\n\n        :param algorithm: Algorithm on which to base verifier\n        :type algorithm: aws_encryption_sdk.identifiers.Algorithm\n        :param bytes encoded_point: Raw verification key\n        :returns: Instance of Verifier generated from encoded point\n        :rtype: aws_encryption_sdk.internal.crypto.Verifier\n        \"\"\"", "output": "\n        return Verifier(\n            algorithm=algorithm,\n            key_bytes=key_bytes,\n            key_info=cls._parse_key_info(key_bytes),\n            key_info_digest=cls._parse_key_digest(key_bytes),\n            key_info_verify=cls._parse_key_verify(key_bytes),\n            key_info_encrypt=cls._parse_key_encrypt(key_bytes),\n            key_info_decrypt=cls._parse_key_decrypt(key_bytes),\n            key_info_encrypt_digest=cls._parse_key_digest", "solution": "\n        return cls(\n            algorithm=algorithm, key=serialization.load_der_public_key(data=key_bytes, backend=default_backend())\n        )"}
{"prompt": "def _get_oauth_token(self):\n        \"\"\"\n        Get Monzo access token via OAuth2 `authorization code` grant type.\n\n        Official docs:\n            https://monzo.com/docs/#acquire-an-access-token\n\n        :returns: OAuth 2 access token\n        :rtype: dict\n        \"\"\"", "output": "\n        return {\n            'access_token': self.access_token,\n            'expires_in': self.expires_in,\n           'scope': self.scope,\n            'token_type': 'Bearer',\n            'expires_in_seconds': self.expires_in_seconds,\n           'refresh_token': self.refresh_token,\n           'scope_type': 'Bearer',\n           'refresh_token_seconds': self.refresh_token_seconds,\n            'client_id': self.client_id,\n            'client_secret': self.client_secret,\n            'code': self", "solution": "\n        url = urljoin(self.api_url, '/oauth2/token')\n\n        oauth = OAuth2Session(\n            client_id=self._client_id,\n            redirect_uri=config.REDIRECT_URI,\n        )\n\n        token = oauth.fetch_token(\n            token_url=url,\n            code=self._auth_code,\n            client_secret=self._client_secret,\n        )\n\n        return token"}
{"prompt": "def xml_decode(string):\n    \"\"\" Returns the string with special characters decoded.\n    \"\"\"", "output": "\n    if not string:\n        return \"\"\n    if not string.startswith(\"'\") and not string.endswith(\"'\"):\n        return string\n    return string[1:-1]\n\ndef xml_encode(string):\n    \"\"\" Returns the string with special characters encoded.\n    \"\"\"\n    if not string:\n        return \"\"\n    if not string.startswith('\"') and not string.endswith(\"'\"):\n        return string\n    return string[1:-1].encode('utf-8')\n\ndef xml_escape(string):\n    \"\"\" Returns the string with special characters escaped.\n    \"\"\"\n    if not string:\n        return \"\"\n    if not string.", "solution": "\n    string = string.replace(\"&amp;\", \"&\")\n    string = string.replace(\"&lt;\",  \"<\")\n    string = string.replace(\"&gt;\",  \">\")\n    string = string.replace(\"&quot;\",\"\\\"\")\n    string = string.replace(\"/\", SLASH)\n    return string"}
{"prompt": "def convert_python_regex_to_ecma(value, flags=[]):\n    \"\"\"Convert Python regex to ECMA 262 regex.\n\n    If given value is already ECMA regex it will be returned unchanged.\n\n    :param string value: Python regex.\n    :param list flags: List of flags (allowed flags: `re.I`, `re.M`)\n    :return: ECMA 262 regex\n    :rtype: str\n\n    \"\"\"", "output": "\n    if isinstance(value, str):\n        return re.compile(value, flags)\n    else:\n        return value\n\n\ndef convert_to_bytes(value, flags=[]):\n    \"\"\"Convert value to bytes.\n\n    :param bytes value: Value to be converted.\n    :param list flags: List of flags (allowed flags: `re.I`, `re.M`)\n    :return: Bytes value.\n    :rtype: int\n\n    \"\"\"\n    if isinstance(value, int):\n        return value\n    else:\n        return int.from_bytes(value, flags)\n\n\ndef convert_to_bits(value", "solution": "\n    if is_ecma_regex(value):\n        return value\n\n    result_flags = [PYTHON_TO_ECMA_FLAGS[f] for f in flags]\n    result_flags = ''.join(result_flags)\n\n    return '/{value}/{flags}'.format(value=value, flags=result_flags)"}
{"prompt": "def _independent_lattice(self, shape, lattice=None):\n        \"\"\" Helper to construct the list of nodes and edges. \"\"\"", "output": "\n        if lattice is None:\n            lattice = self.lattice\n        return [self.node(i) for i in shape]\n\n    def _get_lattice_nodes(self, lattice):\n        \"\"\"\n        Returns the list of nodes in the lattice.\n        \"\"\"\n        if lattice.is_periodic:\n            return [self.node(i) for i in range(self.n_nodes)]\n        else:\n            return [self.node(i) for i in lattice.get_nodes()]\n\n    def _get_lattice_edges(self, lattice):\n        \"\"\"\n        Returns the list of edges in the lattice.\n       ", "solution": "\n        I, J = shape\n\n        if lattice is not None:\n            end_I = min(I, max(lattice[..., 3])) - 1\n            end_J = min(J, max(lattice[..., 4])) - 1\n            unvisited_nodes = deque([(i, j, s)\n                                     for i in range(end_I)\n                                     for j in range(end_J)\n                                     for s in self._start_states])\n            lattice = lattice.tolist()\n        else:\n            lattice = []\n            unvisited_nodes = deque([(0, 0, s) for s in self._start_states])\n        lattice += _grow_independent_lattice(self._transitions, \n                                             self.n_states, (I, J), \n                                             unvisited_nodes)\n        lattice = np.array(sorted(lattice), dtype='int64')\n        return lattice"}
{"prompt": "def _textOutput(self, gaObjects):\n        \"\"\"\n        Prints out the specified Variant objects in a VCF-like form.\n        \"\"\"", "output": "\n        print(\"## INFO: vcf2pileup.py: textOutput() called\\n\")\n        for gaObject in gaObjects:\n            print(\"## %s\\n\" % gaObject.getName())\n            _textOutput(gaObject.getVariantList())\n\n    def _getVcfHeader(self, gaObjects):\n        \"\"\"\n        Prints out the VCF header for the specified Variant object.\n        \"\"\"\n        print(\"## INFO: vcf2pileup.py: getVcfHeader() called\\n\")\n        for gaObject in gaObjects:\n            print(\"## %s\\", "solution": "\n        for variantAnnotation in gaObjects:\n            print(\n                variantAnnotation.id, variantAnnotation.variant_id,\n                variantAnnotation.variant_annotation_set_id,\n                variantAnnotation.created, sep=\"\\t\", end=\"\\t\")\n            for effect in variantAnnotation.transcript_effects:\n                print(effect.alternate_bases, sep=\"|\", end=\"|\")\n                for so in effect.effects:\n                    print(so.term, sep=\"&\", end=\"|\")\n                    print(so.term_id, sep=\"&\", end=\"|\")\n                print(effect.hgvs_annotation.transcript,\n                      effect.hgvs_annotation.protein, sep=\"|\", end=\"\\t\")\n            print()"}
{"prompt": "def security_group_rule_delete(auth=None, **kwargs):\n    \"\"\"\n    Delete a security group\n\n    name_or_id\n        The unique ID of the security group rule\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' neutronng.security_group_rule_delete name_or_id=1dcac318a83b4610b7a7f7ba01465548\n\n    \"\"\"", "output": "\n    if auth is None:\n        auth = current_app.config['DEFAULT_AUTH']\n    if auth is None:\n        return 'ERROR: Must provide a valid auth'\n    if 'name_or_id' in kwargs:\n        name_or_id = kwargs['name_or_id']\n    else:\n        name_or_id = current_app.config['DEFAULT_TENANT_NAME']\n    if name_or_id in ('1', '2', '3'):\n        return 'ERROR: Cannot delete a security group with ID 1 or 2'\n    if 'name_or_id'", "solution": "\n    cloud = get_operator_cloud(auth)\n    kwargs = _clean_kwargs(**kwargs)\n    return cloud.delete_security_group_rule(**kwargs)"}
{"prompt": "def _ParseInformationalOptions(self, options):\n    \"\"\"Parses the informational options.\n\n    Args:\n      options (argparse.Namespace): command line arguments.\n    \"\"\"", "output": "\n    self._Parse(options.infoal_options, 'infoal_options')\n    self._Parse(options.infoal_args, 'infoal_args')\n    self._Parse(options.infoal_env_options, 'infoal_env_options')\n    self._Parse(options.infoal_env_args, 'infoal_env_args')\n    self._Parse(options.infoal_log_options, 'infoal_log_options')\n    self._Parse(options.infoal_log_env_options, 'infoal_log_env_options')", "solution": "\n    self._debug_mode = getattr(options, 'debug', False)\n    self._quiet_mode = getattr(options, 'quiet', False)\n\n    if self._debug_mode and self._quiet_mode:\n      logger.warning(\n          'Cannot use debug and quiet mode at the same time, defaulting to '\n          'debug output.')"}
{"prompt": "def add(self, document):\n        \"\"\"\n        Add a document to the database.\n        \"\"\"", "output": "\n        self.db.add(document)\n        self.db.commit()\n        return self.db.get(document.id)\n\n    def get_document(self, document_id):\n        \"\"\"\n        Get a document by its id.\n        \"\"\"\n        return self.documents.get(document_id)\n\n    def get_document_by_id(self, document_id):\n        \"\"\"\n        Get a document by its id.\n        \"\"\"\n        return self.documents.get(document_id)\n\n    def get_documents_by_id(self, document_ids):\n        \"\"\"\n        Get all documents.\n       ", "solution": "\n        docid = int(document.uniqueIdentifier())\n        text = u' '.join(document.textParts())\n\n        self.store.executeSQL(self.addSQL, (docid, text))"}
{"prompt": "def generate_version(\n    current_version: str, increment: str, prerelease: Optional[str] = None\n) -> Version:\n    \"\"\"Based on the given increment a proper semver will be generated.\n\n    For now the rules and versioning scheme is based on\n    python's PEP 0440.\n    More info: https://www.python.org/dev/peps/pep-0440/\n\n    Example:\n        PATCH 1.0.0 -> 1.0.1\n        MINOR 1.0.0 -> 1.1.0\n        MAJOR 1.0.0 -> 2.0.0\n    \"\"\"", "output": ".0\n        PATCH 1.0.0 -> 2.0.1\n        MINOR 1.1.0 -> 2.1.0\n        MAJOR 2.0.0 -> 2.0.1\n        PATCH 1.0.0 -> 2.0.1\n        MINOR 2.1.0 -> 2.0.0\n        MAJOR 2.1.0 -> 2.0.0\n    \"\"\"\n    if prerelease:\n        current_version = prerelease + \" \" + increment\n    return Version(current_version)\n\n\ndef get_version_from_changelog(changelog: str)", "solution": "\n    pre_version = prerelease_generator(current_version, prerelease=prerelease)\n    semver = semver_generator(current_version, increment=increment)\n    # TODO: post version\n    # TODO: dev version\n    return Version(f\"{semver}{pre_version}\")"}
{"prompt": "def vec(self):\n        \"\"\":obj:`numpy.ndarray` : Vector representation for this camera.\n        \"\"\"", "output": "\n        return self._vector\n\n    @vec.setter\n    def vector(self, value):\n        \"\"\":obj:`vec.Set` :obj:`value` :obj:`vec.Vec` representation for this camera.\n        \"\"\"\n        self._vector = value\n\n    @property\n    def name(self):\n        \"\"\":obj:`str` : The name of this camera.\n        \"\"\"\n        return self._name\n\n    @name.setter\n    def name(self, value):\n        \"\"\":obj:`str.Set` :obj:`value` :obj:`name` representation for this camera.\n        \"\"\"\n        self._name = value\n\n   ", "solution": "\n        return np.r_[self.fx, self.fy, self.cx, self.cy, self.skew, self.height, self.width]"}
{"prompt": "def AddFareObject(self, fare, problem_reporter=None):\n    \"\"\"Deprecated. Please use AddFareAttributeObject.\"\"\"", "output": "\n    return self.AddFareAttributeObject(fare, 'fare_object', problem_reporter)\n\n  def AddFareObjectAttribute(self, fare, name, value, problem_reporter=None):\n    \"\"\"Deprecated. Please use AddFareAttributeObjectAttribute.\"\"\"\n    return self.AddFareAttributeObjectAttributeObject(\n        self.AddFareAttributeAttribute(fare, name, value, 'fare_object'),\n        problem_reporter)\n\n  def AddFareAttributeAttribute(self, fare, name, value, problem_reporter=None):\n    \"\"\"Deprecated. Please use Add", "solution": "\n    warnings.warn(\"No longer supported. The Fare class was renamed to \"\n                  \"FareAttribute, and all related functions were renamed \"\n                  \"accordingly.\", DeprecationWarning)\n    self.AddFareAttributeObject(fare, problem_reporter)"}
{"prompt": "def get(self, path, default=None):\n        \"\"\"\n        Returns given path value.\n\n        :param path: Path name.\n        :type path: unicode\n        :param default: Default value if path is not found.\n        :type default: object\n        :return: Action.\n        :rtype: QAction\n        \"\"\"", "output": "\n        return QAction(self.tr('Select'), self.tr('Select a layer'), self)\n\n    def setData(self, path, value, default=None):\n        \"\"\"\n        Sets given path value.\n\n        :param path: Path name.\n        :type path: unicode\n        :param value: Value.\n        :type value: unicode\n        :param default: Default value if path is not found.\n        :type default: object\n        :return: self.\n        :rtype: QAction\n        \"\"\"\n        return QAction(self.tr('Set data'), self.tr('Set data'), self)\n\n    def add", "solution": "\n\n        try:\n            return self.__getitem__(path)\n        except KeyError as error:\n            return default"}
{"prompt": "def pullup(self, pin, enabled):\n        \"\"\"Turn on the pull-up resistor for the specified pin if enabled is True,\n        otherwise turn off the pull-up resistor.\n        \"\"\"", "output": "\n        if enabled:\n            self.write_register(pin, 0)\n        else:\n            self.write_register(pin, 1)\n\n    def pulldown(self, pin, enabled):\n        \"\"\"Turn on the pull-down resistor for the specified pin if enabled is True,\n        otherwise turn off the pull-down resistor.\n        \"\"\"\n        if enabled:\n            self.write_register(pin, 0)\n        else:\n            self.write_register(pin, -1)\n\n    def set_pin_state(self, pin, state):\n        \"\"\"Set the pin state for the specified pin.\n       ", "solution": "\n        self._validate_channel(pin)\n        if enabled:\n            self.gppu[int(pin/8)] |= 1 << (int(pin%8))\n        else:\n            self.gppu[int(pin/8)] &= ~(1 << (int(pin%8)))\n        self._write_gppu()"}
{"prompt": "def get_url_reports(self, resources):\n        \"\"\"Retrieves a scan report on a given URL.\n\n        Args:\n            resources: list of URLs.\n        Returns:\n            A dict with the URL as key and the VT report as value.\n        \"\"\"", "output": "\n        report = {}\n        for resource in resources:\n            if resource.url == self.url:\n                report[resource.id] = resource.report\n            else:\n                report[resource.id] = resource.report_url\n        return report\n\n    def get_url_reports_by_id(self, report_id):\n        \"\"\"Retrieves a scan report on a given URL.\n\n        Args:\n            report_id: URL id.\n        Returns:\n            A dict with the URL as key and the VT report as value.\n        \"\"\"\n        return get_url_reports(self.url_reports, report_id", "solution": "\n        api_name = 'virustotal-url-reports'\n\n        (all_responses, resources) = self._bulk_cache_lookup(api_name, resources)\n        resource_chunks = self._prepare_resource_chunks(resources, '\\n')\n        response_chunks = self._request_reports(\"resource\", resource_chunks, 'url/report')\n        self._extract_response_chunks(all_responses, response_chunks, api_name)\n\n        return all_responses"}
{"prompt": "def read_caffemodel(prototxt_fname, caffemodel_fname):\n    \"\"\"Return a caffe_pb2.NetParameter object that defined in a binary\n    caffemodel file\n    \"\"\"", "output": "\n    with open(caffemodel_fname, 'rb') as f:\n        caffe_pb2.NetParameter.ParseFromString(f.read())\n    return caffe_pb2.NetParameter.from_proto(caffemodel[\"proto\"])\n\n\ndef read_caffemodel_from_file(prototxt_fname, caffemodel_fname):\n    \"\"\"Return a caffe_pb2.NetParameter object that defined in a binary\n    caffemodel file\n    \"\"\"\n    with open(caffemodel_fname, 'rb') as f:\n        caffe_pb2.NetParameter.ParseFromString(f", "solution": "\n    if use_caffe:\n        caffe.set_mode_cpu()\n        net = caffe.Net(prototxt_fname, caffemodel_fname, caffe.TEST)\n        layer_names = net._layer_names\n        layers = net.layers\n        return (layers, layer_names)\n    else:\n        proto = caffe_pb2.NetParameter()\n        with open(caffemodel_fname, 'rb') as f:\n            proto.ParseFromString(f.read())\n        return (get_layers(proto), None)"}
{"prompt": "def fileToMD5(filename, block_size=256*128, binary=False):\n    \"\"\"\n    A function that calculates the MD5 hash of a file.\n\n    Args:\n    -----\n        filename: Path to the file.\n        block_size: Chunks of suitable size. Block size directly depends on\n            the block size of your filesystem to avoid performances issues.\n            Blocks of 4096 octets (Default NTFS).\n        binary: A boolean representing whether the returned info is in binary\n            format or not.\n\n    Returns:\n    --------\n        string: The  MD5 hash of the file.\n    \"\"\"", "output": "\n    if binary:\n        return hashlib.md5(filename).hexdigest()\n    else:\n        return hashlib.md5(filename.encode('utf-8')).hexdigest()\n\ndef get_file_md5(filename, block_size=256*128, binary=False):\n    \"\"\"\n    A function that returns the MD5 hash of a file.\n\n    Args:\n    -----\n        filename: Path to the file.\n        block_size: Chunks of suitable size. Block size directly depends on\n            the block size of your filesystem to avoid performances issues.\n            Blocks of 4096 octets (Default NTFS).", "solution": "\n    md5 = hashlib.md5()\n    with open(filename,'rb') as f:\n        for chunk in iter(lambda: f.read(block_size), b''):\n             md5.update(chunk)\n    if not binary:\n        return md5.hexdigest()\n    return md5.digest()"}
{"prompt": "def _request(self, action='GET', url='/', data=None, query_params=None):\n        \"\"\"\n        Requests to Hetzner by current session and returns the response.\n        \"\"\"", "output": "\n        if data is None:\n            data = {}\n        if query_params is None:\n            query_params = {}\n        if self.session.cookies:\n            for c in self.session.cookies:\n                if c.name == 'csrftoken':\n                    query_params['csrfmiddlewaretoken'] = c.value\n                elif c.name =='sessionid':\n                    query_params['sessionid'] = c.value\n                elif c.name == 'url':\n                    url = c.value\n                else:\n                    raise ValueError(\"Unknown cookie: %s\" % c.name)\n\n        if self.session.cookies:", "solution": "\n        if data is None:\n            data = {}\n        if query_params is None:\n            query_params = {}\n        response = self.session.request(action, self.api[self.account]['endpoint'] + url,\n                                        params=query_params, data=data)\n        response.raise_for_status()\n        return response"}
{"prompt": "def get_networks(base_file, ipaddr_file):\n    \"\"\" Gather network and host information from ipplan export files. \"\"\"", "output": "\n    # TODO: Add support for multiple networks\n    # TODO: Add support for multiple hosts\n    # TODO: Add support for multiple hosts\n    # TODO: Add support for multiple hosts\n    # TODO: Add support for multiple hosts\n    # TODO: Add support for multiple hosts\n    # TODO: Add support for multiple hosts\n    # TODO: Add support for multiple hosts\n    # TODO: Add support for multiple hosts\n    # TODO: Add support for multiple hosts\n    # TODO: Add support for multiple hosts\n    # TODO: Add support for multiple hosts\n    # TODO: Add support for multiple hosts\n    # TODO: Add support for multiple hosts\n    #", "solution": "\n    networks = []\n\n    base = open(base_file, 'r')\n\n    csv_reader = csv.reader(base, delimiter='\\t')\n\n    buffer = \"\"\n    for row in csv_reader:\n\n        # Fixes quotation bug in ipplan exporter for base.txt\n        if len(networks) > 0 and len(buffer) > 0:\n            networks[-1]['comment'] += \" \".join(buffer)\n            buffer = \"\"\n        if len(row) < 3:\n            buffer = row\n        else:\n\n            network = {\n                'network': ipaddress.ip_network(\"{}/{}\".format(row[0], row[2])),\n                'description': row[1],\n                'hosts': [],\n                'comment': \"\"\n            }\n\n            if len(row) > 3:\n                network['additional'] = \" \".join(row[3:])\n\n            networks.append(network)\n\n    base.close()\n\n    ipaddr = open(ipaddr_file, 'r')\n\n    csv_reader = csv.reader(ipaddr, delimiter='\\t')\n    for row in csv_reader:\n\n        host = {\n            'ipaddr': ipaddress.ip_address(row[0]),\n            'user': row[1],\n            'location': row[2],\n            'description': row[3],\n            'fqdn': row[4],\n            'phone': row[5],\n            'mac': row[6]\n        }\n\n        if len(row) > 7:\n            host['additional'] = \" \".join(row[7:])\n\n        add_ip_to_net(networks, host)\n\n    ipaddr.close()\n\n    return networks"}
{"prompt": "def bdd(*keywords):\n    \"\"\"\n    Run tests matching keywords.\n    \"\"\"", "output": "\n    global _BDD_TESTS\n    for kw in keywords:\n        _BDD_TESTS.append(bdd_test(kw))\n    return _BDD_TESTS\n\ndef bdd_test(name):\n    \"\"\"\n    BDD tests are a test suite that is run in a specific order.\n    \"\"\"\n    test = globals().get('BDD_%s' % name)\n    if test is None:\n        test = _BDD_TEST\n    return test(test(test(keywords[0])) or test(keywords[1])\n\ndef bdd_keywords():\n    \"\"\"\n    Run bdd", "solution": "\n    settings = _personal_settings().data\n    _storybook().with_params(\n        **{\"python version\": settings[\"params\"][\"python version\"]}\n    ).only_uninherited().shortcut(*keywords).play()"}
{"prompt": "async def get_me(self, input_peer=False):\n        \"\"\"\n        Gets \"me\" (the self user) which is currently authenticated,\n        or None if the request fails (hence, not authenticated).\n\n        Args:\n            input_peer (`bool`, optional):\n                Whether to return the :tl:`InputPeerUser` version or the normal\n                :tl:`User`. This can be useful if you just need to know the ID\n                of yourself.\n\n        Returns:\n            Your own :tl:`User`.\n        \"\"\"", "output": "\n        return None\n\n    @abc.abstractmethod\n    def get_me_by_id(self, input_peer_id):\n        \"\"\"\n        Gets the self user object from your account.\n\n        Args:\n            input_peer_id (`int`, optional):\n                The :tl:`InputPeerID` to return. This can be useful if you just\n                need to know the ID of yourself.\n\n        Returns:\n            :class:`InputPeerUser`:\n                The :tl:`InputPeerUser` object.\n        \"\"\"\n        return None\n\n    @abc.abstractmethod\n    async def get_me_by_name(self, input_name", "solution": "\n        if input_peer and self._self_input_peer:\n            return self._self_input_peer\n\n        try:\n            me = (await self(\n                functions.users.GetUsersRequest([types.InputUserSelf()])))[0]\n\n            self._bot = me.bot\n            if not self._self_input_peer:\n                self._self_input_peer = utils.get_input_peer(\n                    me, allow_self=False\n                )\n\n            return self._self_input_peer if input_peer else me\n        except errors.UnauthorizedError:\n            return None"}
{"prompt": "def parse_libxc_docs(path):\n    \"\"\"\n    Parse libxc_docs.txt file, return dictionary with mapping:\n    libxc_id --> info_dict\n    \"\"\"", "output": "\n    doc_dict = {}\n    with open(path, 'r') as f:\n        for line in f:\n            if line.startswith('libxc'):\n                doc_id, info_dict = line.strip().split(' ', 2)\n                doc_dict[doc_id] = {}\n                doc_dict[doc_id]['id'] = int(doc_id)\n                doc_dict[doc_id]['name'] = info_dict.strip()\n                doc_dict[doc_id]['id'] = int(doc_id)\n                doc_dict[doc_id]['version'] =", "solution": "\n    def parse_section(section):\n\td = {}\n\tfor l in section:\n\t    key, value = l.split(\":\")\n\t    key = key.strip()\n\t    d[key] = value.strip()\n\n\treturn int(d[\"Number\"]), d\n\n    d = OrderedDict()\n    with open(path, \"rt\") as fh:\n\tsection = []\n\tfor line in fh:\n\t    if not line.startswith(\"-\"):\n\t\tsection.append(line)\n\t    else:\n\t\tnum, entry = parse_section(section)\n\t\tassert num not in d\n\t\td[num] = entry\n\t\tsection = []\n\tassert not section\n\n    return d"}
{"prompt": "def _create_application_version_request(app_metadata, application_id, template):\n    \"\"\"\n    Construct the request body to create application version.\n\n    :param app_metadata: Object containing app metadata\n    :type app_metadata: ApplicationMetadata\n    :param application_id: The Amazon Resource Name (ARN) of the application\n    :type application_id: str\n    :param template: A packaged YAML or JSON SAM template\n    :type template: str\n    :return: SAR CreateApplicationVersion request body\n    :rtype: dict\n    \"\"\"", "output": "\n    request_body = {\n        'ApplicationName': application_id,\n        'TemplateName': template,\n        'VersionLabel': app_metadata.version_label,\n        'VersionId': app_metadata.version_id,\n        'IsDefaultVersion': True if app_metadata.is_default_version else False\n    }\n    if is_s3_enabled(app_metadata):\n        request_body['VersionId'] = app_metadata.version_id\n    if is_aws_enabled(app_metadata):\n        request_body['IsDefaultVersion'] = True if app_metadata.is_", "solution": "\n    app_metadata.validate(['semantic_version'])\n    request = {\n        'ApplicationId': application_id,\n        'SemanticVersion': app_metadata.semantic_version,\n        'SourceCodeUrl': app_metadata.source_code_url,\n        'TemplateBody': template\n    }\n    return {k: v for k, v in request.items() if v}"}
{"prompt": "def crypt(word, salt=None, rounds=_ROUNDS_DEFAULT):\n    \"\"\"Return a string representing the one-way hash of a password, with a salt\n    prepended.\n    If ``salt`` is not specified or is ``None``, the strongest\n    available method will be selected and a salt generated.  Otherwise,\n    ``salt`` may be one of the ``crypt.METHOD_*`` values, or a string as\n    returned by ``crypt.mksalt()``.\n    \"\"\"", "output": "\n    if salt is None:\n        salt = get_random_bytes(8)\n    return b2a_hex(sha1(word + salt).digest())\n\n\ndef gen_password(length=20, chars=string.letters + string.digits):\n    \"\"\"Generate a random password with the specified ``length`` and\n    ``chars`` characters.\"\"\"\n    return ''.join(random.choice(chars) for _ in range(length))\n\n\ndef gen_salt(length=20, chars=string.letters + string.digits):\n    \"\"\"Generate a random salt with the specified ``length`` and\n    ``", "solution": "\n    if salt is None or isinstance(salt, _Method):\n        salt = mksalt(salt, rounds)\n\n    algo, rounds, salt = extract_components_from_salt(salt)\n    if algo == 5:\n        hashfunc = hashlib.sha256\n    elif algo == 6:\n        hashfunc = hashlib.sha512\n    else:\n        raise ValueError('Unsupported algorithm, must be either 5 (sha256) or 6 (sha512)')\n\n    return sha2_crypt(word, salt, hashfunc, rounds)"}
{"prompt": "def generate_phonetic_representation(self, word):\n        \"\"\"\n        Returns a generated phonetic representation for a word.\n\n        :param str word: a word to be phoneticized.\n        :return: A list of phonemes representing the phoneticized word.\n\n        This method is used for words for which there is no pronunication\n        entry in the CMU dictionary. The function generates a\n        pronunication for the word in the standard CMU format. This can then\n        be converted to a compact phonetic representation using\n        modify_phonetic_representation().\n\n        \"\"\"", "output": "\n        if word in self.CMU_PHASES:\n            return self.CMU_PHASES[word]\n        else:\n            return []\n\n    def modify_phonetic_representation(self, phonetic_representation, word):\n        \"\"\"\n        Modifies a phonetic representation for a word.\n\n        :param str phonetic_representation: a phonememe representation\n            of the word.\n\n        This method is used for words for which there is no pronunication\n        entry in the CMU dictionary. The function generates a\n        compact representation for the phonememe in the standard CMU\n        format. This can", "solution": "\n        with NamedTemporaryFile() as temp_file:\n            # Write the word to a temp file\n            temp_file.write(word)\n            #todo - clean up this messy t2p path\n            t2pargs = [os.path.abspath(os.path.join(os.path.dirname(__file__),'t2p/t2p')),\n                       '-transcribe', os.path.join(data_path, 'cmudict.0.7a.tree'),\n                       temp_file.name]\n            temp_file.seek(0)\n            output, error = subprocess.Popen(\n                t2pargs, stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE\n            ).communicate()\n            output = output.split()\n            phonetic_representation = output[1:]\n\n        return phonetic_representation"}
{"prompt": "def _evaluate(self,R,z,phi=0.,t=0.):\n        \"\"\"\n        NAME:\n           _evaluate\n        PURPOSE:\n           evaluate the potential at R,z\n        INPUT:\n           R - Galactocentric cylindrical radius\n           z - vertical height\n           phi - azimuth\n           t - time\n        OUTPUT:\n           Phi(R,z)\n        HISTORY:\n           2013-09-08 - Written - Bovy (IAS)\n        \"\"\"", "output": "\n        R = np.atleast_1d(R)\n        z = np.atleast_1d(z)\n        phi = np.atleast_1d(phi)\n        t = np.atleast_1d(t)\n        if phi.shape!= (self.n_phi,self.n_azimuth):\n            raise ValueError(\"phi must be a square matrix\")\n        if phi.shape[0]!= self.n_time:\n            raise ValueError(\"time must be a scalar\")\n        if phi.shape[1]!= self.n_phi:\n            raise ValueError(\"number of points must be", "solution": "\n        r2= R**2.+z**2.\n        rb= nu.sqrt(r2+self.b2)\n        return -1./(self.b+rb)"}
{"prompt": "def category_helper(form_tag=True):\n    \"\"\"\n    Category's form layout helper\n    \"\"\"", "output": "\n    if form_tag:\n        return 'category_form_layout.html'\n    else:\n        return 'category_form.html'\n\n\n@register.simple_tag\ndef category_form_layout(parser, token):\n    \"\"\"\n    Category's form layout helper\n    \"\"\"\n    if token.contents == 'category_form_layout.html':\n        return 'category_form_layout.html'\n    else:\n        return 'category_form.html'\n\n\n@register.simple_tag\ndef category_form_layout_js(parser, token):\n    \"\"\"\n    Category's form layout helper\n   ", "solution": "\n    helper = FormHelper()\n    helper.form_action = '.'\n    helper.attrs = {'data_abide': ''}\n    helper.form_tag = form_tag\n    \n    helper.layout = Layout(\n        Row(\n            Column(\n                'title',\n                css_class='small-12'\n            ),\n        ),\n        Row(\n            Column(\n                'slug',\n                css_class='small-12 medium-10'\n            ),\n            Column(\n                'order',\n                css_class='small-12 medium-2'\n            ),\n        ),\n        Row(\n            Column(\n                'description',\n                css_class='small-12'\n            ),\n        ),\n        Row(\n            Column(\n                'visible',\n                css_class='small-12'\n            ),\n        ),\n        ButtonHolderPanel(\n            Submit('submit', _('Submit')),\n            css_class='text-right',\n        ),\n    )\n    \n    return helper"}
{"prompt": "def load(self, modules):\n        \"\"\"Load Python modules and check their usability\n\n        :param modules: list of the modules that must be loaded\n        :return:\n        \"\"\"", "output": "\n        for module in modules:\n            self.check_module(module)\n\n    def check_module(self, module):\n        \"\"\"Check if the module is a valid module\n\n        :param module: module name\n        :return:\n        \"\"\"\n        if module not in self.modules:\n            raise ValueError(\"Module %s not found\" % module)\n\n    def load_all(self):\n        \"\"\"Load all modules\n\n        :return:\n        \"\"\"\n        for module in self.modules:\n            self.load_module(module)\n\n    def load_all_modules(self):\n        \"\"\"Load all modules\n\n        :return:\n        \"\"\"\n       ", "solution": "\n        self.modules_assoc = []\n        for module in modules:\n            if not module.enabled:\n                logger.info(\"Module %s is declared but not enabled\", module.name)\n                # Store in our modules list but do not try to load\n                # Probably someone else will load this module later...\n                self.modules[module.uuid] = module\n                continue\n            logger.info(\"Importing Python module '%s' for %s...\", module.python_name, module.name)\n            try:\n                python_module = importlib.import_module(module.python_name)\n\n                # Check existing module properties\n                # Todo: check all mandatory properties\n                if not hasattr(python_module, 'properties'):  # pragma: no cover\n                    self.configuration_errors.append(\"Module %s is missing a 'properties' \"\n                                                     \"dictionary\" % module.python_name)\n                    raise AttributeError\n                logger.info(\"Module properties: %s\", getattr(python_module, 'properties'))\n\n                # Check existing module get_instance method\n                if not hasattr(python_module, 'get_instance') or \\\n                        not isinstance(getattr(python_module, 'get_instance'),\n                                       collections.Callable):  # pragma: no cover\n                    self.configuration_errors.append(\"Module %s is missing a 'get_instance' \"\n                                                     \"function\" % module.python_name)\n                    raise AttributeError\n\n                self.modules_assoc.append((module, python_module))\n                logger.info(\"Imported '%s' for %s\", module.python_name, module.name)\n            except ImportError as exp:  # pragma: no cover, simple protection\n                self.configuration_errors.append(\"Module %s (%s) can't be loaded, Python \"\n                                                 \"importation error: %s\" % (module.python_name,\n                                                                            module.name,\n                                                                            str(exp)))\n            except AttributeError:  # pragma: no cover, simple protection\n                self.configuration_errors.append(\"Module %s (%s) can't be loaded, \"\n                                                 \"module configuration\" % (module.python_name,\n                                                                           module.name))\n            else:\n                logger.info(\"Loaded Python module '%s' (%s)\", module.python_name, module.name)"}
{"prompt": "def __software_to_pkg_id(self, publisher, name, is_component, is_32bit):\n        \"\"\"\n        Determine the Package ID of a software/component using the\n        software/component ``publisher``, ``name``, whether its a software or a\n        component, and if its 32bit or 64bit archiecture.\n\n        Args:\n            publisher (str): Publisher of the software/component.\n            name (str): Name of the software.\n            is_component (bool): True if package is a component.\n            is_32bit (bool): True if the software/component is 32bit architecture.\n\n        Returns:\n            str: Package Id\n        \"\"\"", "output": ".\n\n        Returns:\n            int: Package ID of the specified software/component.\n\n        Raises:\n            ValueError: If the specified package is not a valid software/component.\n\n        \"\"\"\n        if not is_component:\n            raise ValueError(\"Invalid argument 'is_component'.\")\n\n        if not self.is_valid_package_name(name):\n            raise ValueError(\"Invalid argument 'name'.\")\n\n        if not self.is_valid_package_publisher(publisher):\n            raise ValueError(\"Invalid argument 'publisher'.\")\n\n        if not self.is_valid_package_is_32bit_component(is_32bit):\n            raise ValueError", "solution": "\n        if publisher:\n            # remove , and lowercase as , are used as list separators\n            pub_lc = publisher.replace(',', '').lower()\n\n        else:\n            # remove , and lowercase\n            pub_lc = 'NoValue'  # Capitals/Special Value\n\n        if name:\n            name_lc = name.replace(',', '').lower()\n            # remove ,   OR we do the URL Encode on chars we do not want e.g. \\\\ and ,\n        else:\n            name_lc = 'NoValue'  # Capitals/Special Value\n\n        if is_component:\n            soft_type = 'comp'\n        else:\n            soft_type = 'soft'\n\n        if is_32bit:\n            soft_type += '32'  # Tag only the 32bit only\n\n        default_pkg_id = pub_lc+'\\\\\\\\'+name_lc+'\\\\\\\\'+soft_type\n\n        # Check to see if class was initialise with pkg_obj with a method called\n        # to_pkg_id, and if so use it for the naming standard instead of the default\n        if self.__pkg_obj and hasattr(self.__pkg_obj, 'to_pkg_id'):\n            pkg_id = self.__pkg_obj.to_pkg_id(publisher, name, is_component, is_32bit)\n            if pkg_id:\n                return pkg_id\n\n        return default_pkg_id"}
{"prompt": "def to_netcdf(data, filename, *, group=\"posterior\", coords=None, dims=None):\n    \"\"\"Save dataset as a netcdf file.\n\n    WARNING: Only idempotent in case `data` is InferenceData\n\n    Parameters\n    ----------\n    data : InferenceData, or any object accepted by `convert_to_inference_data`\n        Object to be saved\n    filename : str\n        name or path of the file to load trace\n    group : str (optional)\n        In case `data` is not InferenceData, this is the group it will be saved to\n    coords : dict (optional)\n        See `convert_to_inference_data`\n    dims : dict (optional)\n        See `convert_to_inference_data`\n\n    Returns\n    -------\n    str\n        filename saved to\n    \"\"\"", "output": " `convert_to_inference_data` for more information.\n    dims : dict (optional)\n        See `convert_to_inference_data` for more information.\n\n    \"\"\"\n    if isinstance(data, InferenceData):\n        data = data.copy()\n    if isinstance(data, str):\n        data = to_netcdf_string(data)\n    if isinstance(group, str):\n        group = to_netcdf(group)\n    if isinstance(coords, dict):\n        coords = coords.copy()\n    if isinstance(dims, str):\n        dims = to_netcdf_string(dims)\n   ", "solution": "\n    inference_data = convert_to_inference_data(data, group=group, coords=coords, dims=dims)\n    file_name = inference_data.to_netcdf(filename)\n    return file_name"}
{"prompt": "def main():\n    \"\"\"Create and use a logger.\"\"\"", "output": "\n    # Create a logger with a higher log level\n    logger = logging.getLogger('py3canvas')\n    logger.setLevel(logging.DEBUG)\n\n    # Create a new window\n    win = gtk.Window()\n\n    # Create a new canvas\n    canvas = gtk.DrawingArea()\n    canvas.connect(\"draw\", lambda *args: win.draw_rectangle(0, 0, 400, 400, fill=\"red\"))\n\n    # Create a new canvas widget\n    win.connect(\"destroy\", lambda *args: canvas.destroy())\n\n    # Create a new canvas widget\n    win.add(canvas)\n\n    # Create a new", "solution": "\n    logger = setup_logger()\n\n    logger.debug('a debug message')\n    logger.info('an info message')\n    logger.warning('a warning message')\n    logger.error('an error message')\n    logger.critical('a critical message')"}
{"prompt": "def _node_to_dict(cls, node, json, json_fields):\n        \"\"\" Helper method for ``get_tree``.\n        \"\"\"", "output": "\n        return {\n            'id': node._id,\n            'title': node.title,\n            'category': node.category,\n            'is_registration': node.is_registration,\n            'is_collection': node.is_collection,\n            'contributors': [\n                contributor._id\n                for contributor in node.contributors\n                if contributor.is_registered\n            ],\n            'is_fork': True,\n            'is_registration_of': node.is_registration_of(json_fields),\n            'is_forked_from': node.is_forked_from,\n            'is_registration_", "solution": "\n        if json:\n            pk_name = node.get_pk_name()\n            # jqTree or jsTree format\n            result = {'id': getattr(node, pk_name), 'label': node.__repr__()}\n            if json_fields:\n                result.update(json_fields(node))\n        else:\n            result = {'node': node}\n        return result"}
{"prompt": "async def route_wrapper(self, route, request, context, request_args,\n                            request_kw, *decorator_args, with_context=None,\n                            **decorator_kw):\n        \"\"\"This is the function that is called when a route is decorated with\n           your plugin decorator. Context will normally be None, but the user\n           can pass use_context=True so the route will get the plugin\n           context\n        \"\"\"", "output": "\n        if with_context is None:\n            with_context = False\n        if decorator_args is None:\n            decorator_args = []\n        if decorator_kw is None:\n            decorator_kw = {}\n        if with_context:\n            return await self.plugin_manager.async_call(\n                'GET', '/plugin/{0}/'.format(route.name),\n                request,\n                context,\n                request_args,\n                request_kw,\n                *decorator_args,\n                with_context=with_context,\n                **decorator_kw)\n        else:\n            return await self.plugin_manager.async_", "solution": "\n        # by default, do nothing, just run the wrapped function\n        if with_context:\n            resp = route(request, context, *request_args, **request_kw)\n        else:\n            resp = route(request, *request_args, **request_kw)\n        if isawaitable(resp):\n            resp = await resp\n        return resp"}
{"prompt": "def convert(model, input_features, output_features):\n    \"\"\"Convert a boosted tree model to protobuf format.\n\n    Parameters\n    ----------\n    decision_tree : GradientBoostingRegressor\n        A trained scikit-learn tree model.\n\n    input_feature: [str]\n        Name of the input columns.\n\n    output_features: str\n        Name of the output column.\n\n    Returns\n    -------\n    model_spec: An object of type Model_pb.\n        Protobuf representation of the model\n    \"\"\"", "output": "\n    model_spec = Model_pb.ModelSpec(\n        name=model.name,\n        input_columns=model.input_columns,\n        output_columns=model.output_columns,\n        loss=model.loss,\n        train_op=model.train_op,\n        training_steps=len(input_features),\n        steps=len(output_features),\n        train_step_kwargs=dict(\n            learning_rate=LEARNING_RATE,\n            gradient_clipping_threshold=LEARNING_STEP_THRESHOLD,\n            gradient_clipping_window_size=1,\n           ", "solution": "\n    if not(_HAS_SKLEARN):\n        raise RuntimeError('scikit-learn not found. scikit-learn conversion API is disabled.')\n\n    _sklearn_util.check_expected_type(model, _ensemble.GradientBoostingRegressor)\n    def is_gbr_model(m):\n        if len(m.estimators_) == 0:\n            return False\n        if hasattr(m, 'estimators_') and m.estimators_ is not None:\n            for t in m.estimators_.flatten():\n                if not hasattr(t, 'tree_') or t.tree_ is None:\n                    return False\n            return True\n        else:\n            return False\n\n    _sklearn_util.check_fitted(model, is_gbr_model)\n\n    base_prediction = model.init_.mean\n\n    return _MLModel(_convert_tree_ensemble(model, input_features, output_features,\n            base_prediction = base_prediction))"}
{"prompt": "def from_pytime(cls, pytime):\n        \"\"\"\n        Converts Python time object to sql time object\n        ignoring timezone\n        @param pytime: Python time object\n        @return: sql time object\n        \"\"\"", "output": "\n        return pytime.replace(tzinfo=None)\n\n    @classmethod\n    def to_pytime(cls, pytime):\n        \"\"\"\n        Converts SQL time object to python time object\n        ignoring timezone\n        @param pytime: SQL time object\n        @return: pytime object\n        \"\"\"\n        return pytime.replace(tzinfo=None)\n\n    @classmethod\n    def to_sql(cls, sql_type):\n        \"\"\"\n        Converts a time object to a string\n        @param sql_type: time object type\n        @return: string\n        \"\"\"\n        return sql_type.replace(\":\", \"_\")\n\n    @classmethod\n    def to", "solution": "\n        secs = pytime.hour * 60 * 60 + pytime.minute * 60 + pytime.second\n        nsec = secs * 10 ** 9 + pytime.microsecond * 1000\n        return cls(nsec=nsec)"}
{"prompt": "def _get_mappedids(entry, graph):\n        \"\"\"\n        Extract the Orphanet and UMLS ids as equivalences from the entry\n        :param entry:\n        :return:\n        \"\"\"", "output": "\n        orphet_id = entry.get('id')\n        umls_id = entry.get('name')\n        if orphet_id:\n            return (orphet_id, umls_id)\n        else:\n            return None, None\n\n    def _get_orphet_id(self, graph, entry):\n        \"\"\"\n        Extract the Orphanet and UMLS ids as equivalences from the entry\n        :param graph:\n        :param entry:\n        :return:\n        \"\"\"\n        orphet_id = entry.get('id')\n        if orph", "solution": "\n        model = Model(graph)\n        omim_num = str(entry['mimNumber'])\n        omim_curie = 'OMIM:' + omim_num\n        orpha_mappings = []\n        if 'externalLinks' in entry:\n            links = entry['externalLinks']\n            if 'orphanetDiseases' in links:\n                # triple semi-colon delimited list of\n                # double semi-colon delimited orphanet ID/disease pairs\n                # 2970;;566;;Prune belly syndrome\n                items = links['orphanetDiseases'].strip().split(';;;')\n                for item in items:\n                    orphdis = item.strip().split(';;')\n                    orpha_num = orphdis[0].strip()\n                    orpha_label = orphdis[2].strip()\n                    orpha_curie = 'ORPHA:' + orpha_num\n                    orpha_mappings.append(orpha_curie)\n                    model.addClassToGraph(orpha_curie, orpha_label)\n                    model.addXref(omim_curie, orpha_curie)\n\n            if 'umlsIDs' in links:\n                umls_mappings = links['umlsIDs'].split(',')\n                for umls in umls_mappings:\n                    umls_curie = 'UMLS:' + umls\n                    model.addClassToGraph(umls_curie, None)\n                    model.addXref(omim_curie, umls_curie)"}
{"prompt": "def genes_with_homology_models(self):\n        \"\"\"DictList: All genes that have at least one homology model.\"\"\"", "output": "\n        return self._genes.values()\n\n    def get_homology_models(self, hom_id):\n        \"\"\"DictList: All genes that have at least one homology model.\"\"\"\n        return self._homology_models.get(hom_id, [])\n\n    def add_het_model(self, het_id, model):\n        \"\"\"DictList: All genes that have at least one het model.\"\"\"\n        self._hets.append(model)\n        self._genes[model.id].hom_id = het_id\n        self._genes", "solution": "\n        return DictList(x for x in self.genes_with_structures if x.protein.num_structures_homology > 0)"}
{"prompt": "def kill_session(self, target_session=None):\n        \"\"\"\n        Kill the tmux session with ``$ tmux kill-session``, return ``self``.\n\n        Parameters\n        ----------\n        target_session : str, optional\n            target_session: str. note this accepts ``fnmatch(3)``. 'asdf' will\n            kill 'asdfasd'.\n\n        Returns\n        -------\n        :class:`Server`\n\n        Raises\n        ------\n        :exc:`exc.BadSessionName`\n        \"\"\"", "output": "\n        if target_session is None:\n            target_session = self.get_target_session()\n        if target_session is None:\n            raise BadSessionName(\"No tmux session\")\n        if target_session is None:\n            raise BadSessionName(\"No tmux session\")\n        if target_session is None:\n            raise BadSessionName(\"No tmux session\")\n\n        if target_session == 'all':\n            return self.get_server_all()\n        elif target_session == 'kill':\n            return self.kill_server_all(target_session)\n        elif target_session == 'kill", "solution": "\n        session_check_name(target_session)\n\n        proc = self.cmd('kill-session', '-t%s' % target_session)\n\n        if proc.stderr:\n            raise exc.LibTmuxException(proc.stderr)\n\n        return self"}
{"prompt": "def iam_device_info(self, apdu):\n        \"\"\"Create a device information record based on the contents of an\n        IAmRequest and put it in the cache.\"\"\"", "output": "\n        return self.cache.add_iam_device_info(apdu)\n\n    def get_iam_device_info(self, apdu):\n        \"\"\"Get the device information record for the given IAmRequest.\"\"\"\n        return self.cache.get_iam_iam_device_info(apdu)\n\n    def get_iam_role_id(self, apdu):\n        \"\"\"Get the role ID for the given IAmRequest.\"\"\"\n        return self.cache.get_iam_role_id(apdu)\n\n    def get_iam_role_name(self, apdu):\n        \"\"\"Get", "solution": "\n        if _debug: DeviceInfoCache._debug(\"iam_device_info %r\", apdu)\n\n        # make sure the apdu is an I-Am\n        if not isinstance(apdu, IAmRequest):\n            raise ValueError(\"not an IAmRequest: %r\" % (apdu,))\n\n        # get the device instance\n        device_instance = apdu.iAmDeviceIdentifier[1]\n\n        # get the existing cache record if it exists\n        device_info = self.cache.get(device_instance, None)\n\n        # maybe there is a record for this address\n        if not device_info:\n            device_info = self.cache.get(apdu.pduSource, None)\n\n        # make a new one using the class provided\n        if not device_info:\n            device_info = self.device_info_class(device_instance, apdu.pduSource)\n\n        # jam in the correct values\n        device_info.deviceIdentifier = device_instance\n        device_info.address = apdu.pduSource\n        device_info.maxApduLengthAccepted = apdu.maxAPDULengthAccepted\n        device_info.segmentationSupported = apdu.segmentationSupported\n        device_info.vendorID = apdu.vendorID\n\n        # tell the cache this is an updated record\n        self.update_device_info(device_info)"}
{"prompt": "def visualize_tensors(name, imgs, scale_func=lambda x: (x + 1.) * 128., max_outputs=1):\n    \"\"\"Generate tensor for TensorBoard (casting, clipping)\n\n    Args:\n        name: name for visualization operation\n        *imgs: multiple tensors as list\n        scale_func: scale input tensors to fit range [0, 255]\n\n    Example:\n        visualize_tensors('viz1', [img1])\n        visualize_tensors('viz2', [img1, img2, img3], max_outputs=max(30, BATCH))\n    \"\"\"", "output": "\n\n    Args:\n        name: name for operation\n        *imgs: multiple tensors as list\n        *scale_func: scale input tensors to fit range [0, 255]\n\n    Returns:\n        TensorBoard object\n    \"\"\"\n    if len(imgs) == 1:\n        return imgs[0]\n    else:\n        return visualize_tensor(name, imgs, max_outputs)\n\n\ndef visualize_tensors_from_csv(name, csv_file, scale_func=lambda x: (x + 1.) * 128., max_outputs=1):\n    \"\"\"Generate tensor for TensorBoard (cast", "solution": "\n    xy = scale_func(tf.concat(imgs, axis=2))\n    xy = tf.cast(tf.clip_by_value(xy, 0, 255), tf.uint8, name='viz')\n    tf.summary.image(name, xy, max_outputs=30)"}
{"prompt": "def get_ip_mac_arp_list(auth, url, devid=None, devip=None):\n    \"\"\"\n    function takes devid of specific device and issues a RESTFUL call to get the IP/MAC/ARP list\n    from the target device.\n\n    :param auth: requests auth object #usually auth.creds from auth pyhpeimc.auth.class\n\n    :param url: base url of IMC RS interface #usually auth.url from pyhpeimc.auth.authclass\n\n    :param devid: int or str value of the target device.\n\n    :param devip: str of ipv4 address of the target device\n\n    :return: list of dictionaries containing the IP/MAC/ARP list of the target device.\n\n    :rtype: list\n\n    >>> from pyhpeimc.auth import *\n\n    >>> from pyhpeimc.plat.termaccess import *\n\n    >>> auth = IMCAuth(\"http://\", \"10.101.0.203\", \"8080\", \"admin\", \"admin\")\n\n    >>> ip_mac_list = get_ip_mac_arp_list( auth.creds, auth.url, devid='10')\n\n    >>> ip_mac_list = get_ip_mac_arp_list( auth.creds, auth.url, devip='10.101.0.221')\n\n    >>> assert type(ip_mac_list) is list\n\n    >>> assert 'deviceId' in ip_mac_list[0]\n\n    \"\"\"", "output": " devip: int or str value of the target device.\n    \"\"\"\n    url = url + \"/arp\"\n    if devid:\n        url = url + \"/dev/\" + str(devid) + \"/ipaddr\"\n    if devip:\n        url = url + \"/ip/\" + str(devip) + \"/arp\"\n    return get_request(url, auth=auth)\n\n\ndef get_ip_mac_address(auth, devid=None, address=None):\n    \"\"\"\n    function takes auth object and issues a RESTFUL call to get the IP/MAC address\n    from the target device", "solution": "\n    if devip is not None:\n        dev_details = get_dev_details(devip, auth, url)\n        if isinstance(dev_details, str):\n            print(\"Device not found\")\n            return 403\n        else:\n            devid = get_dev_details(devip, auth, url)['id']\n    f_url = url + \"/imcrs/res/access/ipMacArp/\" + str(devid)\n    response = requests.get(f_url, auth=auth, headers=HEADERS)\n    try:\n        if response.status_code == 200:\n            ipmacarplist = (json.loads(response.text))\n            if 'ipMacArp' in ipmacarplist:\n                return ipmacarplist['ipMacArp']\n            else:\n                return ['this function is unsupported']\n    except requests.exceptions.RequestException as error:\n        return \"Error:\\n\" + str(error) + \" get_ip_mac_arp_list: An Error has occured\""}
{"prompt": "def set_password_prompt(self, regex=None):\n        \"\"\"\n        Defines a pattern that is used to monitor the response of the\n        connected host for a password prompt.\n\n        :type  regex: RegEx\n        :param regex: The pattern that, when matched, causes an error.\n        \"\"\"", "output": "\n        self.password_prompt = regex\n\n    def set_ssh_options(self, **kwargs):\n        \"\"\"\n        Defines a set of ssh options that are passed to the underlying\n        :class:`SSHClient <plumbum.machines.remote.RemoteClient>` instance.\n\n        :type  kwargs: dict\n        :param kwargs: The set of ssh options that, when matched, causes an\n            error.\n        \"\"\"\n        self.ssh_options = kwargs\n\n    def set_ssh_key(self, **kwargs):\n        \"\"\"\n        Defines a key that can be used to connect to the remote host.\n\n        :type  kwargs", "solution": "\n        if regex is None:\n            self.manual_password_re = regex\n        else:\n            self.manual_password_re = to_regexs(regex)"}
{"prompt": "def get_context_object_name(self, object_list):\n        \"\"\"Get the name of the item to be used in the context.\n\n        See original in ``django.views.generic.list.MultipleObjectMixin``.\n        \"\"\"", "output": "\n        if len(object_list) == 1:\n            return object_list[0].name\n        return '%s-%s' % (object_list[0].name, object_list[0].pk)\n\n    def get_context_data(self, **kwargs):\n        context = super(ItemAdmin, self).get_context_data(**kwargs)\n        context['item'] = self.get_object()\n        return context\n\n    def get_urls(self):\n        \"\"\"Get the admin urls.\n\n        This is used to generate URLs for admins: use {% url 'admin:index' %}\n        to", "solution": "\n        if self.context_object_name:\n            return self.context_object_name\n        elif hasattr(object_list, 'model'):\n            object_name = object_list.model._meta.object_name.lower()\n            return smart_str('{0}_list'.format(object_name))\n        else:\n            return None"}
{"prompt": "def get_experiment_summ_ids( self, coinc_event_id ):\n\t\t\"\"\"\n\t\tGets all the experiment_summ_ids that map to a given coinc_event_id.\n\t\t\"\"\"", "output": "\n\t\treturn self.experiment_summ_ids( coinc_event_id )\n\n\tdef experiment_summ_ids( self, coinc_event_id ):\n\t\t\"\"\"\n\t\tGets all the experiment_summ_ids that map to a given coinc_event_id.\n\t\t\"\"\"\n\t\treturn [ coinc_event_id ]\n\n\tdef get_coinc_event_id( self, coinc_event_id ):\n\t\t\"\"\"\n\t\tGets the coinc_event_id that maps to a given coinc_event_id.\n\t\t\"\"\"\n\t", "solution": "\n\t\texperiment_summ_ids = []\n\t\tfor row in self:\n\t\t\tif row.coinc_event_id == coinc_event_id:\n\t\t\t\texperiment_summ_ids.append(row.experiment_summ_id)\n\t\tif len(experiment_summ_ids) == 0:\n\t\t\traise ValueError(\"'%s' could not be found in the experiment_map table\" % coinc_event_id)\n\t\treturn experiment_summ_ids"}
{"prompt": "def register(self, auth_body=None, kind=\"user\", bind_email=None,\n                 username=None, password=None, device_id=None,\n                 initial_device_display_name=None, inhibit_login=None):\n        \"\"\"Performs /register.\n\n        Args:\n            auth_body (dict): Authentication Params.\n            kind (str): Specify kind of account to register. Can be 'guest' or 'user'.\n            bind_email (bool): Whether to use email in registration and authentication.\n            username (str): The localpart of a Matrix ID.\n            password (str): The desired password of the account.\n            device_id (str): ID of the client device.\n            initial_device_display_name (str): Display name to be assigned.\n            inhibit_login (bool): Whether to login after registration. Defaults to false.\n        \"\"\"", "output": "str): The localpart of a Password.\n            device_id (str): The device ID.\n            initial_device_display_name (str): The initial display name.\n            inhibit_login (str): The inhibit login.\n        \"\"\"\n        self.auth_body = auth_body\n        self.kind = kind\n        self.bind_email = bind_email\n        self.username = username\n        self.password = password\n        self.account_id = device_id\n        self.username_locked = False\n        self.password_locked = False\n        self.device_id = initial_device_", "solution": "\n        content = {}\n        content[\"kind\"] = kind\n        if auth_body:\n            content[\"auth\"] = auth_body\n        if username:\n            content[\"username\"] = username\n        if password:\n            content[\"password\"] = password\n        if device_id:\n            content[\"device_id\"] = device_id\n        if initial_device_display_name:\n            content[\"initial_device_display_name\"] = \\\n                    initial_device_display_name\n        if bind_email:\n            content[\"bind_email\"] = bind_email\n        if inhibit_login:\n            content[\"inhibit_login\"] = inhibit_login\n        return self._send(\n            \"POST\",\n            \"/register\",\n            content=content,\n            query_params={'kind': kind}\n        )"}
{"prompt": "def contrast(self, color, step):\n        \"\"\"if color is dark, will return a lighter one, otherwise darker\"\"\"", "output": "\n        if step == 0:\n            return self.darker\n        else:\n            return self.lighter\n\n    def set_contrast(self, contrast):\n        \"\"\"if contrast is a float, will set the step to a float, otherwise float\"\"\"\n        if isinstance(contrast, float):\n            self.contrast = contrast\n        else:\n            self.contrast = contrast\n\n    def set_darker(self, darker):\n        \"\"\"if darker is a float, will set the step to a float, otherwise float\"\"\"\n        if isinstance(darker, float):\n            self.darker = darker\n        else", "solution": "\n        hls = colorsys.rgb_to_hls(*self.rgb(color))\n        if self.is_light(color):\n            return colorsys.hls_to_rgb(hls[0], hls[1] - step, hls[2])\n        else:\n            return colorsys.hls_to_rgb(hls[0], hls[1] + step, hls[2])"}
{"prompt": "def set(self, *components):\n        \"\"\" Set the possible components of the block\n        \n        :param components: components to append Optionables or Composables\n        \"\"\"", "output": "\n        if not components:\n            return\n        \n        if len(components) == 1:\n            self.add_option(Option(self, 'component', components[0]))\n        else:\n            self.add_option(Option(self, 'component',''.join(components)))\n\n    def get_option_by_name(self, name):\n        \"\"\" Get an Option instance by its name\n        \n        :param name: name of the option\n        \"\"\"\n        return self.options.get_by_name(name)\n\n    def get_option_by_id(self, id):\n        \"\"\" Get an Option instance by its id", "solution": "\n        self.reset()\n        if len(components) == 1:\n            self.append(components[0])\n        else:\n            for comp in components:\n                self.append(comp)"}
{"prompt": "def multidict(ordered_pairs):\n    \"\"\"Convert duplicate keys values to lists.\"\"\"", "output": "\n    return [dict(pair) for pair in ordered_pairs]\n\n\ndef get_unique_values(values):\n    \"\"\"Return unique values.\"\"\"\n    seen = set()\n    unique_values = set()\n    for value in values:\n        if value not in seen:\n            unique_values.add(value)\n            unique_values.add(seen.add(value))\n    return unique_values\n\n\ndef get_unique_values_and_order(values):\n    \"\"\"Return unique values and order.\"\"\"\n    return [multidict(pair) for pair in values] + [multidict(order) for order", "solution": "\n    # read all values into lists\n    d = defaultdict(list)\n    for k, v in ordered_pairs:\n        d[k].append(v)\n    # unpack lists that have only 1 item\n    dict_copy = deepcopy(d)\n    for k, v in iteritems(dict_copy):\n        if len(v) == 1:\n            d[k] = v[0]\n    return dict(d)"}
{"prompt": "def search_form(context, search_model_names=None):\n    \"\"\"\n    Includes the search form with a list of models to use as choices\n    for filtering the search by. Models should be a string with models\n    in the format ``app_label.model_name`` separated by spaces. The\n    string ``all`` can also be used, in which case the models defined\n    by the ``SEARCH_MODEL_CHOICES`` setting will be used.\n    \"\"\"", "output": "\n    search_model_names = SEARCH_MODEL_CHOICES if SEARCH_MODEL_CHOICES else []\n    return context['choices'] + [(k, v) for k, v in get_app_choices(context)] \\\n        + [(k, v) for k, v in get_search_model_choices(context, k))] \\\n        + search_model_names\n\n\ndef get_app_choices(context):\n    \"\"\"\n    Returns a list of tuples containing the name and model for\n    ``app_label.model`` if there are choices for the given model.\n    \"\"\"\n    app_labels", "solution": "\n    template_vars = {\n        \"request\": context[\"request\"],\n    }\n    if not search_model_names or not settings.SEARCH_MODEL_CHOICES:\n        search_model_names = []\n    elif search_model_names == \"all\":\n        search_model_names = list(settings.SEARCH_MODEL_CHOICES)\n    else:\n        search_model_names = search_model_names.split(\" \")\n    search_model_choices = []\n    for model_name in search_model_names:\n        try:\n            model = apps.get_model(*model_name.split(\".\", 1))\n        except LookupError:\n            pass\n        else:\n            verbose_name = model._meta.verbose_name_plural.capitalize()\n            search_model_choices.append((verbose_name, model_name))\n    template_vars[\"search_model_choices\"] = sorted(search_model_choices)\n    return template_vars"}
{"prompt": "def update(self, fields=None, **kwargs):\n        \"\"\"Update the current entity.\n\n        Make an HTTP PUT call to ``self.path('base')``. Return the response.\n\n        :param fields: An iterable of field names. Only the fields named in\n            this iterable will be updated. No fields are updated if an empty\n            iterable is passed in. All fields are updated if ``None`` is passed\n            in.\n        :return: A ``requests.response`` object.\n\n        \"\"\"", "output": "\n        path = self.path('update')\n        return self._put(path, fields, **kwargs)\n\n    def update_with_body(self, path, body, **kwargs):\n        \"\"\"Update the current entity with a body.\n\n        Make an HTTP PUT call to ``self.path('base')``. Return the response.\n\n        :param path: The path to the resource.\n        :param body: The body to update.\n        :param kwargs: The optional arguments for the ``body`` parameter.\n            See :py:meth:`update` for more details.\n        :return: A ``requests.response`` object", "solution": "\n        kwargs = kwargs.copy()  # shadow the passed-in kwargs\n        kwargs.update(self._server_config.get_client_kwargs())\n        # a content upload is always multipart\n        headers = kwargs.pop('headers', {})\n        headers['content-type'] = 'multipart/form-data'\n        kwargs['headers'] = headers\n        return client.put(\n            self.path('self'),\n            fields,\n            **kwargs\n        )"}
{"prompt": "def _get_elementwise_name_from_keras_layer(keras_layer):\n    \"\"\"\n    Get the keras layer name from the activation name.\n    \"\"\"", "output": "\n    if keras_layer is None:\n        return None\n    else:\n        return keras_layer.name\n\n\ndef _get_default_variable_store():\n    \"\"\"\n    Return the default VariableStore instance used by keras.\n    \"\"\"\n    if keras_defaults is None:\n        from tensorflow.python.keras.initializers import VariableStore\n        keras_defaults = VariableStore()\n    return keras_defaults\n\n\ndef _get_partitioner(shape, num_partitions, num_rows, num_cols, padding='valid'):\n    \"\"\"\n    Return a function to create a partitioner for the given shape.\n    \"\"\"\n   ", "solution": "\n    if isinstance(keras_layer, _keras.layers.Add):\n        return 'ADD'\n    elif isinstance(keras_layer, _keras.layers.Multiply):\n        return 'MULTIPLY'\n    elif isinstance(keras_layer, _keras.layers.Concatenate):\n        if len(keras_layer.input_shape[0]) == 3 and (keras_layer.axis == 1 or keras_layer.axis == -2):\n            return 'SEQUENCE_CONCAT'\n        elif len(keras_layer.input_shape[0]) == 4 and (keras_layer.axis == 3 or keras_layer.axis == -1):\n            return 'CONCAT'\n        elif len(keras_layer.input_shape[0]) == 2 and (keras_layer.axis == 1 or keras_layer.axis == -1):\n            return 'CONCAT'\n        else:\n            raise ValueError('Only channel and sequence concatenation are supported.')\n    elif isinstance(keras_layer, _keras.layers.Dot):\n        if len(keras_layer.input_shape[0]) == 2:\n            if type(keras_layer.axes) is list or type(keras_layer.axes) is tuple:\n                if len(keras_layer.axes) > 1:\n                    raise ValueError('Only vector dot-product is supported.')\n                else:\n                    axis = keras_layer.axes[0]\n            else:\n                axis = keras_layer.axes\n            if axis != -1 and axis != 1:\n                 raise ValueError('Only vector dot-product is supported.')\n\n            if keras_layer.normalize:\n                return 'COS'\n            else:\n                return 'DOT'\n        else:\n            raise ValueError('Only vector dot-product is supported.')\n    elif isinstance(keras_layer, _keras.layers.Maximum):\n        return 'MAX'\n    elif isinstance(keras_layer, _keras.layers.Average):\n        return 'AVE'\n    else:\n        _utils.raise_error_unsupported_option(str(type(keras_layer)), 'merge',\n                keras_layer.name)"}
{"prompt": "def is_match(subject: Expression, pattern: Expression) -> bool:\n    \"\"\"\n    Check whether the given *subject* matches given *pattern*.\n\n    Args:\n        subject:\n            The subject.\n        pattern:\n            The pattern.\n\n    Returns:\n        True iff the subject matches the pattern.\n    \"\"\"", "output": "\n    return (\n        subject.matches(pattern)\n        and subject.matches_pattern(pattern)\n        and subject.matches_pattern_str(pattern)\n    )\n\n\ndef is_match_str(subject: Expression, pattern: Expression) -> bool:\n    \"\"\"\n    Check whether the given *subject* matches given *pattern*.\n\n    Args:\n        subject:\n            The subject.\n        pattern:\n            The pattern.\n\n    Returns:\n        True iff the subject matches the pattern.\n    \"\"\"\n    return (\n        pattern.matches_pattern_str(subject)\n        and subject.matches_pattern_str_str(subject)", "solution": "\n    return any(True for _ in match(subject, pattern))"}
{"prompt": "def list_cubes(self):\n        \"\"\" List all available JSON files. \"\"\"", "output": "\n        return self.get_json_files()\n\n    def list_cubes_json(self):\n        \"\"\" List all available JSON files as a JSON string. \"\"\"\n        return json.dumps(self.list_cubes(), cls=JSONEncoder)\n\n    def list_templates(self):\n        \"\"\" List all available template files. \"\"\"\n        return self.get_template_files()\n\n    def list_templates_json(self):\n        \"\"\" List all available JSON files as a JSON string. \"\"\"\n        return json.dumps(self.list_templates_json(), cls=JSONEncoder)\n\n    def list_templates_template(self", "solution": "\n        for file_name in os.listdir(self.directory):\n            if '.' in file_name:\n                name, ext = file_name.rsplit('.', 1)\n                if ext.lower() == 'json':\n                    yield name"}
{"prompt": "def run_forever(self, **kwargs):\n        \"\"\"\n        Run the motor until another command is sent.\n        \"\"\"", "output": "\n        self.logger.debug(\"run_forever called\")\n        while True:\n            try:\n                self.logger.debug(\"sending command\")\n                self.send_command(self.command_dict[self.command])\n                self.logger.debug(\"sent command\")\n                break\n            except KeyError:\n                self.logger.debug(\"no command\")\n                self.send_command(self.command_dict[self.command])\n                self.logger.debug(\"no command\")\n                self.logger.debug(\"sending command\")\n                self.send_command(self.command_dict[self.send_command])", "solution": "\n        for key in kwargs:\n            setattr(self, key, kwargs[key])\n        self.command = self.COMMAND_RUN_FOREVER"}
{"prompt": "def get_cell_rect(self, row, col, tab):\n        \"\"\"Returns rectangle of cell on canvas\"\"\"", "output": "\n        return tab.bbox(row, col)\n\n    def get_cell_rect_color(self, row, col, color):\n        \"\"\"Returns rectangle of cell on canvas\"\"\"\n        return self.get_cell_rect(row, col, color)\n\n    def get_cell_rect_width(self, row, color):\n        \"\"\"Returns rectangle of cell on canvas\"\"\"\n        return self.get_cell_rect(row, COL_CELL_W, color)\n\n    def get_cell_rect_height(self, color):\n        \"\"\"Returns rectangle of cell on canvas\"\"\"\n        return color.bbox(row", "solution": "\n\n        top_row = self.row_tb[0]\n        left_col = self.col_rl[0]\n\n        pos_x = self.x_offset\n        pos_y = self.y_offset\n\n        merge_area = self._get_merge_area((row, col, tab))\n\n        for __row in xrange(top_row, row):\n            __row_height = self.code_array.get_row_height(__row, tab)\n            pos_y += __row_height\n\n        for __col in xrange(left_col, col):\n            __col_width = self.code_array.get_col_width(__col, tab)\n            pos_x += __col_width\n\n        if merge_area is None:\n            height = self.code_array.get_row_height(row, tab)\n            width = self.code_array.get_col_width(col, tab)\n        else:\n            # We have a merged cell\n            top, left, bottom, right = merge_area\n            # Are we drawing the top left cell?\n            if top == row and left == col:\n                # Set rect to merge area\n                heights = (self.code_array.get_row_height(__row, tab)\n                           for __row in xrange(top, bottom+1))\n                widths = (self.code_array.get_col_width(__col, tab)\n                          for __col in xrange(left, right+1))\n                height = sum(heights)\n                width = sum(widths)\n            else:\n                # Do not draw the cell because it is hidden\n                return\n\n        return pos_x, pos_y, width, height"}
{"prompt": "def ssn(self, taxpayer_identification_number_type=SSN_TYPE):\n        \"\"\" Generate a random United States Taxpayer Identification Number of the specified type.\n\n        If no type is specified, a US SSN is returned.\n        \"\"\"", "output": "\n        if not ssn_type:\n            ssn_type = random.choice(self.random_state_types)\n        return self.random_state_generator.generate_random_state_number(\n            taxpayer_identification_number_type=taxpayer_identification_number_type)\n\n    def get_random_number_generator(self):\n        \"\"\" Generate a random United States Taxpayer Identification Number generator.\n\n        Returns:\n            UnitedStatesTaxpayerIdentificationNumberGenerator:\n        \"\"\"\n        return UnitedStatesTaxpayerNumberGenerator(\n            self.random_", "solution": "\n\n        if taxpayer_identification_number_type == self.ITIN_TYPE:\n            return self.itin()\n        elif taxpayer_identification_number_type == self.EIN_TYPE:\n            return self.ein()\n        elif taxpayer_identification_number_type == self.SSN_TYPE:\n\n            # Certain numbers are invalid for United States Social Security\n            # Numbers. The area (first 3 digits) cannot be 666 or 900-999.\n            # The group number (middle digits) cannot be 00. The serial\n            # (last 4 digits) cannot be 0000.\n\n            area = self.random_int(min=1, max=899)\n            if area == 666:\n                area += 1\n            group = self.random_int(1, 99)\n            serial = self.random_int(1, 9999)\n\n            ssn = \"{0:03d}-{1:02d}-{2:04d}\".format(area, group, serial)\n            return ssn\n\n        else:\n            raise ValueError(\"taxpayer_identification_number_type must be one of 'SSN', 'EIN', or 'ITIN'.\")"}
{"prompt": "def _merge_list_of_dict(first, second, prepend=True):\n    \"\"\"\n    Merge lists of dictionaries.\n    Each element of the list is a dictionary having one single key.\n    That key is then used as unique lookup.\n    The first element list has higher priority than the second.\n    When there's an overlap between the two lists,\n    it won't change the position, but the content.\n    \"\"\"", "output": "\n    if not prepend:\n        return first + second\n    else:\n        return first + second + [None] + second + [None]\n\n\ndef _get_content_type_from_path(path):\n    \"\"\"\n    Get content type from path.\n    \"\"\"\n    return mimetypes.guess_type(path)[0] or 'application/octet-stream'\n\n\ndef _get_content_type_from_headers(headers):\n    \"\"\"\n    Get content type from headers.\n    \"\"\"\n    content_type = headers.get('content-type')\n    if not content_type:\n        content_type = _", "solution": "\n    first = _cleanup(first)\n    second = _cleanup(second)\n    if not first and not second:\n        return []\n    if not first and second:\n        return second\n    if first and not second:\n        return first\n    # Determine overlaps\n    # So we dont change the position of the existing terms/filters\n    overlaps = []\n    merged = []\n    appended = []\n    for ele in first:\n        if _lookup_element(second, ele.keys()[0]):\n            overlaps.append(ele)\n        elif prepend:\n            merged.append(ele)\n        elif not prepend:\n            appended.append(ele)\n    for ele in second:\n        ele_key = ele.keys()[0]\n        if _lookup_element(overlaps, ele_key):\n            # If theres an overlap, get the value from the first\n            # But inserted into the right position\n            ele_val_first = _lookup_element(first, ele_key)\n            merged.append({ele_key: ele_val_first})\n        else:\n            merged.append(ele)\n    if not prepend:\n        merged.extend(appended)\n    return merged"}
{"prompt": "def update_ref(profile, ref, sha):\n    \"\"\"Point a ref to a new SHA.\n\n    Args:\n\n        profile\n            A profile generated from ``simplygithub.authentication.profile``.\n            Such profiles tell this module (i) the ``repo`` to connect to,\n            and (ii) the ``token`` to connect with.\n\n        ref\n            The ref to update, e.g., ``heads/my-feature-branch``.\n\n        sha\n            The SHA of the commit to point the ref to.\n\n    Returns\n        A dict with data about the ref.\n\n    \"\"\"", "output": "\n    data = {}\n    data['ref'] = repo.get_ref(ref).get_data()\n    data['sha'] = sha\n    return data\n\n\ndef update_branch(branch, profile, token):\n    \"\"\"Point a branch to a new profile.\n\n    Args:\n\n        branch\n            A branch to point.\n\n        profile\n            A profile generated from ``simplygithub.authentication.profile``.\n            Such profiles tell this module (i) the ``branch`` to connect to,\n            and (ii) the ``token`` to connect with.\n\n    Returns\n        A dict with data about the branch.\n\n    \"\"\"\n    data =", "solution": "\n    resource = \"/refs/\" + ref\n    payload = {\"sha\": sha}\n    data = api.patch_request(profile, resource, payload)\n    return prepare(data)"}
{"prompt": "def run_analysis(self, argv):\n        \"\"\"Run this analysis\"\"\"", "output": "\n        self.logger.info(\"Running analysis\")\n        self.logger.info(\"argv = %s\" % argv)\n        self.logger.debug(\"Analysis parameters = %s\" % self.analysis_parameters)\n        self.logger.debug(\"Analysis results = %s\" % self.analysis_results)\n        self.logger.debug(\"Analysis results_dir = %s\" % self.analysis_results_dir)\n        self.logger.debug(\"Analysis_dir = %s\" % self.analysis_results_dir)\n        self.logger.debug(\"Analysis_dir = %s\" % self", "solution": "\n        args = self._parser.parse_args(argv)\n\n        name_keys = dict(target_type=args.ttype,\n                         target_name=args.target,\n                         sim_name=args.sim,\n                         fullpath=True)\n\n        orig_dir = NAME_FACTORY.targetdir(**name_keys)\n        dest_dir = NAME_FACTORY.sim_targetdir(**name_keys)\n        self.copy_target_dir(orig_dir, dest_dir,\n                             args.roi_baseline, args.extracopy)"}
{"prompt": "def image_information_response(self):\n        \"\"\"Parse image information request and create response.\"\"\"", "output": "\n        return self._parse_image_info(self._image_info_response)\n\n    def _parse_image_info(self, response):\n        \"\"\"Parse image info request.\"\"\"\n        image_info = {}\n\n        if response.status_code == 200:\n            image_info['image_url'] = response.json()['image_url']\n            image_info['image_url_https'] = response.json()['image_url_https']\n            image_info['image_url_https_https'] = response.json()['image_url_https_https']\n            image_info['image_url", "solution": "\n        dr = degraded_request(self.identifier)\n        if (dr):\n            self.logger.info(\"image_information: degraded %s -> %s\" %\n                             (self.identifier, dr))\n            self.degraded = self.identifier\n            self.identifier = dr\n        else:\n            self.logger.info(\"image_information: %s\" % (self.identifier))\n        # get size\n        self.manipulator.srcfile = self.file\n        self.manipulator.do_first()\n        # most of info.json comes from config, a few things specific to image\n        info = {'tile_height': self.config.tile_height,\n                'tile_width': self.config.tile_width,\n                'scale_factors': self.config.scale_factors\n                }\n        # calculate scale factors if not hard-coded\n        if ('auto' in self.config.scale_factors):\n            info['scale_factors'] = self.manipulator.scale_factors(\n                self.config.tile_width, self.config.tile_height)\n        i = IIIFInfo(conf=info, api_version=self.api_version)\n        i.server_and_prefix = self.server_and_prefix\n        i.identifier = self.iiif.identifier\n        i.width = self.manipulator.width\n        i.height = self.manipulator.height\n        if (self.api_version >= '2.0'):\n            # FIXME - should come from manipulator\n            i.qualities = [\"default\", \"color\", \"gray\"]\n        else:\n            # FIXME - should come from manipulator\n            i.qualities = [\"native\", \"color\", \"gray\"]\n        i.formats = [\"jpg\", \"png\"]  # FIXME - should come from manipulator\n        if (self.auth):\n            self.auth.add_services(i)\n        return self.make_response(i.as_json(),\n                                  headers={\"Content-Type\": self.json_mime_type})"}
{"prompt": "def submodules(self):\n        \"\"\"\n        Returns all documented sub-modules in the module sorted\n        alphabetically as a list of `pydoc.Module`.\n        \"\"\"", "output": "\n        modules = []\n        for name, module in self.modules.items():\n            if not module.__doc__:\n                modules.append((name, module))\n        return modules\n\n    def get_submodules(self, name):\n        \"\"\"\n        Returns all documented modules in the module sorted\n        alphabetically as a list of module names.\n        \"\"\"\n        return sorted(self.submodules.get(name, []))\n\n    def get_object(self, name):\n        \"\"\"\n        Returns all documented objects in the module sorted\n        alphabetically as a list of `pydoc.Text` objects.\n        \"\"\"\n        objects = []\n        for name,", "solution": "\n        p = lambda o: isinstance(o, Module) and self._docfilter(o)\n        return sorted(filter(p, self.doc.values()))"}
{"prompt": "def rebin(a, newshape):\n    \"\"\"Rebin an array to a new shape.\"\"\"", "output": "\n    # Only supports int32 for now\n    if a.dtype == 'int32':\n        return np.lib.stride_tricks.as_strided(a, shape=newshape)\n    # Otherwise, fall back to int64\n    else:\n        return np.lib.stride_tricks.as_strided(a, shape=newshape).copy()\n\n\ndef _get_strides_from_shape(shape):\n    \"\"\"Return strides from the shape of a tensor.\n\n    Parameters\n    ----------\n    shape : tuple of int\n        The shape of the tensor.\n\n    Returns\n    -------\n    tuple of int", "solution": "\n\n    slices = [slice(0, old, float(old)/new)\n              for old, new in zip(a.shape, newshape)]\n    coordinates = numpy.mgrid[slices]\n    # choose the biggest smaller integer index\n    indices = coordinates.astype('i')\n    return a[tuple(indices)]"}
{"prompt": "def interpolate2dStructuredPointSpreadIDW(grid, mask, kernel=15, power=2,\r\n                                          maxIter=1e5, copy=True):\r\n    \"\"\"\r\n    same as interpolate2dStructuredIDW but using the point spread method\r\n    this is faster if there are bigger connected masked areas and the border\r\n    length is smaller\r\n\r\n    replace all values in [grid] indicated by [mask]\r\n    with the inverse distance weighted interpolation of all values within\r\n    px+-kernel\r\n\r\n    [power] -> distance weighting factor: 1/distance**[power]\r\n    [copy] -> False: a bit faster, but modifies 'grid' and 'mask'\r\n    \"\"\"", "output": " but uses the same method as interpolate2dStructuredIDW\r\n\r\n    \"\"\"\r\n    # find the nearest neighbor of the grid\r\n    # px = np.array(grid)\r\n    # mask = np.array(mask)\r\n    # kernel = np.array(kernel)\r\n    # if copy:\r\n    #     return np.array(grid), np.array(mask)\r\n    # else:\r\n    #     return np.array(grid), np.array(mask)\r\n    # # find the nearest kernel\r\n    # # kernel = np.array(kernel)\r\n    # # if power > 0:\r\n    # #     return np.array", "solution": "\r\n    assert grid.shape == mask.shape, 'grid and mask shape are different'\r\n\r\n    border = np.zeros(shape=mask.shape, dtype=np.bool)\r\n    if copy:\r\n        # copy mask as well because if will be modified later:\r\n        mask = mask.copy()\r\n        grid = grid.copy()\r\n    return _calc(grid, mask, border, kernel, power, maxIter)"}
{"prompt": "def converged_ionic(self):\n        \"\"\"\n        Checks that ionic step convergence has been reached, i.e. that vasp\n        exited before reaching the max ionic steps for a relaxation run\n        \"\"\"", "output": "\n        self.converged = True\n        self.converged_ionic_steps = 0\n        self.converged_ionic_steps_max = self.max_ionic_steps\n        self.converged_ionic_steps_converged = 0\n        self.converged_ionic_steps_converged_max = self.converged_ionic_steps\n        return self.converged_ionic_steps >= self.converged_ionic_steps_max\n\n    def get_potcar_symbols(self):\n        \"\"\"\n        Returns the list of symbols that are present in the v", "solution": "\n        nsw = self.parameters.get(\"NSW\", 0)\n        return nsw <= 1 or len(self.ionic_steps) < nsw"}
{"prompt": "def object_to_bytes(obj):\n    \"\"\"\n    Convert a object to a bytearray or call get_raw() of the object\n    if no useful type was found.\n    \"\"\"", "output": "\n    if isinstance(obj, dict):\n        return obj\n    elif isinstance(obj, list):\n        return bytearray(obj)\n    elif isinstance(obj, str):\n        return obj.encode('utf-8')\n    else:\n        return bytearray(obj)\n\n\ndef get_raw_bytes(obj):\n    \"\"\"\n    Convert a list of bytes to a bytearray or call get_raw() of the object\n    if no useful type was found.\n    \"\"\"\n    if isinstance(obj, dict):\n        return obj\n    elif isinstance(obj, list):\n        return bytearray(obj)\n    elif isinstance(obj, str):\n        return", "solution": "\n    if isinstance(obj, str):\n        return bytearray(obj, \"UTF-8\")\n    elif isinstance(obj, bool):\n        return bytearray()\n    elif isinstance(obj, int):\n        return pack(\"<L\", obj)\n    elif obj is None:\n        return bytearray()\n    elif isinstance(obj, bytearray):\n        return obj\n    else:\n        return obj.get_raw()"}
{"prompt": "def login(self):\n        \"\"\"\n        View function to log a user in. Supports html and json requests.\n        \"\"\"", "output": "\n        if not self.is_browser_on_page(HomePage):\n            self.location('/login', wait_for_page_load=False)\n\n    def logout(self):\n        \"\"\"\n        Log out function\n        \"\"\"\n        if self.is_browser_on_page(HomePage):\n            self.location('/logout', wait_for_page_load=False)\n        else:\n            self.location('/logout', wait_for_page_load=True)\n\n    def get_current_user(self):\n        \"\"\"\n        Returns the currently logged in user.\n        \"\"\"\n        return self.get('/users", "solution": "\n        form = self._get_form('SECURITY_LOGIN_FORM')\n        if form.validate_on_submit():\n            try:\n                self.security_service.login_user(form.user, form.remember.data)\n            except AuthenticationError as e:\n                form._errors = {'_error': [str(e)]}\n            else:\n                self.after_this_request(self._commit)\n                if request.is_json:\n                    return self.jsonify({'token': form.user.get_auth_token(),\n                                         'user': form.user})\n                self.flash(_('flask_unchained.bundles.security:flash.login'),\n                           category='success')\n                return self.redirect('SECURITY_POST_LOGIN_REDIRECT_ENDPOINT')\n        else:\n            # FIXME-identity\n            identity_attrs = app.config.SECURITY_USER_IDENTITY_ATTRIBUTES\n            msg = f\"Invalid {', '.join(identity_attrs)} and/or password.\"\n\n            # we just want a single top-level form error\n            form._errors = {'_error': [msg]}\n            for field in form._fields.values():\n                field.errors = None\n\n        if form.errors and request.is_json:\n            return self.jsonify({'error': form.errors.get('_error')[0]},\n                                code=HTTPStatus.UNAUTHORIZED)\n\n        return self.render('login',\n                           login_user_form=form,\n                           **self.security.run_ctx_processor('login'))"}
{"prompt": "def _place_tables_section(skeleton_section, sheet, keys_section):\n    \"\"\"\n    Place data into skeleton for either a paleo or chron section.\n    :param dict skeleton_section: Empty or current progress of skeleton w/ data\n    :param dict sheet: Sheet metadata\n    :param list keys_section: Paleo or Chron specific keys\n    :return dict: Skeleton section full of data\n    \"\"\"", "output": "\n    progress = 0\n    if not keys_section:\n        keys_section = [\n            \"paleo_id\",\n            \"paleo_name\",\n            \"progress\",\n            \"paleo_progress\",\n            \"paleo_progress_percent\",\n            \"progress_percent\",\n            \"progress_percent_percent\",\n            \"progress_percent_percent\",\n            \"progress_percent_percent\",\n            \"progress_percent_percent\",\n            \"progress_percent_percent\",\n            \"progress_percent_percent\",\n            \"progress_percent_percent\",\n            \"progress_percent_percent\",\n            \"progress", "solution": "\n    logger_excel.info(\"enter place_tables_section\")\n    try:\n        logger_excel.info(\"excel: place_tables_section: placing table: {}\".format(sheet[\"new_name\"]))\n        new_name = sheet[\"new_name\"]\n        logger_excel.info(\"placing_tables_section: {}\".format(new_name))\n        # get all the sheet metadata needed for this function\n        idx_pc = sheet[\"idx_pc\"] - 1\n        idx_model = sheet[\"idx_model\"]\n        idx_table = sheet[\"idx_table\"]\n        table_type = sheet[\"table_type\"]\n        data = sheet[\"data\"]\n        # paleoMeas or chronMeas key\n        key_1 = keys_section[0]\n        # paleoModel or chronModel key\n        key_2 = keys_section[1]\n        # Is this a measurement, or distribution table?\n        if idx_table:\n            # Yes, a table idx exists, so decrement it.\n            idx_table = sheet[\"idx_table\"] - 1\n        # Is this a ensemble, dist, or summary table?\n        if idx_model:\n            # Yes, a model idx exists, so decrement it.\n            idx_model -= 1\n    except Exception as e:\n        logger_excel.debug(\"excel: place_tables_section: error during setup, {}\".format(e))\n\n    # If it's measurement table, it goes in first.\n    try:\n        if table_type == \"measurement\":\n            skeleton_section[idx_pc][key_1][idx_table] = data\n        # Other types of tables go one step below\n        elif table_type in [\"ensemble\", \"distribution\", \"summary\"]:\n            if table_type == \"summary\":\n                skeleton_section[idx_pc][key_2][idx_model][\"summaryTable\"] = data\n            elif table_type == \"ensemble\":\n                skeleton_section[idx_pc][key_2][idx_model][\"ensembleTable\"] = data\n            elif table_type == \"distribution\":\n                skeleton_section[idx_pc][key_2][idx_model][\"distributionTable\"][idx_table] = data\n    except Exception as e:\n        logger_excel.warn(\"excel: place_tables_section: Unable to place table {}, {}\".format(new_name, e))\n    logger_excel.info(\"exit place_tables_section\")\n    return skeleton_section"}
{"prompt": "def crossValidation(self,seed=0,n_folds=10,fullVector=True,verbose=None,D=None,**keywords):\n        \"\"\"\n        Split the dataset in n folds, predict each fold after training the model on all the others\n\n        Args:\n            seed:        seed\n            n_folds:     number of folds to train the model on\n            fullVector:  Bolean indicator, if true it stops if no convergence is observed for one of the folds, otherwise goes through and returns a pheno matrix with missing values\n            verbose:     if true, prints the fold that is being used for predicitons\n            **keywords:  params to pass to the function optimize\n        Returns:\n            Matrix of phenotype predictions [N,P]\n        \"\"\"", "output": "\n            D:           if not None, this is the training set, if not None, the model will be trained on all the others\n        \"\"\"\n        if D is None:\n            D = self.D\n        if verbose is None:\n            verbose = self.verbose\n        if self.model is None:\n            self.model = self.createModel()\n        if self.model is None:\n            self.model = self.model\n        if fullVector:\n            return self.model.crossValidation(seed,n_folds,D,**keywords)\n        else:\n            return self.model.crossValidation(seed,n_folds", "solution": "\n        verbose = dlimix.getVerbose(verbose)\n\n        # split samples into training and test\n        sp.random.seed(seed)\n        r = sp.random.permutation(self.Y.shape[0])\n        nfolds = 10\n        Icv = sp.floor(((sp.ones((self.Y.shape[0]))*nfolds)*r)/self.Y.shape[0])\n\n        RV = {}\n        if self.P==1:  RV['var'] = sp.zeros((nfolds,self.n_randEffs))\n        else:          RV['var'] = sp.zeros((nfolds,self.P,self.n_randEffs))\n\n        Ystar = sp.zeros_like(self.Y)\n\n        for fold_j in range(n_folds):\n\n            if verbose:\n                print((\".. predict fold %d\"%fold_j))\n\n            Itrain  = Icv!=fold_j\n            Itest   = Icv==fold_j\n            Ytrain  = self.Y[Itrain,:]\n            Ytest   = self.Y[Itest,:]\n            vc = VarianceDecomposition(Ytrain)\n            vc.setTestSampleSize(Itest.sum())\n            for term_i in range(self.n_fixedEffs):\n                F      = self.vd.getFixed(term_i)\n                Ftest  = F[Itest,:]\n                Ftrain = F[Itrain,:]\n                if self.P>1:    A = self.vd.getDesign(term_i)\n                else:           A = None\n                vc.addFixedEffect(F=Ftrain,Ftest=Ftest,A=A)\n            for term_i in range(self.n_randEffs):\n                if self.P>1:\n                    tct  = self.trait_covar_type[term_i]\n                    rank = self.rank[term_i]\n                    ftc  = self.fixed_tc[term_i]\n                    jitt = self.jitter[term_i]\n                    if tct=='lowrank_diag1' or tct=='freeform1':\n                        d = D[fold_j,:,term_i]\n                    else:\n                        d = None\n                else:\n                    tct  = None\n                    rank = None\n                    ftc  = None\n                    jitt = None\n                    d    = None\n                if term_i==self.noisPos:\n                    vc.addRandomEffect(is_noise=True,trait_covar_type=tct,rank=rank,jitter=jitt,fixed_trait_covar=ftc,d=d)\n                else:\n                    R = self.vd.getTerm(term_i).getK()\n                    Rtrain = R[Itrain,:][:,Itrain]\n                    Rcross = R[Itrain,:][:,Itest]\n                    vc.addRandomEffect(K=Rtrain,Kcross=Rcross,trait_covar_type=tct,rank=rank,jitter=jitt,fixed_trait_covar=ftc,d=d)\n            conv = vc.optimize(verbose=False,**keywords)\n            if self.P==1:\n                RV['var'][fold_j,:] = vc.getVarianceComps()[0,:]\n            else:\n                RV['var'][fold_j,:,:] = vc.getVarianceComps()\n\n            if fullVector:\n                assert conv, 'VarianceDecompositon:: not converged for fold %d. Stopped here' % fold_j\n            if conv:\n                Ystar[Itest,:] = vc.predictPhenos()\n            else:\n                warnings.warn('not converged for fold %d' % fold_j)\n                Ystar[Itest,:] = sp.nan\n\n        return Ystar,RV"}
{"prompt": "def load_pubkeys(loadpath, pubkeys):\n\n    \"\"\"Append the file contents in loadpath directory onto pubkeys list\"\"\"", "output": "\n\n    # read in the public key file\n    with open(loadpath + \".pub\", 'a') as f:\n        f.write(pubkeys)\n\n\ndef load_privatekeys(loadpath):\n\n    \"\"\"Read in the private key file\"\"\"\n\n    with open(loadpath + \".private\", 'r') as f:\n        privatekeys = f.read()\n\n\ndef save_privatekeys(loadpath, privatekeys):\n\n    \"\"\"Write out the file contents in loadpath directory onto private key list\"\"\"\n\n    # write out the file contents\n    with open(loadpath + \".private\", 'w') as f", "solution": "\n\n    filenames = os.listdir(loadpath)\n    logger.debug('loading authorized pubkeys {0}'.format(filenames))\n    for filename in filenames:\n        pubkeys.append(open(join(loadpath, filename)).read())"}
{"prompt": "def main():\n    \"\"\"\n    Run the main CLI program.\n\n    Initializes the stack, parses command line arguments, and fires requested\n    logic.\n    \"\"\"", "output": "\n    # Parse command line arguments\n    parser = argparse.ArgumentParser(description='Run the Mesos test suite.')\n    parser.add_argument('-c', '--config', help='Path to Mesos test suite config file', required=True)\n    parser.add_argument('-p', '--port', help='Port to run Mesos test suite on', type=int, default=9090)\n    parser.add_argument('-s', '--skip', help='Skip the specified number of tests', type=int, default=0)\n    parser.add_argument('-d', '--debug', help='Enable", "solution": "\n    plugins = DefaultPluginManager()\n    plugins.load_plugins()\n\n    parser, _ = create_argparser()\n    # add the updater protocol options to the CLI:\n    for kls in updater_classes():\n        kls.register_arguments(parser)\n\n    for kls in detector_classes():\n        kls.register_arguments(parser)\n\n    # add the plugin options to the CLI:\n    from os import environ\n    plugins.options(parser, environ)\n\n    args = parser.parse_args()\n\n    if args.debug:\n        args.verbose_count = 5  # some high number\n\n    log_level = max(int(logging.WARNING / 10) - args.verbose_count, 0) * 10\n    # print(log_level)\n    logging.basicConfig(level=log_level, format=\"%(levelname)s %(message)s\")\n    # logging.debug(\"args %r\", args)\n\n    if args.version:\n        from . import __version__\n        print(\"dyndnsc %s\" % __version__)  # noqa\n        return 0\n\n    # silence 'requests' logging\n    requests_log = logging.getLogger(\"requests\")\n    requests_log.setLevel(logging.WARNING)\n\n    logging.debug(parser)\n    cfg = get_configuration(args.config)\n\n    if args.listpresets:\n        list_presets(cfg)\n        return 0\n\n    if args.config:\n        collected_configs = collect_config(cfg)\n    else:\n        parsed_args = parse_cmdline_args(args, updater_classes().union(detector_classes()))\n        logging.debug(\"parsed_args %r\", parsed_args)\n\n        collected_configs = {\n            \"cmdline\": {\n                \"interval\": int(args.sleeptime)\n            }\n        }\n        collected_configs[\"cmdline\"].update(parsed_args)\n\n    plugins.configure(args)\n    plugins.initialize()\n\n    logging.debug(\"collected_configs: %r\", collected_configs)\n    dyndnsclients = []\n    for thisconfig in collected_configs:\n        logging.debug(\"Initializing client for '%s'\", thisconfig)\n        # done with options, bring on the dancing girls\n        dyndnsclient = getDynDnsClientForConfig(\n            collected_configs[thisconfig], plugins=plugins)\n        if dyndnsclient is None:\n            return 1\n        # do an initial synchronization, before going into endless loop:\n        dyndnsclient.sync()\n        dyndnsclients.append(dyndnsclient)\n\n    run_forever_callable = partial(run_forever, dyndnsclients)\n\n    if args.daemon:\n        import daemonocle\n        daemon = daemonocle.Daemon(worker=run_forever_callable)\n        daemon.do_action(\"start\")\n        args.loop = True\n\n    if args.loop:\n        run_forever_callable()\n\n    return 0"}
{"prompt": "def _get_buildout_ver(directory='.'):\n    \"\"\"Check for buildout versions.\n\n    In any cases, check for a version pinning\n    Also check for buildout.dumppickedversions which is buildout1 specific\n    Also check for the version targeted by the local bootstrap file\n    Take as default buildout2\n\n    directory\n        directory to execute in\n    \"\"\"", "output": "\n    if not os.path.isdir(directory):\n        raise ValueError('Directory %s not found' % directory)\n    ver = None\n    for line in os.popen('git ls-tree -r %s' % directory):\n        if line.startswith('version'):\n            ver = line.split()[2]\n            break\n    if not ver:\n        raise ValueError('Cannot find git version')\n    return ver\n\n\ndef _get_git_version(directory='.'):\n    \"\"\"Get the git version.\n\n    In any cases, check for a version pinning\n    Also get the version from the local bootstrap file\n    Take as", "solution": "\n    directory = os.path.abspath(directory)\n    buildoutver = 2\n    try:\n        files = _find_cfgs(directory)\n        for f in files:\n            with salt.utils.files.fopen(f) as fic:\n                buildout1re = re.compile(r'^zc\\.buildout\\s*=\\s*1', RE_F)\n                dfic = salt.utils.stringutils.to_unicode(fic.read())\n                if (\n                        ('buildout.dumppick' in dfic)\n                        or\n                        (buildout1re.search(dfic))\n                ):\n                    buildoutver = 1\n        bcontent = _get_bootstrap_content(directory)\n        if (\n            '--download-base' in bcontent\n            or '--setup-source' in bcontent\n            or '--distribute' in bcontent\n        ):\n            buildoutver = 1\n    except (OSError, IOError):\n        pass\n    return buildoutver"}
{"prompt": "def _extract_extra_keywords(self,params):\n        \"\"\"\n        Return any items in params that are not also\n        parameters of the overridden object.\n        \"\"\"", "output": "\n        return [param for param in params if param.name not in self.extra_keywords]\n\n    def _get_extra_keywords_from_method(self,method):\n        \"\"\"\n        Return any items in method that are not also\n        methods of the overridden object.\n        \"\"\"\n        return [method.im_func.func_code.co_varnames[i] for i in self.im_func.func_code.co_varnames_list if i.startswith(self.extra_keywords)]\n\n    def _get_extra_keywords_from_method_list(self,method_list", "solution": "\n        extra_keywords = {}\n        overridden_object_params = list(self._overridden.param)\n        for name, val in params.items():\n            if name not in overridden_object_params:\n                extra_keywords[name]=val\n                # CEBALERT: should we remove name from params\n                # (i.e. del params[name]) so that it's only available\n                # via extra_keywords()?\n        return extra_keywords"}
{"prompt": "def _parse_non_entities(self):\n        \"\"\"create data for all non-entities in the babelfied text\n        set self._all_entities with merged entity and non-entity data\n        \"\"\"", "output": "\n        self._all_entities = {}\n        for match in re.finditer(u\"&(\\w+);\", self.text):\n            self._all_entities[match.group(1)] = match.group(2)\n        self.text = self.text.replace(u\"&amp;\", u'&')\n        self.text = self.text.replace(u\"&lt;\", u'&')\n        self.text = self.text.replace(u\"&gt;\", u'&')\n\n    def _replace_entity(self, match):\n        \"\"\"\n        replace the character entity in the babelf", "solution": "\n        def _differ(tokens):\n            inner, outer = tokens\n            not_same_start = inner.get('start') != outer.get('start')\n            not_same_end = inner.get('end') != outer.get('end')\n            return not_same_start or not_same_end\n\n        def _get_dot_token():\n            dot_token = dict()\n            dot_token['start'] = (len(self._text) - 1)\n            dot_token['end'] = dot_token['start']\n            dot_token['text'] = '.'\n            dot_token['isEntity'] = False\n            return dot_token\n\n        if self._text.endswith('.'):\n            text = self._text[:-1]\n            add_dot_token = True\n        else:\n            text = self._text\n            add_dot_token = False\n\n        index = 0\n        all_tokens = list()\n        for token in text.split():\n\n            comma_token = False\n            if token.endswith(','):\n                comma_token = True\n                token = token[:-1]\n\n            start = index\n            end = (start + len(token))\n            index += (len(token) + 1)\n\n            all_tokens.append({\n                'start': start,\n                'end': end - 1,\n                'text': self._text[start: end],\n                'isEntity': False,\n            })\n\n            if comma_token:\n                all_tokens.append({\n                    'start': index,\n                    'end': index,\n                    'text': ',',\n                    'isEntity': False,\n                })\n                index += 1\n\n        token_tuples = list(product(all_tokens, self.entities))\n        redundant = [\n            tokens[0] for tokens in token_tuples if not _differ(tokens)]\n        non_entity_tokens = [\n            item for item in all_tokens if item not in redundant]\n\n        if add_dot_token:\n            non_entity_tokens.append(_get_dot_token())\n\n        self._all_entities = sorted(\n            self._entities + non_entity_tokens, key=itemgetter('start'))"}
{"prompt": "def acls(self):\n        \"\"\"The instance bound ACLs operations layer.\"\"\"", "output": "\n        return self.acl_manager.acl_class(self.session)\n\n    def delete(self, session=None):\n        \"\"\"Delete this ACL.\n\n        :param session: The session to use for the request.\n        \"\"\"\n        if session is None:\n            self.session = self.acl_manager.session\n        else:\n            self.session = session\n        return self.session.delete(self)\n\n    def add(self, session=None, acl_type=None, acl_id=None, action=None):\n        \"\"\"Add a new ACL.\n\n        :param session: The session to use for the", "solution": "\n        if self._acls is None:\n            self._acls = InstanceAcls(instance=self)\n        return self._acls"}
{"prompt": "def _rotate_and_chop(self, verts, normal, axis=[0, 0, 1]):\n        r\"\"\"\n        Method to rotate a set of vertices (or coords) to align with an axis\n        points must be coplanar and normal must be given\n        Chops axis coord to give vertices back in 2D\n        Used to prepare verts for printing or calculating convex hull in order\n        to arrange them in hull order for calculations and printing\n        \"\"\"", "output": "\n        # TODO: check if this is correct\n        if self.is_valid_vertex_list(verts):\n            # TODO: check if this is correct\n            if normal == 'x':\n                verts = rotate_x_2d(verts, axis)\n            elif normal == 'y':\n                verts = rotate_y_2d(verts, axis)\n            self._rotate_and_coords(verts)\n        else:\n            raise ValueError(\"The given set of vertices is not valid\")\n\n    def _rotate_and_coords(self, verts):\n        r\"\"\"\n        Method to rotate a set of vertices", "solution": "\n        xaxis = [1, 0, 0]\n        yaxis = [0, 1, 0]\n        zaxis = [0, 0, 1]\n        angle = tr.angle_between_vectors(normal, axis)\n        if angle == 0.0 or angle == np.pi:\n            # We are already aligned\n            facet = verts\n        else:\n            M = tr.rotation_matrix(tr.angle_between_vectors(normal, axis),\n                                   tr.vector_product(normal, axis))\n            try:\n                facet = np.dot(verts, M[:3, :3].T)\n            except ValueError:\n                pass\n        try:\n            x = facet[:, 0]\n            y = facet[:, 1]\n            z = facet[:, 2]\n        except IndexError:\n            x = facet[0]\n            y = facet[1]\n            z = facet[2]\n        # Work out span of points and set axes scales to cover this and be\n        # equal in both dimensions\n        if axis == xaxis:\n            output = np.column_stack((y, z))\n        elif axis == yaxis:\n            output = np.column_stack((x, z))\n        elif axis == zaxis:\n            output = np.column_stack((x, y))\n        else:\n            output = facet\n        return output"}
{"prompt": "def image_from_docker_args(args):\n    \"\"\"This scans docker run args and attempts to find the most likely docker image argument.\n    If excludes any argments that start with a dash, and the argument after it if it isn't a boolean\n    switch.  This can be improved, we currently fallback gracefully when this fails.\n    \"\"\"", "output": "\n    if args.get('format') == 'json':\n        return json.loads(args.get('docker_args'))\n    elif args.get('format') == 'docker':\n        return docker_image_from_args(docker_args_to_dict(args['docker_args']))\n    else:\n        raise Exception(\"Unknown format {}\".format(args['format']))\n\n\ndef docker_image_from_args(args):\n    \"\"\"This scans docker run args and attempts to find the most likely docker image argument.\n    If excludes any argments that start with a dash, and the argument after it if", "solution": "\n    bool_args = [\"-t\", \"--tty\", \"--rm\",\"--privileged\", \"--oom-kill-disable\",\"--no-healthcheck\", \"-i\",\n        \"--interactive\", \"--init\", \"--help\", \"--detach\", \"-d\", \"--sig-proxy\", \"-it\", \"-itd\"]\n    last_flag = -2\n    last_arg = \"\"\n    possible_images = []\n    if len(args) > 0 and args[0] == \"run\":\n        args.pop(0)\n    for i, arg in enumerate(args):\n        if arg.startswith(\"-\"):\n            last_flag = i\n            last_arg = arg\n        elif \"@sha256:\" in arg:\n            # Because our regex doesn't match digests\n            possible_images.append(arg)\n        elif docker_image_regex(arg):\n            if last_flag == i - 2:\n                possible_images.append(arg)\n            elif \"=\" in last_arg:\n                possible_images.append(arg)\n            elif last_arg in bool_args and last_flag == i - 1:\n                possible_images.append(arg)\n    most_likely = None\n    for img in possible_images:\n        if \":\" in img or \"@\" in img or \"/\" in img:\n            most_likely = img\n            break\n    if most_likely == None and len(possible_images) > 0:\n        most_likely = possible_images[0]\n    return most_likely"}
{"prompt": "def basic(type='artist-radio', artist_id=None, artist=None, song_id=None, song=None, track_id=None, dmca=False,\n          results=15, buckets=None, limit=False,genres=None,):\n    \"\"\"Get a basic playlist\n    \n    Args:\n    \n    Kwargs:\n        type (str): a string representing the playlist type ('artist-radio' or 'song-radio')\n        \n        artist_id (str): the artist_id to seed the playlist\n        \n        artist (str): the name of an artist to seed the playlist\n        \n        song_id (str): a song_id to seed the playlist\n        \n        song (str): the name of a song to seed the playlist\n        \n        track_id (str): the name of a track to seed the playlist\n        \n        dmca (bool): make the playlist dmca-compliant\n        \n        results (int): desired length of the playlist\n        \n        buckets (list): A list of strings specifying which buckets to retrieve\n        \n        limit (bool): Whether results should be restricted to any idspaces given in the buckets parameter\n    \"\"\"", "output": "str): the song_id to seed the playlist\n        \n        song (str): the name of an artist to seed the playlist\n    \n    Returns:\n        A list of the form (name, artist, url)\n    \"\"\"\n    if type == 'artist-radio':\n        return basic_artist_radio(type='artist-radio', artist_id=artist_id, results=results)\n    elif type =='song-radio':\n        return basic_song_radio(type='song-radio', artist_id=artist_id, results=results)\n    elif type == 'track-radio':\n        return basic_track_radio", "solution": "\n\n    limit = str(limit).lower()\n    dmca = str(dmca).lower()\n\n    kwargs = locals()\n    kwargs['bucket'] = kwargs['buckets']\n    del kwargs['buckets']\n    kwargs['genre'] = kwargs['genres']\n    del kwargs['genres']\n\n    result = util.callm(\"%s/%s\" % ('playlist', 'basic'), kwargs)\n    return [Song(**util.fix(s_dict)) for s_dict in result['response']['songs']]"}
{"prompt": "def permissions(self):\n        \"\"\"Dynamically generate dictionary of privacy options\n        \"\"\"", "output": "\n        return {\n           'read': self.read_privacy,\n            'write': self.write_privacy,\n            'delete': self.delete_privacy,\n            'undelete': self.undelete_privacy,\n            'delete_all': self.delete_privacy,\n            'undelete_all': self.undelete_privacy,\n        }\n\n    def get_context_data(self, **kwargs):\n        \"\"\"Add the context data to the context\n        \"\"\"\n        context = super(UserSettingsView, self).get_context_data(**kwargs)\n        context['read'] = self.request.user.", "solution": "\n        # TODO: optimize this, it's kind of a bad solution for listing a mostly\n        # static set of files.\n        # We could either add a permissions dict as an attribute or cache this\n        # in some way. Creating a dict would be another place we have to define\n        # the permission, so I'm not a huge fan, but it would definitely be the\n        # easier option.\n        permissions_dict = {\"self\": {}, \"parent\": {}}\n\n        for field in self.properties._meta.get_fields():\n            split_field = field.name.split('_', 1)\n            if len(split_field) <= 0 or split_field[0] not in ['self', 'parent']:\n                continue\n            permissions_dict[split_field[0]][split_field[1]] = getattr(self.properties, field.name)\n\n        return permissions_dict"}
{"prompt": "def _init_h2us(self, fnc_most_specific):\n        \"\"\"Given a set of user GO ids, return GO ids grouped under the \"GO high\" terms.\n\n        Example of a grouped go list:\n\n            gos = ['GO:0044464':[    # grp_term: D1 cell part\n                       'GO:0005737', # child:    D3 cytoplasm\n                       'GO:0048471', # child:    D4 perinuclear region of cytoplasm\n                   'GO:0016020':[    # grp_term: D1 membrane\n                       'GO:0098589', # child:    D2 membrane region\n                       'GO:0005886', # child:    D2 plasma membrane\n                  ]\n        \"\"\"", "output": "mbrane\n                       'GO:0005502', # child:    D5 cytoplasm\n                       'GO:0005503', # child:    D6 perinuclear region of perinuclear region of membrane\n                   ],\n               'D1': [    # grp_term: D1 cell part\n                       'D1:0000305', # child:    D2 cytoplasm\n                       'D1:0000305', # child:    D3 cytoplasm\n                       'D1:0000305', # child:    D4 cytoplasm\n                       'D1:0000305", "solution": "\n        # Header GO IDs are main. User GO IDs are as specified by the user\n        hdrgo2usrgos = cx.defaultdict(set)\n        # Contains user GO IDs which are also header GO IDs, plus user main GO if needed\n        hdrgo_is_usrgo = set()\n        _go2nt = self.gosubdag.go2nt\n        objhi = GrouperInit.GetGoidHigh(self.gosubdag, self.hdrobj.hdrgos,\n                                        self.most_specific_fncs[fnc_most_specific])\n        for goid_usr in self.usrgos:\n            goid_main = _go2nt[goid_usr].id\n            # Add current GO ID to parents_all in case curr GO ID is a high GO.\n            goid_high = objhi.get_goid_high(goid_main)\n            # Don't add user GO ID if it is also the GO header\n            if goid_main != goid_high:\n                hdrgo2usrgos[goid_high].add(goid_usr)\n            elif goid_high not in hdrgo2usrgos:\n                hdrgo2usrgos[goid_high] = set()\n            if goid_main == goid_high:\n                hdrgo_is_usrgo.add(goid_main)\n                if goid_main != goid_usr:\n                    hdrgo_is_usrgo.add(goid_usr)\n        # Initialize data members\n        self.hdrgo2usrgos = hdrgo2usrgos\n        self.hdrgo_is_usrgo = hdrgo_is_usrgo"}
{"prompt": "def write_service_double_file(target_root, service_name, rendered):\n    \"\"\"Render syntactically valid python service double code.\"\"\"", "output": "\n    with open(target_root /'services/' + service_name / 'double.js', 'w') as f:\n        f.write(rendered)\n\n\ndef write_service_template(target_root, service_name, template_name, rendered):\n    \"\"\"Write a template for a python service.\"\"\"\n    with open(target_root / 'templates/' + service_name / template_name, 'w') as f:\n        f.write(rendered)\n\n\ndef write_service_script(target_root, service_name, script_name, rendered):\n    \"\"\"Render a script", "solution": "\n    target_path = os.path.join(\n        target_root,\n        'snapstore_schemas', 'service_doubles', '%s.py' % service_name\n    )\n    with open(target_path, 'w') as target_file:\n        target_file.write(rendered)"}
{"prompt": "def dot(r1, r2):\n    \"\"\"Compute the dot product\n\n       Arguments:\n        | ``r1``, ``r2``  -- two :class:`Vector3` objects\n\n       (Returns a Scalar)\n    \"\"\"", "output": "\n    return dot_product(r1.dot(r2), r1.dot(r2))\n\ndef cross(v1, v2):\n    \"\"\"Compute the cross product of two vectors\n\n       Arguments:\n        | ``v1``, ``v2``  -- a 2-dimensional vector\n\n    (Returns a Scalar)\n    \"\"\"\n    return cross_product(v1.cross(v2), v1.cross(v2))\n\ndef dot_product(v1, v2):\n    \"\"\"Compute the dot product of two vectors\n\n       Arguments:\n        | ``v1``, ``v2`` ", "solution": "\n    if r1.size != r2.size:\n        raise ValueError(\"Both arguments must have the same input size.\")\n    if r1.deriv != r2.deriv:\n        raise ValueError(\"Both arguments must have the same deriv.\")\n    return r1.x*r2.x + r1.y*r2.y + r1.z*r2.z"}
{"prompt": "def match_objective_id(self, objective_id, match):\n        \"\"\"Sets the objective ``Id`` for this query.\n\n        arg:    objective_id (osid.id.Id): an objective ``Id``\n        arg:    match (boolean): ``true`` for a positive match,\n                ``false`` for a negative match\n        raise:  NullArgument - ``objective_id`` is ``null``\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"", "output": "\n        pass\n\n    @abc.abstractmethod\n    def clear_objective_bank_id_terms(self):\n        \"\"\"Clears the bank ``Id`` terms.\n\n\n\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def match_any_objective_bank_id(self, match):\n        \"\"\"Matches an ``Id`` that has any objective banks.\n\n\n\n        :param match: ``true`` to match any objective banks with any\n                term\n        :type: ``boolean``\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n        pass", "solution": "\n        if not isinstance(objective_id, Id):\n            raise errors.InvalidArgument()\n        self._add_match('objectiveId', str(objective_id), match)"}
{"prompt": "def is_convertible_with(self, other):\n        \"\"\"Returns true if `other` is convertible with this Dimension.\n\n        Two known Dimensions are convertible if they have the same value.\n        An unknown Dimension is convertible with all other Dimensions.\n\n        Args:\n          other: Another Dimension.\n\n        Returns:\n          True if this Dimension and `other` are convertible.\n        \"\"\"", "output": "\n        return (self.value == other.value and\n                self.format == other.format and\n                self.value == other.value)\n\n    def __ne__(self, other):\n        \"\"\"Returns true if `self`!= `other`.\"\"\"\n        return not self.__eq__(other)\n\n    def __eq__(self, other):\n        \"\"\"Returns true if `self` and `other` are equal.\"\"\"\n        return self.value == other.value\n\n    def __ne__(self, other):\n        \"\"\"Returns true if `self`!= `other`.\"\"\"\n        return not self.__eq__(other)\n\n    def __hash", "solution": "\n        other = as_dimension(other)\n        return self._value is None or other.value is None or self._value == other.value"}
{"prompt": "def file_command(self, value):\n        \"\"\"\n        Setter for **self.__file_command** attribute.\n\n        :param value: Attribute value.\n        :type value: unicode\n        \"\"\"", "output": "\n\n        self.__file_command = value\n\n    @property\n    def file_name(self):\n        \"\"\"\n        Getter for **self.__file_name** attribute.\n\n        :return: Attribute value.\n        :rtype: unicode\n        \"\"\"\n\n        return self.__file_name\n\n    @file_name.setter\n    def file_name(self, value):\n        \"\"\"\n        Setter for **self.__file_name** attribute.\n\n        :param value: Attribute value.\n        :type value: unicode\n        \"\"\"\n\n        self.__file_name = value\n\n    @property\n    def file_path(self):\n        \"\"\"\n        Getter for", "solution": "\n\n        if value is not None:\n            assert type(value) is unicode, \"'{0}' attribute: '{1}' type is not 'unicode'!\".format(\n                \"file_command\", value)\n        self.__file_command = value"}
{"prompt": "def revoke_permission(user, permission_name):\n    \"\"\"\n    Revoke a specified permission from a user.\n\n    Permissions are only revoked if they are in the scope any of the user's\n    roles. If the permission is out of scope, a RolePermissionScopeException\n    is raised.\n    \"\"\"", "output": "\n    if not user.is_anonymous():\n        raise RolePermissionDenied(\"Cannot revoke permissions for anonymous users\")\n    if not permission_name.lower() in [\"read\", \"write\", \"admin\", \"administrator\"]:\n        raise RolePermissionInvalid(\"Invalid permission name\")\n    if permission_name in get_roles_with_permissions(user, permission_name):\n        raise RoleAccessError(\"You cannot revoke the rights for the requested role\")\n    if not user.is_admin():\n        raise RoleAccessError(\"Cannot revoke permissions for administrators\")\n    if not permission_name.lower() in [\"admin\", \"", "solution": "\n    roles = get_user_roles(user)\n\n    for role in roles:\n        if permission_name in role.permission_names_list():\n            permission = get_permission(permission_name)\n            user.user_permissions.remove(permission)\n            return\n\n    raise RolePermissionScopeException(\n        \"This permission isn't in the scope of \"\n        \"any of this user's roles.\")"}
{"prompt": "def make_redirect_url(self, path_info, query_args=None, domain_part=None):\n        \"\"\"Creates a redirect URL.\n\n        :internal:\n        \"\"\"", "output": "\n        url = self.build_absolute_uri(path_info, self.redirect_field_name)\n        if query_args:\n            url += '?' + self.redirect_field_value(query_args[0])\n        if query_args:\n            url += '&' + self.redirect_field_value(query_args[1])\n        if self.redirect_url:\n            url = url + '&' + self.redirect_url\n        return url\n\n    def build_absolute_uri(self, path_info, path):\n        \"\"\"Creates an absolute URI from the path.\n\n        :internal:\n        \"\"\"", "solution": "\n        suffix = ''\n        if query_args:\n            suffix = '?' + self.encode_query_args(query_args)\n        return str('%s://%s/%s%s' % (\n            self.url_scheme,\n            self.get_host(domain_part),\n            posixpath.join(self.script_name[:-1].lstrip('/'),\n                           url_quote(path_info.lstrip('/'), self.map.charset,\n                                     safe='/:|+')),\n            suffix\n        ))"}
{"prompt": "def contains(self, name: str) -> List[str]:\n        \"\"\"Return a list of all keywords containing the given string.\n\n        >>> from hydpy.core.devicetools import Keywords\n        >>> keywords = Keywords('first_keyword', 'second_keyword',\n        ...                     'keyword_3', 'keyword_4',\n        ...                     'keyboard')\n        >>> keywords.contains('keyword')\n        ['first_keyword', 'keyword_3', 'keyword_4', 'second_keyword']\n        \"\"\"", "output": "\n        return self.keywords(name)\n\n    def __contains__(self, name: str) -> bool:\n        \"\"\"Return whether the given string contains the given keyword.\n\n        >>> from hydpy.core.devicetools import Keywords\n        >>> keywords = Keywords('first_keyword','second_keyword',\n       ...                     'keyword_3', 'keyword_4',\n       ...                     'keyboard')\n        >>> keywords.contains('keyword')\n        True\n        >>> keywords.contains('keyword_3')\n        True\n        >>> keywords.contains('keyword_4')\n        False\n        >>> keywords.contains('keyboard')\n        False\n       ", "solution": "\n        return sorted(keyword for keyword in self if name in keyword)"}
{"prompt": "async def fetch_member(self, member_id):\n        \"\"\"|coro|\n\n        Retreives a :class:`Member` from a guild ID, and a member ID.\n\n        .. note::\n\n            This method is an API call. For general usage, consider :meth:`get_member` instead.\n\n        Parameters\n        -----------\n        member_id: :class:`int`\n            The member's ID to fetch from.\n\n        Raises\n        -------\n        Forbidden\n            You do not have access to the guild.\n        HTTPException\n            Getting the guild failed.\n\n        Returns\n        --------\n        :class:`Member`\n            The member from the member ID.\n        \"\"\"", "output": " member's data.\n        \"\"\"\n        await self.http.delete_member(member_id)\n        return await self.http.get_member(member_id)\n\n    async def get_members(self, *, member_id: int):\n        \"\"\"|coro|\n\n        Retrieves a :class:`Member` from a guild ID, and a member ID.\n\n       .. note::\n\n            This method is an API call. For general usage, consider :meth:`get_guild_members` instead.\n\n        Parameters\n        -----------\n        member_id: :class:`int`\n            The guild ID to fetch members from", "solution": "\n        data = await self._state.http.get_member(self.id, member_id)\n        return Member(data=data, state=self._state, guild=self)"}
{"prompt": "def _add_delta_tdi(self, other):\n        \"\"\"\n        Parameters\n        ----------\n        other : TimedeltaArray or ndarray[timedelta64]\n\n        Returns\n        -------\n        result : ndarray[int64]\n        \"\"\"", "output": "\n        if isinstance(other, TimedeltaArray):\n            return other._data.view('i8') + other._data\n        elif is_timedelta64_dtype(other):\n            return other + self._shift\n\n        return NotImplemented\n\n    def _sub_datelike(self, other):\n        \"\"\"\n        Parameters\n        ----------\n        other : TimedeltaArray or ndarray[timedelta64]\n\n        Returns\n        -------\n        result : ndarray[int64]\n        \"\"\"\n        if isinstance(other, TimedeltaArray):\n            return other - self._data\n        elif is_timedelta64_dtype(other):\n            return other - self._shift\n        return NotImplemented", "solution": "\n        assert isinstance(self.freq, Tick)  # checked by calling function\n\n        delta = self._check_timedeltalike_freq_compat(other)\n        return self._addsub_int_array(delta, operator.add).asi8"}
{"prompt": "def get_wulff_shape(self, material_id):\n        \"\"\"\n        Constructs a Wulff shape for a material.\n\n        Args:\n            material_id (str): Materials Project material_id, e.g. 'mp-123'.\n        Returns:\n            pymatgen.analysis.wulff.WulffShape\n        \"\"\"", "output": "\n        return WulffShape(material_id, self.name, self.version, self.author, self.version_comment, self.url, self.name_comment)\n\n    def __str__(self):\n        return self.name\n\n    def __repr__(self):\n        return self.name\n\n    def __eq__(self, other):\n        return self.name == other.name\n\n    def __ne__(self, other):\n        return self.name!= other.name\n\n    def __hash__(self):\n        return self.name.__hash__()\n\n    def __eq__(self, other):\n        return self.", "solution": "\n        from pymatgen.symmetry.analyzer import SpacegroupAnalyzer\n        from pymatgen.analysis.wulff import WulffShape, hkl_tuple_to_str\n\n        structure = self.get_structure_by_material_id(material_id)\n        surfaces = self.get_surface_data(material_id)[\"surfaces\"]\n        lattice = (SpacegroupAnalyzer(structure)\n                   .get_conventional_standard_structure().lattice)\n        miller_energy_map = {}\n        for surf in surfaces:\n            miller = tuple(surf[\"miller_index\"])\n            # Prefer reconstructed surfaces, which have lower surface energies.\n            if (miller not in miller_energy_map) or surf[\"is_reconstructed\"]:\n                miller_energy_map[miller] = surf[\"surface_energy\"]\n        millers, energies = zip(*miller_energy_map.items())\n        return WulffShape(lattice, millers, energies)"}
{"prompt": "def extract_tables(sql):\n    \"\"\"Extract the table names from an SQL statment.\n    Returns a list of (schema, table, alias) tuples\n    \"\"\"", "output": "\n    return [\n        (\n            schema,\n            table,\n            alias\n        )\n        for schema, table, alias in sql.items()\n        if alias.startswith('pg_')\n    ]\n\n\nclass PGTable(object):\n    \"\"\"Class for handling PostgreSQL table names.\n    \"\"\"\n\n    def __init__(self, name, columns, schema=None):\n        self.name = name\n        self.columns = columns\n        self.schema = schema\n        self.table = None\n\n    def __repr__(self):\n        return self.name\n\n    def __str__(self):\n        if self.schema:\n            return self.schema +''", "solution": "\n    parsed = sqlparse.parse(sql)\n    if not parsed:\n        return []\n\n    # INSERT statements must stop looking for tables at the sign of first\n    # Punctuation. eg: INSERT INTO abc (col1, col2) VALUES (1, 2)\n    # abc is the table name, but if we don't stop at the first lparen, then\n    # we'll identify abc, col1 and col2 as table names.\n    insert_stmt = parsed[0].token_first().value.lower() == 'insert'\n    stream = extract_from_part(parsed[0], stop_at_punctuation=insert_stmt)\n    return list(extract_table_identifiers(stream))"}
{"prompt": "def plot_transaction_rate_heterogeneity(\n    model,\n    suptitle=\"Heterogeneity in Transaction Rate\",\n    xlabel=\"Transaction Rate\",\n    ylabel=\"Density\",\n    suptitle_fontsize=14,\n    **kwargs\n):\n    \"\"\"\n    Plot the estimated gamma distribution of lambda (customers' propensities to purchase).\n\n    Parameters\n    ----------\n    model: lifetimes model\n        A fitted lifetimes model, for now only for BG/NBD\n    suptitle: str, optional\n        Figure suptitle\n    xlabel: str, optional\n        Figure xlabel\n    ylabel: str, optional\n        Figure ylabel\n    kwargs\n        Passed into the matplotlib.pyplot.plot command.\n\n    Returns\n    -------\n    axes: matplotlib.AxesSubplot\n\n    \"\"\"", "output": " optional\n        Figure ylabel\n    suptitle_fontsize: int, optional\n        Font size for the suptitle\n    **kwargs: dict, optional\n        Additional keyword arguments\n\n    Returns\n    -------\n    None\n    \"\"\"\n    if model.fit_method == \"BGL\":\n        if \"alpha\" in kwargs:\n            alpha = kwargs[\"alpha\"]\n            kwargs[\"alpha\"] = alpha / (alpha + 1)\n        if \"beta\" in kwargs:\n            beta = kwargs[\"beta\"]\n            kwargs[\"beta\"] = beta / (beta + 1)\n        if \"gamma\" in kwargs:\n            gamma = kwargs[\"gamma\"]\n            kwargs[\"gamma\"]", "solution": "\n    from matplotlib import pyplot as plt\n\n    r, alpha = model._unload_params(\"r\", \"alpha\")\n    rate_mean = r / alpha\n    rate_var = r / alpha ** 2\n\n    rv = stats.gamma(r, scale=1 / alpha)\n    lim = rv.ppf(0.99)\n    x = np.linspace(0, lim, 100)\n\n    fig, ax = plt.subplots(1)\n    fig.suptitle(\"Heterogeneity in Transaction Rate\", fontsize=suptitle_fontsize, fontweight=\"bold\")\n\n    ax.set_title(\"mean: {:.3f}, var: {:.3f}\".format(rate_mean, rate_var))\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n\n    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.plot(x, rv.pdf(x), **kwargs)\n    return ax"}
{"prompt": "def moments(data, n_neighbors=30, n_pcs=30, mode='connectivities', method='umap', metric='euclidean', use_rep=None,\n            recurse_neighbors=False, renormalize=False, copy=False):\n    \"\"\"Computes moments for velocity estimation.\n\n    Arguments\n    ---------\n    data: :class:`~anndata.AnnData`\n        Annotated data matrix.\n    n_neighbors: `int` (default: 30)\n        Number of neighbors to use.\n    n_pcs: `int` (default: 30)\n        Number of principal components to use.\n    mode: `'connectivities'` or `'distances'`  (default: `'connectivities'`)\n        Distance metric to use for moment computation.\n    renormalize: `bool` (default: `False`)\n        Renormalize the moments by total counts per cell to its median.\n    copy: `bool` (default: `False`)\n        Return a copy instead of writing to adata.\n\n    Returns\n    -------\n    Returns or updates `adata` with the attributes\n    Ms: `.layers`\n        dense matrix with first order moments of spliced counts.\n    Mu: `.layers`\n        dense matrix with first order moments of unspliced counts.\n    \"\"\"", "output": " to use.\n    mode: `str` (default: 'connectivities')\n        Mode to compute the moments.\n    method: `str` (default: 'umap')\n        Method to compute the moments.\n    metric: `str` (default: 'euclidean')\n        Metric to compute the moments.\n    use_rep: `bool` (default: False)\n        Whether to use the repulsion matrix.\n    recurse_neighbors: `bool` (default: False)\n        Whether to recurse the neighbors.\n    renormalize: `bool` (default: False)\n        Whether to renormalize the data matrix", "solution": "\n    adata = data.copy() if copy else data\n\n    if 'spliced' not in adata.layers.keys() or 'unspliced' not in adata.layers.keys():\n        raise ValueError('Could not find spliced / unspliced counts.')\n    if any([not_yet_normalized(adata.layers[layer]) for layer in {'spliced', 'unspliced'}]):\n        normalize_per_cell(adata)\n    if 'neighbors' not in adata.uns.keys() or neighbors_to_be_recomputed(adata, n_neighbors=n_neighbors):\n        if use_rep is None: use_rep = 'X_pca'\n        neighbors(adata, n_neighbors=n_neighbors, use_rep=use_rep, n_pcs=n_pcs, method=method, metric=metric)\n    if mode not in adata.uns['neighbors']:\n        raise ValueError('mode can only be \\'connectivities\\' or \\'distances\\'')\n\n    logg.info('computing moments based on ' + str(mode), r=True)\n\n    connectivities = get_connectivities(adata, mode, n_neighbors=n_neighbors, recurse_neighbors=recurse_neighbors)\n\n    adata.layers['Ms'] = csr_matrix.dot(connectivities, csr_matrix(adata.layers['spliced'])).astype(np.float32).A\n    adata.layers['Mu'] = csr_matrix.dot(connectivities, csr_matrix(adata.layers['unspliced'])).astype(np.float32).A\n    if renormalize: normalize_per_cell(adata, layers={'Ms', 'Mu'}, enforce=True)\n\n    logg.info('    finished', time=True, end=' ' if settings.verbosity > 2 else '\\n')\n    logg.hint(\n        'added \\n'\n        '    \\'Ms\\' and \\'Mu\\', moments of spliced/unspliced abundances (adata.layers)')\n    return adata if copy else None"}
{"prompt": "def effective_balance(self, address: Address, block_identifier: BlockSpecification) -> Balance:\n        \"\"\" The user's balance with planned withdrawals deducted. \"\"\"", "output": "\n        return self.get_balance(address, planned=True)\n\n    def get_balance(self, address: Address, planned: bool) -> Balance:\n        \"\"\" Get the planned balance of the user. \"\"\"\n        return self.get_balance(address, balance_type=BalanceType.PLAN, planned=planned)\n\n    def get_balance_type(self, address: Address) -> BalanceType:\n        \"\"\" Get the balance type of the user. \"\"\"\n        return self.get_balance(address, balance_type=BalanceType.DEDUCT)\n\n    def get_balance", "solution": "\n        fn = getattr(self.proxy.contract.functions, 'effectiveBalance')\n        balance = fn(address).call(block_identifier=block_identifier)\n\n        if balance == b'':\n            raise RuntimeError(f\"Call to 'effectiveBalance' returned nothing\")\n\n        return balance"}
{"prompt": "def equals(self, other):\n        \"\"\"\n        Equality of two SSAFiles.\n\n        Compares :attr:`SSAFile.info`, :attr:`SSAFile.styles` and :attr:`SSAFile.events`.\n        Order of entries in OrderedDicts does not matter. \"ScriptType\" key in info is\n        considered an implementation detail and thus ignored.\n\n        Useful mostly in unit tests. Differences are logged at DEBUG level.\n\n        \"\"\"", "output": "\n        if self.info.info.mode == 'SCRIPT':\n            return self.styles == other.styles\n        elif self.info.info.mode == 'STYLE':\n            return self.events == other.events\n        else:\n            raise ValueError('Cannot compare SSAFiles with mode: %s' %\n                             self.info.info.mode)\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __eq__(self, other):\n        \"\"\"\n        Equality of two SSAFiles.\n\n        Compares :attr:`SSAFile.styles` and :attr:`SSA", "solution": "\n\n        if isinstance(other, SSAFile):\n            for key in set(chain(self.info.keys(), other.info.keys())) - {\"ScriptType\"}:\n                sv, ov = self.info.get(key), other.info.get(key)\n                if sv is None:\n                    logging.debug(\"%r missing in self.info\", key)\n                    return False\n                elif ov is None:\n                    logging.debug(\"%r missing in other.info\", key)\n                    return False\n                elif sv != ov:\n                    logging.debug(\"info %r differs (self=%r, other=%r)\", key, sv, ov)\n                    return False\n\n            for key in set(chain(self.styles.keys(), other.styles.keys())):\n                sv, ov = self.styles.get(key), other.styles.get(key)\n                if sv is None:\n                    logging.debug(\"%r missing in self.styles\", key)\n                    return False\n                elif ov is None:\n                    logging.debug(\"%r missing in other.styles\", key)\n                    return False\n                elif sv != ov:\n                    for k in sv.FIELDS:\n                        if getattr(sv, k) != getattr(ov, k): logging.debug(\"difference in field %r\", k)\n                    logging.debug(\"style %r differs (self=%r, other=%r)\", key, sv.as_dict(), ov.as_dict())\n                    return False\n\n            if len(self) != len(other):\n                logging.debug(\"different # of subtitles (self=%d, other=%d)\", len(self), len(other))\n                return False\n\n            for i, (se, oe) in enumerate(zip(self.events, other.events)):\n                if not se.equals(oe):\n                    for k in se.FIELDS:\n                        if getattr(se, k) != getattr(oe, k): logging.debug(\"difference in field %r\", k)\n                    logging.debug(\"event %d differs (self=%r, other=%r)\", i, se.as_dict(), oe.as_dict())\n                    return False\n\n            return True\n        else:\n            raise TypeError(\"Cannot compare to non-SSAFile object\")"}
{"prompt": "def set_style(style, mpl=False, **kwargs):\n    \"\"\"\n    If mpl is False accept either style name or a TStyle instance.\n    If mpl is True accept either style name or a matplotlib.rcParams-like\n    dictionary\n    \"\"\"", "output": "\n    if mpl:\n        if isinstance(style, TStyle):\n            style = style.copy()\n        else:\n            raise TypeError(\"Invalid argument for set_style\")\n    else:\n        style = TStyle(style)\n    return _set_style(style, mpl=mpl, **kwargs)\n\n\ndef _set_style(style, mpl=False, **kwargs):\n    \"\"\"\n    Set the style of the given name to the given matplotlib rc setting.\n    If mpl is False accept either name or rcParams name.\n    \"\"\"\n    if mpl:\n        if isinstance(style, TStyle):\n            style = style.", "solution": "\n    if mpl:\n        import matplotlib as mpl\n\n        style_dictionary = {}\n        if isinstance(style, string_types):\n            style_dictionary = get_style(style, mpl=True, **kwargs)\n            log.info(\"using matplotlib style '{0}'\".format(style))\n        elif isinstance(style, dict):\n            style_dictionary = style\n            log.info(\"using user-defined matplotlib style\")\n        else:\n            raise TypeError(\"style must be a matplotlib style name or dict\")\n        for k, v in style_dictionary.items():\n            mpl.rcParams[k] = v\n    else:\n        if isinstance(style, string_types):\n            style = get_style(style, **kwargs)\n        log.info(\"using ROOT style '{0}'\".format(style.GetName()))\n        style.cd()"}
{"prompt": "def delete_object(self, mpath):\n        \"\"\"DeleteObject\n        https://apidocs.joyent.com/manta/api.html#DeleteObject\n\n        @param mpath {str} Required. A manta path, e.g. '/trent/stor/myobj'.\n        \"\"\"", "output": "\n        self.mpath = mpath\n        self.path = mpath\n        self.name = self.path.split('/')[-1]\n        self.path = mpath\n        self.name = self.path.split('/')[-1].replace('/', '_')\n        self.path = mpath\n        self.path = mpath\n        self.path = mpath.replace('/', '_')\n        self.path = mpath.replace('/', '_')\n        self.path = mpath.replace('/', '_')\n        self.path = mpath.replace('/', '_')\n        self.path = mpath.replace('/', '_')\n        self.path = mpath.", "solution": "\n        log.debug('DeleteObject %r', mpath)\n        res, content = self._request(mpath, \"DELETE\")\n        if res[\"status\"] != \"204\":\n            raise errors.MantaAPIError(res, content)\n        return res"}
{"prompt": "def dedent(self, node, dirty=True):\n        \"\"\"Dedent an item. Does nothing if the target is not indented under this item.\n\n        Args:\n            node (gkeepapi.node.ListItem): Item to dedent.\n            dirty (bool): Whether this node should be marked dirty.\n        \"\"\"", "output": "\n        if not dirty:\n            node.text = self.text\n        else:\n            node.text = self.text + \"\\n\" + self.text\n        node.text = self.text\n        node.is_dirty = True\n        node.is_open = True\n        node.is_dirty = True\n\n    def insert_child(self, node, index):\n        \"\"\"Insert a child. Does nothing if the target is already a child.\n\n        Args:\n            node (gkeepapi.node.ListItem): Child to insert.\n            index (int): Index of the child to insert.\n        \"\"\"\n        if", "solution": "\n        if node.id not in self._subitems:\n            return\n\n        del self._subitems[node.id]\n        node.super_list_item_id = None\n        node.parent_item = None\n        if dirty:\n            node.touch(True)"}
{"prompt": "def _process_phenotype_cvterm(self):\n        \"\"\"\n        These are the qualifiers for the phenotype location itself.\n        But are just the qualifiers.\n        The actual \"observable\" part of the phenotype is only in\n        the phenotype table. These get added to a lookup variable used to\n        augment a phenotype association statement.\n        :return:\n\n        \"\"\"", "output": "\n        return self._get_observable_part(self.get_observable_id_from_lookup_variable())\n\n    def get_observable_id_from_lookup_variable(self):\n        \"\"\"\n        :param self:\n        :param lookup_variable:\n        :return:\n        \"\"\"\n        return self.get_lookup_variable_observable_id()\n\n    def get_lookup_variable_observable_id(self):\n        \"\"\"\n        :param self:\n        :param lookup_variable:\n        :return:\n        \"\"\"\n        return self.get_lookup_variable_observable_id_", "solution": "\n\n        line_counter = 0\n        raw = '/'.join((self.rawdir, 'phenotype_cvterm'))\n        LOG.info(\"processing phenotype cvterm mappings\")\n\n        with open(raw, 'r') as f:\n            f.readline()  # read the header row; skip\n            filereader = csv.reader(f, delimiter='\\t', quotechar='\\\"')\n            for line in filereader:\n                line_counter += 1\n\n                (phenotype_cvterm_id, phenotype_id, cvterm_id, rank) = line\n\n                # 4532\t8507\t60793\t0\n                # 4533\t8513\t60830\t0\n\n                # add the internal genotype to pub mapping\n                phenotype_key = phenotype_id\n                cvterm_key = cvterm_id\n                phenotype_id = self.idhash['phenotype'][phenotype_key]\n                if cvterm_key in self.idhash['cvterm']:\n                    cvterm_id = self.idhash['cvterm'][cvterm_key]\n                    if phenotype_key not in self.phenocv:\n                        self.phenocv[phenotype_id] = set()\n                    self.phenocv[phenotype_id].add(cvterm_id)\n                else:\n                    LOG.info(\"Not storing the cvterm info for %s\", cvterm_key)\n\n        return"}
{"prompt": "def SInt64(value, min_value=None, max_value=None, encoder=ENC_INT_DEFAULT, fuzzable=True, name=None, full_range=False):\n    \"\"\"Signed 64-bit field\"\"\"", "output": "\n    if min_value is None:\n        min_value = value\n    if max_value is None:\n        max_value = value\n    if value < min_value or value > max_value:\n        raise ValueError(\"Value out of range: %s\" % full_range)\n    if encoder is None:\n        encoder = int\n    if fuzzable:\n        return encoder(value)\n    else:\n        return int(value)\n\n\nclass SInt32(SInt64):\n    \"\"\"Signed 32-bit integer field\"\"\"\n    def __init__(self, value, encoder=ENC_INT_DEFAULT, fu", "solution": "\n    return BitField(value, 64, signed=True, min_value=min_value, max_value=max_value, encoder=encoder, fuzzable=fuzzable, name=name, full_range=full_range)"}
{"prompt": "def delete_resource(self, resource_id):\n        \"\"\"\n        Remove a specific resource by its identifier.\n        \"\"\"", "output": "\n        self.delete_resource_by_id(resource_id)\n\n    def delete_resource_by_id(self, resource_id):\n        \"\"\"\n        Delete a specific resource by its identifier.\n        \"\"\"\n        self.delete_resource(resource_id)\n\n    def delete_resource_by_name(self, name):\n        \"\"\"\n        Delete a specific resource by its name.\n        \"\"\"\n        self.delete_resource_by_name(name)\n\n    def delete_resource_by_name(self, name):\n        \"\"\"\n        Delete a specific resource by its name.\n        \"\"\"\n        self.delete_resource", "solution": "\n        # resource_id could be a path such as '/asset/123' so quote\n        uri = self._get_resource_uri(guid=resource_id)\n        return self.service._delete(uri)"}
{"prompt": "def _getScalesRand(self):\n        \"\"\"\n        Internal function for parameter initialization\n        Return a vector of random scales\n        \"\"\"", "output": "\n        return self.randomScales\n\n    def _setScalesRand(self, randomScales):\n        \"\"\"\n        Internal function for parameter update\n        \"\"\"\n        self.randomScales = randomScales\n\n    def _getSlope(self):\n        \"\"\"\n        Internal function for parameter initialization\n        Return a random slope\n        \"\"\"\n        return random.uniform(0, 1)\n\n    def _setSlope(self, slope):\n        \"\"\"\n        Internal function for parameter update\n        \"\"\"\n        self.randomScales = randomScales\n\n    def _getRandomScale(self):\n        \"\"\"\n        Internal function for parameter random scale\n        Return a", "solution": "\n        if self.P>1:\n            scales = []\n            for term_i in range(self.n_randEffs):\n                _scales = sp.randn(self.diag[term_i].shape[0])\n                if self.jitter[term_i]>0:\n                    _scales = sp.concatenate((_scales,sp.array([sp.sqrt(self.jitter[term_i])])))\n                scales.append(_scales)\n            scales = sp.concatenate(scales)\n        else:\n            scales=sp.randn(self.vd.getNumberScales())\n        return scales"}
{"prompt": "def read_resource_list(self, uri):\n        \"\"\"Read resource list from specified URI else raise exception.\"\"\"", "output": "\n        try:\n            return self.get_resource_list(uri)\n        except Exception as e:\n            raise e\n\n    def get_resource_list(self, uri):\n        \"\"\"Get list of resources from specified URI.\"\"\"\n        try:\n            return self.resource_list[uri]\n        except Exception as e:\n            raise e\n\n    def get_resource_list_from_uri(self, uri):\n        \"\"\"Get list of resources from specified URI.\"\"\"\n        try:\n            return self.resource_list[uri.split('/')[-1]]\n        except Exception as e:\n            raise e\n\n    def get_resource_list_", "solution": "\n        self.logger.info(\"Reading resource list %s\" % (uri))\n        try:\n            resource_list = ResourceList(allow_multifile=self.allow_multifile,\n                                         mapper=self.mapper)\n            resource_list.read(uri=uri)\n        except Exception as e:\n            raise ClientError(\"Can't read source resource list from %s (%s)\" %\n                              (uri, str(e)))\n        self.logger.debug(\"Finished reading resource list\")\n        return(resource_list)"}
{"prompt": "def get_score(self, member, default=None, pipe=None):\n        \"\"\"\n        Return the score of *member*, or *default* if it is not in the\n        collection.\n        \"\"\"", "output": "\n        return self.scores.get(member, default)\n\n    def get_score_display(self, pipe):\n        \"\"\"\n        Return the score display value for *pipe* if it is not in the\n        collection.\n        \"\"\"\n        return self.scores.get(pipe, None)\n\n    def get_score_display_value(self, pipe_value):\n        \"\"\"\n        Return the score display value for *pipe_value*, or *default* if\n        it is not in the collection.\n        \"\"\"\n        return self.scores.get(pipe_value, None)\n\n    def get_score_description(self,", "solution": "\n        pipe = self.redis if pipe is None else pipe\n        score = pipe.zscore(self.key, self._pickle(member))\n\n        if (score is None) and (default is not None):\n            score = float(default)\n\n        return score"}
{"prompt": "def get_one_meth_type(self, mtype, method_list):\n        \"\"\"\n        Get all codes of one type (i.e., 'anisotropy_estimation')\n        \"\"\"", "output": "\n        if mtype == 'anisotropy_estimation':\n            return self.code_is_estimation(method_list)\n        elif mtype == 'code_is_estimation':\n            return self.code_is_estimation(method_list)\n        elif mtype == 'code_is_estimation':\n            return self.code_is_estimation(method_list)\n        elif mtype == 'code_is_estimation':\n            return self.code_is_estimation(method_list)\n        else:\n            raise ValueError('Invalid mtype: %s' % mtype)\n\n    def code", "solution": "\n        cond = method_list['dtype'] == mtype\n        codes = method_list[cond]\n        return codes"}
{"prompt": "def read_value(self, varname, path=\"/\", cmode=None, default=NO_DEFAULT):\n        \"\"\"\n        Returns the values of variable with name varname in the group specified by path.\n\n        Args:\n            varname: Name of the variable\n            path: path to the group.\n            cmode: if cmode==\"c\", a complex ndarrays is constructed and returned\n                (netcdf does not provide native support from complex datatype).\n            default: returns default if varname is not present.\n                self.Error is raised if default is default is NO_DEFAULT\n\n        Returns:\n            numpy array if varname represents an array, scalar otherwise.\n        \"\"\"", "output": " otherwise numpy array.\n        \"\"\"\n        if cmode is None:\n            cmode = self.default_cmode\n        try:\n            return np.array(self.group_variables[path][varname])\n        except KeyError:\n            if default is NO_DEFAULT:\n                return np.array(self.group_variables[path][varname])\n            else:\n                raise self.Error(\"No group variable %s\" % varname)\n\n    def get_value(self, varname, path=\"/\", cmode=None, default=NO_DEFAULT):\n        \"\"\"\n        Returns the values of variable with name varname in the group specified by", "solution": "\n        try:\n            var = self.read_variable(varname, path=path)\n        except self.Error:\n            if default is NO_DEFAULT: raise\n            return default\n\n        if cmode is None:\n            # scalar or array\n            # getValue is not portable!\n            try:\n                return var.getValue()[0] if not var.shape else var[:]\n            except IndexError:\n                return var.getValue() if not var.shape else var[:]\n\n        else:\n            assert var.shape[-1] == 2\n            if cmode == \"c\":\n                return var[...,0] + 1j*var[...,1]\n            else:\n                raise ValueError(\"Wrong value for cmode %s\" % cmode)"}
{"prompt": "def solve_each(expr, vars):\n    \"\"\"Return True if RHS evaluates to a true value with each state of LHS.\n\n    If LHS evaluates to a normal IAssociative object then this is the same as\n    a regular let-form, except the return value is always a boolean. If LHS\n    evaluates to a repeared var (see efilter.protocols.repeated) of\n    IAssociative objects then RHS will be evaluated with each state and True\n    will be returned only if each result is true.\n    \"\"\"", "output": "\n    if expr.is_repeating:\n        return True\n    if expr.is_normal:\n        return False\n    if expr.is_repeating:\n        return True\n    if expr.is_repeated:\n        return True\n    if expr.is_normal:\n        return True\n    if expr.is_repeating:\n        return False\n    if expr.is_repeated:\n        return True\n    return False\n\n\ndef efilter_protocol_is_in(p, p_type):\n    \"\"\"Return True if the given protocol is in the given list of protocols.\n\n    If p_type is a list of protocols,", "solution": "\n    lhs_values, _ = __solve_for_repeated(expr.lhs, vars)\n\n    for lhs_value in repeated.getvalues(lhs_values):\n        result = solve(expr.rhs, __nest_scope(expr.lhs, vars, lhs_value))\n        if not result.value:\n            # Each is required to return an actual boolean.\n            return result._replace(value=False)\n\n    return Result(True, ())"}
{"prompt": "def predict(self, x, add_intercept=False):\n        \"\"\"For some x, predict the bn(x) for each base\n\n        Arguments:\n            x: np.array; Vector of dimension 1\n            add_intercept: bool; should we add the intercept to the final array\n\n        Returns:\n            np.array, of shape (len(x), n_bases + (add_intercept))\n        \"\"\"", "output": "\n        if add_intercept:\n            return self.add_intercept(x)\n        else:\n            return self.predict_proba(x)\n\n    def add_intercept(self, x):\n        \"\"\"Add a column of ones to the linear model\n\n        Arguments:\n            x: np.array; Vector of dimension n_bases\n\n        Returns:\n            np.array; Column of ones\n        \"\"\"\n        return x + np.random.normal(0, self.n_classes) * self.intercept_alpha\n\n    def predict_proba(self, x, add_intercept=False):\n        \"\"\"For some x, predict the pro", "solution": "\n        # sanity check\n        if x.min() < self.start:\n            raise Warning(\"x.min() < self.start\")\n        if x.max() > self.end:\n            raise Warning(\"x.max() > self.end\")\n\n        return get_X_spline(x=x,\n                            knots=self.knots,\n                            n_bases=self.n_bases,\n                            spline_order=self.spline_order,\n                            add_intercept=add_intercept)"}
{"prompt": "def removeCMSPadding(str, blocksize=AES_blocksize):\n  \"\"\"CMS padding: Remove padding with bytes containing the number of padding bytes \"\"\"", "output": "\n  return _core_.AES_removeCMSPadding(str, blocksize)\n\ndef removeCMS(str, pos=0):\n  \"\"\"\n    Removes the specified string from the end of the string,\n    where pos is the position of the end of the string.\n    \"\"\"\n  return _core_.AES_removeCMS(str, pos)\n\ndef removeHuffmanPadding(str, pad, pos=0):\n  \"\"\"\n    Removes the specified string from the end of the string,\n    where pos is the position of the end of the string.\n    \"\"\"\n  return _core_.AES_removeHuffmanPadding", "solution": "\n  try:\n    pad_len = ord(str[-1]) # last byte contains number of padding bytes\n  except TypeError:\n    pad_len = str[-1]\n  assert pad_len <= blocksize, 'padding error' \n  assert pad_len <= len(str), 'padding error'\n    \n  return str[:-pad_len]"}
{"prompt": "def _set_gos_stats(self, v, load=False):\n    \"\"\"\n    Setter method for gos_stats, mapped from YANG variable /cpu_interface_state/gos_stats (container)\n    If this variable is read-only (config: false) in the\n    source YANG file, then _set_gos_stats is considered as a private\n    method. Backends looking to populate this variable should\n    do so via calling thisObj._set_gos_stats() directly.\n    \"\"\"", "output": "\n    if hasattr(v, \"_utype\"):\n        v = v._utype(v)\n    try:\n        t = YANGDynClass(\n            v,\n            base=stats.GosStats,\n            is_container=\"container\",\n            yang_name=\"gos-stats\",\n            parent=self,\n            path_helper=self._path_helper,\n            extmethods=self._extmethods,\n            register_paths=True,\n            namespace=\"http://openconfig.net/yang/network-instance\",\n            defining_module=\"openconfig-network-instance\",\n            yang_type=\"container\",\n            is_", "solution": "\n    if hasattr(v, \"_utype\"):\n      v = v._utype(v)\n    try:\n      t = YANGDynClass(v,base=gos_stats.gos_stats, is_container='container', presence=False, yang_name=\"gos-stats\", rest_name=\"gos-stats\", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'callpoint': u'sysdiag-stats-gos-stats-1'}}, namespace='urn:brocade.com:mgmt:brocade-sysdiag-operational', defining_module='brocade-sysdiag-operational', yang_type='container', is_config=False)\n    except (TypeError, ValueError):\n      raise ValueError({\n          'error-string': "}
{"prompt": "def describe_token_map(self, ):\n    \"\"\"\n    get the mapping between token->node ip\n    without taking replication into consideration\n    https://issues.apache.org/jira/browse/CASSANDRA-4092\n    \"\"\"", "output": "\n    return dict(self.cassandra_server_list)\n\n  def get_keyspace_names(self, ksname=None):\n    if ksname is None:\n      ksname = self.conn.keyspace\n\n    if ksname in ksname_to_value:\n      return ksname_to_value[ksname]\n    else:\n      return []\n\n  def get_keyspace_meta(self, keyspace_name):\n    ksmeta = self.get_keyspace_meta_dict()\n    ksmeta[keyspace_name] = keyspace_name\n    return ksmeta\n\n  def get_keyspace_meta_dict(", "solution": "\n    self._seqid += 1\n    d = self._reqs[self._seqid] = defer.Deferred()\n    self.send_describe_token_map()\n    return d"}
{"prompt": "def get_rating_id(self):\n        \"\"\"Gets the ``Id`` of the ``Grade``.\n\n        return: (osid.id.Id) - the ``Agent``  ``Id``\n        raise:  IllegalState - ``has_rating()`` is ``false``\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"", "output": "\n        return  # osid.id.Id\n\n    rating_id = property(fget=get_rating_id)\n\n    def get_agent_id_for_gradebook_column(self, gradebook_column_id):\n        \"\"\"Gets the ``Id`` of the ``Agent`` for the given gradebook column.\n\n        arg:    gradebook_column_id (osid.id.Id): the ``Id`` of the ``GradebookColumn``\n        return: (osid.id.Id) - the ``Agent``  ``Id``\n        raise:  IllegalState - ``has", "solution": "\n        # Implemented from template for osid.resource.Resource.get_avatar_id_template\n        if not bool(self._my_map['ratingId']):\n            raise errors.IllegalState('this Comment has no rating')\n        else:\n            return Id(self._my_map['ratingId'])"}
{"prompt": "def download(date_array, tag, sat_id, data_path=None, user=None, password=None):\n    \"\"\"Downloads data from Madrigal.\n    \n    The user's names should be provided in field user. John Malkovich should be \n    entered as John+Malkovich \n    \n    The password field should be the user's email address. These parameters\n    are passed to Madrigal when downloading.\n    \n    The affiliation field is set to pysat to enable tracking of pysat downloads.\n    \n    Parameters\n    ----------\n    \n    \n    \"\"\"", "output": "\n    # Check if the data path is given\n    if data_path is None:\n        data_path = get_data_path()\n    # Check if the user and password are given\n    if user is None:\n        user = getpass.getuser()\n    if password is None:\n        password = getpass.getpass()\n    # Download the data\n    url = get_data_url(date_array, tag, sat_id)\n    # Download the affiliation\n    try:\n        response = urllib2.urlopen(url)\n    except urllib2.HTTPError, e:\n        print \"Error: %s\" % e.", "solution": "\n    import subprocess\n    \n    # currently passes things along if no user and password supplied\n    # need to do this for testing\n    # TODO, implement user and password values in test code\n    # specific to DMSP\n    if user is None:\n        print ('No user information supplied for download.')\n        user = 'pysat_testing'\n    if password is None:\n        print ('Please provide email address in password field.')\n        password = 'pysat_testing@not_real_email.org'\n\n    a = subprocess.check_output([\"globalDownload.py\", \"--verbose\", \n                    \"--url=http://cedar.openmadrigal.org\",\n                    '--outputDir='+data_path,\n                    '--user_fullname='+user,\n                    '--user_email='+password,\n                    '--user_affiliation=pysat',\n                    '--format=hdf5',\n                    '--startDate='+date_array[0].strftime('%m/%d/%Y'),\n                    '--endDate='+date_array[-1].strftime('%m/%d/%Y'),\n                    '--inst=8100',\n                    '--kindat='+str(madrigal_tag[sat_id])])\n    print ('Feedback from openMadrigal ', a)"}
{"prompt": "def update(self):\n        \"\"\"\n        Updates the non-primitive objects needed for solution, using primitive\n        attributes.  This includes defining a \"utility from conformity\" function\n        conformUtilityFunc, a grid of population punk proportions, and an array\n        of future punk proportions (for each value in the grid).  Results are\n        stored as attributes of self.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        none\n        \"\"\"", "output": "\n        self.attributes = {}\n        for key in self.population:\n            self.attributes[key] = self.population[key]\n        self.attributes['utility'] = self.utilityFunc.update()\n        return self\n\n    def get_attributes(self):\n        \"\"\"\n        Returns the attributes of the solution.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        dict\n        \"\"\"\n        return self.attributes\n\n    def get_pops(self):\n        \"\"\"\n        Returns the pops of the solution.\n\n        Parameters\n        ----------\n        none\n\n        Returns\n        -------\n        list\n        \"\"\"\n        return self.pops\n\n    def get_value", "solution": "\n        self.conformUtilityFunc = lambda x : stats.beta.pdf(x,self.uParamA,self.uParamB)\n        self.pGrid = np.linspace(0.0001,0.9999,self.pCount)\n        self.updateEvolution()"}
{"prompt": "def rotate_texture(texture, rotation, x_offset=0.5, y_offset=0.5):\n    \"\"\"Rotates the given texture by a given angle.\n\n    Args:\n        texture (texture): the texture to rotate\n        rotation (float): the angle of rotation in degrees\n        x_offset (float): the x component of the center of rotation (optional)\n        y_offset (float): the y component of the center of rotation (optional)\n\n    Returns:\n        texture: A texture.\n    \"\"\"", "output": "\n    return texture.rotate(rotation, x_offset, y_offset)\n\n\ndef rotate_y(rotation, x_offset=0.5, y_offset=0.5):\n    \"\"\"Rotates the given texture by a given angle.\n\n    Args:\n        rotation (float): the angle of rotation in degrees\n        x_offset (float): the x component of the center of rotation (optional)\n        y_offset (float): the y component of the center of rotation (optional)\n\n    Returns:\n        texture: A texture.\n    \"\"\"\n    return rotation.rotate(x_offset, y_offset,", "solution": "\n    x, y = texture\n    x = x.copy() - x_offset\n    y = y.copy() - y_offset\n    angle = np.radians(rotation)\n    x_rot = x * np.cos(angle) + y * np.sin(angle)\n    y_rot = x * -np.sin(angle) + y * np.cos(angle)\n    return x_rot + x_offset, y_rot + y_offset"}
{"prompt": "def getDecimalMark(self):\n        \"\"\"Return the decimal mark to be used on reports for this client\n\n        If the client has DefaultDecimalMark selected, the Default value from\n        the LIMS Setup will be returned.\n\n        Otherwise, will return the value of DecimalMark.\n        \"\"\"", "output": "\n        return self.defaultDecimalMark\n\n    def setDecimalMark(self, decimalMark):\n        \"\"\"Set the decimal mark for this report\n\n        :param decimalMark: the decimal mark to be used on reports\n        \"\"\"\n        self.defaultDecimalMark = decimalMark\n\n    def getReportType(self):\n        \"\"\"Return the type of report\n\n        :return: the type of report\n        \"\"\"\n        return self.reportType\n\n    def setReportType(self, reportType):\n        \"\"\"Set the type of report for this client\n\n        :param reportType: the type of report\n        \"\"\"\n        self.reportType = reportType\n\n    def getReport", "solution": "\n        if self.getDefaultDecimalMark() is False:\n            return self.Schema()[\"DecimalMark\"].get(self)\n        return self.bika_setup.getDecimalMark()"}
{"prompt": "def _persist(self):\n        \"\"\"\n        Run the command inside a thread so that we can catch output for each\n        line as it comes in and display it.\n        \"\"\"", "output": "\n        self.stdout.write(\"Running command: %s\\n\" % self.command)\n        self.stdout.flush()\n        self.command = self.command.strip()\n        self.stdout.write(\"Command: %s\\n\" % self.command)\n        self.stdout.write(\"Thread: %s\\n\" % self.thread)\n        self.stdout.write(\"Command: %s\\n\" % self.command)\n        self.stdout.write(\"Thread: %s\\n\" % self.thread)\n        self.stdout.flush()\n        self.stdout.write(\"", "solution": "\n        # run the block/command\n        for command in self.commands:\n            try:\n                process = Popen(\n                    [command],\n                    stdout=PIPE,\n                    stderr=PIPE,\n                    universal_newlines=True,\n                    env=self.env,\n                    shell=True,\n                )\n            except Exception as e:\n                retcode = process.poll()\n                msg = \"Command '{cmd}' {error} retcode {retcode}\"\n                self.py3.log(msg.format(cmd=command, error=e, retcode=retcode))\n\n            # persistent blocklet output can be of two forms.  Either each row\n            # of the output is on a new line this is much easier to deal with)\n            # or else the output can be continuous and just flushed when ready.\n            # The second form is more tricky, if we find newlines then we\n            # switch to easy parsing of the output.\n\n            # When we have output we store in self.persistent_output and then\n            # trigger the module to update.\n\n            fd = process.stdout.fileno()\n            fl = fcntl.fcntl(fd, fcntl.F_GETFL)\n            has_newlines = False\n            while True:\n                line = process.stdout.read(1)\n                # switch to a non-blocking read as we do not know the output\n                # length\n                fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)\n                line += process.stdout.read(1024)\n                # switch back to blocking so we can wait for the next output\n                fcntl.fcntl(fd, fcntl.F_SETFL, fl)\n                if process.poll():\n                    break\n                if self.py3.is_python_2():\n                    line = line.decode(\"utf-8\")\n                self.persistent_output = line\n                self.py3.update()\n                if line[-1] == \"\\n\":\n                    has_newlines = True\n                    break\n                if line == \"\":\n                    break\n            if has_newlines:\n                msg = \"Switch to newline persist method {cmd}\"\n                self.py3.log(msg.format(cmd=command))\n                # just read the output in a sane manner\n                for line in iter(process.stdout.readline, b\"\"):\n                    if process.poll():\n                        break\n                    if self.py3.is_python_2():\n                        line = line.decode(\"utf-8\")\n                    self.persistent_output = line\n                    self.py3.update()\n        self.py3.log(\"command exited {cmd}\".format(cmd=command))\n        self.persistent_output = \"Error\\nError\\n{}\".format(\n            self.py3.COLOR_ERROR or self.py3.COLOR_BAD\n        )\n        self.py3.update()"}
{"prompt": "def get_state_in_ec_string(self, ec_index, add_colour=True):\n        \"\"\"Get the state of the component in an execution context as a string.\n\n        @param ec_index The index of the execution context to check the state\n                        in. This index is into the total array of contexts,\n                        that is both owned and participating contexts. If the\n                        value of ec_index is greater than the length of @ref\n                        owned_ecs, that length is subtracted from ec_index and\n                        the result used as an index into @ref\n                        participating_ecs.\n\n        \"\"\"", "output": "\n        @param add_colour If True, add a colour to the result. If False, add\n                        a colour to the result.\n        @return The state of the component in an EC string.\n        \"\"\"\n        if add_colour:\n            return self.ref_state_colours[ec_index]\n        else:\n            return self.ref_state[ec_index]\n\n    def get_state_in_ec_index(self, ec_index):\n        \"\"\"Get the state of the component in an execution context as an index.\n\n        @param ec_index The index of the execution context to check the state", "solution": "\n        with self._mutex:\n            if ec_index >= len(self.owned_ecs):\n                ec_index -= len(self.owned_ecs)\n                if ec_index >= len(self.participating_ecs):\n                    raise exceptions.BadECIndexError(ec_index)\n                state = self.participating_ec_states[ec_index]\n            else:\n                state = self.owned_ec_states[ec_index]\n        if state == self.INACTIVE:\n            result = 'Inactive', ['bold', 'blue']\n        elif state == self.ACTIVE:\n            result = 'Active', ['bold', 'green']\n        elif state == self.ERROR:\n            result = 'Error', ['bold', 'white', 'bgred']\n        elif state == self.UNKNOWN:\n            result = 'Unknown', ['bold', 'red']\n        elif state == self.CREATED:\n            result = 'Created', ['reset']\n        if add_colour:\n            return utils.build_attr_string(result[1], supported=add_colour) + \\\n                    result[0] + utils.build_attr_string('reset',\n                    supported=add_colour)\n        else:\n            return result[0]"}
{"prompt": "def done(self):\n        \"\"\" Check if we should stop returning objects \"\"\"", "output": "\n        return self.state =='stopping'\n\n    def stop(self):\n        \"\"\"\n        Stop the job\n        \"\"\"\n        self.state ='stopped'\n        self.stop_time = datetime.now()\n        self.save()\n\n    def save(self):\n        \"\"\"\n        Save the job\n        \"\"\"\n        self.state ='saving'\n        self.save_time = datetime.now()\n        self.state_changed = True\n        self.save_time = datetime.now()\n        self.state = 'done'\n        self.save_time = datetime.now()\n        self.save()\n\n    def get_", "solution": "\n        if self._done:\n            return self._done\n\n        if self._limit is None:\n            self._done = False\n        elif self.itemcount >= self._limit:\n            self._done = True\n\n        return self._done"}
{"prompt": "def bulk_cache_of_tag(self, tag):\r\n        \"\"\" Caches all the symbols by a certain tag.\r\n\r\n        For now, there is no different, than \r\n        caching each symbol individually.  In the future,\r\n        this functionality could have speed improvements.\r\n\r\n        Parameters\r\n        ----------\r\n        tag : str\r\n            Use '%' to enable SQL's \"LIKE\" functionality.\r\n\r\n        Returns\r\n        -------\r\n        TrumpReport\r\n        \"\"\"", "output": "\r\n        self.cache_of_symbol_of_tag[tag] = {}\r\n        for symbol in self.symbols:\r\n            self.cache_of_symbol_of_tag[symbol][tag] = {}\r\n\r\n        for line in self.lines:\r\n            if line.tag == tag:\r\n                self.cache_of_symbol_of_tag[line.symbol][tag][line.date] = line.amount\r\n                self.cache_of_symbol_of_tag[line.symbol][tag][line.amount] = line.date.strftime('%d')\r\n                self.cache_of_symbol_", "solution": "\r\n\r\n        syms = self.search_tag(tag)\r\n        \r\n        name = 'Bulk Cache of Symbols tagged {}'.format(tag)\r\n        tr = TrumpReport(name)\r\n        for sym in syms:\r\n            sr = sym.cache()\r\n            tr.add_symbolreport(sr)\r\n        \r\n        return tr"}
{"prompt": "def active_inactive(self):\n        \"\"\"The indexes of the active and the inactive cell vectors\"\"\"", "output": "\n        return self.active_cells.keys()\n\n    def get_cell(self, row, col):\n        \"\"\"Returns the cell at the given row and column\"\"\"\n        return self.active_cells[row][col]\n\n    def get_cell_by_index(self, row, col):\n        \"\"\"Returns the active and inactive cell at the given column\"\"\"\n        return self.get_cell_by_index(row, col)\n\n    def get_cell_by_name(self, name):\n        \"\"\"Returns the active and inactive cell with the given name\"\"\"\n        return self.get_cell_by_", "solution": "\n        active_indices = []\n        inactive_indices = []\n        for index, active in enumerate(self.active):\n            if active:\n                active_indices.append(index)\n            else:\n                inactive_indices.append(index)\n        return active_indices, inactive_indices"}
{"prompt": "def get_variables_in_scope(scope, collection=tf.GraphKeys.TRAINABLE_VARIABLES):\n  \"\"\"Returns a tuple `tf.Variable`s in a scope for a given collection.\n\n  Args:\n    scope: `tf.VariableScope` or string to retrieve variables from.\n    collection: Collection to restrict query to. By default this is\n        `tf.Graphkeys.TRAINABLE_VARIABLES`, which doesn't include non-trainable\n        variables such as moving averages.\n\n  Returns:\n    A tuple of `tf.Variable` objects.\n  \"\"\"", "output": "\n  if isinstance(collection, tf.Graphkeys.TRAINABLE_VARIABLES):\n    return collection\n  elif isinstance(collection, tf.Variable):\n    return (tf.Variable, tf.Variable)\n  else:\n    raise ValueError(\"Invalid collection type: %s\" % collection)\n\n\ndef get_variables_in_scope_with_name(name, collection=None):\n  \"\"\"Returns a tuple `tf.Variable`s in a scope with the given name.\n\n  Args:\n    name: `string` name to retrieve variables from.\n    collection: Collection to restrict query to. By default this is\n        `tf", "solution": "\n  scope_name = get_variable_scope_name(scope)\n\n  if scope_name:\n    # Escape the name in case it contains any \".\" characters. Add a closing\n    # slash so we will not search any scopes that have this scope name as a\n    # prefix.\n    scope_name = re.escape(scope_name) + \"/\"\n\n  return tuple(tf.get_collection(collection, scope_name))"}
{"prompt": "def column_definition(table_name, col_name):\n    \"\"\"\n    Get the source of a column function.\n\n    If a column is a registered Series and not a function then all that is\n    returned is {'type': 'series'}.\n\n    If the column is a registered function then the JSON returned has keys\n    \"type\", \"filename\", \"lineno\", \"text\", and \"html\". \"text\" is the raw\n    text of the function, \"html\" has been marked up by Pygments.\n\n    \"\"\"", "output": "\n    try:\n        return _column_source[table_name][col_name]\n    except KeyError:\n        raise ValueError(\"No source for column '%s' in %s\" % (col_name, table_name))\n\n\ndef _get_lexer_by_name(name):\n    \"\"\"\n    Get a lexer by name.\n\n    If a name is not a registered function then the lexer returned has\n    \"type\", \"filename\", \"lineno\", \"filename\", and \"lineno\". \"type\" is the name of a\n    registered function then \"html\" has been marked up by Pygments.\n\n    If the name is", "solution": "\n    col_type = orca.get_table(table_name).column_type(col_name)\n\n    if col_type != 'function':\n        return jsonify(type=col_type)\n\n    filename, lineno, source = \\\n        orca.get_raw_column(table_name, col_name).func_source_data()\n\n    html = highlight(source, PythonLexer(), HtmlFormatter())\n\n    return jsonify(\n        type='function', filename=filename, lineno=lineno, text=source,\n        html=html)"}
{"prompt": "def bloomfilter(collection, on, column, capacity=3000, error_rate=0.01):\n    \"\"\"\n    Filter collection on the `on` sequence by BloomFilter built by `column`\n\n    :param collection:\n    :param on: sequence or column name\n    :param column: instance of Column\n    :param capacity: numbers of capacity\n    :type capacity: int\n    :param error_rate: error rate\n    :type error_rate: float\n    :return: collection\n\n    :Example:\n\n    >>> df1 = DataFrame(pd.DataFrame({'a': ['name1', 'name2', 'name3', 'name1'], 'b': [1, 2, 3, 4]}))\n    >>> df2 = DataFrame(pd.DataFrame({'a': ['name1']}))\n    >>> df1.bloom_filter('a', df2.a)\n           a  b\n    0  name1  1\n    1  name1  4\n    \"\"\"", "output": "name3'],\n   ...                    'b': ['name1', 'name2', 'name3'],\n   ...                    'c': [1, 2, 3]}, columns=['a', 'b', 'c']))\n    >>> df2 = DataFrame(pd.DataFrame({'a': ['name1', 'name2', 'name3'],\n   ...                    'b': ['name1', 'name2', 'name3'],\n   ...                    'c': [1, 2, 3]}, columns=['a', 'b', 'c']))\n    >>> bloomfilter(df1, 'a', 'b', error_rate", "solution": "\n\n\n    if not isinstance(column, Column):\n        raise TypeError('bloomfilter can only filter on the column of a collection')\n\n    # to make the class pickled right by the cloudpickle\n    with open(os.path.join(path, 'lib', 'bloomfilter.py')) as bloomfilter_file:\n        local = {}\n        six.exec_(bloomfilter_file.read(), local)\n        BloomFilter = local['BloomFilter']\n\n        col_name = column.source_name or column.name\n\n        on_name = on.name if isinstance(on, SequenceExpr) else on\n        rand_name = '%s_%s'% (on_name, str(uuid.uuid4()).replace('-', '_'))\n        on_col = collection._get_field(on).rename(rand_name)\n        src_collection = collection\n        collection = collection[collection, on_col]\n\n        @output(src_collection.schema.names, src_collection.schema.types)\n        class Filter(object):\n            def __init__(self, resources):\n                table = resources[0]\n\n                bloom = BloomFilter(capacity, error_rate)\n                for row in table:\n                    bloom.add(str(getattr(row, col_name)))\n\n                self.bloom = bloom\n\n            def __call__(self, row):\n                if str(getattr(row, rand_name)) not in self.bloom:\n                    return\n                return row[:-1]\n\n        return collection.apply(Filter, axis=1, resources=[column.input, ])"}
{"prompt": "def enabled(name='allprofiles'):\n    \"\"\"\n    Enable all the firewall profiles (Windows only)\n\n    Args:\n        profile (Optional[str]): The name of the profile to enable. Default is\n            ``allprofiles``. Valid options are:\n\n            - allprofiles\n            - domainprofile\n            - privateprofile\n            - publicprofile\n\n    Example:\n\n    .. code-block:: yaml\n\n        # To enable the domain profile\n        enable_domain:\n          win_firewall.enabled:\n            - name: domainprofile\n\n        # To enable all profiles\n        enable_all:\n          win_firewall.enabled:\n            - name: allprofiles\n    \"\"\"", "output": "\n    ret = {'name': name,\n           'changes': {},\n          'result': True,\n           'comment': ''}\n\n    if profile:\n        ret['comment'] = 'Enabled'\n        ret['changes']['enabled'] = profile\n\n    return ret\n\n\ndef disable_domain(name='winfirewall'):\n    \"\"\"\n    Disable all the windows firewall profiles (Windows only)\n\n    Args:\n        name (Optional[str]): The name of the profile to disable. Default is\n            ``winfirewall``. Valid options are:\n\n            - winfirewall\n            - domainprofile\n            - privateprofile\n            - allprofiles\n            - profile\n            - allprofiles\n            -", "solution": "\n    ret = {'name': name,\n           'result': True,\n           'changes': {},\n           'comment': ''}\n\n    profile_map = {'domainprofile': 'Domain',\n                   'privateprofile': 'Private',\n                   'publicprofile': 'Public',\n                   'allprofiles': 'All'}\n\n    # Make sure the profile name is valid\n    if name not in profile_map:\n        raise SaltInvocationError('Invalid profile name: {0}'.format(name))\n\n    current_config = __salt__['firewall.get_config']()\n    if name != 'allprofiles' and profile_map[name] not in current_config:\n        ret['result'] = False\n        ret['comment'] = 'Profile {0} does not exist in firewall.get_config' \\\n                         ''.format(name)\n        return ret\n\n    for key in current_config:\n        if not current_config[key]:\n            if name == 'allprofiles' or key == profile_map[name]:\n                ret['changes'][key] = 'enabled'\n\n    if __opts__['test']:\n        ret['result'] = not ret['changes'] or None\n        ret['comment'] = ret['changes']\n        ret['changes'] = {}\n        return ret\n\n    # Enable it\n    if ret['changes']:\n        try:\n            ret['result'] = __salt__['firewall.enable'](name)\n        except CommandExecutionError:\n            ret['comment'] = 'Firewall Profile {0} could not be enabled' \\\n                             ''.format(profile_map[name])\n    else:\n        if name == 'allprofiles':\n            msg = 'All the firewall profiles are enabled'\n        else:\n            msg = 'Firewall profile {0} is enabled'.format(name)\n        ret['comment'] = msg\n\n    return ret"}
{"prompt": "def get_f_ty_callback(self):\n        \"\"\" Generates a callback for evaluating ``self.exprs``. \"\"\"", "output": "\n        return lambda x: self.exprs[x]\n\n    def get_f_ty_expr(self, f):\n        \"\"\" Evaluates ``f`` on the expression tree. \"\"\"\n        if isinstance(f, ExprId):\n            return f\n        elif isinstance(f, ExprInt):\n            return f.arg\n        elif isinstance(f, ExprId):\n            return f.id\n        elif isinstance(f, ExprOp):\n            return f.op\n        elif isinstance(f, ExprId):\n            return f.id\n        elif isinstance(f, ExprOp):\n            return f.args[0]\n        else:\n           ", "solution": "\n        cb = self._callback_factory(self.exprs)\n        lb = self.lower_bounds\n        ub = self.upper_bounds\n        if lb is not None or ub is not None:\n            def _bounds_wrapper(t, y, p=(), be=None):\n                if lb is not None:\n                    if np.any(y < lb - 10*self._current_integration_kwargs['atol']):\n                        raise RecoverableError\n                    y = np.array(y)\n                    y[y < lb] = lb[y < lb]\n                if ub is not None:\n                    if np.any(y > ub + 10*self._current_integration_kwargs['atol']):\n                        raise RecoverableError\n                    y = np.array(y)\n                    y[y > ub] = ub[y > ub]\n                return cb(t, y, p, be)\n            return _bounds_wrapper\n        else:\n            return cb"}
{"prompt": "def load(self):\n        \"\"\"load and return the module/attribute.\"\"\"", "output": "\n        # TODO: add a test for this\n        if self.module is not None:\n            return self.module\n        return None\n\n    def get_attribute(self, attribute):\n        \"\"\"Get a module attribute.\"\"\"\n        if attribute is None:\n            return None\n        if attribute == 'name':\n            return self.name\n        elif attribute == 'path':\n            return self.path\n        elif attribute =='module':\n            return self.module\n        elif attribute == 'path':\n            return self.path\n        elif attribute =='module_args':\n            return self.module_args\n        elif attribute == 'path_args':\n            return self", "solution": "\n        module = importlib.import_module(self.import_path)\n        if self.attr:\n            return getattr(module, self.attr)\n        return module"}
{"prompt": "def child(self, group_name, page_size=500):\r\n        \"\"\" Returns the child ad group that matches the provided group_name or none if the child does not exist.\r\n\r\n        :param group_name: The name of the child group. NOTE: A name does not contain 'CN=' or 'OU='\r\n        :type group_name: str\r\n        :param page_size (optional): Many servers have a limit on the number of results that can be returned.\r\n                                     Paged searches circumvent that limit. Adjust the page_size to be below the\r\n                                     server's size limit. (default: 500)\r\n        :type page_size: int\r\n\r\n        \"\"\"", "output": ")\r\n        :type page_size: int\r\n        \"\"\"\r\n        if group_name.lower() == 'cn':\r\n            return self.get_cn_ad_group()\r\n        elif group_name.lower() == 'ou':\r\n            return self.get_ou_ad_group()\r\n        else:\r\n            return None\r\n\r\n    def get_cn_ad_group(self):\r\n        \"\"\" Returns the ad group that matches the provided group_name or none if the child does not exist.\r\n\r\n        :return: The ad group that matches the provided group_name or none if the child does not exist.\r\n        :rtype: str\r\n       ", "solution": "\r\n\r\n        object_class = self.get_attribute(\"objectClass\")\r\n        group_type = object_class[-1] if object_class else None\r\n\r\n        if group_type == \"group\":\r\n            connection_dict = self.GROUP_SINGLE_CHILD_SEARCH\r\n        elif group_type == \"organizationalUnit\":\r\n            connection_dict = self.OU_SINGLE_CHILD_SEARCH\r\n        else:\r\n            logger.debug(\"Unable to process child {child} of group {group_dn} with type {group_type}.\".format(\r\n                child=group_name, group_dn=self.group_dn, group_type=group_type\r\n            ))\r\n            return []\r\n\r\n        entry_list = self.ldap_connection.extend.standard.paged_search(\r\n            search_base=connection_dict['base_dn'],\r\n            search_filter=connection_dict['filter_string'].format(child_group_name=escape_query(group_name)),\r\n            search_scope=connection_dict['scope'],\r\n            attributes=connection_dict['attribute_list'],\r\n            paged_size=page_size\r\n        )\r\n\r\n        results = [result[\"dn\"] for result in entry_list if result[\"type\"] == \"searchResEntry\"]\r\n\r\n        if len(results) != 1:\r\n            logger.debug(\"Search returned {count} results: {results}\".format(count=len(results), results=results))\r\n\r\n        if results:\r\n            return ADGroup(\r\n                group_dn=results[0], server_uri=self.server_uri, base_dn=self.base_dn,\r\n                user_lookup_attr=self.user_lookup_attr, group_lookup_attr=self.group_lookup_attr,\r\n                attr_list=self.attr_list, bind_dn=self.bind_dn, bind_password=self.bind_password,\r\n                user_search_base_dn=self.user_search_base_dn, group_search_base_dn=self.user_search_base_dn\r\n            )\r\n        else:\r\n            return None"}
{"prompt": "def list_nodes_full(**kwargs):\n    \"\"\"\n    Return all data on nodes\n    \"\"\"", "output": "\n    nodes = []\n    for node in NODES:\n        nodes.append(node)\n    return nodes\n\n\ndef list_nodes_select(name, **kwargs):\n    \"\"\"\n    Return all data on nodes\n    \"\"\"\n    nodes = list_nodes_full(**kwargs)\n    return nodes[name]\n\n\ndef list_nodes_select_by_name(name, **kwargs):\n    \"\"\"\n    Return data on nodes\n    \"\"\"\n    nodes = list_nodes_select(name, **kwargs)\n    return nodes\n\n\ndef list_nodes_select_by_id(id, **kwargs):\n    \"\"\"\n    Return", "solution": "\n    nodes = _query('server/list')\n    ret = {}\n\n    for node in nodes:\n        name = nodes[node]['label']\n        ret[name] = nodes[node].copy()\n        ret[name]['id'] = node\n        ret[name]['image'] = nodes[node]['os']\n        ret[name]['size'] = nodes[node]['VPSPLANID']\n        ret[name]['state'] = nodes[node]['status']\n        ret[name]['private_ips'] = nodes[node]['internal_ip']\n        ret[name]['public_ips'] = nodes[node]['main_ip']\n\n    return ret"}
{"prompt": "def train_df(self, df, drop=False):\n        \"\"\"\n        Train scales from a dataframe\n        \"\"\"", "output": "\n        self.scale_df = df\n        self.drop = drop\n\n    def scale_df_to_dict(self):\n        \"\"\"\n        Convert a scale to a dictionary\n        \"\"\"\n        self.scale_dict = {}\n        for key, scale in self.scale_dict.items():\n            self.scale_dict[key] = scale\n\n    def get_scale(self, key):\n        \"\"\"\n        Get a scale\n        \"\"\"\n        return self.scale_dict[key]\n\n    def get_scale_by_name(self, name):\n        \"\"\"\n        Get a scale by name\n        \"\"\"\n        return self.scale_", "solution": "\n        if (len(df) == 0) or (len(self) == 0):\n            return df\n\n        # Each scale trains the columns it understands\n        for sc in self:\n            sc.train_df(df)\n        return df"}
{"prompt": "def validatePopElement(self, ctxt, elem, qname):\n        \"\"\"Pop the element end from the validation stack. \"\"\"", "output": "\n        if not self.isNil():\n            if elem.get(\"name\") == \"body\":\n                self.body = self.getBody()\n            elif elem.get(\"name\") == \"head\":\n                self.head = self.getHead()\n            elif elem.get(\"name\") == \"body-end\":\n                self.body_end = self.getBodyEnd()\n            elif elem.get(\"name\") == \"head-end\":\n                self.head_end = self.getHeadEnd()\n            elif elem.get(\"name\") == \"body-end\":\n                self.body_end =", "solution": "\n        if ctxt is None: ctxt__o = None\n        else: ctxt__o = ctxt._o\n        if elem is None: elem__o = None\n        else: elem__o = elem._o\n        ret = libxml2mod.xmlValidatePopElement(ctxt__o, self._o, elem__o, qname)\n        return ret"}
{"prompt": "def set_error(self, error_shortmsg: str, error_longmsg: str):\n        \"\"\" Set the stage to error and add a message \"\"\"", "output": "\n        self.error_shortmsg = error_shortmsg\n        self.error_longmsg = error_longmsg\n\n    def __str__(self):\n        \"\"\" String representation of the job \"\"\"\n        return \"Job: {0}, {1}, {2}, {3}, {4}, {5}, {6}, {7}, {8}, {9}, {10}, {11}, {12}, {13}, {14}, {15}, {16}, {17}, {18}, {19}, {20}\" \\\n           .format(self.name, self.job_id, self.error_shortmsg", "solution": "\n        LOG.error(f\"Update session: error in stage {self._stage.name}: \"\n                  f\"{error_shortmsg}: {error_longmsg}\")\n        self._error = Value(error_shortmsg, error_longmsg)\n        self.set_stage(Stages.ERROR)"}
{"prompt": "def create_package(package_format, owner, repo, **kwargs):\n    \"\"\"Create a new package in a repository.\"\"\"", "output": "\n    if kwargs.get('name'):\n        name = kwargs.get('name')\n    else:\n        name = package_format.name\n    if kwargs.get('version'):\n        version = kwargs.get('version')\n    else:\n        version = package_format.version\n    if kwargs.get('summary'):\n        summary = kwargs.get('summary')\n    else:\n        summary = package_format.summary\n    if kwargs.get('description'):\n        description = kwargs.get('description')\n    else:\n        description = package_format.description\n    if kwargs.get('url'):\n        url = kwargs.get('url", "solution": "\n    client = get_packages_api()\n\n    with catch_raise_api_exception():\n        upload = getattr(client, \"packages_upload_%s_with_http_info\" % package_format)\n\n        data, _, headers = upload(\n            owner=owner, repo=repo, data=make_create_payload(**kwargs)\n        )\n\n    ratelimits.maybe_rate_limit(client, headers)\n    return data.slug_perm, data.slug"}
{"prompt": "def get_device(ads, **kwargs):\n    \"\"\"Finds a unique AndroidDevice instance from a list that has specific\n    attributes of certain values.\n\n    Deprecated, use `get_devices(ads, **kwargs)[0]` instead.\n    This method will be removed in 1.8.\n\n    Example:\n        get_device(android_devices, label='foo', phone_number='1234567890')\n        get_device(android_devices, model='angler')\n\n    Args:\n        ads: A list of AndroidDevice instances.\n        kwargs: keyword arguments used to filter AndroidDevice instances.\n\n    Returns:\n        The target AndroidDevice instance.\n\n    Raises:\n        Error: None or more than one device is matched.\n    \"\"\"", "output": "Device instance.\n    \"\"\"\n    warnings.warn(\n        'get_device is deprecated, use `get_devices(ads, **kwargs)[0]` instead.',\n        DeprecationWarning)\n    return get_devices(ads, **kwargs)[0]\n\n\ndef get_device_by_name(name):\n    \"\"\"Finds a unique AndroidDevice instance by name.\n\n    Deprecated, use `get_device(android_devices, name)[0]` instead.\n    This method will be removed in 1.8.\n\n    Example:\n        get_device_by_name('foo')\n        get_device_by_name('angler", "solution": "\n\n    filtered = get_devices(ads, **kwargs)\n    if len(filtered) == 1:\n        return filtered[0]\n    else:\n        serials = [ad.serial for ad in filtered]\n        raise Error('More than one device matched: %s' % serials)"}
{"prompt": "def querystring(self):\n        \"\"\"Return original querystring but containing only managed keys\n\n        :return dict: dict of managed querystring parameter\n        \"\"\"", "output": "\n        return self._querystring\n\n    def get_parameter(self, name):\n        \"\"\"Get parameter value\n\n        :param name: name of the parameter\n        :return str: parameter value\n        \"\"\"\n        return self._parameters[name]\n\n    def get_parameter_value(self, name):\n        \"\"\"Get parameter value\n\n        :param name: name of the parameter\n        :return str: parameter value\n        \"\"\"\n        return self._parameters[name]\n\n    def get_parameter_names(self):\n        \"\"\"Return the names of the parameters\n\n        :return list: list of parameter names\n        \"\"\"\n        return self._parameters.keys()\n\n   ", "solution": "\n        return {key: value for (key, value) in self.qs.items()\n                if key.startswith(self.MANAGED_KEYS) or self._get_key_values('filter[')}"}
{"prompt": "def reshape(x,input_dim):\n    \"\"\"\n    Reshapes x into a matrix with input_dim columns\n\n    \"\"\"", "output": "\n    return tf.reshape(x,(-1,input_dim))\n\ndef batch_norm(x,input_dim):\n    \"\"\"\n    Batch Normalization\n\n    \"\"\"\n    return tf.nn.batch_norm(x,input_dim),input_dim\n\ndef conv2d(x,filter_shape,strides,padding='SAME'):\n    \"\"\"\n    Convenience function to apply a 2D convolution to a tensor\n\n    \"\"\"\n    return tf.nn.conv2d(x,filter_shape,strides=strides,padding=padding)\n\ndef max_pool(x,ksize,strides", "solution": "\n    x = np.array(x)\n    if x.size ==input_dim:\n        x = x.reshape((1,input_dim))\n    return x"}
{"prompt": "def nas_auto_qos_set_cos(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"", "output": "\n        config = ET.Element(\"config\")\n        auto_qos_set = ET.SubElement(config, \"auto-qos-set\")\n        auto_qos_set_cos.text = kwargs.pop('cos', None)\n        if kwargs:\n            for k, v in kwargs.items():\n                auto_qos_set_cos_xml = ET.SubElement(auto_qos_set, \"auto-qos-set-cos\")\n                auto_qos_set_cos_xml.text = str(v)\n                ET.SubElement(auto_qos_set_cos, \"value\").text = str", "solution": "\n        config = ET.Element(\"config\")\n        nas = ET.SubElement(config, \"nas\", xmlns=\"urn:brocade.com:mgmt:brocade-qos\")\n        auto_qos = ET.SubElement(nas, \"auto-qos\")\n        set = ET.SubElement(auto_qos, \"set\")\n        cos = ET.SubElement(set, \"cos\")\n        cos.text = kwargs.pop('cos')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)"}
{"prompt": "def extract_objects(self, fname, type_filter=None):\n    \"\"\"Extract objects from a source file\n\n    Args:\n      fname(str): Name of file to read from\n      type_filter (class, optional): Object class to filter results\n    Returns:\n      List of objects extracted from the file.\n    \"\"\"", "output": "\n    return extract_objects_from_file(fname, type_filter)\n\n  def get_objects(self, type_filter=None):\n    \"\"\"Get objects from a source file\n\n    Args:\n      type_filter(class, optional): Object class to filter results\n    Returns:\n      List of objects extracted from the file.\n    \"\"\"\n    return get_objects_from_file(self.fname_to_objects, type_filter)\n\n  def fname_to_objects(self, fname):\n    \"\"\"Convert a source file name to a list of objects\n\n    Args:\n      fname(str): Source file name to convert", "solution": "\n    objects = []\n    if fname in self.object_cache:\n      objects = self.object_cache[fname]\n    else:\n      with io.open(fname, 'rt', encoding='utf-8') as fh:\n        text = fh.read()\n        objects = parse_verilog(text)\n        self.object_cache[fname] = objects\n\n    if type_filter:\n      objects = [o for o in objects if isinstance(o, type_filter)]\n\n    return objects"}
{"prompt": "def encrypt_file(cls, key, in_filename, out_filename=None, chunksize=64 * 1024):\n        \"\"\" Encrypts a file using AES (CBC mode) with the\n            given key.\n    \n            key:\n                The encryption key - a string that must be\n                either 16, 24 or 32 bytes long. Longer keys\n                are more secure.\n    \n            in_filename:\n                Name of the input file\n    \n            out_filename:\n                If None, '<in_filename>.enc' will be used.\n    \n            chunksize:\n                Sets the size of the chunk which the function\n                uses to read and encrypt the file. Larger chunk\n                sizes can be faster for some files and machines.\n                chunksize must be divisible by 16.\n        \"\"\"", "output": " and encrypt. The default is 64MB.\n    \n            Returns:\n                A tuple (status, ciphertext) where status is a\n                string indicating the status of the operation.\n                The ciphertext is a string of bytes.\n        \"\"\"\n        if out_filename is None:\n            out_filename = in_filename\n        if out_filename is None:\n            out_filename = in_filename + '.enc'\n        if out_filename is None:\n            out_filename = out_filename + '.enc'\n        if out_filename is None:\n            out_filename = out_filename + '.enc'\n        if not os.path.", "solution": "\n        if not out_filename:\n            out_filename = in_filename + '.enc'\n\n        iv = ''.join(chr(random.randint(0, 0xFF)) for _ in range(16))\n        encryptor = AES.new(key, AES.MODE_CBC, iv)\n        filesize = os.path.getsize(in_filename)\n\n        with open(in_filename, 'rb') as infile:\n            with open(out_filename, 'wb') as outfile:\n                outfile.write(struct.pack('<Q', filesize))\n                outfile.write(iv)\n\n                while True:\n                    chunk = infile.read(chunksize)\n                    if len(chunk) == 0:\n                        break\n                    elif len(chunk) % 16 != 0:\n                        chunk += ' ' * (16 - len(chunk) % 16)\n\n                    outfile.write(encryptor.encrypt(chunk))"}
{"prompt": "def off_datastream(self, datastream):\n        \"\"\"\n        To turn off datastream\n        :param datastream: string\n        \"\"\"", "output": "\n        self.datastream = datastream\n        self.datastream_name = datastream\n\n    def get_datastream(self):\n        \"\"\"\n        Get the datastream name\n        :return: string\n        \"\"\"\n        return self.datastream\n\n    def get_datastream_name(self):\n        \"\"\"\n        Get the datastream name\n        :return: string\n        \"\"\"\n        return self.datastream_name\n\n    def get_name(self):\n        \"\"\"\n        Get the name of the file\n        :return: string\n        \"\"\"\n        return self.name\n\n    def get_name_with_ext(self, ext):\n        \"\"\"\n       ", "solution": "\n        url = '/datastream/' + str(datastream) + '/off'\n        response = self.http.post(url,\"\")\n        return response"}
{"prompt": "def get_expired_tasks(self, key_prefix, time, batch_size, client=None):\n        \"\"\"\n        Returns a list of expired tasks (older than ``time``) by looking at all\n        active queues. The list is capped at ``batch_size``. The list contains\n        tuples (queue, task_id).\n        \"\"\"", "output": "\n        now = datetime.now()\n        expired_tasks = []\n        for queue in self.active_queues:\n            if client is None:\n                client = self.get_client()\n            expired_tasks.append((queue, now))\n        expired_tasks = expired_tasks[:batch_size]\n        return expired_tasks\n\n    def get_expired_tasks_by_prefix(self, key_prefix, batch_size, client=None):\n        \"\"\"\n        Returns a list of expired tasks (older than ``time``) by looking at all\n        active queues. The list is capped at ``batch_size``. The", "solution": "\n        result = self._get_expired_tasks(args=[key_prefix, time, batch_size],\n                                         client=client)\n\n        # [queue1, task1, queue2, task2] -> [(queue1, task1), (queue2, task2)]\n        return list(zip(result[::2], result[1::2]))"}
{"prompt": "def gather_votes(self):\n        \"\"\"Gather votes from all the underlying slave environments for the\n        current list of candidates.\n\n        The votes are stored in :attr:`votes`, overriding any previous votes.\n        \"\"\"", "output": "\n        self.votes = [\n            self.get_next_candidate(env) for env in self.slave_environments\n        ]\n\n    def get_next_candidate(self, env):\n        \"\"\"Get the next candidate for the given environment.\n\n        :param env: The environment to get the next candidate for.\n        :type env: :class:`robottelo.constants.Environment`\n\n        :returns: The next candidate for the given environment.\n        :rtype: :class:`robottelo.types.Candidate`\n\n        :raises: :class:`robottelo.ssh.exception.PasswordFileFailedToCreateError", "solution": "\n        async def slave_task(addr, candidates):\n            r_manager = await self.env.connect(addr)\n            return await r_manager.gather_votes(candidates)\n\n        if len(self.candidates) == 0:\n            self._log(logging.DEBUG, \"Could not gather votes because there \"\n                      \"are no candidates!\")\n            self._votes = []\n            return\n        self._log(logging.DEBUG, \"Gathering votes for {} candidates.\"\n                  .format(len(self.candidates)))\n\n        if self._single_env:\n            self._votes = self.env.gather_votes(self.candidates)\n        else:\n            mgrs = self.get_managers()\n            tasks = create_tasks(slave_task, mgrs, self.candidates)\n            self._votes = run(tasks)"}
{"prompt": "def unfold_lines(string):\n    \"\"\"Join lines that are wrapped.\n\n    Any line that starts with a space or tab is joined to the previous\n    line.\n    \"\"\"", "output": "\n    return re.sub(r'\\s+','', string)\n\n\ndef _get_indent(line):\n    \"\"\"Return the number of spaces to indent the given line.\n\n    This is like _get_indent but doesn't indent the lines.\n    \"\"\"\n    return len(line) - len(line.lstrip())\n\n\ndef _get_indent_spaces(line):\n    \"\"\"Return number of spaces to indent the given line.\n\n    This is like _get_indent but doesn't indent the lines.\n    \"\"\"\n    return len(line) - len(line.rstrip())\n\n\ndef _get_indent", "solution": "\n    assert isinstance(string, str), 'Expect str. Got {}'.format(type(string))\n    lines = string.splitlines()\n    line_buffer = io.StringIO()\n\n    for line_number in range(len(lines)):\n        line = lines[line_number]\n        if line and line[0:1] in (' ', '\\t'):\n            line_buffer.write(' ')\n        elif line_number != 0:\n            line_buffer.write('\\r\\n')\n        line_buffer.write(line.strip())\n\n    line_buffer.write('\\r\\n')\n\n    return line_buffer.getvalue()"}
{"prompt": "def _extract_symbols(self, symbols, default=None):\n        \"\"\"! @brief Fill 'symbols' field with required flash algo symbols\"\"\"", "output": "\n        if default is not None:\n            return default\n        return [\n            \"0x%x\" % (self.address + i) for i in range(0x100)\n        ]\n\n    def _get_flash_algo_name(self):\n        \"\"\"! @brief Return the flash algo name\"\"\"\n        return self.flash_algo.name\n\n    def _set_flash_algo_name(self, new_name):\n        \"\"\"! @brief Set the flash algo name\"\"\"\n        self.flash_algo.name = new_name\n\n    def _get_flash_algo_address(self):\n        \"\"\"!", "solution": "\n        to_ret = {}\n        for symbol in symbols:\n            symbolInfo = self.elf.symbol_decoder.get_symbol_for_name(symbol)\n            if symbolInfo is None:\n                if default is not None:\n                    to_ret[symbol] = default\n                    continue\n                raise FlashAlgoException(\"Missing symbol %s\" % symbol)\n            to_ret[symbol] = symbolInfo.address\n        return to_ret"}
{"prompt": "def _dataframe_fields(self):\n        \"\"\"\n        Creates a dictionary of all fields to include with DataFrame.\n\n        With the result of the calls to class properties changing based on the\n        class index value, the dictionary should be regenerated every time the\n        index is changed when the dataframe property is requested.\n\n        Returns\n        -------\n        dictionary\n            Returns a dictionary where the keys are the shortened ``string``\n            attribute names and the values are the actual value for each\n            attribute for the specified index.\n        \"\"\"", "output": "\n        return _dataframe_fields(self) + _dataframe_fields(self.index)\n\n    def __repr__(self):\n        return self.string\n\n    def __str__(self):\n        return self._dataframe_fields()\n\n    def __getitem__(self, key):\n        \"\"\"\n        Returns a sliced view of the view corresponding to the given\n        key.\n\n        Parameters\n        ----------\n        key : str\n            The name of the view to be sliced.\n\n        Returns\n        -------\n        sliced view\n            Returns a view of the sliced ``key`` with the given ``string``\n            attribute set to the sliced ``key``.\n       ", "solution": "\n        fields_to_include = {\n            'adjusted_net_yards_per_attempt_index':\n            self.adjusted_net_yards_per_attempt_index,\n            'adjusted_net_yards_per_pass_attempt':\n            self.adjusted_net_yards_per_pass_attempt,\n            'adjusted_yards_per_attempt': self.adjusted_yards_per_attempt,\n            'adjusted_yards_per_attempt_index':\n            self.adjusted_yards_per_attempt_index,\n            'all_purpose_yards': self.all_purpose_yards,\n            'approximate_value': self.approximate_value,\n            'assists_on_tackles': self.assists_on_tackles,\n            'attempted_passes': self.attempted_passes,\n            'birth_date': self.birth_date,\n            'blocked_punts': self.blocked_punts,\n            'catch_percentage': self.catch_percentage,\n            'completed_passes': self.completed_passes,\n            'completion_percentage_index': self.completion_percentage_index,\n            'espn_qbr': self.espn_qbr,\n            'extra_point_percentage': self.extra_point_percentage,\n            'extra_points_attempted': self.extra_points_attempted,\n            'extra_points_made': self.extra_points_made,\n            'field_goal_percentage': self.field_goal_percentage,\n            'field_goals_attempted': self.field_goals_attempted,\n            'field_goals_made': self.field_goals_made,\n            'fifty_plus_yard_field_goal_attempts':\n            self.fifty_plus_yard_field_goal_attempts,\n            'fifty_plus_yard_field_goals_made':\n            self.fifty_plus_yard_field_goals_made,\n            'fourth_quarter_comebacks': self.fourth_quarter_comebacks,\n            'fourty_to_fourty_nine_yard_field_goal_attempts':\n            self.fourty_to_fourty_nine_yard_field_goal_attempts,\n            'fourty_to_fourty_nine_yard_field_goals_made':\n            self.fourty_to_fourty_nine_yard_field_goals_made,\n            'fumbles': self.fumbles,\n            'fumbles_forced': self.fumbles_forced,\n            'fumbles_recovered': self.fumbles_recovered,\n            'fumbles_recovered_for_touchdown':\n            self.fumbles_recovered_for_touchdown,\n            'game_winning_drives': self.game_winning_drives,\n            'games': self.games,\n            'games_started': self.games_started,\n            'height': self.height,\n            'interception_percentage': self.interception_percentage,\n            'interception_percentage_index':\n            self.interception_percentage_index,\n            'interceptions': self.interceptions,\n            'interceptions_returned_for_touchdown':\n            self.interceptions_returned_for_touchdown,\n            'interceptions_thrown': self.interceptions_thrown,\n            'kickoff_return_touchdown': self.kickoff_return_touchdown,\n            'kickoff_return_yards': self.kickoff_return_yards,\n            'kickoff_returns': self.kickoff_returns,\n            'less_than_nineteen_yards_field_goal_attempts':\n            self.less_than_nineteen_yards_field_goal_attempts,\n            'less_than_nineteen_yards_field_goals_made':\n            self.less_than_nineteen_yards_field_goals_made,\n            'longest_field_goal_made': self.longest_field_goal_made,\n            'longest_interception_return': self.longest_interception_return,\n            'longest_kickoff_return': self.longest_kickoff_return,\n            'longest_pass': self.longest_pass,\n            'longest_punt': self.longest_punt,\n            'longest_punt_return': self.longest_punt_return,\n            'longest_reception': self.longest_reception,\n            'longest_rush': self.longest_rush,\n            'name': self.name,\n            'net_yards_per_attempt_index': self.net_yards_per_attempt_index,\n            'net_yards_per_pass_attempt': self.net_yards_per_pass_attempt,\n            'passer_rating_index': self.passer_rating_index,\n            'passes_defended': self.passes_defended,\n            'passing_completion': self.passing_completion,\n            'passing_touchdown_percentage': self.passing_touchdown_percentage,\n            'passing_touchdowns': self.passing_touchdowns,\n            'passing_yards': self.passing_yards,\n            'passing_yards_per_attempt': self.passing_yards_per_attempt,\n            'player_id': self.player_id,\n            'position': self.position,\n            'punt_return_touchdown': self.punt_return_touchdown,\n            'punt_return_yards': self.punt_return_yards,\n            'punt_returns': self.punt_returns,\n            'punts': self.punts,\n            'qb_record': self.qb_record,\n            'quarterback_rating': self.quarterback_rating,\n            'receiving_touchdowns': self.receiving_touchdowns,\n            'receiving_yards': self.receiving_yards,\n            'receiving_yards_per_game': self.receiving_yards_per_game,\n            'receiving_yards_per_reception':\n            self.receiving_yards_per_reception,\n            'receptions': self.receptions,\n            'receptions_per_game': self.receptions_per_game,\n            'rush_attempts': self.rush_attempts,\n            'rush_attempts_per_game': self.rush_attempts_per_game,\n            'rush_touchdowns': self.rush_touchdowns,\n            'rush_yards': self.rush_yards,\n            'rush_yards_per_attempt': self.rush_yards_per_attempt,\n            'rush_yards_per_game': self.rush_yards_per_game,\n            'rushing_and_receiving_touchdowns':\n            self.rushing_and_receiving_touchdowns,\n            'sack_percentage': self.sack_percentage,\n            'sack_percentage_index': self.sack_percentage_index,\n            'sacks': self.sacks,\n            'safeties': self.safeties,\n            'season': self.season,\n            'tackles': self.tackles,\n            'team_abbreviation': self.team_abbreviation,\n            'thirty_to_thirty_nine_yard_field_goal_attempts':\n            self.thirty_to_thirty_nine_yard_field_goal_attempts,\n            'thirty_to_thirty_nine_yard_field_goals_made':\n            self.thirty_to_thirty_nine_yard_field_goals_made,\n            'times_pass_target': self.times_pass_target,\n            'times_sacked': self.times_sacked,\n            'total_punt_yards': self.total_punt_yards,\n            'touchdown_percentage_index': self.touchdown_percentage_index,\n            'touches': self.touches,\n            'twenty_to_twenty_nine_yard_field_goal_attempts':\n            self.twenty_to_twenty_nine_yard_field_goal_attempts,\n            'twenty_to_twenty_nine_yard_field_goals_made':\n            self.twenty_to_twenty_nine_yard_field_goals_made,\n            'weight': self.weight,\n            'yards_from_scrimmage': self.yards_from_scrimmage,\n            'yards_lost_to_sacks': self.yards_lost_to_sacks,\n            'yards_per_attempt_index': self.yards_per_attempt_index,\n            'yards_per_completed_pass': self.yards_per_completed_pass,\n            'yards_per_game_played': self.yards_per_game_played,\n            'yards_per_kickoff_return': self.yards_per_kickoff_return,\n            'yards_per_punt': self.yards_per_punt,\n            'yards_per_punt_return': self.yards_per_punt_return,\n            'yards_per_touch': self.yards_per_touch,\n            'yards_recovered_from_fumble': self.yards_recovered_from_fumble,\n            'yards_returned_from_interception':\n            self.yards_returned_from_interception\n        }\n        return fields_to_include"}
{"prompt": "def update_metric(self, eval_metric, labels, pre_sliced):\n        \"\"\"Accumulate the performance according to `eval_metric` on all devices\n        by comparing outputs from [begin, end) to labels. By default use all\n        outputs.\n\n        Parameters\n        ----------\n        eval_metric : EvalMetric\n            The metric used for evaluation.\n        labels : list of NDArray\n            Typically comes from `label` of a `DataBatch`.\n        pre_sliced : bool\n            Whether labels are already sliced.\n        begin : int\n            Starting index of used outputs.\n        end : int or None\n            Ending index of used outputs.\n        \"\"\"", "output": " of used outputs.\n        \"\"\"\n        if self.binded:\n            self.binded = False\n            self.bin_ids = None\n            self.bin_names = None\n            self.bin_ids = None\n            self.binded_names = None\n        else:\n            self.binded = True\n            self.bin_ids = labels\n            self.bin_names = [x[1] for x in self.bin_names]\n            self.binded_names = [x[0] for x in self.bin_names]\n            if end is None:\n                end = len(self.bin", "solution": "\n        for current_exec, (texec, islice) in enumerate(zip(self.execs, self.slices)):\n            if not pre_sliced:\n                labels_slice = []\n                for label, axis in zip(labels, self.label_layouts):\n                    if axis == 0:\n                        # slicing NDArray along axis 0 can avoid copying\n                        labels_slice.append(label[islice])\n                    elif axis > 0:\n                        # pylint: disable=no-member\n                        label_my_slice = nd.slice_axis(label, axis=axis, begin=islice.start,\n                                                       end=islice.stop).as_in_context(label.context)\n                        # pylint: enable=no-member\n                        labels_slice.append(label_my_slice)\n                    else:\n                        labels_slice.append(label)\n            else:\n                labels_slice = labels[current_exec]\n\n            labels_ = OrderedDict(zip(self.label_names, labels_slice))\n            preds = OrderedDict(zip(self.output_names, texec.outputs))\n            eval_metric.update_dict(labels_, preds)"}
{"prompt": "def get_number(s, cast=int):\n    \"\"\"\n    Try to get a number out of a string, and cast it.\n    \"\"\"", "output": "\n    try:\n        return int(s)\n    except ValueError:\n        return cast(s, 0)\n\n\ndef get_string(s, cast=str):\n    \"\"\"\n    Try to get a string out of a string, and cast it.\n    \"\"\"\n    try:\n        return cast(s, s)\n    except ValueError:\n        return s\n\n\ndef get_number_of_lines(s):\n    \"\"\"\n    Try to get the number of lines in a string, and return the\n    number of lines in the string.\n    \"\"\"\n    try:\n        return str(s).count('\\n')\n    except ValueError:", "solution": "\n    import string\n    d = \"\".join(x for x in str(s) if x in string.digits)\n    return cast(d)"}
{"prompt": "def _to_vcard(self, entry):\n        \"\"\"Return a vCard of the Abook entry\"\"\"", "output": "\n        return self._vcard\n\n    @property\n    def vcard_id(self):\n        \"\"\"Return the vCard ID\"\"\"\n        return self._vcard_id\n\n    @vcard_id.setter\n    def vcard_id(self, vcard_id):\n        \"\"\"Set the vCard ID\"\"\"\n        self._vcard_id = vcard_id\n\n    @property\n    def vcard_name(self):\n        \"\"\"Return the vCard name\"\"\"\n        return self._vcard\n\n    @vcard_name.setter\n    def vcard_name(self, vcard_name):\n        \"\"\"Set the", "solution": "\n        card = vCard()\n\n        card.add('uid').value = Abook._gen_uid(entry)\n        card.add('fn').value = entry['name']\n        card.add('n').value = Abook._gen_name(entry['name'])\n\n        if 'email' in entry:\n            for email in entry['email'].split(','):\n                card.add('email').value = email\n\n        addr_comps = ['address', 'address2', 'city', 'country', 'zip', 'country']\n        if any(comp in entry for comp in addr_comps):\n            card.add('adr').value = Abook._gen_addr(entry)\n\n        if 'other' in entry:\n            tel = card.add('tel')\n            tel.value = entry['other']\n\n        if 'phone' in entry:\n            tel = card.add('tel')\n            tel.type_param = 'home'\n            tel.value = entry['phone']\n\n        if 'workphone' in entry:\n            tel = card.add('tel')\n            tel.type_param = 'work'\n            tel.value = entry['workphone']\n\n        if 'mobile' in entry:\n            tel = card.add('tel')\n            tel.type_param = 'cell'\n            tel.value = entry['mobile']\n\n        if 'nick' in entry:\n            card.add('nickname').value = entry['nick']\n\n        if 'url' in entry:\n            card.add('url').value = entry['url']\n\n        if 'notes' in entry:\n            card.add('note').value = entry['notes']\n\n        self._add_photo(card, entry['name'])\n\n        return card"}
{"prompt": "def bgp_normalize_table_data(bgp_table):\n    \"\"\"The 'show bgp all summary vrf all' table can have entries that wrap multiple lines.\n\n    2001:db8:4:701::2\n                4 65535  163664  163693      145    0    0     3w2d 3\n    2001:db8:e0:dd::1\n                4    10  327491  327278      145    0    0     3w1d 4\n\n    Normalize this so the line wrap doesn't exit.\n    \"\"\"", "output": "\n    bgp_summary_vrf = {}\n    for entry in bgp_table:\n        bgp_summary_vrf[entry] = bgp_summary_vrf.get(entry, 0) + \\\n            bgp_summary_vrf.get(entry, 65535)\n\n\ndef bgp_normalize_table(bgp_table):\n    \"\"\"The'show bgp all summary vrf all' table can have entries that wrap multiple lines.\n\n    2001:db8:4:701::2\n                4 65535  163664  163693      145    0    0     3w2d 3\n    2001:db8:e0", "solution": "\n    bgp_table = bgp_table.strip()\n    bgp_multiline_pattern = r\"({})\\s*\\n\".format(IPV4_OR_IPV6_REGEX)\n    # Strip out the newline\n    return re.sub(bgp_multiline_pattern, r\"\\1\", bgp_table)"}
{"prompt": "def update_config(config_new, config_default):\n\n    \"\"\"\n    Updates the loaded method configuration with default values.\n    \"\"\"", "output": "\n\n    if config_new is None:\n        config_new = {}\n\n    if config_default is None:\n        config_default = {}\n\n    if not isinstance(config_new, dict):\n        raise ValueError(\"config_new must be a dictionary\")\n\n    if not isinstance(config_default, dict):\n        raise ValueError(\"config_default must be a dictionary\")\n\n    if'method' not in config_new:\n        config_new['method'] = {}\n\n    if'method' not in config_default:\n        config_default['method'] = {}\n\n    if'method' not in config_new:\n        config_", "solution": "\n    if any([isinstance(v, dict) for v in list(config_new.values())]):\n        for k,v in list(config_new.items()):\n            if isinstance(v,dict) and k in config_default:\n                update_config(config_new[k],config_default[k])\n            else:\n                config_default[k] = v\n    else:\n        config_default.update(config_new)\n    return config_default"}
{"prompt": "def run_analysis(self, argv):\n        \"\"\"Run this analysis\"\"\"", "output": "\n        self.set_options(self.options.keys())\n        self.options.update(self.options.get(self.options['--options'], {}))\n        self.options['--output'] = self.options['--output']\n        self.options['--verbose'] = self.options['--verbose']\n        self.options['--force'] = self.options['--force']\n        self.options['--no-check'] = self.options['--no-check']\n        self.options['--no-check-unstable'] = self.options['--no-check-unstable']", "solution": "\n        args = self._parser.parse_args(argv)\n\n        if not HAVE_ST:\n            raise RuntimeError(\n                \"Trying to run fermipy analysis, but don't have ST\")\n\n        workdir = os.path.dirname(args.config)\n        _config_file = self._clone_config_and_srcmaps(args.config, args.seed)\n\n        gta = GTAnalysis(_config_file, logging={'verbosity': 3},\n                         fileio={'workdir_regex': '\\.xml$|\\.npy$'})\n        gta.load_roi(args.roi_baseline)\n\n        simfile = os.path.join(workdir, 'sim_%s_%s.yaml' %\n                               (args.sim, args.sim_profile))\n\n        mcube_file = \"%s_%s_%06i\" % (args.sim, args.sim_profile, args.seed)\n        sim_config = utils.load_yaml(simfile)\n\n        injected_source = sim_config.get('injected_source', None)\n        if injected_source is not None:\n            src_dict =  injected_source['source_model']\n            src_dict['ra'] = gta.config['selection']['ra']\n            src_dict['dec'] = gta.config['selection']['dec']\n            injected_name = injected_source['name']\n            gta.add_source(injected_name, src_dict)\n            gta.write_model_map(mcube_file)\n            mc_spec_dict = dict(true_counts=gta.model_counts_spectrum(injected_name),\n                                energies=gta.energies,\n                                model=src_dict)\n            mcspec_file = os.path.join(workdir,\n                                       \"mcspec_%s_%06i.yaml\" % (mcube_file, args.seed))\n            utils.write_yaml(mc_spec_dict, mcspec_file)\n        else:\n            injected_name = None\n\n        gta.write_roi('sim_baseline_%06i' % args.seed)\n\n        test_sources = []\n        for profile in args.profiles:\n            profile_path = os.path.join(workdir, 'profile_%s.yaml' % profile)\n            test_source = load_yaml(profile_path)\n            test_sources.append(test_source)\n            first = args.seed\n            last = first + args.nsims\n            for seed in range(first, last):\n                self._run_simulation(gta, args.roi_baseline,\n                                     injected_name, test_sources, first, seed,\n                                     non_null_src=args.non_null_src)"}
{"prompt": "def get_check(self, check):\n        \"\"\"\n        Returns an instance of the specified check.\n        \"\"\"", "output": "\n        # This is the main check.\n        if check.name in self.checks:\n            return self.checks[check.name](self)\n\n        # Check if the check is a string.\n        if not isinstance(check, str):\n            raise ValueError(\"The check should be a string, not %s\" % check)\n\n        # Check if the check has an attribute.\n        if check.name in self.checks_attrs:\n            return self.checks_attrs[check.name](self)\n\n        # Check if the check is a method.\n        if not hasattr(check,'method'):\n            raise ValueError(\"The", "solution": "\n        chk = self._check_manager.get(check)\n        chk.set_entity(self)\n        return chk"}
{"prompt": "def GetRunlevelsLSB(states):\n  \"\"\"Accepts a string and returns a list of strings of numeric LSB runlevels.\"\"\"", "output": "\n  return [s.strip() for s in states.split(',')]\n\n\ndef GetRunlevels(states):\n  \"\"\"Accepts a string and returns a list of strings of numeric runlevels.\"\"\"\n  return [s.strip() for s in states.split(',')]\n\n\ndef GetRunlevel(runlevel):\n  \"\"\"Accepts a numeric runlevel, returns a numeric runlevel.\"\"\"\n  if runlevel == 'None':\n    return None\n  return int(runlevel)\n\n\ndef GetRunlevelLSB(runlevels):\n  \"\"\"Accepts a list of runlevels, returns a list of strings of numeric runlevel", "solution": "\n  if not states:\n    return set()\n  valid = set([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\"])\n  _LogInvalidRunLevels(states, valid)\n  return valid.intersection(set(states.split()))"}
{"prompt": "def add_coupon(self, coupon, idempotency_key=None):\n\t\t\"\"\"\n\t\tAdd a coupon to a Customer.\n\n\t\tThe coupon can be a Coupon object, or a valid Stripe Coupon ID.\n\t\t\"\"\"", "output": "\n\t\tif not isinstance(coupon, StripeCoupon):\n\t\t\traise ValueError(\"Coupon ID must be a valid Stripe Coupon object.\")\n\t\tif not isinstance(idempotency_key, str):\n\t\t\traise ValueError(\"Stripe Coupon ID must be a string.\")\n\t\tif not idempotency_key:\n\t\t\traise ValueError(\"Customer ID must be specified.\")\n\t\tif not isinstance(coupon.id, int):\n\t\t\traise ValueError(\"Stripe Coupon ID must be an integer.\")\n\t\tif not isinstance(idempotency_key, str):\n\t\t\traise ValueError(\"Stri", "solution": "\n\t\tif isinstance(coupon, StripeModel):\n\t\t\tcoupon = coupon.id\n\n\t\tstripe_customer = self.api_retrieve()\n\t\tstripe_customer[\"coupon\"] = coupon\n\t\tstripe_customer.save(idempotency_key=idempotency_key)\n\t\treturn self.__class__.sync_from_stripe_data(stripe_customer)"}
{"prompt": "def on_privmsg(self, c, e):\n        \"\"\"\n        This function runs when the bot receives a private message (query).\n        \"\"\"", "output": "\n        if len(e.arguments) < 2:\n            return\n        if e.arguments[1] == 'PRIVMSG':\n            if len(e.arguments[2].split()) > 1:\n                self.log.info('privmsg: %s', e.arguments[2])\n            else:\n                self.log.info('privmsg: %s', e.arguments[2])\n        else:\n            self.log.info('privmsg: %s', e.arguments[1])\n            self.log.info('privmsg: %s', e.arguments[2])\n\n    def on_pub", "solution": "\n        text = e.arguments[0]\n        logger.debug('{0!s}'.format(e.source))\n        metadata = self.set_metadata(e)\n        metadata['is_private_message'] = True\n        message = Message(text=text, metadata=metadata).__dict__\n        self.baseplate.tell(message)"}
{"prompt": "def delete_vnet(access_token, subscription_id, resource_group, name):\n    \"\"\"Delete a virtual network.\n\n    Args:\n        access_token (str): A valid Azure authentication token.\n        subscription_id (str): Azure subscription id.\n        resource_group (str): Azure resource group name.\n        name (str): Name of the VNet.\n\n    Returns:\n        HTTP response. VNet JSON body.\n    \"\"\"", "output": "\n    return _delete_vnet_from_subscription(access_token, subscription_id, resource_group, name)\n\n\ndef _delete_vnet_from_subscription(access_token, subscription_id, resource_group, name):\n    \"\"\"Delete a body from a subscription.\n\n    Args:\n        access_token (str): A valid Azure authentication token.\n        subscription_id (str): Azure subscription id.\n        resource_group (str): Azure resource group name.\n        name (str): Name of the VNet.\n    \"\"\"\n    return _delete_vnet_from_subscription_with_", "solution": "\n    endpoint = ''.join([get_rm_endpoint(),\n                        '/subscriptions/', subscription_id,\n                        '/resourceGroups/', resource_group,\n                        '/providers/Microsoft.Network/virtualNetworks/', name,\n                        '?api-version=', NETWORK_API])\n    return do_delete(endpoint, access_token)"}
{"prompt": "def savefig(\n    filename, width=None, height=None, fig=None, timeout_seconds=10, output_widget=None, headless=False, devmode=False\n):\n    \"\"\"Save the figure to an image file.\n\n    :param str filename: must have extension .png, .jpeg or .svg\n    :param int width: the width of the image in pixels\n    :param int height: the height of the image in pixels\n    :type fig: ipyvolume.widgets.Figure or None\n    :param fig: if None use the current figure\n    :param float timeout_seconds: maximum time to wait for image data to return\n    :param ipywidgets.Output output_widget: a widget to use as a context manager for capturing the data\n    :param bool headless: if True, use headless chrome to save figure\n    :param bool devmode: if True, attempt to get index.js from local js/dist folder\n    \"\"\"", "output": " the timeout in seconds\n    :param bool headless: if True, the figure is not shown until it is closed\n    :param bool devmode: if True, the figure is opened in dev mode\n    :type headless: bool\n    :return: None\n    :rtype: None\n\n    \"\"\"\n    if fig is None:\n        fig = ipyvolume.Figure()\n    if headless:\n        fig.show_headless = True\n    if devmode:\n        fig.show_devmode = True\n    if output_widget is None:\n        output_widget = ipyvolume.widgets.Image()\n    if fig.show", "solution": "\n    __, ext = os.path.splitext(filename)\n    format = ext[1:]\n    assert format in ['png', 'jpeg', 'svg'], \"image format must be png, jpeg or svg\"\n    with open(filename, \"wb\") as f:\n        f.write(\n            _screenshot_data(\n                timeout_seconds=timeout_seconds,\n                output_widget=output_widget,\n                format=format,\n                width=width,\n                height=height,\n                fig=fig,\n                headless=headless,\n                devmode=devmode,\n            )\n        )"}
{"prompt": "def remove(cls, module_name):\n        \"\"\"\n        drop a previously created guard on ``module_name``\n        if the module is not guarded, then this is a no-op.\n        \"\"\"", "output": "\n        cls.objects.filter(name=cls.guard_name).delete()\n\n    @classmethod\n    def get_guard_by_name(cls, name):\n        \"\"\"\n        return a previously created guard object, or ``None`` if not found.\n        \"\"\"\n        try:\n            return cls.objects.get(name=name)\n        except cls.DoesNotExist:\n            return None\n\n    @classmethod\n    def get_or_create(cls, module_name, name=None, description=None,\n                     is_active=True, is_expired=True, is_expired_date=None):\n        \"\"\"\n        Create", "solution": "\n        module_guards = cls._guards.get(module_name, False)\n        if module_guards:\n            module_guards.pop()\n            cls._num_entries -= 1\n            if cls._num_entries < 1:\n                if cls._num_entries < 0:\n                    raise Exception(\n                        'Bug: ImportGuardHook._num_entries became negative!'\n                    )\n                cls._uninstall()"}
{"prompt": "async def reset(request: web.Request) -> web.Response:\n    \"\"\" Execute a reset of the requested parts of the user configuration.\n    \"\"\"", "output": "\n    if request.method == 'POST':\n        await request.post()\n        return web.json_response({'status': 'ok'})\n    else:\n        return web.json_response({'status': 'error'})\n\n\n@routes.api_route('/reset', methods=['POST'])\n@web.authenticated\nasync def reset_part(request: web.Request) -> web.Response:\n    \"\"\" Execute a reset of the part of the user configuration.\n    \"\"\"\n    await request.post()\n    return web.json_response({'status': 'ok'})\n\n\n@routes.api_route('/part', methods", "solution": "\n    data = await request.json()\n    ok, bad_key = _check_reset(data)\n    if not ok:\n        return web.json_response(\n            {'message': '{} is not a valid reset option'\n             .format(bad_key)},\n            status=400)\n    log.info(\"Reset requested for {}\".format(', '.join(data.keys())))\n    if data.get('tipProbe'):\n        config = rc.load()\n        if ff.use_protocol_api_v2():\n            config = config._replace(\n                instrument_offset=rc.build_fallback_instrument_offset({}))\n        else:\n            config.tip_length.clear()\n        rc.save_robot_settings(config)\n    if data.get('labwareCalibration'):\n        if ff.use_protocol_api_v2():\n            labware.clear_calibrations()\n        else:\n            db.reset()\n\n    if data.get('bootScripts'):\n        if IS_ROBOT:\n            if os.path.exists('/data/boot.d'):\n                shutil.rmtree('/data/boot.d')\n        else:\n            log.debug('Not on pi, not removing /data/boot.d')\n    return web.json_response({}, status=200)"}
{"prompt": "def say_tmp_filepath(\n    text               = None,\n    preference_program = \"festival\"\n    ):\n    \"\"\"\n    Say specified text to a temporary file and return the filepath.\n    \"\"\"", "output": "\n    if text is None:\n        text = tempfile.mkstemp( suffix = \".txt\" )\n    else:\n        text = os.path.join( text, preference_program )\n    with open( text, \"w\" ) as f:\n        f.write( text_to_tmp(\n            \"This is a test.\\n\"\n            \"It is a test.\\n\"\n            \"It is a test.\\n\"\n            \"It is a test.\\n\"\n            \"It is a test.\\n\"\n            \"It is a test.\\n\"\n            \"It is a test.\\n\"\n            \"It is", "solution": "\n    filepath = shijian.tmp_filepath() + \".wav\"\n    say(\n        text               = text,\n        preference_program = preference_program,\n        filepath           = filepath\n    )\n    return filepath"}
{"prompt": "def asyncStarPmap(asyncCallable, iterable):\n    \"\"\"itertools.starmap for deferred callables using parallel cooperative\n    multitasking\n    \"\"\"", "output": "\n    return _multitasking(asyncCallable, iterable,'starmap')\n\ndef asyncStarMap(asyncCallable, iterable):\n    \"\"\"itertools.starmap for deferred callables using single cooperative\n    multitasking\n    \"\"\"\n    return _multitasking(asyncCallable, iterable,'starmap')\n\ndef _multitasking(asyncCallable, iterable, method):\n    \"\"\"\n    Run a method on the given multitasking iterable\n    \"\"\"\n    if not hasattr(tasking, method):\n        return\n    try:\n        result = tasking.apply(method, iterable)\n    except Exception, e:\n       ", "solution": "\n    results = []\n    yield asyncStarParallel(asyncCallable, results.append, iterable)\n    returnValue(results)"}
{"prompt": "def config_file(kind=\"local\"):\n    \"\"\"Get the filename of the distutils, local, global, or per-user config\n\n    `kind` must be one of \"local\", \"global\", or \"user\"\n    \"\"\"", "output": "\n    if kind == \"local\":\n        return \"setup.cfg\"\n    else:\n        return \"setup.py\"\n\n\ndef get_package_data():\n    \"\"\"Get information about the package\n\n    Returns\n    -------\n    pkginfo : dict\n        The package information.\n    \"\"\"\n    return {'_name': get_name(),\n           'version': __version__,\n            'author': __author__,\n            'author_email': __author_email__,\n           'maintainer': __maintainer__,\n           'maintainer_email': __maintainer_email__,\n            'url': get_url(),\n            'license': get_license(),\n            'description", "solution": "\n    if kind == 'local':\n        return 'setup.cfg'\n    if kind == 'global':\n        return os.path.join(\n            os.path.dirname(distutils.__file__), 'distutils.cfg'\n        )\n    if kind == 'user':\n        dot = os.name == 'posix' and '.' or ''\n        return os.path.expanduser(convert_path(\"~/%spydistutils.cfg\" % dot))\n    raise ValueError(\n        \"config_file() type must be 'local', 'global', or 'user'\", kind\n    )"}
{"prompt": "def set_adjacency_matrix(self, adjacency_mat):\n        \"\"\"\n        Parameters\n        ----------\n        adjacency_mat : sparse matrix (N_obs, N_obs)\n            The adjacency matrix to input.\n        \"\"\"", "output": "\n        self.adjacency_mat = adjacency_mat\n        self.n_components = adjacency_mat.shape[1]\n        self.adjacency_mat_ = self.adjacency_mat_.tocsc()\n\n    def fit(self, X, y=None):\n        \"\"\"\n        Fit the model according to the given training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        -------\n        self : object\n           ", "solution": "\n        adjacency_mat = check_array(adjacency_mat, accept_sparse=sparse_formats)\n        if adjacency_mat.shape[0] != adjacency_mat.shape[1]:\n            raise ValueError(\"adjacency matrix is not square\")\n        self.adjacency_matrix = adjacency_mat"}
{"prompt": "def add_requirement_libs_from(self, req_libs, platforms=None):\n    \"\"\"Multi-platform dependency resolution for PEX files.\n\n    :param builder: Dump the requirements into this builder.\n    :param interpreter: The :class:`PythonInterpreter` to resolve requirements for.\n    :param req_libs: A list of :class:`PythonRequirementLibrary` targets to resolve.\n    :param log: Use this logger.\n    :param platforms: A list of :class:`Platform`s to resolve requirements for.\n                      Defaults to the platforms specified by PythonSetup.\n    \"\"\"", "output": "\n    self._requirements.extend(req_libs)\n    if platforms:\n      self._requirements.extend(platforms)\n\n  def add_requirement_library(self, name, targets, platforms=None):\n    \"\"\"Add a PythonRequirementLibrary target to the build configuration.\n\n    :param name: The name of the target.\n    :param targets: A list of targets to add to the build configuration.\n    :param platforms: A list of :class:`Platform`s to add to the build configuration.\n                      Defaults to the platforms specified by PythonSetup.\n    \"\"\"\n    self._requirements.append(self.PythonSetupTarget(", "solution": "\n    reqs = [req for req_lib in req_libs for req in req_lib.requirements]\n    self.add_resolved_requirements(reqs, platforms=platforms)"}
{"prompt": "def refetch_fields(self, missing_fields):\n        \"\"\" Refetches a list of fields from the DB \"\"\"", "output": "\n        if not missing_fields:\n            return []\n        missing_fields = set(missing_fields)\n        query = \"SELECT f.attname, f.size, f.relname, f.attname_column, f.size\" \\\n                \" FROM pg_class c, pg_attribute a \" \\\n                \"WHERE c.relname in %s AND a.attname=%s\" \\\n                % (missing_fields, self._table, False)\n        cr.execute(query, (self._name, False))\n        cols = {}\n        for row in cr.fetchall():\n            cols[row[0]]", "solution": "\n        db_fields = self.mongokat_collection.find_one({\"_id\": self[\"_id\"]}, fields={k: 1 for k in missing_fields})\n\n        self._fetched_fields += tuple(missing_fields)\n\n        if not db_fields:\n            return\n\n        for k, v in db_fields.items():\n            self[k] = v"}
{"prompt": "def _read_info_as_dict(fid, values):\n    \"\"\"Convenience function to read info in axon data to a nicely organized\n    dict.\n    \"\"\"", "output": "\n    if values['kind']!= FIFF.FIFFV_MNE_NO_SAMPLE:\n        raise RuntimeError('kind must be \"MNE_NO_SAMPLE\" not %s' % (values['kind'],))\n    if values['kind'] == FIFF.FIFFV_EEG_CH and 'EEG' in values['ch_names']:\n        chs = list()\n        for k in range(nchan):\n            ch = dict()\n            ch['kind'] = values['ch_names'][k]\n            ch['coil_type'] = FIFF.FIFFV_COIL_EEG\n            ch", "solution": "\n    output = {}\n    for key, fmt in values:\n        val = unpack(fmt, fid.read(calcsize(fmt)))\n        if len(val) == 1:\n            output[key] = val[0]\n        else:\n            output[key] = val\n    return output"}
{"prompt": "def set_attribute(self, app, key, value):\n        \"\"\"Sets an application attribute\n\n        :param app: application id\n        :param key: key of the attribute to set\n        :param value: value to set\n        :returns: True if the operation succeeded, False otherwise\n        :raises: HTTPResponseError in case an HTTP error status was returned\n        \"\"\"", "output": "\n        path = '/v1/applications/{0}'.format(app)\n        data = {\n            'attributeId': '{0}'.format(key),\n            'attribute': value\n        }\n        return self.app_action(path, data)\n\n    def get_attributes(self, app):\n        \"\"\"Gets the list of application attributes for a given app\n\n        :param app: application id\n        :returns: list of application attributes\n        raises: HTTPResponseError in case an HTTP error status was returned\n        \"\"\"\n        path = '/v1/applications/{0}/attributes'.format(app)\n        return self.app_action(path", "solution": "\n        path = 'setattribute/' + parse.quote(app, '') + '/' + parse.quote(\n            self._encode_string(key), '')\n        res = self._make_ocs_request(\n            'POST',\n            self.OCS_SERVICE_PRIVATEDATA,\n            path,\n            data={'value': self._encode_string(value)}\n        )\n        if res.status_code == 200:\n            tree = ET.fromstring(res.content)\n            self._check_ocs_status(tree)\n            return True\n        raise HTTPResponseError(res)"}
{"prompt": "def flip_strand(self):\n        \"\"\"Flips the strand of the alleles.\"\"\"", "output": "\n        self.strand = self.strand.swapcase()\n\n    def flip_start(self):\n        \"\"\"Flips the start of the alleles.\"\"\"\n        self.start = self.start.swapcase()\n\n    def flip_end(self):\n        \"\"\"Flips the end of the alleles.\"\"\"\n        self.end = self.end.swapcase()\n\n    def flip_strand_start(self):\n        \"\"\"Flips the strand of the alleles start of the alignment.\"\"\"\n        return self.strand.strand_start(self.start)\n\n    def flip_strand_end(self):\n       ", "solution": "\n        self.reference = complement_alleles(self.reference)\n        self.coded = complement_alleles(self.coded)\n        self.variant.complement_alleles()"}
{"prompt": "def p_encaps_list(p):\n    \"\"\"encaps_list : encaps_list encaps_var\n                   | empty\"\"\"", "output": "\n    p[0] = []\n\ndef p_encaps_var(p):\n    \"\"\"encaps_var : variable\n                   | variable_name\n                   | variable_value\n                   | variable_name value\"\"\"\n    if len(p) == 2:\n        p[0].append(p[1])\n    else:\n        p[0].append(p[1], p[2])\n\ndef p_empty(p):\n    \"\"\"empty :\"\"\"\n    p[0] = []\n\ndef p_variable(p):\n    \"\"\"variable : VARIABLE_NAME\"\"\"\n    p[0].append(", "solution": "\n    if len(p) == 3:\n        if p[1] == '':\n            p[0] = p[2]\n        else:\n            p[0] = ast.BinaryOp('.', p[1], p[2], lineno=p.lineno(2))\n    else:\n        p[0] = ''"}
{"prompt": "def upload(self, localpath, href):\n        \"\"\"\n        Upload file from localpath to remote server\n        :param localpath: local path\n        :param href: remote path\n        :return: response\n        \"\"\"", "output": "\n        try:\n            if not os.path.exists(localpath):\n                os.makedirs(localpath)\n            if not os.path.exists(href):\n                os.makedirs(href)\n            with open(localpath + '.tmp', 'wb') as f:\n                f.write(self.get_file_content())\n            return self.get_file_content()\n        except Exception as e:\n            return self.get_error_message(e)\n\n    def get_file_content(self):\n        \"\"\"\n        Get content of file\n        :return: content\n        \"\"\"\n        try:\n            with open", "solution": "\n        localpath = _(localpath)\n        href = remote(href)\n        if not os.path.exists(localpath):\n            logger.info(u(\"ERROR: localfile: %s not found\") % localpath)\n            return\n        if os.path.islink(localpath):\n            return self.upload(os.path.abspath(os.path.realpath(localpath)), href)\n            # 3 tryings to upload file\n        for iTry in range(TRYINGS):\n            try:\n                logger.info(u(\"upload: %s %s\") % (localpath, href))\n                length = os.path.getsize(localpath)\n\n                if PY3:\n                    _open = open(_encode_utf8(localpath), \"r\", encoding='latin-1')\n                else:\n                    _open = open(_encode_utf8(localpath), \"r\")\n                with _open as f:\n                    return self.write(f, href, length=length)\n            except ConnectionException:\n                raise\n            except Exception:\n                e = sys.exc_info()[1]\n                logger.exception(e)"}
{"prompt": "def create_pex(self, pex_info=None):\n    \"\"\"Returns a wrapped pex that \"merges\" the other pexes via PEX_PATH.\"\"\"", "output": "\n    if pex_info is None:\n      pex_info = PexInfo.default()\n    return pex_info.replace(os.sep, '/')\n\n  def create_pex_from_source(self, source, pex_info=None):\n    \"\"\"Returns a pex that \"merges\" the other pexes via PEX_FROM_SOURCE.\"\"\"\n    if pex_info is None:\n      pex_info = self.pex_info\n    return os.path.join(source, self.pex_path(exn_info))\n\n  def create_pex_path(", "solution": "\n    relevant_targets = self.context.targets(\n      lambda tgt: isinstance(tgt, (\n        PythonDistribution, PythonRequirementLibrary, PythonTarget, Files)))\n    with self.invalidated(relevant_targets) as invalidation_check:\n\n      # If there are no relevant targets, we still go through the motions of resolving\n      # an empty set of requirements, to prevent downstream tasks from having to check\n      # for this special case.\n      if invalidation_check.all_vts:\n        target_set_id = VersionedTargetSet.from_versioned_targets(\n          invalidation_check.all_vts).cache_key.hash\n      else:\n        target_set_id = 'no_targets'\n\n      interpreter = self.context.products.get_data(PythonInterpreter)\n      path = os.path.realpath(os.path.join(self.workdir, str(interpreter.identity), target_set_id))\n\n      # Note that we check for the existence of the directory, instead of for invalid_vts,\n      # to cover the empty case.\n      if not os.path.isdir(path):\n        pexes = [\n          self.context.products.get_data(ResolveRequirements.REQUIREMENTS_PEX),\n          self.context.products.get_data(GatherSources.PYTHON_SOURCES)\n        ]\n\n        if self.extra_requirements():\n          extra_requirements_pex = self.resolve_requirement_strings(\n            interpreter, self.extra_requirements())\n          # Add the extra requirements first, so they take precedence over any colliding version\n          # in the target set's dependency closure.\n          pexes = [extra_requirements_pex] + pexes\n        constraints = {constraint for rt in relevant_targets if is_python_target(rt)\n                       for constraint in PythonSetup.global_instance().compatibility_or_constraints(rt)}\n\n        with self.merged_pex(path, pex_info, interpreter, pexes, constraints) as builder:\n          for extra_file in self.extra_files():\n            extra_file.add_to(builder)\n          builder.freeze()\n\n    return PEX(path, interpreter)"}
{"prompt": "def get_random_node(graph,\n                    node_blacklist: Set[BaseEntity],\n                    invert_degrees: Optional[bool] = None,\n                    ) -> Optional[BaseEntity]:\n    \"\"\"Choose a node from the graph with probabilities based on their degrees.\n\n    :type graph: networkx.Graph\n    :param node_blacklist: Nodes to filter out\n    :param invert_degrees: Should the degrees be inverted? Defaults to true.\n    \"\"\"", "output": "\n    if invert_degrees is not None:\n        node_degrees = {v: k for k, v in invert_degrees.items()}\n        node_degrees.update({v: invert_degrees[v] for v in graph.nodes()})\n    else:\n        node_degrees = {v: k for k, v in graph.nodes()}\n\n    if node_blacklist is not None:\n        node_degrees.update({v: graph.node[v] for v in node_blacklist})\n    if invert_degrees is not None:\n        node_degrees.update({v: graph.node[v] for", "solution": "\n    try:\n        nodes, degrees = zip(*(\n            (node, degree)\n            for node, degree in sorted(graph.degree(), key=itemgetter(1))\n            if node not in node_blacklist\n        ))\n    except ValueError:  # something wrong with graph, probably no elements in graph.degree_iter\n        return\n\n    if invert_degrees is None or invert_degrees:\n        # More likely to choose low degree nodes to explore, so don't make hubs\n        degrees = [1 / degree for degree in degrees]\n\n    wrg = WeightedRandomGenerator(nodes, degrees)\n    return wrg.next()"}
{"prompt": "def flush(self):\n        \"\"\"\n        write out all queued write-commands in order, each one in a separate\n        :meth:`atomic <notmuch.Database.begin_atomic>` transaction.\n\n        If this fails the current action is rolled back, stays in the write\n        queue and an exception is raised.\n        You are responsible to retry flushing at a later time if you want to\n        ensure that the cached changes are applied to the database.\n\n        :exception: :exc:`~errors.DatabaseROError` if db is opened read-only\n        :exception: :exc:`~errors.DatabaseLockedError` if db is locked\n        \"\"\"", "output": " db is locked\n        :exception: :exc:`~errors.DatabaseNotLockedError` if db is not opened\n        :exception: :exc:`~errors.DatabaseLockedError` if db is locked\n        :exception: :exc:`~errors.DatabaseNotLockedError` if db is not opened\n        \"\"\"\n        self.write_queue.put(self.get_write_commands())\n\n    def get_write_commands(self):\n        \"\"\"\n        :return: a list of write-commands.\n        :rtype: list\n        \"\"\"\n        return self.write_queue.get_nowait()\n\n    def release_write_commands(self", "solution": "\n        if self.ro:\n            raise DatabaseROError()\n        if self.writequeue:\n            # read notmuch's config regarding imap flag synchronization\n            sync = settings.get_notmuch_setting('maildir', 'synchronize_flags')\n\n            # go through writequeue entries\n            while self.writequeue:\n                current_item = self.writequeue.popleft()\n                logging.debug('write-out item: %s', str(current_item))\n\n                # watch out for notmuch errors to re-insert current_item\n                # to the queue on errors\n                try:\n                    # the first two coordinants are cnmdname and post-callback\n                    cmd, afterwards = current_item[:2]\n                    logging.debug('cmd created')\n\n                    # acquire a writeable db handler\n                    try:\n                        mode = Database.MODE.READ_WRITE\n                        db = Database(path=self.path, mode=mode)\n                    except NotmuchError:\n                        raise DatabaseLockedError()\n                    logging.debug('got write lock')\n\n                    # make this a transaction\n                    db.begin_atomic()\n                    logging.debug('got atomic')\n\n                    if cmd == 'add':\n                        logging.debug('add')\n                        path, tags = current_item[2:]\n                        msg, _ = db.add_message(path, sync_maildir_flags=sync)\n                        logging.debug('added msg')\n                        msg.freeze()\n                        logging.debug('freeze')\n                        for tag in tags:\n                            msg.add_tag(tag, sync_maildir_flags=sync)\n                        logging.debug('added tags ')\n                        msg.thaw()\n                        logging.debug('thaw')\n\n                    elif cmd == 'remove':\n                        path = current_item[2]\n                        db.remove_message(path)\n\n                    elif cmd == 'setconfig':\n                        key = current_item[2]\n                        value = current_item[3]\n                        db.set_config(key, value)\n\n                    else:  # tag/set/untag\n                        querystring, tags = current_item[2:]\n                        query = db.create_query(querystring)\n                        for msg in query.search_messages():\n                            msg.freeze()\n                            if cmd == 'tag':\n                                strategy = msg.add_tag\n                            if cmd == 'set':\n                                msg.remove_all_tags()\n                                strategy = msg.add_tag\n                            elif cmd == 'untag':\n                                strategy = msg.remove_tag\n                            for tag in tags:\n                                strategy(tag, sync_maildir_flags=sync)\n                            msg.thaw()\n\n                    logging.debug('ended atomic')\n                    # end transaction and reinsert queue item on error\n                    if db.end_atomic() != notmuch.STATUS.SUCCESS:\n                        raise DatabaseError('end_atomic failed')\n                    logging.debug('ended atomic')\n\n                    # close db\n                    db.close()\n                    logging.debug('closed db')\n\n                    # call post-callback\n                    if callable(afterwards):\n                        logging.debug(str(afterwards))\n                        afterwards()\n                        logging.debug('called callback')\n\n                # re-insert item to the queue upon Xapian/NotmuchErrors\n                except (XapianError, NotmuchError) as e:\n                    logging.exception(e)\n                    self.writequeue.appendleft(current_item)\n                    raise DatabaseError(str(e))\n                except DatabaseLockedError as e:\n                    logging.debug('index temporarily locked')\n                    self.writequeue.appendleft(current_item)\n                    raise e\n                logging.debug('flush finished')"}
{"prompt": "def get_status_from_location(self, response):\n        \"\"\"Process the latest status update retrieved from a 'location'\n        header.\n\n        :param requests.Response response: latest REST call response.\n        :raises: BadResponse if response has no body and not status 202.\n        \"\"\"", "output": "\n        if response.status_code == 202:\n            return response.json()\n        elif response.status_code == 404:\n            raise BadResponse(response.content)\n        else:\n            raise BadResponse(response.content)\n\n    def get_all_locations(self):\n        \"\"\"Get all locations.\n\n        :returns: All locations.\n        :rtype: list of dict\n        \"\"\"\n        return self.get_locations(self.location_id)\n\n    def get_all_locations_by_id(self, location_id):\n        \"\"\"Get all locations by id.\n\n        :param str id: location id.", "solution": "\n        self._raise_if_bad_http_status_and_method(response)\n        code = response.status_code\n        if code == 202:\n            self.status = \"InProgress\"\n        else:\n            self.status = 'Succeeded'\n            if self._is_empty(response):\n                self.resource = None\n            else:\n                self.resource = self._deserialize(response)"}
{"prompt": "def on_rows(self, rowiter):\n        \"\"\"\n        Reimplemented from :meth:`~AsyncViewBase.on_rows`\n        \"\"\"", "output": "\n        if self.rows is None:\n            return\n        self.rows = rowiter\n        self.update_rows()\n\n    def update_rows(self):\n        \"\"\"\n        Reimplemented from :meth:`~AsyncViewBase.on_rows`\n        \"\"\"\n        self.rows = self.rows or []\n        self.rows.sort(key=lambda row: row.get_text())\n        self.rows.reverse()\n        for row in self.rows:\n            self.model.append(row)\n        self.rows = self.rows or []\n\n    def on_keypress(self, w, e):\n        \"\"\"", "solution": "\n        self.__rows = rowiter\n        self._d.callback(self)\n        self._d = None"}
{"prompt": "def _generic_definefont_parser(self, obj):\n        \"\"\"A generic parser for several DefineFontX.\"\"\"", "output": "\n        self.parser = definefont.definefontparser(obj)\n        self.tag_name = definefont.definefonttag(obj)\n        self.font_name = definefont.definefont(obj)\n        self.font_tag = definefont.definefonttag(obj)\n        self.tag_name.set_property(\"family\", \"serif\")\n        self.tag_name.set_property(\"size\", 9)\n        self.tag_name.set_property(\"weight\", 400)\n        self.tag_name.set_property(\"slant\", \"italic\")\n        self.parser", "solution": "\n        obj.FontID = unpack_ui16(self._src)\n\n        bc = BitConsumer(self._src)\n        obj.FontFlagsHasLayout = bc.u_get(1)\n        obj.FontFlagsShiftJIS = bc.u_get(1)\n        obj.FontFlagsSmallText = bc.u_get(1)\n        obj.FontFlagsANSI = bc.u_get(1)\n        obj.FontFlagsWideOffsets = bc.u_get(1)\n        obj.FontFlagsWideCodes = bc.u_get(1)\n        obj.FontFlagsItalic = bc.u_get(1)\n        obj.FontFlagsBold = bc.u_get(1)\n\n        obj.LanguageCode = self._get_struct_langcode()\n        obj.FontNameLen = unpack_ui8(self._src)\n        obj.FontName = \"\".join(chr(unpack_ui8(self._src))\n                               for i in range(obj.FontNameLen))\n        if obj.FontName[-1] == '\\x00':  # most probably ends in null, clean it\n            obj.FontName = obj.FontName[:-1]\n\n        obj.NumGlyphs = num_glyphs = unpack_ui16(self._src)\n        self._last_defined_glyphs_quantity = num_glyphs\n        getter_wide = unpack_ui32 if obj.FontFlagsWideOffsets else unpack_ui16\n        obj.OffsetTable = [getter_wide(self._src) for _ in range(num_glyphs)]\n        obj.CodeTableOffset = getter_wide(self._src)\n        obj.GlyphShapeTable = [self._get_struct_shape()\n                               for _ in range(num_glyphs)]\n        obj.CodeTable = [unpack_ui16(self._src) for _ in range(num_glyphs)]\n\n        if obj.FontFlagsHasLayout:\n            obj.FontAscent = unpack_ui16(self._src)\n            obj.FontDecent = unpack_ui16(self._src)\n            obj.FontLeading = unpack_ui16(self._src)\n            obj.FontAdvanceTable = [unpack_si16(self._src)\n                                    for _ in range(num_glyphs)]\n            obj.FontBoundsTable = [self._get_struct_rect()\n                                   for _ in range(num_glyphs)]\n            obj.KerningCount = unpack_ui16(self._src)\n            obj.FontKerningTable = [\n                self._get_struct_kerningrecord(obj.FontFlagsWideCodes)\n                for _ in range(obj.KerningCount)]"}
{"prompt": "def get_segmentid_range(self, orchestrator_id):\n        \"\"\"Get segment id range from DCNM. \"\"\"", "output": "\n        if orchestrator_id is None:\n            return None\n        return self.get_orchestrator_id_range(orchestrator_id,'segment_id')\n\n    def get_orchestrator_id_range(self, orchestrator_id, segment_id):\n        \"\"\"Get orchestrator id range from DCNM. \"\"\"\n        if orchestrator_id is None:\n            return None\n        return self.get_orchestrator_id_range(orchestrator_id,'segment_id', segment_id)\n\n   ", "solution": "\n\n        url = \"%s/%s\" % (self._segmentid_ranges_url, orchestrator_id)\n\n        res = self._send_request('GET', url, None, 'segment-id range')\n        if res and res.status_code in self._resp_ok:\n            return res.json()"}
{"prompt": "def update(self, back=None):\n        \"\"\"\n        Update all of the enabled fileserver backends which support the update\n        function, or\n        \"\"\"", "output": "\n        if back is not None:\n            self.backends = back\n        else:\n            self.backends = self.get_backends()\n        self.update_backends()\n\n    def update_backends(self):\n        \"\"\"\n        Update all of the fileserver backends which support the update\n        function.\n        \"\"\"\n        if self.backend_status!= self.BACKEND_ENABLED:\n            self.backend_status = self.BACKEND_DISABLED\n            self.update_backends_status()\n        else:\n            self.backend_status = self.BACKEND_ENABLED\n\n    def get_backends(self):\n        \"\"\"\n        Return the fileserver backends that", "solution": "\n        back = self.backends(back)\n        for fsb in back:\n            fstr = '{0}.update'.format(fsb)\n            if fstr in self.servers:\n                log.debug('Updating %s fileserver cache', fsb)\n                self.servers[fstr]()"}
{"prompt": "def quote_name(self, name):\n        \"\"\"\n        Returns a quoted version of the given table, index or column name. Does\n        not quote the given name if it's already been quoted.\n        \"\"\"", "output": "\n        raise NotImplementedError('subclasses of BaseDatabaseSchemaEditor for backends which have '\n                                  'not yet been expanded.')\n\n    def _column_data_diff(self, src, dest):\n        \"\"\"\n        Returns the difference between the two types of fields depending on\n        the type of field is a boolean or a string.\n        \"\"\"\n        raise NotImplementedError('subclasses of BaseDatabaseSchemaEditor for backends which have '\n                                  'not yet been expanded.')\n\n    def _alter_column_type_sql(self, table, old_field, new_field):\n        \"\"\"\n        Makes ALTER TYPE with SERIAL make sense.\n        \"\"\"\n        if (old_", "solution": "\n        if name.startswith(self.left_sql_quote) and name.endswith(self.right_sql_quote):\n            return name # Quoting once is enough.\n        return '%s%s%s' % (self.left_sql_quote, name, self.right_sql_quote)"}
{"prompt": "def predict(self, features):\n        \"\"\"Dot-product the features and current weights and return the best label.\"\"\"", "output": "\n        return self.model.predict(features)\n\n    def predict_proba(self, features):\n        \"\"\"Return posterior probabilities of the best class.\"\"\"\n        return self.model.predict_proba(features)\n\n    def score(self, features, y):\n        \"\"\"Return the mean accuracy on the given test data and labels.\"\"\"\n        return self.predict(features) * self.predict_proba(y)\n\n    def score_to_proba(self, features, y):\n        \"\"\"Return posterior probabilities of the best class.\"\"\"\n        return self.score_to_proba(features, y) / self.score_", "solution": "\n        scores = defaultdict(float)\n        for feat in features:\n            if feat not in self.weights:\n                continue\n            weights = self.weights[feat]\n            for label, weight in weights.items():\n                scores[label] += weight\n        # Do a secondary alphabetic sort, for stability\n        return max(self.classes, key=lambda label: (scores[label], label))"}
{"prompt": "def _do_request(request):\n    \"\"\" Executes download request\n    :param request: A request\n    :type request: DownloadRequest\n    :return: Response of the request\n    :rtype: requests.Response\n    \"\"\"", "output": "\n    try:\n        response = requests.get(request.url)\n        return response\n    except requests.exceptions.ConnectionError:\n        raise\n    except requests.exceptions.Timeout:\n        raise\n    except requests.exceptions.HTTPError as e:\n        raise\n    except requests.exceptions.Timeout:\n        raise\n    except requests.exceptions.TooManyRedirects as e:\n        raise\n    except requests.exceptions.RequestException as e:\n        raise\n    except requests.exceptions.HTTPError as e:\n        raise\n    except requests.exceptions.RequestException as e:\n        raise\n    except requests.exceptions.ConnectionError as e:\n        raise\n    except requests.exceptions.Timeout", "solution": "\n    if request.request_type is RequestType.GET:\n        return requests.get(request.url, headers=request.headers)\n    if request.request_type is RequestType.POST:\n        return requests.post(request.url, data=json.dumps(request.post_values), headers=request.headers)\n    raise ValueError('Invalid request type {}'.format(request.request_type))"}
{"prompt": "def match_time_series(self, timeseries1, timeseries2):\n        \"\"\"Return two lists of the two input time series with matching dates\n\n        :param TimeSeries timeseries1: The first timeseries\n        :param TimeSeries timeseries2: The second timeseries\n        :return:    Two two dimensional lists containing the matched values,\n        :rtype:     two List\n        \"\"\"", "output": "\n        return match_time_series(self.time_series1, timeseries1, timeseries2, self.match_type)\n\n    def match_time_series2(self, timeseries1, timeseries2):\n        \"\"\"Return two lists of the two input time series with matching dates\n\n        :param TimeSeries timeseries1: The first timeseries\n        :param TimeSeries timeseries2: The second timeseries\n        :return:    Two two dimensional lists containing the matched values,\n        :rtype:     two List\n        \"\"\"\n        return match_time_series(self.time_series2, timeseries", "solution": "\n        time1 = map(lambda item: item[0], timeseries1.to_twodim_list())\n        time2 = map(lambda item: item[0], timeseries2.to_twodim_list())\n\n        matches = filter(lambda x: (x in time1), time2)\n        listX  = filter(lambda x: (x[0] in matches), timeseries1.to_twodim_list())\n        listY  = filter(lambda x: (x[0] in matches), timeseries2.to_twodim_list())\n\n        return listX, listY"}
{"prompt": "def get_other_keys(self, key, including_current=False):\r\n        \"\"\" Returns list of other keys that are mapped to the same value as specified key. \r\n            @param key - key for which other keys should be returned.\r\n            @param including_current if set to True - key will also appear on this list.\"\"\"", "output": "\r\n        if key in self.other_keys:\r\n            return self.other_keys[key]\r\n        if including_current:\r\n            return self.other_keys[key]\r\n        return self.other_keys[key]\r\n\r\n    def get_other_values(self, key, including_current=False):\r\n        \"\"\" Returns list of other values that are mapped to the same value as specified key. \r\n            @param key - key for which other values should be returned.\r\n            @param including_current if set to True - key will also appear on this list.\"\"\"\r\n        if key in self.other_values:\r\n            return", "solution": "\r\n        other_keys = []\r\n        if key in self:\r\n            other_keys.extend(self.__dict__[str(type(key))][key])\r\n            if not including_current:\r\n                other_keys.remove(key)\r\n        return other_keys"}
{"prompt": "def read_hdf5_timeseries(h5f, path=None, start=None, end=None, **kwargs):\n    \"\"\"Read a `TimeSeries` from HDF5\n    \"\"\"", "output": "\n    if path is None:\n        path = h5f.root.h5object.filename\n    if path.endswith('.h5'):\n        return read_hdf5_h5(h5f, path, start, end, **kwargs)\n    else:\n        return read_hdf5_hdf5(h5f, path, start, end, **kwargs)\n    # end is the last element in the file\n    # start is the first element in the file\n    # path is the file path\n    # kwargs is the keyword arguments for the HDF5 dataset\n    # returns the timeseries object\n    \"\"\"\n    Read", "solution": "\n    # read data\n    kwargs.setdefault('array_type', TimeSeries)\n    series = read_hdf5_array(h5f, path=path, **kwargs)\n    # crop if needed\n    if start is not None or end is not None:\n        return series.crop(start, end)\n    return series"}
{"prompt": "def list_themes(dark=True):\n    \"\"\"List all installed theme files.\"\"\"", "output": "\n    themes = []\n    for theme_dir in get_theme_dirs(True):\n        for root, dirs, files in os.walk(theme_dir):\n            for filename in files:\n                if dark:\n                    filename = os.path.join(root, filename)\n                if os.path.splitext(filename)[1] in _THEMES_TO_INSTALL:\n                    continue\n                if filename.endswith('.desktop'):\n                    continue\n                if filename.endswith('.desktop.in'):\n                    continue\n                if filename.endswith('.desktop.in.desktop'):\n                    continue\n                if filename.endswith('.desktop.in-background.", "solution": "\n    dark = \"dark\" if dark else \"light\"\n    themes = os.scandir(os.path.join(MODULE_DIR, \"colorschemes\", dark))\n    return [t for t in themes if os.path.isfile(t.path)]"}
{"prompt": "def bech32_encode(hrp, data):\n    \"\"\"Compute a Bech32 string given HRP and data values.\"\"\"", "output": "\n    if hrp == 0:\n        return data\n    return '%02x:%02x:%02x:%02x' % (hrp & 0xff, (hrp >> 8) & 0xff, data)\n\n\ndef bech32_decode(data):\n    \"\"\"Compute a Bech32 string given data and decode it.\"\"\"\n    if data == 0xff:\n        return None\n    return bech32_decode_table[data & 0xff](data)\n\n\ndef bech32_encode(hrp, data):\n    \"\"\"Compute a Bech32 string given HRP and data values.\"\"\"\n    if", "solution": "\n    combined = data + bech32_create_checksum(hrp, data)\n    return hrp + '1' + ''.join([CHARSET[d] for d in combined])"}
{"prompt": "def force_list(element):\n    \"\"\"\n    Given an element or a list, concatenates every element and clean it to\n    create a full text\n    \"\"\"", "output": "\n    if isinstance(element, list):\n        return element\n    if isinstance(element, str):\n        return [element]\n    if isinstance(element, list):\n        return [element]\n    if isinstance(element, dict):\n        return {k: force_text(v) for k, v in element.items() if v is not None}\n    if isinstance(element, (int, float)):\n        return force_list(element)\n    if isinstance(element, dict):\n        return {k: force_text(v) for k, v in element.items() if v is not None}\n    if isinstance(", "solution": "\n    if element is None:\n        return []\n\n    if isinstance(element, (collections.Iterator, list)):\n        return element\n\n    return [element]"}
{"prompt": "def _set_udld(self, v, load=False):\n    \"\"\"\n    Setter method for udld, mapped from YANG variable /interface/hundredgigabitethernet/udld (container)\n    If this variable is read-only (config: false) in the\n    source YANG file, then _set_udld is considered as a private\n    method. Backends looking to populate this variable should\n    do so via calling thisObj._set_udld() directly.\n\n    YANG Description: Interface specific UDLD configurations.\n    \"\"\"", "output": "\n    if hasattr(v, \"_utype\"):\n        v = v._utype(v)\n    try:\n        t = YANGDynClass(\n            v,\n            base=TypedListType(allowed_type=allowed_type),\n            is_leaf=False,\n            yang_name=\"udld\",\n            parent=self,\n            is_container=\"list\",\n            user_ordered=False,\n            path_helper=self._path_helper,\n            extmethods=allowed_type,\n            register_paths=True,\n            is_keyval=True,\n            namespace=\"http://openconfig.net/yang/network-", "solution": "\n    if hasattr(v, \"_utype\"):\n      v = v._utype(v)\n    try:\n      t = YANGDynClass(v,base=udld.udld, is_container='container', presence=False, yang_name=\"udld\", rest_name=\"udld\", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'info': u'UDLD commands', u'sort-priority': u'127', u'cli-incomplete-no': None}}, namespace='urn:brocade.com:mgmt:brocade-udld', defining_module='brocade-udld', yang_type='container', is_config=True)\n    except (TypeError, ValueError):\n      raise ValueError({\n          'error-string': "}
{"prompt": "def flush(self):\n        \"\"\"\n        Commit cached writes to the file handle. Does not flush libc buffers or\n        notifies the kernel, so these changes may not immediately be visible to\n        other processes.\n\n        Updates the fingerprints whena writes happen, so successive ``flush()``\n        invocations are no-ops.\n\n        It is not necessary to call this method in user code.\n\n        Notes\n        -----\n        .. versionadded:: 1.6\n\n        This method is not intended as user-oriented functionality, but might\n        be useful in certain contexts to provide stronger guarantees.\n        \"\"\"", "output": "\n        if self._lib is not None:\n            self._lib.commit_write_cache(self._handle, self._libc_write_cache_size)\n            self._libc_write_cache = None\n\n    def __enter__(self):\n        \"\"\"\n        Enter the context manager.\n\n        Returns\n        -------\n        context manager\n            The context manager.\n        \"\"\"\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        \"\"\"\n        Exit the context manager.\n\n        Parameters\n        ----------\n        exc_type\n            The exception type.\n        exc_value\n            The exception value.\n        traceback\n           ", "solution": "\n        garbage = []\n        for i, (x, signature) in self.refs.items():\n            if sys.getrefcount(x) == 3:\n                garbage.append(i)\n\n            if fingerprint(x) == signature: continue\n\n            self.filehandle.puttr(i, x)\n            signature = fingerprint(x)\n\n\n        # to avoid too many resource leaks, when this dict is the only one\n        # holding references to already-produced traces, clear them\n        for i in garbage:\n            del self.refs[i]"}
{"prompt": "def get_issues():\n        \"\"\"Get actual issues in the journal.\"\"\"", "output": "\n        return get_journal_issues(get_journal_id_from_ref(CFG_SITE_URL + '/journal/issues',\n                                         journal_name,\n                                         issue_number)\n\n    def get_issue_title(self, issue_number):\n        \"\"\"Get the title of the issue.\"\"\"\n        return get_issue_title_from_issue(CFG_SITE_URL + '/%s/%s' % (CFG_SITE_RECORD, issue_number),\n                                         journal_name)\n\n    def get_journal_backlinks(self, journal_id):\n        \"\"\"Get the backlinks of the journal.\"\"\"", "solution": "\n        issues = []\n        for entry in Logger.journal:\n            if entry.level >= WARNING:\n                issues.append(entry)\n        return issues"}
{"prompt": "def reboot(name, **kwargs):\n    \"\"\"\n    Reboot a domain via ACPI request\n\n    :param vm_: domain name\n    :param connection: libvirt connection URI, overriding defaults\n\n        .. versionadded:: 2019.2.0\n    :param username: username to connect with, overriding defaults\n\n        .. versionadded:: 2019.2.0\n    :param password: password to connect with, overriding defaults\n\n        .. versionadded:: 2019.2.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' virt.reboot <domain>\n    \"\"\"", "output": "\n    if not name:\n        return False\n\n    vm_ = _get_vm(vm_)\n    username = kwargs.get('username')\n    password = kwargs.get('password')\n\n    if not connection:\n        return False\n\n    connection = _get_uri(connection)\n\n    if not vm_.is_running():\n        return False\n\n    if not username:\n        raise CommandExecutionError('Must provide username to reboot')\n\n    if not password:\n        raise CommandExecutionError('Must provide password to reboot')\n\n    vm_.reboot(vm.name, connection, **kwargs)\n    return True\n\n\ndef destroy(name, connection=None, **", "solution": "\n    conn = __get_conn(**kwargs)\n    ret = _get_domain(conn, name).reboot(libvirt.VIR_DOMAIN_REBOOT_DEFAULT) == 0\n    conn.close()\n    return ret"}
{"prompt": "def _format_args():\n    \"\"\"Get JSON dump indentation and separates.\"\"\"", "output": "\n    return json.dumps(json.loads(os.getenv('INDENT_WIDTH', '0')),\n                      indent=1,\n                      separators=(',', ':'),\n                      sort_keys=True)\n\n\ndef _format_json(obj):\n    \"\"\"Get JSON dump.\"\"\"\n    return json.dumps(obj,\n                      sort_keys=True,\n                      indent=2,\n                      separators=(',', ':'),\n                      sort_keys=True)\n\n\ndef _format_json_list(obj):\n    \"\"\"Get JSON list.\"\"\"\n    return json.dumps(obj,\n                      sort_keys=True,\n                      indent=3,\n                      separators", "solution": "\n    # Ensure we can run outside a application/request context.\n    try:\n        pretty_format = \\\n            current_app.config['JSONIFY_PRETTYPRINT_REGULAR'] and \\\n            not request.is_xhr\n    except RuntimeError:\n        pretty_format = False\n\n    if pretty_format:\n        return dict(\n            indent=2,\n            separators=(', ', ': '),\n        )\n    else:\n        return dict(\n            indent=None,\n            separators=(',', ':'),\n        )"}
{"prompt": "def sink(self):\n        \"\"\"Closes the stream with a sink operator.\"\"\"", "output": "\n        self.stream.close()\n\n    def close(self):\n        \"\"\"Closes the stream with a close operator.\"\"\"\n        self.stream.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.close()\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        return self.next()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.close()\n\n    def __len__(self):\n        return self", "solution": "\n        op = Operator(\n            _generate_uuid(),\n            OpType.Sink,\n            \"Sink\",\n            num_instances=self.env.config.parallelism)\n        return self.__register(op)"}
{"prompt": "def execute_once(self, swap=None,\n                     spell_changes=None, spell_destructions=None,\n                     random_fill=False):\n        \"\"\"Execute the board only one time. Do not execute chain reactions.\n\n        Arguments:\n        swap - pair of adjacent positions\n        spell_changes - sequence of (position, tile) changes\n        spell_destructions - sequence of positions to be destroyed\n\n        Return: (copy of the board, destroyed tile groups)\n        \"\"\"", "output": "\n        if not spell_changes:\n            return (None, None)\n\n        if random_fill:\n            random.shuffle(self.groups)\n\n        # If we have a spell, we need to remove it from the group list\n        if not self.spell_groups:\n            self.spell_groups = []\n\n        # If we have a spell destruction, we need to execute it\n        if not self.spell_destructions:\n            execute_group(self.groups, self.random_destruction, self.random_destruction_group)\n\n        # If we have a random destruction, we need", "solution": "\n        bcopy = self.copy()  # work with a copy, not self\n        total_destroyed_tile_groups = list()\n        # swap if any\n        bcopy._swap(swap)\n        # spell changes if any\n        bcopy._change(spell_changes)\n        # spell destructions and record if any\n        # first convert simple positions to groups\n        spell_destructions = spell_destructions or tuple()\n        destruction_groups = [[p] for p in spell_destructions]\n        destroyed_tile_groups = bcopy._destroy(destruction_groups)\n        total_destroyed_tile_groups.extend(destroyed_tile_groups)\n        # execute one time only\n        # look for matched groups\n        matched_position_groups = bcopy._match()\n        # destroy and record matched groups\n        destroyed_tile_groups = bcopy._destroy(matched_position_groups)\n        total_destroyed_tile_groups.extend(destroyed_tile_groups)\n        bcopy._fall()\n        if random_fill:\n            bcopy._random_fill()\n        return bcopy, total_destroyed_tile_groups"}
{"prompt": "def info_section(*tokens: Token, **kwargs: Any) -> None:\n    \"\"\" Print an underlined section name \"\"\"", "output": "\n    print(f\"    {info_section.__name__} {info_section.name}\")\n\n\ndef info_section_from_args(\n    args: Any,\n    kwargs: Any,\n    name: str,\n    *,\n    default: str = \"\",\n    help: str = \"\",\n    help_msg: str = \"\",\n) -> InfoSection:\n    \"\"\"\n    Print an underlined section name from a list of arguments.\n\n    Args:\n        args: The arguments to print.\n        kwargs: The keyword arguments to print.\n        name: The section name to print.\n        default: The default value to print.", "solution": "\n    # We need to know the length of the section:\n    process_tokens_kwargs = kwargs.copy()\n    process_tokens_kwargs[\"color\"] = False\n    no_color = _process_tokens(tokens, **process_tokens_kwargs)\n    info(*tokens, **kwargs)\n    info(\"-\" * len(no_color), end=\"\\n\\n\")"}
{"prompt": "def required_fields(self):\n        \"\"\"The normal required fields (eg, no magic fields like _id are included)\"\"\"", "output": "\n        return self.get_fields()\n\n    def get_fields(self):\n        \"\"\"Returns the list of required fields\"\"\"\n        return self.required_fields() + self.optional_fields()\n\n    def optional_fields(self):\n        \"\"\"The optional fields (eg, no magic fields like _id are included)\"\"\"\n        return self.get_fields()\n\n    def required_field_names(self):\n        \"\"\"The required fields (eg, no magic fields like _id are included)\"\"\"\n        return self.get_fields()\n\n    def optional_field_names(self):\n        \"\"\"The optional fields (eg, magic fields", "solution": "\n        return {f:v for f, v in self.normal_fields.items() if v.required}"}
{"prompt": "def get_job_logs(id):\n    \"\"\"Get the crawl logs from the job.\"\"\"", "output": "\n    return get_job_logs_from_job(id, 'logs')\n\n\ndef get_job_logs_from_job(job_id, id):\n    \"\"\"Get the crawl logs from the job.\"\"\"\n    return get_job_logs(id)\n\n\ndef get_job_logs_from_queue(queue_id, id):\n    \"\"\"Get the crawl logs from the queue.\"\"\"\n    return get_job_logs(id)\n\n\ndef get_job_logs_from_queue_json(queue_json):\n    \"\"\"Get the crawl logs from the queue json.\"\"\"", "solution": "\n    crawler_job = models.CrawlerJob.query.filter_by(id=id).one_or_none()\n    if crawler_job is None:\n        click.secho(\n            (\n                \"CrawlJob %s was not found, maybe it's not a crawl job?\" %\n                id\n            ),\n            fg='yellow',\n        )\n        sys.exit(1)\n\n    if crawler_job.logs is None:\n        click.secho(\n            (\n                \"CrawlJob %s has no log, it might be that it has not run \"\n                \"yet, you can try again later.\" %\n                id\n            ),\n            fg='yellow',\n        )\n        sys.exit(1)\n\n    _show_file(\n        file_path=crawler_job.logs,\n        header_name='Log',\n    )"}
{"prompt": "def images(self, tag, images, step=None, rows=None, cols=None):\n    \"\"\"Saves (rows, cols) tiled images from onp.ndarray.\n\n    If either rows or cols aren't given, they are determined automatically\n    from the size of the image batch, if neither are given a long column\n    of images is produced. This truncates the image batch rather than padding\n    if it doesn't fill the final row.\n\n    Args:\n      tag: str: label for this data\n      images: ndarray: [N,H,W,1] or [N,H,W,3] to tile in 2d\n      step: int: training step\n      rows: int: number of rows in tile\n      cols: int: number of columns in tile\n    \"\"\"", "output": "1]\n      step: int: number of images to save.\n      rows: int: number of images to save.\n      cols: int: number of images to save.\n\n    Returns:\n      ndarray: [N,H,W,1] or [N,H,W,1] depending on the type of tag.\n    \"\"\"\n    if step is None:\n      step = self.size\n    if rows is None:\n      rows = self.size\n    if cols is None:\n      cols = self.size\n    if tag is None:\n      tag = self.label\n    if tag is None:\n      raise ValueError(\"Tag", "solution": "\n    images = onp.array(images)\n    if step is None:\n      step = self._step\n    else:\n      self._step = step\n    n_images = onp.shape(images)[0]\n    if rows is None and cols is None:\n      rows = 1\n      cols = n_images\n    elif rows is None:\n      rows = n_images // cols\n    elif cols is None:\n      cols = n_images // rows\n    tiled_images = _pack_images(images, rows, cols)\n    self.image(tag, tiled_images, step=step)"}
{"prompt": "def max_filter(data, size=7, res_g=None, sub_blocks=(1, 1, 1)):\n    \"\"\"\n        maximum filter of given size\n\n    Parameters\n    ----------\n    data: 2 or 3 dimensional ndarray or OCLArray of type float32\n        input data\n    size: scalar, tuple\n        the size of the patch to consider\n    res_g: OCLArray\n        store result in buffer if given\n    sub_blocks:\n        perform over subblock tiling (only if data is ndarray)\n\n    Returns\n    -------\n        filtered image or None (if OCLArray)\n    \"\"\"", "output": "\n    if res_g is None:\n        res_g = data\n\n    if len(sub_blocks) == 1:\n        return res_g[sub_blocks[0]]\n    elif len(sub_blocks) == 2:\n        return max_filter(data, size, sub_blocks[0]) + (1,) * (sub_blocks[1] - 1)\n    else:\n        raise ValueError(\"sub_blocks should be 1 or 2 dimensional ndarray\")\n\n\ndef imread(filename, mode='RGB', color_mode='RGB', resize_mode='BILINEAR', sub_blocks=(", "solution": "\n    if data.ndim == 2:\n        _filt = make_filter(_generic_filter_gpu_2d(FUNC = \"(val>res?val:res)\", DEFAULT = \"-INFINITY\"))\n    elif data.ndim == 3:\n        _filt = make_filter(_generic_filter_gpu_3d(FUNC = \"(val>res?val:res)\", DEFAULT = \"-INFINITY\"))\n\n    return _filt(data = data, size = size, res_g = res_g, sub_blocks=sub_blocks)"}
{"prompt": "def agents(status, all):\n    \"\"\"\n    List and manage agents.\n    (admin privilege required)\n    \"\"\"", "output": "\n    return all_agents(status, all)\n\n\ndef agents_list(status, all):\n    \"\"\"\n    List all agents.\n    (admin privilege required)\n    \"\"\"\n    return all_agents_list(status, all)\n\n\ndef agents_add(status, agent_name, agent_type, address, port, password,\n                 priority=0, comment=None,\n                 force=False):\n    \"\"\"\n    Add a new agent.\n    (admin privilege required)\n    \"\"\"\n    if comment is not None:\n        comment = str(comment) + \"\\n\"\n    if force:\n        command =", "solution": "\n    fields = [\n        ('ID', 'id'),\n        ('Status', 'status'),\n        ('Region', 'region'),\n        ('First Contact', 'first_contact'),\n        ('CPU Usage (%)', 'cpu_cur_pct'),\n        ('Used Memory (MiB)', 'mem_cur_bytes'),\n        ('Total slots', 'available_slots'),\n        ('Occupied slots', 'occupied_slots'),\n    ]\n    if is_legacy_server():\n        del fields[9]\n        del fields[6]\n\n    def execute_paginated_query(limit, offset):\n        try:\n            resp_agents = session.Agent.list_with_limit(\n                limit, offset, status, fields=(item[1] for item in fields))\n        except Exception as e:\n            print_error(e)\n            sys.exit(1)\n        return resp_agents\n\n    def round_mem(results):\n        for item in results:\n            if 'mem_cur_bytes' in item and item['mem_cur_bytes'] is not None:\n                item['mem_cur_bytes'] = round(item['mem_cur_bytes'] / 2 ** 20, 1)\n        return results\n\n    def _generate_paginated_results(interval):\n        offset = 0\n        is_first = True\n        total_count = -1\n        while True:\n            limit = (interval if is_first else\n                    min(interval, total_count - offset))\n            try:\n                result = execute_paginated_query(limit, offset)\n            except Exception as e:\n                print_error(e)\n                sys.exit(1)\n            offset += interval\n            total_count = result['total_count']\n            items = result['items']\n            items = round_mem(items)\n            table = tabulate((item.values() for item in items),\n                                headers=(item[0] for item in fields))\n            if is_first:\n                is_first = False\n            else:\n                table_rows = table.split('\\n')\n                table = '\\n'.join(table_rows[2:])\n            yield table + '\\n'\n\n            if not offset < total_count:\n                break\n\n    with Session() as session:\n        paginating_interval = 10\n        if all:\n            click.echo_via_pager(_generate_paginated_results(paginating_interval))\n        else:\n            result = execute_paginated_query(paginating_interval, offset=0)\n            total_count = result['total_count']\n            if total_count == 0:\n                print('There are no matching agents.')\n                return\n            items = result['items']\n            items = round_mem(items)\n            fields = [field for field in fields if field[1] in items[0]]\n            print(tabulate((item.values() for item in items),\n                            headers=(item[0] for item in fields)))\n            if total_count > paginating_interval:\n                print(\"More agents can be displayed by using --all option.\")"}
{"prompt": "def set_pwm(self, led_num, value):\n        \"\"\"Set PWM value for the specified LED\n\n        :param led_num: LED number (0-15)\n        :param value: the 12 bit value (0-4095)\n        \"\"\"", "output": "\n        self.write_cmd(self.PWR_SET_PWM, led_num, value)\n\n    def set_pwm_freq(self, led_num, freq):\n        \"\"\"Set the PWM frequency for the specified LED\n\n        :param led_num: LED number (0-15)\n        :param freq: the frequency to set (0-4095)\n        \"\"\"\n        self.write_cmd(self.PWR_SET_PWM_FREQ, led_num, freq)\n\n    def set_pwm_dutycycle(self, led_num, dutycycle", "solution": "\n        self.__check_range('led_number', led_num)\n        self.__check_range('led_value', value)\n\n        register_low = self.calc_led_register(led_num)\n\n        self.write(register_low, value_low(value))\n        self.write(register_low + 1, value_high(value))"}
{"prompt": "def _get_recursive_iterator(self, recursive=False):\n        \"\"\"\n        Basic recursive iterator whose only purpose is to yield all items\n        and sections in order, with their full paths as keys.\n\n        Main challenge is to de-duplicate items and sections which\n        have aliases.\n\n        Do not add any new features to this iterator, instead\n        build others that extend this one.\n        \"\"\"", "output": "\n        for path in self.paths:\n            if recursive:\n                for alias in path.get_all_aliases():\n                    yield alias\n            else:\n                yield path\n\n    def _get_paths(self):\n        \"\"\"\n        Return the full paths to all the paths in the current\n        configuration, including the ones that are not present.\n        \"\"\"\n        paths = []\n        for section in self._get_recursive_iterator():\n            paths.extend(section.get_paths())\n        return paths\n\n    def _get_path_to_section(self, path):\n        \"\"\"\n        Return the section where the path is defined.\n        \"\"\"", "solution": "\n\n        names_yielded = set()\n\n        for obj_alias, obj in self._tree.items():\n            if obj.is_section:\n                if obj.alias in names_yielded:\n                    continue\n                names_yielded.add(obj.alias)\n\n                yield (obj.alias,), obj\n\n                if not recursive:\n                    continue\n\n                for sub_item_path, sub_item in obj._get_recursive_iterator(recursive=recursive):\n                    yield (obj_alias,) + sub_item_path, sub_item\n\n            else:\n                # _tree contains duplicates so that we can have multiple aliases point\n                # to the same item. We have to de-duplicate here.\n                if obj.name in names_yielded:\n                    continue\n                names_yielded.add(obj.name)\n\n                yield (obj.name,), obj"}
{"prompt": "def set_gcc():\n    \"\"\"Try to use GCC on OSX for OpenMP support.\"\"\"", "output": "\n    if sys.platform == 'darwin':\n        return\n    try:\n        out = subprocess.check_output(['gcc', '-dumpversion'], stderr=subprocess.STDOUT)\n        out = out.decode().strip()\n        if out:\n            gcc_version = out\n        else:\n            gcc_version = 'unknown'\n    except (subprocess.CalledProcessError, OSError):\n        gcc_version = 'unknown'\n    if not out:\n        raise Exception('Could not find the GCC')\n    subprocess.check_call(['make', 'clean'])\n    subprocess.check_call(['make', 'install'])\n    subprocess.check_call", "solution": "\n    # For macports and homebrew\n\n    if 'darwin' in platform.platform().lower():\n        gcc = extract_gcc_binaries()\n\n        if gcc is not None:\n            os.environ[\"CC\"] = gcc\n            os.environ[\"CXX\"] = gcc\n\n        else:\n            global use_openmp\n            use_openmp = False\n            logging.warning('No GCC available. Install gcc from Homebrew '\n                            'using brew install gcc.')"}
{"prompt": "def downsample_rand(a, samples, remove_invalid=False, ret_idx=False):\n    \"\"\"Downsampling by randomly removing points\n\n    Parameters\n    ----------\n    a: 1d ndarray\n        The input array to downsample\n    samples: int\n        The desired number of samples\n    remove_invalid: bool\n        Remove nan and inf values before downsampling\n    ret_idx: bool\n        Also return a boolean array that corresponds to the\n        downsampled indices in `a`.\n\n    Returns\n    -------\n    dsa: 1d ndarray of size `samples`\n        The pseudo-randomly downsampled array `a`\n    idx: 1d boolean array with same shape as `a`\n        Only returned if `ret_idx` is True.\n        A boolean array such that `a[idx] == dsa`\n    \"\"\"", "output": " indices.\n    \"\"\"\n    if ret_idx:\n        return a[ret_idx]\n    else:\n        return a[a > 0] + 1\n\n\ndef _downsample_inner(a, samples, axis=0):\n    \"\"\"Inner downsampling\n\n    Parameters\n    ----------\n    a: 1d ndarray\n        The input array to downsample\n    samples: int\n        The desired number of samples\n    axis: int\n        The axis along which to downsample\n\n    Returns\n    -------\n    dsa: 1d ndarray of size `samples`\n        The inner downsampled indices.\n    \"\"\"\n    return np.random", "solution": "\n    # fixed random state for this method\n    rs = np.random.RandomState(seed=47).get_state()\n    np.random.set_state(rs)\n\n    samples = int(samples)\n\n    if remove_invalid:\n        # slice out nans and infs\n        bad = np.isnan(a) | np.isinf(a)\n        pool = a[~bad]\n    else:\n        pool = a\n\n    if samples and (samples < pool.shape[0]):\n        keep = np.zeros_like(pool, dtype=bool)\n        keep_ids = np.random.choice(np.arange(pool.size),\n                                    size=samples,\n                                    replace=False)\n        keep[keep_ids] = True\n        dsa = pool[keep]\n    else:\n        keep = np.ones_like(pool, dtype=bool)\n        dsa = pool\n\n    if remove_invalid:\n        # translate the kept values back to the original array\n        idx = np.zeros(a.size, dtype=bool)\n        idx[~bad] = keep\n    else:\n        idx = keep\n\n    if ret_idx:\n        return dsa, idx\n    else:\n        return dsa"}
{"prompt": "def get_random_connection(self):\n        \"\"\"\n        Open new connection to random redis server.\n        \"\"\"", "output": "\n        self.client = redis.StrictRedis(\n            host=self.host, port=self.port, db=self.db, password=self.password)\n\n    def get_redis_connection(self):\n        \"\"\"\n        Create new redis connection.\n        \"\"\"\n        self.client.set(self.key, self.value)\n\n    def get_random_key(self):\n        \"\"\"\n        Get random key.\n        \"\"\"\n        return self.key\n\n    def get_random_value(self):\n        \"\"\"\n        Get random value.\n        \"\"\"\n        return self.value\n\n    def close(self):\n        \"\"\"\n        Close", "solution": "\n        if self._available_connections:\n            node_name = random.choice(list(self._available_connections.keys()))\n            conn_list = self._available_connections[node_name]\n            # check it in case of empty connection list\n            if conn_list:\n                return conn_list.pop()\n        for node in self.nodes.random_startup_node_iter():\n            connection = self.get_connection_by_node(node)\n\n            if connection:\n                return connection\n\n        raise Exception(\"Cant reach a single startup node.\")"}
{"prompt": "def apply(self, func, workers=1, job_size=10000):\n    \"\"\"Apply `func` to lines of text in parallel or sequential.\n\n    Args:\n      func : a function that takes a list of lines.\n    \"\"\"", "output": "\n    if workers == 1:\n      self.run_parallel(func)\n    else:\n      self.run_sequential(func, workers, job_size)\n\n  def run_parallel(self, func):\n    \"\"\"Run `func` in parallel.\n\n    Args:\n      func : a function that takes a list of lines.\n    \"\"\"\n    if self.is_parallel:\n      self.pool.map(func, self.lines)\n    else:\n      self.pool.map(func, self.lines)\n\n  def run_parallel_raw(self, func):\n    \"\"\"Run `func` in parallel.\n\n   ", "solution": "\n    if workers == 1:\n      for lines in self.iter_chunks(job_size):\n        yield func(lines)\n    else:\n      with ProcessPoolExecutor(max_workers=workers) as executor:\n        for result in executor.map(func, self.iter_chunks(job_size)):\n          yield result"}
{"prompt": "def copy_spline_array(a):\n    \"\"\"\n    This returns an instance of a new spline_array with all the fixins, and the data from a.\n    \"\"\"", "output": "\n    return SplineArray(a.copy(), a.copy())\n\n\ndef copy_spline_points(a):\n    \"\"\"\n    This returns an instance of a new spline_points with all the fixins, and the data from a.\n    \"\"\"\n    return SplinePoints(a.copy(), a.copy())\n\n\ndef copy_spline_lines(a):\n    \"\"\"\n    This returns an instance of a new spline_lines with all the fixins, and the data from a.\n    \"\"\"\n    return SplineLines(a.copy(), a.copy())\n\n\ndef copy_spline_curves(a", "solution": "\n\n    b = spline_array()\n\n    b.x_splines = a.x_splines\n    b.y_splines = a.y_splines\n    b.max_y_splines = a.max_y_splines\n    b.xmin = a.xmin     \n    b.xmax = a.xmax\n    b.ymin = a.ymin\n    b.ymax = a.ymax\n    b.xlabel = a.xlabel\n    b.ylabel = a.ylabel\n    b.zlabel = a.zlabel\n    b.simple = a.simple\n    \n\n    b.generate_y_values()    \n\n    return b"}
{"prompt": "def validate():\n    \"\"\"Display error messages and exit if no lore environment can be found.\n    \"\"\"", "output": "\n    if not os.path.exists(os.environ.get('LOOSE_ENVIRONMENT_PATH')):\n        print 'No lore environment found.  Run loose-env to generate a new one.'\n        sys.exit(1)\n    if not os.path.exists(os.environ.get('LOOSE_ENVIRONMENT_PATH_DEBUG')):\n        print 'Debug mode.  Run loose-env to generate a new one.'\n        sys.exit(1)\n    if not os.path.exists(os.environ.get('LOOSE_ENVIRON", "solution": "\n    if not os.path.exists(os.path.join(ROOT, APP, '__init__.py')):\n        message = ansi.error() + ' Python module not found.'\n        if os.environ.get('LORE_APP') is None:\n            message += ' $LORE_APP is not set. Should it be different than \"%s\"?' % APP\n        else:\n            message += ' $LORE_APP is set to \"%s\". Should it be different?' % APP\n        sys.exit(message)\n\n    if exists():\n        return\n\n    if len(sys.argv) > 1:\n        command = sys.argv[1]\n    else:\n        command = 'lore'\n    sys.exit(\n        ansi.error() + ' %s is only available in lore '\n                       'app directories (missing %s)' % (\n            ansi.bold(command),\n            ansi.underline(VERSION_PATH)\n        )\n    )"}
{"prompt": "def get_current_task_info(self):\n        \"\"\"Return dictionary of current task function info for this object.\"\"\"", "output": "\n        return self._task_info\n\n    def get_task_info(self):\n        \"\"\"Return dictionary of current task function info for this object.\"\"\"\n        return self._task_info\n\n    def get_task_id(self):\n        \"\"\"Return current task id for this object.\"\"\"\n        return self._task_id\n\n    def get_task_name(self):\n        \"\"\"Return current task name for this object.\"\"\"\n        return self._task_info.name\n\n    def get_task_id_str(self):\n        \"\"\"Return current task id as a string.\"\"\"\n        return self.get_task_id()\n\n    def get", "solution": "\n        name = self.model.workflow.name\n        if not name:\n            return\n\n        current_task = workflows[name].workflow\n        for step in self.callback_pos:\n            current_task = current_task[step]\n            if callable(current_task):\n                return get_func_info(current_task)"}
{"prompt": "def commit(self):\n        \"\"\"\n        Insert the text at the current cursor position.\n        \"\"\"", "output": "\n        self.textCursor().insertText(self.document().lastBlock().text())\n        self.textCursor().setPosition(self.textCursor().position() + 1, QTextCursor.KeepAnchor)\n\n    def findBlock(self, text):\n        \"\"\"\n        Find the block containing the given text.\n        \"\"\"\n        return self.document().findBlock(text)\n\n    def findText(self, text):\n        \"\"\"\n        Find the text at the current cursor position.\n        \"\"\"\n        return self.textCursor().position() + 1, QTextCursor.KeepAnchor, text\n\n    def findBlockAt(self,", "solution": "\n\n        tc = self.qteWidget.textCursor()\n\n        # If this is the first ever call to this undo/redo element then\n        # backup the current cursor position and the selected text (may be\n        # none). This information will be required for the redo operation\n        # to position the cursor (and selected text) where it was at the\n        # very first call.\n        if self.cursorPos0 is None:\n            self.cursorPos0 = tc.position()\n            self.selText = tc.selection().toHtml()\n            self.selStart = tc.selectionStart()\n            self.selEnd = tc.selectionEnd()\n        else:\n            tc.setPosition(self.cursorPos0, QtGui.QTextCursor.MoveAnchor)\n\n        # Remove the originally selected text (may be none).\n        tc.setPosition(self.selStart, QtGui.QTextCursor.MoveAnchor)\n        tc.setPosition(self.selEnd, QtGui.QTextCursor.KeepAnchor)\n        tc.removeSelectedText()\n\n        # Move to the start of the (just deleted) text block and insert\n        # the characters there.\n        if len(self.selText) > 0:\n            pos = self.selStart\n        else:\n            pos = tc.position()\n        tc.setPosition(pos)\n\n        # Backup the cursor position before the insertion operation,\n        # insert the new character(s), move the cursor forward, and\n        # backup the new cursor position as well.\n        self.cursorPos1 = tc.position()\n        tc.insertText(self.text)\n        self.cursorPos2 = tc.position()\n\n        self.qteWidget.setTextCursor(tc)"}
{"prompt": "def camelToSnake(s):\n    \"\"\"\n    https://gist.github.com/jaytaylor/3660565\n    Is it ironic that this function is written in camel case, yet it\n    converts to snake case? hmm..\n    \"\"\"", "output": "\n    return re.sub(r'([A-Z])', lambda m: m.group(1).upper(), s)\n\n\ndef snakeToSpace(s):\n    \"\"\"\n    https://gist.github.com/jaytaylor/3660565\n    Is it a space that is not a multiple of 4?\n    \"\"\"\n    return re.sub(r'([A-Z])', lambda m: m.group(1).upper(), s)\n\n\ndef spaceToSke(space):\n    \"\"\"\n    https://gist.github.com/jaytaylor/3660565\n   ", "solution": "\n    _underscorer1 = re.compile(r'(.)([A-Z][a-z]+)')\n    _underscorer2 = re.compile('([a-z0-9])([A-Z])')\n\n    subbed = _underscorer1.sub(r'\\1_\\2', s)\n    return _underscorer2.sub(r'\\1_\\2', subbed).lower()"}
{"prompt": "def yank_fields_from_attrs(attrs, _as=None, sort=True):\n    \"\"\"\n    Extract all the fields in given attributes (dict)\n    and return them ordered\n    \"\"\"", "output": "\n    if _as is not None:\n        return _as(attrs, sort=sort)\n    else:\n        return sorted(attrs.items())\n\n\ndef yank_fields_to_str(yank_fields):\n    \"\"\"\n    Convert a dictionary of attributes to a string\n    \"\"\"\n    return json.dumps(yank_fields, indent=4, sort_keys=True)\n\n\ndef yank_fields_to_int(yank_fields):\n    \"\"\"\n    Convert a dictionary of attributes to an int\n    \"\"\"\n    return int(yank_fields.items())\n\n\ndef yank_", "solution": "\n    fields_with_names = []\n    for attname, value in list(attrs.items()):\n        field = get_field_as(value, _as)\n        if not field:\n            continue\n        fields_with_names.append((attname, field))\n\n    if sort:\n        fields_with_names = sorted(fields_with_names, key=lambda f: f[1])\n    return OrderedDict(fields_with_names)"}
{"prompt": "def resolve_aliases(data_type):\n    \"\"\"\n    Resolve all chained / nested aliases. This will recursively point\n    nested aliases to their resolved data type (first non-alias in the chain).\n\n    Note: This differs from unwrap_alias which simply identifies/returns\n    the resolved data type.\n\n    Args:\n        data_type (DataType): The target DataType/Alias to resolve.\n    Return:\n        DataType: The resolved type.\n    \"\"\"", "output": "\n    if data_type.is_primitive():\n        return DataType.Primitive\n    elif data_type.is_primitive_array():\n        return DataType.PrimitiveArray\n    elif data_type.is_primitive_map():\n        return DataType.PrimitiveMap\n    elif data_type.is_primitive_map_array():\n        return DataType.PrimitiveMapArray\n    elif data_type.is_primitive_map_map():\n        return DataType.PrimitiveMapMap\n    elif data_type.is_primitive_map_map():\n        return DataType.PrimitiveMapMap\n    elif data_type.is_", "solution": "\n    if not is_alias(data_type):\n        return data_type\n\n    resolved = resolve_aliases(data_type.data_type)\n    data_type.data_type = resolved\n\n    return resolved"}
{"prompt": "def epomeo_gpx(max_iters=200, optimize=True, plot=True):\n    \"\"\"\n    Perform Gaussian process regression on the latitude and longitude data\n    from the Mount Epomeo runs. Requires gpxpy to be installed on your system\n    to load in the data.\n    \"\"\"", "output": "\n    import gpxpy\n    from gpxpy.gpx import GPX\n    from gpxpy.gpx.GPX import GPX_File\n    from gpxpy.gpx.GPX import GPX_File_Type\n    from gpxpy.gpx.GPX import GPX_File_Type_Options\n    from gpxpy.gpx.GPX import GPX_File_Type_Options_No_File\n    from gpxpy.gpx.GPX import GPX_File_Type_Options_No_File_Type\n    from gpxpy.gpx", "solution": "\n    try:import pods\n    except ImportError:\n        print('pods unavailable, see https://github.com/sods/ods for example datasets')\n        return\n    data = pods.datasets.epomeo_gpx()\n    num_data_list = []\n    for Xpart in data['X']:\n        num_data_list.append(Xpart.shape[0])\n\n    num_data_array = np.array(num_data_list)\n    num_data = num_data_array.sum()\n    Y = np.zeros((num_data, 2))\n    t = np.zeros((num_data, 2))\n    start = 0\n    for Xpart, index in zip(data['X'], range(len(data['X']))):\n        end = start+Xpart.shape[0]\n        t[start:end, :] = np.hstack((Xpart[:, 0:1],\n                                    index*np.ones((Xpart.shape[0], 1))))\n        Y[start:end, :] = Xpart[:, 1:3]\n\n    num_inducing = 200\n    Z = np.hstack((np.linspace(t[:,0].min(), t[:, 0].max(), num_inducing)[:, None],\n                   np.random.randint(0, 4, num_inducing)[:, None]))\n\n    k1 = GPy.kern.RBF(1)\n    k2 = GPy.kern.Coregionalize(output_dim=5, rank=5)\n    k = k1**k2\n\n    m = GPy.models.SparseGPRegression(t, Y, kernel=k, Z=Z, normalize_Y=True)\n    m.constrain_fixed('.*variance', 1.)\n    m.inducing_inputs.constrain_fixed()\n    m.Gaussian_noise.variance.constrain_bounded(1e-3, 1e-1)\n    m.optimize(max_iters=max_iters,messages=True)\n\n    return m"}
{"prompt": "def style(self, value):\n        \"\"\"gets/sets the style\"\"\"", "output": "\n        return self._style\n\n    @style.setter\n    def style(self, value):\n        \"\"\"gets/sets the style\"\"\"\n        self._style = value\n\n    @property\n    def style_type(self):\n        \"\"\"gets/sets the style type\"\"\"\n        return self._style_type\n\n    @style_type.setter\n    def style_type(self, value):\n        \"\"\"gets/sets the style type\"\"\"\n        self._style_type = value\n\n    @property\n    def style_name(self):\n        \"\"\"gets/sets the style name\"\"\"\n        return self._style_name\n\n    @style_name.setter\n   ", "solution": "\n        if self._style != value and \\\n           value in self._styles:\n            self._style = value"}
{"prompt": "def parameters_changed(self):\n        \"\"\"\n        Parameters have now changed\n        \"\"\"", "output": "\n        self.logger.debug(\"Parameters changed\")\n        self.parameters_changed = True\n\n    def get_parameters(self):\n        \"\"\"\n        Get the current parameters\n        \"\"\"\n        return self.parameters_list\n\n    def get_parameter_by_name(self, name):\n        \"\"\"\n        Get the parameter with the given name\n        \"\"\"\n        return self.parameters_list[name]\n\n    def get_parameter_by_index(self, index):\n        \"\"\"\n        Get the parameter with the given index\n        \"\"\"\n        return self.parameters_list[index]\n\n    def get_parameter_by_name_and_index(", "solution": "\n        # Get the model matrices from the kernel\n        (F,L,Qc,H,Pinf,dF,dQc,dPinf) = self.kern.sde()\n\n        # Use the Kalman filter to evaluate the likelihood\n        self._log_marginal_likelihood = self.kf_likelihood(F,L,Qc,H,self.sigma2,Pinf,self.X.T,self.Y.T)\n        gradients  = self.compute_gradients()\n        self.sigma2.gradient_full[:] = gradients[-1]\n        self.kern.gradient_full[:] = gradients[:-1]"}
{"prompt": "def _create_dd_event(self, events, image, c_tags, priority='Normal'):\n        \"\"\"Create the actual event to submit from a list of similar docker events\"\"\"", "output": "\n        return {\n            'type': 'docker',\n            'image': image,\n            'command': 'docker run -d -p {0} {1}'.format(\n                self.project.name, image),\n            'environment': self.environment,\n            'environment_vars': {'PULP_VERSION': self.project.version},\n            'labels': {'airflow-version': 'v' + airflow_version.replace('.', '-').replace('+', '-')},\n                     'airflow-version': airflow_version.replace('+', '-'),\n                     'airflow-branch': self.branch,\n                     'airflow-task-id':", "solution": "\n        if not events:\n            return\n\n        max_timestamp = 0\n        status = defaultdict(int)\n        status_change = []\n\n        for ev, c_name in events:\n            max_timestamp = max(max_timestamp, int(ev['time']))\n            status[ev['status']] += 1\n            status_change.append([c_name, ev['status']])\n\n        status_text = \", \".join([\"%d %s\" % (count, st) for st, count in status.iteritems()])\n        msg_title = \"%s %s on %s\" % (image, status_text, self.hostname)\n        msg_body = (\n            \"%%%\\n\"\n            \"{image_name} {status} on {hostname}\\n\"\n            \"```\\n{status_changes}\\n```\\n\"\n            \"%%%\"\n        ).format(\n            image_name=image,\n            status=status_text,\n            hostname=self.hostname,\n            status_changes=\"\\n\".join(\n                [\"%s \\t%s\" % (change[1].upper(), change[0]) for change in status_change])\n        )\n\n        if any(error in status_text for error in ERROR_ALERT_TYPE):\n            alert_type = \"error\"\n        else:\n            alert_type = None\n\n        return {\n            'timestamp': max_timestamp,\n            'host': self.hostname,\n            'event_type': EVENT_TYPE,\n            'msg_title': msg_title,\n            'msg_text': msg_body,\n            'source_type_name': EVENT_TYPE,\n            'event_object': 'docker:%s' % image,\n            'tags': list(c_tags),\n            'alert_type': alert_type,\n            'priority': priority\n        }"}
{"prompt": "def prefix(self, name):\n        \"\"\"\n        :param string name: the name of an attribute to look up.\n\n        :return: the prefix component of the named attribute's name,\n            or None.\n        \"\"\"", "output": "\n        return self.attribute_prefix_map.get(name, None)\n\n    def __getitem__(self, name):\n        \"\"\"\n        :param string name: the name of an attribute to look up.\n\n        :return: the attribute value or None.\n        \"\"\"\n        return self.attribute_value_map.get(name, None)\n\n    def __setitem__(self, name, value):\n        \"\"\"\n        :param string name: the name of an attribute to look up.\n\n        :param string value: the attribute value.\n        \"\"\"\n        self.attribute_value_map[name] = value\n\n    def __delitem__(", "solution": "\n        a_node = self.adapter.get_node_attribute_node(self.impl_element, name)\n        if a_node is None:\n            return None\n        return a_node.prefix"}
{"prompt": "def create_review(self, commit=github.GithubObject.NotSet, body=None, event=github.GithubObject.NotSet, comments=github.GithubObject.NotSet):\n        \"\"\"\n        :calls: `POST /repos/:owner/:repo/pulls/:number/reviews <https://developer.github.com/v3/pulls/reviews/>`_\n        :param commit: github.Commit.Commit\n        :param body: string\n        :param event: string\n        :param comments: list\n        :rtype: :class:`github.PullRequestReview.PullRequestReview`\n        \"\"\"", "output": "\n        assert commit is github.GithubObject.NotSet or isinstance(commit, github.Commit.Commit), commit\n        assert body is github.GithubObject.NotSet or isinstance(body, (str, unicode)), body\n        assert event is github.GithubObject.NotSet or isinstance(event, (str, unicode)), event\n        assert isinstance(comments, (list, tuple)), comments\n        post_parameters = {\n            \"commit\": commit._identity,\n            \"body\": body,\n            \"event\": event,\n        }\n        if body is not github.GithubObject.NotSet:\n            post_parameters[\"body\"] = body._identity\n        if event is not", "solution": "\n        assert commit is github.GithubObject.NotSet or isinstance(commit, github.Commit.Commit), commit\n        assert isinstance(body, str), body\n        assert event is github.GithubObject.NotSet or isinstance(event, str), event\n        assert comments is github.GithubObject.NotSet or isinstance(comments, list), comments\n        post_parameters = dict()\n        if commit is not github.GithubObject.NotSet:\n            post_parameters['commit_id'] = commit.sha\n        post_parameters['body'] = body\n        post_parameters['event'] = 'COMMENT' if event == github.GithubObject.NotSet else event\n        if comments is github.GithubObject.NotSet:\n            post_parameters['comments'] = []\n        else:\n            post_parameters['comments'] = comments\n        headers, data = self._requester.requestJsonAndCheck(\n            \"POST\",\n            self.url + \"/reviews\",\n            input=post_parameters\n        )\n        self._useAttributes(data)\n        return github.PullRequestReview.PullRequestReview(self._requester, headers, data, completed=True)"}
{"prompt": "def draw(self):\r\n        \"\"\"Draws the image at the given location.\"\"\"", "output": "\r\n        if self.image:\r\n            self.image.draw(self.canvas, self.x, self.y, self.width, self.height)\r\n\r\n    def __str__(self):\r\n        \"\"\"Returns a string representation of this image.\"\"\"\r\n        return 'Image: %s' % self.image.filename\r\n\r\n    def __repr__(self):\r\n        \"\"\"Returns a string representation of this image.\"\"\"\r\n        return 'Image(%s)' % self.image.filename\r\n\r\nclass _DrawingMixin(object):\r\n    \"\"\"This mixin provides common functionality for all drawing commands.\"\"\"\r\n    def draw(self):\r\n        \"\"\"Draws the image", "solution": "\r\n        if not self.visible:\r\n            return\r\n\r\n        self.window.blit(self.image, self.loc)"}
{"prompt": "def advance(self, length):\n        \"\"\"Advance the cursor in data buffer 'length' bytes.\"\"\"", "output": "\n        self.data_buffer.extend(self.cursor)\n        self.cursor = b''\n        self.data_buffer_len = length\n\n    def read(self, length):\n        \"\"\"Read 'length' bytes from the data buffer and advance cursor.\"\"\"\n        if not length:\n            return b''\n        if length > len(self.data_buffer):\n            self.data_buffer_len = len(self.data_buffer)\n        else:\n            self.data_buffer_len = length\n        return self.data_buffer_len\n\n    def write(self, data):\n        \"\"\"Write 'data' bytes", "solution": "\n        new_position = self._position + length\n        if new_position < 0 or new_position > len(self._data):\n            raise Exception('Invalid advance amount (%s) for cursor.  '\n                            'Position=%s' % (length, new_position))\n        self._position = new_position"}
{"prompt": "def thumb(self, size=BIGTHUMB):\n        \"\"\"Get a thumbnail as string or None if the file isnt an image\n\n        size would be one of JFSFile.BIGTHUMB, .MEDIUMTHUMB, .SMALLTHUMB or .XLTHUMB\"\"\"", "output": "\n        if size == JFSFile.BIGTHUMB:\n            return self.get_file_icon(JFSFile.MEDIUMTHUMB)\n        elif size == JFSFile.XLTHUMB:\n            return self.get_file_icon(JFSFile.SMALLTHUMB)\n        else:\n            return self.get_file_icon(JFSFile.UNKNOWN)\n\n    def get_file_icon(self, filename):\n        \"\"\"Get the icon for a file\n\n        filename is the filename of the file to get the icon for\n        \"\"\"\n        if filename is None:\n            return None", "solution": "\n        if not self.is_image():\n            return None\n        if not size in (self.BIGTHUMB, self.MEDIUMTHUMB, self.SMALLTHUMB, self.XLTHUMB):\n            raise JFSError('Invalid thumbnail size: %s for image %s' % (size, self.path))\n        #return self.jfs.raw('%s?mode=thumb&ts=%s' % (self.path, size))\n        return self.jfs.raw(url=self.path,\n                            params={'mode':'thumb', 'ts':size})"}
{"prompt": "def sorted(self):\r\n        \"\"\"Utility function for sort_file_tabs_alphabetically().\"\"\"", "output": "\r\n        return self._sort_file_tabs_alphabetically\r\n\r\n    def _sort_file_tabs_alphabetically(self, files, tab_size=8):\r\n        \"\"\"Sort the files in a tabular way, so that the tab bar is always\r\n        the one with the largest values.\"\"\"\r\n        if len(files) == 0:\r\n            return\r\n        for i in range(len(files)):\r\n            if len(files[i]) > tab_size:\r\n                files[i] = sorted(files[i], key=lambda f: len(f[0]), reverse=True)\r\n\r\n    def _get_", "solution": "\r\n        for i in range(0, self.tabs.tabBar().count() - 1):\r\n            if (self.tabs.tabBar().tabText(i) >\r\n                    self.tabs.tabBar().tabText(i + 1)):\r\n                return False\r\n        return True"}
{"prompt": "def _make_request(self, url, headers, params=None):\n        \"\"\"\n        Generic request handler for OpenStack API requests\n        Raises specialized Exceptions for commonly encountered error codes\n        \"\"\"", "output": "\n        if params is None:\n            params = {}\n        if self.auth_version == 'v2.0':\n            self.log.debug('Request: %s' % url)\n        else:\n            self.log.debug('Request: %s' % url +'(auth_version: %s)' % self.auth_version)\n        if self.tenant_id:\n            params['tenant_id'] = self.tenant_id\n        if self.user_id:\n            params['user_id'] = self.user_id\n        if self.password:\n            params['password'] = self", "solution": "\n        self.logger.debug(\"Request URL, Headers and Params: %s, %s, %s\", url, headers, params)\n\n        # Checking if request is in cache\n        cache_key = \"|\".join([url, json.dumps(headers), json.dumps(params), str(self.timeout)])\n        if cache_key in self.cache:\n            self.logger.debug(\"Request found in cache. cache key %s\", cache_key)\n            return self.cache.get(cache_key)\n\n        try:\n            resp = requests.get(\n                url, headers=headers, verify=self.ssl_verify, params=params, timeout=self.timeout, proxies=self.proxies\n            )\n            resp.raise_for_status()\n        except requests.exceptions.HTTPError as e:\n            self.logger.debug(\"Error contacting openstack endpoint: %s\", e)\n            if resp.status_code == 401:\n                self.logger.info('Need to reauthenticate before next check')\n                raise AuthenticationNeeded()\n            elif resp.status_code == 409:\n                raise InstancePowerOffFailure()\n            else:\n                raise e\n        except Exception:\n            self.logger.exception(\"Unexpected error contacting openstack endpoint {}\".format(url))\n            raise\n        jresp = resp.json()\n        self.logger.debug(\"url: %s || response: %s\", url, jresp)\n\n        # Adding response to the cache\n        self.cache[cache_key] = jresp\n        return jresp"}
{"prompt": "def __skip_this(self, level):\n        \"\"\"\n        Check whether this comparison should be skipped because one of the objects to compare meets exclusion criteria.\n        :rtype: bool\n        \"\"\"", "output": "\n        if level == 0:\n            return True\n        elif level == 1:\n            return self.__skip_this(1)\n        else:\n            raise ValueError(\"Level must be > 0\")\n\n    def __eq__(self, other):\n        \"\"\"\n        Compare this comparison with another one.\n        :rtype: bool\n        \"\"\"\n        if self.__skip_this(0):\n            return True\n        elif other.__skip_this(0):\n            return False\n        else:\n            return self.__compare_with_other(0, other)\n\n    def __compare_with_other(self, other, other_type):\n        \"\"\"\n        Compare", "solution": "\n        skip = False\n        if self.exclude_paths and level.path() in self.exclude_paths:\n            skip = True\n        elif self.exclude_regex_paths and any(\n                [exclude_regex_path.search(level.path()) for exclude_regex_path in self.exclude_regex_paths]):\n            skip = True\n        else:\n            if self.exclude_types_tuple and (isinstance(level.t1, self.exclude_types_tuple) or\n                                             isinstance(level.t2, self.exclude_types_tuple)):\n                skip = True\n\n        return skip"}
{"prompt": "def _call_vagrant_command(self, args):\n        \"\"\"\n        Run a vagrant command.  Return None.\n        args: A sequence of arguments to a vagrant command line.\n\n        \"\"\"", "output": "\n        if not self.vagrant_command:\n            return None\n        if len(args) == 1:\n            return self.vagrant_command[0]\n        if len(args) == 2:\n            return self.vagrant_command[1]\n        if len(args) == 3:\n            return self.vagrant_command[2]\n        return self._call_vagrant_command(args)\n\n    def _call_vagrant_command(self, args):\n        \"\"\"\n        Run a vagrant command.  Return None.\n        args: A sequence of arguments to", "solution": "\n        # Make subprocess command\n        command = self._make_vagrant_command(args)\n        with self.out_cm() as out_fh, self.err_cm() as err_fh:\n            subprocess.check_call(command, cwd=self.root, stdout=out_fh,\n                                  stderr=err_fh, env=self.env)"}
{"prompt": "def _do_download(version, download_base, to_dir, download_delay):\n    \"\"\"Download Setuptools.\"\"\"", "output": "\n    if not os.path.exists(to_dir):\n        os.makedirs(to_dir)\n    url = \"https://github.com/pypa/setuptools\"\n    data = {\"version\": version, \"download_base\": download_base, \"to_dir\": to_dir,\n            \"download_delay\": str(download_delay)}\n    saveto = os.path.join(to_dir, \"setup.py\")\n    if not os.path.exists(saveto):\n        with open(saveto, \"w\", encoding=\"utf-8\") as f:\n            f", "solution": "\n    py_desig = 'py{sys.version_info[0]}.{sys.version_info[1]}'.format(sys=sys)\n    tp = 'setuptools-{version}-{py_desig}.egg'\n    egg = os.path.join(to_dir, tp.format(**locals()))\n    if not os.path.exists(egg):\n        archive = download_setuptools(version, download_base,\n            to_dir, download_delay)\n        _build_egg(egg, archive, to_dir)\n    sys.path.insert(0, egg)\n\n    # Remove previously-imported pkg_resources if present (see\n    # https://bitbucket.org/pypa/setuptools/pull-request/7/ for details).\n    if 'pkg_resources' in sys.modules:\n        _unload_pkg_resources()\n\n    import setuptools\n    setuptools.bootstrap_install_from = egg"}
{"prompt": "def _init_settings(self):\n        \"\"\" Init setting \"\"\"", "output": "\n        self.settings = QSettings()\n        self.settings.beginGroup('qgis')\n        self.settings.setValue('inasafe/analysis_extents_mode', 'HazardExposure')\n        self.settings.setValue('inasafe/use_native_zonal_stats', True)\n        self.settings.setValue('inasafe/analysis_extents_extent', extents_mode)\n        self.settings.endGroup()\n        self.settings.beginGroup('gdal')\n        self.gdal_version = gdal.VersionInfo('VERSION_NUM')\n        self.gdal_full_", "solution": "\n        self._show_whitespaces = False\n        self._tab_length = 4\n        self._use_spaces_instead_of_tabs = True\n        self.setTabStopWidth(self._tab_length *\n                             self.fontMetrics().width(\" \"))\n        self._set_whitespaces_flags(self._show_whitespaces)"}
{"prompt": "def endLoop(self, useDriverLoop):\n        \"\"\"\n        Called by the engine to stop an event loop.\n        \"\"\"", "output": "\n        self.running = False\n        self.loop = None\n        self.loopContext = None\n        self.loopContextStack = []\n        self.loopContext = None\n        self.loopContextStack.append(self)\n        self.loop = None\n        self.lastEvent = None\n        self.lastEventStack = []\n        self.lastEvent = None\n        self.loopContext = None\n        self.loopContextStack.pop()\n\n    def get_loop_index(self):\n        \"\"\"\n        Returns the index of the event loop.\n        \"\"\"\n        return self.loopContextStack[-1]\n\n    def get_loop_", "solution": "\n        self._queue = []\n        self._driver.stop()\n        if useDriverLoop:\n            self._driver.endLoop()\n        else:\n            self._iterator = None\n        self.setBusy(True)"}
{"prompt": "def precision(self):\n        \"\"\"Calculates precision\n\n        :return: Precision of matrix\n        \"\"\"", "output": "\n        return self.get_value(self.get_key(self.get_value(self.get_key(self.get_value(self.get_key(self.get_value(self.get_key(self.get_value(self.get_key(self.get_value(self.get_key(self.get_value(self.get_key(self.get_value(self.get_key(self.get_value(self.get_key(self.get_value(self.get_key(self.get_value(", "solution": "\n        true_pos = self.matrix[0][0]\n        false_pos = self.matrix[1][0]\n        return divide(1.0 * true_pos, true_pos + false_pos)"}
{"prompt": "def percentile(self, percentile):\n        \"\"\" Returns the nth percentile of the histogram. \"\"\"", "output": "\n        return self.get_percentile(percentile)\n\n    def get_percentile(self, percentile):\n        \"\"\" Returns the percentile of the histogram. \"\"\"\n        return self.get_percentile_impl(percentile)\n\n    def get_percentile_impl(self, percentile):\n        \"\"\" Returns the percentile of the histogram. \"\"\"\n        return self.get_percentile_impl_impl(percentile)\n\n    def get_percentile_impl_impl(self, percentile):\n        \"\"\" Returns the percentile of the histogram's impl. \"\"\"\n        return self.get_percentile_impl_impl(percentile)\n\n    def get_percentile_impl(self,", "solution": "\n        assert(percentile >= 0 and percentile <= 100)\n        assert(self.kind in [\"exponential\", \"linear\", \"enumerated\", \"boolean\"])\n\n        fraction = percentile / 100\n        to_count = fraction * self.buckets.sum()\n        percentile_bucket = 0\n\n        for percentile_bucket in range(len(self.buckets)):\n            freq = self.buckets.values[percentile_bucket]\n            if to_count - freq <= 0:\n                break\n            to_count -= freq\n\n        percentile_lower_boundary = self.buckets.index[percentile_bucket]\n        percentile_frequency = self.buckets.values[percentile_bucket]\n\n        if percentile_bucket == len(self.buckets) - 1 or percentile_frequency == 0:\n            return percentile_lower_boundary\n\n        width = self.buckets.index[percentile_bucket + 1] - self.buckets.index[percentile_bucket]\n        return percentile_lower_boundary + width * to_count / percentile_frequency"}
{"prompt": "def compute_diff(dir_base, dir_cmp):\n    \"\"\" Compare `dir_base' and `dir_cmp' and returns a list with\n    the following keys:\n     - deleted files `deleted'\n     - created files `created'\n     - updated files `updated'\n     - deleted directories `deleted_dirs'\n\n    \"\"\"", "output": "\n    return [\n        os.path.join(dir_base, f)\n        for f in os.listdir(dir_base)\n        if os.path.isfile(os.path.join(dir_base, f)) and not dir_cmp.has_key(f)\n    ]\n\n\ndef get_diff(dir_base, dir_cmp):\n    \"\"\" Return the list of files that are changed in `dir_cmp'\n\n    \"\"\"\n    return [\n        os.path.join(dir_base, f)\n        for f in os.listdir(dir_base)\n        if not dir_", "solution": "\n    data = {}\n    data['deleted'] = list(set(dir_cmp['files']) - set(dir_base['files']))\n    data['created'] = list(set(dir_base['files']) - set(dir_cmp['files']))\n    data['updated'] = []\n    data['deleted_dirs'] = list(set(dir_cmp['subdirs']) - set(dir_base['subdirs']))\n\n    for f in set(dir_cmp['files']).intersection(set(dir_base['files'])):\n        if dir_base['index'][f] != dir_cmp['index'][f]:\n            data['updated'].append(f)\n\n    return data"}
{"prompt": "def _get_reconciled_name_object(self, other):\n        \"\"\"\n        If the result of a set operation will be self,\n        return self, unless the name changes, in which\n        case make a shallow copy of self.\n        \"\"\"", "output": "\n        if self.op!= other.op:\n            return self.copy()\n\n    def is_valid(self):\n        \"\"\"\n        Check if the current state of the symbol is valid.\n        \"\"\"\n        return self.op in self.valid_operations\n\n    def is_final(self):\n        \"\"\"\n        Check if the current state of the symbol is final.\n        \"\"\"\n        return self.op in self.final_operations\n\n    def is_defined(self):\n        \"\"\"\n        Check if the current state of the symbol is defined.\n        \"\"\"\n        return self.valid_definitions\n\n    def is_final_definition(self, definition", "solution": "\n        name = get_op_result_name(self, other)\n        if self.name != name:\n            return self._shallow_copy(name=name)\n        return self"}
{"prompt": "def plotplanarPotentials(Pot,*args,**kwargs):\n    \"\"\"\n    NAME:\n\n       plotplanarPotentials\n\n    PURPOSE:\n\n       plot a planar potential\n\n    INPUT:\n\n       Rrange - range (can be Quantity)\n\n       xrange, yrange - if relevant (can be Quantity)\n\n       grid, gridx, gridy - number of points to plot\n\n       savefilename - save to or restore from this savefile (pickle)\n\n       ncontours - number of contours to plot (if applicable)\n\n       +bovy_plot(*args,**kwargs) or bovy_dens2d(**kwargs)\n\n    OUTPUT:\n\n       plot to output device\n\n    HISTORY:\n\n       2010-07-13 - Written - Bovy (NYU)\n\n    \"\"\"", "output": "(*args,**kwargs)\n\n    OUTPUT:\n\n       plot a planar potential (if applicable)\n\n    HISTORY:\n\n       2010-07-12 - Written - Bovy (NYU)\n\n    \"\"\"\n    if isinstance(Rrange,Quantity):\n        Rrange = Rrange.to(units.Rsun)\n\n    if isinstance(yrange,units.Quantity):\n        yrange = yrange.to(units.Rsun)\n\n    if isinstance(Pot,units.Quantity):\n        Pot = Pot.to(units.Quantity)\n\n    if isinstance(savefilename,str):\n        if not os.path.", "solution": "\n    Pot= flatten(Pot)\n    Rrange= kwargs.pop('Rrange',[0.01,5.])\n    xrange= kwargs.pop('xrange',[-5.,5.])\n    yrange= kwargs.pop('yrange',[-5.,5.])\n    if _APY_LOADED:\n        if hasattr(Pot,'_ro'):\n            tro= Pot._ro\n        else:\n            tro= Pot[0]._ro\n        if isinstance(Rrange[0],units.Quantity):\n            Rrange[0]= Rrange[0].to(units.kpc).value/tro\n        if isinstance(Rrange[1],units.Quantity):\n            Rrange[1]= Rrange[1].to(units.kpc).value/tro\n        if isinstance(xrange[0],units.Quantity):\n            xrange[0]= xrange[0].to(units.kpc).value/tro\n        if isinstance(xrange[1],units.Quantity):\n            xrange[1]= xrange[1].to(units.kpc).value/tro\n        if isinstance(yrange[0],units.Quantity):\n            yrange[0]= yrange[0].to(units.kpc).value/tro\n        if isinstance(yrange[1],units.Quantity):\n            yrange[1]= yrange[1].to(units.kpc).value/tro\n    grid= kwargs.pop('grid',100)\n    gridx= kwargs.pop('gridx',100)\n    gridy= kwargs.pop('gridy',gridx)\n    savefilename= kwargs.pop('savefilename',None)\n    isList= isinstance(Pot,list)\n    nonAxi= ((isList and Pot[0].isNonAxi) or (not isList and Pot.isNonAxi))\n    if not savefilename is None and os.path.exists(savefilename):\n        print(\"Restoring savefile \"+savefilename+\" ...\")\n        savefile= open(savefilename,'rb')\n        potR= pickle.load(savefile)\n        if nonAxi:\n            xs= pickle.load(savefile)\n            ys= pickle.load(savefile)\n        else:\n            Rs= pickle.load(savefile)\n        savefile.close()\n    else:\n        if nonAxi:\n            xs= nu.linspace(xrange[0],xrange[1],gridx)\n            ys= nu.linspace(yrange[0],yrange[1],gridy)\n            potR= nu.zeros((gridx,gridy))\n            for ii in range(gridx):\n                for jj in range(gridy):\n                    thisR= nu.sqrt(xs[ii]**2.+ys[jj]**2.)\n                    if xs[ii] >= 0.:\n                        thisphi= nu.arcsin(ys[jj]/thisR)\n                    else:\n                        thisphi= -nu.arcsin(ys[jj]/thisR)+nu.pi\n                    potR[ii,jj]= evaluateplanarPotentials(Pot,thisR,\n                                                          phi=thisphi,\n                                                          use_physical=False)\n        else:\n            Rs= nu.linspace(Rrange[0],Rrange[1],grid)\n            potR= nu.zeros(grid)\n            for ii in range(grid):\n                potR[ii]= evaluateplanarPotentials(Pot,Rs[ii],\n                                                   use_physical=False)\n        if not savefilename is None:\n            print(\"Writing planar savefile \"+savefilename+\" ...\")\n            savefile= open(savefilename,'wb')\n            pickle.dump(potR,savefile)\n            if nonAxi:\n                pickle.dump(xs,savefile)\n                pickle.dump(ys,savefile)\n            else:\n                pickle.dump(Rs,savefile)\n            savefile.close()\n    if nonAxi:\n        if not 'orogin' in kwargs:\n            kwargs['origin']= 'lower'\n        if not 'cmap' in kwargs:\n            kwargs['cmap']= 'gist_yarg'\n        if not 'contours' in kwargs:\n            kwargs['contours']= True\n        if not 'xlabel' in kwargs:\n            kwargs['xlabel']= r\"$x / R_0$\"\n        if not 'ylabel' in kwargs:\n            kwargs['ylabel']= \"$y / R_0$\"\n        if not 'aspect' in kwargs:\n            kwargs['aspect']= 1.\n        if not 'cntrls' in kwargs:\n            kwargs['cntrls']= '-'\n        ncontours= kwargs.pop('ncontours',10)\n        if not 'levels' in kwargs:\n            kwargs['levels']= nu.linspace(nu.nanmin(potR),nu.nanmax(potR),ncontours)\n        return plot.bovy_dens2d(potR.T,\n                                xrange=xrange,\n                                yrange=yrange,**kwargs)\n    else:\n        kwargs['xlabel']=r\"$R/R_0$\"\n        kwargs['ylabel']=r\"$\\Phi(R)$\"\n        kwargs['xrange']=Rrange\n        return plot.bovy_plot(Rs,potR,*args,**kwargs)"}
{"prompt": "def get_delta_TR(tail_check_max, y_int):\n    \"\"\"\n    input: tail_check_max, y_intercept\n    output: delta_TR\n    \"\"\"", "output": "\n    return np.array([tail_check_max - y_int, tail_check_max + y_int])\n\n\ndef get_delta_T(tail_check_min, y_int):\n    \"\"\"\n    input: tail_check_min, y_int\n    output: delta_T\n    \"\"\"\n    return np.array([tail_check_min - y_int, tail_check_min + y_int])\n\n\ndef get_delta_T_error(y_int, tail_check_max, delta_max):\n    \"\"\"\n    input: y_int, delta", "solution": "\n    if tail_check_max == 0 or numpy.isnan(tail_check_max):\n        return float('nan')\n    delta_TR = (old_div(tail_check_max, abs(y_int))) * 100.\n    return delta_TR"}
{"prompt": "def _calc(count, \n              last_count, \n              start_time, \n              max_count, \n              speed_calc_cycles, \n              q, \n              last_speed,\n              lock):\n        \"\"\"do the pre calculations in order to get TET, speed, TTG\n        \n        :param count:               count \n        :param last_count:          count at the last call, allows to treat the case of no progress\n            between sequential calls\n        :param start_time:          the time when start was triggered\n        :param max_count:           the maximal value count \n        :type max_count:\n        :param speed_calc_cycles:\n        :type speed_calc_cycles:\n        :param q:\n        :type q:\n        :param last_speed:\n        :type last_speed:\n        :param lock:\n        :type lock:\n        \"\"\"", "output": ":             int\n        :type q:                      str\n        :type q:                      str\n        :type  last_q:                      str\n        :type  start_time:                  int\n        :type  max_count:                  int\n        :type  speed_calc_cycles:            int\n        :type  q:                      str\n        :type  last_speed:                  int\n        :type  start_time:                  int\n        :type  max_count:                  int\n        :type  speed_calc_cycles:            int\n        :type  q:                      str\n        :type  last_speed:                  int\n       ", "solution": "\n        count_value = count.value\n        start_time_value = start_time.value\n        current_time = time.time()\n        \n        if last_count.value != count_value:\n            # some progress happened\n        \n            with lock:\n                # save current state (count, time) to queue\n                \n                q.put((count_value, current_time))\n    \n                # get older state from queue (or initial state)\n                # to to speed estimation                \n                if q.qsize() > speed_calc_cycles:\n                    old_count_value, old_time = q.get()\n                else:\n                    old_count_value, old_time = 0, start_time_value\n            \n            last_count.value = count_value\n            #last_old_count.value = old_count_value\n            #last_old_time.value = old_time\n            \n            speed = (count_value - old_count_value) / (current_time - old_time)\n            last_speed.value = speed \n        else:\n            # progress has not changed since last call\n            # use also old (cached) data from the queue\n            #old_count_value, old_time = last_old_count.value, last_old_time.value\n            speed = last_speed.value  \n\n        if (max_count is None):\n            max_count_value = None\n        else:\n            max_count_value = max_count.value\n            \n        tet = (current_time - start_time_value)\n        \n        if (speed == 0) or (max_count_value is None) or (max_count_value == 0):\n            ttg = None\n        else:\n            ttg = math.ceil((max_count_value - count_value) / speed)\n            \n        return count_value, max_count_value, speed, tet, ttg"}
{"prompt": "def symbol_for_ticker(self, ticker, field, provider, providers):\r\n        \"\"\"Return an instance of *symboldata* containing\r\ninformation about the data provider, the data provider ticker name\r\nand the data provider field.\"\"\"", "output": "\r\n        return symboldata.Symbol(ticker, field, self.context)\r\n\r\n    def get_ticker_name(self, ticker):\r\n        \"\"\"Return the name of the data provider field for *ticker*.\"\"\"\r\n        return self.get_field_name(ticker, 'name')\r\n\r\n    def get_field_name(self, ticker, field):\r\n        \"\"\"Return the instance of *fielddata* containing\r\ninformation about the data provider, the field name and the instance\r\nof *symboldata* containing information about the data provider,\r\nthe data provider ticker name and the instance of *symboldata*\r\ncontaining", "solution": "\r\n        provider = provider or settings.default_provider\r\n        if provider:\r\n            provider = providers.get(provider, None)\r\n        return self.symboldata(ticker, field, provider)"}
{"prompt": "def copy_to(self, dest, buffering: int = -1):\n        \"\"\"\n        copy the file to dest path.\n\n        `dest` canbe `str`, `FileInfo` or `DirectoryInfo`.\n\n        if `dest` is `DirectoryInfo`, that mean copy into the dir with same name.\n        \"\"\"", "output": "\n        if not isinstance(dest, str):\n            raise TypeError(\"dest must be a str\")\n        if not isinstance(buffer, int):\n            raise TypeError(\"buffer must be an int\")\n        if buffering < 0:\n            raise ValueError(\"buffer must be non-negative\")\n        if dest.is_dir():\n            dest = dest.parent\n        if not dest.is_file():\n            raise TypeError(\"dest must be a FileInfo or DirectoryInfo\")\n        if not dest.is_directory():\n            raise TypeError(\"dest must be a directory\")\n        if not dest.is_file():\n            raise TypeError(\"dest must be a", "solution": "\n        if isinstance(dest, str):\n            dest_path = dest\n        elif isinstance(dest, FileInfo):\n            dest_path = dest.path\n        elif isinstance(dest, DirectoryInfo):\n            dest_path = dest.path / self.path.name\n        else:\n            raise TypeError('dest is not one of `str`, `FileInfo`, `DirectoryInfo`')\n\n        with open(self._path, 'rb', buffering=buffering) as source:\n            # use x mode to ensure dest does not exists.\n            with open(dest_path, 'xb') as dest_file:\n                for buffer in source:\n                    dest_file.write(buffer)"}
{"prompt": "def seperate_symbols(func):\n    \"\"\"\n    Seperate the symbols in symbolic function func. Return them in alphabetical\n    order.\n\n    :param func: scipy symbolic function.\n    :return: (vars, params), a tuple of all variables and parameters, each \n        sorted in alphabetical order.\n    :raises TypeError: only symfit Variable and Parameter are allowed, not sympy\n        Symbols.\n    \"\"\"", "output": "\n    if isinstance(func, sympy.Symbol):\n        return func.variables, func.params\n    elif isinstance(func, sympy.Symbol):\n        return tuple(seperate_symbols(seperate_symbol) for seperate_symbol in seperate_symbols(func))\n    elif isinstance(func, sympy.Symbol):\n        return seperate_symbols(seperate_symbol)\n    else:\n        raise TypeError(\"Invalid function: %s\" % str(func))\n\ndef seperate_symbols_list(seperate_symbols_list):\n    \"\"\"\n    Seperate the", "solution": "\n    params = []\n    vars = []\n    for symbol in func.free_symbols:\n        if not isidentifier(str(symbol)):\n            continue  # E.g. Indexed objects might print to A[i, j]\n        if isinstance(symbol, Parameter):\n            params.append(symbol)\n        elif isinstance(symbol, Idx):\n            # Idx objects are not seen as parameters or vars.\n            pass\n        elif isinstance(symbol, (MatrixExpr, Expr)):\n            vars.append(symbol)\n        else:\n            raise TypeError('model contains an unknown symbol type, {}'.format(type(symbol)))\n\n    for der in func.atoms(sympy.Derivative):\n        # Used by jacobians and hessians, where derivatives are treated as\n        # Variables. This way of writing it is purposefully discriminatory\n        # against derivatives wrt variables, since such derivatives should be\n        # performed explicitly in the case of jacs/hess, and are treated\n        # differently in the case of ODEModels.\n        if der.expr in vars and all(isinstance(s, Parameter) for s in der.variables):\n            vars.append(der)\n\n    params.sort(key=lambda symbol: symbol.name)\n    vars.sort(key=lambda symbol: symbol.name)\n    return vars, params"}
{"prompt": "def search(d, recursive=True, store_meta=True):\n    \"\"\"\n    Search for DICOM files within a given directory and receive back a \n    dictionary of {StudyInstanceUID: {SeriesNumber: [files]}}\n    \n    Example usage::\n        >>> import yaxil.dicom\n        >>> yaxil.dicom.search(\"~/dicoms\").keys()\n        ['1.2.340.500067.8.9.10.11012.13000001401516017181900000200']\n        \n    :param d: Directory name\n    :type d: str\n    :param recursive: Search recursively\n    :type recursive: bool\n    :param store_meta: Read and store metadata for each file for fast lookups\n    :type store_meta: bool\n    :returns: Dictionary of {StudyInstanceUID: {SeriesNumber: [files]}}\n    :rtype: dict\n    \"\"\"", "output": " recursively for files\n    :type recursive: bool\n    :param store_meta: If True, the metadata will be stored in the dicom \n        directory.  Otherwise, the default database will be used.\n    :type store_meta: bool\n    :returns: A dictionary of {StudyInstanceUID: {SeriesNumber: [files]}}\n    \"\"\"\n    if not os.path.isdir(d):\n        raise ValueError(\"Directory %s does not exist\" % d)\n    return search_files(d, recursive, store_meta)\n\ndef search_files(d, recursive=True, store_meta=", "solution": "\n    # say this fast three times\n    scans = col.defaultdict(lambda: col.defaultdict(lambda: col.defaultdict(list)))\n    for dirpath,dirnames,filenames in os.walk(os.path.expanduser(d)):\n        for f in filenames:\n            fullfile = os.path.join(dirpath, f)\n            try:\n                d = pydicom.read_file(fullfile, stop_before_pixels=True)\n            except pydicom.filereader.InvalidDicomError:\n                continue\n            meta = {k: getattr(d, k, None) for k in d.dir()} if store_meta else None\n            scans[d.StudyInstanceUID][d.SeriesNumber][d.InstanceNumber].append(DicomFile(meta=meta, file=fullfile))\n        if not recursive:\n            del dirnames[:]\n    return scans"}
{"prompt": "def _wait_for_lock_to_disappear(handle, lock_file, lock_file_timeout_sec):\n  \"\"\"Waits for the lock file to disappear.\n\n  The lock file was created by another process that is performing a download\n  into its own temporary directory. The name of this temp directory is\n  sha1(<module>).<uuid>.tmp where <uuid> comes from the lock file.\n\n  Args:\n    handle: The location from where a module is being download.\n    lock_file: Lock file created by another process downloading this module.\n    lock_file_timeout_sec: The amount of time to wait (in seconds) before we\n                           can declare that the other downloaded has been\n                           abandoned. The download is declared abandoned if\n                           there is no file size change in the temporary\n                           directory within the last 'lock_file_timeout_sec'.\n  \"\"\"", "output": " timeout in seconds for the lock file.\n\n  Returns:\n    The path to the lock file.\n  \"\"\"\n  handle = handle.lower()\n  if not lock_file.is_locked():\n    raise LockError('Lock file %s is not locked' % lock_file.name)\n  if not lock_file_timeout_sec.is_locked():\n    raise LockTimeoutError('Lock timeout in seconds for %s' % lock_file.name)\n  if not os.path.exists(lock_file.lock_file):\n    raise LockError('Lock file %s does not exist' % lock_", "solution": "\n  locked_tmp_dir_size = 0\n  locked_tmp_dir_size_check_time = time.time()\n  lock_file_content = None\n  while tf_v1.gfile.Exists(lock_file):\n    try:\n      logging.log_every_n(\n          logging.INFO,\n          \"Module '%s' already being downloaded by '%s'. Waiting.\", 10,\n          handle, tf_utils.read_file_to_string(lock_file))\n      if (time.time() - locked_tmp_dir_size_check_time >\n          lock_file_timeout_sec):\n        # Check whether the holder of the current lock downloaded anything\n        # in its temporary directory in the last 'lock_file_timeout_sec'.\n        cur_locked_tmp_dir_size = _locked_tmp_dir_size(lock_file)\n        cur_lock_file_content = tf_utils.read_file_to_string(lock_file)\n        if (cur_locked_tmp_dir_size == locked_tmp_dir_size and\n            cur_lock_file_content == lock_file_content):\n          # There is was no data downloaded in the past\n          # 'lock_file_timeout_sec'. Steal the lock and proceed with the\n          # local download.\n          logging.warning(\"Deleting lock file %s due to inactivity.\",\n                          lock_file)\n          tf_v1.gfile.Remove(lock_file)\n          break\n        locked_tmp_dir_size = cur_locked_tmp_dir_size\n        locked_tmp_dir_size_check_time = time.time()\n        lock_file_content = cur_lock_file_content\n    except tf.errors.NotFoundError:\n      # Lock file or temp directory were deleted during check. Continue\n      # to check whether download succeeded or we need to start our own\n      # download.\n      pass\n    finally:\n      time.sleep(5)"}
{"prompt": "def contains_pts(self, pts):\n        \"\"\"Containment test on arrays.\"\"\"", "output": "\n        return self.test_point_in_polygon(pts, self.polygon)\n\n    def test_point_in_polygon(self, pt):\n        \"\"\"Test if a point is inside a polygon.\"\"\"\n        return self.contains_pts(self.polygon.exterior.coords)\n\n    def test_point_in_polygon_with_holes(self):\n        \"\"\"Test if a point is inside a polygon with holes.\"\"\"\n        return self.contains_pts(self.polygon.exterior.coords)\n\n    def test_point_in_polygon_with_points(self):\n        \"\"\"Test if a", "solution": "\n        obj1, obj2 = self.objects\n        arg1 = obj2.contains_pts(pts)\n        arg2 = np.logical_not(obj1.contains_pts(pts))\n        return np.logical_and(arg1, arg2)"}
{"prompt": "def is_holiday(now=None, holidays=\"/etc/acct/holidays\"):\n    \"\"\"is_holiday({now}, {holidays=\"/etc/acct/holidays\"}\"\"\"", "output": "\n    if now is None:\n        now = datetime.now()\n    if holidays is None:\n        return now.weekday() in [0, 1]\n    else:\n        return now.weekday() in [1, 2]\n\n\ndef is_holiday_date(now=None, holidays=\"/etc/acct/holidays\"):\n    \"\"\"is_holiday_date({now}, {holidays=\"/etc/acct/holidays\"})\"\"\"\n    if is_holiday(now=None, holidays=\"/etc/acct/holidays\"):\n        return False\n    else:\n        return True\n\n\ndef is_holiday_", "solution": "\n    now = _Time(now)\n    # Now, parse holiday file.\n    if not os.path.exists(holidays):\n        raise Exception(\"There is no holidays file: %s\" % holidays)\n\n    f = open(holidays, \"r\")\n    # First, read all leading comments.\n    line = f.readline()\n    while line[0] == '*': line = f.readline()\n\n    # We just got the year line.\n    (year, primestart, primeend) = str.split(line)\n    # If not the right year, we have no idea for certain.  Skip.\n    if not year == now.year: return 0\n\n    # Now the dates.  Check each against now.\n    while line != '':\n        # Of course, ignore comments.\n        if line[0] == '*':\n            line = f.readline()\n            continue\n\n        try:\n            # Format: \"1/1\tNew Years Day\"\n            (month, day) = str.split(str.split(line)[0], \"/\")\n            # The _Time class has leading-zero padded day numbers.\n            if len(day) == 1: day = '0' + day\n            # Get month number from index map (compensate for zero indexing).\n            month = MONTH_MAP[int(month) - 1]\n\n            # Check the date.\n            #print month, now.month, day, now.day\n            if month == now.month and day == now.day:\n                return 1\n\n            line = f.readline()\n        except:\n            # Skip malformed lines.\n            line = f.readline()\n            continue\n\n    # If no match found, we must not be in a holiday.\n    return 0"}
{"prompt": "def runs(self, path=\"\", filters={}, order=\"-created_at\", per_page=None):\n        \"\"\"Return a set of runs from a project that match the filters provided.\n        You can filter by config.*, summary.*, state, username, createdAt, etc.\n\n        The filters use the same query language as MongoDB:\n\n        https://docs.mongodb.com/manual/reference/operator/query\n\n        Order can be created_at, heartbeat_at, config.*.value, or summary.*.  By default\n        the order is descending, if you prepend order with a + order becomes ascending.\n        \"\"\"", "output": "\n        if per_page is None:\n            per_page = 10\n        return self._get_project_runs(path, filters, order, path)\n\n    def _get_project_runs(self, path=\"\", filters={}, path_params={}):\n        \"\"\"Return a set of runs from a project that match the filters provided.\n        You can filter by config.*, summary.*, state, username, path, filters, path_params,\n        and heartbeat.*.\n\n        The filters use the same query language as MongoDB:\n\n        https://docs.mongodb.com/manual/reference/operator/query\n\n        The path params use", "solution": "\n        username, project, run = self._parse_path(path)\n        if not self._runs.get(path):\n            self._runs[path + str(filters) + str(order)] = Runs(self.client, username, project,\n                                                                filters=filters, order=order, per_page=per_page)\n        return self._runs[path + str(filters) + str(order)]"}
{"prompt": "def to_utc(self, dt):\n        \"\"\"Convert any timestamp to UTC (with tzinfo).\"\"\"", "output": "\n        if dt.tzinfo is not None:\n            return dt.astimezone(self.tzinfo)\n        else:\n            return dt.replace(tzinfo=pytz.utc)\n\n    def to_local(self, dt):\n        \"\"\"Convert any timestamp to local time (with tzinfo).\"\"\"\n        if dt.tzinfo is not None:\n            return dt.astimezone(self.tzinfo)\n        else:\n            return pytz.utc.localize(dt)\n\n    def to_utc_with_tzinfo(self, dt):\n        \"\"\"Convert any timestamp to UTC (with tzinfo).\"\"\"\n        if dt.tzinfo is not None:\n            return to_utc(", "solution": "\n        if dt.tzinfo is None:\n            return dt.replace(tzinfo=self.utc)\n        return dt.astimezone(self.utc)"}
{"prompt": "def get_experiment(self, coll_name, exp_name):\n        \"\"\"\n        Convenience method that gets experiment resource.\n        \n        Args:\n            coll_name (str): Collection name\n            exp_name (str): Experiment name\n\n        Returns:\n            (ExperimentResource)\n        \"\"\"", "output": "\n        return self.get_resource(exp_name)\n    \n    def get_collection(self, coll_name):\n        \"\"\"\n        Returns a resource for the given experiment name.\n        \n        Args:\n            coll_name (str): Experiment name\n\n        Returns:\n            (ExperimentResource)\n        \"\"\"\n        return self.get_experiment(coll_name)\n    \n    def get_resource(self, coll_name):\n        \"\"\"\n        Returns a resource for the given experiment name.\n        \n        Args:\n            coll_name (str): Experiment name\n\n        Returns:\n            (ExperimentResource)\n        \"\"\"\n        return self.get_resource_collection(coll_", "solution": "\n        exp = ExperimentResource(exp_name, coll_name)\n        return self.get_project(exp)"}
{"prompt": "def add_status_code(code):\n    \"\"\"\n    Decorator used for adding exceptions to :class:`SanicException`.\n    \"\"\"", "output": "\n    def wrapper(func):\n        @wraps(func)\n        def wrapper_wrapper(self, *args, **kwargs):\n            try:\n                return func(self, *args, **kwargs)\n            except SanicException as e:\n                return SanicException(e.message, e.code, e.code_type, e.message_id, code)\n        return wrapper_wrapper\n    return wrapper\n\n\nclass SanicException(Exception):\n    \"\"\"\n    Base class for all Sanic exceptions.\n    \"\"\"\n    pass\n\n\nclass SanicExceptionWithMessage(SanicException):\n    \"\"\"\n    Base class for", "solution": "\n\n    def class_decorator(cls):\n        cls.status_code = code\n        _sanic_exceptions[code] = cls\n        return cls\n\n    return class_decorator"}
{"prompt": "def detect_mode(cls, **params):\n        \"\"\"Detect which listing mode of the given params.\n\n        :params kwargs params: the params\n        :return: one of the available modes\n        :rtype: str\n        :raises ValueError: if multiple modes are detected\n        \"\"\"", "output": "\n        return params.get('mode', cls.DEFAULT_MODE)\n\n    @classmethod\n    def get_all_modes(cls, **params):\n        \"\"\"Get all modes of the given params.\n\n        :params kwargs params: the params\n        :return: a list of the available modes\n        :rtype: list of str\n        :raises ValueError: if multiple modes are detected\n        \"\"\"\n        return cls.get_listing_modes(**params)\n\n    @classmethod\n    def get_listing_modes(cls, **params):\n        \"\"\"Get a list of modes of the given params.\n\n        :params kwargs params: the params\n        :return", "solution": "\n        modes = []\n        for mode in cls.modes:\n            if params.get(mode) is not None:\n                modes.append(mode)\n        if len(modes) > 1:\n            error_message = 'ambiguous mode, must be one of {}'\n            modes_csv = ', '.join(list(cls.modes))\n            raise ValueError(error_message.format(modes_csv))\n        return modes[0] if modes else cls.default_mode"}
{"prompt": "def markPartitionForEvent(self, db_name, tbl_name, part_vals, eventType):\n    \"\"\"\n    Parameters:\n     - db_name\n     - tbl_name\n     - part_vals\n     - eventType\n    \"\"\"", "output": "\n    try:\n      self.dbCatalog[db_name]['partitions'].update(part_vals)\n      self.dbCatalog[db_name]['partitions'][tbl_name]['events'].update(\n          part_vals)\n    except KeyError:\n      self.dbCatalog[db_name]['partitions'][tbl_name] = {\n          'partitions': set(),\n          'events': set()}\n\n  def markPartitionForPartition(self, db_name, tbl_name, eventType):\n    \"\"\"\n    Parameters:\n     - db_name\n     - tbl_name\n     - part_vals\n     - eventType\n    \"\"\"\n    try", "solution": "\n    self.send_markPartitionForEvent(db_name, tbl_name, part_vals, eventType)\n    self.recv_markPartitionForEvent()"}
{"prompt": "def connect(self, host, port):\n        \"\"\"Connect to a host on a given port.\n        If the hostname ends with a colon (`:') followed by a number, and\n        there is no port specified, that suffix will be stripped off and the\n        number interpreted as the port number to use.\n        \"\"\"", "output": "\n        if ':' in host:\n            host, port = host.split(':')\n        if not port and self.default_port:\n            self.default_port = int(self.default_port)\n        self.host = host\n        self.port = int(port)\n        self.pid = os.getpid()\n        self.name = 'connection from %s:%s' % (self.host, self.port)\n\n    def __enter__(self):\n        self.open()\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.close()\n\n    def open", "solution": "\n        if not port and (host.find(':') == host.rfind(':')):\n            i = host.rfind(':')\n            if i >= 0:\n                host, port = host[:i], host[i+1:]\n            try: port = int(port)\n            except ValueError:\n                raise socket.error, \"nonnumeric port\"\n        #if self.verbose > 0:\n        #    print 'connect:', (host, port)\n        msg = \"getaddrinfo returns an empty list\"\n        self.sock = None\n        for res in socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM):\n            af, socktype, proto, canonname, sa = res\n            try:\n                self.sock = socket.socket(af, socktype, proto)\n                #if self.debuglevel > 0: print 'connect:', (host, port)\n                self.sock.connect(sa)\n            except socket.error, msg:\n                #if self.debuglevel > 0: print 'connect fail:', (host, port)\n                self.close()\n                continue\n            break\n        if not self.sock:\n            raise socket.error, msg"}
{"prompt": "def _elements_to_dict(data, position, obj_end, opts, subdocument=None):\n    \"\"\"Decode a BSON document.\"\"\"", "output": "\n    if subdocument is None:\n        subdocument = data\n\n    if subdocument is None:\n        return data\n\n    if subdocument is None:\n        return data\n\n    if subdocument is None:\n        return data\n\n    if subdocument is None:\n        return data\n\n    if subdocument is not None:\n        return subdocument\n\n    if opts.get('subdocument_as_id', None) is not None:\n        return subdocument.as_dict()\n\n    return data\n\n\ndef _parse_timestamp(timestamp):\n    \"\"\"Parse a BSON timestamp.\"\"\"\n    try:\n        return datetime.datetime.strptime(timestamp, '%Y", "solution": "\n    if type(opts.document_class) == tuple:\n        result = opts.document_class[0](**opts.document_class[1]) if not subdocument else dict()\n    else:\n        result = opts.document_class() if not subdocument else dict()\n    end = obj_end - 1\n    while position < end:\n        (key, value, position) = _element_to_dict(data, position, obj_end, opts)\n        result[key] = value\n    return result"}
{"prompt": "def from_dict(cls, d):\n        \"\"\"\n        Convert a dictionary into an xarray.Dataset.\n\n        Input dict can take several forms::\n\n            d = {'t': {'dims': ('t'), 'data': t},\n                 'a': {'dims': ('t'), 'data': x},\n                 'b': {'dims': ('t'), 'data': y}}\n\n            d = {'coords': {'t': {'dims': 't', 'data': t,\n                                  'attrs': {'units':'s'}}},\n                 'attrs': {'title': 'air temperature'},\n                 'dims': 't',\n                 'data_vars': {'a': {'dims': 't', 'data': x, },\n                               'b': {'dims': 't', 'data': y}}}\n\n        where 't' is the name of the dimesion, 'a' and 'b' are names of data\n        variables and t, x, and y are lists, numpy.arrays or pandas objects.\n\n        Parameters\n        ----------\n        d : dict, with a minimum structure of {'var_0': {'dims': [..], \\\n                                                         'data': [..]}, \\\n                                               ...}\n\n        Returns\n        -------\n        obj : xarray.Dataset\n\n        See also\n        --------\n        Dataset.to_dict\n        DataArray.from_dict\n        \"\"\"", "output": "': {'t': {'dims': 't', 'data': t,\n                                'attrs': {'units':'K'}}},\n                 'dims': 't',\n                 'data': {'t': {'dims': 't', 'data': t,\n                                'attrs': {'units':'K'}}},\n                 'attrs': {'title': 'air temperature'},\n                 'title': 'air temperature'}\n\n        \"\"\"\n        if isinstance(d, dict):\n            return cls(d)\n        else:\n            return cls(np.asarray(d))\n\n    def to_dict(self):\n        \"\"\"\n        Convert a dictionary into an xarray.Dataset", "solution": "\n\n        if not set(['coords', 'data_vars']).issubset(set(d)):\n            variables = d.items()\n        else:\n            import itertools\n            variables = itertools.chain(d.get('coords', {}).items(),\n                                        d.get('data_vars', {}).items())\n        try:\n            variable_dict = OrderedDict([(k, (v['dims'],\n                                              v['data'],\n                                              v.get('attrs'))) for\n                                         k, v in variables])\n        except KeyError as e:\n            raise ValueError(\n                \"cannot convert dict without the key \"\n                \"'{dims_data}'\".format(dims_data=str(e.args[0])))\n        obj = cls(variable_dict)\n\n        # what if coords aren't dims?\n        coords = set(d.get('coords', {})) - set(d.get('dims', {}))\n        obj = obj.set_coords(coords)\n\n        obj.attrs.update(d.get('attrs', {}))\n\n        return obj"}
{"prompt": "def evaluate(self, truth, predicted):\n        \"\"\"\n        Evaluates the predicted outputs against the gold data.\n        \"\"\"", "output": "\n        if self.verbose:\n            print(\"  %s: %s\" % (self.name, predicted))\n        if truth == predicted:\n            print(\"  OK\")\n        else:\n            print(\"  FAIL\")\n            print(\"  %s: %s\" % (self.name, truth))\n            print(\"  %s: %s\" % (self.name, predicted))\n\n        # Check if the gold data is valid\n        if not self.validate(predicted):\n            return False\n\n        # Check if the gold data is correct\n        if not self.compare(self.gold_data, predicted):", "solution": "\n        correct = 0.0\n        total = 0.0\n        for i in range(len(truth)):\n            if(truth[i] == predicted[i]):\n                correct += 1\n            total += 1\n        return 1.0*correct/total"}
{"prompt": "def date_range(cls,start_time,end_time,freq):\n        \"\"\"\n        Returns a new SArray that represents a fixed frequency datetime index.\n\n        Parameters\n        ----------\n        start_time : datetime.datetime\n          Left bound for generating dates.\n\n        end_time : datetime.datetime\n          Right bound for generating dates.\n\n        freq : datetime.timedelta\n          Fixed frequency between two consecutive data points.\n\n        Returns\n        -------\n        out : SArray\n\n        Examples\n        --------\n        >>> import datetime as dt\n        >>> start = dt.datetime(2013, 5, 7, 10, 4, 10)\n        >>> end = dt.datetime(2013, 5, 10, 10, 4, 10)\n        >>> sa = tc.SArray.date_range(start,end,dt.timedelta(1))\n        >>> print sa\n        dtype: datetime\n        Rows: 4\n        [datetime.datetime(2013, 5, 7, 10, 4, 10),\n         datetime.datetime(2013, 5, 8, 10, 4, 10),\n         datetime.datetime(2013, 5, 9, 10, 4, 10),\n         datetime.datetime(2013, 5, 10, 10, 4, 10)]\n       \"\"\"", "output": ", 5, 7, 10, 5, 10)\n        >>> freq = dt.date_range(start, end,freq)\n        >>> out = dt.scatter(dt.datetime(2013, 5, 7, 10, 4, 10), dt.datetime(2013, 5, 7, 10, 5, 10), dt.datetime(2013, 5, 7, 10, 4, 10))\n        \"\"\"\n        return cls(start_time, end, freq)\n\n    @classmethod\n    def date_interval(cls,start_time,end_time):\n        \"\"\"\n        Returns a new SArray that represents a fixed frequency", "solution": "\n\n        if not isinstance(start_time,datetime.datetime):\n            raise TypeError(\"The ``start_time`` argument must be from type datetime.datetime.\")\n\n        if not isinstance(end_time,datetime.datetime):\n            raise TypeError(\"The ``end_time`` argument must be from type datetime.datetime.\")\n\n        if not isinstance(freq,datetime.timedelta):\n            raise TypeError(\"The ``freq`` argument must be from type datetime.timedelta.\")\n\n        from .. import extensions\n        return extensions.date_range(start_time,end_time,freq.total_seconds())"}
{"prompt": "def list(self, path=None, with_metadata=False, include_partitions=False):\n        \"\"\"get a list of all of the files in the repository\"\"\"", "output": "\n        if path is None:\n            path = self.path\n\n        if path is None:\n            path = self.path / \"**\"\n\n        if path.is_dir():\n            return [path / f for f in self.ls(path.name, with_metadata=with_metadata, include_partitions=include_partitions)]\n        else:\n            return [path / f for f in self.ls(path.name, with_metadata=with_metadata, include_partitions=include_partitions)]\n\n    def ls(self, path=None, with_metadata=False):\n        \"\"\"get a list of all", "solution": "\n        return self.upstream.list(\n            path,\n            with_metadata=with_metadata,\n            include_partitions=include_partitions)"}
{"prompt": "def _greater_or_close(a, value, **kwargs):\n    r\"\"\"Compare values for greater or close to boolean masks.\n\n    Returns a boolean mask for values greater than or equal to a target within a specified\n    absolute or relative tolerance (as in :func:`numpy.isclose`).\n\n    Parameters\n    ----------\n    a : array-like\n        Array of values to be compared\n    value : float\n        Comparison value\n\n    Returns\n    -------\n    array-like\n        Boolean array where values are greater than or nearly equal to value.\n\n    \"\"\"", "output": "\n    return np.less_equal(a, value, **kwargs)\n\n\ndef _less_or_equal(a, b, **kwargs):\n    r\"\"\"Compare values for less or equal to boolean masks.\n\n    Returns a boolean mask for values less than or equal to a target within a specified\n    absolute or relative tolerance (as in :func:`numpy.isclose`).\n\n    Parameters\n    ----------\n    a : array-like\n        Array of values to be compared\n    b : array-like\n        Array of boolean values\n\n    \"\"\"\n    return np.equal(a, b, **kwargs)\n\n\ndef _less_or_", "solution": "\n    return (a > value) | np.isclose(a, value, **kwargs)"}
{"prompt": "def psycopg2_wait_callback(conn):\n    \"\"\"A wait callback to allow greenlet to work with Psycopg.\n    The caller must be from a greenlet other than the main one.\n\n    :param conn: psycopg2 connection or file number\n\n    This function must be invoked from a coroutine with parent, therefore\n    invoking it from the main greenlet will raise an exception.\n    \"\"\"", "output": "\n    if not isinstance(conn, int):\n        raise Exception(\"conn must be an int\")\n    while True:\n        try:\n            yield conn.wait()\n        except psycopg2.OperationalError:\n            if conn == 0:\n                raise StopIteration\n            else:\n                raise\n        except StopIteration:\n            break\n", "solution": "\n    while True:\n        state = conn.poll()\n        if state == extensions.POLL_OK:\n            # Done with waiting\n            break\n        elif state == extensions.POLL_READ:\n            _wait_fd(conn)\n        elif state == extensions.POLL_WRITE:\n            _wait_fd(conn, read=False)\n        else:  # pragma    nocover\n            raise OperationalError(\"Bad result from poll: %r\" % state)"}
{"prompt": "def bbox(self):\n        \"\"\"\n        Minimal bounding box that contains all the visible layers.\n\n        Use :py:attr:`~psd_tools.api.psd_image.PSDImage.viewbox` to get\n        viewport bounding box. When the psd is empty, bbox is equal to the\n        canvas bounding box.\n\n        :return: (left, top, right, bottom) `tuple`.\n        \"\"\"", "output": "\n        return (self.viewbox.bbox()[0], self.viewbox.bbox()[1],\n                self.viewbox.bbox()[2], self.viewbox.bbox()[3])\n\n    def get_image_data(self, image):\n        \"\"\"\n        Get the data from a PIL Image object.\n\n        :param image: (width, height, 3) `PIL Image` object.\n        :return: (width, height, 3) `tuple`.\n        \"\"\"\n        return (image.size[0], image.size[1], 3)\n\n    def get_image_data_size(self,", "solution": "\n        bbox = super(PSDImage, self).bbox\n        if bbox == (0, 0, 0, 0):\n            bbox = self.viewbox\n        return bbox"}
{"prompt": "def main() -> int:\n    \"\"\"\n    Utility to create and publish the Docker cache to Docker Hub\n    :return:\n    \"\"\"", "output": "\n    parser = argparse.ArgumentParser(description='Cache a Docker image')\n    parser.add_argument('--cache-dir', type=str, default=None, help='Directory to cache images')\n    parser.add_argument('--docker-host', type=str, default=None, help='Hostname of the Docker Hub')\n    parser.add_argument('--docker-port', type=int, default=None, help='Port of the Docker Hub')\n    parser.add_argument('--docker-user', type=str, default=None, help='Username of the Docker Hub')\n    parser.add", "solution": "\n    # We need to be in the same directory than the script so the commands in the dockerfiles work as\n    # expected. But the script can be invoked from a different path\n    base = os.path.split(os.path.realpath(__file__))[0]\n    os.chdir(base)\n\n    logging.getLogger().setLevel(logging.DEBUG)\n    logging.getLogger('botocore').setLevel(logging.INFO)\n    logging.getLogger('boto3').setLevel(logging.INFO)\n    logging.getLogger('urllib3').setLevel(logging.INFO)\n    logging.getLogger('s3transfer').setLevel(logging.INFO)\n\n    def script_name() -> str:\n        return os.path.split(sys.argv[0])[1]\n\n    logging.basicConfig(format='{}: %(asctime)-15s %(message)s'.format(script_name()))\n\n    parser = argparse.ArgumentParser(description=\"Utility for preserving and loading Docker cache\", epilog=\"\")\n    parser.add_argument(\"--docker-registry\",\n                        help=\"Docker hub registry name\",\n                        type=str,\n                        required=True)\n\n    args = parser.parse_args()\n\n    platforms = build_util.get_platforms()\n    try:\n        _login_dockerhub()\n        return build_save_containers(platforms=platforms, registry=args.docker_registry, load_cache=True)\n    finally:\n        _logout_dockerhub()"}
{"prompt": "def query_pager_by_user(userid, current_page_num=1):\n        \"\"\"\n        Query pager\n        \"\"\"", "output": "\n        if not current_page_num:\n            current_page_num = 1\n        pager = Paginator(\n            self.object_list,\n            per_page=current_page_num,\n            orphans=1,\n            allow_empty_first_page=True\n        )\n        return pager.page(userid, current_page_num)\n\n    def get_page(self, page_num):\n        \"\"\"\n        Get page\n        \"\"\"\n        return Paginator(\n            self.object_list,\n            page_num,\n            orphans=1\n        )\n\n    def get_page_range(self,", "solution": "\n        return TabLog.select().where(TabLog.user_id == userid).order_by(\n            TabLog.time_create.desc()\n        ).paginate(\n            current_page_num, CMS_CFG['list_num']\n        )"}
{"prompt": "def get_random_id(length):\n    \"\"\"Generate a random, alpha-numerical id.\"\"\"", "output": "\n    return ''.join(random.choice(string.ascii_lowercase) for _ in range(length))\n\n\ndef get_random_string(length):\n    \"\"\"Generate a random, alpha-numerical string.\"\"\"\n    return ''.join(random.choice(string.ascii_lowercase) for _ in range(length))\n\n\ndef get_random_alpha(length):\n    \"\"\"Generate a random, alpha-numerical alpha.\"\"\"\n    return ''.join(random.choice(string.ascii_lowercase) for _ in range(length))\n\n\ndef get_random_alphanumeric(length):\n    \"\"\"Generate a random", "solution": "\n    alphabet = string.ascii_uppercase + string.ascii_lowercase + string.digits\n    return ''.join(random.choice(alphabet) for _ in range(length))"}
{"prompt": "def translate(self, package, into=None):\n    \"\"\"From a SourcePackage, translate to a binary distribution.\"\"\"", "output": "\n    if into is None:\n      into = self.distribution.name\n    if package.version:\n      version = package.version\n    else:\n      version = ''\n    return '%s-%s-%s' % (package.name, version, package.version)\n\n  def __init__(self, package, version, revision, patches, options):\n    self.package = package\n    self.version = version\n    self.revision = revision\n    self.patches = patches\n    self.options = options\n    self.deps_file = os.path.join(package.deps_file, package.name + '.deps')\n   ", "solution": "\n    if not isinstance(package, SourcePackage):\n      return None\n    if not package.local:\n      raise ValueError('SourceTranslator cannot translate remote packages.')\n\n    installer = None\n    version = self._interpreter.version\n    unpack_path = Archiver.unpack(package.local_path)\n    into = into or safe_mkdtemp()\n\n    try:\n      if self._use_2to3 and version >= (3,):\n        with TRACER.timed('Translating 2->3 %s' % package.name):\n          self.run_2to3(unpack_path)\n      installer = self._installer_impl(unpack_path, interpreter=self._interpreter)\n      with TRACER.timed('Packaging %s' % package.name):\n        try:\n          dist_path = installer.bdist()\n        except self._installer_impl.InstallFailure as e:\n          TRACER.log('Failed to install package at %s: %s' % (unpack_path, e))\n          return None\n        target_path = os.path.join(into, os.path.basename(dist_path))\n        safe_copy(dist_path, target_path)\n        target_package = Package.from_href(target_path)\n        if not target_package:\n          TRACER.log('Target path %s does not look like a Package.' % target_path)\n          return None\n        if not target_package.compatible(self._supported_tags):\n          TRACER.log('Target package %s is not compatible with %s' % (\n              target_package, self._supported_tags))\n          return None\n        return DistributionHelper.distribution_from_path(target_path)\n    except Exception as e:\n      TRACER.log('Failed to translate %s' % package)\n      TRACER.log(traceback.format_exc())\n    finally:\n      if installer:\n        installer.cleanup()\n      if unpack_path:\n        safe_rmtree(unpack_path)"}
{"prompt": "def find_user_file(self, option_name, filename_list):\n        \"\"\"! @brief Search the project directory for a file.\"\"\"", "output": "\n        if not filename_list:\n            return None\n        if not os.path.isdir(filename_list):\n            return None\n        for filename in filename_list:\n            if os.path.isfile(filename):\n                return filename\n        return None\n\n    def find_project_file_by_name(self, option_name, filename_list):\n        \"\"\"! @brief Search the project directory for a file by name.\"\"\"\n        if not filename_list:\n            return None\n        if not os.path.isdir(filename_list):\n            return None\n        for filename in filename_list:\n            if os.path.isfile", "solution": "\n        if option_name is not None:\n            filePath = self._options.get(option_name, None)\n        else:\n            filePath = None\n        \n        # Look for default filenames if a path wasn't provided.\n        if filePath is None:\n            for filename in filename_list:\n                thisPath = os.path.join(self.project_dir, filename)\n                if os.path.isfile(thisPath):\n                    filePath = thisPath\n                    break\n        # Use the path passed in options, which may be absolute, relative to the\n        # home directory, or relative to the project directory.\n        else:\n            filePath = os.path.expanduser(filePath)\n            if not os.path.isabs(filePath):\n                filePath = os.path.join(self.project_dir, filePath)\n        \n        return filePath"}
{"prompt": "def terminate(self):\n        \"\"\"Terminate running processes.\"\"\"", "output": "\n        self.logger.info(\"Terminating processes\")\n        self.logger.debug(\"Terminating processes: %s\", self.processes)\n        self.processes = []\n        for p in self.processes:\n            p.terminate()\n        self.logger.info(\"Terminating processes\")\n        self.logger.debug(\"Terminating processes: %s\", self.processes)\n\n    def _get_process_list(self):\n        \"\"\"Return a list of processes.\"\"\"\n        return self.processes\n\n    def _get_process_list_lock(self):\n        \"\"\"Lock process list.\"\"\"\n        return self.processes_lock\n\n   ", "solution": "\n        if self._process.state() == QProcess.Running:\n            try:\n                self._process.terminate()\n            except Exception:\n                pass\n        self._fired = True"}
{"prompt": "def dock_json_has_plugin_conf(self, plugin_type, plugin_name):\n        \"\"\"\n        Check whether a plugin is configured.\n        \"\"\"", "output": "\n        return self.get_plugin_conf(plugin_type).get(plugin_name)\n\n    def get_plugin_conf(self, plugin_type, plugin_name):\n        \"\"\"\n        Get the configuration for a given plugin.\n        \"\"\"\n        if plugin_type not in self.plugins_by_type:\n            return {}\n        return self.plugins_by_type[plugin_type].get(plugin_name)\n\n    def get_plugin_names(self):\n        \"\"\"\n        Get the names of all the plugins.\n        \"\"\"\n        return self.plugins.keys()\n\n    def get_plugin_names_", "solution": "\n\n        try:\n            self.dock_json_get_plugin_conf(plugin_type, plugin_name)\n            return True\n        except (KeyError, IndexError):\n            return False"}
{"prompt": "def dcounts(self):\n        \"\"\"\n        :return: a data frame with names and distinct counts and fractions for all columns in the database\n        \"\"\"", "output": "\n        return pd.DataFrame(self.data, index=self.index, columns=self.fractions)\n\n    def __repr__(self):\n        return \"Dcounts(data=%s, index=%s, fractions=%s)\" % (self.data, self.index, self.fractions)\n\n\nclass Dcounts(object):\n    \"\"\"\n    A class to store the data frame of a database.\n\n    :param name: the name of the database\n    :param db: the database\n    :param name_type: the type of the database\n    :param name_type_desc: the description of the", "solution": "\n        print(\"WARNING: Distinct value count for all tables can take a long time...\", file=sys.stderr)\n        sys.stderr.flush()\n\n        data = []\n        for t in self.tables():\n            for c in t.columns():\n                data.append([t.name(), c.name(), c.dcount(), t.size(), c.dcount() / float(t.size())])\n        df = pd.DataFrame(data, columns=[\"table\", \"column\", \"distinct\", \"size\", \"fraction\"])\n        return df"}
{"prompt": "def compare_recursive_mtime(path, cutoff, newest=True):\n    \"\"\"Compare the newest/oldest mtime for all files in a directory.\n    Cutoff should be another mtime to be compared against. If an mtime that is\n    newer/older than the cutoff is found it will return True.\n    E.g. if newest=True, and a file in path is newer than the cutoff, it will\n    return True.\n    \"\"\"", "output": "\n    if path in path_map:\n        if newest:\n            return path_map[path] < cutoff\n        else:\n            return path_map[path] > cutoff\n    else:\n        if newest:\n            return path_map[path] < newest\n        else:\n            return path_map[path] < path_map[path_map.keys()[0]]\n\n\ndef get_file_mtime(path):\n    \"\"\"Return the newest/oldest mtime for a file in the given directory.\n    \"\"\"\n    return get_file_map(path, file_mtime_map)\n\n\ndef get_file", "solution": "\n    if os.path.isfile(path):\n        mt = mtime(path)\n        if newest:\n            if mt > cutoff:\n                return True\n        elif mt < cutoff:\n            return True\n    for dirname, _, filenames in os.walk(path, topdown=False):\n        for filename in filenames:\n            mt = mtime(os.path.join(dirname, filename))\n            if newest:  # Put outside of loop?\n                if mt > cutoff:\n                    return True\n            elif mt < cutoff:\n                return True\n    return False"}
{"prompt": "def is_numeric_dtype(dtype):\n    \"\"\"Return ``True`` if ``dtype`` is a numeric type.\"\"\"", "output": "\n    return _is_dtype_type(dtype) and not is_numeric_dtype(dtype).all()\n\n\ndef is_float_dtype(dtype):\n    \"\"\"Return ``True`` if ``dtype`` is a float type.\"\"\"\n    return _is_dtype_type(dtype) and not is_float_dtype(dtype).all()\n\n\ndef is_complex_dtype(dtype):\n    \"\"\"Return ``True`` if ``dtype`` is a complex type.\"\"\"\n    return _is_dtype_type(dtype) and not is_complex_dtype(dtype).all()\n\n\ndef is_complex_", "solution": "\n    dtype = np.dtype(dtype)\n    return np.issubsctype(getattr(dtype, 'base', None), np.number)"}
{"prompt": "def get_facility_status(self, facility):\n        \"\"\"Get the current status of a Guest Additions facility.\n\n        in facility of type :class:`AdditionsFacilityType`\n            Facility to check status for.\n\n        out timestamp of type int\n            Timestamp (in ms) of last status update seen by the host.\n\n        return status of type :class:`AdditionsFacilityStatus`\n            The current (latest) facility status.\n\n        \"\"\"", "output": "\n        return self.get_host_facility_status(facility).get_last_status()\n\n    def get_host_facility_status(self, host):\n        \"\"\"Get the current (latest) status of a host's Additions.\n\n        :param host: Host's Additions.\n\n        out timestamp of type int\n            Timestamp (in ms) of last status update seen by the host.\n\n        return status of type :class:`AdditionsHostFacilityStatus`\n            The current (latest) host's Additions status.\n\n        \"\"\"\n        return self.get_host_facility_status(host).get_status()", "solution": "\n        if not isinstance(facility, AdditionsFacilityType):\n            raise TypeError(\"facility can only be an instance of type AdditionsFacilityType\")\n        (status, timestamp) = self._call(\"getFacilityStatus\",\n                     in_p=[facility])\n        status = AdditionsFacilityStatus(status)\n        return (status, timestamp)"}
{"prompt": "def disassemble_string(self, lpAddress, code):\n        \"\"\"\n        Disassemble instructions from a block of binary code.\n\n        @type  lpAddress: int\n        @param lpAddress: Memory address where the code was read from.\n\n        @type  code: str\n        @param code: Binary code to disassemble.\n\n        @rtype:  list of tuple( long, int, str, str )\n        @return: List of tuples. Each tuple represents an assembly instruction\n            and contains:\n             - Memory address of instruction.\n             - Size of instruction in bytes.\n             - Disassembly line of instruction.\n             - Hexadecimal dump of instruction.\n        \"\"\"", "output": "adecimal disassembly line of instruction.\n             - Hexadecimal instruction line of instruction.\n             - Hexadecimal instruction line of instruction.\n             - Hexadecimal instruction line of instruction.\n             - Address of instruction.\n             - Disassembly line of instruction.\n        \"\"\"\n        return self._disassemble_string(lpAddress, code, disassemble_string_length=16)\n\n    def _disassemble_string(self, lpAddress, code, disassemble_length=16):\n        \"\"\"\n        Disassemble instructions from a block of binary code.\n\n        @type  lpAddress: int\n        @param lpAddress: Memory address where the code was", "solution": "\n        aProcess = self.get_process()\n        return aProcess.disassemble_string(lpAddress, code)"}
{"prompt": "def create_database(self, name, owner=None):\n        \"\"\"\n        Create a new MapD database\n\n        Parameters\n        ----------\n        name : string\n          Database name\n        \"\"\"", "output": "\n        if owner is None:\n            owner = self.user\n        if not self.exists:\n            raise ValueError(\"Database %s does not exist\" % name)\n        if not self.is_open:\n            raise ValueError(\"Database %s is already opened\" % name)\n        if not self.is_closed:\n            raise ValueError(\"Database %s is closed\" % name)\n        if not self.is_open:\n            raise ValueError(\"Database %s is already opened\" % name)\n        if not self.is_closed:\n            raise ValueError(\"Database %s is already closed\" % name)\n        if self.", "solution": "\n        statement = ddl.CreateDatabase(name, owner=owner)\n        self._execute(statement)"}
{"prompt": "def get_buckets(self, bucket_type=None, timeout=None):\n        \"\"\"\n        Serialize bucket listing request and deserialize response\n        \"\"\"", "output": "\n        if bucket_type is None:\n            bucket_type = self.bucket_type\n        if not isinstance(bucket_type, (list, tuple)):\n            raise ValueError(\"bucket_type should be list or tuple\")\n        if len(bucket_type) == 0:\n            bucket_type = [\"all\"]\n        if not isinstance(timeout, (int, float)):\n            raise ValueError(\"timeout should be int or float\")\n        if not isinstance(self.timeout, (int, float)):\n            raise ValueError(\"timeout should be int or float\")\n        if not isinstance(self.wait_time, (int, float)):", "solution": "\n        msg_code = riak.pb.messages.MSG_CODE_LIST_BUCKETS_REQ\n        codec = self._get_codec(msg_code)\n        msg = codec.encode_get_buckets(bucket_type,\n                                       timeout, streaming=False)\n        resp_code, resp = self._request(msg, codec)\n        return resp.buckets"}
{"prompt": "def doDup(self, WHAT={}, **params):\n\t\t\"\"\"This function will perform the command -dup.\"\"\"", "output": "\n\t\tif self.is_admin:\n\t\t\treturn self.admin_do( WHAT, params )\n\t\telse:\n\t\t\treturn self.doDup_1( WHAT, **params )\n\n\tdef doDup_1( self, WHAT={}, **params ):\n\t\t\"\"\"This function will perform the command -dup.\"\"\"\n\t\tif self.is_admin:\n\t\t\treturn self.admin_do( WHAT, params )\n\t\telse:\n\t\t\treturn self.doDup_2( WHAT, **params )\n\n\tdef doDup_2( self, WHAT={}, **params ):", "solution": "\n\n\t\tif hasattr(WHAT, '_modified'):\n\t\t\tfor key, value in WHAT._modified():\n\t\t\t\tif WHAT.__new2old__.has_key(key):\n\t\t\t\t\tself._addDBParam(WHAT.__new2old__[key].encode('utf-8'), value)\n\t\t\t\telse:\n\t\t\t\t\tself._addDBParam(key, value)\n\t\t\tself._addDBParam('RECORDID', WHAT.RECORDID)\n\t\t\tself._addDBParam('MODID', WHAT.MODID)\n\t\telif type(WHAT) == dict:\n\t\t\tfor key in WHAT:\n\t\t\t\tself._addDBParam(key, WHAT[key])\n\t\telse:\n\t\t\traise FMError, 'Python Runtime: Object type (%s) given to function doDup as argument WHAT cannot be used.' % type(WHAT)\n\n\t\tif self._layout == '':\n\t\t\traise FMError, 'No layout was selected'\n\n\t\tfor key in params:\n\t\t\tself._addDBParam(key, params[key])\n\n\t\tif self._checkRecordID() == 0:\n\t\t\traise FMError, 'RecordID is missing'\n\n\t\treturn self._doAction('-dup')"}
{"prompt": "def load_system_host_keys(self, filename=None):\n        \"\"\"\n        Load host keys from a system (read-only) file.  Host keys read with\n        this method will not be saved back by `save_host_keys`.\n\n        This method can be called multiple times.  Each new set of host keys\n        will be merged with the existing set (new replacing old if there are\n        conflicts).\n\n        If ``filename`` is left as ``None``, an attempt will be made to read\n        keys from the user's local \"known hosts\" file, as used by OpenSSH,\n        and no exception will be raised if the file can't be read.  This is\n        probably only useful on posix.\n\n        :param str filename: the filename to read, or ``None``\n\n        :raises: ``IOError`` --\n            if a filename was provided and the file could not be read\n        \"\"\"", "output": " be raised if any of the host keys could not be\n        read.\n        \"\"\"\n        self._known_hosts = {}\n        if filename is not None:\n            self.load_host_keys(filename)\n\n    def load_system_host_keys_file(self, filename):\n        \"\"\"\n        Load host keys from a system (read-only) file.\n\n        If the file exists and is readable, parsing will stop here.  Otherwise,\n        parsing will try to read the keys from the user's local \"known hosts\"\n        file, as a set of \"known hosts\" keys.  Host keys read with this method", "solution": "\n        if filename is None:\n            # try the user's .ssh key file, and mask exceptions\n            filename = os.path.expanduser(\"~/.ssh/known_hosts\")\n            try:\n                self._system_host_keys.load(filename)\n            except IOError:\n                pass\n            return\n        self._system_host_keys.load(filename)"}
{"prompt": "def next_url(request):\n    \"\"\"\n    Returns URL to redirect to from the ``next`` param in the request.\n    \"\"\"", "output": "\n    next = request.GET.get('next', None)\n    if not next:\n        next = request.META.get('HTTP_REFERER', None)\n    if not next:\n        next = request.META.get('HTTP_REFERER', None)\n    return next\n\n\ndef login_required(view_func):\n    \"\"\"\n    Decorator for views that checks that the user is logged in, redirecting\n    to the log-in page if necessary.\n    \"\"\"\n    def _checklogin(request, *args, **kwargs):\n        if request.user.is_authenticated():\n            return view_func(", "solution": "\n    next = request.GET.get(\"next\", request.POST.get(\"next\", \"\"))\n    host = request.get_host()\n    return next if next and is_safe_url(next, host=host) else None"}
{"prompt": "def _urlopen_as_json(self, url, headers=None):\n        \"\"\"Shorcut for return contents as json\"\"\"", "output": "\n        try:\n            return json.loads(self.opener.open(url, headers=headers).read())\n        except (IOError, ValueError):\n            return None\n\n    def _request(self, url, method, headers=None, data=None):\n        \"\"\"Shorcut for return contents as http request\"\"\"\n        if not self.session:\n            self.session = requests.Session()\n        if not data:\n            data = self._urlopen_as_json(url, headers)\n        if not method:\n            method = 'GET'\n        if not data.get('data'):\n            data['data'] = {}\n       ", "solution": "\n        req = Request(url, headers=headers)\n        return json.loads(urlopen(req).read())"}
{"prompt": "def to_json_format(conf):\n    \"\"\"Convert fields of a python dictionary to be dumped in json format\"\"\"", "output": "\n    return json.dumps(conf, indent=4, sort_keys=True)\n\n\ndef to_json(conf):\n    \"\"\"Convert fields of a python dictionary to be dumped in json format\"\"\"\n    return to_json_format(conf)\n\n\ndef to_yaml(conf):\n    \"\"\"Convert fields of a YAML string to be dumped in yaml format\"\"\"\n    return yaml.safe_dump(conf, indent=4, default_flow_style=False)\n\n\ndef to_yaml_file(conf):\n    \"\"\"Convert a python dictionary to a YAML file\"\"\"\n    return to_yaml(conf)", "solution": "\n    if 'fmode' in conf:\n        conf['fmode'] = oct(conf['fmode'])[-3:]\n    if 'dmode' in conf:\n        conf['dmode'] = oct(conf['dmode'])[-3:]"}
{"prompt": "def load_file(self, file_obj, verbose):\n        \"\"\"\n        The type of open file objects such as sys.stdout; alias of the built-in file.\n        @TODO: when is this used? \n        \"\"\"", "output": "\n        self.file_obj = file_obj\n        self.verbose = verbose\n        self.file_obj.seek(0)\n        self.file_obj.truncate()\n        self.file_obj.seek(0)\n        self.file_obj.seek(0)\n        self.file_obj.seek(0)\n        self.file_obj.seek(0)\n        self.file_obj.seek(0)\n        self.file_obj.seek(0)\n        self.file_obj.seek(0)\n        self.file_obj.seek(0)\n        self.file", "solution": "\n        if verbose: printDebug(\"----------\")\n        if verbose: printDebug(\"Reading: <%s> ...'\" % file_obj.name)\n\n        if type(file_obj) == file:\n            self.rdfgraph = self.rdfgraph + file_obj\n            self.sources_valid += [file_obj.NAME]\n        else:\n            self.loading_failed(self.rdf_format_opts)\n            self.sources_invalid += [file_obj.NAME]"}
{"prompt": "def metadata(self, filename):\n        \"\"\"\n        Get some metadata for a given file.\n\n        Can vary from a backend to another but some are always present:\n        - `filename`: the base filename (without the path/prefix)\n        - `url`: the file public URL\n        - `checksum`: a checksum expressed in the form `algo:hash`\n        - 'mime': the mime type\n        - `modified`: the last modification date\n        \"\"\"", "output": "\n        return self._get_metadata(filename)\n\n    def _get_metadata(self, filename):\n        \"\"\"\n        Get some metadata for a given file.\n\n        Can vary from a backend to another but some are always present:\n        - `filename`: the base filename (without the path/prefix)\n        - `url`: the file public URL\n        - `checksum`: a checksum expressed in the form `algo:hash`\n        -'mime': the mime type\n        - `modified`: the last modification date\n        \"\"\"\n        if filename.startswith(self.url):\n            return filename[len(self.url):]\n        else:", "solution": "\n        metadata = self.backend.metadata(filename)\n        metadata['filename'] = os.path.basename(filename)\n        metadata['url'] = self.url(filename, external=True)\n        return metadata"}
{"prompt": "def process_result_value(self, value, dialect):\n        \"\"\"Convert a JSON encoded string to a dictionary structure.\"\"\"", "output": "\n        if value is None:\n            return None\n        return json.loads(value)\n\n    def process_literal_param(self, value, dialect):\n        \"\"\"Convert a JSON encoded string to a Python datetime.\"\"\"\n        if value is None:\n            return None\n        return datetime.strptime(value, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n\n    def process_literal_date(self, value, dialect):\n        \"\"\"Convert a Python datetime to a Python date.\"\"\"\n        if value is None:\n            return None\n        return value.strftime(\"%Y-%m-%d\")\n\n    def process_literal_", "solution": "\n        if value is not None:\n            with BytesIO(value) as stream:\n                with GzipFile(fileobj=stream, mode=\"rb\") as file_handle:\n                    value = json.loads(file_handle.read().decode(\"utf-8\"))\n        return value"}
{"prompt": "def get_full_name(self, role, name):\n        \"\"\"\n        If ``name`` is already the full name of an object, return\n        ``name``.  Otherwise, if ``name`` is a partial object name,\n        look up the full name and return it.\n        \"\"\"", "output": "\n        if isinstance(role, ParameterizedThing):\n            return role.name\n        else:\n            return self.get_name(role, name)\n\n    def get_name(self, thing):\n        \"\"\"\n        If ``thing`` is a string, return it as-is.  Otherwise,\n        return ``thing``.\n        \"\"\"\n        if isinstance(thing, str):\n            return thing\n        else:\n            return self.get_full_name(thing, self.get_name_for_role(role))\n\n    def get_name_for_role(self, role):\n        \"\"\"\n        If ``role`` is", "solution": "\n\n        # An initial '.' indicates a partial name\n        if name[0] == '.':\n            # Find matches for the partial name in the string\n            # containing all full names for this role\n            ptrn = r'(?<= )[^,]*' + name + r'(?=,)'\n            ml = re.findall(ptrn, self.rolnam[role])\n            # Handle cases depending on the number of returned matches,\n            # raising an error if exactly one match is not found\n            if len(ml) == 0:\n                raise KeyError('name matching %s not found' % name,\n                               'name', len(ml))\n            elif len(ml) > 1:\n                raise KeyError('multiple names matching %s found' % name,\n                               'name', len(ml))\n            else:\n                return ml[0]\n        else:\n            # The absence of an initial '.' indicates a full\n            # name. Return the name if it is present in the inventory,\n            # otherwise raise an error\n            try:\n                dom = IntersphinxInventory.roledomain[role]\n            except KeyError:\n                raise KeyError('role %s not found' % role, 'role', 0)\n            if name in self.inv[dom]:\n                return name\n            else:\n                raise KeyError('name %s not found' % name, 'name', 0)"}
{"prompt": "def saveSession(self, filepath):\n        \"\"\"Save androguard session.\"\"\"", "output": "\n        self.save(filepath)\n        self.saveAs(filepath)\n\n    def save(self, filepath):\n        \"\"\"Save session to file.\"\"\"\n        self.saveAs(filepath)\n        self.saveAs(filepath)\n\n    def saveAs(self, filepath):\n        \"\"\"Save session to file.\"\"\"\n        self.save(filepath)\n        self.saveAsBinary(filepath)\n\n    def saveAsBinary(self, filepath):\n        \"\"\"Save session to file.\"\"\"\n        self.save(filepath, binary=True)\n\n    def saveAsBinaryBinary(self, filepath):\n        \"\"\"Save session to file.\"\"\"\n        self.save", "solution": "\n        try:\n            session_module.Save(self.session, filepath)\n        except RuntimeError as e:\n            log.exception(e)\n            os.remove(filepath)\n            log.warning(\"Session not saved\")"}
{"prompt": "def ensure_list(value: Union[T, Sequence[T]]) -> Sequence[T]:\n    \"\"\"Wrap value in list if it is not one.\"\"\"", "output": "\n    if isinstance(value, list):\n        return value\n    return [value]\n\n\ndef ensure_list_item(\n    value: Union[T, Sequence[T]],\n    *,\n    index: int = 0,\n    data: Any = None,\n    description: str = \"\",\n    options: Optional[Dict[str, Any]] = None,\n) -> Union[T, Sequence[T]]:\n    \"\"\"Wrap value in list_item if it is not one.\"\"\"\n    if isinstance(value, list_item):\n        return value\n    return [value, index]\n\n\ndef ensure_list_dict(\n    value", "solution": "\n    if value is None:\n        return []\n    return value if isinstance(value, list) else [value]"}
{"prompt": "def get_parser(parser=None):\n    \"\"\"Get parser for mpu.\"\"\"", "output": "\n    if not parser:\n        parser = argparse.ArgumentParser(\n            description='Test the MPU-based MXNet model on MXNet-100k data')\n    parser.add_argument(\n        '--use_fp16',\n        action='store_true',\n        help='Whether to use fp16 instead of fp32 for training.')\n    parser.add_argument(\n        '--use_fp32',\n        action='store_false',\n        help='Whether to use fp32 instead of fp16 for inference.')\n    return parser\n\n\ndef main(_):\n    \"\"\"Test the MXNet model on MXNet-", "solution": "\n    from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n    if parser is None:\n        parser = ArgumentParser(description=__doc__,\n                                formatter_class=ArgumentDefaultsHelpFormatter)\n    subparsers = parser.add_subparsers()\n    pkg_init_parser = subparsers.add_parser('init')\n    pkg_init_parser.add_argument(\"root\",\n                                 nargs='?',\n                                 help=\"project root - should be empty\")\n    pkg_init_parser.set_defaults(func=run_init)\n    return parser"}
{"prompt": "def symlink(target, link, target_is_directory=False):\n\t\"\"\"\n\tAn implementation of os.symlink for Windows (Vista and greater)\n\t\"\"\"", "output": "\n\tif target_is_directory:\n\t\ttarget = os.path.join(os.path.dirname(target), link.lstrip(os.sep))\n\telse:\n\t\ttarget = os.path.join(os.path.dirname(target), link)\n\tif not os.path.islink(target):\n\t\tos.symlink(target, target_is_directory)\n\telse:\n\t\tos.symlink(target, target_is_directory)\n\ndef copy_win32_files(src, dst, symlinks=False):\n\t\"\"\"\n\tCopies the", "solution": "\n\ttarget_is_directory = (\n\t\ttarget_is_directory or\n\t\t_is_target_a_directory(link, target)\n\t)\n\t# normalize the target (MS symlinks don't respect forward slashes)\n\ttarget = os.path.normpath(target)\n\thandle_nonzero_success(\n\t\tapi.CreateSymbolicLink(link, target, target_is_directory))"}
{"prompt": "def confirm_lock(lockfile):\n    \"\"\"\n    Confirm that the given lockfile contains our pid.\n    Should be entirely unecessary, but paranoia always served me well.\n    \"\"\"", "output": "\n    try:\n        os.kill(lockfile, 0)\n    except OSError:\n        pass\n    else:\n        raise AssertionError(\"Expected to fail\")\n\n\n@attr(shard=2)\n@ddt.ddt\nclass TestLock(TestCase):\n    \"\"\"\n    Tests for locking and unlocking an item\n    \"\"\"\n    def setUp(self):\n        super(TestLock, self).setUp()\n        self.course = CourseFactory.create()\n        self.chapter = ItemFactory.create(category='chapter', parent_location=self.course.location)  # pylint: disable=no-member\n        self.sequential = ItemFactory.create", "solution": "\n    pidfile = open(lockfile, \"r\")\n    pidfile_pid = pidfile.readline().strip()\n    pidfile.close()\n    if int(pidfile_pid) != os.getpid():\n        raise RuntimeError, (\"pidfile %s contains pid %s; expected pid %s!\" %\n                             (lockfile, os.getpid(), pidfile_pid))\n    return True"}
{"prompt": "def calc_individual_chi_squares(residuals,\n                                long_probabilities,\n                                rows_to_obs):\n    \"\"\"\n    Calculates individual chi-squared values for each choice situation in the\n    dataset.\n\n    Parameters\n    ----------\n    residuals : 1D ndarray.\n        The choice vector minus the predicted probability of each alternative\n        for each observation.\n    long_probabilities : 1D ndarray.\n        The probability of each alternative being chosen in each choice\n        situation.\n    rows_to_obs : 2D scipy sparse array.\n        Should map each row of the long format dataferame to the unique\n        observations in the dataset.\n\n    Returns\n    -------\n    ind_chi_squareds : 1D ndarray.\n        Will have as many elements as there are columns in `rows_to_obs`. Each\n        element will contain the pearson chi-squared value for the given choice\n        situation.\n    \"\"\"", "output": " Returns\n    -------\n    chi_squares : 1D ndarray.\n        The individual chi-squared values for each row of the long format\n        dataferame.\n\n    \"\"\"\n    # TODO: add a test for the following\n    # TODO: add a test for the following\n    # TODO: add a test for the following\n    # TODO: add a test for the following\n    # TODO: add a test for the following\n    # TODO: add a test for the following\n    # TODO: add a test for the following\n    # TODO: add a test for the following\n    # TODO: add a test for the following\n    # TODO: add", "solution": "\n    chi_squared_terms = np.square(residuals) / long_probabilities\n    return rows_to_obs.T.dot(chi_squared_terms)"}
{"prompt": "def _resolve_user_group_names(opts):\n    \"\"\"\n    Resolve user and group names in related opts\n    \"\"\"", "output": "\n    user_group_names = []\n    group_names = []\n    for opt in opts:\n        if opt in [u'user', u'group']:\n            if opt == u'user':\n                user_group_names = user_group_names + user_group_names\n            elif opt == u'group':\n                group_names = group_names + group_names\n        else:\n            raise ValueError(\"Unknown option %s\" % opt)\n    for opt in [u'ipauserauthtype', u'ipauserauthtype_password', u'ipauserauthtype_user', u'ipauser", "solution": "\n    name_id_opts = {'uid': 'user.info',\n                    'gid': 'group.info'}\n    for ind, opt in enumerate(opts):\n        if opt.split('=')[0] in name_id_opts:\n            _givenid = opt.split('=')[1]\n            _param = opt.split('=')[0]\n            _id = _givenid\n            if not re.match('[0-9]+$', _givenid):\n                _info = __salt__[name_id_opts[_param]](_givenid)\n                if _info and _param in _info:\n                    _id = _info[_param]\n            opts[ind] = _param + '=' + six.text_type(_id)\n        opts[ind] = opts[ind].replace('\\\\040', '\\\\ ')\n    return opts"}
{"prompt": "def edge_has_annotation(edge_data: EdgeData, key: str) -> Optional[Any]:\n    \"\"\"Check if an edge has the given annotation.\n\n    :param edge_data: The data dictionary from a BELGraph's edge\n    :param key: An annotation key\n    :return: If the annotation key is present in the current data dictionary\n\n    For example, it might be useful to print all edges that are annotated with 'Subgraph':\n\n    >>> from pybel.examples import sialic_acid_graph\n    >>> for u, v, data in sialic_acid_graph.edges(data=True):\n    >>>     if edge_has_annotation(data, 'Species')\n    >>>         print(u, v, data)\n    \"\"\"", "output": "edge_has_annotation(\n   ...     sialic_acid.SialicAcidGraph, 'Subgraph'):\n   ...     print(u, v, data)\n   ...\n    \"\"\"\n    return edge_data.has_annotation(key)\n\n\ndef edge_has_annotation_with_key(\n    edge_data: EdgeData, key: str, default: bool = True\n) -> Optional[EdgeData]:\n    \"\"\"Check if an edge has the given annotation.\n\n    :param edge_data: The data dictionary from a BELGraph's edge\n    :param key: An annotation key", "solution": "\n    annotations = edge_data.get(ANNOTATIONS)\n\n    if annotations is None:\n        return None\n\n    return annotations.get(key)"}
{"prompt": "def init_db(self):\n        \"\"\"\u8fd9\u4e2a\u4efb\u52a1\u6570\u636e\u5e93\u53ea\u5728\u7a0b\u5e8f\u5f00\u59cb\u65f6\u8bfb\u5165, \u5728\u7a0b\u5e8f\u5173\u95ed\u65f6\u5bfc\u51fa.\n\n        \u56e0\u4e3aGtk\u6ca1\u6709\u50cf\u5728Qt\u4e2d\u90a3\u4e48\u65b9\u4fbf\u7684\u4f7f\u7528SQLite, \u800c\u5fc5\u987b\u5c06\u6240\u6709\u6570\u636e\u8bfb\u5165\u4e00\u4e2a\n        liststore\u4e2d\u624d\u884c.\n        \"\"\"", "output": "\n        self.db = sqlite3.connect(self.db_path)\n        self.db.row_factory = sqlite3.Row\n        self.db.text_factory = str\n        self.db.executescript(self.db_path, \"create table if not exists \" + self.db_table + \" (id int primary key autoincrement, \" + self.db_id + \")\")\n        self.db.executescript(self.db_table, \"create table if not exists \" + self.db_if_not_exists + \" (id int primary key autoincrement, \" + self", "solution": "\n        cache_path = os.path.join(Config.CACHE_DIR,\n                                  self.app.profile['username'])\n        if not os.path.exists(cache_path):\n            os.makedirs(cache_path, exist_ok=True)\n        db = os.path.join(cache_path, TASK_FILE)\n        self.conn = sqlite3.connect(db)\n        self.cursor = self.conn.cursor()\n        sql = "}
{"prompt": "def _set_system_mode(self, v, load=False):\n    \"\"\"\n    Setter method for system_mode, mapped from YANG variable /rbridge_id/system_mode (container)\n    If this variable is read-only (config: false) in the\n    source YANG file, then _set_system_mode is considered as a private\n    method. Backends looking to populate this variable should\n    do so via calling thisObj._set_system_mode() directly.\n    \"\"\"", "output": "\n    def _set_system_mode_with_value(self, v, load=False, system_mode=None):\n        \"\"\"\n        Setter method for system_mode, mapped from YANG variable /rbridge_id/system_mode (container)\n        If this variable is read-only (config: false) in the\n        source YANG file, then _set_system_mode is considered as a private\n        method. Backends looking to populate this variable should\n        do so via calling thisObj._set_system_mode() directly.\n        \"\"\"\n        if system_mode is None:\n            raise ValueError(\"Invalid", "solution": "\n    if hasattr(v, \"_utype\"):\n      v = v._utype(v)\n    try:\n      t = YANGDynClass(v,base=system_mode.system_mode, is_container='container', presence=False, yang_name=\"system-mode\", rest_name=\"system-mode\", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'info': u'Maintenance mode on/off', u'callpoint': u'MaintenanceCallPoint', u'cli-incomplete-no': None}}, namespace='urn:brocade.com:mgmt:brocade-rbridge', defining_module='brocade-rbridge', yang_type='container', is_config=True)\n    except (TypeError, ValueError):\n      raise ValueError({\n          'error-string': "}
{"prompt": "def connect(self):\n        \"\"\" Simple connect \"\"\"", "output": "\n        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.socket.connect(self.address)\n        self.socket.setblocking(0)\n        self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        self.socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n        self.socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEPALIVE, 1)\n        self.socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEP", "solution": "\n        try:\n            self.telnet = Telnet(self.host, self.port)\n            time.sleep(1)\n            self.get()\n            self.get('login admin admin')\n            self.update()\n        except socket.gaierror:\n            self.telnet = None\n            LOGGER.error(\"Cannot connect to %s (%d)\",\n                         self.host, self.retries)"}
{"prompt": "def _itertuples(df):\n    \"\"\"Custom implementation of ``DataFrame.itertuples`` that\n    returns plain tuples instead of namedtuples. About 50% faster.\n    \"\"\"", "output": "\n    for i, row in df.itertuples(index=False):\n        yield tuple(row)\n\n\ndef _itertuples_except(df):\n    \"\"\"Custom implementation of ``DataFrame.itertuples`` that\n    raises a ValueError exception if more than one tuple in ``df`` fail.\n    \"\"\"\n    for i, row in _iterrows(df):\n        if len(row) > 1:\n            raise ValueError(\"tuples are not unique: {}!= {}\".format(\n                i, len(row)\n            ))\n\n\ndef _iterrows_except_ignore_index(df):\n    \"\"\"Custom implementation of ``DataFrame", "solution": "\n    cols = [df.iloc[:, k] for k in range(len(df.columns))]\n    return zip(df.index, *cols)"}
{"prompt": "def sample(self, nmr_samples, burnin=0, thinning=1):\n        \"\"\"Take additional samples from the given likelihood and prior, using this sampler.\n\n        This method can be called multiple times in which the sample state is stored in between.\n\n        Args:\n            nmr_samples (int): the number of samples to return\n            burnin (int): the number of samples to discard before returning samples\n            thinning (int): how many sample we wait before storing a new one. This will draw extra samples such that\n                    the total number of samples generated is ``nmr_samples * (thinning)`` and the number of samples\n                    stored is ``nmr_samples``. If set to one or lower we store every sample after the burn in.\n\n        Returns:\n            SamplingOutput: the sample output object\n        \"\"\"", "output": "ning + 1)``.\n        \"\"\"\n        if self.likelihood is None:\n            raise ValueError(\"No likelihood provided\")\n        if self.prior is None:\n            raise ValueError(\"No prior provided\")\n        if self.burnin is None:\n            raise ValueError(\"No burnin provided\")\n        if self.thinning is None:\n            raise ValueError(\"No thinning provided\")\n        if self.nmr_samples is None:\n            raise ValueError(\"No nmr_samples provided\")\n        if self.nmr_prior is None:\n            raise ValueError(\"No nmr_prior provided\")\n        if self.n_samples is None", "solution": "\n        if not thinning or thinning < 1:\n            thinning = 1\n        if not burnin or burnin < 0:\n            burnin = 0\n\n        max_samples_per_batch = max(1000 // thinning, 100)\n\n        with self._logging(nmr_samples, burnin, thinning):\n            if burnin > 0:\n                for batch_start, batch_end in split_in_batches(burnin, max_samples_per_batch):\n                    self._sample(batch_end - batch_start, return_output=False)\n            if nmr_samples > 0:\n                outputs = []\n                for batch_start, batch_end in split_in_batches(nmr_samples, max_samples_per_batch):\n                    outputs.append(self._sample(batch_end - batch_start, thinning=thinning))\n                return SimpleSampleOutput(*[np.concatenate([o[ind] for o in outputs], axis=-1) for ind in range(3)])"}
{"prompt": "def excludeSNPs(inPrefix, outPrefix, exclusionFileName):\n    \"\"\"Exclude some SNPs using Plink.\n\n    :param inPrefix: the prefix of the input file.\n    :param outPrefix: the prefix of the output file.\n    :param exclusionFileName: the name of the file containing the markers to be\n                              excluded.\n\n    :type inPrefix: str\n    :type outPrefix: str\n    :type exclusionFileName: str\n\n    Using Plink, exclude a list of markers from ``inPrefix``, and saves the\n    results in ``outPrefix``. The list of markers are in ``exclusionFileName``.\n\n    \"\"\"", "output": " \"\"\"\n    with open(exclusionFileName, 'w') as f:\n        for marker in inPrefix.split(','):\n            f.write(marker + '\\n')\n\n    return inPrefix, outPrefix, exclusionFileName\n\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument('-i', '--input',\n                        dest='input',\n                        required=True,\n                        help='the input file')\n    parser.add_argument('-o', '--output',\n                        dest='output',\n                        required=True,\n                        help='the output file')\n    parser.add_", "solution": "\n    plinkCommand = [\"plink\", \"--noweb\", \"--bfile\", inPrefix, \"--exclude\",\n                    exclusionFileName, \"--make-bed\", \"--out\", outPrefix]\n    runCommand(plinkCommand)"}
{"prompt": "def get_interaction_energy(self, assign_ff=True, ff=None, mol2=False,\n                               force_ff_assign=False):\n        \"\"\"Calculates the interaction energy of the AMPAL object.\n\n        Parameters\n        ----------\n        assign_ff: bool, optional\n            If true the force field will be updated if required.\n        ff: BuffForceField, optional\n            The force field to be used for scoring.\n        mol2: bool, optional\n            If true, mol2 style labels will also be used.\n        force_ff_assign: bool, optional\n            If true, the force field will be completely reassigned, \n            ignoring the cached parameters.\n\n        Returns\n        -------\n        buff_score: buff.BUFFScore\n            A BUFFScore object with information about each of the\n            interactions and the `Atoms` involved.\n\n        Raises\n        ------\n        AttributeError\n            Raise if a component molecule does not have an `update_ff`\n            method.\n        \"\"\"", "output": " updated if required.\n\n        Returns\n        -------\n        float\n            The interaction energy of the AMPAL object.\n\n        \"\"\"\n        if assign_ff_assign:\n            self.ff_assign_to_energy(ff)\n        if not mol2:\n            return 0.0\n        if force_ff_assign:\n            return self.force_field_to_energy(force_ff)\n        if not self.ff_assign_to_energy(ff):\n            return 0.0\n        return self.get_interaction_energy(assign_ff=assign_ff, ff=ff)\n\n    def get_interaction_energy_", "solution": "\n        if not ff:\n            ff = global_settings['buff']['force_field']\n        if assign_ff:\n            for molecule in self._molecules:\n                if hasattr(molecule, 'update_ff'):\n                    molecule.update_ff(\n                        ff, mol2=mol2, force_ff_assign=force_ff_assign)\n                else:\n                    raise AttributeError(\n                        'The following molecule does not have a update_ff'\n                        'method:\\n{}\\nIf this is a custom molecule type it'\n                        'should inherit from BaseAmpal:'.format(molecule))\n        interactions = find_inter_ampal(self, ff.distance_cutoff)\n        buff_score = score_interactions(interactions, ff)\n        return buff_score"}
{"prompt": "def _get_summary_struct(self):\n        \"\"\"\n        Returns a structured description of the model, including (where\n        relevant) the schema of the training data, description of the training\n        data, training statistics, and model hyperparameters.\n\n        Returns\n        -------\n        sections : list (of list of tuples)\n            A list of summary sections.\n              Each section is a list.\n                Each item in a section list is a tuple of the form:\n                  ('<label>','<field>')\n\n        section_titles: list\n            A list of section titles.\n              The order matches that of the 'sections' object.\n        \"\"\"", "output": "\n        summary_struct = []\n        for section in self.sections:\n            if section.label == section_names[self.label]:\n                for field in section.field:\n                    if field.type =='string':\n                        summary_struct.append(section.label)\n                        field_title = field.string_value\n                        break\n        for item in self.schema:\n            if item.type =='section':\n                for label in item.label:\n                    if label.startswith(section_names[self.label]):\n                        for field in summary_struct:\n                            if field.type =='section_title':\n                                if", "solution": "\n        section = []\n        section_titles = ['Attributes']\n        for f in self._list_fields():\n            section.append( (\"%s\" % f,\"%s\"% f) )\n\n        return ([section], section_titles)"}
{"prompt": "def degree_prune(graph, max_degree=20):\n    \"\"\"Prune the k-neighbors graph back so that nodes have a maximum\n    degree of ``max_degree``.\n\n    Parameters\n    ----------\n    graph: sparse matrix\n        The adjacency matrix of the graph\n\n    max_degree: int (optional, default 20)\n        The maximum degree of any node in the pruned graph\n\n    Returns\n    -------\n    result: sparse matrix\n        The pruned graph.\n    \"\"\"", "output": "\n    # TODO: this is a bit of a hack, but it works\n    # TODO: this is a bit of a hack, but it works\n    # TODO: this is a bit of a hack, but it works\n    # TODO: this is a bit of a hack, but it works\n    # TODO: this is a bit of a hack, but it works\n    # TODO: this is a bit of a hack, but it works\n    # TODO: this is a bit of a hack, but it works\n    # TODO: this is a bit of a hack, but it works\n    # TODO: this is a bit", "solution": "\n\n    result = graph.tolil()\n    for i, row_data in enumerate(result.data):\n        if len(row_data) > max_degree:\n            cut_value = np.argsort(row_data)[max_degree]\n            row_data = [x if x <= cut_value else 0.0 for x in row_data]\n            result.data[i] = row_data\n    result = result.tocsr()\n    result.eliminate_zeros()\n    return result"}
{"prompt": "def load_bytes(buf, num, pos):\n    \"\"\"Load sequence of bytes\"\"\"", "output": "\n    return _read_bytes(buf, num, pos)\n\ndef load_int(buf, num, pos):\n    \"\"\"Load integer\"\"\"\n    return _read_int(buf, num, pos)\n\ndef load_float(buf, num, pos):\n    \"\"\"Load floating point\"\"\"\n    return _read_float(buf, num, pos)\n\ndef load_int_from_str(buf, pos):\n    \"\"\"Load integer from string\"\"\"\n    return _read_int(buf, pos)\n\ndef _read_int(buf, pos, size=None):\n    \"\"\"Read", "solution": "\n    end = pos + num\n    if end > len(buf):\n        raise BadRarFile('cannot load bytes')\n    return buf[pos : end], end"}
{"prompt": "def path(self, which=None):\n        \"\"\"Extend ``nailgun.entity_mixins.Entity.path``.\n\n        The format of the returned path depends on the value of ``which``:\n\n        facts\n            /discovered_hosts/facts\n\n        ``super`` is called otherwise.\n\n        \"\"\"", "output": "\n        if which in (None, '', 'local'):\n            return super(DiscoveredHostCollection, self).path()\n        return Path(self._server_url, self._server_url + '/v2/discovered_hosts')\n\n    def list(self, params=None):\n        \"\"\"List all discovered hosts.\n\n        :param params: a dict with additional parameters.\n\n        :returns: a list of ``nailgun.entity_mixins.DiscoveredHost`` objects.\n\n        \"\"\"\n        if params is None:\n            params = {}\n\n        if self._server_url is None:\n            return self._discovered_hosts", "solution": "\n        if which == 'facts':\n            return '{0}/{1}'.format(\n                super(DiscoveredHost, self).path(which='base'),\n                which\n            )\n        return super(DiscoveredHost, self).path(which)"}
{"prompt": "def git_status(self, r):\n        \"\"\"return tuple of (status, adds, dels).\n        status like !! ??\n        adds and dels are lists of additions and deletions.\n        \"\"\"", "output": "\n        return r.git.status(self.path)\n\n    def git_add(self, files):\n        \"\"\"add files to git repository.\n        files is a list of files to add.\n        \"\"\"\n        return r.git.add(self.path, files)\n\n    def git_delete(self, filename):\n        \"\"\"delete file from git repository.\n        filename is a path to a file.\n        \"\"\"\n        return r.git.delete(self.path, filename)\n\n    def git_diff(self, filename):\n        \"\"\"return tuple of (diff, adds, dels).\n        diff is a list of", "solution": "\n        ret = self._cachedStatus.get(r.filename, None) if r else None\n        return ret if ret else [\"//\", None, None]"}
{"prompt": "def get_parameter(name, datatype, properties):\n    \"\"\"\n    Build a Parameter object using its name, datatype and properties list\n    :param name: name of the parameter\n    :type name: basestring\n    :param datatype: datatype of the parameter (must be a value of\n    PARAMETER_CLASSES keys\n    :type datatype: basestring\n    :param properties: property values to be set in attributes or qualifiers\n    :type properties: dict\n    \"\"\"", "output": "\n    return Parameter(name, datatype, properties)\n", "solution": "\n    return PARAMETER_CLASSES.get(datatype, Parameter)(name, datatype,\n                                                      properties)"}
{"prompt": "def _get_isolated(self, hostport):\n        \"\"\"Get a Peer for the given destination for a request.\n\n        A new Peer is added and returned if one does not already exist for the\n        given host-port. Otherwise, the existing Peer is returned.\n\n        **NOTE** new peers will not be added to the peer heap.\n        \"\"\"", "output": "\n        if hostport in self.peers:\n            return self.peers[hostport]\n        else:\n            new_peer = Peer(self.host, self.port, self.timeout)\n            self.peers[hostport] = new_peer\n            return new_peer\n\n    def _get_peers(self):\n        \"\"\"Get a list of all peers.\n\n        **NOTE** list of peers will not be returned.\n        \"\"\"\n        return self.peers.values()\n\n    def _get_timeout(self):\n        \"\"\"Get a list of all timeouts.\n\n        **NOTE** list of timeouts will not be returned.", "solution": "\n        assert hostport, \"hostport is required\"\n        if hostport not in self._peers:\n            # Add a peer directly from a hostport, do NOT add it to the peer\n            # heap\n            peer = self.peer_class(\n                tchannel=self.tchannel,\n                hostport=hostport,\n            )\n            self._peers[peer.hostport] = peer\n\n        return self._peers[hostport]"}
{"prompt": "def remove_permission(self, queue, label):\n        \"\"\"\n        Remove a permission from a queue.\n\n        :type queue: :class:`boto.sqs.queue.Queue`\n        :param queue: The queue object\n\n        :type label: str or unicode\n        :param label: The unique label associated with the permission\n                      being removed.\n\n        :rtype: bool\n        :return: True if successful, False otherwise.\n        \"\"\"", "output": "\n        return self.get_boolean(\n            \"RemovePermission\",\n            params=dict(\n                label=label,\n                force=True,\n            )\n        )\n\n    def get_all_permissions(self, label):\n        \"\"\"\n        Get all permissions associated with a queue.\n\n        :type label: str or unicode\n        :param label: The unique label associated with the permission\n                      being removed.\n\n        :rtype: list of strings\n        :return: The unique label associated with the permission\n                      being removed.\n        \"\"\"\n        params = dict(\n            label=label,\n            force=True,\n        )\n        return self.get_list(\"", "solution": "\n        params = {'Label': label}\n        return self.get_status('RemovePermission', params, queue.id)"}
{"prompt": "def get_environment(self, fUnicode = None):\n        \"\"\"\n        Retrieves the environment with wich the program is running.\n\n        @note: Duplicated keys are joined using null characters.\n            To avoid this behavior, call L{get_environment_variables} instead\n            and convert the results to a dictionary directly, like this:\n            C{dict(process.get_environment_variables())}\n\n        @see: L{win32.GuessStringType}\n\n        @type  fUnicode: bool or None\n        @param fUnicode: C{True} to return a list of Unicode strings, C{False}\n            to return a list of ANSI strings, or C{None} to return whatever\n            the default is for string types.\n\n        @rtype:  dict(str S{->} str)\n        @return: Dictionary of environment keys and values.\n\n        @raise WindowsError: On error an exception is raised.\n        \"\"\"", "output": " return a list of strings, and C{None} to return the default value.\n\n        @rtype:  L{Environment}\n        @return:  The environment object.\n\n        @raise WindowsError: An error occured while trying to access the program\n            or the environment is not supported.\n        \"\"\"\n        if fUnicode is None:\n            fUnicode = self.fUnicode\n        if fUnicode:\n            return self.get_unicode_environment(fUnicode)\n        else:\n            return Environment(0)\n\n    def get_unicode_environment(self, fUnicode = None):\n        \"\"\"\n        Retrieves the Unicode strings from the current environment.", "solution": "\n\n        # Get the environment variables.\n        variables = self.get_environment_variables()\n\n        # Convert the strings to ANSI if requested.\n        if fUnicode is None:\n            gst = win32.GuessStringType\n            fUnicode = gst.t_default == gst.t_unicode\n        if not fUnicode:\n            variables = [ ( key.encode('cp1252'), value.encode('cp1252') ) \\\n                        for (key, value) in variables ]\n\n        # Add the variables to a dictionary, concatenating duplicates.\n        environment = dict()\n        for key, value in variables:\n            if key in environment:\n                environment[key] = environment[key] + u'\\0' + value\n            else:\n                environment[key] = value\n\n        # Return the dictionary.\n        return environment"}
{"prompt": "def run_conf(self):\n        \"\"\"Run configuration (namedtuple)\n        \"\"\"", "output": "\n        self.conf = namedtuple('conf', ['name', 'value'])\n        self.conf.name = self.name\n        self.conf.value = self.value\n\n    def __str__(self):\n        \"\"\"Return string representation of configuration\n        \"\"\"\n        return self.conf.__str__() + '\\n' + self.conf.name + '\\n' + self.conf.value.__str__()\n\n    def __repr__(self):\n        \"\"\"Return string representation of configuration\n        \"\"\"\n        return self.conf.__repr__() + '\\n' + self.conf.name + '\\n' + self.conf.value.__repr", "solution": "\n        run_conf = namedtuple('run_conf', field_names=self._run_conf.keys())\n        return run_conf(**self._run_conf)"}
{"prompt": "def iscontainer(*items):\n    \"\"\"\n    Checks whether all the provided items are containers (i.e of class list,\n    dict, tuple, etc...)\n    \"\"\"", "output": "\n    return isinstance(items, list) and all(iscontainer_like(item) for item in items)\n\n\ndef iscontainer_like(obj):\n    \"\"\"\n    Checks whether an object is an instance of a container (list,\n    tuple, dict, etc...)\n    \"\"\"\n    return isinstance(obj, list) and all(isinstance(item, Container) for item in obj)\n\n\ndef is_container_like(obj):\n    \"\"\"\n    Checks whether an object is an instance of a container (list,\n    tuple, dict, etc...)\n    \"\"\"\n    return isinstance(obj, list) and all(isinstance", "solution": "\n    return all(isinstance(i, Iterable) and not isinstance(i, basestring)\n               for i in items)"}
{"prompt": "def pathFromIndex( self, index ):\r\n        \"\"\"\r\n        Returns the joined path from the given model index.  This will\r\n        join together the full path with periods.\r\n        \r\n        :param      index | <QModelIndex>\r\n        \r\n        :return     <str>\r\n        \"\"\"", "output": "\r\n        return self.model().path( index.row(), index.column(), self.name() )\r\n    \r\n    def name( self ):\r\n        \"\"\"\r\n        Returns the name of the model.\r\n        \r\n        :return     <str>\r\n        \r\n        :return     <str>\r\n        \"\"\"\r\n        return self.model().name()\r\n    \r\n    def rowCount( self ):\r\n        \"\"\"\r\n        Returns the number of rows.\r\n        \r\n        :return     <int>\r\n        \r\n        :return     <int>\r\n        \"\"\"\r\n        return self.rowCount\r\n    \r\n    def columnCount( self ):\r\n        \"\"\"\r\n        Returns the number of columns.\r\n        \r\n        :return     <int>\r\n        \r\n        :return     <int", "solution": "\r\n        item = self._model.itemFromIndex(index)\r\n        out  = []\r\n        \r\n        while ( item ):\r\n            out.append(nativestring(item.text()))\r\n            item = item.parent()\r\n        \r\n        return '.'.join(reversed(out))"}
{"prompt": "def explicit(fixed_qubits: Iterable[raw_types.Qid],\n                 fallback: Optional['QubitOrder']=None) -> 'QubitOrder':\n        \"\"\"A basis that contains exactly the given qubits in the given order.\n\n        Args:\n            fixed_qubits: The qubits in basis order.\n            fallback: A fallback order to use for extra qubits not in the\n                fixed_qubits list. Extra qubits will always come after the\n                fixed_qubits, but will be ordered based on the fallback. If no\n                fallback is specified, a ValueError is raised when extra qubits\n                are specified.\n\n        Returns:\n            A Basis instance that forces the given qubits in the given order.\n        \"\"\"", "output": "\n                are specified.\n\n        Returns:\n            The explicit qubit list.\n        \"\"\"\n        if fallback is None:\n            if len(fixed_qubits) == 0:\n                raise ValueError('No explicit qubits specified.')\n            else:\n                raise ValueError('No explicit qubits specified.')\n\n        if len(fixed_qubits) == 1:\n            return QubitOrder(fixed_qubits[0])\n\n        if len(fixed_qubits) == 2:\n            return QubitOrder(fallback[0] if fallback else None,\n                             fixed_qubits[1] if fixed_qubits else None],\n                            ", "solution": "\n        result = tuple(fixed_qubits)\n        if len(set(result)) < len(result):\n            raise ValueError(\n                'Qubits appear in fixed_order twice: {}.'.format(result))\n\n        def func(qubits):\n            remaining = set(qubits) - set(fixed_qubits)\n            if not remaining:\n                return result\n            if not fallback:\n                raise ValueError(\n                    'Unexpected extra qubits: {}.'.format(remaining))\n            return result + fallback.order_for(remaining)\n\n        return QubitOrder(func)"}
{"prompt": "def _sincedb_start_position(self):\n        \"\"\"Retrieves the starting position from the sincedb sql db\n        for a given file\n        \"\"\"", "output": "\n        return self.db.execute('SELECT pos FROM sincedb WHERE file=?', (self.filename,)).fetchone()[0]\n\n    def _sincedb_end_position(self):\n        \"\"\"Retrieves the ending position from the sincedb sql db\n        for a given file\n        \"\"\"\n        return self.db.execute('SELECT pos FROM sincedb WHERE file=?', (self.filename,)).fetchone()[0]\n\n    def _get_file_position(self):\n        \"\"\"Retrieves the file position from the file name\n        \"\"\"\n        return self.db.execute('SELECT pos FROM files WHERE name=?', (self.filename", "solution": "\n        if not self._sincedb_path:\n            return None\n\n        self._sincedb_init()\n        self._log_debug('retrieving start_position from sincedb')\n        conn = sqlite3.connect(self._sincedb_path, isolation_level=None)\n        cursor = conn.cursor()\n        cursor.execute('select position from sincedb where fid = :fid and filename = :filename', {\n            'fid': self._fid,\n            'filename': self._filename\n        })\n\n        start_position = None\n        for row in cursor.fetchall():\n            start_position, = row\n\n        return start_position"}
{"prompt": "def create_controller(self):\n        \"\"\"Create a controller to handle the request\n\n        :returns: Controller, this Controller instance should be able to handle\n            the request\n        \"\"\"", "output": "\n        raise NotImplementedError()\n\n    def update_controller(self, context, id, controller):\n        \"\"\"Update a controller\n\n        :param context: The request context\n        :param id: The UUID of the controller\n        :param controller: The new controller instance\n        \"\"\"\n        raise NotImplementedError()\n\n    def delete_controller(self, context, id):\n        \"\"\"Delete a controller\n\n        :param context: The request context\n        :param id: The UUID of the controller\n        \"\"\"\n        raise NotImplementedError()\n\n    def get_controller_id_by_uuid(self, context, uuid):\n        \"\"\"Get the ID of the specified controller\n\n        :param context", "solution": "\n        body = None\n        req = self.request\n        res = self.response\n        rou = self.router\n        con = None\n\n        controller_info = {}\n        try:\n            controller_info = rou.find(req, res)\n\n        except IOError as e:\n            logger.warning(str(e), exc_info=True)\n            raise CallError(\n                408,\n                \"The client went away before the request body was retrieved.\"\n            )\n\n        except (ImportError, AttributeError, TypeError) as e:\n            exc_info = sys.exc_info()\n            logger.warning(str(e), exc_info=exc_info)\n            raise CallError(\n                404,\n                \"{} not found because of {} \\\"{}\\\" on {}:{}\".format(\n                    req.path,\n                    exc_info[0].__name__,\n                    str(e),\n                    os.path.basename(exc_info[2].tb_frame.f_code.co_filename),\n                    exc_info[2].tb_lineno\n                )\n            )\n\n        else:\n            con = controller_info['class_instance']\n\n        return con"}
{"prompt": "def on(self, event, f=None):\n        \"\"\"Registers the function ``f`` to the event name ``event``.\n\n        If ``f`` isn't provided, this method returns a function that\n        takes ``f`` as a callback; in other words, you can use this method\n        as a decorator, like so::\n\n            @ee.on('data')\n            def data_handler(data):\n                print(data)\n\n        In both the decorated and undecorated forms, the event handler is\n        returned. The upshot of this is that you can call decorated handlers\n        directly, as well as use them in remove_listener calls.\n        \"\"\"", "output": "url_rule.\n\n       .. versionadded:: 1.0\n        \"\"\"\n        if f is None:\n            return self.on_form_pre(event, f)\n        self.on_form_post(event, f)\n        return self.remove_url_rule(event)\n\n    def on_form_pre(self, event, f=None):\n        \"\"\"Registers a function ``f`` as a callback.\n\n        If ``f`` isn't provided, this method returns a function that\n        takes ``f`` as a callback; in other words, you can use this method\n        as a decorator, like so::\n\n           ", "solution": "\n\n        def _on(f):\n            self._add_event_handler(event, f, f)\n            return f\n\n        if f is None:\n            return _on\n        else:\n            return _on(f)"}
{"prompt": "def from_url(url, **options):\n        \"\"\"\n        Downloads the contents of a given URL and loads it\n        into a new TableFu instance\n        \"\"\"", "output": "\n        if options.get('format', None) == 'json':\n            return TableFu(url, **options)\n        elif options.get('format', None) == 'csv':\n            return TableFu(url, **options)\n        elif options.get('format', None) == 'tab':\n            return TableFu(url, **options)\n        elif options.get('format', None) == 'json':\n            return TableFu(url, **options)\n        else:\n            raise ValueError(\"Unknown format: %s\" % options['format'])\n\n    def to_url(self, table):\n        \"\"\"\n        Converts", "solution": "\n        resp = urllib2.urlopen(url)\n        return TableFu(resp, **options)"}
{"prompt": "def reversetext(contenttoreverse, reconvert=True):\n    \"\"\"\n    Reverse any content\n\n    :type contenttoreverse: string\n    :param contenttoreverse: The content to be reversed\n\n    :type reeval: boolean\n    :param reeval: Wether or not to reconvert the object back into it's initial state. Default is \"True\".\n    \"\"\"", "output": "\n    if reeval:\n        return contenttoreverse\n    else:\n        return contenttoreverse[::-1]\n\ndef get_content_range(start, end, step=1):\n    \"\"\"\n    Get a list of the content between two ranges\n\n    :type start: string\n    :param start: The content range start\n\n    :type end: string\n    :param end: The content range end\n\n    :type step: integer\n    :param step: The content range step\n\n    :rtype: list\n    :return: A list of the content between two ranges.\n    \"\"\"\n    if step > 0:\n        return list(range(start", "solution": "\n\n    # If reconvert is specified\n    if reconvert is True:\n        # Return the evalated form\n        return eval(\n            str(type(contenttoreverse)).split(\"'\")[1] + \"('\" +\n            str(contenttoreverse)[::-1] + \"')\")\n\n    # Return the raw version\n    return contenttoreverse[::-1]"}
{"prompt": "def __ensure_message_length_multiple(bit_data, bit_len: int, pauses, bit_sample_pos, divisor: int):\n        \"\"\"\n        In case of ASK modulation, this method tries to use pauses after messages as zero bits so that\n        the bit lengths of messages are divisible by divisor\n        :param bit_data: List of bit arrays\n        :param bit_len: Bit length that was used for demodulation\n        :param pauses: List of pauses\n        :param bit_sample_pos: List of Array of bit sample positions\n        :param divisor: Divisor the messages should be divisible by\n        \"\"\"", "output": "visor: Demodulation factor\n        :return:\n        \"\"\"\n        # TODO: Check if this is necessary\n        if bit_len == 0:\n            return\n\n        # TODO: Check if this is necessary\n        if pauses == 0:\n            return\n\n        # TODO: Check if this is necessary\n        if len(bit_data) < bit_len:\n            self.__logger.warning(\"Not enough bits to demodulate message\")\n            return\n\n        # TODO: Check if this is necessary\n        if len(bit_data) < pauses:\n            self.__logger.warning(\"Not enough pauses to demodulate message\")\n           ", "solution": "\n        for i in range(len(bit_data)):\n            missing_bits = (divisor - (len(bit_data[i]) % divisor)) % divisor\n            if missing_bits > 0 and pauses[i] >= bit_len * missing_bits:\n                bit_data[i].extend([0] * missing_bits)\n                pauses[i] = pauses[i] - missing_bits * bit_len\n\n                try:\n                    bit_sample_pos[i][-1] = bit_sample_pos[i][-2] + bit_len\n                except IndexError as e:\n                    logger.warning(\"Error padding message \" + str(e))\n                    continue\n\n                bit_sample_pos[i].extend([bit_sample_pos[i][-1] + (k + 1) * bit_len for k in range(missing_bits - 1)])\n                bit_sample_pos[i].append(bit_sample_pos[i][-1] + pauses[i])"}
{"prompt": "def show_item(h):\n    \"\"\"Show any RAR3/5 record.\n    \"\"\"", "output": "\n    if h.has_key('r'):\n        r = h['r']\n        del h['r']\n        del r['id']\n        del r['title']\n        del r['authors']\n        del r['tags']\n        del r['comments']\n        del r['isbn']\n        del r['publisher']\n        del r['series']\n        del r['series_index']\n        del r['series_index_2']\n        del r['isbn']\n        del r['publisher_sort']\n        del r['publisher_sort_2']\n        del r['publisher_sort_1']\n        del r", "solution": "\n    if isinstance(h, rf.Rar3Info):\n        show_item_v3(h)\n    elif isinstance(h, rf.Rar5Info):\n        show_item_v5(h)\n    else:\n        xprint('Unknown info record')"}
{"prompt": "def get_varname_from_locals(val, locals_, default='varname-not-found',\n                            strict=False, cmpfunc_=operator.is_):\n    \"\"\" Finds the string name which has where locals_[name] is val\n\n    Check the varname is in the parent namespace\n    This will only work with objects not primatives\n\n    Args:\n        val (): some value\n        locals_ (dict): local dictionary to search\n        default (str):\n        strict (bool):\n\n    Returns:\n        str: the varname which is Val (if it exists)\n\n    \"\"\"", "output": "\n    for key in locals_:\n        if key in locals_ and locals_[key] == val:\n            return key\n    if strict:\n        raise ValueError('Cannot find default value for %s' % default)\n    return default\n\n\ndef get_varname_from_dict(dict_, default='varname-not-found',\n                        strict=False, cmpfunc_=operator.is_):\n    \"\"\" Finds the string name which has where dict_[\"name\"] is val]\n\n    Check the varname is in the parent namespace\n    This will only work with objects not primatives\n\n    Args:\n        dict_ (dict): the dictionary to search", "solution": "\n    if val is None or isinstance(val, (int, float, bool)):\n        # Cannot work on primative types\n        return default\n    try:\n        for count, val_ in enumerate(six.itervalues(locals_)):\n            if cmpfunc_(val, val_):\n                index_ = count\n        varname = six.text_type(list(locals_.keys())[index_])\n    except NameError:\n        varname = default\n        if strict:\n            raise\n    return varname"}
{"prompt": "def setup_ui(self, ):\n        \"\"\"Setup the general ui\n\n        :returns: None\n        :rtype: None\n        :raises: None\n        \"\"\"", "output": "\n        self.ui = Ui(self)\n        self.ui.setupUi(self)\n        self.ui.buttonBox.button(QDialogButtonBox.Ok).setText(self.tr(\"OK\"))\n        self.ui.buttonBox.button(QDialogButtonBox.Cancel).setText(self.tr(\"Cancel\"))\n        self.ui.buttonBox.button(QDialogButtonBox.Ok).setDefault(True)\n        self.ui.buttonBox.button(QDialogButtonBox.Ok).setFocus(Qt.OtherFocusReason)\n        self.ui.buttonBox.button(QDialogButtonBox.Ok", "solution": "\n        plus_icon = get_icon('glyphicons_433_plus_bright.png', asicon=True)\n        self.add_tb.setIcon(plus_icon)\n\n        self.shot_browser = ListBrowser(4, parent=self, headers=[\"Project\", \"Sequence\", \"Shot\", \"Type\"])\n        self.asset_browser = ListBrowser(4, parent=self, headers=[\"Project\", \"Assettype\", \"Asset\", \"Type\"])\n\n        self.shotmodel = self.create_shot_model()\n        self.assetmodel = self.create_asset_model()\n\n        self.shot_browser.set_model(self.shotmodel)\n        self.asset_browser.set_model(self.assetmodel)\n\n        self.shot_vbox.addWidget(self.shot_browser)\n        self.asset_vbox.addWidget(self.asset_browser)"}
{"prompt": "def normalize(expr):\n    \"\"\"No elimination, but normalize arguments.\"\"\"", "output": "\n    return expr\n\ndef is_number(expr):\n    \"\"\"Is x a number?\"\"\"\n    return isinstance(expr, (int, long))\n\ndef is_sequence(expr):\n    \"\"\"Is x a sequence?\"\"\"\n    return isinstance(expr, (list, tuple))\n\ndef is_set(expr):\n    \"\"\"Is x a set?\"\"\"\n    return isinstance(expr, (set, frozenset))\n\ndef is_dict_like(expr):\n    \"\"\"Is x a dict-like object?\"\"\"\n    try:\n        _ = frozenset(expr)\n        return True\n    except TypeError:\n        return False\n\ndef", "solution": "\n    args = [normalize(arg) for arg in expr.args]\n\n    return type(expr)(expr.func, *args, start=expr.start, end=expr.end)"}
{"prompt": "def add_types(graph, phenotypes):  # TODO missing expression phenotypes! also basket type somehow :(\n    \"\"\" Add disjoint union classes so that it is possible to see the invariants\n        associated with individual phenotypes \"\"\"", "output": "\n    for phenotype in phenotypes:\n        if phenotype not in graph:\n            graph[phenotype] = {}\n        if phenotype not in graph[phenotype]:\n            graph[phenotype][\n                'disjoint_union_class'] = {}\n        if phenotype not in graph[phenotype][\n                'disjoint_union_class']:\n            graph[phenotype][\n                'disjoint_union_class'][\n                    phenotype] = {}\n        if phenotype not in graph[phenotype][\n                'individual_classes']:\n            graph[phenotype][\n                'individual_classes", "solution": "\n\n    collect = defaultdict(set)\n    def recurse(id_, start, level=0):\n        #print(level)\n        for t in graph.g.triples((None, None, id_)):\n            if level == 0:\n                if t[1] != rdflib.term.URIRef('http://www.w3.org/2002/07/owl#someValuesFrom'):\n                    continue\n            if type_check(t, (rdflib.term.URIRef, rdflib.term.URIRef, rdflib.term.BNode)):\n                #print(start, t[0])\n                collect[start].add(t[0])\n                return  #  we're done here, otherwise we hit instantiated subclasses\n            if level > 1:\n                if t[1] == rdflib.URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#first') or \\\n                   t[1] == rdflib.URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#rest'):\n                    continue\n\n            recurse(t[0], start, level + 1)\n\n\n    for phenotype in phenotypes:\n        recurse(phenotype, phenotype)\n\n    return collect"}
{"prompt": "def mongoengine_validate_wrapper(old_clean, new_clean):\n    \"\"\"\n    A wrapper function to validate formdata against mongoengine-field\n    validator and raise a proper django.forms ValidationError if there\n    are any problems.\n    \"\"\"", "output": "\n    def _check_for_duplicates(form_data, field):\n        \"\"\"\n        Check for duplicate values.\n        \"\"\"\n        # Check for duplicate values.\n        if form_data[field] in (None, ''):\n            return True\n\n        # Check for duplicate values.\n        for key in form_data:\n            if key not in ('id', 'name', 'email', 'password', 'confirm_password'):\n                if form_data[key] == '':\n                    msg = _('This field is required.')\n                    self._errors['name'] = self.error_class([msg])\n                if not form_data[key", "solution": "\n\n    def inner_validate(value):\n        value = old_clean(value)\n        try:\n            new_clean(value)\n            return value\n        except ValidationError, e:\n            raise forms.ValidationError(e)\n    return inner_validate"}
{"prompt": "def _float_check(self, attribute_array, value, irow, key):\n        \"\"\"Checks if value is valid float, appends to array if valid, appends\n        nan if not\"\"\"", "output": "\n        try:\n            float(value)\n        except ValueError:\n            return\n\n        if not math.isfinite(value):\n            return\n\n        if not irow:\n            return\n\n        if not key:\n            return\n\n        if not isinstance(irow, list):\n            return\n\n        for i in irow:\n            try:\n                float(i)\n            except ValueError:\n                return\n\n        return\n\n    def _validate_float_range(self, attribute_array, value, irow, key):\n        \"\"\"Checks if value is valid float, appends to array if valid, appends\n        nan if not\"\"\"\n        try:\n            float(value)", "solution": "\n        value = value.strip(' ')\n        try:\n            if value:\n                attribute_array = np.hstack([attribute_array, float(value)])\n            else:\n                attribute_array = np.hstack([attribute_array, np.nan])\n        except:\n            print(irow, key)\n            msg = 'Input file format error at line: %d' % (irow + 2)\n            msg += ' key: %s' % (key)\n            raise ValueError(msg)\n        return attribute_array"}
{"prompt": "def add_arguments(parser, doc_str, add_base_url=True):\n    \"\"\"Add standard arguments for DataONE utilities to a command line parser.\"\"\"", "output": "\n    parser.add_argument(\n        '--api-version',\n        action='version',\n        version='v1',\n        help='API version to use.',\n        version_string=__version__)\n    parser.add_argument(\n        '--api-url',\n        action='store',\n        dest='api_url',\n        help='API URL to use.',\n        default=None)\n    parser.add_argument(\n        '--api-key',\n        action='store',\n        dest='api_key',\n        help='API key to use.',\n        default=None)\n    parser.add_argument(\n        '--api-secret',", "solution": "\n    parser.description = doc_str\n    parser.formatter_class = argparse.RawDescriptionHelpFormatter\n    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Debug level logging\")\n    parser.add_argument(\n        \"--cert-pub\",\n        dest=\"cert_pem_path\",\n        action=\"store\",\n        default=django.conf.settings.CLIENT_CERT_PATH,\n        help=\"Path to PEM formatted public key of certificate\",\n    )\n    parser.add_argument(\n        \"--cert-key\",\n        dest=\"cert_key_path\",\n        action=\"store\",\n        default=django.conf.settings.CLIENT_CERT_PRIVATE_KEY_PATH,\n        help=\"Path to PEM formatted private key of certificate\",\n    )\n    parser.add_argument(\n        \"--public\", action=\"store_true\", help=\"Do not use certificate even if available\"\n    )\n    parser.add_argument(\n        \"--disable-server-cert-validation\",\n        action=\"store_true\",\n        help=\"Do not validate the TLS/SSL server side certificate of the source node (insecure)\",\n    )\n    parser.add_argument(\n        \"--timeout\",\n        type=float,\n        action=\"store\",\n        default=DEFAULT_TIMEOUT_SEC,\n        help=\"Timeout for DataONE API calls to the source MN\",\n    )\n    parser.add_argument(\n        \"--retries\",\n        type=int,\n        action=\"store\",\n        default=DEFAULT_RETRY_COUNT,\n        help=\"Retry DataONE API calls that raise HTTP level exceptions\",\n    )\n    parser.add_argument(\n        \"--page-size\",\n        type=int,\n        action=\"store\",\n        default=DEFAULT_PAGE_SIZE,\n        help=\"Number of objects to retrieve in each list method API call to source MN\",\n    )\n    parser.add_argument(\n        \"--major\",\n        type=int,\n        action=\"store\",\n        help=\"Skip automatic detection of API major version and use the provided version\",\n    )\n    parser.add_argument(\n        \"--max-concurrent\",\n        type=int,\n        action=\"store\",\n        default=DEFAULT_MAX_CONCURRENT_TASK_COUNT,\n        help=\"Max number of concurrent DataONE API\",\n    )\n    if not add_base_url:\n        parser.add_argument(\n            \"--baseurl\",\n            action=\"store\",\n            default=django.conf.settings.DATAONE_ROOT,\n            help=\"Remote MN or CN BaseURL\",\n        )\n    else:\n        parser.add_argument(\"baseurl\", help=\"Remote MN or CN BaseURL\")"}
{"prompt": "def __set_labels(self, labels):\n        \"\"\"\n        Add a label on the document.\n        \"\"\"", "output": "\n        if self.labels is not None:\n            self.labels.append(label)\n\n    def __get_labels(self):\n        \"\"\"\n        Return the list of labels on this document.\n        \"\"\"\n        return self.labels\n\n    def __set_title(self, title):\n        \"\"\"\n        Add a title on the document.\n        \"\"\"\n        if self.title is not None:\n            self.title.append(title)\n\n    def __get_title(self):\n        \"\"\"\n        Return the list of titles on this document.\n        \"\"\"\n        return self.title\n\n    def __set_title_and_ids(self,", "solution": "\n        with self.fs.open(self.fs.join(self.path, self.LABEL_FILE), 'w') \\\n                as file_desc:\n            for label in labels:\n                file_desc.write(\"%s,%s\\n\" % (label.name,\n                                             label.get_color_str()))"}
{"prompt": "def answer(part, module='mlai2014.json'):\n    \"\"\"Returns the answers to the lab classes.\"\"\"", "output": "\n    return json.loads(part.get('answers', '[]'))\n\n\ndef get_lab_class(module='mlai2014.json'):\n    \"\"\"Returns the lab class.\"\"\"\n    return json.loads(json.dumps(json.loads(get_lab_answers(module))))\n\n\ndef get_lab_answers(module='mlai2014.json'):\n    \"\"\"Returns the answers to the lab classes.\"\"\"\n    return json.loads(part.get('answers', '[]'))\n\n\ndef get_lab_class_by_id(id):\n    \"\"\"Returns the lab class by ID.\"\"\"\n   ", "solution": "\n    marks = json.load(open(os.path.join(data_directory, module), 'rb'))\n    return marks['Lab '  + str(part+1)]"}
{"prompt": "def output_ip(gandi, ip, datacenters, vms, ifaces, output_keys, justify=11):\n    \"\"\" Helper to output an ip information.\"\"\"", "output": "\n    ifaces = [i.strip() for i in ifaces.split(',')]\n    if len(ifaces) == 0:\n        return\n    ifaces = [i.strip() for i in ifaces.split(',')]\n    if len(ifaces) == 1:\n        return ifaces[0]\n    ifaces = [i.strip() for i in ifaces.split(',')]\n    if len(ifaces) == 2:\n        return ifaces[0] + '.' + ifaces[1] + '.' + ifaces[2]\n    if len(i", "solution": "\n    output_generic(gandi, ip, output_keys, justify)\n\n    if 'type' in output_keys:\n        iface = ifaces.get(ip['iface_id'])\n        type_ = 'private' if iface.get('vlan') else 'public'\n        output_line(gandi, 'type', type_, justify)\n        if type_ == 'private':\n            output_line(gandi, 'vlan', iface['vlan']['name'], justify)\n\n    if 'vm' in output_keys:\n        iface = ifaces.get(ip['iface_id'])\n        vm_id = iface.get('vm_id')\n        if vm_id:\n            vm_name = vms.get(vm_id, {}).get('hostname')\n            if vm_name:\n                output_line(gandi, 'vm', vm_name, justify)\n\n    if 'dc' in output_keys:\n        for dc in datacenters:\n            if dc['id'] == ip.get('datacenter_id',\n                                  ip.get('datacenter', {}).get('id')):\n                dc_name = dc.get('dc_code', dc.get('iso', ''))\n                break\n\n        output_line(gandi, 'datacenter', dc_name, justify)"}
{"prompt": "def set_density_matrix(self, density_matrix_repr: Union[int, np.ndarray]):\n        \"\"\"Set the density matrix to a new density matrix.\n\n        Args:\n            density_matrix_repr: If this is an int, the density matrix is set to\n            the computational basis state corresponding to this state. Otherwise\n            if this is a np.ndarray it is the full state, either a pure state\n            or the full density matrix.  If it is the pure state it must be the\n            correct size, be normalized (an L2 norm of 1), and be safely\n            castable to an appropriate dtype for the simulator.  If it is a\n            mixed state it must be correctly sized and positive semidefinite\n            with trace one.\n        \"\"\"", "output": " type.\n        \"\"\"\n        if isinstance(density_matrix_repr, int):\n            density_matrix_repr = np.array([density_matrix_repr])\n        if isinstance(density_matrix_repr, np.ndarray):\n            density_matrix_repr = np.array([1 if x == 0 else 0 for x in density_matrix_repr])\n        self.density_matrix = density_matrix_repr\n\n    def set_density_matrix_repr(self, density_matrix_repr: Union[int, np.ndarray]):\n        \"\"\"Set the density matrix to a new density matrix.\n\n        Args:\n           ", "solution": "\n        density_matrix = density_matrix_utils.to_valid_density_matrix(\n            density_matrix_repr, len(self._qubit_map), self._dtype)\n        density_matrix = np.reshape(density_matrix,\n                                    self.simulator_state().density_matrix.shape)\n        np.copyto(dst=self.simulator_state().density_matrix, src=density_matrix)"}
{"prompt": "def _set_interface_priority(self, v, load=False):\n    \"\"\"\n    Setter method for interface_priority, mapped from YANG variable /routing_system/interface/ve/intf_isis/interface_isis/interface_priority (container)\n    If this variable is read-only (config: false) in the\n    source YANG file, then _set_interface_priority is considered as a private\n    method. Backends looking to populate this variable should\n    do so via calling thisObj._set_interface_priority() directly.\n    \"\"\"", "output": "\n    def __set_interface_priority(self, v, load=False):\n        \"\"\"\n        Setter method for interface_priority, mapped from YANG variable /routing_system/interface/ve/intf_isis/interface_isis/interface_priority (container)\n        If this variable is read-only (config: false) in the\n        source YANG file, then _set_interface_priority is considered as a private\n        method. Backends looking to populate this variable should\n        do so via calling thisObj._set_interface_priority() directly.\n        \"\"\"\n        if hasattr(v, \"_utype\"):\n            v", "solution": "\n    if hasattr(v, \"_utype\"):\n      v = v._utype(v)\n    try:\n      t = YANGDynClass(v,base=interface_priority.interface_priority, is_container='container', presence=False, yang_name=\"interface-priority\", rest_name=\"priority\", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'info': u'Router priority for ISIS', u'alt-name': u'priority', u'cli-incomplete-no': None, u'cli-incomplete-command': None}}, namespace='urn:brocade.com:mgmt:brocade-isis', defining_module='brocade-isis', yang_type='container', is_config=True)\n    except (TypeError, ValueError):\n      raise ValueError({\n          'error-string': "}
{"prompt": "def public_data_download_cli(source, username, directory, max_size, quiet,\n                             debug):\n    \"\"\"\n    Command line tools for downloading public data.\n    \"\"\"", "output": "\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    if not quiet:\n        print('Downloading public data from %s' % source)\n\n    if not os.path.exists(source):\n        raise ValueError('Source directory does not exist: %s' % source)\n\n    if not quiet:\n        print('Downloading public data from %s' % directory)\n\n    if not os.path.exists(source):\n        raise ValueError('Source directory does not exist: %s' % source)\n\n    if not quiet:\n        print('Downloading public data from %s' % source)\n\n    if", "solution": "\n    return public_download(source, username, directory, max_size, quiet, debug)"}
{"prompt": "def start(self):\n        \"\"\"\n        Start all the processes\n        \"\"\"", "output": "\n        self.process_list = []\n        self.process_list.append(self.process_handler)\n        self.process_list.append(self.process_handler)\n        self.process_list.append(self.process_handler)\n        self.process_handler.start()\n        self.process_list.append(self.process_handler)\n\n    def stop(self):\n        \"\"\"\n        Stop all the processes\n        \"\"\"\n        self.process_handler.stop()\n        self.process_handler.join()\n        for process in self.process_list:\n            process.join()\n\n    def join", "solution": "\n        Global.LOGGER.info(\"starting the flow manager\")\n        self._start_actions()\n        self._start_message_fetcher()\n        Global.LOGGER.debug(\"flow manager started\")"}
{"prompt": "def _start_zmq_monitor(self):\n        \"\"\"\n        Starts ZMQ monitor for debugging purposes.\n        :return:\n        \"\"\"", "output": "\n        self.log.info(\"Starting ZMQ Monitor\")\n        self.log.info(\"Using %s as ZMQ context\" % self.config.get('zmq', 'debug'))\n        self.zmq_context = zmq.Context()\n        self.zmq_context.linger = 0\n        self.zmq_context.socket(zmq.PUB)\n        self.zmq_monitor = gevent.spawn(self.zmq_context.socket, 'inproc://zeromq.ipc', log=self.log)\n\n    def _stop_zmq_context(self):\n        \"\"\"\n        Stops ZMQ context", "solution": "\n        # Socket monitor shall be used the only for debug\n        # purposes so using threading doesn't look too bad here\n\n        if HAS_ZMQ_MONITOR and self.opts['zmq_monitor']:\n            log.debug('Starting ZMQ monitor')\n            import threading\n            self._w_monitor = ZeroMQSocketMonitor(self._socket)\n            threading.Thread(target=self._w_monitor.start_poll).start()\n            log.debug('ZMQ monitor has been started started')"}
{"prompt": "def get_policy(self):\n        \"\"\"\n        Returns an instance of :attr:`~policy_class`.\n\n        :return: An instance of the current policy class.\n        :rtype: dockermap.map.policy.base.BasePolicy\n        \"\"\"", "output": "\n        return self._policy\n\n    def get_docker_host(self):\n        \"\"\"\n        Returns an instance of :attr:`~docker_host`.\n\n        :return: An instance of :attr:`~docker_host`.\n        :rtype: dockermap.map.host.DockerHost\n        \"\"\"\n        return self._docker_host\n\n    def get_docker_port(self):\n        \"\"\"\n        Returns an instance of :attr:`~docker_port`.\n\n        :return: An instance of :attr:`~docker_port`.\n        :rtype: int\n        \"\"\"\n        return self._docker_port\n\n    def get_docker_host_config(self", "solution": "\n        if not self._policy:\n            self._policy = self.policy_class(self._maps, self._clients)\n        return self._policy"}
{"prompt": "def get_posts(self, count=10, offset=0, recent=True,  tag=None,\n                  user_id=None, include_draft=False):\n        \"\"\"\n        Get posts given by filter criteria\n\n        :param count: The number of posts to retrieve (default 10). If count\n         is ``None``, all posts are returned.\n        :type count: int\n        :param offset: The number of posts to offset (default 0)\n        :type offset: int\n        :param recent: Order by recent posts or not\n        :type recent: bool\n        :param tag: Filter by a specific tag\n        :type tag: str\n        :param user_id: Filter by a specific user\n        :type user_id: str\n        :param include_draft: Whether to include posts marked as draft or not\n        :type include_draft: bool\n\n        :return: A list of posts, with each element a dict containing values\n         for the following keys: (title, text, draft, post_date,\n         last_modified_date). If count is ``None``, then all the posts are\n         returned.\n        \"\"\"", "output": " :type tag: str\n        :param user_id: Filter by a specific user id\n        :type user_id: str\n        :returns: Generator of :class:`Post` objects.\n        \"\"\"\n        if tag is None:\n            for i in range(count):\n                yield self.get_post(i, offset, recent,  user_id)\n        else:\n            for i in range(count):\n                yield self.get_tag(i, tag)\n\n    def get_post(self, post_id):\n        \"\"\"\n        Get a single post\n\n        :param post_id: The id of the post", "solution": "\n        raise NotImplementedError(\"This method needs to be implemented by the \"\n                                  \"inheriting class\")"}
{"prompt": "def import_element(self, xml_element):\n        \"\"\"\n        Imports the element from an lxml element and loads its content.\n\n        :param lxml.etree._Element xml_element: XML element to import.\n        \"\"\"", "output": "\n        self.xml_root = xml_element\n        self.xml_element = xml_element.text\n        self.import_list = self.xml_root.xpath('//*[@id=\"import\"]')\n        self.import_element = self.import_list[0]\n        self.import_list = self.xml_root.xpath('//*[@id=\"import\"]')\n        self.import_element.set('class', 'import-element')\n        self.import_element.set('data-oe-id', xml_element.get('id'))\n        self.import_list.set", "solution": "\n\n        self.xml_element = xml_element\n\n        uri, tag = Element.get_namespace_and_tag(self.xml_element.tag)\n        self.namespace = uri\n        self.name = tag\n\n        self.load_xml_attrs()\n\n        if self.xml_element.text is None:\n            self.text = ''\n        else:\n            self.text = self.xml_element.text"}
{"prompt": "def print_table(lines, separate_head=True):\n\t\"\"\"Prints a formatted table given a 2 dimensional array\"\"\"", "output": "\n\tfor line in lines:\n\t\tprint line\n\n\tif separate_head:\n\t\tprint '\\n'\n\n\tfor i in range(len(lines)):\n\t\tfor j in range(i):\n\t\t\tprint '\\t'.join(lines[i][j:i+1])\n\n\tif separate_head:\n\t\tprint '\\n'\n\n\tfor i in range(len(lines)):\n\t\tfor j in range(i):\n\t\t\tprint '\\t'.join(lines[i][j:i+1])\n\n\tif separate_head:\n\t\tprint '\\n'\n\n\t", "solution": "\n\t#Count the column width\n\twidths = []\n\tfor line in lines:\n\t\t\tfor i,size in enumerate([len(x) for x in line]):\n\t\t\t\t\twhile i >= len(widths):\n\t\t\t\t\t\t\twidths.append(0)\n\t\t\t\t\tif size > widths[i]:\n\t\t\t\t\t\t\twidths[i] = size\n\t   \n\t#Generate the format string to pad the columns\n\tprint_string = \"\"\n\tfor i,width in enumerate(widths):\n\t\t\tprint_string += \"{\" + str(i) + \":\" + str(width) + \"} | \"\n\tif (len(print_string) == 0):\n\t\t\treturn\n\tprint_string = print_string[:-3]\n\t   \n\t#Print the actual data\n\tfor i,line in enumerate(lines):\n\t\t\tprint(print_string.format(*line))\n\t\t\tif (i == 0 and separate_head):\n\t\t\t\t\tprint(\"-\"*(sum(widths)+3*(len(widths)-1)))"}
{"prompt": "def _decompress(self, fp):\n        \"\"\"\n        Internal function for decompressing a backup file with the DEFLATE algorithm\n\n        :rtype: Proxy\n        \"\"\"", "output": "\n        raise NotImplementedError()\n\n    def _open_backup(self, fp):\n        \"\"\"\n        Internal function for opening a backup file\n\n        :rtype: Proxy\n        \"\"\"\n        raise NotImplementedError()\n\n    def _close_backup(self, backup):\n        \"\"\"\n        Internal function for closing a backup file\n\n        :param backup: the backup file to close\n        \"\"\"\n        raise NotImplementedError()\n\n    def _get_backup_file_list(self, backup):\n        \"\"\"\n        Internal function for getting a list of backups\n\n        :rtype: list\n        \"\"\"\n        raise NotImplementedError()\n\n    def _get_backup_files(self, backup):\n        \"\"\"\n        Internal", "solution": "\n        decompressor = zlib.decompressobj()\n        if self.stream:\n            return Proxy(decompressor.decompress, fp)\n        else:\n            out = io.BytesIO(decompressor.decompress(fp.read()))\n            out.write(decompressor.flush())\n            out.seek(0)\n            return out"}
{"prompt": "def change_event_params(self, handler, **kwargs):\n        \"\"\"\n        This allows the client to change the parameters for an event, in the case that there is a desire for slightly different behavior, such as reasigning keys.\n\n        handler - the handler object that the desired changes are made to.\n\n        kwargs - the variable number of keyword arguments for the parameters that must match the properties of the corresponding event.\n            \"\"\"", "output": "\n        if handler.name == 'on_change':\n            self.on_change(kwargs['value'])\n        elif handler.name == 'on_change_event':\n            self.on_change_event(kwargs['value'])\n        elif handler.name == 'on_close':\n            self.on_close(kwargs['event'])\n        elif handler.name == 'on_close_event':\n            self.on_close_event(kwargs['event'])\n        elif handler.name == 'on_close_event':\n            self.on_close_event(kwargs['event'])\n        elif handler.name ==", "solution": "\n        if not isinstance(handler, Handler):\n            raise TypeError(\"given object must be of type Handler.\")\n        if not self.remove_handler(handler):\n            raise ValueError(\"You must pass in a valid handler that already exists.\")\n\n        self.add_handler(handler.type, handler.actions, **kwargs)\n        self.event = handler.event"}
{"prompt": "def vector_sum(vector1, vector2, coeff=1.0):\n    \"\"\" Sums the vectors.\n\n    This function computes the result of the vector operation :math:`\\\\overline{v}_{1} + c * \\\\overline{v}_{2}`, where\n    :math:`\\\\overline{v}_{1}` is ``vector1``, :math:`\\\\overline{v}_{2}`  is ``vector2`` and :math:`c` is ``coeff``.\n\n    :param vector1: vector 1\n    :type vector1: list, tuple\n    :param vector2: vector 2\n    :type vector2: list, tuple\n    :param coeff: multiplier for vector 2\n    :type coeff: float\n    :return: updated vector\n    :rtype: list\n    \"\"\"", "output": " :type vector2: list, tuple\n    :return: sum of the vectors\n    :rtype: float\n    \"\"\"\n    if len(vector1)!= len(vector2):\n        raise ValueError(\"vector_sum: length of vectors must be equal to length of vector1\")\n    if len(vector1) == 0:\n        return 0.0\n    if len(vector2) == 0:\n        return 0.0\n    if len(vector1) == 0:\n        return 0.0\n    if len(vector2) == 0:\n        return 0.0\n    if len(vector1) == 0:\n        return", "solution": "\n    summed_vector = [v1 + (coeff * v2) for v1, v2 in zip(vector1, vector2)]\n    return summed_vector"}
{"prompt": "def create(self):\n        \"\"\"\n        Create an instance of the Time Series Service with the typical\n        starting settings.\n        \"\"\"", "output": "\n        if self.is_time_series_service_enabled():\n            self.client.system.time_series_service.create(self.resource_group, self.name, self.location, self.sku, self.location_type, self.tags, self.sku_name, self.location_name, self.tags)\n        else:\n            raise Exception(f\"Time Series Service {self.name} is not enabled.\")\n\n    def is_time_series_service_enabled(self):\n        \"\"\"\n        Check if the Time Series service is enabled.\n        \"\"\"\n        return self.client.", "solution": "\n        self.service.create()\n\n        predix.config.set_env_value(self.use_class, 'ingest_uri',\n                self.get_ingest_uri())\n        predix.config.set_env_value(self.use_class, 'ingest_zone_id',\n                self.get_ingest_zone_id())\n\n        predix.config.set_env_value(self.use_class, 'query_uri',\n                self.get_query_uri())\n        predix.config.set_env_value(self.use_class, 'query_zone_id',\n                self.get_query_zone_id())"}
{"prompt": "def as_span(cls, lower_version=None, upper_version=None,\n                lower_inclusive=True, upper_inclusive=True):\n        \"\"\"Create a range from lower_version..upper_version.\n\n        Args:\n            lower_version: Version object representing lower bound of the range.\n            upper_version: Version object representing upper bound of the range.\n\n        Returns:\n            `VersionRange` object.\n        \"\"\"", "output": "\n        if lower_version is None:\n            lower_version = cls.DEFAULT_VERSION\n        if upper_version is None:\n            upper_version = cls.DEFAULT_VERSION\n        if lower_inclusive:\n            lower_version = VersionRange(lower_version.lower_bound,\n                lower_version.upper_bound, upper_version.lower_bound)\n        if upper_inclusive:\n            lower_version = VersionRange(upper_version.lower_bound,\n                lower_version.upper_bound, lower_version.lower_bound)\n        return cls(lower_version, upper_version)\n\n    def", "solution": "\n        lower = (None if lower_version is None\n                 else _LowerBound(lower_version, lower_inclusive))\n        upper = (None if upper_version is None\n                 else _UpperBound(upper_version, upper_inclusive))\n        bound = _Bound(lower, upper)\n\n        range = cls(None)\n        range.bounds = [bound]\n        return range"}
{"prompt": "def get_local_file_dist(self):\n        \"\"\"\n        Handle importing from a source archive; this also uses setup_requires\n        but points easy_install directly to the source archive.\n        \"\"\"", "output": "\n        # Create a source distribution object until we have enough data to\n        # install it (ie. we have one, even though we don't, and the data\n        # directory itself, and one without.\n        from rez.package_maker import get_distribution\n        dist = get_distribution('rez')\n        for dist_name in dist.get_entry_map('console_scripts').keys():\n            dist_path = os.path.join(dist.location, dist_name)\n            if not os.path.isdir(dist_path):\n                os.makedirs(dist_path)\n            dist_", "solution": "\n\n        if not os.path.isfile(self.path):\n            return\n\n        log.info('Attempting to unpack and import astropy_helpers from '\n                 '{0!r}'.format(self.path))\n\n        try:\n            dist = self._do_download(find_links=[self.path])\n        except Exception as e:\n            if DEBUG:\n                raise\n\n            log.warn(\n                'Failed to import {0} from the specified archive {1!r}: '\n                '{2}'.format(PACKAGE_NAME, self.path, str(e)))\n            dist = None\n\n        if dist is not None and self.auto_upgrade:\n            # A version of astropy-helpers was found on the available path, but\n            # check to see if a bugfix release is available on PyPI\n            upgrade = self._do_upgrade(dist)\n            if upgrade is not None:\n                dist = upgrade\n\n        return dist"}
{"prompt": "def registerSave(self, obj) :\n\t\t\"\"\"Each object can only be save donce during a session, returns False if the object has already been saved. True otherwise\"\"\"", "output": "\n\t\tif self.save_lock :\n\t\t\tself.save_lock.acquire()\n\t\t\ttry :\n\t\t\t\tif self.save_lock.locked() :\n\t\t\t\t\treturn False\n\t\t\tfinally :\n\t\t\t\tself.save_lock.release()\n\t\t\treturn True\n\t\telse :\n\t\t\treturn False\n\n\tdef registerLoad(self, obj) :\n\t\t\"\"\"Each object can only be load donce during a session, returns False if the object has already been loaded. True otherwise\"\"\"\n\t\tif self.load_lock :\n\t\t\tself.load_lock.acquire()\n\t\t\ttry :\n\t\t\t", "solution": "\n\t\tif obj._runtimeId in self.savedObject :\n\t\t\treturn False\n\n\t\tself.savedObject.add(obj._runtimeId)\n\t\treturn True"}
{"prompt": "def string_or_bool(value):\n    \"\"\"\n    Ritorna True o False in caso venga passata la stringa 'true' o 'false'\n    (o 't' o 'f') altrimenti ritorna una stringa.\n\n    :param value: Stringa da analizzare.\n    :type value: str\n    \"\"\"", "output": "\n    if isinstance(value, str):\n        if value.lower() == \"true\":\n            return True\n        elif value.lower() == \"false\":\n            return False\n        else:\n            raise ValueError(\"Falha deber\u00eda ser un stringo: %s\" % value)\n    elif isinstance(value, bool):\n        if value:\n            return True\n        else:\n            raise ValueError(\"Falha deber\u00eda ser un booleano: %s\" % value)\n    else:\n        raise ValueError(\"Falha deber\u00eda ser un stringo: %s\" % value)\n\n\ndef string", "solution": "\n    if value.lower() in ['t', 'true']:\n        value = True\n    elif value.lower() in ['f', 'false']:\n        value = False\n    elif str.isdigit(str(value)):\n        value = int(value) != 0\n    else:\n        value = str(value) # pylint: disable=redefined-variable-type\n    return value"}
{"prompt": "def apply(self, doc):\n        \"\"\"\n        Generate MentionParagraphs from a Document by parsing all of its Paragraphs.\n\n        :param doc: The ``Document`` to parse.\n        :type doc: ``Document``\n        :raises TypeError: If the input doc is not of type ``Document``.\n        \"\"\"", "output": "\n        if not isinstance(doc, Paragraph):\n            raise TypeError('Input must be of type Paragraph.')\n\n        for i, paragraph in enumerate(doc.paras):\n            if not isinstance(paragraph, Paragraph):\n                raise TypeError('Input must be of type Paragraph.')\n\n            for j, field in enumerate(paragraph):\n                if field.tag == 'p':\n                    self.add_paragraph(field, i, j, paragraph.paras[j + 1])\n                elif field.tag == 'r':\n                    self.add_paragraph(field, i, j, paragraph.paras[j + 1", "solution": "\n        if not isinstance(doc, Document):\n            raise TypeError(\n                \"Input Contexts to MentionParagraphs.apply() must be of type Document\"\n            )\n\n        for paragraph in doc.paragraphs:\n            yield TemporaryParagraphMention(paragraph)"}
{"prompt": "def authorize_client_credentials(\n        self, client_id, client_secret=None, scope=\"private_agent\"\n    ):\n        \"\"\"Authorize to platform with client credentials\n\n        This should be used if you posses client_id/client_secret pair\n        generated by platform.\n        \"\"\"", "output": "\n        self.client_id = client_id\n        self.client_secret = client_secret\n        self.scope = scope\n        self.client_secret_expires = None\n        self.client_secret_expires_in = None\n        self.client_secret_expires_at = None\n        if client_id and not client_secret:\n            raise ValueError(\"You must provide client_secret_key to authorize.\")\n        elif client_id and client_secret:\n            raise ValueError(\"You must provide client_id and client_secret_key to authorize.\")\n        elif client_id and not client_secret:\n            raise", "solution": "\n        self.auth_data = {\n            \"grant_type\": \"client_credentials\",\n            \"scope\": [ scope ],\n            \"client_id\": client_id,\n            \"client_secret\": client_secret\n        }\n\n        self._do_authorize()"}
{"prompt": "def BatchConvert(self, metadata_value_pairs, token=None):\n    \"\"\"Convert batch of FileStoreHashs.\"\"\"", "output": "\n    return [self._ConvertHash(metadata_value_pair) for metadata_value_pair in metadata_value_pairs]\n\n  def _ConvertHash(self, metadata_value_pair):\n    \"\"\"Converts a FileStoreHash entry to a list of FileInfo objects.\"\"\"\n    return [self._FileInfo(metadata_value) for metadata_value in metadata_value_pair.values]\n\n  def _FileInfo(self, metadata_value):\n    \"\"\"Creates FileInfo proto from a FileStoreHash entry.\n\n    Args:\n      metadata_value: A FileInfo proto to be used for the new instance.\n\n   ", "solution": "\n\n    urns = [urn for metadata, urn in metadata_value_pairs]\n    urns_dict = dict((urn, metadata) for metadata, urn in metadata_value_pairs)\n\n    results = []\n    for hash_urn, client_files in filestore.HashFileStore.GetClientsForHashes(\n        urns, token=token):\n      for hit in client_files:\n        metadata = ExportedMetadata(urns_dict[hash_urn])\n        metadata.client_urn = rdfvalue.RDFURN(hit).Split(2)[0]\n\n        result = ExportedFileStoreHash(\n            metadata=metadata,\n            hash=hash_urn.hash_value,\n            fingerprint_type=hash_urn.fingerprint_type,\n            hash_type=hash_urn.hash_type,\n            target_urn=hit)\n        results.append(result)\n\n    return results"}
{"prompt": "def strace_configure(self, port_width):\n        \"\"\"Configures the trace port width for tracing.\n\n        Note that configuration cannot occur while STRACE is running.\n\n        Args:\n          self (JLink): the ``JLink`` instance\n          port_width (int): the trace port width to use.\n\n        Returns:\n          ``None``\n\n        Raises:\n          ValueError: if ``port_width`` is not ``1``, ``2``, or ``4``.\n          JLinkException: on error.\n        \"\"\"", "output": "\n        if port_width == 1:\n            raise ValueError('trace port 1 is not supported.')\n        elif port_width == 2:\n            raise ValueError('trace port 2 is not supported.')\n        elif port_width == 4:\n            raise ValueError('trace port 4 is not supported.')\n        else:\n            raise ValueError('trace port width must be 1 or 2 or 4.')\n\n    def strace_attach(self, port_width):\n        \"\"\"Attaches a trace port to the current thread.\n\n        Args:\n          self (JLink): the ``JLink`` instance\n          port_width (int): the trace port to attach.", "solution": "\n        if port_width not in [1, 2, 4]:\n            raise ValueError('Invalid port width: %s' % str(port_width))\n\n        config_string = 'PortWidth=%d' % port_width\n        res = self._dll.JLINK_STRACE_Config(config_string.encode())\n        if res < 0:\n            raise errors.JLinkException('Failed to configure STRACE port')\n\n        return None"}
{"prompt": "def with_slots(cls):\n    \"\"\"\n    Decorator for a class with _slots_. It automatically defines\n    the methods __eq__, __ne__, assert_equal.\n    \"\"\"", "output": "\n    def __init__(self, *args, **kwargs):\n        super(WithSlots, cls).__init__(*args, **kwargs)\n        self.assertEqual = cls.assertEqual\n        self.assertNotEqual = cls.assertNotEqual\n        self.assertNotEquals = cls.assertNotEquals\n        self.assertFalse = cls.assertFalse\n\n    def __call__(self, f):\n        @wraps(f)\n        def wrapper(*args, **kwargs):\n            with self.subTest(f.__name__):\n                return f(*args, **kwargs)\n        return wrapper\n\n\n@override_settings(ROOT_URLCONF='test_client_regress.urls')", "solution": "\n    def _compare(self, other):\n        for slot in self.__class__._slots_:\n            attr = operator.attrgetter(slot)\n            source = attr(self)\n            target = attr(other)\n            if isinstance(source, numpy.ndarray):\n                eq = numpy.array_equal(source, target)\n            elif hasattr(source, '_slots_'):\n                source.assert_equal(target)\n                eq = True\n            else:\n                eq = source == target\n            yield slot, source, target, eq\n\n    def __eq__(self, other):\n        return all(eq for slot, source, target, eq in _compare(self, other))\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def assert_equal(self, other, ignore=()):\n        for slot, source, target, eq in _compare(self, other):\n            if not eq and slot not in ignore:\n                raise AssertionError('slot %s: %s is different from %s' %\n                                     (slot, source, target))\n\n    cls._slots_  # raise an AttributeError for missing slots\n    cls.__eq__ = __eq__\n    cls.__ne__ = __ne__\n    cls.assert_equal = assert_equal\n    return cls"}
{"prompt": "def _get_simple(self, name):\n        \"\"\"\n        Query the stack for a non-dotted name.\n\n        \"\"\"", "output": "\n        parts = name.split('.')\n        if len(parts) > 1:\n            raise ValueError(\"Can't use * as a wildcard for the stack\")\n        obj = self.get_object(parts[0])\n        if not obj:\n            raise ValueError(\"No object found for %s\" % name)\n        if len(parts) == 1:\n            return self.get_object(obj[0])\n        return obj\n\n    def _get_dotted(self, name):\n        \"\"\"\n        Query the stack for a dotted name.\n\n        \"\"\"\n        parts = name.split('.')\n        if len(parts) > 1:\n           ", "solution": "\n        for item in reversed(self._stack):\n            result = _get_value(item, name)\n            if result is not _NOT_FOUND:\n                return result\n\n        raise KeyNotFoundError(name, \"part missing\")"}
{"prompt": "def get_class(mcs):\n        \"\"\" Generates new class to gether logic of all available extensions\n            ::\n\n                mc = ExtensibleType._(\"MyClass\")\n                @six.add_metaclass(mc)\n                class MyClassBase(object):\n                    pass\n\n                # get class with all extensions enabled\n                MyClass = mc.get_class()\n\n        \"\"\"", "output": "\n        class_name = mcs.get_name()\n        if class_name is None:\n            return None\n        return type(class_name, (object,), {\n            '__doc__': self.docstring,\n            'name': class_name,\n            'docstring': self.docstring,\n            'enabled': mcs.get_enabled(),\n           'metadata': mcs.get_metadata(),\n            'enabled_metadata': mcs.get_metadata(),\n           'metadata_metadata': mcs.get_metadata_metadata_metadata(),\n           'metadata_enabled': mcs.get_enabled_metadata_metadata_", "solution": "\n        if mcs._generated_class is None:\n            mcs._generated_class = type(\n                mcs._cls_name,\n                tuple(mcs._base_classes),\n                {'_generated': True})\n        return mcs._generated_class"}
{"prompt": "def get_doctypes(self, default_doctypes=None):\n        \"\"\"Returns the doctypes (or mapping type names) to use.\"\"\"", "output": "\n        if default_doctypes is None:\n            default_doctypes = self.doctypes\n        return default_doctypes\n\n    def get_doctype(self, dname):\n        \"\"\"Returns the doctypes for a document type.\n\n        @param dname: the document type name\n        @type dname: string\n        @return: the doctype objects\n        @rtype: list of doctypes.keys\n        \"\"\"\n        if dname in self.doctypes:\n            return self.doctypes[dname]\n        return []\n\n    def get_doctypes_from_path(self, path):\n        \"\"\"Returns the doctypes for", "solution": "\n        doctypes = self.type.get_mapping_type_name()\n        if isinstance(doctypes, six.string_types):\n            doctypes = [doctypes]\n        return super(S, self).get_doctypes(default_doctypes=doctypes)"}
{"prompt": "def _execute(self, endpoint, database, query, default_timeout, properties=None):\r\n        \"\"\"Executes given query against this client\"\"\"", "output": "\r\n        self.log.debug(\"Executing query: %s\" % query)\r\n        return self._client.execute(endpoint, database, query, properties)\r\n\r\n    def _execute_with_retry(self, database, properties=None):\r\n        \"\"\"Executes given query against this client\"\"\"\r\n        self.log.debug(\"Executing query: %s\" % query)\r\n        return self._execute(database, database, query, default_timeout, properties)\r\n\r\n    def _execute_with_timeout(self, database, properties=None):\r\n        \"\"\"Executes given query against this client\"\"\"\r\n        self.log.debug", "solution": "\r\n\r\n        request_payload = {\"db\": database, \"csl\": query}\r\n        if properties:\r\n            request_payload[\"properties\"] = properties.to_json()\r\n\r\n        request_headers = {\r\n            \"Accept\": \"application/json\",\r\n            \"Accept-Encoding\": \"gzip,deflate\",\r\n            \"Content-Type\": \"application/json; charset=utf-8\",\r\n            \"x-ms-client-version\": \"Kusto.Python.Client:\" + VERSION,\r\n            \"x-ms-client-request-id\": \"KPC.execute;\" + str(uuid.uuid4()),\r\n        }\r\n\r\n        if self._auth_provider:\r\n            request_headers[\"Authorization\"] = self._auth_provider.acquire_authorization_header()\r\n\r\n        timeout = self._get_timeout(properties, default_timeout)\r\n        response = self._session.post(endpoint, headers=request_headers, json=request_payload, timeout=timeout.seconds)\r\n\r\n        if response.status_code == 200:\r\n            if endpoint.endswith(\"v2/rest/query\"):\r\n                return KustoResponseDataSetV2(response.json())\r\n            return KustoResponseDataSetV1(response.json())\r\n\r\n        raise KustoServiceError([response.json()], response)"}
{"prompt": "def make_duplicate_request(request):\n    \"\"\"\n    Since werkzeug request objects are immutable, this is needed to create an\n    identical reuet object with immutable values so it can be retried after a\n    POST failure.\n    \"\"\"", "output": "\n    return werkzeug.datastructures.ImmutableMultiDict(request.values)\n\n\ndef make_response(body, status=200, headers=None):\n    \"\"\"\n    Similar to :func:`werkzeug.wrappers.Response.get_response` but the\n    response object is not writable.\n    \"\"\"\n    if not isinstance(body, werkzeug.datastructures.Response):\n        body = werkzeug.datastructures.Response(body)\n\n    if not headers:\n        headers = []\n\n    if isinstance(body, werkzeug.datastructures.FileStorage):\n        response = werkzeug.wrappers.Response(body, status, headers)\n    else:\n        with tempfile.NamedTemporaryFile", "solution": "\n    class FakeRequest(object):\n        method = 'GET'\n        path = request.path\n        headers = request.headers\n        GET = request.GET\n        POST = request.POST\n        user = getattr(request, 'user', None)\n        cookies = request.cookies\n        is_xhr = request.is_xhr\n    return FakeRequest()"}
{"prompt": "def a_not_committed(ctx):\n    \"\"\"Provide the message that current software is not committed and reload is not possible.\"\"\"", "output": "\n    return ctx.message.get(\"message\", {}).get(\"reload\", False)\n\n\ndef a_not_loaded(ctx):\n    \"\"\"Provide the message that current user is not loaded.\"\"\"\n    return ctx.message.get(\"message\", {}).get(\"not_loaded\", False)\n\n\ndef a_no_data(ctx):\n    \"\"\"Provide the message that current user has no data.\"\"\"\n    return ctx.message.get(\"message\", {}).get(\"data\", False)\n\n\ndef a_no_data_available(ctx):\n    \"\"\"Provide the message that current user has no data available.\"\"\"\n    return ctx", "solution": "\n    ctx.ctrl.sendline('n')\n    ctx.msg = \"Some active software packages are not yet committed. Reload may cause software rollback.\"\n    ctx.device.chain.connection.emit_message(ctx.msg, log_level=logging.ERROR)\n    ctx.failed = True\n    return False"}
{"prompt": "def _log_length_error(self, key, length):\n        \"\"\" Helper function for logging a response length error. \"\"\"", "output": "\n        log.error(u\"Response length error: %s (length: %s)\" % (key, length))\n\n    def _parse_response_headers(self, response):\n        \"\"\" Parses the response for errors and returns the data in a dict. \"\"\"\n        self.title = response['title']\n        self.body = response['body']\n        self.status_code = response['status_code']\n        self.status_message = response['status_message']\n        self.retry_after = response['retry_after']\n        self.retry_after_count = response['retry_after_count']", "solution": "\n        extra = {\n            \"max_detail_length\": settings.defaults[\"max_detail_length\"],\n            \"len\": length\n        }\n        if self.key_name:\n            extra[self.key_name] = key\n        msg = \"Length of data in %s is too long.\" % self.__class__.__name__\n        log.error(msg, extra=extra)"}
{"prompt": "def _check_values(in_values):\n        \"\"\" Check if values need to be converted before they get mogrify'd\n        \"\"\"", "output": "\n        if isinstance(in_values, dict):\n            for key, val in in_values.items():\n                if key == 'name':\n                    if val == 'default':\n                        return True\n                    else:\n                        return False\n                elif key == 'id':\n                    if val == 'default':\n                        return True\n                    else:\n                        return False\n                elif key == 'description':\n                    if val == 'default':\n                        return True\n                    else:\n                        return False\n                elif key == 'type':\n                    if val == 'default':\n                        return True\n                    else:\n                        return False\n            return True\n        else:\n            return False\n\n    def _get_", "solution": "\n        out_values = []\n        for value in in_values:\n            # if isinstance(value, (dict, list)):\n            #     out_values.append(json.dumps(value))\n            # else:\n            out_values.append(value)\n\n        return tuple(out_values)"}
{"prompt": "def are_you_sure(flag_changed, evt, parent=None, title=\"File has been changed\",\r\n                 msg=\"Are you sure you want to exit?\"):\r\n    \"\"\"\r\n    \"Are you sure you want to exit\" question dialog.\r\n\r\n    If flag_changed, shows question dialog. If answer is not yes, calls evt.ignore()\r\n\r\n    Arguments:\r\n      flag_changed\r\n      evt -- QCloseEvent instance\r\n      parent=None -- parent form, used to centralize the question dialog at\r\n      title -- title for question dialog\r\n      msg -- text of question dialog\r\n\r\n    Returns True or False. True means: \"yes, I want to exit\"\r\n    \"\"\"", "output": " close\"\r\n    \"\"\"\r\n    if parent is None:\r\n        parent = self.form\r\n        self.setParent(parent)\r\n    if not flag_changed:\r\n        self.form.close()\r\n        return False\r\n    if not self.isVisible():\r\n        self.hide()\r\n        return False\r\n    if not flag_changed:\r\n        self.setFocus()\r\n        return False\r\n    if not self.is_file_loaded:\r\n        self.form.load(self.model.filePath(self.model.fileName()))\r\n    return self.ignore()\r\n\r\n\r\ndef show_question(parent, title=\"File has been loaded\", msg=\"File", "solution": "\r\n    if flag_changed:\r\n        r = QMessageBox.question(parent, title, msg,\r\n             QMessageBox.Yes|QMessageBox.No, QMessageBox.Yes)\r\n        if r != QMessageBox.Yes:\r\n            evt.ignore()"}
{"prompt": "def str_with_sizes(self, max_name, max_remote_id, max_size):\n        \"\"\"\n        Create string for report based on internal properties using sizes to line up columns.\n        :param max_name: int width of the name column\n        :param max_remote_id: int width of the remote_id column\n        :return: str info from this report item\n        \"\"\"", "output": "\n        return self.name + \" \" + str(max_name) + \" \" + str(max_size)\n\n    def __str__(self):\n        \"\"\"\n        String representation of this report.\n        :return: str info from this report item\n        \"\"\"\n        return self.name + \" \" + str(self.size)\n\n    def __repr__(self):\n        \"\"\"\n        String representation of this report.\n        :return: str info from this report item\n        \"\"\"\n        return self.name + \" \" + str(self.size)\n\n\nclass ReportItem(object):\n    \"\"\"\n    A report item that is used", "solution": "\n        name_str = self.name.ljust(max_name)\n        remote_id_str = self.remote_id.ljust(max_remote_id)\n        size_str = self.size.ljust(max_size)\n        return u'{}    {}    {}    {}'.format(name_str, remote_id_str, size_str, self.file_hash)"}
{"prompt": "def _handle_timeout(self, msg_id, start_time):\n        \"\"\"Handle a timed-out callback request.\n\n        Parameters\n        ----------\n        msg_id : uuid.UUID for message\n            The name of the reply which was expected.\n\n        \"\"\"", "output": "\n        self.logger.info('Received request for %s: %s', msg_id, msg)\n        self._last_msg_id = msg_id\n        self._last_msg_name = msg.name\n        self.logger.info('  - last_msg_name: %s', self._last_msg_name)\n        self.logger.info('  - last_msg_id: %s', msg.id)\n        self.logger.info('  - start_time: %s', start_time)\n        self.logger.info('  - self.name: %", "solution": "\n        msg, reply_cb, inform_cb, user_data, timeout_handle = \\\n            self._pop_async_request(msg_id, None)\n        # We may have been racing with the actual reply handler if the reply\n        # arrived close to the timeout expiry,\n        # which means the self._pop_async_request() call gave us None's.\n        # In this case, just bail.\n        #\n        # NM 2014-09-17 Not sure if this is true after porting to tornado,\n        # but I'm too afraid to remove this code :-/\n        if timeout_handle is None:\n            return\n\n        reason = \"Request {0.name} timed out after {1:f} seconds.\".format(\n            msg, self.ioloop.time() - start_time)\n        self._do_fail_callback(\n            reason, msg, reply_cb, inform_cb, user_data, timeout_handle)"}
{"prompt": "def retryable(fn, protocol=None):\n    \"\"\"\n    Wraps a client operation that can be retried according to the set\n    :attr:`RiakClient.retries`. Used internally.\n    \"\"\"", "output": "\n    def wrapped(self, *args, **kwargs):\n        if self.retries is not None:\n            self.RiakClient.retries -= 1\n            if self.RiakClient.retries == 0:\n                raise RiakError(\"Riak client operation failed\")\n        return fn(self, *args, **kwargs)\n    return wrapped\n\n\nclass RiakClient(object):\n    \"\"\"\n    A :class:`RiakClient` object is a context manager that uses the\n    :class:`RiakClient.get_context` method to get a context from the server.\n    \"\"\"\n\n    def __init", "solution": "\n    def wrapper(self, *args, **kwargs):\n        pool = self._choose_pool(protocol)\n\n        def thunk(transport):\n            return fn(self, transport, *args, **kwargs)\n\n        return self._with_retries(pool, thunk)\n\n    wrapper.__doc__ = fn.__doc__\n    wrapper.__repr__ = fn.__repr__\n\n    return wrapper"}
{"prompt": "def _send(self):\n        \"\"\" Send the webhook method \"\"\"", "output": "\n        if self.token:\n            self.send_webhook(self.token)\n        else:\n            self.send_webhook(self.url)\n        return self.send_response()\n\n    def send_response(self):\n        \"\"\" Send the webhook response \"\"\"\n        if self.token:\n            self.token = self._send()\n        if self.token:\n            self.send_webhook(self.url, response=self.response)\n        else:\n            self.send_webhook(self.url)\n        return self.send_response_from_file()\n\n    def send_response_from_file(", "solution": "\n\n        payload = self.payload\n        sending_metadata = {'success': False}\n        post_attributes = {'timeout': self.timeout}\n\n        if self.custom_headers:\n            post_attributes['headers'] = self.custom_headers\n        if not post_attributes.get('headers', None):\n            post_attributes['headers'] = {}\n        post_attributes['headers']['Content-Type'] = self.encoding\n\n        post_attributes['data'] = self.format_payload()\n\n        if self.signing_secret:\n            post_attributes['headers']['x-hub-signature'] = self.create_signature(post_attributes['data'], \\\n                                                                                  self.signing_secret)\n\n        for i, wait in enumerate(range(len(self.attempts) - 1)):\n\n            self.attempt = i + 1\n            sending_metadata['attempt'] = self.attempt\n\n            try:\n                print(self.url)\n                self.response = requests.post(self.url, **post_attributes)\n\n                if sys.version > '3':\n                    # Converts bytes object to str object in Python 3+\n                    self.response_content = self.response.content.decode('utf-8')\n                else:\n                    self.response_content = self.response.content\n\n                sending_metadata['status_code'] = self.response.status_code\n\n                # anything with a 200 status code  is a success\n                if self.response.status_code >= 200 and self.response.status_code < 300:\n                    # Exit the sender method.  Here we provide the payload as a result.\n                    #   This is useful for reporting.\n                    self.notify(\"Attempt {}: Successfully sent webhook {}\".format(\n                        self.attempt, self.hash_value)\n                    )\n                    sending_metadata['response'] = self.response_content\n                    sending_metadata['success'] = True\n                    break\n                else:\n                    self.error = \"Status code (%d). Message: %s\" % (self.response.status_code, self.response.text)\n\n\n            except Exception as ex:\n                err_formatted = str(ex).replace('\"',\"'\")\n                sending_metadata['response'] = '{\"status_code\": 500, \"status\":\"failure\",\"error\":\"'+err_formatted+'\"}'\n                self.error = err_formatted\n\n            self.notify(\"Attempt {}: Could not send webhook {}\".format(\n                    self.attempt, self.hash_value)\n            )\n            self.notify_debug(\"Webhook {}. Body: {}\".format(\n                    self.hash_value, self.payload)\n            )\n\n            # If last attempt\n            if self.attempt == (len(self.attempts) - 1):\n                self.notify_error(\"Failed to send webhook {}. Body: {}\".format(\n                    self.hash_value, self.payload)\n                )\n            else:\n                # Wait a bit before the next attempt\n                sleep(wait)\n\n        sending_metadata['error'] = None if sending_metadata['success'] or not self.error else self.error\n        sending_metadata['post_attributes'] = post_attributes\n        merged_dict = sending_metadata.copy()\n        if isinstance(payload, string_types):\n            payload = {'payload': payload}\n\n        # Add the hash value if there is one.\n        if self.hash_value is not None and len(self.hash_value) > 0:\n            payload['hash'] = self.hash_value\n\n        merged_dict.update(payload)\n        return merged_dict"}
{"prompt": "def attention(query: torch.Tensor,\n              key: torch.Tensor,\n              value: torch.Tensor,\n              mask: torch.Tensor = None,\n              dropout: Callable = None) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute 'Scaled Dot Product Attention'\"\"\"", "output": "\n    if mask is not None:\n        query = query.masked_fill(mask)\n    if dropout is not None:\n        query = dropout(query)\n    return query, value, mask, mask, mask\n\n\ndef attention_layer(query: torch.Tensor,\n                    key: torch.Tensor,\n                    value: torch.Tensor,\n                    mask: torch.Tensor = None,\n                    dropout: Callable = None) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Compute 'Dot Product Attention Layer'\"\"\"\n    if mask is not None:\n        query = query.masked_fill(mask)\n    if dropout", "solution": "\n    d_k = query.size(-1)\n    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)\n    p_attn = F.softmax(scores, dim=-1)\n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn"}
{"prompt": "def ToCsv(self, columns_order=None, order_by=(), separator=\",\"):\n    \"\"\"Writes the data table as a CSV string.\n\n    Output is encoded in UTF-8 because the Python \"csv\" module can't handle\n    Unicode properly according to its documentation.\n\n    Args:\n      columns_order: Optional. Specifies the order of columns in the\n                     output table. Specify a list of all column IDs in the order\n                     in which you want the table created.\n                     Note that you must list all column IDs in this parameter,\n                     if you use it.\n      order_by: Optional. Specifies the name of the column(s) to sort by.\n                Passed as is to _PreparedData.\n      separator: Optional. The separator to use between the values.\n\n    Returns:\n      A CSV string representing the table.\n      Example result:\n       'a','b','c'\n       1,'z',2\n       3,'w',''\n\n    Raises:\n      DataTableException: The data does not match the type.\n    \"\"\"", "output": " the column(s) to sort by.\n                If not provided, the default sort order is used.\n      separator: Optional. Specifies the separator to use between columns.\n    \"\"\"\n    if columns_order is None:\n      columns_order = self.GetColumnOrder()\n    if order_by is None:\n      order_by = self.GetColumnName()\n\n    if columns_order:\n      self.WriteCSVLine(separator)\n\n    if order_by:\n      self.WriteCSVLine(separator)\n\n    if separator:\n      self.WriteCSVLine(separator)\n\n    if order_by:\n      self.WriteCSV", "solution": "\n\n    csv_buffer = six.StringIO()\n    writer = csv.writer(csv_buffer, delimiter=separator)\n\n    if columns_order is None:\n      columns_order = [col[\"id\"] for col in self.__columns]\n    col_dict = dict([(col[\"id\"], col) for col in self.__columns])\n\n    def ensure_str(s):\n      \"Compatibility function. Ensures using of str rather than unicode.\"\n      if isinstance(s, str):\n        return s\n      return s.encode(\"utf-8\")\n\n    writer.writerow([ensure_str(col_dict[col][\"label\"])\n                     for col in columns_order])\n\n    # We now go over the data and add each row\n    for row, unused_cp in self._PreparedData(order_by):\n      cells_list = []\n      # We add all the elements of this row by their order\n      for col in columns_order:\n        value = \"\"\n        if col in row and row[col] is not None:\n          value = self.CoerceValue(row[col], col_dict[col][\"type\"])\n        if isinstance(value, tuple):\n          # We have a formatted value. Using it only for date/time types.\n          if col_dict[col][\"type\"] in [\"date\", \"datetime\", \"timeofday\"]:\n            cells_list.append(ensure_str(self.ToString(value[1])))\n          else:\n            cells_list.append(ensure_str(self.ToString(value[0])))\n        else:\n          cells_list.append(ensure_str(self.ToString(value)))\n      writer.writerow(cells_list)\n    return csv_buffer.getvalue()"}
{"prompt": "def catalog(self):\n        \"\"\"Primary registered catalog for the wrapped portal type\n        \"\"\"", "output": "\n        return self._catalog\n\n    @property\n    def name(self):\n        \"\"\"The name of the wrapped portal type\n        \"\"\"\n        return self._name\n\n    @property\n    def description(self):\n        \"\"\"Description of the wrapped portal type\n        \"\"\"\n        return self._description\n\n    @property\n    def version(self):\n        \"\"\"Version of the wrapped portal type\n        \"\"\"\n        return self._version\n\n    @property\n    def is_active(self):\n        \"\"\"Whether the portal is currently active\n        \"\"\"\n        return self._is_active\n\n    @property\n    def is_enabled(self):\n        \"\"\"Whether the portal is enabled\n        \"\"\"\n        return", "solution": "\n        if self._catalog is None:\n            logger.debug(\"SuperModel::catalog: *Fetch catalog*\")\n            self._catalog = self.get_catalog_for(self.brain)\n        return self._catalog"}
{"prompt": "def get_ip_prefixes_from_bird(filename):\n    \"\"\"Build a list of IP prefixes found in Bird configuration.\n\n    Arguments:\n        filename (str): The absolute path of the Bird configuration file.\n\n    Notes:\n        It can only parse a file with the following format\n\n            define ACAST_PS_ADVERTISE =\n                [\n                    10.189.200.155/32,\n                    10.189.200.255/32\n                ];\n\n    Returns:\n        A list of IP prefixes.\n\n    \"\"\"", "output": "\n    ip_prefixes = []\n    with open(filename, 'r') as f:\n        for line in f:\n            if line.startswith('define'):\n                if line.startswith('type'):\n                    ip_prefixes.append(line.split()[1])\n                elif line.startswith('value'):\n                    ip_prefixes.append(line.split()[1])\n    return ip_prefixes\n\n\ndef get_ip_prefixes_from_ip_address(ip_address):\n    \"\"\"Build a list of IP prefixes found in the given IP address.\n\n    Arguments:\n        ip_address (str): The IP address of the", "solution": "\n    prefixes = []\n    with open(filename, 'r') as bird_conf:\n        lines = bird_conf.read()\n\n    for line in lines.splitlines():\n        line = line.strip(', ')\n        if valid_ip_prefix(line):\n            prefixes.append(line)\n\n    return prefixes"}
{"prompt": "def cd(dir_name):\n    \"\"\"\n    do something in other directory and return back after block ended\n    \"\"\"", "output": "\n    if not os.path.isdir(dir_name):\n        raise Exception(\"Directory %s does not exist\" % dir_name)\n\n    for root, dirs, files in os.walk(dir_name):\n        for dir_name in dirs:\n            if not cd(root + dir_name):\n                return False\n        for file in files:\n            if not cd(root + file):\n                return False\n    return True\n\n\ndef get_all_files(dir_name):\n    \"\"\"\n    get all the files in a directory\n    \"\"\"\n    return [os.path.join(root, file) for file", "solution": "\n    old_path = os.path.abspath('.')\n    os.chdir(dir_name)\n    try:\n        yield\n        os.chdir(old_path)\n    except Exception:\n        os.chdir(old_path)\n        raise"}
{"prompt": "def queryByPortSensor(portiaConfig, edgeId, port, sensor, last=False, params={ 'from': None, 'to': None, 'order': None, 'precision': 'ms', 'limit': None }):\n    \"\"\"Returns a pandas data frame with the portia select resultset\"\"\"", "output": "\n    if not last:\n        last = True\n    if not edgeId:\n        edgeId = port\n    if last:\n        return _query_by_port(portiaConfig, sensor, edgeId, params)\n    else:\n        return _query_by_port(portiaConfig, sensor, edgeId, params)\n\ndef _query_by_port(portiaConfig, sensor, edgeId, params):\n    \"\"\"Returns pandas data frame with the portia select resultset\"\"\"\n    if not last:\n        edgeId = sensor.getEdgeId()\n    if not edgeId:\n        return _query_", "solution": "\n\n    header = {'Accept': 'text/csv'}\n\n    if last == False:\n        endpoint = '/select/device/{0}/port/{1}/sensor/{2}{3}'.format( edgeId, port, sensor, utils.buildGetParams(params) )\n    else:\n        endpoint = '/select/device/{0}/port/{1}/sensor/{2}/last{3}'.format( edgeId, port, sensor, utils.buildGetParams(params) )\n\n    response = utils.httpGetRequest(portiaConfig, endpoint, header)\n\n    if response.status_code == 200:\n        try:\n\n            dimensionSeries = pandas.read_csv( StringIO(response.text), sep=';' )\n            if portiaConfig['debug']:\n                print( '[portia-debug]: {0} rows'.format( len(dimensionSeries.index) ) )\n\n            return dimensionSeries\n\n        except:\n            raise Exception('couldn\\'t create pandas data frame')\n    else:\n        raise Exception('couldn\\'t retrieve data')"}
{"prompt": "def related_archives(self):\n        \"\"\"\n        The pathnames of the source distribution(s) for this requirement (a list of strings).\n\n        .. note:: This property is very new in pip-accel and its logic may need\n                  some time to mature. For now any misbehavior by this property\n                  shouldn't be too much of a problem because the pathnames\n                  reported by this property are only used for cache\n                  invalidation (see the :attr:`last_modified` and\n                  :attr:`checksum` properties).\n        \"\"\"", "output": "\n        return self._related_archives\n\n    @property\n    def has_hash(self):\n        \"\"\"\n        Whether this requirement has a hash (a string) across\n        all its dependencies.\n\n       .. versionadded:: 1.1\n        \"\"\"\n        return self._hash is not None\n\n    @property\n    def has_build_tag(self):\n        \"\"\"\n        Whether this requirement has a build tag (a string) across all its\n        dependencies.\n\n       .. versionadded:: 1.1\n        \"\"\"\n        return self._build_tag is not None\n\n    @property\n    def has_ext_modules(self):\n        \"\"\"\n        Whether this requirement is an extension", "solution": "\n        # Escape the requirement's name for use in a regular expression.\n        name_pattern = escape_name(self.name)\n        # Escape the requirement's version for in a regular expression.\n        version_pattern = re.escape(self.version)\n        # Create a regular expression that matches any of the known source\n        # distribution archive extensions.\n        extension_pattern = '|'.join(re.escape(ext) for ext in ARCHIVE_EXTENSIONS if ext != '.whl')\n        # Compose the regular expression pattern to match filenames of source\n        # distribution archives in the local source index directory.\n        pattern = '^%s-%s(%s)$' % (name_pattern, version_pattern, extension_pattern)\n        # Compile the regular expression for case insensitive matching.\n        compiled_pattern = re.compile(pattern, re.IGNORECASE)\n        # Find the matching source distribution archives.\n        return [os.path.join(self.config.source_index, fn)\n                for fn in os.listdir(self.config.source_index)\n                if compiled_pattern.match(fn)]"}
{"prompt": "def find_phase_transformation(Ne, Nl, r, Lij, verbose=0,\n                              return_equations=False, **kwds):\n    \"\"\"This function returns a phase transformation specified as a list of\n    lenght Ne whose elements correspond to each theta_i. Each element is a\n    list of length Nl which specifies the coefficients multiplying each\n    optical frequency omega^l. So for instance [[1,1],[1,0],[0,0]] means\n\n    theta_1=omega^1+omega^2\n    theta_2=omega^1\n    theta_3=0.\n    \"\"\"", "output": " \"\"\"\n    # Find the number of frequencies\n    Nf = len(Ne)\n    # Find the coefficients\n    if verbose:\n        print('find_phase_transformation')\n    # Find the optical frequency\n    if len(r) == Nf:\n        if len(kwds) == Nf:\n                return_optical_frequency = kwds[Nf]\n            else:\n                return_optical_frequency = [0.0 for i in range(len(Ne[0]))]\n        else:\n                return_optical_frequency = [0.0 for i in range(len(Ne[0]))]", "solution": "\n    # We first define the needed variables\n    _omega_laser = [Symbol('omega_laser'+str(l+1)) for l in range(Nl)]\n    _theta = [Symbol('theta'+str(i+1)) for i in range(Ne)]\n\n    # We find all the equations that the specified problem has to fulfil.\n    eqs = []\n    for i in range(Ne):\n        for j in range(i+1, Ne):\n            if type(r[0]) == list:\n                if (r[0][i][j] != 0) or (r[1][i][j] != 0) or (r[2][i][j] != 0):\n                    for l in range(Nl):\n                        if l+1 in Lij[i][j]:\n                            eqs += [_omega_laser[l] + _theta[j] - _theta[i]]\n            else:\n                if (r[0][i, j] != 0) or (r[1][i, j] != 0) or (r[2][i, j] != 0):\n                    for l in range(Nl):\n                        if l+1 in Lij[i][j]:\n                            eqs += [_omega_laser[l] + _theta[j] - _theta[i]]\n\n    if return_equations:\n        return eqs\n\n    sol = solve(eqs, _theta, dict=True)[0]\n    for i in range(Ne):\n        if _theta[i] not in sol.keys():\n            sol.update({_theta[i]: _theta[i]})\n\n    sol_simple = {_theta[i]: sol[_theta[i]]-sol[_theta[-1]] for i in range(Ne)}\n\n    sol = []\n    for i in range(Ne):\n        soli = []\n        for l in range(Nl):\n            soli += [diff(sol_simple[_theta[i]], _omega_laser[l])]\n        sol += [soli]\n\n    return sol"}
{"prompt": "def profile(self):\n        \"\"\"\n        Buffered result of :meth:`build_profile`\n        \"\"\"", "output": "\n        return self.build_profile(self.profile_name, self.profile_version)\n\n    def build_profile(self, profile_name, profile_version):\n        \"\"\"\n        Builds a :class:`Profile` object\n        \"\"\"\n        return Profile(self, profile_name, profile_version, self.profile_path)\n\n    def get_profile_path(self):\n        \"\"\"\n        Returns the path to the profile file\n        \"\"\"\n        return self.profile_path\n\n    def get_version(self):\n        \"\"\"\n        Returns the version of the current profile\n        \"\"\"\n        return self.profile_version\n\n    def", "solution": "\n        if self._profile is None:\n            self._profile = self.build_profile()\n        return self._profile"}
{"prompt": "def symlink_create(self, symlink, target, type_p):\n        \"\"\"Creates a symbolic link in the guest.\n\n        in symlink of type str\n            Path to the symbolic link that should be created.  Guest path\n            style.\n\n        in target of type str\n            The path to the symbolic link target.  If not an absolute, this will\n            be relative to the @a symlink location at access time.  Guest path\n            style.\n\n        in type_p of type :class:`SymlinkType`\n            The symbolic link type (mainly for Windows). See :py:class:`SymlinkType` \n            for more information.\n\n        raises :class:`OleErrorNotimpl`\n            The method is not implemented yet.\n        \n        \"\"\"", "output": " for more details.\n\n       .. versionadded:: 0.3.0\n\n        \"\"\"\n        if isinstance(target, SymlinkTarget):\n            if not self.exists():\n                raise RuntimeError(\"File %s does not exist\" % self.path)\n            if not os.path.isabs(target):\n                target = os.path.join(self.path, target)\n            if not os.path.exists(os.path.dirname(symlink)):\n                os.makedirs(os.path.dirname(symlink))\n            if not os.path.exists(os.path.join(self.path, 'bin', 'win", "solution": "\n        if not isinstance(symlink, basestring):\n            raise TypeError(\"symlink can only be an instance of type basestring\")\n        if not isinstance(target, basestring):\n            raise TypeError(\"target can only be an instance of type basestring\")\n        if not isinstance(type_p, SymlinkType):\n            raise TypeError(\"type_p can only be an instance of type SymlinkType\")\n        self._call(\"symlinkCreate\",\n                     in_p=[symlink, target, type_p])"}
{"prompt": "def open_fastq(in_file):\n    \"\"\" open a fastq file, using gzip if it is gzipped\n    from bcbio package\n    \"\"\"", "output": "\n    if in_file.endswith(\".gz\"):\n        return gzip.open(in_file, \"rt\")\n    else:\n        return open(in_file, \"rt\")\n\n\ndef _is_fastq_valid_file(in_file):\n    \"\"\" check if a fastq file is valid\n    \"\"\"\n    if not _is_valid_fastq(in_file):\n        return False\n    return True\n\n\ndef _is_fastq_valid_file_or_url(in_file):\n    \"\"\" check if a fastq file is valid\n    \"\"\"\n    if not _is_valid_file(in", "solution": "\n    _, ext = os.path.splitext(in_file)\n    if ext == \".gz\":\n        return gzip.open(in_file, 'rb')\n    if ext in [\".fastq\", \".fq\", \".fasta\", \".fa\"]:\n        return open(in_file, 'r')\n    return ValueError(\"File needs to be fastq|fasta|fq|fa [.gz]\")"}
{"prompt": "def IOR(classical_reg1, classical_reg2):\n    \"\"\"\n    Produce an inclusive OR instruction.\n\n    :param classical_reg1: The first classical register, which gets modified.\n    :param classical_reg2: The second classical register or immediate value.\n    :return: A ClassicalOr instance.\n    \"\"\"", "output": "\n    return IORInstruction(classical_reg1, classical_reg2)\n\n\ndef IORInstruction(classical_reg1, classical_reg2):\n    \"\"\"\n    Produce an instruction.\n\n    :param classical_reg1: The first classical register, which gets modified.\n    :param classical_reg2: The second classical register or immediate value.\n    :return: A ClassicalOr instance.\n    \"\"\"\n    return IORInstruction(classical_reg1, classical_reg2)\n\n\nclass IORInstructionFactory(object):\n    \"\"\"\n    Factory for", "solution": "\n    left, right = unpack_reg_val_pair(classical_reg1, classical_reg2)\n    return ClassicalInclusiveOr(left, right)"}
{"prompt": "def resolver(self, vocab_data, attribute):\n        \"\"\"Pull the requested attribute based on the given vocabulary\n        and content.\n        \"\"\"", "output": "\n        if attribute in vocab_data:\n            return vocab_data[attribute]\n        return None\n\n    def _get_vocab_data(self, attribute):\n        \"\"\"Get the requested attribute based on the given attribute.\n        \"\"\"\n        if attribute in self.vocab_data:\n            return self.vocab_data[attribute]\n        return None\n\n    def _get_attribute_from_dict(self, dict_name, attribute):\n        \"\"\"Get the requested attribute from a dictionary.\n        \"\"\"\n        if dict_name in dict_to_attr:\n            return dict_to_attr[dict_name][attribute]\n        return", "solution": "\n        term_list = vocab_data.get(self.content_vocab, [])\n        # Loop through the terms from the vocabulary.\n        for term_dict in term_list:\n            # Match the name to the current content.\n            if term_dict['name'] == self.content:\n                return term_dict[attribute]\n        return self.content"}
{"prompt": "def mappedPolygon(self, polygon, path=None, percent=0.5):\n        \"\"\"\n        Maps the inputed polygon to the inputed path \\\n        used when drawing items along the path.  If no \\\n        specific path is supplied, then this object's own \\\n        path will be used.  It will rotate and move the \\\n        polygon according to the inputed percentage.\n        \n        :param      polygon     <QPolygonF>\n        :param      path        <QPainterPath>\n        :param      percent     <float>\n        \n        :return     <QPolygonF> mapped_poly\n        \"\"\"", "output": "\n        if path is None:\n            path = self.path()\n        if path.isEmpty():\n            return QPainterPath()\n        if path.isClipPath():\n            path = clip_to_rect(path)\n        if path.isNull():\n            return QPainterPath()\n        if path.contains(path.arc(polygon.center().x(), polygon.center().y(), percent*2)) or \\\n           path.contains(path.line(polygon.center().x(), polygon.center().y(), percent*2)) or \\\n           path.contains(path.arc(polygon.center().x(),", "solution": "\n        translatePerc = percent\n        anglePerc = percent\n\n        # we don't want to allow the angle percentage greater than 0.85\n        # or less than 0.05 or we won't get a good rotation angle\n        if 0.95 <= anglePerc:\n            anglePerc = 0.98\n        elif anglePerc <= 0.05:\n            anglePerc = 0.05\n\n        if not path:\n            path = self.path()\n        if not (path and path.length()):\n            return QPolygonF()\n\n        # transform the polygon to the path\n        point = path.pointAtPercent(translatePerc)\n        angle = path.angleAtPercent(anglePerc)\n\n        # rotate about the 0 axis\n        transform = QTransform().rotate(-angle)\n        polygon = transform.map(polygon)\n\n        # move to the translation point\n        transform = QTransform().translate(point.x(), point.y())\n\n        # create the rotated polygon\n        mapped_poly = transform.map(polygon)\n        self._polygons.append(mapped_poly)\n\n        return mapped_poly"}
{"prompt": "def p_with_statement(self, p):\n        \"\"\"with_statement : WITH LPAREN expr RPAREN statement\"\"\"", "output": "\n        p[0] = self.handleChildren(p[2], p[4])\n\n    def p_with_statement_error(self, p):\n        \"\"\"with_statement : WITH LBRACE expr RBRACE statement_error\"\"\"\n        p[0] = self.handleChildren(p[2], p[4], p[6])\n\n    def p_with_statement_error_in_block(self, p):\n        \"\"\"with_statement : WITH LPAREN expr RPAREN statement_error\"\"\"\n        p[0] = self.handleChildren(p[2], p[4], p[6", "solution": "\n        p[0] = self.asttypes.With(expr=p[3], statement=p[5])\n        p[0].setpos(p)"}
{"prompt": "def hmac_hex_key(self, hmac_hex_key):\n        \"\"\"\n        Sets the hmac_hex_key of this CfsslAuthCredentials.\n        The key that is used to compute the HMAC of the request using the HMAC-SHA-256 algorithm. Must contain an even number of hexadecimal characters. \n\n        :param hmac_hex_key: The hmac_hex_key of this CfsslAuthCredentials.\n        :type: str\n        \"\"\"", "output": "\n        self._hmac_sha256 = hmac_sha256\n        return self\n    \n    def set_hmac_sha256(self, hmac_sha256):\n        \"\"\"\n        Sets the hmac_sha256 of this CfsslAuthCredentials.\n        The key that is used to compute the HMAC of the request using the HMAC-SHA-256 algorithm. Must contain an even number of hexadecimal characters. \n\n        :param hmac_sha256: The hmac_sha256 of this CfsslAuthCredentials.\n        :type: str\n        \"\"\"\n        self._hmac_sha256 = hmac_sha256\n        return self\n    \n    def get_hmac_", "solution": "\n        if hmac_hex_key is None:\n            raise ValueError(\"Invalid value for `hmac_hex_key`, must not be `None`\")\n        if hmac_hex_key is not None and len(hmac_hex_key) > 64:\n            raise ValueError(\"Invalid value for `hmac_hex_key`, length must be less than or equal to `64`\")\n        if hmac_hex_key is not None and not re.search('^([a-fA-F0-9][a-fA-F0-9]){1,32}$', hmac_hex_key):\n            raise ValueError(\"Invalid value for `hmac_hex_key`, must be a follow pattern or equal to `/^([a-fA-F0-9][a-fA-F0-9]){1,32}$/`\")\n\n        self._hmac_hex_key = hmac_hex_key"}
{"prompt": "def sorted_feed_cols(df):\n    \"\"\"\n    takes a dataframe's columns that would be of the form:\n    ['feed003', 'failsafe_feed999', 'override_feed000', 'feed001', 'feed002']\n    and returns:\n    ['override_feed000', 'feed001', 'feed002', 'feed003', 'failsafe_feed999']\n    \"\"\"", "output": "\n    return df.sort_values('feed_id', ascending=False)\n\n\ndef get_feed_ids(df):\n    \"\"\"\n    takes a dataframe's columns that would be of the form:\n    ['feed003', 'failsafe_feed999', 'override_feed000', 'feed001', 'feed002']\n    and returns:\n    ['failsafe_feed000', 'failsafe_feed999', 'override_feed000', 'failsafe_feed001', 'failsafe_feed002']\n    \"\"\"\n    return sorted_feed_cols(df)\n\n\ndef get_feed_ids_", "solution": "\n    cols = df.columns\n    ind = [int(c.split(\"feed\")[1]) for c in cols]\n    cols = zip(ind,cols)\n    cols.sort()\n    cols = [c[1] for c in cols]\n    return cols"}
{"prompt": "def extract_domain(host):\n    \"\"\"\n    Domain name extractor. Turns host names into domain names, ported\n    from pwdhash javascript code\"\"\"", "output": "\n    return re.sub(r'[^a-zA-Z0-9]', '', host)\n\n\ndef extract_id(host):\n    \"\"\"\n    Extracts the id from the javascript code and returns it as a string\n    \"\"\"\n    return re.sub(r'id([0-9]+)', '', host)\n\n\ndef extract_name(host):\n    \"\"\"\n    Extracts the name from the javascript code and returns it as a string\n    \"\"\"\n    return re.sub(r'[^a-zA-Z0-9]', '', host)\n\n\ndef extract_value(value):\n    \"\"\"\n    Extracts the", "solution": "\n    host = re.sub('https?://', '', host)\n    host = re.match('([^/]+)', host).groups()[0]\n    domain = '.'.join(host.split('.')[-2:])\n    if domain in _domains:\n        domain = '.'.join(host.split('.')[-3:])\n    return domain"}
{"prompt": "def process_table(fname):\n    \"\"\"Return processor by processing a given sheet of a spreadsheet file.\n\n    Parameters\n    ----------\n    fname : str\n        The name of the Excel file (typically .xlsx extension) to process\n\n    Returns\n    -------\n    sp : indra.sources.sofia.processor.SofiaProcessor\n        A SofiaProcessor object which has a list of extracted INDRA\n        Statements as its statements attribute.\n    \"\"\"", "output": "\n    with open(fname, 'r') as f:\n        reader = csv.reader(f, delimiter='\\t')\n        for row in reader:\n            if row[0] =='state':\n                yield row[1:]\n\n    return\n\n\ndef process_sheet(fname):\n    \"\"\"Return processor by processing a given sheet of a spreadsheet file.\n\n    Parameters\n    ----------\n    fname : str\n        The name of the spreadsheet file (typically.xlsx extension) to process\n\n    Returns\n    -------\n    sp : indra.sources.sofia.processor.SofiaProcessor\n        A SofiaProcessor object which has a", "solution": "\n    book = openpyxl.load_workbook(fname, read_only=True)\n    try:\n        rel_sheet = book['Relations']\n    except Exception as e:\n        rel_sheet = book['Causal']\n    event_sheet = book['Events']\n    entities_sheet = book['Entities']\n    sp = SofiaExcelProcessor(rel_sheet.rows, event_sheet.rows,\n                             entities_sheet.rows)\n    return sp"}
{"prompt": "def demean(in_file, in_mask, only_mask=False, newpath=None):\n    \"\"\"Demean ``in_file`` within the mask defined by ``in_mask``\"\"\"", "output": "\n    if newpath is None:\n        newpath = in_file\n    if only_mask:\n        out_file = in_file + \".demean\"\n    else:\n        out_file = in_file + \".demean.txt\"\n    with open(out_file, 'w') as out_f:\n        for line in open(in_file):\n            line = line.strip()\n            if line == \"\" or line[0] == \"#\":\n                continue\n            line = line.split()\n            if newpath is not None:\n                out_f.write(newpath + \"\\n\")\n            else:\n               ", "solution": "\n    import os\n    import numpy as np\n    import nibabel as nb\n    from nipype.utils.filemanip import fname_presuffix\n\n    out_file = fname_presuffix(in_file, suffix='_demeaned',\n                               newpath=os.getcwd())\n    nii = nb.load(in_file)\n    msk = nb.load(in_mask).get_data()\n    data = nii.get_data()\n    if only_mask:\n        data[msk > 0] -= np.median(data[msk > 0])\n    else:\n        data -= np.median(data[msk > 0])\n    nb.Nifti1Image(data, nii.affine, nii.header).to_filename(\n        out_file)\n    return out_file"}
{"prompt": "def filter_permissions(self, search):\n        \"\"\"Filter given query based on permissions of the user in the request.\n\n        :param search: ElasticSearch query object\n\n        \"\"\"", "output": "\n        if not search.query_key:\n            return\n        if not search.query_key.startswith('search.'):\n            return\n        if not search.query_key.startswith('search.'):\n            return\n        if search.query_key.startswith('search.'):\n            search.query_key ='search.' + search.query_key[len('search.'):]\n        if search.query_key.startswith('search.'):\n            search.query_key ='search' + search.query_key[len('search.'):]\n        if search.query_key in self.request.session:\n            del self.request", "solution": "\n        user = self.request.user\n        if user.is_superuser:\n            return search\n        if user.is_anonymous:\n            user = get_anonymous_user()\n\n        filters = [Q('match', users_with_permissions=user.pk)]\n        filters.extend([\n            Q('match', groups_with_permissions=group.pk) for group in user.groups.all()\n        ])\n        filters.append(Q('match', public_permission=True))\n\n        # `minimum_should_match` is set to 1 by default\n        return search.query('bool', should=filters)"}
{"prompt": "def colors_to_needle_positions(rows):\n    \"\"\"Convert rows to needle positions.\n\n    :return:\n    :rtype: list\n    \"\"\"", "output": "\n    return [\n        (\n            row[\"name\"],\n            row[\"color\"],\n            row[\"position\"]) for row in rows]\n\n\ndef get_needle_positions(db_session):\n    \"\"\"Get the list of needle positions.\n\n    :param db_session:\n    :type db_session: sqlalchemy.orm.session.Session\n    :return:\n    :rtype: list\n    \"\"\"\n    needle_positions = db_session.query(\n        func.count(func.distinct(NeedlePosition.name))\n    ).filter(\n        NeedlePosition.color == 'blue',\n        NeedlePosition.position", "solution": "\n    needles = []\n    for row in rows:\n        colors = set(row)\n        if len(colors) == 1:\n            needles.append([NeedlePositions(row, tuple(colors), False)])\n        elif len(colors) == 2:\n            color1, color2 = colors\n            if color1 != row[0]:\n                color1, color2 = color2, color1\n            needles_ = _row_color(row, color1)\n            needles.append([NeedlePositions(needles_, (color1, color2), True)])\n        else:\n            colors = []\n            for color in row:\n                if color not in colors:\n                    colors.append(color)\n            needles_ = []\n            for color in colors:\n                needles_.append(NeedlePositions(_row_color(row, color),\n                                                (color,), False))\n            needles.append(needles_)\n    return needles"}
{"prompt": "def _decode_datetime(obj):\n    \"\"\"Decode a msgpack'ed datetime.\"\"\"", "output": "\n    if isinstance(obj, datetime):\n        return obj.isoformat()\n    elif isinstance(obj, datetime_tz):\n        return obj.isoformat() + \"Z\"\n    elif isinstance(obj, timedelta):\n        return str(obj)\n    elif isinstance(obj, Decimal):\n        return str(obj)\n    elif isinstance(obj, uuid.UUID):\n        return str(obj)\n    else:\n        raise TypeError(\"Unsupported type: %s\" % type(obj))\n\n\ndef _decode_timedelta(obj):\n    \"\"\"Decode a msgpack'ed timedelta.\"\"\"\n    if isinstance(obj, timedelta):\n        return obj.total", "solution": "\n\n    if '__datetime__' in obj:\n        obj = datetime.datetime.strptime(obj['as_str'].decode(), \"%Y%m%dT%H:%M:%S.%f\")\n    return obj"}
{"prompt": "def delete_plat_operator(operator, auth, url):\n    \"\"\"\n    Function to set the password of an existing operator\n    :param operator: str Name of the operator account\n\n    :param auth: requests auth object #usually auth.creds from auth pyhpeimc.auth.class\n\n    :param url: base url of IMC RS interface #usually auth.url from pyhpeimc.auth.authclass\n\n    :return: int of 204 if successfull\n\n    :rtype: int\n\n    >>> from pyhpeimc.auth import *\n\n    >>> from pyhpeimc.plat.operator import *\n\n    >>> auth = IMCAuth(\"http://\", \"10.101.0.203\", \"8080\", \"admin\", \"admin\")\n\n    >>> success_delete_operator = delete_plat_operator('testadmin', auth.creds, auth.url)\n\n    >>> assert type(success_delete_operator) is int\n\n    >>> assert success_delete_operator == 204\n\n    >>> fail_delete_operator = delete_plat_operator('testadmin', auth.creds, auth.url)\n\n    >>> assert type(fail_delete_operator) is int\n\n    >>> assert fail_delete_operator == 409\n\n    \"\"\"", "output": " import *\n\n    >>> delete_plat_operator('get_password', auth, 'http://localhost:8080/api/v1')\n    200\n    \"\"\"\n    if not isinstance(operator, str):\n        raise TypeError('The parameter \"operator\" must be a string.')\n\n    auth.creds = auth.creds or auth.url\n\n    return delete_request(auth, url, operator, 'DELETE')\n\n\ndef delete_request(auth, url, method, data=None, action=None):\n    \"\"\"\n    Function to delete an IMC RS request\n    :param auth: requests auth object #usually auth.url", "solution": "\n    oper_id = None\n    plat_oper_list = get_plat_operator(auth, url)\n    for i in plat_oper_list:\n        if operator == i['name']:\n            oper_id = i['id']\n        else:\n            oper_id = None\n    if oper_id is None:\n        # print (\"User does not exist\")\n        return 409\n    f_url = url + \"/imcrs/plat/operator/\" + str(oper_id)\n    response = requests.delete(f_url, auth=auth, headers=HEADERS)\n    try:\n        if response.status_code == 204:\n            # print(\"Operator: \" + operator +\n            #  \" was successfully deleted\")\n            return response.status_code\n    except requests.exceptions.RequestException as error:\n        return \"Error:\\n\" + str(error) + ' delete_plat_operator: An Error has occured'"}
{"prompt": "def bait(self):\n        \"\"\"\n        Use bbduk to bait FASTQ reads from input files using the primer file as the target\n        \"\"\"", "output": "\n        return self.run_command('bbduk', self.primer_file)\n\n    def bait_f(self, f):\n        \"\"\"\n        Use bbduk to bait FASTQ reads from the primer file\n        \"\"\"\n        return self.run_command('bbduk', f)\n\n    def bait_g(self, g):\n        \"\"\"\n        Use bbduk to bait GFF3 reads from the primer file\n        \"\"\"\n        return self.run_command('bbduk', self.gff3_file)\n\n    def bait_g_f(self,", "solution": "\n        with progressbar(self.metadata) as bar:\n            for sample in bar:\n                if sample.general.bestassemblyfile != 'NA':\n                    # Only need to perform baiting on FASTQ files\n                    if sample[self.analysistype].filetype == 'fastq':\n                        # Make the system call - allow for single- or paired-end reads\n                        if len(sample.general.fastqfiles) == 2:\n                            # Create the command to run the baiting - ref: primer file, k: shortest primer length\n                            # in1, in2: paired inputs, hdist: number of mismatches, interleaved: use interleaved output\n                            # outm: single, zipped output file of reads that match the target file\n                            sample[self.analysistype].bbdukcmd = \\\n                                'bbduk.sh ref={primerfile} k={klength} in1={forward} in2={reverse} ' \\\n                                'hdist={mismatches} threads={threads} interleaved=t outm={outfile}' \\\n                                .format(primerfile=self.formattedprimers,\n                                        klength=self.klength,\n                                        forward=sample.general.trimmedcorrectedfastqfiles[0],\n                                        reverse=sample.general.trimmedcorrectedfastqfiles[1],\n                                        mismatches=self.mismatches,\n                                        threads=str(self.cpus),\n                                        outfile=sample[self.analysistype].baitedfastq)\n                        else:\n                            sample[self.analysistype].bbdukcmd = \\\n                                'bbduk.sh ref={primerfile} k={klength} in={fastq} hdist={mismatches} ' \\\n                                'threads={threads} interleaved=t outm={outfile}' \\\n                                .format(primerfile=self.formattedprimers,\n                                        klength=self.klength,\n                                        fastq=sample.general.trimmedcorrectedfastqfiles[0],\n                                        mismatches=self.mismatches,\n                                        threads=str(self.cpus),\n                                        outfile=sample[self.analysistype].baitedfastq)\n                        # Run the system call (if necessary)\n                        if not os.path.isfile(sample[self.analysistype].baitedfastq):\n                            run_subprocess(sample[self.analysistype].bbdukcmd)"}
{"prompt": "def profile_stats(adapter, threshold = 0.9):\n\n    \"\"\"\n        Compares the pairwise hamming distances for all the sample profiles in\n        the database. Returns a table of the number of distances within given\n        ranges.\n\n        Args:\n            adapter (MongoAdapter): Adapter to mongodb\n            threshold (float): If any distance is found above this threshold\n                a warning will be given, stating the two matching samples.\n\n        Returns:\n            distance_dict (dict): dictionary with ranges as keys, and the number\n                of distances that are within these ranges as values.\n\n    \"\"\"", "output": "\n\n    # Get the number of samples\n    n = adapter.find_one(\n       'stats',\n        {\n            'type': 'table',\n            'database':'stats',\n            'collection':'stats',\n            'threshold': threshold\n        }\n    )\n\n    # Get the number of matches\n    n_matches = adapter.find(\n       'stats',\n        {\n            'type': 'table',\n            'database':'matches',\n            'collection':'matches',\n            'n':'stats_n'\n        }\n    )\n\n    # Get the number of matches\n    n_matches = adapter.find(\n       'stats',", "solution": "\n    profiles = []\n    samples = []\n\n    #Instatiate the distance dictionary with a count 0 for all the ranges\n    distance_dict = {key: 0 for key in HAMMING_RANGES.keys()}\n\n    for case in adapter.cases():\n\n        for individual in case['individuals']:\n\n            if individual.get('profile'):\n                #Make sample name <case_id>.<sample_id>\n                sample_id = f\"{case['case_id']}.{individual['ind_id']}\"\n                ind_profile = individual['profile']\n\n                #Numpy array to hold all the distances for this samples profile\n                distance_array = np.array([], dtype=np.float)\n\n                for sample, profile in zip(samples, profiles):\n\n                    #Get distance and append to distance array\n                    distance = compare_profiles(ind_profile, profile)\n                    distance_array = np.append(distance_array, distance)\n\n                    #Issue warning if above threshold\n                    if distance >= threshold:\n                        LOG.warning(f\"{sample_id} is {distance} similar to {sample}\")\n\n                #Check number of distances in each range and add to distance_dict\n                for key,range in HAMMING_RANGES.items():\n\n                    #Calculate the number of hamming distances found within the\n                    #range for current individual\n                    distance_dict[key] += np.sum(\n                        (distance_array >= range[0]) & (distance_array < range[1])\n                    )\n\n                #Append profile and sample_id for this sample for the next\n                #iteration\n                profiles.append(ind_profile)\n                samples.append(sample_id)\n\n    return distance_dict"}
{"prompt": "def _settings_part(self):\n        \"\"\"\n        A |SettingsPart| object providing access to the document-level\n        settings for this document. Creates a default settings part if one is\n        not present.\n        \"\"\"", "output": "\n        if not self.settings:\n            self.settings = Settings()\n        return self.settings\n\n    def _get_settings(self):\n        \"\"\"\n        A |Settings| object providing access to the document-level\n        settings for this document.\n        \"\"\"\n        if not self.settings:\n            self.settings = Settings()\n        return self.settings\n\n    def _get_text(self):\n        \"\"\"\n        A |Text| object providing access to the document-level\n        text for this document.\n        \"\"\"\n        if not self.text:\n            self.text = Text()\n        return self.text\n\n    def _get_text_", "solution": "\n        try:\n            return self.part_related_by(RT.SETTINGS)\n        except KeyError:\n            settings_part = SettingsPart.default(self.package)\n            self.relate_to(settings_part, RT.SETTINGS)\n            return settings_part"}
{"prompt": "def _self_time(self):\n    \"\"\"Returns the time spent in this workunit outside of any children.\"\"\"", "output": "\n    return self._self_time_in_seconds\n\n  def _set_time(self, t):\n    \"\"\"Sets the time spent in this workunit.\"\"\"\n    self._self_time_in_seconds = t\n\n  time = property(_get_time, _set_time, doc=\"\"\"\\\n      Retrieves the time in seconds for the given task.\n\n    :type: int\n    \"\"\")\n\n  def _get_time_in_seconds(self):\n    \"\"\"Returns the time in seconds for the given task.\"\"\"\n    return self._self_time_in_seconds\n\n  def _set_time_in_seconds(", "solution": "\n    return self.duration() - sum([child.duration() for child in self.children])"}
{"prompt": "def from_tibiadata(cls, content):\n        \"\"\"Parses the content of the World Overview section from TibiaData.com into an object of this class.\n\n        Notes\n        -----\n        Due to TibiaData limitations, :py:attr:`record_count` and :py:attr:`record_date` are unavailable\n        object.\n\n        Additionally, the listed worlds in :py:attr:`worlds` lack some information when obtained from TibiaData.\n        The following attributes are unavailable:\n\n        - :py:attr:`ListedWorld.status` is always ``Online``.\n        - :py:attr:`ListedWorld.battleye_protected` is always ``False``\n        - :py:attr:`ListedWorld.battleye_date` is always ``None``.\n\n\n        Parameters\n        ----------\n        content: :class:`str`\n            The JSON response of the worlds section in TibiaData.com\n\n        Returns\n        -------\n        :class:`WorldOverview`\n            An instance of this class containing only the available worlds.\n\n        Raises\n        ------\n        InvalidContent\n            If the provided content is the json content of the world section in TibiaData.com\n        \"\"\"", "output": " - :py:attr:`ListedWorld.date` is ``None``.\n        \"\"\"\n        status = content.split()[0]\n        if status == 'Online':\n            return cls(content=content)\n        elif status == 'Offline':\n            return cls(content=content)\n        else:\n            raise ValueError('Unknown Overview status: %s' % status)\n\n    def to_tibiadata(self):\n        \"\"\"Returns the :py:class:`World` object as a :py:class:`TibiaData` object.\n\n        Notes\n        -----\n        Due to TibiaData limitations, :py:", "solution": "\n        json_data = parse_json(content)\n        try:\n            worlds_json = json_data[\"worlds\"][\"allworlds\"]\n            world_overview = cls()\n            for world_json in worlds_json:\n                world = ListedWorld(world_json[\"name\"], world_json[\"location\"], world_json[\"worldtype\"])\n                world._parse_additional_info(world_json[\"additional\"])\n                world.online_count = world_json[\"online\"]\n                world_overview.worlds.append(world)\n            return world_overview\n        except KeyError:\n            raise InvalidContent(\"content is not a worlds json response from TibiaData.com.\")"}
{"prompt": "def from_name(cls, name):\n        \"\"\" Retrieve a disk id associated to a name. \"\"\"", "output": "\n        if name == cls.DISK_ID:\n            return cls.DISK_ID\n        if name == cls.DISK_SIZE:\n            return cls.disk_size\n        if name == cls.DISK_FREE:\n            return cls.disk_free\n        if name == cls.DISK_TYPE:\n            return cls.disk_type\n        if name == cls.DISK_FREE_SPACE:\n            return cls.disk_free_space\n        if name == cls.DISK_TYPE_SPARSE:\n            return cls.disk_type_sparse\n        if name == cls.DISK_TYPE_SPARSE_SPARSE:\n           ", "solution": "\n        disks = cls.list({'name': name})\n        if len(disks) == 1:\n            return disks[0]['id']\n        elif not disks:\n            return\n\n        raise DuplicateResults('disk name %s is ambiguous.' % name)"}
{"prompt": "def add_variables_from_file(self, file_path):\n        \"\"\"\n        Adds all OpenFisca variables contained in a given file to the tax and benefit system.\n        \"\"\"", "output": "\n        self.tax_benefit_system.add_variables_from_file(file_path)\n        self.benefit_system.add_variables_from_file(file_path)\n\n    def add_tax_benefit_system(self, tax_id, benefit_system):\n        \"\"\"\n        Adds all OpenFisca tax-related variables contained in a given OpenFisca ID.\n        \"\"\"\n        self.tax_benefit_system.add_variables_from_taxid(tax_id, benefit_system)\n        self.benefit_", "solution": "\n        try:\n            file_name = path.splitext(path.basename(file_path))[0]\n\n            #  As Python remembers loaded modules by name, in order to prevent collisions, we need to make sure that:\n            #  - Files with the same name, but located in different directories, have a different module names. Hence the file path hash in the module name.\n            #  - The same file, loaded by different tax and benefit systems, has distinct module names. Hence the `id(self)` in the module name.\n            module_name = '{}_{}_{}'.format(id(self), hash(path.abspath(file_path)), file_name)\n\n            module_directory = path.dirname(file_path)\n            try:\n                module = load_module(module_name, *find_module(file_name, [module_directory]))\n            except NameError as e:\n                logging.error(str(e) + \": if this code used to work, this error might be due to a major change in OpenFisca-Core. Checkout the changelog to learn more: <https://github.com/openfisca/openfisca-core/blob/master/CHANGELOG.md>\")\n                raise\n            potential_variables = [getattr(module, item) for item in dir(module) if not item.startswith('__')]\n            for pot_variable in potential_variables:\n                # We only want to get the module classes defined in this module (not imported)\n                if isclass(pot_variable) and issubclass(pot_variable, Variable) and pot_variable.__module__ == module_name:\n                    self.add_variable(pot_variable)\n        except Exception:\n            log.error('Unable to load OpenFisca variables from file \"{}\"'.format(file_path))\n            raise"}
{"prompt": "def _validate_page(self):\n        \"\"\"\n        Run PageValidator on the PAGE-XML documents referenced in the METS.\n        \"\"\"", "output": "\n        # TODO: We should probably validate the entire document here and just return the XML?\n        #       We can't use etree.iterparse because we need to support\n        #       Python 2.6\n        #       And we need to support 2.7\n        #       For Python 2.6, we need to parse the XML using etree.\n        #       That will require a lot of testing.\n        # TODO: Test the whole document.\n        pass\n\n    def _check_element(self, xml_str, tag_name, text):\n        \"\"\"\n        Check that the XML matches the given tag.\n\n        Arguments:\n            xml", "solution": "\n        for ocrd_file in self.mets.find_files(mimetype=MIMETYPE_PAGE, local_only=True):\n            self.workspace.download_file(ocrd_file)\n            page_report = PageValidator.validate(ocrd_file=ocrd_file, strictness=self.page_strictness)\n            self.report.merge_report(page_report)"}
{"prompt": "def getreference(self, validate=True):\n        \"\"\"Return and validate the Phonetic Content's reference. Raises UnresolvableTextContent when invalid\"\"\"", "output": "\n        if validate:\n            self.validate()\n        return self.get_text_record()\n\n    def get_reference_list(self):\n        \"\"\"Return a list of all references in the Phonetic Content.\"\"\"\n        return self.getreference()\n\n    def validate(self):\n        \"\"\"Check the validity of the content.\"\"\"\n        errors = []\n        if self.is_valid_record():\n            errors.append(self.get_text_record())\n        return errors\n\n    def get_text_record(self):\n        \"\"\"Return a list of all text records in the Phonetic Content.\"\"\"\n        return self.text_records\n\n   ", "solution": "\n\n        if self.offset is None: return None #nothing to test\n        if self.ref:\n            ref = self.doc[self.ref]\n        else:\n            ref = self.finddefaultreference()\n\n        if not ref:\n            raise UnresolvableTextContent(\"Default reference for phonetic content not found!\")\n        elif not ref.hasphon(self.cls):\n            raise UnresolvableTextContent(\"Reference has no such phonetic content (class=\" + self.cls+\")\")\n        elif validate and self.phon() != ref.textcontent(self.cls).phon()[self.offset:self.offset+len(self.data[0])]:\n            raise UnresolvableTextContent(\"Reference (class=\" + self.cls+\") found but no phonetic match at specified offset (\"+str(self.offset)+\")! Expected '\" + self.text() + \"', got '\" + ref.textcontent(self.cls).text()[self.offset:self.offset+len(self.data[0])] +\"'\")\n        else:\n            #finally, we made it!\n            return ref"}
{"prompt": "def detect(self):\n        \"\"\"Detect all currently known devices. Returns the root device.\"\"\"", "output": "\n        return self._devices\n\n    def get_device_name(self, device):\n        \"\"\"Return the name of the given device or None if we don't know.\"\"\"\n        return self._devices[device][\"name\"]\n\n    def get_extra_attributes(self, device):\n        \"\"\"Return the device specific attributes.\"\"\"\n        return self._devices[device][\"attributes\"]\n\n    def get_extra_state_attributes(self, device):\n        \"\"\"Return device specific state attributes.\"\"\"\n        return self._devices[device][\"attributes\"]\n\n    def get_extra_attributes_by_name(self, device, name):\n        \"\"\"Return device", "solution": "\n        root = self._actions.detect()\n        prune_empty_node(root, set())\n        return root"}
{"prompt": "def _derive_stereographic():\n    \"\"\"Compute the formulae to cut-and-paste into the routine below.\"\"\"", "output": "\n    # The following two lines are to be removed:\n    #\n    # - The following two lines are to be replaced by the above\n    # - The following two lines are to be removed by the above\n    #\n    # - The following two lines are to be replaced by the above\n    # - The following two lines are to be removed by the above\n    #\n    # - The following two lines are to be removed by the above\n    # - The following two lines are to be removed by the above\n    #\n    # - The following two lines are to be removed by the above\n    # - The following two lines are to be removed", "solution": "\n    from sympy import symbols, atan2, acos, rot_axis1, rot_axis3, Matrix\n    x_c, y_c, z_c, x, y, z = symbols('x_c y_c z_c x y z')\n\n    # The angles we'll need to rotate through.\n    around_z = atan2(x_c, y_c)\n    around_x = acos(-z_c)\n\n    # Apply rotations to produce an \"o\" = output vector.\n    v = Matrix([x, y, z])\n    xo, yo, zo = rot_axis1(around_x) * rot_axis3(-around_z) * v\n\n    # Which we then use the stereographic projection to produce the\n    # final \"p\" = plotting coordinates.\n    xp = xo / (1 - zo)\n    yp = yo / (1 - zo)\n\n    return xp, yp"}
{"prompt": "def _ixs(self, i, axis=0):\n        \"\"\"\n        Parameters\n        ----------\n        i : int, slice, or sequence of integers\n        axis : int\n        \"\"\"", "output": "\n        if isinstance(i, slice):\n            return self.__getitem__(i)\n        elif isinstance(i, int):\n            return self.reindex(self.index[axis])\n        else:\n            raise TypeError('invalid index type: %s' % type(i))\n\n    def _get_slice_axis(self, slice_obj, axis=0):\n        \"\"\"\n        Get the axis of concatenation (appending) along axis.\n\n        Parameters\n        ----------\n        slice_obj : sequence of slice, int, or sequence of integers\n        axis : int\n        \"\"\"\n        if isinstance(slice_obj, int):\n            return slice_", "solution": "\n\n        ax = self._get_axis(axis)\n        key = ax[i]\n\n        # xs cannot handle a non-scalar key, so just reindex here\n        # if we have a multi-index and a single tuple, then its a reduction\n        # (GH 7516)\n        if not (isinstance(ax, MultiIndex) and isinstance(key, tuple)):\n            if is_list_like(key):\n                indexer = {self._get_axis_name(axis): key}\n                return self.reindex(**indexer)\n\n        # a reduction\n        if axis == 0:\n            values = self._data.iget(i)\n            return self._box_item_values(key, values)\n\n        # xs by position\n        self._consolidate_inplace()\n        new_data = self._data.xs(i, axis=axis, copy=True, takeable=True)\n        return self._construct_return_type(new_data)"}
{"prompt": "def find_dependencies(self, dataset_keys, **dfilter):\n        \"\"\"Create the dependency tree.\n\n        Args:\n            dataset_keys (iterable): Strings or DatasetIDs to find dependencies for\n            **dfilter (dict): Additional filter parameters. See\n                              `satpy.readers.get_key` for more details.\n\n        Returns:\n            (Node, set): Root node of the dependency tree and a set of unknown datasets\n\n        \"\"\"", "output": "\n        # TODO: This is a temporary solution, but it is not used by the\n        #       `readers.get_key` method.\n        dfilter = copy.deepcopy(dfilter)\n        dfilter.update(dfilter)\n        nodes = set()\n        for key in dataset_keys:\n            node = self.add_node(key)\n            nodes.add(node)\n        return (nodes, set())\n\n    def get_node(self, key):\n        \"\"\"Get the node of the dependency tree.\n\n        Args:\n            key (str): String of the key to find the node of.\n\n        Returns:", "solution": "\n        unknown_datasets = set()\n        for key in dataset_keys.copy():\n            n, unknowns = self._find_dependencies(key, **dfilter)\n\n            dataset_keys.discard(key)  # remove old non-DatasetID\n            if n is not None:\n                dataset_keys.add(n.name)  # add equivalent DatasetID\n            if unknowns:\n                unknown_datasets.update(unknowns)\n                continue\n\n            self.add_child(self, n)\n\n        return unknown_datasets"}
{"prompt": "def _element_to_node(self, node, name, value):\n        \"\"\" Insert the parsed element (``name``, ``value`` pair) into the node.\n\n        You should always use the returned node and forget the one\n        that was given in parameter.\n\n        :param node: the node where the is added to\n        :returns: the node. Note that this may be a new node instance.\n        \"\"\"", "output": "\n        if isinstance(node, Node):\n            node.name = name\n            node.value = value\n            return node\n        elif isinstance(node, list):\n            return NodeList(node)\n        elif isinstance(node, dict):\n            return NodeDict(node)\n        else:\n            raise TypeError(\"Unknown type of node: %s\" % type(node))\n\n    def _parse_node_list(self, node_list):\n        \"\"\" Parse a list of ``Node`` objects.\n\n        :param node_list: a list of ``Node`` objects\n        :returns: a list of ``Node`` objects\n        \"\"\"\n        return", "solution": "\n\n        # is the target node a list?\n        try:\n            node.append(value)\n        except AttributeError:\n            pass\n        else:\n            return node\n\n        # target node is a dict\n        if name in node:\n            # there's already an element with same name -> convert the node into list\n            node = node.values() + [value]\n        else:\n            # just add the value into the node\n            node[name] = value\n        return node"}
{"prompt": "def _add_pos_args(self, *args):\n        \"\"\"\n        Set values for *positional* placeholders (``$1,$2,...``)\n\n        :param args: Values to be used\n        \"\"\"", "output": "\n        self._pos_args = args\n        self._pos_kwargs = args\n\n    def _get_pos_kwargs(self):\n        \"\"\"\n        Return values for *pos_args* and *pos_kwargs* as a dictionary.\n        \"\"\"\n        return self._pos_kwargs\n\n    def _set_pos_args(self, pos_args, pos_kwargs):\n        \"\"\"\n        Set values for *positional* placeholders (``$1,$2,...``)\n\n        :param pos_args: Values to be used\n        :param pos_kwargs: Values for *pos_args* and *pos_kwargs*\n        \"\"\"\n       ", "solution": "\n        arg_array = self._body.setdefault('args', [])\n        arg_array.extend(args)"}
{"prompt": "def add_read_groups(job, job_vars):\n    \"\"\"\n    This function adds read groups to the headers\n\n    job_vars: tuple     Tuple of dictionaries: input_args and ids\n    \"\"\"", "output": "\n    if job.id not in job_vars:\n        job_vars[job.id] = {}\n    for arg in job_vars[job.id]:\n        if arg['id'] not in job.input_args:\n            job.input_args[arg['id']] = []\n        if arg['input_id'] not in job.ids:\n            job.ids[arg['id']] = []\n        if arg['id'] not in job.ids[arg_id]:\n            job.ids[arg_id][arg['input_id']] = []\n        if arg['input_id'] not", "solution": "\n    input_args, ids = job_vars\n    work_dir = job.fileStore.getLocalTempDir()\n    sudo = input_args['sudo']\n    # I/O\n    alignments = return_input_paths(job, work_dir, ids, 'alignments.bam')\n    output = os.path.join(work_dir, 'rg_alignments.bam')\n    # Command and callg\n    parameter = ['AddOrReplaceReadGroups',\n                 'INPUT={}'.format(docker_path(alignments)),\n                 'OUTPUT={}'.format(docker_path(output)),\n                 'RGSM={}'.format(input_args['uuid']),\n                 'RGID={}'.format(input_args['uuid']),\n                 'RGLB=TruSeq',\n                 'RGPL=illumina',\n                 'RGPU=barcode',\n                 'VALIDATION_STRINGENCY=SILENT']\n    docker_call(tool='quay.io/ucsc_cgl/picardtools:1.95--dd5ac549b95eb3e5d166a5e310417ef13651994e',\n                tool_parameters=parameter, work_dir=work_dir, sudo=sudo)\n    # Write to FileStore\n    ids['rg_alignments.bam'] = job.fileStore.writeGlobalFile(output)\n    # Run child job\n    return job.addChildJobFn(bamsort_and_index, job_vars, disk='30 G').rv()"}
{"prompt": "def format_registryfield_nodes(field_name, field, field_id, state, lineno):\n    \"\"\"Create a section node that documents a RegistryField config field.\n\n    Parameters\n    ----------\n    field_name : `str`\n        Name of the configuration field (the attribute name of on the config\n        class).\n    field : ``lsst.pex.config.RegistryField``\n        A configuration field.\n    field_id : `str`\n        Unique identifier for this field. This is used as the id and name of\n        the section node. with a -section suffix\n    state : ``docutils.statemachine.State``\n        Usually the directive's ``state`` attribute.\n    lineno (`int`)\n        Usually the directive's ``lineno`` attribute.\n\n    Returns\n    -------\n    ``docutils.nodes.section``\n        Section containing documentation nodes for the RegistryField.\n    \"\"\"", "output": "``\n        The current state.\n    lineno : int\n        The line number in the source file where the config class is defined.\n\n    Returns\n    -------\n    docutils.nodes.section\n        The section node.\n    \"\"\"\n    node = docutils.nodes.section()\n    node.document = self.state.document\n    node.settings = self.state.settings\n    node.settings.env.note_dependency(line=lineno, node=self.env)\n    return node\n\n  def format_node_list(self, node_list):\n    \"\"\"Create a list of section nodes.\n\n    Parameters\n    ----------\n    node_list : `", "solution": "\n    from lsst.pex.config.registry import ConfigurableWrapper\n\n    # Create a definition list for the choices\n    # This iteration is over field.registry.items(), not field.items(), so\n    # that the directive shows the configurables, not their ConfigClasses.\n    choice_dl = nodes.definition_list()\n    for choice_value, choice_class in field.registry.items():\n        # Introspect the class name from item in the registry. This is harder\n        # than it should be. Most registry items seem to fall in the first\n        # category. Some are ConfigurableWrapper types that expose the\n        # underlying task class through the _target attribute.\n        if hasattr(choice_class, '__module__') \\\n                and hasattr(choice_class, '__name__'):\n            name = '.'.join((choice_class.__module__, choice_class.__name__))\n        elif isinstance(choice_class, ConfigurableWrapper):\n            name = '.'.join((choice_class._target.__class__.__module__,\n                             choice_class._target.__class__.__name__))\n        else:\n            name = '.'.join((choice_class.__class__.__module__,\n                             choice_class.__class__.__name__))\n\n        item = nodes.definition_list_item()\n        item_term = nodes.term()\n        item_term += nodes.literal(text=repr(choice_value))\n        item += item_term\n        item_definition = nodes.definition()\n        def_para = nodes.paragraph()\n        def_para += pending_task_xref(rawsource=name)\n        item_definition += def_para\n        item += item_definition\n        choice_dl.append(item)\n\n    choices_node = nodes.definition_list_item()\n    choices_node.append(nodes.term(text='Choices'))\n    choices_definition = nodes.definition()\n    choices_definition.append(choice_dl)\n    choices_node.append(choices_definition)\n\n    # Field type\n    field_type_item = nodes.definition_list_item()\n    field_type_item.append(nodes.term(text=\"Field type\"))\n    field_type_item_content = nodes.definition()\n    field_type_item_content_p = nodes.paragraph()\n    if field.multi:\n        multi_text = \"Multi-selection \"\n    else:\n        multi_text = \"Single-selection \"\n    field_type_item_content_p += nodes.Text(multi_text, multi_text)\n    field_type_item_content_p += make_python_xref_nodes_for_type(\n        type(field),\n        state,\n        hide_namespace=True)[0].children[0]\n    if field.optional:\n        field_type_item_content_p += nodes.Text(' (optional)', ' (optional)')\n    field_type_item_content += field_type_item_content_p\n    field_type_item += field_type_item_content\n\n    dl = nodes.definition_list()\n    dl += create_default_item_node(field, state)\n    dl += field_type_item\n    dl += choices_node\n\n    # Doc for this field, parsed as rst\n    desc_node = create_description_node(field, state)\n\n    # Title for configuration field\n    title = create_title_node(field_name, field, field_id, state, lineno)\n\n    return [title, dl, desc_node]"}
{"prompt": "def plot(data, output_dir_path='.', width=10, height=8):\n    \"\"\"Create two plots: 1) loss 2) accuracy.\n        Args:\n            data: Panda dataframe in *the* format.\n    \"\"\"", "output": "\n    plt.figure(figsize=(width, height))\n    plt.plot(data.index, data.loss, 'ro', markersize=10)\n    plt.plot(data.index, data.accuracy, 'b', markersize=10)\n    plt.xlabel('Time (s)')\n    plt.ylabel('Accuracy (m)')\n    plt.title('loss vs accuracy')\n    plt.legend(loc='best')\n    plt.savefig(output_dir_path + 'plots.png')\n\n\ndef main():\n    \"\"\"Create two plots: 1) loss 3) accuracy.\n        Args:\n           ", "solution": "\n    if not isinstance(data, pd.DataFrame):\n        data = pd.DataFrame(data)\n    plot_accuracy(data, output_dir_path=output_dir_path,\n                  width=width, height=height)\n    plot_loss(data, output_dir_path, width=width, height=height)"}
{"prompt": "def delete(self, obj, force=False):\n        \"\"\"Deletes all of the fields at the specified locations.\n\n        args:\n\n            ``obj=``\\ *OBJECT*\n                the object to remove the fields from\n\n            ``force=``\\ *BOOL*\n                if True, missing attributes do not raise errors.  Otherwise,\n                the first failure raises an exception without making any\n                changes to ``obj``.\n        \"\"\"", "output": "\n        if force:\n            self.force_delete(obj)\n        else:\n            self.delete_all(obj)\n\n    def force_delete(self, obj):\n        \"\"\"Forcefully deletes the specified object.\n\n        args:\n\n            ``obj=``\\ *OBJECT*\n                the object to remove the fields from\n\n            ``force=``\\ *BOOL*\n                if True, missing attributes do not raise errors.  Otherwise,\n                the first failure raises an exception without making any\n                changes to ``obj``.\n        \"\"\"\n        if self.db_type =='mysql':\n            self.db_type ='mysql'\n            self.db", "solution": "\n        # TODO: this could be a whole lot more efficient!\n        if not force:\n            for fs in self:\n                try:\n                    fs.get(obj)\n                except FieldSelectorException:\n                    raise\n\n        for fs in self:\n            try:\n                fs.delete(obj)\n            except FieldSelectorException:\n                pass"}
{"prompt": "def actuator_control_target_send(self, time_usec, group_mlx, controls, force_mavlink1=False):\n                \"\"\"\n                Set the vehicle attitude and body angular rates.\n\n                time_usec                 : Timestamp (micros since boot or Unix epoch) (uint64_t)\n                group_mlx                 : Actuator group. The \"_mlx\" indicates this is a multi-instance message and a MAVLink parser should use this field to difference between instances. (uint8_t)\n                controls                  : Actuator controls. Normed to -1..+1 where 0 is neutral position. Throttle for single rotation direction motors is 0..1, negative range for reverse direction. Standard mapping for attitude controls (group 0): (index 0-7): roll, pitch, yaw, throttle, flaps, spoilers, airbrakes, landing gear. Load a pass-through mixer to repurpose them as generic outputs. (float)\n\n                \"\"\"", "output": " is normal, 1 is linear, 2 is quadratic.\n                force_mavlink1                 : Force the MAVLink version to be 1.0 or 0.0 (bool)\n                \"\"\"\n                self.attitude.attitude = time_usec\n                self.body.body.angular_velocity = -1.0 if not force_mavlink1 else 1.0 if not force_mavlink1 else 0.0\n                self.attitude.body.timestamp = self.body.body.attitude\n                self.attitude.body.timestamp_type = time_usec.header.timestamp_type\n                self.", "solution": "\n                return self.send(self.actuator_control_target_encode(time_usec, group_mlx, controls), force_mavlink1=force_mavlink1)"}
{"prompt": "def _restore_output(self, statement: Statement, saved_state: utils.RedirectionSavedState) -> None:\n        \"\"\"Handles restoring state after output redirection as well as\n        the actual pipe operation if present.\n\n        :param statement: Statement object which contains the parsed input from the user\n        :param saved_state: contains information needed to restore state data\n        \"\"\"", "output": "\n        if not statement.is_valid():\n            raise utils.InvalidStatement(\n                \"Invalid output statement: {}\".format(statement.get_description())\n            )\n\n        if saved_state.is_error:\n            raise utils.ErrorStatement(\n                \"Error while restoring state: {}\".format(saved_state.get_description())\n            )\n\n        saved_state.restore()\n\n    def _get_output_stream(self, output_stream_name: str) -> Optional[Optional[Stream]]:\n        \"\"\"Returns the stream to write the output to.\n\n        :param output_stream_name: Name of the output", "solution": "\n        if saved_state.redirecting:\n            # If we redirected output to the clipboard\n            if statement.output and not statement.output_to:\n                self.stdout.seek(0)\n                write_to_paste_buffer(self.stdout.read())\n\n            try:\n                # Close the file or pipe that stdout was redirected to\n                self.stdout.close()\n            except BrokenPipeError:\n                pass\n\n            # Restore the stdout values\n            self.stdout = saved_state.saved_self_stdout\n            sys.stdout = saved_state.saved_sys_stdout\n\n            # Check if we need to wait for the process being piped to\n            if self.cur_pipe_proc_reader is not None:\n                self.cur_pipe_proc_reader.wait()\n\n        # Restore cur_pipe_proc_reader. This always is done, regardless of whether this command redirected.\n        self.cur_pipe_proc_reader = saved_state.saved_pipe_proc_reader"}
{"prompt": "def logical_or(f1, f2):  # function factory\r\n    \"\"\"Logical or from functions.\r\n\r\n    Parameters\r\n    ----------\r\n    f1, f2 : function\r\n        Function that takes array and returns true or false for each item in array.\r\n\r\n    Returns\r\n    -------\r\n    Function.\r\n    \"\"\"", "output": "\r\n    return lambda x, y: x or y\r\n    # end function_or\r\n\r\n# end class _SpectralShape\r\n\r\nclass _SpectralShape(Shape):\r\n    \"\"\"The base class for spectral shapes.\r\n\r\n    This class is used to represent a shape that is a spectral shape.\r\n    \"\"\"\r\n    def __init__(self, **kwargs):\r\n        super(_SpectralShape, self).__init__(**kwargs)\r\n    # end function_or\r\n\r\n# end class _SpectralShape\r\n\r\n\r\nclass _Spectral(Shape):\r\n    \"\"\"The base class for spectral objects.\r\n\r\n    This class is used to represent a spectral object.", "solution": "\r\n    def f(value):\r\n        return np.logical_or(f1(value), f2(value))\r\n    f.__name__ = \"(\" + f1.__name__ + \"_or_\" + f2.__name__ + \")\"\r\n    return f"}
{"prompt": "def star(self, **args):\n\t\t\"\"\"\n\t\tstar any gist by providing gistID or gistname(for authenticated user)\n\t\t\"\"\"", "output": "\n\t\tif len(args) == 0:\n\t\t\traise ArgumentError(\"Missing argument\")\n\t\tif len(args) == 1:\n\t\t\tgistID = args[0]\n\t\telse:\n\t\t\tgistID = args[1]\n\t\tif not args:\n\t\t\traise ArgumentError(\"Missing argument\")\n\t\tif len(args) == 2:\n\t\t\tgistName = args[0]\n\t\telse:\n\t\t\tgistName = args[2]\n\t\tif not args:\n\t\t\traise ArgumentError(\"Missing argument\")\n\t\tif len(args) == 3:\n\t\t", "solution": "\n\t\tif 'name' in args:\n\t\t\tself.gist_name = args['name']\n\t\t\tself.gist_id = self.getMyID(self.gist_name)\n\t\telif 'id' in args:\n\t\t\tself.gist_id = args['id']\n\t\telse:\n\t\t\traise Exception('Either provide authenticated user\\'s Unambigious Gistname or any unique Gistid to be starred')\n\n\t\tr = requests.put(\n\t\t\t'%s'%BASE_URL+'/gists/%s/star' % self.gist_id,\n\t\t\theaders=self.gist.header\n\t\t\t)\n\t\tif (r.status_code == 204):\n\t\t\tresponse = {\n\t\t\t\t'id': self.gist_id\n\t\t\t}\n\t\t\treturn response\n\n\t\traise Exception('Gist can\\'t be starred')"}
{"prompt": "def create_experiment(self, workflow_type, microscope_type, plate_format,\n            plate_acquisition_mode):\n        \"\"\"Creates the experiment.\n\n        Parameters\n        ----------\n        workflow_type: str\n            workflow type\n        microscope_type: str\n            microscope type\n        plate_format: int\n            well-plate format, i.e. total number of wells per plate\n        plate_acquisition_mode: str\n            mode of image acquisition that determines whether acquisitions will\n            be interpreted as time points as part of a time series experiment\n            or as multiplexing cycles as part of a serial multiplexing\n            experiment\n\n        Returns\n        -------\n        dict\n            experiment resource representation\n\n        See also\n        --------\n        :func:`tmserver.api.experiment.create_experiment`\n        :class:`tmlib.models.experiment.ExperimentReference`\n        :class:`tmlib.models.experiment.Experiment`\n        \"\"\"", "output": ".\n\n        Returns\n        -------\n        image_acquisition : :class:`ImageAcquisition`\n            image acquisition\n        \"\"\"\n        if workflow_type == 'image':\n            return ImageAcquisition(self.image_data, self.image_metadata,\n                self.image_data_type, self.image_data_metadata_type)\n        elif workflow_type == 'plate':\n            return PlateAcquisition(self.plate_data, self.plate_metadata,\n                self.plate_data_type, self.plate_data_metadata_type)\n        elif workflow_type == 'acquisition':\n           ", "solution": "\n        logger.info('create experiment \"%s\"', self.experiment_name)\n        content = {\n            'name': self.experiment_name,\n            'workflow_type': workflow_type,\n            'microscope_type': microscope_type,\n            'plate_format': plate_format,\n            'plate_acquisition_mode': plate_acquisition_mode\n        }\n        url = self._build_api_url('/experiments')\n        res = self._session.post(url, json=content)\n        res.raise_for_status()\n        data = res.json()['data']\n        self._experiment_id = data['id']\n        return data"}
{"prompt": "def dot(self, other, dims=None):\n        \"\"\"Perform dot product of two DataArrays along their shared dims.\n\n        Equivalent to taking taking tensordot over all shared dims.\n\n        Parameters\n        ----------\n        other : DataArray\n            The other array with which the dot product is performed.\n        dims: list of strings, optional\n            Along which dimensions to be summed over. Default all the common\n            dimensions are summed over.\n\n        Returns\n        -------\n        result : DataArray\n            Array resulting from the dot product over all shared dimensions.\n\n        See also\n        --------\n        dot\n        numpy.tensordot\n\n        Examples\n        --------\n\n        >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))\n        >>> da = DataArray(da_vals, dims=['x', 'y', 'z'])\n        >>> dm_vals = np.arange(4)\n        >>> dm = DataArray(dm_vals, dims=['z'])\n\n        >>> dm.dims\n        ('z')\n        >>> da.dims\n        ('x', 'y', 'z')\n\n        >>> dot_result = da.dot(dm)\n        >>> dot_result.dims\n        ('x', 'y')\n        \"\"\"", "output": " = DataArray(np.arange(6).reshape(2, 3), dims=['x', 'y'])\n        >>> da.dot(da)\n        <xarray.DataArray (x: 6)>\n        array([[ 0.,  1.,  2.],\n               [ 1.,  3.,  4.],\n               [ 2.,  4.,  6.],\n               [ 3.,  6.,  9.]])\n        Coordinates:\n        * x        - x coordinates\n        * y        - y coordinates\n\n        >>> da.dims\n        ['x', 'y']\n        >>> da.dims = ['x', 'y']\n        >>>", "solution": "\n        if isinstance(other, Dataset):\n            raise NotImplementedError('dot products are not yet supported '\n                                      'with Dataset objects.')\n        if not isinstance(other, DataArray):\n            raise TypeError('dot only operates on DataArrays.')\n\n        return computation.dot(self, other, dims=dims)"}
{"prompt": "def And(*xs, simplify=True):\n    \"\"\"Expression conjunction (product, AND) operator\n\n    If *simplify* is ``True``, return a simplified expression.\n    \"\"\"", "output": "\n    if not isinstance(xs, list):\n        return [xs]\n    if len(xs) == 1:\n        return xs[0]\n    return reduce(operator.and_, xs)\n\n\ndef Or(*xs):\n    \"\"\"Expression conjunction (product, OR) operator\n\n    If *xs* is a list of ``Expression`` objects, return a single ``Expression``\n    object.\n    \"\"\"\n    if not isinstance(xs, list):\n        return [xs]\n    if len(xs) == 1:\n        return xs[0]\n    return reduce(operator.or_, xs)\n\n\ndef Not(xs):\n   ", "solution": "\n    xs = [Expression.box(x).node for x in xs]\n    y = exprnode.and_(*xs)\n    if simplify:\n        y = y.simplify()\n    return _expr(y)"}
{"prompt": "def load_from_path(self, filepath, container, **opts):\n        \"\"\"\n        :param filepath: XML file path\n        :param container: callble to make a container object\n        :param opts: optional keyword parameters to be sanitized\n\n        :return: Dict-like object holding config parameters\n        \"\"\"", "output": "\n        self.log.debug(\"Loading config file %s\", filepath)\n        try:\n            self.config = container.find('configuration')\n            self.log.debug(\"Found configuration %s\", self.config.text)\n            return self.config\n        except Exception as e:\n            self.log.debug(\"Failed to find configuration %s: %s\", self.config.text, e)\n            return {}\n\n    def save_to_path(self, filepath, xml):\n        \"\"\"\n        :param filepath: XML file path\n        :param xml: The config object to save\n        \"\"\"\n        self.log.debug", "solution": "\n        root = ET.parse(filepath).getroot()\n        nspaces = _namespaces_from_file(filepath)\n        return root_to_container(root, container=container,\n                                 nspaces=nspaces, **opts)"}
{"prompt": "def login():\n    \"\"\"Log in a registered user by adding the user id to the session.\"\"\"", "output": "\n    session['user_id'] = session.get('user_id', None) + 1\n    return redirect(url_for('login'))\n\n\n@app.route('/logout')\n@login_required\ndef logout():\n    \"\"\"Log out of a registered user by removing the user id from the session.\"\"\"\n    session.pop('user_id', None)\n    return redirect(url_for('login'))\n\n\n@app.route('/login', methods=['GET', 'POST'])\ndef login_post():\n    \"\"\"\n    This function is used to log in a user by sending the login form to the\n   ", "solution": "\n    if request.method == \"POST\":\n        username = request.form[\"username\"]\n        password = request.form[\"password\"]\n        error = None\n        user = User.query.filter_by(username=username).first()\n\n        if user is None:\n            error = \"Incorrect username.\"\n        elif not user.check_password(password):\n            error = \"Incorrect password.\"\n\n        if error is None:\n            # store the user id in a new session and return to the index\n            session.clear()\n            session[\"user_id\"] = user.id\n            return redirect(url_for(\"index\"))\n\n        flash(error)\n\n    return render_template(\"auth/login.html\")"}
{"prompt": "def create_issue(self, request, group, form_data, **kwargs):\n        \"\"\"\n        Creates the issue on the remote service and returns an issue ID.\n        \"\"\"", "output": "\n        try:\n            return self.remote_service.create_issue(\n                group_id=group.id,\n                title=form_data.get('title', ''),\n                body=form_data.get('body', ''),\n                assignee_id=request.user.id,\n                milestone_id=request.user.milestone.id,\n                description=form_data.get('description', ''),\n                due_on=self.parse_date(form_data.get('due_on', '2013-12-31')),\n                due_on_str=form_data.get('", "solution": "\n        instance = self.get_option('instance', group.project)\n        project = (\n            form_data.get('project') or\n            self.get_option('default_project', group.project)\n        )\n\n        client = self.get_client(request.user)\n\n        title = form_data['title']\n        description = form_data['description']\n        link = absolute_uri(group.get_absolute_url(params={'referrer': 'vsts_plugin'}))\n        try:\n            created_item = client.create_work_item(\n                instance=instance,\n                project=project,\n                title=title,\n                comment=markdown(description),\n                link=link,\n            )\n        except Exception as e:\n            self.raise_error(e, identity=client.auth)\n\n        return {\n            'id': created_item['id'],\n            'url': created_item['_links']['html']['href'],\n            'title': title,\n        }"}
{"prompt": "def handle_unsubscribe(self, request, path):\n        # type: (Subscribe, List[str]) -> CallbackResponses\n        \"\"\"Remove from the notifier list and send a return\n\n        Args:\n            request (Subscribe): The original subscribe request\n            path (list): The relative path from ourself\n\n        Returns:\n            list: [(callback, Response)] that need to be called\n        \"\"\"", "output": "\n        # We don't want to call the notifier if the path is empty\n        if path == []:\n            return\n\n        # We don't want to call the notifier if the path is not empty\n        if path == []:\n            return\n\n        # We don't want to call the notifier if the path is empty\n        if path == []:\n            return\n\n        # We don't want to call the notifier if the path is empty\n        if path == []:\n            return\n\n        # We don't want to call the notifier if the path is empty\n        if path == []:\n            return\n\n        # We don't want to call the notifier if the path is empty\n        if", "solution": "\n        ret = []\n        if path:\n            # Recurse down\n            name = path[0]\n            child = self.children[name]\n            ret += child.handle_unsubscribe(request, path[1:])\n            if not child.children and not child.update_requests \\\n                    and not child.delta_requests:\n                del self.children[name]\n        else:\n            # This is for us\n            if request in self.update_requests:\n                self.update_requests.remove(request)\n            else:\n                self.delta_requests.remove(request)\n            ret.append(request.return_response())\n        return ret"}
{"prompt": "def rbinomial(n, p, size=None):\n    \"\"\"\n    Random binomial variates.\n    \"\"\"", "output": "\n    if size is None:\n        size = n\n    if size > 1:\n        return [p * (1 - p) + n for n in range(size)]\n    else:\n        return [p * (1 - p) + p * (1 - p) * (1 - p) + p * (1 - p)]\n\ndef rbinomial_n(n, p, size=None):\n    \"\"\"\n    Random binomial variates.\n    \"\"\"\n    if size is None:\n        size = n\n    if size > 1:\n        return [n * (1 - n) + p * (1", "solution": "\n    if not size:\n        size = None\n    return np.random.binomial(np.ravel(n), np.ravel(p), size)"}
{"prompt": "def build_statusbar(self):\n        \"\"\"construct and return statusbar widget\"\"\"", "output": "\n        self.statusbar = gtk.Statusbar()\n        self.statusbar.set_has_resize_grip(False)\n        self.statusbar.set_has_resize_grip(False)\n        self.statusbar.set_has_resize_grip(False)\n        self.statusbar.set_size_request(self.statusbar_width, -1)\n        self.statusbar.set_has_resize_grip(False)\n        self.statusbar.set_no_show_all(True)\n        self.statusbar.set_no_show", "solution": "\n        info = {}\n        cb = self.current_buffer\n        btype = None\n\n        if cb is not None:\n            info = cb.get_info()\n            btype = cb.modename\n            info['buffer_no'] = self.buffers.index(cb)\n            info['buffer_type'] = btype\n        info['total_messages'] = self.dbman.count_messages('*')\n        info['pending_writes'] = len(self.dbman.writequeue)\n        info['input_queue'] = ' '.join(self.input_queue)\n\n        lefttxt = righttxt = u''\n        if cb is not None:\n            lefttxt, righttxt = settings.get(btype + '_statusbar', (u'', u''))\n            lefttxt = string_decode(lefttxt, 'UTF-8')\n            lefttxt = lefttxt.format(**info)\n            righttxt = string_decode(righttxt, 'UTF-8')\n            righttxt = righttxt.format(**info)\n\n        footerleft = urwid.Text(lefttxt, align='left')\n        pending_writes = len(self.dbman.writequeue)\n        if pending_writes > 0:\n            righttxt = ('|' * pending_writes) + ' ' + righttxt\n        footerright = urwid.Text(righttxt, align='right')\n        columns = urwid.Columns([\n            footerleft,\n            ('pack', footerright)])\n        footer_att = settings.get_theming_attribute('global', 'footer')\n        return urwid.AttrMap(columns, footer_att)"}
{"prompt": "def finalize_file(self):\n        # type: (Descriptor) -> None\n        \"\"\"Finalize file for download\n        :param Descriptor self: this\n        \"\"\"", "output": "\n        self.log.info('Downloading %s', self.filename)\n        if self.filename.endswith('.zip'):\n            self.log.info('Decompressing %s', self.filename)\n            zip_file = zipfile.ZipFile(self.filename, 'r')\n            for member in zip_file.namelist():\n                if member.endswith('/'):\n                    continue\n                member_path = os.path.join(self.path, member)\n                if not member_path.endswith('/'):\n                    os.makedirs(os.path.dirname(member_path))\n                with open(member_path, 'wb')", "solution": "\n        # delete bad file if integrity failed\n        if self._integrity_failed:\n            self.final_path.unlink()\n        else:\n            self._restore_file_attributes()\n            self._restore_file_lmt()\n        # update resume file\n        self._update_resume_for_completed()\n        with self._meta_lock:\n            self._finalized = True"}
{"prompt": "def to_api_repr(self):\n        \"\"\"Construct JSON API representation for the parameter.\n\n        :rtype: dict\n        :returns: JSON mapping\n        \"\"\"", "output": "\n        return {\n            'name': self.name,\n            'description': self.description,\n           'required': self.required,\n            'default': self.default,\n            'type': self.type,\n            'default_from': self.default_from,\n            'default_to': self.default_to,\n           'required_from': self.required_from,\n           'required_to': self.required_to,\n            'default_from': self.default_from,\n           'required_to': self.required_to,\n            'default_to': self.default_to,\n           ", "solution": "\n        value = self.value\n        converter = _SCALAR_VALUE_TO_JSON_PARAM.get(self.type_)\n        if converter is not None:\n            value = converter(value)\n        resource = {\n            \"parameterType\": {\"type\": self.type_},\n            \"parameterValue\": {\"value\": value},\n        }\n        if self.name is not None:\n            resource[\"name\"] = self.name\n        return resource"}
{"prompt": "def _filenames_from_arg(filename):\n    \"\"\"Utility function to deal with polymorphic filenames argument.\"\"\"", "output": "\n    if os.path.isdir(filename):\n        filenames = []\n        for filename in sorted(os.listdir(filename)):\n            if filename.endswith(\".py\") and not filename.startswith(\".\"):\n                module_name = filename.split(\".\")[0]\n                module = importlib.import_module(module_name)\n                filenames.append(filename)\n        return filenames\n    else:\n        raise ValueError(\"Unknown filename '%s'\" % filename)\n\n\ndef _get_filename(filename):\n    \"\"\"Return the absolute path of a filename.\"\"\"\n    return os.path.join(os.path.dirname(os.path", "solution": "\n    if isinstance(filename, string_types):\n        filenames = [filename]\n    elif isinstance(filename, (list, tuple)):\n        filenames = filename\n    else:\n        raise Exception('filename argument must be string, list or tuple')\n    for fn in filenames:\n        if not os.path.exists(fn):\n            raise ValueError('file not found: %s' % fn)\n        if not os.path.isfile(fn):\n            raise ValueError('not a file: %s' % fn)\n    return filenames"}
{"prompt": "def listen_on_tcp_port():\n    \"\"\"listen_on_tcp_port\n\n    Run a simple server for processing messages over ``TCP``.\n\n    ``LISTEN_ON_HOST`` - listen on this host ip address\n\n    ``LISTEN_ON_PORT`` - listen on this ``TCP`` port\n\n    ``LISTEN_SIZE`` - listen on to packets of this size\n\n    ``LISTEN_SLEEP`` - sleep this number of seconds per loop\n\n    ``LISTEN_SHUTDOWN_HOOK`` - shutdown if file is found on disk\n\n    \"\"\"", "output": "\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    sock.bind(('', 0))\n    sock.listen(1)\n    return sock\n\n\ndef listen_on_udp_port():\n    \"\"\"listen_on_udp_port\n\n    Run a simple server for processing messages over UDP.\n\n    ``LISTEN_ON_HOST`` - listen on this host ip address\n\n    ``LISTEN_ON_PORT`` - listen on this ``UDP`` port\n\n    ``LISTEN_SLEEP`` - sleep this number of seconds per loop\n\n    ``LISTEN_SHUTDOWN_HOOK``", "solution": "\n\n    host = os.getenv(\n        \"LISTEN_ON_HOST\",\n        \"127.0.0.1\").strip().lstrip()\n    port = int(os.getenv(\n        \"LISTEN_ON_PORT\",\n        \"80\").strip().lstrip())\n    backlog = int(os.getenv(\n        \"LISTEN_BACKLOG\",\n        \"5\").strip().lstrip())\n    size = int(os.getenv(\n        \"LISTEN_SIZE\",\n        \"1024\").strip().lstrip())\n    sleep_in_seconds = float(os.getenv(\n        \"LISTEN_SLEEP\",\n        \"0.5\").strip().lstrip())\n    shutdown_hook = os.getenv(\n        \"LISTEN_SHUTDOWN_HOOK\",\n        \"/tmp/shutdown-listen-server-{}-{}\".format(\n            host,\n            port)).strip().lstrip()\n\n    if os.path.exists(shutdown_hook):\n        print((\"Please remove the shutdown hook file: \"\n               \"\\nrm -f {}\")\n              .format(\n                shutdown_hook))\n        sys.exit(1)\n\n    now = datetime.datetime.now().isoformat()\n    print((\"{} - Starting Server address={}:{} \"\n           \"backlog={} size={} sleep={} shutdown={}\")\n          .format(\n            now,\n            host,\n            port,\n            backlog,\n            size,\n            sleep_in_seconds,\n            shutdown_hook))\n\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    s.bind((host, port))\n    s.listen(backlog)\n\n    msg = 0\n    while 1:\n        client, address = s.accept()\n        send_data = False\n        data = None\n        while not data:\n            data = client.recv(size)\n            if data:\n                now = datetime.datetime.now().isoformat()\n                print((\"{} received msg={} \"\n                       \"data={} replying\")\n                      .format(\n                        now,\n                        msg,\n                        data))\n                msg += 1\n                if msg > 1000000:\n                    msg = 0\n\n                send_data = True\n            else:\n                time.sleep(sleep_in_seconds)\n\n        if send_data:\n            client.send(data)\n        if os.path.exists(shutdown_hook):\n            now = datetime.datetime.now().isoformat()\n            print((\"{} detected shutdown \"\n                   \"file={}\")\n                  .format(\n                    now,\n                    shutdown_hook))\n\n        client.close()"}
{"prompt": "def _as_graph_element(self):\n    \"\"\"Returns the underlying graph element if possible.\"\"\"", "output": "\n    if self._graph is not None:\n      return self._graph.as_graph_element(self.handle)\n    return self._handle\n\n  def _AsTensor(self):\n    \"\"\"Returns a Tensor after converting it to a Tensor.\"\"\"\n    self.stack.append([])\n    self.tensor.op._set_attr(\"_class\", \"Const\")  # pylint: disable=protected-access\n    if self._shape_val is not None:\n      self.tensor.shape.CopyFrom(self._shape_val.name)\n    if self._handle_data is not None:\n      self.tensor.handle.", "solution": "\n    if self.is_sequence():\n      raise TypeError('A Pretty Tensor that holds a sequence cannot be '\n                      'represented as a graph element.')\n    else:\n      # Self might be holding something else that isn't a true tensor, so\n      # if the 'tensor' can behave like a graph element, look for its\n      # _AsGraphElement method and call it. Graph elements themselves may not\n      # have or need this method, so just return other items directly.\n      obj = self.tensor\n      conv_fn = getattr(obj, '_as_graph_element', None)\n      if conv_fn and isinstance(conv_fn, collections.Callable):\n        obj = conv_fn()\n      return obj"}
{"prompt": "def initialize(\n    plugins,\n    exclude_files_regex=None,\n    exclude_lines_regex=None,\n    path='.',\n    scan_all_files=False,\n):\n    \"\"\"Scans the entire codebase for secrets, and returns a\n    SecretsCollection object.\n\n    :type plugins: tuple of detect_secrets.plugins.base.BasePlugin\n    :param plugins: rules to initialize the SecretsCollection with.\n\n    :type exclude_files_regex: str|None\n    :type exclude_lines_regex: str|None\n    :type path: str\n    :type scan_all_files: bool\n\n    :rtype: SecretsCollection\n    \"\"\"", "output": "_files: bool\n    :type path: str\n    :type scan_all_lines: bool\n\n    :rtype: SecretsCollection\n    :return: a SecretsCollection object.\n\n    :raises SecretsCollectionError: if the rules are not met.\n    \"\"\"\n    if exclude_files_regex is None:\n        exclude_files_regex = tuple()\n    if exclude_lines_regex is None:\n        exclude_lines_regex = tuple()\n\n    if exclude_files_regex is not None:\n        for rule in plugins:\n            if isinstance(rule, BaseRule):\n                for exclude_line in exclude", "solution": "\n    output = SecretsCollection(\n        plugins,\n        exclude_files=exclude_files_regex,\n        exclude_lines=exclude_lines_regex,\n    )\n\n    if os.path.isfile(path):\n        # This option allows for much easier adhoc usage.\n        files_to_scan = [path]\n    elif scan_all_files:\n        files_to_scan = _get_files_recursively(path)\n    else:\n        files_to_scan = _get_git_tracked_files(path)\n\n    if not files_to_scan:\n        return output\n\n    if exclude_files_regex:\n        exclude_files_regex = re.compile(exclude_files_regex, re.IGNORECASE)\n        files_to_scan = filter(\n            lambda file: (\n                not exclude_files_regex.search(file)\n            ),\n            files_to_scan,\n        )\n\n    for file in files_to_scan:\n        output.scan_file(file)\n\n    return output"}
{"prompt": "def getDate():\n    \"\"\"Returns a formatted string with the current date.\"\"\"", "output": "\n    return time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n\ndef getTime():\n    \"\"\"Returns a formatted string with the current time.\"\"\"\n    return time.strftime(\"%H:%M:%S\", time.localtime())\n\ndef getDate():\n    \"\"\"Returns a formatted string with the current date.\"\"\"\n    return time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n\ndef getTime():\n    \"\"\"Returns a formatted string with the current time.\"\"\"\n    return time.strftime(\"%H:%M:%S\", time.localtime())\n", "solution": "\n\n    _ltime = _time.localtime(_time.time())\n    date_str = _time.strftime('%Y-%m-%dT%H:%M:%S',_ltime)\n\n    return date_str"}
{"prompt": "def _update_classifier(self, data, labels, w, classes):\n        \"\"\"Update the classifier parameters theta and bias\n\n        Parameters\n        ----------\n\n        data : list of 2D arrays, element i has shape=[voxels_i, samples_i]\n            Each element in the list contains the fMRI data of one subject for\n            the classification task.\n\n        labels : list of arrays of int, element i has shape=[samples_i]\n            Each element in the list contains the labels for the data samples\n            in data_sup.\n\n        w : list of 2D array, element i has shape=[voxels_i, features]\n            The orthogonal transforms (mappings) :math:`W_i` for each subject.\n\n        classes : int\n            The number of classes in the classifier.\n\n\n        Returns\n        -------\n\n        theta : array, shape=[features, classes]\n            The MLR parameter for the class planes.\n\n        bias : array shape=[classes,]\n            The MLR parameter for class biases.\n        \"\"\"", "output": "_i, n_components]\n            Each element in the list contains the weight of each subject\n            component.\n\n        classes : list of int, element i has shape=[voxels_i]\n            Each element in the list contains the number of subjects for the\n            classification task.\n        \"\"\"\n        if self.n_components is None:\n            raise ValueError('n_components is required for online learning')\n        if self.theta is None:\n            raise ValueError('theta is required for online learning')\n        if self.bias is None:\n            raise ValueError('bias is required for online learning')\n        if self.theta is not None", "solution": "\n\n        # Stack the data and labels for training the classifier\n        data_stacked, labels_stacked, weights = \\\n            SSSRM._stack_list(data, labels, w)\n\n        features = w[0].shape[1]\n        total_samples = weights.size\n\n        data_th = S.shared(data_stacked.astype(theano.config.floatX))\n        val_ = S.shared(labels_stacked)\n        total_samples_S = S.shared(total_samples)\n        theta_th = T.matrix(name='theta', dtype=theano.config.floatX)\n        bias_th = T.col(name='bias', dtype=theano.config.floatX)\n        constf2 = S.shared(self.alpha / self.gamma, allow_downcast=True)\n        weights_th = S.shared(weights)\n\n        log_p_y_given_x = \\\n            T.log(T.nnet.softmax((theta_th.T.dot(data_th.T)).T + bias_th.T))\n        f = -constf2 * T.sum((log_p_y_given_x[T.arange(total_samples_S), val_])\n                             / weights_th) + 0.5 * T.sum(theta_th ** 2)\n\n        manifold = Product((Euclidean(features, classes),\n                            Euclidean(classes, 1)))\n        problem = Problem(manifold=manifold, cost=f, arg=[theta_th, bias_th],\n                          verbosity=0)\n        solver = ConjugateGradient(mingradnorm=1e-6)\n        solution = solver.solve(problem)\n        theta = solution[0]\n        bias = solution[1]\n\n        del constf2\n        del theta_th\n        del bias_th\n        del data_th\n        del val_\n        del solver\n        del solution\n\n        return theta, bias"}
{"prompt": "def parse(cls, key):\n        \"\"\"\n        Parse a key represented by a string and return its character code.\n        \"\"\"", "output": "\n        if key == cls.ESCAPE:\n            return cls.ESCAPE\n        elif key == cls.SPACE:\n            return cls.SPACE\n        elif key == cls.END:\n            return cls.END\n        elif key == cls.BACKSPACE:\n            return cls.BACKSPACE\n        elif key == cls.DELETE:\n            return cls.DELETE\n        elif key == cls.INSERT:\n            return cls.INSERT\n        elif key == cls.DELETE_ALL:\n            return cls.DELETE_ALL\n        elif key == cls.INSERT_ALL:\n            return cls.INSERT_ALL\n        elif key == cls.DELETE_BACKSPACE:\n            return cls", "solution": "\n\n        try:\n            if isinstance(key, int):\n                return key\n            elif re.match('[<]KEY_.*[>]', key):\n                # Curses control character\n                return getattr(curses, key[1:-1])\n            elif re.match('[<].*[>]', key):\n                # Ascii control character\n                return getattr(curses.ascii, key[1:-1])\n            elif key.startswith('0x'):\n                # Ascii hex code\n                return int(key, 16)\n            elif len(key) == 2:\n                # Double presses\n                return tuple(cls.parse(k) for k in key)\n            else:\n                # Ascii character\n                code = ord(key)\n                if 0 <= code <= 255:\n                    return code\n                # Python 3.3 has a curses.get_wch() function that we can use\n                # for unicode keys, but Python 2.7 is limited to ascii.\n                raise exceptions.ConfigError('Invalid configuration! `%s` is '\n                                             'not in the ascii range' % key)\n\n        except (AttributeError, ValueError, TypeError):\n            raise exceptions.ConfigError('Invalid configuration! \"%s\" is not a '\n                                         'valid key' % key)"}
{"prompt": "def get_git_repositories_activity_metrics(self, project, from_date, aggregation_type, skip, top):\n        \"\"\"GetGitRepositoriesActivityMetrics.\n        [Preview API] Retrieves git activity metrics for repositories matching a specified criteria.\n        :param str project: Project ID or project name\n        :param datetime from_date: Date from which, the trends are to be fetched.\n        :param str aggregation_type: Bucket size on which, trends are to be aggregated.\n        :param int skip: The number of repositories to ignore.\n        :param int top: The number of repositories for which activity metrics are to be retrieved.\n        :rtype: [RepositoryActivityMetrics]\n        \"\"\"", "output": " to return.\n        :return: list[dict]\n        \"\"\"\n        params = {}\n        if project is not None:\n            params['ProjectId'] = project\n        if aggregation_type is not None:\n            params['BucketSize'] = {\n                'Total': bucket_size_to_total(from_date),\n                'Filtered': bucket_size_to_filtered(skip),\n                'TotalFiltered': bucket_size_to_total(skip),\n            }\n        if skip is not None:\n            params['Skip'] = {\n                'Total': skip,\n                'Filtered': skip,\n                'Total", "solution": "\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        query_parameters = {}\n        if from_date is not None:\n            query_parameters['fromDate'] = self._serialize.query('from_date', from_date, 'iso-8601')\n        if aggregation_type is not None:\n            query_parameters['aggregationType'] = self._serialize.query('aggregation_type', aggregation_type, 'str')\n        if skip is not None:\n            query_parameters['$skip'] = self._serialize.query('skip', skip, 'int')\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        response = self._send(http_method='GET',\n                              location_id='df7fbbca-630a-40e3-8aa3-7a3faf66947e',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[RepositoryActivityMetrics]', self._unwrap_collection(response))"}
{"prompt": "def pretty_print_graphql(query, use_four_spaces=True):\n    \"\"\"Take a GraphQL query, pretty print it, and return it.\"\"\"", "output": "\n    return query.replace(' ', '\\n').replace('\\t','')\n\n\ndef get_graphql_schema(query):\n    \"\"\"Return the schema for a GraphQL query.\"\"\"\n    return 'graphql'\n\n\ndef get_graphql_query(query):\n    \"\"\"Return a GraphQL query.\"\"\"\n    return query.replace(' ', '\\n').replace('\\t','')\n\n\ndef get_graphql_query_from_file(filename):\n    \"\"\"Return a GraphQL query from a file.\"\"\"\n    with open(filename, 'r') as f:\n        return get_graphql_query", "solution": "\n    # Use our custom visitor, which fixes directive argument order\n    # to get the canonical representation\n    output = visit(parse(query), CustomPrintingVisitor())\n\n    # Using four spaces for indentation makes it easier to edit in\n    # Python source files.\n    if use_four_spaces:\n        return fix_indentation_depth(output)\n    return output"}
{"prompt": "def hdf5_read_out(self):\n        \"\"\"Read out an hdf5 file.\n\n        Takes the output of :class:`gwsnrcalc.genconutils.genprocess.GenProcess`\n        and reads it out to an HDF5 file.\n\n        \"\"\"", "output": "\n        self.logger.debug(\"Reading out file %s\", self.filename)\n        self.h5file = tables.openFile(self.filename, mode=\"w\", title=\"out\")\n        self.h5file.createTable(self.h5file.root, 'gwsnrcalc_out',\n                                 self.gwsnrcalc_out,\n                                 'gwsnrcalc_out',\n                                 'gwsnrcalc_out',\n                                 'gwsnrcalc_out',\n                                 'gwsnrcalc_out',\n                                 'gwsnrcalc_out',\n                                 'gwsnrcalc", "solution": "\n        with h5py.File(self.WORKING_DIRECTORY + '/' + self.output_file_name, 'w') as f:\n\n            header = f.create_group('header')\n            header.attrs['Title'] = 'Generated SNR Out'\n            header.attrs['Author'] = 'Generator by: Michael Katz'\n            header.attrs['Date/Time'] = str(datetime.datetime.now())\n\n            for which in ['x', 'y']:\n                header.attrs[which + 'val_name'] = getattr(self, which + 'val_name')\n                header.attrs['num_' + which + '_pts'] = getattr(self, 'num_' + which)\n\n            ecc = 'eccentricity' in self.__dict__\n            if ecc:\n                name_list = ['observation_time', 'start_frequency', 'start_separation'\n                             'eccentricity']\n            else:\n                name_list = ['spin_1', 'spin_2', 'spin', 'end_time']\n\n            name_list += ['total_mass', 'mass_ratio', 'start_time', 'luminosity_distance',\n                          'comoving_distance', 'redshift']\n\n            for name in name_list:\n                if name != self.xval_name and name != self.yval_name:\n                    try:\n                        getattr(self, name)\n                        header.attrs[name] = getattr(self, name)\n                    except AttributeError:\n                        pass\n\n            if self.added_note != '':\n                header.attrs['Added note'] = self.added_note\n\n            data = f.create_group('data')\n\n            # read out x,y values in compressed data set\n            dset = data.create_dataset(self.x_col_name, data=self.xvals,\n                                       dtype='float64', chunks=True,\n                                       compression='gzip', compression_opts=9)\n\n            dset = data.create_dataset(self.y_col_name, data=self.yvals,\n                                       dtype='float64', chunks=True,\n                                       compression='gzip', compression_opts=9)\n\n            # read out all datasets\n            for key in self.output_dict.keys():\n                dset = data.create_dataset(key, data=self.output_dict[key],\n                                           dtype='float64', chunks=True,\n                                           compression='gzip', compression_opts=9)"}
{"prompt": "def load(self):\n        \"\"\"Load all available DRPs in 'entry_point'.\"\"\"", "output": "\n        self.load_drp_entries()\n        self.load_drp_entries_from_file()\n        self.load_drp_entries_from_file()\n        self.load_drp_entries_from_file()\n\n    def load_drp_entries_from_file(self):\n        \"\"\"Load DRP entries from a file.\"\"\"\n        self.drp_entries = []\n        for line in self.file:\n            line = line.strip()\n            if line:\n                entry = self.parse_entry(line)\n                self.drp_entries.append(entry)", "solution": "\n\n        for drpins in self.iload(self.entry):\n            self.drps[drpins.name] = drpins\n\n        return self"}
{"prompt": "def prev_moment_operating_on(\n            self,\n            qubits: Sequence[ops.Qid],\n            end_moment_index: Optional[int] = None,\n            max_distance: Optional[int] = None) -> Optional[int]:\n        \"\"\"Finds the index of the next moment that touches the given qubits.\n\n        Args:\n            qubits: We're looking for operations affecting any of these qubits.\n            end_moment_index: The moment index just after the starting point of\n                the reverse search. Defaults to the length of the list of\n                moments.\n            max_distance: The number of moments (starting just before from the\n                end index and moving backward) to check. Defaults to no limit.\n\n        Returns:\n            None if there is no matching moment, otherwise the index of the\n            latest matching moment.\n\n        Raises:\n            ValueError: negative max_distance.\n        \"\"\"", "output": ": The maximum distance to search. If not None,\n                then the search will be limited to this number of operations.\n                Defaults to the number of operations.\n\n        Returns:\n            The index of the next moment that touches the given qubits.\n        \"\"\"\n        if end_moment_index is None:\n            end_moment_index = len(qubits)\n        if max_distance is None:\n            max_distance = len(self.qubits)\n        if max_distance < 0:\n            return None\n        if end_moment_index < 0:\n            return end_distance\n        if max_distance > 0:", "solution": "\n        if end_moment_index is None:\n            end_moment_index = len(self._moments)\n\n        if max_distance is None:\n            max_distance = len(self._moments)\n        elif max_distance < 0:\n            raise ValueError('Negative max_distance: {}'.format(max_distance))\n        else:\n            max_distance = min(end_moment_index, max_distance)\n\n        # Don't bother searching indices past the end of the list.\n        if end_moment_index > len(self._moments):\n            d = end_moment_index - len(self._moments)\n            end_moment_index -= d\n            max_distance -= d\n        if max_distance <= 0:\n            return None\n\n        return self._first_moment_operating_on(qubits,\n                                               (end_moment_index - k - 1\n                                                for k in range(max_distance)))"}
{"prompt": "def format_value(column_dict, value, key=None):\n    \"\"\"\n    Format a value coming from the database (for example converts datetimes to\n    strings)\n\n    :param column_dict: The column datas collected during inspection\n    :param value: A value coming from the database\n    :param key: The exportation key\n    \"\"\"", "output": "\n    if key:\n        return column_dict[key]\n    return value\n\n\nclass Exportation(object):\n    \"\"\"\n    A class for exportation of course data.\n\n    :param course_id: The id of the course\n    :param exportation_type: The type of exportation (e.g. \"verified\", \"honor\", \"audit\", etc)\n    \"\"\"\n    def __init__(self, course_id, exportation_type):\n        self.course_id = course_id\n        self.exportation_type = exportation_type\n        self.start_date = None\n        self.end", "solution": "\n    formatter = column_dict.get('formatter')\n    prop = column_dict['__col__']\n\n    res = value\n\n    if value in ('', None,):\n        res = ''\n\n    elif formatter is not None:\n        res = formatter(value)\n\n    else:\n        if hasattr(prop, \"columns\"):\n            sqla_column = prop.columns[0]\n            column_type = getattr(sqla_column.type, 'impl', sqla_column.type)\n\n            formatter = FORMATTERS_REGISTRY.get_formatter(column_type, key)\n            if formatter is not None:\n                res = formatter(value)\n    return res"}
{"prompt": "def modified_created(instance):\n    \"\"\"`modified` property must be later or equal to `created` property\n    \"\"\"", "output": "\n    if instance.modified:\n        return True\n    return False\n\n\ndef get_default_ip_protocol():\n    \"\"\"`ip_protocol` property must be the IP protocol used by the\n    server.\n    \"\"\"\n    return IP_PROTOCOL_MAP.get(DEFAULT_PROTOCOL, DEFAULT_PROTOCOL)\n\n\ndef get_default_ip_protocol_by_name(name):\n    \"\"\"`ip_protocol` property must be the IP protocol used by the\n    server.\n    \"\"\"\n    return IP_PROTOCOL_MAP.get(DEFAULT_PROTOCOL, DEFAULT_IP_PROTOCOL_MAP.get(name, DEFAULT_", "solution": "\n    if 'modified' in instance and 'created' in instance and \\\n            instance['modified'] < instance['created']:\n        msg = \"'modified' (%s) must be later or equal to 'created' (%s)\"\n        return JSONError(msg % (instance['modified'], instance['created']),\n                         instance['id'])"}
{"prompt": "def _state_command(self, container_id=None, command='start', sudo=None):\n\n    \"\"\" A generic state command to wrap pause, resume, kill, etc., where the\n        only difference is the command. This function will be unwrapped if the\n        child functions get more complicated (with additional arguments).\n\n        Equivalent command line example:      \n           singularity oci <command> <container_ID>\n           \n        Parameters\n        ==========\n        container_id: the id to start.\n        command: one of start, resume, pause, kill, defaults to start.\n        sudo: Add sudo to the command. If the container was created by root,\n              you need sudo to interact and get its state.\n\n        Returns\n        =======\n        return_code: the return code to indicate if the container was started.\n    \"\"\"", "output": "\n    \"\"\"\n\n    def _state_command(self, sudo=None):\n\n        \"\"\"\n        This function will wrap pause, resume, kill, etc., where the\n        only difference is the command. This function will be unwrapped if the\n        child functions get more complicated (with additional arguments).\n\n        Equivalent command line example:      \n           pause oci <command> <container_ID>\n           \n        Parameters\n        ==========\n        sudo: Add sudo to the command.\n        \"\"\"\n\n        if not self.args.pause:\n            self.args.pause = True\n\n        if not self.args.resume:\n            self.args.resume = True\n\n       ", "solution": "\n    sudo = self._get_sudo(sudo)\n    container_id = self.get_container_id(container_id)\n\n    # singularity oci state\n    cmd = self._init_command(command)\n\n    # Finally, add the container_id\n    cmd.append(container_id)\n\n    # Run the command, return return code\n    return self._run_and_return(cmd, sudo)"}
{"prompt": "def compile_truncate(self, query):\n        \"\"\"\n        Compile a truncate statement into SQL\n\n        :param query: A QueryBuilder instance\n        :type query: QueryBuilder\n\n        :return: The compiled truncate statement\n        :rtype: str\n        \"\"\"", "output": "\n        return query.tokens[0]\n\n    def compile_update(self, query):\n        \"\"\"\n        Compile a update statement into SQL\n\n        :param query: A QueryBuilder instance\n        :type query: QueryBuilder\n\n        :return: The compiled update statement\n        :rtype: str\n        \"\"\"\n        return query.tokens[1]\n\n    def compile_delete(self, query):\n        \"\"\"\n        Compile a delete statement into SQL\n\n        :param query: A QueryBuilder instance\n        :type query: QueryBuilder\n\n        :return: The compiled delete statement\n        :rtype: str\n        \"\"\"\n        return query.tokens[2]\n\n    def compile_", "solution": "\n        sql = {\n            \"DELETE FROM sqlite_sequence WHERE name = %s\"\n            % self.get_marker(): [query.from__]\n        }\n\n        sql[\"DELETE FROM %s\" % self.wrap_table(query.from__)] = []\n\n        return sql"}
{"prompt": "def get_queue_for_job(self, job):\n        \"\"\"\n        Returns a queue to put job into.\n        \"\"\"", "output": "\n        return self.queues.get(job, None)\n\n    def get_job_for_job_id(self, job_id):\n        \"\"\"\n        Returns a job to get from the queue with the given job id.\n        \"\"\"\n        return self.jobs.get(job_id, None)\n\n    def get_job_status(self, job_id):\n        \"\"\"\n        Returns a job status.\n        \"\"\"\n        return self.jobs.get(job_id, None)\n\n    def get_job_status_id(self, job_id):\n        \"\"\"\n        Returns a job status id.\n        \"\"\"", "solution": "\n        if self._queue is not None:\n            return self._queue\n        key = '{0}{1}'.format(self.queue_class.redis_queue_namespace_prefix,\n                              job.origin)\n        return self.queue_class.from_queue_key(\n                key, connection=self.connection, job_class=self.job_class)"}
{"prompt": "def _add_transcripts(self, variant_obj, info_dict):\n        \"\"\"Return all transcripts sound in the vcf file\"\"\"", "output": "\n        for transcript in self.transcripts:\n            if transcript.name == variant_obj.name:\n                transcript.add_transcript(variant_obj)\n                return transcript\n        return None\n\n    def _add_exons(self, exon_obj, info_dict):\n        \"\"\"Return all exons in the vcf file\"\"\"\n        for exon in self.exons:\n            if exon.name == exon_obj.name:\n                exon.add_exon(exon_obj)\n                return exon\n        return None\n\n    def _add_introns(self, intron_obj", "solution": "\n        vep_string = info_dict.get('CSQ')\n\n        #Check if snpeff annotation:\n        snpeff_string = info_dict.get('ANN')\n        \n        # We check one of these.\n        # VEP has presedence over snpeff\n        if vep_string:\n            #Get the vep annotations\n            vep_info = get_vep_info(\n                vep_string = vep_string,\n                vep_header = self.vep_header\n                )\n            for transcript_info in vep_info:\n                transcript = self._get_vep_transcript(transcript_info)\n                variant_obj.add_transcript(transcript)\n\n        elif snpeff_string:\n            #Get the vep annotations\n            snpeff_info = get_snpeff_info(\n                snpeff_string = snpeff_string,\n                snpeff_header = self.snpeff_header\n                )\n            for transcript_info in snpeff_info:\n                transcript = self._get_snpeff_transcript(transcript_info)\n                variant_obj.add_transcript(transcript)"}
{"prompt": "def background(self):\n        \"\"\"Only a getter on purpose. See the tests.\"\"\"", "output": "\n        return self._background\n\n    @background.setter\n    def background(self, value):\n        \"\"\"Setter for background property.\"\"\"\n        self._background = value\n\n    @property\n    def state(self):\n        \"\"\"Getter for state property.\"\"\"\n        return self._state\n\n    @state.setter\n    def state(self, value):\n        \"\"\"Setter for state property.\"\"\"\n        self._state = value\n\n    @property\n    def supported_features(self):\n        \"\"\"Getter for supported features property.\"\"\"\n        return self._supported_features\n\n    @supported_features.setter\n    def supported_features(self, value):\n        \"\"\"Setter for supported features property", "solution": "\n        if self._background is None:\n            self._background = GSBackgroundLayer()\n            self._background._foreground = self\n        return self._background"}
{"prompt": "def kem(request):\n\t\"\"\"\n\tdue to the base directory settings of django, the model_path needs to be different when\n\ttesting with this section.\n\t\"\"\"", "output": "\n\treturn request.path\n\ndef get_test_path(request):\n\t\"\"\"\n\tget_test_path is a method of the model_path to test the file system.\n\t\"\"\"\n\treturn request.model_path\n\ndef get_test_path_from_path(path):\n\t\"\"\"\n\tget_test_path_from_path is a method of the model_path to test the file system.\n\t\"\"\"\n\treturn path.split(path_delimiter)[0]\n\ndef get_test_path_from_file(path):\n\t", "solution": "\n\tkeyword = request.GET['keyword']\n\tlang = request.GET['lang']\n\tontology = 'ontology' if 'ontology' in request.GET and bool(json.loads(request.GET['ontology'].lower())) else 'origin'\n\tresult = multilanguage_model[lang][ontology].most_similar(keyword, int(request.GET['num']) if 'num' in request.GET else 10)\n\treturn JsonResponse(result, safe=False)"}
{"prompt": "def _CaptureRequestLogId(self):\n    \"\"\"Captures the request log id if possible.\n\n    The request log id is stored inside the breakpoint labels.\n    \"\"\"", "output": "\n    if self._request_log_id:\n      return self._request_log_id\n\n    if self._process_id:\n      return self._process_id\n\n    if self._process_name:\n      return self._process_name\n\n    return None\n\n  def _GetProcessName(self):\n    \"\"\"Returns the name of the process that owns the current thread.\n\n    Returns:\n      The name of the process that owns the current thread or None if the\n      current thread has not been determined.\n    \"\"\"\n    if self._process_id:\n      return self._process_id.split(\".\")[-1]\n\n    return", "solution": "\n    # pylint: disable=not-callable\n    if callable(request_log_id_collector):\n      request_log_id = request_log_id_collector()\n      if request_log_id:\n        # We have a request_log_id, save it into the breakpoint labels\n        self.breakpoint['labels'][\n            labels.Breakpoint.REQUEST_LOG_ID] = request_log_id"}
{"prompt": "def resize(self, package):\n        \"\"\"\n        ::\n        \n            POST /:login/machines/:id?action=resize\n        \n        Initiate resizing of the remote machine to a new package.\n        \"\"\"", "output": "\n        return self._post(\"/machines/%s/action\" % package.id,\n                         data=dict(id=package.id, action=self._get_action_info(package)))\n\n    def _get_action_info(self, action):\n        \"\"\"\n        ::\n        \n            _get_action_info(action)\n        \n        Retrieve the info about an action.\n        \"\"\"\n        return self._get(\"/machines/%s/action/%s/info\" % (self.id, action.id),\n                         data={\"action\": action.to_dict()})\n\n    def _post(self, url, data=None):\n       ", "solution": "\n        if isinstance(package, dict):\n            package = package['name']\n        action = {'action': 'resize',\n                  'package': package}\n        j, r = self.datacenter.request('POST', self.path, params=action)\n        r.raise_for_status()"}
{"prompt": "def _parse_raid_config_data(self, raid_config):\n        \"\"\"It will parse raid config data based on raid controllers\n\n        :param raid_config: A dictionary containing target raid configuration\n                            data. This data stucture should be as follows:\n                            raid_config = {'logical_disks': [{'raid_level': 1,\n                            'size_gb': 100, 'controller':\n                            'HPE Smart Array P408i-a SR Gen10'},\n                            <info-for-logical-disk-2>]}\n        :returns: A dictionary of controllers, each containing list of\n                  their respected logical drives.\n        \"\"\"", "output": " of disks and\n                  a list of dictionaries with keys:\n                      'logical_disks': [{'raid_level': 1,\n                           'size_gb': 100, 'controller':\n                            'HPE Smart Array P408i-a'},\n                            <info-for-logical-disk-2>]}\n        \"\"\"\n        raid_controllers = {}\n        for controller in raid_config['logical_disks']:\n            if controller['raid_level'] not in raid_controllers:\n                raid_controllers[controller['raid_level']] = {}\n            if controller['size_gb'] not in raid_controllers[controller['", "solution": "\n        default = (\n            self.smart_storage.array_controllers.get_default_controller.model)\n        controllers = {default: []}\n        for ld in raid_config['logical_disks']:\n            if 'controller' not in ld.keys():\n                controllers[default].append(ld)\n            else:\n                ctrl = ld['controller']\n                if ctrl not in controllers:\n                    controllers[ctrl] = []\n                controllers[ctrl].append(ld)\n        return controllers"}
{"prompt": "def on_switch_page(self, notebook, page_pointer, page_num, user_param1=None):\n        \"\"\"Update state selection when the active tab was changed\n        \"\"\"", "output": "\n        self.notebook = notebook\n        self.page_pointer = page_pointer\n        self.page_num = page_num\n        self.user_param1 = user_param1\n        self.user_param2 = user_param2\n\n    def on_switch_to_next_button(self, notebook, page_num, user_param1=None):\n        \"\"\"Update state selection when the active tab was changed\n        \"\"\"\n        self.notebook.set_current_page(self.page_pointer + 1)\n        self.notebook.set_next_page(self.page_pointer + 2)", "solution": "\n        page = notebook.get_nth_page(page_num)\n\n        # find state of selected tab\n        for tab_info in list(self.tabs.values()):\n            if tab_info['page'] is page:\n                state_m = tab_info['state_m']\n                sm_id = state_m.state.get_state_machine().state_machine_id\n                selected_state_m = self.current_state_machine_m.selection.get_selected_state()\n\n                # If the state of the selected tab is not in the selection, set it there\n                if selected_state_m is not state_m and sm_id in self.model.state_machine_manager.state_machines:\n                    self.model.selected_state_machine_id = sm_id\n                    self.current_state_machine_m.selection.set(state_m)\n                return"}
{"prompt": "def radio_status_encode(self, rssi, remrssi, txbuf, noise, remnoise, rxerrors, fixed):\n                \"\"\"\n                Status generated by radio and injected into MAVLink stream.\n\n                rssi                      : Local signal strength (uint8_t)\n                remrssi                   : Remote signal strength (uint8_t)\n                txbuf                     : Remaining free buffer space in percent. (uint8_t)\n                noise                     : Background noise level (uint8_t)\n                remnoise                  : Remote background noise level (uint8_t)\n                rxerrors                  : Receive errors (uint16_t)\n                fixed                     : Count of error corrected packets (uint16_t)\n\n                \"\"\"", "output": " errors (uint8_t)\n                fixed                   : Fixed signal strength (uint8_t)\n                \"\"\"\n                if not fixed:\n                        return rssi\n                if not rxerrors:\n                        return remrssi\n                if not rssi:\n                        return rxbuf\n                if noise > 0:\n                        return self.encode_float(txbuf, noise)\n                return rssi\n\n        def radio_status_decode(self, rssi, remrssi, txbuf, noise, remnoise, rxerrors, fixed):\n                \"\"\"\n                Status generated by radio and injected into MAVLink stream.\n\n                rssi                      : Local signal strength", "solution": "\n                return MAVLink_radio_status_message(rssi, remrssi, txbuf, noise, remnoise, rxerrors, fixed)"}
{"prompt": "def _residual(self, x, in_filter, out_filter, stride,\n                activate_before_residual=False):\n    \"\"\"Residual unit with 2 sub layers.\"\"\"", "output": "\n    with tf.variable_scope('residual'):\n      x = self._conv_block(x, in_filter, out_filter, stride)\n      x = self._conv_block(x, in_filter, out_filter, out_filter)\n      x = self._conv_block(x, out_filter, out_filter, activate_before_residual)\n      x = self._conv_block(x, out_filter, out_filter, stride)\n      x = self._conv_block(x, out_filter, out_filter, False)\n      x = self._conv_", "solution": "\n    if activate_before_residual:\n      with tf.variable_scope('shared_activation'):\n        x = self._layer_norm('init_bn', x)\n        x = self._relu(x, self.hps.relu_leakiness)\n        orig_x = x\n    else:\n      with tf.variable_scope('residual_only_activation'):\n        orig_x = x\n        x = self._layer_norm('init_bn', x)\n        x = self._relu(x, self.hps.relu_leakiness)\n\n    with tf.variable_scope('sub1'):\n      x = self._conv('conv1', x, 3, in_filter, out_filter, stride)\n\n    with tf.variable_scope('sub2'):\n      x = self._layer_norm('bn2', x)\n      x = self._relu(x, self.hps.relu_leakiness)\n      x = self._conv('conv2', x, 3, out_filter, out_filter, [1, 1, 1, 1])\n\n    with tf.variable_scope('sub_add'):\n      if in_filter != out_filter:\n        orig_x = tf.nn.avg_pool(orig_x, stride, stride, 'VALID')\n        orig_x = tf.pad(\n            orig_x, [[0, 0], [0, 0], [0, 0],\n                     [(out_filter - in_filter) // 2,\n                      (out_filter - in_filter) // 2]])\n      x += orig_x\n\n    return x"}
{"prompt": "def build_re_pattern_from_intervals(intervals: IntervalListType) -> BuiltInReType:\n    \"\"\"\n    Convert intervals to regular expression pattern.\n\n    :param intervals: Unicode codepoint intervals.\n    \"\"\"", "output": "\n    if not intervals:\n        return None\n    if len(intervals) == 1:\n        return IntervalPattern(intervals[0])\n    if len(intervals) == 2:\n        return BuiltInReType(intervals[0], intervals[1])\n    if len(intervals) == 3:\n        return IntervalType(intervals[0], intervals[1], intervals[2])\n    if len(intervals) == 4:\n        return IntervalType(intervals[0], intervals[1], intervals[2], intervals[3], intervals[4])\n    if len(intervals) == 5:\n        return IntervalPattern(intervals[0", "solution": "\n\n    inner = [f'{chr(lb)}-{chr(ub)}' for lb, ub in intervals]\n    joined_inner = ''.join(inner)\n    pattern = f'[{joined_inner}]+'\n\n    return re.compile(pattern, re.UNICODE)"}
{"prompt": "def thirds(reference_labels, estimated_labels):\n    \"\"\"Compare chords along root & third relationships.\n\n    Examples\n    --------\n    >>> (ref_intervals,\n    ...  ref_labels) = mir_eval.io.load_labeled_intervals('ref.lab')\n    >>> (est_intervals,\n    ...  est_labels) = mir_eval.io.load_labeled_intervals('est.lab')\n    >>> est_intervals, est_labels = mir_eval.util.adjust_intervals(\n    ...     est_intervals, est_labels, ref_intervals.min(),\n    ...     ref_intervals.max(), mir_eval.chord.NO_CHORD,\n    ...     mir_eval.chord.NO_CHORD)\n    >>> (intervals,\n    ...  ref_labels,\n    ...  est_labels) = mir_eval.util.merge_labeled_intervals(\n    ...      ref_intervals, ref_labels, est_intervals, est_labels)\n    >>> durations = mir_eval.util.intervals_to_durations(intervals)\n    >>> comparisons = mir_eval.chord.thirds(ref_labels, est_labels)\n    >>> score = mir_eval.chord.weighted_accuracy(comparisons, durations)\n\n    Parameters\n    ----------\n    reference_labels : list, len=n\n        Reference chord labels to score against.\n    estimated_labels : list, len=n\n        Estimated chord labels to score against.\n\n    Returns\n    -------\n    comparison_scores : np.ndarray, shape=(n,), dtype=float\n        Comparison scores, in [0.0, 1.0]\n\n    \"\"\"", "output": "min(), est_intervals.max())\n    >>> (ref_labels,\n   ...  ref_intervals) = mir_eval.util.thirds(reference_labels, ref_labels)\n    >>> (est_labels,\n   ...  est_labels) = mir_eval.util.thirds(reference_labels, ref_labels)\n    >>> (ref_labels,\n   ...  ref_intervals) = mir_eval.util.ref_intervals(\n   ...  ref_labels, ref_intervals)\n    (0, 0)\n\n    \"\"\"\n    # Check that the reference labels", "solution": "\n    validate(reference_labels, estimated_labels)\n    ref_roots, ref_semitones = encode_many(reference_labels, False)[:2]\n    est_roots, est_semitones = encode_many(estimated_labels, False)[:2]\n\n    eq_roots = ref_roots == est_roots\n    eq_thirds = ref_semitones[:, 3] == est_semitones[:, 3]\n    comparison_scores = (eq_roots * eq_thirds).astype(np.float)\n\n    # Ignore 'X' chords\n    comparison_scores[np.any(ref_semitones < 0, axis=1)] = -1.0\n    return comparison_scores"}
{"prompt": "def get_sections(self, section_name):\n        \"\"\"\n        Return the list of sections stored in self.timers() given `section_name`\n        A fake section is returned if the timer does not have section_name.\n        \"\"\"", "output": "\n        return []\n\n    def get_timers(self):\n        \"\"\"\n        Return a list of fake timers stored in self.timers()\n        \"\"\"\n        return []\n\n    def get_timers_count(self):\n        \"\"\"\n        Return the number of fake timers stored in self.timers()\n        \"\"\"\n        return len(self.timers_list)\n\n    def get_timer_by_name(self, name):\n        \"\"\"\n        Return a fake timer object by name\n        \"\"\"\n        return self.timers_list[self.get_timer_by_name_key(name)]\n\n    def get_timer_by_name_", "solution": "\n        sections = []\n        for timer in self.timers():\n            for sect in timer.sections:\n                if sect.name == section_name:\n                    sections.append(sect)\n                    break\n            else:\n                sections.append(AbinitTimerSection.fake())\n\n        return sections"}
{"prompt": "def draw_MM0(self):\n        \"\"\"\n        Draws the M/M0 plot in the GUI on canvas3\n        \"\"\"", "output": "\n        self.plot_M0.set_xlabel('Time (s)')\n        self.plot_M0.set_ylabel('M/M0 (dB)')\n        self.plot_M0.set_title('M/M0 (dB)')\n        self.plot_M0.set_ylim(0, self.plot_M0.get_ylim()[1])\n        self.plot_M0.set_ylim(0, self.plot_M0.get_ylim()[2])\n        self.plot_M0.set_xlim(0, self.plot_M0", "solution": "\n        self.fig3.clf()\n        self.fig3.text(0.02, 0.96, 'M/M0', {'family': self.font_type, 'fontsize': 10 *\n                                            self.GUI_RESOLUTION, 'style': 'normal', 'va': 'center', 'ha': 'left'})\n        self.mplot = self.fig3.add_axes(\n            [0.2, 0.15, 0.7, 0.7], frameon=True, facecolor='None')\n\n        thermal_x, thermal_y = [], []\n        thermal_x_bad, thermal_y_bad = [], []\n        af_x, af_y = [], []\n        af_x_bad, af_y_bad = [], []\n        for i in range(len(self.Data[self.s]['zijdblock'])):\n            step = self.Data[self.s]['zijdblock_steps'][i]\n            # bad point\n            if self.Data[self.s]['measurement_flag'][i] == 'b':\n                if step == \"0\":\n                    thermal_x_bad.append(self.Data[self.s]['zijdblock'][i][0])\n                    af_x_bad.append(self.Data[self.s]['zijdblock'][i][0])\n                    thermal_y_bad.append(\n                        self.Data[self.s]['zijdblock'][i][3]/self.Data[self.s]['zijdblock'][0][3])\n                    af_y_bad.append(\n                        self.Data[self.s]['zijdblock'][i][3]/self.Data[self.s]['zijdblock'][0][3])\n                elif \"C\" in step:\n                    thermal_x_bad.append(self.Data[self.s]['zijdblock'][i][0])\n                    thermal_y_bad.append(\n                        self.Data[self.s]['zijdblock'][i][3]/self.Data[self.s]['zijdblock'][0][3])\n                elif \"T\" in step:\n                    af_x_bad.append(self.Data[self.s]['zijdblock'][i][0])\n                    af_y_bad.append(\n                        self.Data[self.s]['zijdblock'][i][3]/self.Data[self.s]['zijdblock'][0][3])\n                else:\n                    continue\n\n            else:\n                if step == \"0\":\n                    thermal_x.append(self.Data[self.s]['zijdblock'][i][0])\n                    af_x.append(self.Data[self.s]['zijdblock'][i][0])\n                    thermal_y.append(\n                        self.Data[self.s]['zijdblock'][i][3]/self.Data[self.s]['zijdblock'][0][3])\n                    af_y.append(self.Data[self.s]['zijdblock'][i]\n                                [3]/self.Data[self.s]['zijdblock'][0][3])\n                elif \"C\" in step:\n                    thermal_x.append(self.Data[self.s]['zijdblock'][i][0])\n                    thermal_y.append(\n                        self.Data[self.s]['zijdblock'][i][3]/self.Data[self.s]['zijdblock'][0][3])\n                elif \"T\" in step:\n                    af_x.append(self.Data[self.s]['zijdblock'][i][0])\n                    af_y.append(self.Data[self.s]['zijdblock'][i]\n                                [3]/self.Data[self.s]['zijdblock'][0][3])\n                else:\n                    continue\n\n        if len(thermal_x)+len(thermal_x_bad) > self.Data[self.s]['zijdblock_steps'].count('0'):\n            self.mplot.plot(thermal_x, thermal_y, 'ro-',\n                            markersize=self.MS, lw=1, clip_on=False, zorder=1)\n            for i in range(len(thermal_x_bad)):\n                self.mplot.plot([thermal_x_bad[i]], [thermal_y_bad[i]], 'o',\n                                mfc='None', mec='k', markersize=self.MS, clip_on=False, zorder=1)\n\n        self.mplot.set_xlabel('Thermal (C)', color='r')\n        for tl in self.mplot.get_xticklabels():\n            tl.set_color('r')\n\n        self.mplot_af = self.mplot.twiny()\n        if len(af_x)+len(af_x_bad) > self.Data[self.s]['zijdblock_steps'].count('0'):\n            self.mplot_af.plot(\n                af_x, af_y, 'bo-', markersize=self.MS, lw=1, clip_on=False, zorder=1)\n            for i in range(len(af_x_bad)):\n                self.mplot_af.plot([af_x_bad[i]], [\n                                   af_y_bad[i]], 'o', mfc='None', mec='k', markersize=self.MS, clip_on=False, zorder=1)\n\n        self.mplot_af.set_xlabel('AF (mT)', color='b')\n        for tl in self.mplot_af.get_xticklabels():\n            tl.set_color('b')\n\n        self.mplot.tick_params(axis='both', which='major', labelsize=7)\n        self.mplot_af.tick_params(axis='both', which='major', labelsize=7)\n        self.mplot.spines[\"right\"].set_visible(False)\n        self.mplot_af.spines[\"right\"].set_visible(False)\n        self.mplot.get_xaxis().tick_bottom()\n        self.mplot.get_yaxis().tick_left()\n        self.mplot.set_ylabel(\"M / NRM0\", fontsize=8*self.GUI_RESOLUTION)\n\n        self.canvas3.draw()"}
{"prompt": "def show_driver(devname):\n    \"\"\"\n    Queries the specified network device for associated driver information\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ethtool.show_driver <devname>\n    \"\"\"", "output": "\n    cmd = [\n        'ethtool',\n       'show',\n        'network',\n       'show-interface',\n        'driver',\n       'show-ip',\n       'show-address',\n       'show-carrier',\n       'show-carrier-port',\n       'show-carrier-state',\n       'show-carrier-type',\n       'show-carrier-ip',\n       'show-carrier-address',\n       'show-carrier-state',\n       'show-carrier-type',\n       'show-carrier-ip',\n       'show-carrier-state',\n       'show-carrier-type',\n       'show", "solution": "\n\n    try:\n        module = ethtool.get_module(devname)\n    except IOError:\n        log.error('Driver information not implemented on %s', devname)\n        return 'Not implemented'\n\n    try:\n        businfo = ethtool.get_businfo(devname)\n    except IOError:\n        log.error('Bus information no available on %s', devname)\n        return 'Not available'\n\n    ret = {\n        'driver': module,\n        'bus_info': businfo,\n    }\n\n    return ret"}
{"prompt": "def receive_information_confirmation(self, message):\n        \"\"\"A InformationConfirmation is received.\n\n        If :meth:`the api version is supported\n        <AYABInterface.communication.Communication.api_version_is_supported>`,\n        the communication object transitions into a\n        :class:`InitializingMachine`, if unsupported, into a\n        :class:`UnsupportedApiVersion`\n        \"\"\"", "output": "\n        raise NotImplementedError\n\n    def api_version_is_supported(self, version):\n        \"\"\"Check if the API version is supported.\n\n        :param version: The API version to check.\n        :type version: :class:`ayab.Interface.APIVersion`\n        :returns: True if the API version is supported, False otherwise.\n        :rtype: bool\n        \"\"\"\n        raise NotImplementedError\n\n    def get_information_confirmation(self, message):\n        \"\"\"Get information confirmation.\n\n        :param message: The information confirmation message.\n        :type message: :class:`information_confirmation.InformationConfirmationMessage`\n        :returns:", "solution": "\n        if message.api_version_is_supported():\n            self._next(InitializingMachine)\n        else:\n            self._next(UnsupportedApiVersion)\n\n        self._communication.controller = message"}
{"prompt": "def _check_worktree_support(failhard=True):\n    \"\"\"\n    Ensure that we don't try to operate on worktrees in git < 2.5.0.\n    \"\"\"", "output": "\n    if not failhard:\n        run_cmd(\"git\", \"checkout\", \"-b\", \"test-branch\", \"master\")\n        run_cmd(\"git\", \"add\", \"test-branch\")\n        run_cmd(\"git\", \"commit\", \"-m\", \"test commit\")\n        run_cmd(\"git\", \"rm\", \"test-branch\")\n    else:\n        run_cmd(\"git\", \"checkout\", \"-b\", \"master\")\n        run_cmd(\"git\", \"add\", \"master\")\n        run_cmd(\"git\", \"commit\", \"-m\", \"test commit\")\n\n\ndef _check", "solution": "\n    git_version = version(versioninfo=False)\n    if _LooseVersion(git_version) < _LooseVersion('2.5.0'):\n        if failhard:\n            raise CommandExecutionError(\n                'Worktrees are only supported in git 2.5.0 and newer '\n                '(detected git version: ' + git_version + ')'\n            )\n        return False\n    return True"}
{"prompt": "def enumeration_to_list(enm):\n    \"\"\"\n    Turns the java.util.Enumeration into a list.\n\n    :param enm: the enumeration to convert\n    :type enm: JB_Object\n    :return: the list\n    :rtype: list\n    \"\"\"", "output": "\n    return javabridge.call(javabridge.get_env(), \"java.util.EnumerationToList\", \"()V\")\n\n\ndef get_class_name(class_name):\n    \"\"\"\n    Returns the name of the Java class.\n\n    :param class_name: the name of the class\n    :type class_name: str\n    :return: the class name\n    :rtype: str\n    \"\"\"\n    return javabridge.call(\n        env(), \"java.lang.String\", class_name, \"()Lweka/core/String;\")\n\n\ndef get_field_", "solution": "\n    result = []\n    while javabridge.call(enm, \"hasMoreElements\", \"()Z\"):\n        result.append(javabridge.call(enm, \"nextElement\", \"()Ljava/lang/Object;\"))\n    return result"}
{"prompt": "def get_unique_name(self, name):\n        \"\"\" returns an available unique name\n\n        Parameters\n        ----------\n        name : str\n            name to be made unique\n\n        Returns\n        -------\n        unique_name : str\n            new unique name\n\n        \"\"\"", "output": "\n        return self.unique_name\n\n    def get_name(self, name):\n        \"\"\" returns an available name\n\n        Parameters\n        ----------\n        name : str\n            name to be made unique\n\n        Returns\n        -------\n        name : str\n            new name\n\n        \"\"\"\n        return self.name\n\n    def get_name_and_type(self, name, dtype):\n        \"\"\" returns an available name and type\n\n        Parameters\n        ----------\n        name : str\n            name to be made unique\n        dtype : str\n            type to be made unique\n\n        Returns\n        -------\n        name : str\n            name and type\n\n        \"\"\"\n        return self.name, self.dtype, self", "solution": "\n\n        if name not in self._db:\n            self._db[name] = 0\n            return name\n        else:\n            index = self._db[name]\n            self._db[name] = index + 1\n            return f\"{name}_{index}\""}
{"prompt": "def update_compaction(model):\n    \"\"\"Updates the compaction options for the given model if necessary.\n\n    :param model: The model to update.\n\n    :return: `True`, if the compaction options were modified in Cassandra,\n        `False` otherwise.\n    :rtype: bool\n    \"\"\"", "output": "\n    if model.compaction_options is None:\n        return False\n\n    if not model.compaction_options.get('compaction_strategy', None):\n        strategy = model.compaction_options.get('compaction_strategy', None)\n        if strategy is None:\n            return False\n        strategy = strategy.upper()\n        if strategy == 'ONE':\n            model.compaction_options['compaction_strategy'] = 'ONE_TO_ONE'\n        elif strategy == 'TWO':\n            model.compaction_options['compaction_strategy'] = 'TWO_TO_TWO'\n       ", "solution": "\n    logger.debug(\"Checking %s for compaction differences\", model)\n    table = get_table_settings(model)\n\n    existing_options = table.options.copy()\n\n    existing_compaction_strategy = existing_options['compaction_strategy_class']\n\n    existing_options = json.loads(existing_options['compaction_strategy_options'])\n\n    desired_options = get_compaction_options(model)\n\n    desired_compact_strategy = desired_options.get('class', SizeTieredCompactionStrategy)\n\n    desired_options.pop('class', None)\n\n    do_update = False\n\n    if desired_compact_strategy not in existing_compaction_strategy:\n        do_update = True\n\n    for k, v in desired_options.items():\n        val = existing_options.pop(k, None)\n        if val != v:\n            do_update = True\n\n    # check compaction_strategy_options\n    if do_update:\n        options = get_compaction_options(model)\n        # jsonify\n        options = json.dumps(options).replace('\"', \"'\")\n        cf_name = model.column_family_name()\n        query = \"ALTER TABLE {} with compaction = {}\".format(cf_name, options)\n        logger.debug(query)\n        execute(query)\n        return True\n\n    return False"}
{"prompt": "def _copyDPToClipboard(self):\n        \"\"\"Callback for item menu.\"\"\"", "output": "\n        self.on_dmenu_copy_activate(None)\n\n    def on_dmenu_copy_activate(self, widget):\n        \"\"\"Callback for item menu.\"\"\"\n        if widget.get_active():\n            self.dmenu.copy_clipboard(widget.get_active())\n        else:\n            self.dmenu.copy_clipboard(widget.get_active(), False)\n\n    def on_dmenu_paste_activate(self, widget):\n        \"\"\"Callback for item menu.\"\"\"\n        if widget.get_active():\n            self.dmenu.paste_clipboard(widget.get_", "solution": "\n        dp = self._dp_menu_on\n        if dp and dp.archived:\n            path = dp.fullpath.replace(\" \", \"\\\\ \")\n            QApplication.clipboard().setText(path, QClipboard.Clipboard)\n            QApplication.clipboard().setText(path, QClipboard.Selection)"}
{"prompt": "def _buildElementTree(self,):\n        \"\"\"Turns object into a Element Tree\n        \"\"\"", "output": "\n        element = ElementTree.Element('test')\n        element.text = 'test'\n        element.tail = '\\n'\n        return element\n\n    def test_parse_xml_file(self):\n        \"\"\"XML file parsing\n        \"\"\"\n        file = open(os.path.join(self.test_dir, 'test.xml'), 'r')\n        xml_file = file.read()\n        file.close()\n        self.assertEqual(xml_file, TEST_XML_FILE)\n        self.assertEqual(self._buildElementTree(), TEST_XML_FILE)\n        self.assertEqual(self._buildElementTree(file", "solution": "\n        t_binder = ctree.Element(self.name)\n\n        for k,v in self.__dict__.items():\n            if k not in ('name', 'urls', 'inputs', 'paging') and v :\n                t_binder.set(k,v)\n\n        self.etree = t_binder\n        return t_binder"}
{"prompt": "def _get_vectors(self, tree, precomputed_info):\n        \"\"\"\n        Populate the vectors m and M.\n        \"\"\"", "output": "\n        self.m = {}\n        self.M = {}\n        for i in range(len(tree)):\n            self.m[i] = {}\n            self.M[i]['m'] = tree[i]\n            self.M[i]['m']['m'] = self.m[i]['m']['m']\n            self.M[i]['m']['m']['m'] = self.M[i]['m']['m']\n            self.M[i]['m']['m']['m'] = self.M[i]['m']['m']\n            self.M[i]['m']['m']['m']", "solution": "\n        little_m = []\n        big_m = []\n\n        leaf_nodes = sorted(tree.leaf_nodes(), key=lambda x: x.taxon.label)\n        # inner nodes, sorted order\n        for leaf_a, leaf_b in combinations(leaf_nodes, 2):\n            mrca = tree.mrca(taxa=[leaf_a.taxon, leaf_b.taxon])\n            little_m.append(precomputed_info[mrca].edges_from_root)\n            big_m.append(precomputed_info[mrca].dist_from_root)\n        \n        # leaf nodes, sorted order\n        for leaf in leaf_nodes:\n            little_m.append(1)\n            big_m.append(leaf.edge_length)\n\n        return np.array(little_m), np.array(big_m)"}
{"prompt": "def get_commit_command(self, message, author=None):\n        \"\"\"Get the command to commit changes to tracked files in the working tree.\"\"\"", "output": "\n        if author is None:\n            author = self.author\n        if author is None:\n            return 'git commit'\n        return 'git commit -m \"%s\" %s' % (author, get_commit_message(message, author))\n\n    def get_commit_message(self, message, author=None):\n        \"\"\"Get the commit message for a file in the working tree.\"\"\"\n        if author is None:\n            author = self.author\n        if author is None:\n            return 'unknown'\n        return '%s <%s>' % (author, get_commit_message(message, author))\n\n    def get", "solution": "\n        command = ['git']\n        if author:\n            command.extend(('-c', 'user.name=%s' % author.name))\n            command.extend(('-c', 'user.email=%s' % author.email))\n        command.append('commit')\n        command.append('--all')\n        command.append('--message')\n        command.append(message)\n        return command"}
{"prompt": "def from_file_url(url):\n    \"\"\" Convert from file:// url to file path\n    \"\"\"", "output": "\n    return urlparse.urlparse(url).path\n\n\ndef to_file_url(file_url):\n    \"\"\" Convert to file:// url\n    \"\"\"\n    return urlparse.urljoin(file_url, to_file_path(file_url))\n\n\ndef to_file_path(file_url):\n    \"\"\" Convert to file:// file_url\n    \"\"\"\n    return to_file_url(file_url)\n\n\ndef get_file_content(file_url):\n    \"\"\" Get content of file\n    \"\"\"\n    return urllib2.urlopen(file_url).read()\n\n\ndef get_file_", "solution": "\n    if url.startswith('file://'):\n        url = url[len('file://'):].replace('/', os.path.sep)\n\n    return url"}
{"prompt": "def _binary_stdio():\n    \"\"\"Construct binary stdio streams (not text mode).\n\n    This seems to be different for Window/Unix Python2/3, so going by:\n        https://stackoverflow.com/questions/2850893/reading-binary-data-from-stdin\n    \"\"\"", "output": "\n    # TODO: use a more robust way to read binary data from stdin\n    #       (not text mode)\n    #       (not binary mode)\n    #       (not text mode)\n    #       (not binary mode)\n    #       (not text mode)\n    #       (not text mode)\n    #       (not text mode)\n    #       (not text mode)\n    #       (not text mode)\n    #       (not text mode)\n    #       (not text mode)\n    #       (not text mode)\n    #       (not text mode)\n    #       (not text mode)\n   ", "solution": "\n    PY3K = sys.version_info >= (3, 0)\n\n    if PY3K:\n        # pylint: disable=no-member\n        stdin, stdout = sys.stdin.buffer, sys.stdout.buffer\n    else:\n        # Python 2 on Windows opens sys.stdin in text mode, and\n        # binary data that read from it becomes corrupted on \\r\\n\n        if sys.platform == \"win32\":\n            # set sys.stdin to binary mode\n            # pylint: disable=no-member,import-error\n            import os\n            import msvcrt\n            msvcrt.setmode(sys.stdin.fileno(), os.O_BINARY)\n            msvcrt.setmode(sys.stdout.fileno(), os.O_BINARY)\n        stdin, stdout = sys.stdin, sys.stdout\n\n    return stdin, stdout"}
{"prompt": "def populate_values(self, rows, field):\n        \"\"\"\n        Populates the filter values of this filter using list of rows.\n\n        :param list[dict[str,T]] rows: The row set.\n        :param str field: The field name.\n        \"\"\"", "output": "\n        for row in rows:\n            self.values.append(row)\n\n    def __iter__(self):\n        \"\"\"\n        Iterates over the filters.\n        \"\"\"\n        for filter in self.filters:\n            yield filter\n\n    def __len__(self):\n        \"\"\"\n        Returns the number of rows in this filter.\n        \"\"\"\n        return len(self.values)\n\n    def __getitem__(self, index):\n        \"\"\"\n        Returns the row at the given index.\n\n        :param int index: The row index.\n        \"\"\"\n        return self.values[index]\n\n    def __str__(self):\n        \"\"\"\n        Returns the filter as a", "solution": "\n        self._values.clear()\n        for row in rows:\n            condition = SimpleConditionFactory.create_condition(self._field, row[field])\n            if condition.scheme == 'plain':\n                self._values.append(condition.expression)\n            else:\n                self._conditions.append(condition)"}
{"prompt": "def _saferound(value, decimal_places):\n    \"\"\"\n    Rounds a float value off to the desired precision\n    \"\"\"", "output": "\n    return round(value, decimal_places)\n\n\ndef _get_date_range(start_date, end_date):\n    \"\"\"\n    Returns a two-tuple of (start_date, end_date)\n    \"\"\"\n    start_date = start_date.replace(tzinfo=pytz.UTC)\n    end_date = end_date.replace(tzinfo=pytz.UTC)\n    return start_date, end_date\n\n\ndef _get_date_range_by_name(start_date, end_date, name):\n    \"\"\"\n    Returns a two-tuple of (start_date", "solution": "\n    try:\n        f = float(value)\n    except ValueError:\n        return ''\n    format = '%%.%df' % decimal_places\n    return format % f"}
{"prompt": "def _process_change(self, payload, user, repo, repo_url, project, event,\n                        properties):\n        \"\"\"\n        Consumes the JSON as a python object and actually starts the build.\n\n        :arguments:\n            payload\n                Python Object that represents the JSON sent by GitHub Service\n                Hook.\n        \"\"\"", "output": "\n        self.update_json(payload, 'change')\n        self.start_change_thread()\n\n    def _process_comment(self, payload, comment):\n        \"\"\"\n        Consumes the JSON as a python object and actually starts the build.\n\n        :arguments:\n            payload\n                Python Object that represents the JSON sent by GitHub Service\n                Hook.\n        \"\"\"\n        self.update_json(payload, 'comment')\n        self.start_change_thread()\n\n    def _get_comments(self, repo, project, number):\n        \"\"\"\n        Retrieves the number of comments for a given project.\n\n        :arguments:\n           ", "solution": "\n        changes = []\n        refname = payload['ref']\n\n        # We only care about regular heads or tags\n        match = re.match(r\"^refs/(heads|tags)/(.+)$\", refname)\n        if not match:\n            log.msg(\"Ignoring refname `{}': Not a branch\".format(refname))\n            return changes\n        category = None  # None is the legacy category for when hook only supported push\n        if match.group(1) == \"tags\":\n            category = \"tag\"\n\n        branch = match.group(2)\n        if payload.get('deleted'):\n            log.msg(\"Branch `{}' deleted, ignoring\".format(branch))\n            return changes\n\n        # check skip pattern in commit message. e.g.: [ci skip] and [skip ci]\n        head_msg = payload['head_commit'].get('message', '')\n        if self._has_skip(head_msg):\n            return changes\n        commits = payload['commits']\n        if payload.get('created'):\n            commits = [payload['head_commit']]\n        for commit in commits:\n            files = []\n            for kind in ('added', 'modified', 'removed'):\n                files.extend(commit.get(kind, []))\n\n            when_timestamp = dateparse(commit['timestamp'])\n\n            log.msg(\"New revision: {}\".format(commit['id'][:8]))\n\n            change = {\n                'author': '{} <{}>'.format(commit['author']['name'],\n                                           commit['author']['email']),\n                'files': files,\n                'comments': commit['message'],\n                'revision': commit['id'],\n                'when_timestamp': when_timestamp,\n                'branch': branch,\n                'revlink': commit['url'],\n                'repository': repo_url,\n                'project': project,\n                'properties': {\n                    'github_distinct': commit.get('distinct', True),\n                    'event': event,\n                },\n                'category': category\n            }\n            # Update with any white-listed github event properties\n            change['properties'].update(properties)\n\n            if callable(self._codebase):\n                change['codebase'] = self._codebase(payload)\n            elif self._codebase is not None:\n                change['codebase'] = self._codebase\n\n            changes.append(change)\n\n        return changes"}
{"prompt": "def redirect(self, url: str, permanent: bool = False, status: int = None) -> None:\n        \"\"\"Sends a redirect to the given (optionally relative) URL.\n\n        If the ``status`` argument is specified, that value is used as the\n        HTTP status code; otherwise either 301 (permanent) or 302\n        (temporary) is chosen based on the ``permanent`` argument.\n        The default is 302 (temporary).\n        \"\"\"", "output": "\n        if self.closed:\n            raise ValueError(\"Request closed on a closed session.\")\n        if status is None:\n            status = 303 if permanent else 302\n        self.redirect_cache[url] = (status, permanent)\n\n    def http_error_302(self, url: str, permanent: bool = False, status: int = None) -> None:\n        \"\"\"Sends a HTTP error 302 to the given (optionally relative) URL.\n\n        If the ``status`` argument is specified, that value is used as the\n        HTTP status code; otherwise either 301 (permanent) or 302\n        (temporary) is chosen based", "solution": "\n        if self._headers_written:\n            raise Exception(\"Cannot redirect after headers have been written\")\n        if status is None:\n            status = 301 if permanent else 302\n        else:\n            assert isinstance(status, int) and 300 <= status <= 399\n        self.set_status(status)\n        self.set_header(\"Location\", utf8(url))\n        self.finish()"}
{"prompt": "def insert_or_replace_entity(self, entity):\n        \"\"\"\n        Adds an insert or replace entity operation to the batch. See \n        :func:`~azure.storage.table.tableservice.TableService.insert_or_replace_entity` for more \n        information on insert or replace operations.\n\n        The operation will not be executed until the batch is committed.\n\n        :param entity:\n            The entity to insert or replace. Could be a dict or an entity object. \n            Must contain a PartitionKey and a RowKey.\n        :type entity: a dict or :class:`azure.storage.table.models.Entity`\n       \"\"\"", "output": "\n        :keyword int timeout:\n            The timeout parameter is expressed in seconds.\n        :keyword bool skip_token:\n            If true, the first token in a page will be skipped.\n        :keyword bool continuation_token:\n            If true, the continuation token will be appended to each page \n            before it is returned in the result.\n        :keyword bool is_last_page:\n            If true, the operation will return only the last \n            page of data. If false, the operation will \n            return only the data that you have in the last page.\n        :keyword bool is_empty:\n            If true, the operation will return", "solution": "\n        request = _insert_or_replace_entity(entity)\n        self._add_to_batch(entity['PartitionKey'], entity['RowKey'], request)"}
{"prompt": "def _log_config_on_step(self, trial_state, new_state, trial,\n                            trial_to_clone, new_config):\n        \"\"\"Logs transition during exploit/exploit step.\n\n        For each step, logs: [target trial tag, clone trial tag, target trial\n        iteration, clone trial iteration, old config, new config].\n        \"\"\"", "output": "\n        for tag in trial.tags:\n            if tag.name == tag_name:\n                trial_state.log_tags.append(tag)\n                new_trial_tag = tag\n                new_config = trial_config.copy()\n                _log_tag_to_clone(new_config, trial_tag=trial_tag,\n                                iteration=trial.iteration)\n                trial_state.log_iteration_tag = trial_tag\n                trial_state.log_target_iteration = iteration_tag = None\n                trial_iteration_tag = None\n                iteration_tag = None\n                target_config =", "solution": "\n        trial_name, trial_to_clone_name = (trial_state.orig_tag,\n                                           new_state.orig_tag)\n        trial_id = \"\".join(itertools.takewhile(str.isdigit, trial_name))\n        trial_to_clone_id = \"\".join(\n            itertools.takewhile(str.isdigit, trial_to_clone_name))\n        trial_path = os.path.join(trial.local_dir,\n                                  \"pbt_policy_\" + trial_id + \".txt\")\n        trial_to_clone_path = os.path.join(\n            trial_to_clone.local_dir,\n            \"pbt_policy_\" + trial_to_clone_id + \".txt\")\n        policy = [\n            trial_name, trial_to_clone_name,\n            trial.last_result[TRAINING_ITERATION],\n            trial_to_clone.last_result[TRAINING_ITERATION],\n            trial_to_clone.config, new_config\n        ]\n        # Log to global file.\n        with open(os.path.join(trial.local_dir, \"pbt_global.txt\"), \"a+\") as f:\n            f.write(json.dumps(policy) + \"\\n\")\n        # Overwrite state in target trial from trial_to_clone.\n        if os.path.exists(trial_to_clone_path):\n            shutil.copyfile(trial_to_clone_path, trial_path)\n        # Log new exploit in target trial log.\n        with open(trial_path, \"a+\") as f:\n            f.write(json.dumps(policy) + \"\\n\")"}
{"prompt": "def only_trashed(cls):\n        \"\"\"\n        Get a new query builder that only includes soft deletes\n\n        :type cls: orator.orm.model.Model\n\n        :rtype: orator.orm.builder.Builder\n        \"\"\"", "output": "\n        return Orator.objects.filter(\n            Q(trashed=True) |\n            Q(trashed=False)\n        ).order_by('pk').first()\n\n    @classmethod\n    def get_by_id(cls, orator_id):\n        \"\"\"\n        Get a query builder by its id\n\n        :type orator_id: int\n\n        :rtype: QueryBuilder\n        \"\"\"\n        return cls.objects.get(pk=int(orator_id))\n\n    @classmethod\n    def get_by_name(cls, name):\n        \"\"\"\n        Get a query builder by its name\n\n        :type name:", "solution": "\n        instance = cls()\n\n        column = instance.get_qualified_deleted_at_column()\n\n        return instance.new_query_without_scope(SoftDeletingScope()).where_not_null(\n            column\n        )"}
{"prompt": "def assignee(self, assignee_id, action='ADD'):\n        \"\"\"\n        Adds a assignee to the task\n\n        Args:\n            assignee_id: The id of the assignee to be added\n            action:\n\n        \"\"\"", "output": "\n        if action == 'ADD':\n            self.add_assignee(assignee_id)\n        elif action == 'REMOVE':\n            self.remove_assignee(assignee_id)\n        else:\n            raise ValueError('Invalid action')\n\n    def add_assignee(self, assignee_id, assignee_type='Assignee', action='ADD'):\n        \"\"\"\n        Adds a new assignee to the task\n\n        Args:\n            assignee_id: The id of the assignee to be added\n            assignee_type: The type of the assignee", "solution": "\n        if not self.can_update():\n            self._tcex.handle_error(910, [self.type])\n\n        return self.tc_requests.assignee(\n            self.api_type, self.api_sub_type, self.unique_id, assignee_id, action=action\n        )"}
{"prompt": "def unique_addresses(data=None, file_path=None):\n    \"\"\"\n    The function to search an input string and/or file, extracting and\n    counting IPv4/IPv6 addresses/networks. Summarizes ports with sub-counts.\n    If both a string and file_path are provided, it will process them both.\n\n    Args:\n        data (:obj:`str`): The data to process.\n        file_path (:obj:`str`): An optional file path to process.\n\n    Returns:\n        dict: The addresses/networks mapped to ports and counts:\n\n        ::\n\n            {\n                '1.2.3.4' (dict) - Each address or network found is a\n                    dictionary:\n                    {\n                        'count' (int) - Total number of times seen.\n                        'ports' (dict) - Mapping of port numbers as keys and\n                            the number of times seen for this ip as values.\n                    }\n            }\n\n    Raises:\n        ValueError: Arguments provided are invalid.\n    \"\"\"", "output": ".4': {\n                    'address': '1.2.3.4',\n                    'count': '1.2.3.4'\n                }\n            }\n\n        Otherwise, it will return None.\n    \"\"\"\n    if data is None:\n        data = get_unique_input_addresses()\n\n    if file_path is not None:\n        return _parse_ports_file(file_path, data)\n    else:\n        return _parse_ports(data, None)\n\n\ndef get_unique_input_addresses():\n    \"\"\"\n    Returns:\n        list: The list of IPv4/IPv6 addresses/", "solution": "\n\n    if not data and not file_path:\n\n        raise ValueError('No data or file path provided.')\n\n    ret = {}\n    base = {\n        'count': 0,\n        'ports': {}\n    }\n\n    file_data = None\n    if file_path:\n\n        log.debug('Opening file for unique address analysis: {0}'.format(\n                str(file_path)))\n\n        f = open(str(file_path), 'r')\n\n        # Read the file.\n        file_data = f.read()\n\n    pattern = re.compile(\n        str(IP_REGEX),\n        re.DOTALL\n    )\n\n    # Check if there is data.\n    log.debug('Analyzing input/file data'.format(\n                str(file_path)))\n    for input_data in [data, file_data]:\n\n        if input_data:\n\n            # Search for IPs.\n            for match in pattern.finditer(input_data):\n\n                is_net = False\n                port = None\n                try:\n\n                    found = match.group('ip')\n\n                    if '.' in found and ':' in found:\n\n                        split = found.split(':')\n                        ip_or_net = split[0]\n                        port = split[1]\n\n                    elif '[' in found:\n\n                        split = found.split(']:')\n                        ip_or_net = split[0][1:]\n                        port = split[1]\n\n                    elif '/' in found:\n\n                        is_net = True\n                        ip_or_net = found\n\n                    else:\n\n                        ip_or_net = found\n\n                    if is_net:\n\n                        ip_obj = ip_network(ip_or_net)\n\n                    else:\n                        ip_obj = ip_address(ip_or_net)\n\n                    obj_str = ip_obj.__str__()\n\n                    if obj_str not in ret.keys():\n\n                        ret[obj_str] = copy.deepcopy(base)\n\n                    ret[obj_str]['count'] += 1\n\n                    if port:\n\n                        try:\n\n                            ret[obj_str]['ports'][str(port)] += 1\n\n                        except KeyError:\n\n                            ret[obj_str]['ports'][str(port)] = 1\n\n                except (KeyError, ValueError):\n\n                    continue\n\n    return ret"}
{"prompt": "def _get_cached_path(\n            self,\n            field_name,\n            path_or_url,\n            download_if_missing=False,\n            overwrite=False):\n        \"\"\"\n        Get the local path for a possibly remote file, invoking either\n        a download or install error message if it's missing.\n        \"\"\"", "output": "\n        if not path_or_url:\n            raise ValueError(\n                \"Either a path to a file or a URL must be provided.\")\n\n        if not os.path.isfile(path_or_url):\n            raise ValueError(\n                \"The file '%s' does not exist.\" % path_or_url)\n\n        if os.path.isdir(path_or_url):\n            if not overwrite:\n                raise ValueError(\n                    \"The file '%s' exists, overwrite it?\" % path_or_url)\n            else:\n                return path_or_url\n\n        if os.path.isfile(path_", "solution": "\n        if len(field_name) == 0:\n            raise ValueError(\"Expected non-empty field name\")\n        if len(path_or_url) == 0:\n            raise ValueError(\"Expected non-empty path_or_url\")\n        return self.download_cache.local_path_or_install_error(\n            field_name=field_name,\n            path_or_url=path_or_url,\n            download_if_missing=download_if_missing,\n            overwrite=overwrite)"}
{"prompt": "def generate_help_text(self):\n        \"\"\" generates the help text based on commands typed \"\"\"", "output": "\n        text = \"\"\n        if self.commands:\n            text += \"commands:\\n\"\n            for command in self.commands:\n                text += generate_help_text(command)\n        return text\n\n    def __str__(self):\n        \"\"\" return a string representation of the command \"\"\"\n        text = \"\"\n        if self.name:\n            text += \"name: %s\\n\" % self.name\n        if self.description:\n            text += \"description: %s\\n\" % self.description\n        if self.enabled:\n            text += \"enabled: %s\\n\" % self.enabled\n        if self.", "solution": "\n        param_descrip = example = \"\"\n        self.description_docs = u''\n\n        rows, _ = get_window_dim()\n        rows = int(rows)\n\n        param_args = self.completer.leftover_args\n        last_word = self.completer.unfinished_word\n        command = self.completer.current_command\n        new_command = ' '.join([command, last_word]).strip()\n\n        if not self.completer.complete_command and new_command in self.completer.command_description:\n            command = new_command\n\n        # get command/group help\n        if self.completer and command in self.completer.command_description:\n            self.description_docs = u'{}'.format(self.completer.command_description[command])\n\n        # get parameter help if full command\n        if self.completer and command in self.completer.command_param_info:\n            param = param_args[-1] if param_args else ''\n            param = last_word if last_word.startswith('-') else param\n\n            if param in self.completer.command_param_info[command] and self.completer.has_description(\n                    command + \" \" + param):\n                param_descrip = ''.join([\n                    param, \":\", '\\n', self.completer.param_description.get(command + \" \" + param, '')])\n\n            if command in self.completer.command_examples:\n                string_example = []\n                for example in self.completer.command_examples[command]:\n                    for part in example:\n                        string_example.append(part)\n                ''.join(string_example)\n                example = self._space_examples(\n                    self.completer.command_examples[command], rows, self.example_page)\n\n        return param_descrip, example"}
{"prompt": "def read_header(self):\n        \"\"\"Read the header of a MPQ archive.\"\"\"", "output": "\n        with open(self.filename, 'rb') as f:\n            return read_mm_header(f)\n\n    def read_frames(self, nframes):\n        \"\"\"Read the frames of a MPQ archive.\"\"\"\n        with open(self.filename, 'rb') as f:\n            return read_mm_frames(f, nframes)\n\n    def read_frames_old(self, nframes):\n        \"\"\"Read the frames of a MPQ archive with a new version.\"\"\"\n        with open(self.filename, 'rb') as f:\n            return read_mm_frames(f, nframes)", "solution": "\n\n        def read_mpq_header(offset=None):\n            if offset:\n                self.file.seek(offset)\n            data = self.file.read(32)\n            header = MPQFileHeader._make(\n                struct.unpack(MPQFileHeader.struct_format, data))\n            header = header._asdict()\n            if header['format_version'] == 1:\n                data = self.file.read(12)\n                extended_header = MPQFileHeaderExt._make(\n                    struct.unpack(MPQFileHeaderExt.struct_format, data))\n                header.update(extended_header._asdict())\n            return header\n\n        def read_mpq_user_data_header():\n            data = self.file.read(16)\n            header = MPQUserDataHeader._make(\n                struct.unpack(MPQUserDataHeader.struct_format, data))\n            header = header._asdict()\n            header['content'] = self.file.read(header['user_data_header_size'])\n            return header\n\n        magic = self.file.read(4)\n        self.file.seek(0)\n\n        if magic == b'MPQ\\x1a':\n            header = read_mpq_header()\n            header['offset'] = 0\n        elif magic == b'MPQ\\x1b':\n            user_data_header = read_mpq_user_data_header()\n            header = read_mpq_header(user_data_header['mpq_header_offset'])\n            header['offset'] = user_data_header['mpq_header_offset']\n            header['user_data_header'] = user_data_header\n        else:\n            raise ValueError(\"Invalid file header.\")\n\n        return header"}
{"prompt": "def construct_request(self, request_args=None, **kwargs):\n        \"\"\"\n        The method where everything is setup for sending the request.\n        The request information is gathered and the where and how of sending the\n        request is decided.\n\n        :param request_args: Initial request arguments\n        :param kwargs: Extra keyword arguments\n        :return: A dictionary with the keys 'url' and possibly 'body', 'kwargs',\n            'request' and 'ht_args'.\n        \"\"\"", "output": "\n        if request_args is None:\n            request_args = {}\n        if request_args.get('url', None):\n            request_args['url'] = self.url_for(request_args)\n        if request_args.get('body', None):\n            request_args['body'] = self.body_for(request_args)\n        if request_args.get('kwargs', None):\n            request_args['kwargs'] = self.kwargs_for(request_args)\n        if request_args.get('ht_args', None):\n            request_args['ht_args'] =", "solution": "\n        if request_args is None:\n            request_args = {}\n\n        # remove arguments that should not be included in the request\n        # _args = dict(\n        #    [(k, v) for k, v in kwargs.items() if v and k not in SPECIAL_ARGS])\n\n        return self.construct(request_args, **kwargs)"}
{"prompt": "def get_details(self, obj):\n        \"\"\" return detail url \"\"\"", "output": "\n        return obj.get_absolute_url()\n\n    def get_context_data(self, **kwargs):\n        \"\"\"\n        Return context data for rendering the view\n        \"\"\"\n        context = super(PageAdmin, self).get_context_data(**kwargs)\n        context['title'] = self.title\n        context['page_type'] = self.page_type\n        context['page_id'] = self.id\n        context['page_name'] = self.name\n        context['page_url'] = self.get_details(self.get_object())\n        return context\n\n    def get_urls(self):", "solution": "\n        return reverse('api_user_social_links_detail',\n                       args=[obj.user.username, obj.pk],\n                       request=self.context.get('request'),\n                       format=self.context.get('format'))"}
{"prompt": "def __get_package_manager(self):\n        \"\"\"\n        Installs and verifies package manager\n        \"\"\"", "output": "\n        if not self.is_installed():\n            raise RuntimeError(\"Package manager is not installed\")\n        if not self.is_installed():\n            raise RuntimeError(\"Package manager is not installed\")\n        if not self.is_installed():\n            raise RuntimeError(\"Package manager is not installed\")\n        if not self.is_installed():\n            raise RuntimeError(\"Package manager is not installed\")\n\n    def is_installed(self):\n        \"\"\"\n        Checks if the package manager is installed\n        \"\"\"\n        if not self.is_installed_path():\n            return False\n        return True\n\n    def is_installed_path(self):\n        \"\"\"\n        Checks if", "solution": "\n        package_manager = \"\"\n        args = \"\"\n        sudo_required = True\n        if system.is_osx():\n            package_manager = \"brew\"\n            sudo_required = False\n            args = \" install\"\n        elif system.is_debian():\n            package_manager = \"apt-get\"\n            args = \" -y install\"\n        elif system.is_fedora():\n            package_manager = \"yum\"\n            args = \" install\"\n        elif system.is_arch():\n            package_manager = \"pacman\"\n            args = \" --noconfirm -S\"\n        if lib.which(package_manager) is None:\n            self.logger.warn(\"Package manager %s not installed! Packages will not be installed.\"\n                             % package_manager)\n            self.package_manager = None\n        self.package_manager = package_manager\n        self.sudo_required = sudo_required\n        self.args = args"}
{"prompt": "def use(wcspkg, raise_err=True):\n    \"\"\"Choose WCS package.\"\"\"", "output": "\n    if not raise_err:\n        return WCSPackage(wcspkg, 'wcs', '1.0.0', wcspkg.wcs.naxispath)\n    else:\n        return WCSPackage(wcspkg, 'wcs', '1.0.0', wcspkg.wcs.naxispath, raise_err=False)\n\n\ndef use_wcs_time(wcspkg, raise_err=True):\n    \"\"\"Choose WCS time.\"\"\"\n    if not raise_err:\n        return WCSTimePackage(wcspkg, 'wcs', '1.0.", "solution": "\n    global coord_types, wcs_configured, WCS\n\n    if wcspkg not in common.custom_wcs:\n        # Try to dynamically load WCS\n        modname = 'wcs_%s' % (wcspkg)\n        path = os.path.join(wcs_home, '%s.py' % (modname))\n        try:\n            my_import(modname, path)\n        except ImportError:\n            return False\n\n    if wcspkg in common.custom_wcs:\n        bnch = common.custom_wcs[wcspkg]\n        WCS = bnch.wrapper_class\n        coord_types = bnch.coord_types\n        wcs_configured = True\n        return True\n\n    return False"}
{"prompt": "def addRandomEffect(self,K=None,covar_type='freeform',is_noise=False,normalize=True,Ks=None,offset=1e-4,rank=1,covar_K0=None):\n        \"\"\"\n        Add random effect Term\n        depending on self.P=1 or >1 add single trait or multi trait random effect term\n        \"\"\"", "output": "\n        if self.P==1:\n            if covar_type=='freeform':\n                if is_noise:\n                    self.P = 1\n                else:\n                    self.P = random.randint(1,self.P)\n            else:\n                if is_noise:\n                    self.P = 1\n                else:\n                    self.P = random.randint(1,self.P)\n            if Ks==None:\n                Ks = self.Ks\n            if offset==None:\n                offset = self.offset\n            if rank==None:\n                rank = self.rank\n        else:\n            if covar_", "solution": "\n        if self.P==1:\tself.addSingleTraitTerm(K=K,is_noise=is_noise,normalize=normalize,Ks=Ks)\n        else:\t\t\tself.addMultiTraitTerm(K=K,covar_type=covar_type,is_noise=is_noise,normalize=normalize,Ks=Ks,offset=offset,rank=rank,covar_K0=covar_K0)"}
{"prompt": "def _get_pool(name=None, session=None):\n    \"\"\"\n    Get XEN resource pool object reference\n    \"\"\"", "output": "\n    if name is None:\n        name = \"default\"\n    if session is None:\n        session = get_session()\n    return session.xenapi.pool.get_all()[name]\n\n\ndef _get_record(record_type, ref):\n    \"\"\"\n    Get record by ref\n    \"\"\"\n    if record_type == \"VM\":\n        return _VMRecord(ref)\n    elif record_type == \"VDI\":\n        return _VDIRecord(ref)\n    elif record_type == \"VBD\":\n        return _VBDRecord(ref)\n    elif record_type == \"VBD_snapshot\":\n       ", "solution": "\n    if session is None:\n        session = _get_session()\n    pools = session.xenapi.pool.get_all()\n    for pool in pools:\n        pool_record = session.xenapi.pool.get_record(pool)\n        if name in pool_record.get('name_label'):\n            return pool\n    return None"}
{"prompt": "def _getBasicOrbit(self, orbit=None):\n        \"\"\"Load a particular orbit into .data for loaded day.\n\n        Parameters\n        ----------\n        orbit : int\n            orbit number, 1 indexed, negative indexes allowed, -1 last orbit\n\n        Note\n        ----\n        A day of data must be loaded before this routine functions properly.\n        If the last orbit of the day is requested, it will NOT automatically be\n        padded with data from the next day.\n        \"\"\"", "output": "\n        if orbit is None:\n            orbit = -1\n        if orbit < 0:\n            orbit = len(self.data) + orbit\n        if orbit > len(self.data):\n            raise ValueError(\"Invalid orbit number\")\n        return self.data[orbit:orbit+1]\n\n    def _getDate(self, date):\n        \"\"\"Get the date from the date string.\n\n        Parameters\n        ----------\n        date : str\n            date string\n\n        Returns\n        -------\n        datetime.date\n            date object\n        \"\"\"\n        if date is None:\n            return None\n        return datetime.datetime.strptime(", "solution": "\n        # ensure data exists\n        if not self.sat.empty:\n            # ensure proper orbit metadata present\n            self._calcOrbits()\n\n            # ensure user is requesting a particular orbit\n            if orbit is not None:\n                # pull out requested orbit\n                if orbit == -1:\n                    # load orbit data into data\n                    self.sat.data = self._fullDayData[self._orbit_breaks[self.num + orbit]:]\n                    self._current = self.num + orbit + 1\n                elif ((orbit < 0) & (orbit >= -self.num)):\n                    # load orbit data into data\n                    self.sat.data = self._fullDayData[\n                                    self._orbit_breaks[self.num + orbit]:self._orbit_breaks[self.num + orbit + 1]]\n                    self._current = self.num + orbit + 1\n                elif (orbit < self.num) & (orbit != 0):\n                    # load orbit data into data\n                    self.sat.data = self._fullDayData[self._orbit_breaks[orbit - 1]:self._orbit_breaks[orbit]]\n                    self._current = orbit\n                elif orbit == self.num:\n                    self.sat.data = self._fullDayData[self._orbit_breaks[orbit - 1]:]\n                    # recent addition, wondering why it wasn't there before,\n                    # could just be a bug that is now fixed.\n                    self._current = orbit\n                elif orbit == 0:\n                    raise ValueError('Orbits internally indexed by 1, 0 not ' +\n                                     'allowed')\n                else:\n                    # gone too far\n                    self.sat.data = []\n                    raise ValueError('Requested an orbit past total orbits ' +\n                                     'for day')\n            else:\n                raise ValueError('Must set an orbit')"}
{"prompt": "def multi_raw(query, params, models, model_to_fields):\n    \"\"\"Scoop multiple model instances out of the DB at once, given a query that\n    returns all fields of each.\n\n    Return an iterable of sequences of model instances parallel to the\n    ``models`` sequence of classes. For example::\n\n        [(<User such-and-such>, <Watch such-and-such>), ...]\n\n    \"\"\"", "output": "\n    if isinstance(models, basestring):\n        models = [models]\n    if not isinstance(models, (list, tuple)):\n        models = [models]\n\n    for model in models:\n        if isinstance(model, basestring):\n            yield model\n        elif isinstance(model, (tuple, list)):\n            for item in multi_raw(item, params, models, model_to_fields):\n                yield item\n        else:\n            raise ValueError(\"Invalid model: %s\" % model)\n\n\ndef model_to_dict(model, fields):\n    \"\"\"Convert a model to a dictionary.\n\n    Return an iterable of ``{", "solution": "\n    cursor = connections[router.db_for_read(models[0])].cursor()\n    cursor.execute(query, params)\n    rows = cursor.fetchall()\n\n    for row in rows:\n        row_iter = iter(row)\n        yield [model_class(**dict((a, next(row_iter))\n                           for a in model_to_fields[model_class]))\n               for model_class in models]"}
{"prompt": "def shift_christmas_boxing_days(self, year):\n        \"\"\" When Christmas and/or Boxing Day falls on a weekend, it is rolled\n            forward to the next weekday.\n        \"\"\"", "output": "\n        if year < 1900:\n            return self.get_weekend_date(year + 1)\n        else:\n            return self.get_weekend_date(year + 2)\n\n    def get_weekend_date(self, year):\n        \"\"\" Returns the date of the next weekday.\n        \"\"\"\n        if year < 1900:\n            return datetime.date(year, 1, 1)\n        else:\n            return datetime.date(year, 2, 1)\n\n    def get_next_weekday_date(self, year):\n        \"\"\" Returns the date of the next weekday.\n        \"\"\"\n        if year", "solution": "\n        christmas = date(year, 12, 25)\n        boxing_day = date(year, 12, 26)\n        boxing_day_label = \"{} Shift\".format(self.boxing_day_label)\n        results = []\n        if christmas.weekday() in self.get_weekend_days():\n            shift = self.find_following_working_day(christmas)\n            results.append((shift, \"Christmas Shift\"))\n            results.append((shift + timedelta(days=1), boxing_day_label))\n        elif boxing_day.weekday() in self.get_weekend_days():\n            shift = self.find_following_working_day(boxing_day)\n            results.append((shift, boxing_day_label))\n        return results"}
{"prompt": "def _readBlock(self):\n        \"\"\"Read a block of data from the remote reader.\"\"\"", "output": "\n        if self._block:\n            return self._block\n\n        # Read the data from the remote reader\n        self._block = self.remoteReader.read(self.remoteReader.CHUNK_SIZE)\n        self._block = self._readBlock()\n\n        # If we have a partial message, we need to check the message size\n        # and if it's not we'll raise an exception.\n        if self._block:\n            self._block = self._readBlock()\n            if self._block:\n                self._block = _bytes2NativeString(self._block)\n                if self._block:\n                    self._block =", "solution": "\n\n        if self.interrupted or self.fp is None:\n            if self.debug:\n                log.msg('WorkerFileDownloadCommand._readBlock(): end')\n            return True\n\n        length = self.blocksize\n        if self.bytes_remaining is not None and length > self.bytes_remaining:\n            length = self.bytes_remaining\n\n        if length <= 0:\n            if self.stderr is None:\n                self.stderr = \"Maximum filesize reached, truncating file '{0}'\".format(\n                    self.path)\n                self.rc = 1\n            return True\n        else:\n            d = self.reader.callRemote('read', length)\n            d.addCallback(self._writeData)\n            return d"}
{"prompt": "def is_nash(self, action_profile, tol=None):\n        \"\"\"\n        Return True if `action_profile` is a Nash equilibrium.\n\n        Parameters\n        ----------\n        action_profile : array_like(int or array_like(float))\n            An array of N objects, where each object must be an integer\n            (pure action) or an array of floats (mixed action).\n\n        tol : scalar(float)\n            Tolerance level used in determining best responses. If None,\n            default to each player's `tol` attribute value.\n\n        Returns\n        -------\n        bool\n            True if `action_profile` is a Nash equilibrium; False\n            otherwise.\n\n        \"\"\"", "output": "action_profile` is a Nash equilibrium.\n\n        \"\"\"\n        if tol is None:\n            if self.player_equilibrium is not None:\n                tol = self.player_equilibrium.get_response_tolerance(self.player_equilibrium.get_player_id(self.player_id))\n            else:\n                tol = self.default_tolerance\n        else:\n            if self.player_equilibrium is not None:\n                tol = self.player_equilibrium.get_response_tolerance(self.", "solution": "\n        if self.N == 2:\n            for i, player in enumerate(self.players):\n                own_action, opponent_action = \\\n                    action_profile[i], action_profile[1-i]\n                if not player.is_best_response(own_action, opponent_action,\n                                               tol):\n                    return False\n\n        elif self.N >= 3:\n            for i, player in enumerate(self.players):\n                own_action = action_profile[i]\n                opponents_actions = \\\n                    tuple(action_profile[i+1:]) + tuple(action_profile[:i])\n\n                if not player.is_best_response(own_action, opponents_actions,\n                                               tol):\n                    return False\n\n        else:  # Trivial case with self.N == 1\n            if not self.players[0].is_best_response(action_profile[0], None,\n                                                    tol):\n                return False\n\n        return True"}
{"prompt": "def parse_plugin_metadata(content):\n  \"\"\"Parse summary metadata to a Python object.\n\n  Arguments:\n    content: The `content` field of a `SummaryMetadata` proto\n      corresponding to the pr_curves plugin.\n\n  Returns:\n    A `PrCurvesPlugin` protobuf object.\n  \"\"\"", "output": "\n  return ProtobufPlugin(content)\n\n\ndef parse_plugin_metadata_from_proto(content):\n  \"\"\"Parse summary metadata from a Python object.\n\n  Arguments:\n    content: The `content` field of a `SummaryMetadata` proto\n      corresponding to the pr_curves plugin.\n\n  Returns:\n    A `SummaryMetadata` protobuf object.\n  \"\"\"\n  return ProtobufPlugin.from_summary_metadata(content)\n\n\ndef parse_plugin_metadata_from_summary(summary_metadata):\n  \"\"\"Parse Python object from a `SummaryMetadata` proto.\n\n  Arguments:\n    summary_metadata:", "solution": "\n  if not isinstance(content, bytes):\n    raise TypeError('Content type must be bytes')\n  result = plugin_data_pb2.PrCurvePluginData.FromString(content)\n  if result.version == 0:\n    return result\n  else:\n    logger.warn(\n        'Unknown metadata version: %s. The latest version known to '\n        'this build of TensorBoard is %s; perhaps a newer build is '\n        'available?', result.version, PROTO_VERSION)\n    return result"}
{"prompt": "def refresh_items(self):\n        \"\"\" Refresh the items of the pattern.\n        This method destroys the old items and creates and initializes\n        the new items.\n\n        It is overridden to NOT insert the children to the parent. The Fragment\n        adapter handles this.\n\n        \"\"\"", "output": "\n        pass\n\n    def _get_pattern_text(self):\n        \"\"\" Return the text of the pattern.\n        This method returns the text of the pattern.\n\n        \"\"\"\n        return _(\"Refreshing %(count)d of %(total)d\") % {\n            'count': self.pattern_text.format(count=self.count, total=self.total),\n            'total': self.pattern_text.format(total=self.pattern_text.format(count=self.total))}\n\n    def _set_pattern_text(self, text):\n        \"\"\" Set the text of the pattern.\n\n        \"\"\"", "solution": "\n        items = []\n        if self.condition:\n            for nodes, key, f_locals in self.pattern_nodes:\n                with new_scope(key, f_locals):\n                    for node in nodes:\n                        child = node(None)\n                        if isinstance(child, list):\n                            items.extend(child)\n                        else:\n                            items.append(child)\n\n        for old in self.items:\n            if not old.is_destroyed:\n                old.destroy()\n\n        #: Insert items into THIS node, NOT the PARENT\n        #if len(items) > 0:\n        #    self.parent.insert_children(self, items)\n        self.items = items"}
{"prompt": "def request(self, method, url, data=None, files=None, query=None, headers=None, timeout=60):\n        \"\"\"\n        Mechanism for issuing an API call\n        \"\"\"", "output": "\n        if data is None:\n            data = {}\n        if files is None:\n            files = {}\n        if query is None:\n            query = {}\n        if timeout is None:\n            timeout = self.timeout\n        if self.token:\n            headers['Authorization'] = self.token\n        if self.timeout:\n            headers['Timeout'] = self.timeout\n        if self.timeout_read:\n            headers['Timeout'] = self.timeout_read\n        if self.timeout_write:\n            headers['Timeout'] = self.timeout_write\n        if self.timeout_read:\n            headers['Timeout'] = self.timeout_", "solution": "\n        headers = headers or {}\n\n        if self.token:\n            my_token = self.token\n        else:\n            from figure import token\n            my_token = token\n\n        if my_token:\n            self.__set_authorization(headers, my_token)\n\n        METHODS = {\n            'get': self.__get,\n            'post': self.__post,\n            'put': self.__put,\n            'head': self.__head,\n            'patch': self.__patch\n        }\n        request_method = METHODS[method.lower()]\n\n        abs_url = urlparse.urljoin(self.api_base, url)\n\n        encoded_query = urllib.urlencode(query or {})\n\n        abs_url = _build_api_url(abs_url, encoded_query)\n\n        try:\n            response = request_method(abs_url, data=data, files=files, headers=headers, timeout=timeout)\n            response.encoding = 'utf-8'\n        except RequestException:\n            raise error.APIConnectionError()\n\n        return self._interpret_response(response.text, response.status_code)"}
{"prompt": "def _createbound(obj):\n    \"\"\"Create a new BoundNode representing a given object.\"\"\"", "output": "\n    if obj.parent is None:\n        return BoundNode(parent=None, parent_context=None)\n    else:\n        return BoundNode(parent=obj.parent, parent_context=obj)\n\n\nclass _ScopedExpr(Expr):\n    \"\"\"\n    ScopedExpr is a compiled expression that can be used as a scope.\n    \"\"\"\n    subexprs = ['target', 'value']\n\n    def as_const(self, eval_ctx=None):\n        # This is needed because the parser could be called with \"as_call\"\n        # as well, which would otherwise conflict with Python's scope\n       ", "solution": "\n    # Start by allowing objects to define custom unbound reference hooks\n    try:\n        kls = obj._unboundreference_()\n    except AttributeError:\n        kls = type(obj)\n    \n    unbound = _createunbound(kls)\n    def valueget():\n        return obj\n    for t in (BoundBitfieldNode, BoundStructureNode, BoundArrayNode):\n        if isinstance(unbound, t._unboundtype):\n            kls = t\n            break\n    else:\n        kls = BoundSimpleNode\n    \n    child = kls(unbound, valueget)\n    return child"}
{"prompt": "def do_exit(self, line):\n        \"\"\"exit Exit from the CLI.\"\"\"", "output": "\n        self.exit_code = self.EXIT_OK\n        self.exit_message = \"\"\n        self.exit_code_str = \"\"\n        self.exit_message_str = \"\"\n        self.exit_message_str = \"\"\n        self.exit_message = line\n        return self\n\n    def do_quit(self, line):\n        \"\"\"quit.\"\"\"\n        self.exit_code = self.EXIT_OK\n        self.exit_message = \"\"\n        return self\n\n    def do_EOF(self, line):\n        \"\"\"EOF.\"\"\"\n        self.exit_code = self.EXIT_OK\n        self.exit", "solution": "\n        n_remaining_operations = len(self._command_processor.get_operation_queue())\n        if n_remaining_operations:\n            d1_cli.impl.util.print_warn(\n                "}
{"prompt": "def addprojecthook(self, project_id, url, push=False, issues=False, merge_requests=False, tag_push=False):\n        \"\"\"\n        add a hook to a project\n\n        :param project_id: project id\n        :param url: url of the hook\n        :return: True if success\n        \"\"\"", "output": "\n        if not self.projects:\n            self.projects = self.get_projects()\n        if not self.projects:\n            return False\n        if not self.push:\n            self.push_status = 'pending'\n            if issues:\n                self.push_status = 'open'\n        else:\n            self.push_status = 'open'\n        if push:\n            self.push_status = 'push'\n        if issues:\n            self.push_status = 'issues'\n        if merge_requests:\n            self.merge_status ='merge_requests'\n        if tag_push:\n            self.tag", "solution": "\n        data = {\n            'id': project_id,\n            'url': url,\n            'push_events': int(bool(push)),\n            'issues_events': int(bool(issues)),\n            'merge_requests_events': int(bool(merge_requests)),\n            'tag_push_events': int(bool(tag_push)),\n        }\n\n        request = requests.post(\n            '{0}/{1}/hooks'.format(self.projects_url, project_id),\n            headers=self.headers, data=data, verify=self.verify_ssl, auth=self.auth, timeout=self.timeout)\n\n        if request.status_code == 201:\n            return request.json()\n        else:\n            return False"}
{"prompt": "def clipTo(self, bsp):\n        \"\"\" \n        Remove all polygons in this BSP tree that are inside the other BSP tree\n        `bsp`.\n        \"\"\"", "output": "\n        if self.isClosed():\n            raise ValueError(\"Cannot close a closed BSP\")\n        self.removeAll()\n\n    def removeAll(self):\n        \"\"\" \n        Remove all nodes and subpolygons from this BSP.\n        \"\"\"\n        self.nodes = []\n        self.subpolygons = []\n\n    def getNode(self, id):\n        \"\"\" \n        Return a node from this BSP by ID.\n        \"\"\"\n        return self.nodes[id]\n\n    def getSubpolygons(self, id):\n        \"\"\" \n        Return a subpolygon from this BSP.\n        \"\"\"\n        return self.subpolygons[id", "solution": "\n        self.polygons = bsp.clipPolygons(self.polygons)\n        if self.front: \n            self.front.clipTo(bsp)\n        if self.back: \n            self.back.clipTo(bsp)"}
{"prompt": "def set_time_items(glob):\n    \"\"\"\n    This function prepares the benchmark items for inclusion in main script's\n     global scope.\n    \n    :param glob: main script's global scope dictionary reference\n    \"\"\"", "output": "\n    global time_items\n    time_items = {}\n    for key in glob.keys():\n        if key.startswith(\"time_\"):\n            time_items[key[7:]] = glob[key]\n    return time_items\n\ndef get_time_items(glob):\n    \"\"\"\n    This function prepares the benchmark items for inclusion in main script's\n     global scope.\n    \n    :param glob: main script's global scope dictionary reference\n    \"\"\"\n    global time_items\n    if glob.has_key(\"time_\"):\n        return glob[\"time_\"]\n    else:\n        return time_items\n\ndef get_", "solution": "\n    a = glob['args']\n    l = glob['logger']\n\n    class __TimeManager(object):\n        def __init__(self):\n            c = a._collisions\n            self._stats = getattr(a, c.get(\"stats\") or \"stats\", False)\n            self._timings = getattr(a, c.get(\"timings\") or \"timings\", False)\n            self.enabled = self._stats or self._timings\n            self.last = self.start = time.time()\n            self.times = []\n        \n        def stats(self):\n            end = time.time()\n            b = \"\"\n            for d, s, e in self.times:\n                b += \"\\n{}\\n> {} seconds\".format(d, e - s)\n            l.time(\"Total time: {} seconds{}\".format(end - self.start, b))\n    \n    glob['time_manager'] = manager = __TimeManager()\n\n    def _take_time(start=None, descr=None):\n        t = manager.last = time.time()\n        if start is not None and descr is not None:\n            manager.times.append((descr, float(start), float(t)))\n        return t - (start or 0)\n\n    class Timer(object):\n        class TimeoutError(Exception):\n            pass  # TimeoutError is not handled in Python 2\n    \n        def __init__(self, description=None, message=TO_MSG, timeout=None,\n                     fail_on_timeout=False):\n            self.fail = fail_on_timeout\n            self.id = len(manager.times)\n            self.descr = \"#\" + str(self.id) + \\\n                         (\": \" + (description or \"\")).rstrip(\": \")\n            self.message = message\n            self.start = _take_time()\n            self.timeout = timeout\n\n        def __enter__(self):\n            if manager.enabled:\n                if self.timeout is not None:\n                    signal.signal(signal.SIGALRM, self._handler)\n                    signal.alarm(self.timeout)\n                if manager._timings and self.descr:\n                    l.time(self.descr)\n                return self\n        \n        def __exit__(self, exc_type, exc_value, exc_traceback):\n            if manager.enabled:\n                d = _take_time(self.start, self.descr)\n                if manager._timings:\n                    l.time(\"> Time elapsed: {} seconds\".format(d))\n                if self.timeout is not None:\n                    if self.fail and exc_type is Timer.TimeoutError:\n                        return True\n\n        def _handler(self, signum, frame):\n            raise Timer.TimeoutError(self.message)\n    \n    glob['Timer'] = Timer\n    \n    def get_time(message=None, start=manager.start):\n        if manager._timings:\n            l.time(\"> {}: {} seconds\".format(message or \"Time elapsed since \"\n                                          \"execution start\", _take_time(start)))\n    \n    glob['get_time'] = get_time\n    \n    def get_time_since_last(message=None):\n        get_time(message or \"Time elapsed since last measure\", manager.last)\n    \n    glob['get_time_since_last'] = get_time_since_last"}
{"prompt": "def setup_environment():\n    \"\"\"Ensures that the environmental variable RAFCON_LIB_PATH is existent\n    \"\"\"", "output": "\n    if not os.path.exists(RAFCON_LIB_PATH):\n        os.mkdir(RAFCON_LIB_PATH)\n    if not os.path.exists(RAFCON_LIB_PATH + \"/lib\"):\n        os.mkdir(RAFCON_LIB_PATH + \"/lib\")\n    if not os.path.exists(RAFCON_LIB_PATH + \"/lib/libboost_program_options.jar\"):\n        print(\"ERROR: Could not find libboost_program_options.jar\")\n        sys.exit(1)\n    if not os.path.exists(", "solution": "\n    try:\n        from gi.repository import GLib\n        user_data_folder = GLib.get_user_data_dir()\n    except ImportError:\n        user_data_folder = join(os.path.expanduser(\"~\"), \".local\", \"share\")\n    rafcon_root_path = dirname(realpath(rafcon.__file__))\n    user_library_folder = join(user_data_folder, \"rafcon\", \"libraries\")\n\n    # The RAFCON_LIB_PATH points to a path with common RAFCON libraries\n    # If the env variable is not set, we have to determine it. In the future, this should always be\n    # ~/.local/share/rafcon/libraries, but for backward compatibility, also a relative RAFCON path is supported\n    if not os.environ.get('RAFCON_LIB_PATH', None):\n        if exists(user_library_folder):\n            os.environ['RAFCON_LIB_PATH'] = user_library_folder\n        else:\n            os.environ['RAFCON_LIB_PATH'] = join(dirname(dirname(rafcon_root_path)), 'share', 'libraries')\n\n    # Install dummy _ builtin function in case i18.setup_l10n() is not called\n    if sys.version_info >= (3,):\n        import builtins as builtins23\n    else:\n        import __builtin__ as builtins23\n    if \"_\" not in builtins23.__dict__:\n        builtins23.__dict__[\"_\"] = lambda s: s"}
{"prompt": "def sharp(self) -> 'Channel':\n        r\"\"\"Return the 'sharp' transpose of the superoperator.\n\n        The transpose :math:`S^\\#` switches the two covariant (bra)\n        indices of the superoperator. (Which in our representation\n        are the 2nd and 3rd super-indices)\n\n        If :math:`S^\\#` is Hermitian, then :math:`S` is a Hermitian-map\n        (i.e. transforms Hermitian operators to hJrmitian operators)\n\n        Flattening the :math:`S^\\#` superoperator to a matrix gives\n        the Choi matrix representation. (See channel.choi())\n        \"\"\"", "output": " :math:`S^\\#` transpose of the matrix.\n\n        Parameters\n        ----------\n        x : `Tensor`\n            The superoperator to be sharpened.\n\n        Returns\n        -------\n        x_sharpened : `Tensor`\n            The'sharpened' transpose of the superoperator.\n\n        Examples\n        --------\n        >>> x = tf.constant([[1, 2, 3], [4, 5, 6]])\n        >>> sharp = tf.sharp()\n        >>> sharp.eval(x)\n        [1, 4, 5, 6]\n\n        \"\"\"\n        return self._sharp_transpose(self", "solution": "\n\n        N = self.qubit_nb\n\n        tensor = self.tensor\n        tensor = bk.reshape(tensor, [2**N] * 4)\n        tensor = bk.transpose(tensor, (0, 2, 1, 3))\n        tensor = bk.reshape(tensor, [2] * 4 * N)\n        return Channel(tensor, self.qubits)"}
{"prompt": "def find_widening_points(function_addr, function_endpoints, graph):  # pylint: disable=unused-argument\n        \"\"\"\n        Given a local transition graph of a function, find all widening points inside.\n\n        Correctly choosing widening points is very important in order to not lose too much information during static\n        analysis. We mainly consider merge points that has at least one loop back edges coming in as widening points.\n\n        :param int function_addr: Address of the function.\n        :param list function_endpoints: Endpoints of the function, typically coming from Function.endpoints.\n        :param networkx.DiGraph graph: A local transition graph of a function, normally Function.graph.\n        :return: A list of addresses of widening points.\n        :rtype: list\n        \"\"\"", "output": "\n        :param int graph: The transition graph.\n        :return: A list of the number of widening points inside the transition graph.\n        :rtype: int\n        \"\"\"\n        return [graph.get_edge_data(function_addr, i) for i in range(len(function_endpoints))]\n\n    @staticmethod\n    def find_loops(graph, addr):  # pylint: disable=unused-argument\n        \"\"\"\n        Given a transition graph of a function, find all loops in the transition graph.\n\n        :param int graph: The transition graph.\n        :param str addr: Address of the function.\n        :", "solution": "\n\n        sccs = networkx.strongly_connected_components(graph)\n\n        widening_addrs = set()\n\n        for scc in sccs:\n            if len(scc) == 1:\n                node = next(iter(scc))\n                if graph.has_edge(node, node):\n                    # self loop\n                    widening_addrs.add(node.addr)\n            else:\n                for n in scc:\n                    predecessors = graph.predecessors(n)\n                    if any([ p not in scc for p in predecessors]):\n                        widening_addrs.add(n.addr)\n                        break\n\n        return list(widening_addrs)"}
{"prompt": "def build(\n    c,\n    clean=False,\n    browse=False,\n    nitpick=False,\n    opts=None,\n    source=None,\n    target=None,\n):\n    \"\"\"\n    Build the project's Sphinx docs.\n    \"\"\"", "output": "\n    if opts is None:\n        opts = {}\n    if source is None:\n        source = []\n    if target is None:\n        target = []\n    if not opts.get(\"no_toc\", False):\n        if not nitpick:\n            c.generate_toc(opts=opts)\n        else:\n            c.generate_toctree(opts=opts)\n    if not opts.get(\"no_toc_depth\", False):\n        if not browse:\n            c.generate_toctree(opts=opts)\n        else:\n            c.generate_toc(opts=opts, depth=1)\n    if", "solution": "\n    if clean:\n        _clean(c)\n    if opts is None:\n        opts = \"\"\n    if nitpick:\n        opts += \" -n -W -T\"\n    cmd = \"sphinx-build{0} {1} {2}\".format(\n        (\" \" + opts) if opts else \"\",\n        source or c.sphinx.source,\n        target or c.sphinx.target,\n    )\n    c.run(cmd, pty=True)\n    if browse:\n        _browse(c)"}
{"prompt": "def note_update(self, note_id, coor_x=None, coor_y=None, width=None,\n                    height=None, body=None):\n        \"\"\"Function to update a note (Requires login) (UNTESTED).\n\n        Parameters:\n            note_id (int): Where note_id is the note id.\n            coor_x (int): The x coordinates of the note in pixels,\n                          with respect to the top-left corner of the image.\n            coor_y (int): The y coordinates of the note in pixels,\n                          with respect to the top-left corner of the image.\n            width (int): The width of the note in pixels.\n            height (int): The height of the note in pixels.\n            body (str): The body of the note.\n        \"\"\"", "output": " corner of the image.\n            width (int): The width of the image.\n            height (int): The height of the image.\n            body (str): The body of the note.\n        \"\"\"\n        self.note_id = note_id\n        self.width = width\n        self.height = height\n        self.coor_x = coor_x\n        self.coor_y = coor_y\n        self.body = body\n        self.coor_x = coor_x\n        self.coor_y = coor_y\n\n    def get_note_data(self, coor", "solution": "\n        params = {\n            'note[x]': coor_x,\n            'note[y]': coor_y,\n            'note[width]': width,\n            'note[height]': height,\n            'note[body]': body\n            }\n        return self._get('notes/{0}.jso'.format(note_id), params, method='PUT',\n                         auth=True)"}
{"prompt": "def translate_detector(self, vector):\n        \"\"\"Translate the detector by a given vector\"\"\"", "output": "\n        return self.detector.translate(vector)\n\n    def get_detector_name(self, detector):\n        \"\"\"Get the name of the detector\"\"\"\n        return self.detector.get_name(detector)\n\n    def get_detector_type(self, detector):\n        \"\"\"Get the type of the detector\"\"\"\n        return self.detector.get_type(detector)\n\n    def get_detector_parameters(self, detector):\n        \"\"\"Get the parameters of the detector\"\"\"\n        return self.detector.get_parameters(detector)\n\n    def get_detector_data(self, detector):\n        \"\"\"Get the data of the", "solution": "\n        vector = np.array(vector, dtype=float)\n        self.pmts.pos_x += vector[0]\n        self.pmts.pos_y += vector[1]\n        self.pmts.pos_z += vector[2]\n        self.reset_caches()"}
{"prompt": "def get(self, uri):\n        \"\"\"Get request\n        \n        Args:\n            uri (str): URI\n            \n        Returns:\n            Json: API response\n            \n        Raises:\n            Exception: Network issue\n        \"\"\"", "output": "\n        return self._request(uri, method='GET')\n\n    def post(self, uri, data):\n        \"\"\"Post request\n        \n        Args:\n            uri (str): URI\n            \n        Returns:\n            Json: API response\n            \n        Raises:\n            Exception: Network issue\n        \"\"\"\n        return self._request(uri, method='POST', data=data)\n\n    def put(self, uri, data):\n        \"\"\"Put request\n        \n        Args:\n            uri (str): URI\n            \n        Returns:\n            Json: API response\n            \n        Raises:\n            Exception: Network issue\n        \"\"\"\n        return self._request(uri, method='PUT', data=data)\n\n    def", "solution": "\n        r = None\n        \n        try:\n            r = requests.get(uri,\n                             allow_redirects=True,\n                             timeout=Settings.requests_timeout,\n                             headers={},\n                             auth=HTTPDigestAuth(self.user, self.password))\n            return self.answer(r.status_code, r.json())\n        except:\n            raise\n        finally:\n            if r:\n                r.connection.close()"}
{"prompt": "def AVP(avpId, **fields):\n    \"\"\" Craft an AVP based on its id and optional parameter fields\"\"\"", "output": "\n    return _AVP(avpId, 'a', **fields)\n\ndef AVP_uint32(avpId, **fields):\n    \"\"\" Craft an AVP based on its id and optional parameter fields\"\"\"\n    return _AVP(avpId, 'a', **fields)\n\ndef AVP_uint32_array(avpId, **fields):\n    \"\"\" Craft an AVP based on its id and optional parameter fields\"\"\"\n    return _AVP(avpId, 'a', **fields)\n\ndef _AVP(avpId, type, **", "solution": "\n    val = None\n    classType = AVP_Unknown\n    if isinstance(avpId, str):\n        try:\n            for vnd in AvpDefDict:\n                for code in AvpDefDict[vnd]:\n                    val = AvpDefDict[vnd][code]\n                    if val[0][:len(\n                            avpId)] == avpId:  # A prefix of the full name is considered valid  # noqa: E501\n                        raise\n            found = False\n        except BaseException:\n            found = True\n    else:\n        if isinstance(avpId, list):\n            code = avpId[0]\n            vnd = avpId[1]\n        else:  # Assume this is an int\n            code = avpId\n            vnd = 0\n        try:\n            val = AvpDefDict[vnd][code]\n            found = True\n        except BaseException:\n            found = False\n    if not found:\n        warning('The AVP identifier %s has not been found.' % str(avpId))\n        if isinstance(avpId, str):  # The string input is not valid\n            return None\n    # At this point code, vnd are provisionned val may be set (if found is True)  # noqa: E501\n    # Set/override AVP code\n    fields['avpCode'] = code\n    # Set vendor if not already defined and relevant\n    if 'avpVnd' not in fields and vnd:\n        fields['avpVnd'] = vnd\n    # Set flags if not already defined and possible ...\n    if 'avpFlags' not in fields:\n        if val:\n            fields['avpFlags'] = val[2]\n        else:\n            fields['avpFlags'] = vnd and 128 or 0\n    # Finally, set the name and class if possible\n    if val:\n        classType = val[1]\n    _ret = classType(**fields)\n    if val:\n        _ret.name = 'AVP ' + val[0]\n    return _ret"}
{"prompt": "def find_best_candidate(self, elev_source_files=None):\n        \"\"\"\n        Heuristically determines which tile should be recalculated based on\n        updated edge information. Presently does not check if that tile is\n        locked, which could lead to a parallel thread closing while one thread\n        continues to process tiles.\n        \"\"\"", "output": "\n        if elev_source_files is None:\n            elev_source_files = self.get_source_files()\n        if len(elev_source_files) == 0:\n            return None\n        if elev_source_files[0] in elev_source_files:\n            return 'locked'\n        if elev_source_files[1] in elev_source_files:\n            return'recalculated'\n        if elev_source_files[0] in self.edge_list:\n            return'recalculated'\n        return None\n\n    def get_source_files", "solution": "\n        self.fill_percent_done()\n        i_b = np.argmax(self.percent_done.values())\n        if self.percent_done.values()[i_b] <= 0:\n            return None\n\n        # check for ties\n        I = np.array(self.percent_done.values()) == \\\n            self.percent_done.values()[i_b]\n        if I.sum() == 1:\n            pass  # no ties\n        else:\n            I2 = np.argmax(np.array(self.max_elev.values())[I])\n            i_b = I.nonzero()[0][I2]\n\n            # Make sure the apples are still apples\n            assert(np.array(self.max_elev.keys())[I][I2]\n                   == np.array(self.percent_done.keys())[I][I2])\n\n        if elev_source_files is not None:\n            fn = self.percent_done.keys()[i_b]\n            lckfn = _get_lockfile_name(fn)\n            if os.path.exists(lckfn):  # another process is working on it\n                # Find a different Candidate\n                i_alt = np.argsort(self.percent_done.values())[::-1]\n                for i in i_alt:\n                    fn = self.percent_done.keys()[i]\n                    lckfn = _get_lockfile_name(fn)\n                    if not os.path.exists(lckfn):\n                        break\n            # Get and return the index\n            i_b = elev_source_files.index(fn)\n\n        return i_b"}
{"prompt": "def RgbToGreyscale(r, g, b):\n    \"\"\"Convert the color from RGB to its greyscale equivalent\n\n    Parameters:\n      :r:\n        The Red component value [0...1]\n      :g:\n        The Green component value [0...1]\n      :b:\n        The Blue component value [0...1]\n\n    Returns:\n      The color as an (r, g, b) tuple in the range:\n      the range:\n      r[0...1],\n      g[0...1],\n      b[0...1]\n\n    >>> '(%g, %g, %g)' % Color.RgbToGreyscale(1, 0.8, 0)\n    '(0.6, 0.6, 0.6)'\n\n    \"\"\"", "output": " RgbToGreyscale((0,0,0), (0,0,255,0))\n    (255, 0, 0, 255)\n\n    >>> '(%g, %g, %g)' % RgbToGreyscale((0,0,0), (0,255,0,0))\n    (0, 0, 0, 0)\n\n    >>> '(%g, %g, %g)' % RgbToGreyscale((0,0,0), (0,0,0,255))\n    (0, 0, 0, 0)\n    \"\"\"\n    return (0, 0", "solution": "\n    v = (r + g + b) / 3.0\n    return (v, v, v)"}
{"prompt": "def install_dap(name, version='', update=False, update_allpaths=False, first=True,\n                force=False, nodeps=False, reinstall=False, __ui__=''):\n    \"\"\"Install a dap from dapi\n    If update is True, it will remove previously installed daps of the same name\"\"\"", "output": "\n    if not nodeps:\n        dap_cmd = 'dap -n -i'\n    else:\n        dap_cmd = 'dap -n -r'\n    if not reinstall:\n        dap_cmd +='-y'\n    cmd = dap_cmd +'install'\n    if version:\n        cmd +='-v %s' % version\n    if first:\n        cmd +='-a'\n    if force:\n        cmd +='-f'\n    else:\n        cmd +='-y'\n    if update_allpaths:\n        cmd +='-a'\n    if __", "solution": "\n    m, d = _get_metadap_dap(name, version)\n    if update:\n        available = d['version']\n        current = get_installed_version_of(name)\n        if not current:\n            raise DapiLocalError('Cannot update not yet installed DAP.')\n        if dapver.compare(available, current) <= 0:\n            return []\n    path, remove_dir = download_dap(name, d=d)\n\n    ret = install_dap_from_path(path, update=update, update_allpaths=update_allpaths, first=first,\n                                force=force, nodeps=nodeps, reinstall=reinstall, __ui__=__ui__)\n\n    try:\n        if remove_dir:\n            shutil.rmtree(os.dirname(path))\n        else:\n            os.remove(path)\n    except:\n        pass\n\n    return ret"}
{"prompt": "def rk5_stepper(x, h, y , deriv, errors=False):\n    \"\"\" Compute y(x+h) from y and dy/dx=deriv(x,y).\n\n    Uses a one-step 5th-order Runge-Kutta algorithm.\n\n    Returns x+h, y(x+h) if errors is False; otherwise\n    returns x+h, y(x+h), yerr where yerr is an error\n    estimate.\n\n    Adapted from Numerical Recipes.\n    \"\"\"", "output": "\n    if errors:\n        return (x+h, y, yerr)\n    else:\n        return (x+h, y+h)\n\ndef rk5_stepper_scalar(x, h, y, deriv, errors=False):\n    \"\"\" Compute y(x+h) from y and dy/dx=deriv(x,y,x).\n\n    Uses a scalar algorithm.\n\n    Returns x+h, y(x+h) if errors is False; otherwise\n    returns x+h, y(x+h), yerr where yerr is an error\n    estimate.\n\n    Adapted", "solution": "\n    k1 = h * deriv(x,y)\n    k2 = h * deriv(x+0.2*h, y+0.2*k1)\n    k3 = h * deriv(x+0.3*h, y+0.075*k1+0.225*k2)\n    k4 = h * deriv(x+0.6*h, y+0.3*k1-0.9*k2+1.2*k3)\n    k5 = h * deriv(x+h, y-.2037037037037037037037037*k1\n             +2.5*k2-2.592592592592592592592593*k3\n             +1.296296296296296296296296*k4)\n    k6 = h * deriv(x+0.875*h, y+.2949580439814814814814815e-1*k1\n             +.341796875*k2+.4159432870370370370370370e-1*k3\n             +.4003454137731481481481481*k4+.61767578125e-1*k5)\n    yn = y + (.9788359788359788359788361e-1*k1\n             +.4025764895330112721417070*k3\n             +.2104377104377104377104378*k4\n             +.2891022021456804065499718*k6)\n    xn = x+h\n    if errors:\n        yerr = (-.429377480158730158730159e-2*k1\n                +.186685860938578329882678e-1*k3\n                -.341550268308080808080807e-1*k4\n                -.1932198660714285714285714e-1*k5\n                +.391022021456804065499718e-1*k6)\n        return xn,yn,yerr\n    else:\n        return xn,yn"}
{"prompt": "def write_pkg_file(self, file):\n    \"\"\"Write the PKG-INFO format data to a file object.\n    \"\"\"", "output": "\n    if not file:\n      return\n    if self.version:\n      file.write(\"%s\\n\" % self.version)\n    if self.release:\n      file.write(\"%s\\n\" % self.release)\n    if self.epoch:\n      file.write(\"%s\\n\" % self.epoch)\n    if self.release_epoch:\n      file.write(\"%s\\n\" % self.release_epoch)\n    if self.release_version:\n      file.write(\"%s\\n\" % self.release_version)\n    if self.build_time:\n      file.write(\"%", "solution": "\n    version = self.get_metadata_version()\n\n    if six.PY2:\n        def write_field(key, value):\n            file.write(\"%s: %s\\n\" % (key, self._encode_field(value)))\n    else:\n        def write_field(key, value):\n            file.write(\"%s: %s\\n\" % (key, value))\n\n    write_field('Metadata-Version', str(version))\n    write_field('Name', self.get_name())\n    write_field('Version', self.get_version())\n    write_field('Summary', self.get_description())\n    write_field('Home-page', self.get_url())\n\n    if version < StrictVersion('1.2'):\n        write_field('Author', self.get_contact())\n        write_field('Author-email', self.get_contact_email())\n    else:\n        optional_fields = (\n            ('Author', 'author'),\n            ('Author-email', 'author_email'),\n            ('Maintainer', 'maintainer'),\n            ('Maintainer-email', 'maintainer_email'),\n        )\n\n        for field, attr in optional_fields:\n            attr_val = getattr(self, attr)\n\n            if attr_val is not None:\n                write_field(field, attr_val)\n\n    write_field('License', self.get_license())\n    if self.download_url:\n        write_field('Download-URL', self.download_url)\n    for project_url in self.project_urls.items():\n        write_field('Project-URL',  '%s, %s' % project_url)\n\n    long_desc = rfc822_escape(self.get_long_description())\n    write_field('Description', long_desc)\n\n    keywords = ','.join(self.get_keywords())\n    if keywords:\n        write_field('Keywords', keywords)\n\n    if version >= StrictVersion('1.2'):\n        for platform in self.get_platforms():\n            write_field('Platform', platform)\n    else:\n        self._write_list(file, 'Platform', self.get_platforms())\n\n    self._write_list(file, 'Classifier', self.get_classifiers())\n\n    # PEP 314\n    self._write_list(file, 'Requires', self.get_requires())\n    self._write_list(file, 'Provides', self.get_provides())\n    self._write_list(file, 'Obsoletes', self.get_obsoletes())\n\n    # Setuptools specific for PEP 345\n    if hasattr(self, 'python_requires'):\n        write_field('Requires-Python', self.python_requires)\n\n    # PEP 566\n    if self.long_description_content_type:\n        write_field(\n            'Description-Content-Type',\n            self.long_description_content_type\n        )\n    if self.provides_extras:\n        for extra in self.provides_extras:\n            write_field('Provides-Extra', extra)"}
{"prompt": "def get_uservars(self, user=None):\n        \"\"\"Get all variables about a user (or all users).\n\n        :param optional str user: The user ID to retrieve all variables for.\n            If not passed, this function will return all data for all users.\n\n        :return dict: All the user variables.\n\n            * If a ``user`` was passed, this is a ``dict`` of key/value pairs\n              of that user's variables. If the user doesn't exist in memory,\n              this returns ``None``.\n            * Otherwise, this returns a ``dict`` of key/value pairs that map\n              user IDs to their variables (a ``dict`` of ``dict``).\n        \"\"\"", "output": " variables to their values.\n        \"\"\"\n        if user is None:\n            return None\n        return self._get_uservars(user)\n\n    def _get_uservars(self, user=None):\n        \"\"\"Get all variables about a user (or all users).\n\n        :param optional str user: The user ID to retrieve all variables for.\n            If not passed, this function will return all data for all users.\n\n        :return dict: All the user variables.\n\n            * If a ``user`` was passed, this is a ``dict`` of key/value pairs\n              of that user's variables. If the user doesn't", "solution": "\n\n        if user is None:\n            # All the users!\n            return self._session.get_all()\n        else:\n            # Just this one!\n            return self._session.get_any(user)"}
{"prompt": "def scan(self, seqs, nreport=100, scan_rc=True, normalize=False):\n        \"\"\"\n        scan a set of regions / sequences\n        \"\"\"", "output": "\n        for seq in seqs:\n            self.add_region(seq)\n        if scan_rc:\n            self.scan_rc()\n        if normalize:\n            self.normalize()\n\n    def add_region(self, seq):\n        \"\"\"\n        add a region\n        \"\"\"\n        self.regions.append(seq)\n\n    def scan_rc(self):\n        \"\"\"\n        scan the region\n        \"\"\"\n        for i in range(len(self.regions)):\n            if self.regions[i] is not None:\n                if self.report_rc:\n                        print(self.regions[i], self.report_rc)", "solution": "\n\n        if not self.threshold:\n            sys.stderr.write(\n                \"Using default threshold of 0.95. \"\n                \"This is likely not optimal!\\n\"\n                )\n            self.set_threshold(threshold=0.95)\n\n        seqs = as_fasta(seqs, genome=self.genome)\n           \n        it = self._scan_sequences(seqs.seqs, \n                    nreport, scan_rc)\n       \n        if normalize:\n            if len(self.meanstd) == 0:\n                self.set_meanstd()\n            mean_std = [self.meanstd.get(m_id) for m_id in self.motif_ids]\n            means = [x[0] for x in  mean_std]\n            stds = [x[1] for x in  mean_std]\n\n\n        for result in it:\n            if normalize:\n                zresult = [] \n                for i,mrow in enumerate(result):\n                    mrow = [((x[0] - means[i]) / stds[i], x[1], x[2]) for x in mrow]\n                    zresult.append(mrow)\n                yield zresult\n            else:\n                yield result"}
{"prompt": "def get_invalidation_bits(self, group_index, channel, fragment):\n        \"\"\" get invalidation indexes for the channel\n\n        Parameters\n        ----------\n        group_index : int\n            group index\n        channel : Channel\n            channel object\n        fragment : (bytes, int)\n            (fragment bytes, fragment offset)\n\n        Returns\n        -------\n        invalidation_bits : iterable\n            iterable of valid channel indexes; if all are valid `None` is\n            returned\n\n        \"\"\"", "output": "\n        if not self.valid_group_index(group_index):\n            return None\n\n        invalidation_bits = []\n        for i in range(group_index.num_channels):\n            if not self.channel_valid_index(channel, i):\n                invalidation_bits.append(None)\n            else:\n                break\n        else:\n            invalidation_bits = tuple(invalidation_bits)\n\n        return invalidation_bits\n\n    def channel_valid_index(self, channel, i):\n        \"\"\" Check if the channel index `i` is valid\n\n        Parameters\n        ----------\n        channel : Channel\n            channel object", "solution": "\n        group = self.groups[group_index]\n        dtypes = group.types\n\n        data_bytes, offset, _count = fragment\n        try:\n            invalidation = self._invalidation_cache[(group_index, offset, _count)]\n        except KeyError:\n            record = group.record\n            if record is None:\n                dtypes = group.types\n                if dtypes.itemsize:\n                    record = fromstring(data_bytes, dtype=dtypes)\n                else:\n                    record = None\n\n            invalidation = record[\"invalidation_bytes\"].copy()\n            self._invalidation_cache[(group_index, offset, _count)] = invalidation\n\n        ch_invalidation_pos = channel.pos_invalidation_bit\n        pos_byte, pos_offset = divmod(ch_invalidation_pos, 8)\n\n        mask = 1 << pos_offset\n\n        invalidation_bits = invalidation[:, pos_byte] & mask\n        invalidation_bits = invalidation_bits.astype(bool)\n\n        return invalidation_bits"}
{"prompt": "def GenerateCalendarDatesFieldValuesTuples(self):\n    \"\"\"Generates tuples of calendar_dates.txt values. Yield zero tuples if\n    this ServicePeriod should not be in calendar_dates.txt .\"\"\"", "output": "\n    if not self.IsInfinite():\n      return\n\n    for tuple in self.GetTuples():\n      yield tuple[0], tuple[1], tuple[2], tuple[3], tuple[4], tuple[5], tuple[6], tuple[7], tuple[8], tuple[9], tuple[10], tuple[11]\n\n  def GetTuples(self):\n    \"\"\"Returns a list of calendar_tuples.txt values.\n\n    The list is sorted by the start date.  Each tuple is a tuple of the\n    start date and the number of days since the start date.  The tuple is\n   ", "solution": "\n    for date, (exception_type, _) in self.date_exceptions.items():\n      yield (self.service_id, date, unicode(exception_type))"}
{"prompt": "def remove_datasets_from_ckan(portal_url, apikey, filter_in=None,\n                              filter_out=None, only_time_series=False,\n                              organization=None):\n    \"\"\"Borra un dataset en el portal pasado por par\u00e1metro.\n\n            Args:\n                portal_url (str): La URL del portal CKAN de destino.\n                apikey (str): La apikey de un usuario con los permisos que le\n                    permitan borrar el dataset.\n                filter_in(dict): Diccionario de filtrado positivo, similar al\n                    de search.get_datasets.\n                filter_out(dict): Diccionario de filtrado negativo, similar al\n                    de search.get_datasets.\n                only_time_series(bool): Filtrar solo los datasets que tengan\n                    recursos con series de tiempo.\n                organization(str): Filtrar solo los datasets que pertenezcan a\n                    cierta organizacion.\n            Returns:\n                None\n    \"\"\"", "output": "sibles.\n                filter_out(dict): Filtrado posibles que no se encuentra el dataset.\n                only_time_series(bool): Si se encuentra el dataset, no se\n                        encuentra el permiso.\n                organization (str): La par\u00e1metro de la organizaci\u00f3n.\n    \"\"\"\n    # Borrar un usuario\n    if not apikey:\n        return portal_url\n\n    # Cargar datos\n    if filter_in:\n        for key, value in filter_in.items():\n            if value:\n                apikey", "solution": "\n    ckan_portal = RemoteCKAN(portal_url, apikey=apikey)\n    identifiers = []\n    datajson_filters = filter_in or filter_out or only_time_series\n    if datajson_filters:\n        identifiers += get_datasets(\n            portal_url + '/data.json',\n            filter_in=filter_in, filter_out=filter_out,\n            only_time_series=only_time_series, meta_field='identifier'\n        )\n    if organization:\n        query = 'organization:\"' + organization + '\"'\n        search_result = ckan_portal.call_action('package_search', data_dict={\n            'q': query, 'rows': 500, 'start': 0})\n        org_identifiers = [dataset['id']\n                           for dataset in search_result['results']]\n        start = 500\n        while search_result['count'] > start:\n            search_result = ckan_portal.call_action(\n                'package_search', data_dict={\n                    'q': query, 'rows': 500, 'start': start})\n            org_identifiers += [dataset['id']\n                                for dataset in search_result['results']]\n            start += 500\n\n        if datajson_filters:\n            identifiers = set(identifiers).intersection(set(org_identifiers))\n        else:\n            identifiers = org_identifiers\n\n    for identifier in identifiers:\n        ckan_portal.call_action('dataset_purge', data_dict={'id': identifier})"}
{"prompt": "def set(self, value):\n        \"\"\"This parameter method attempts to set a specific value for this parameter. The value will be validated first, and if it can not be set. An error message will be set in the error property of this parameter\"\"\"", "output": "\n        if value is None:\n            return\n        if value is None:\n            self.error = None\n        else:\n            self.error = None\n        self.value = value\n\n    def get(self):\n        \"\"\"This parameter method returns the value of this parameter as a string\"\"\"\n        return self.value\n\n    def set_error(self, error):\n        \"\"\"This method attempts to set a specific value for this parameter. The error will be validated first, and if it can not be set. An error message will be set in the error property of this parameter\"\"\"\n        self.value = None\n        self.error = error\n\n    def", "solution": "\n        if self.validate(value):\n            #print \"Parameter \" + self.id + \" successfully set to \" + repr(value)\n            self.hasvalue = True\n            if isinstance(value, float):\n                self.value = round(value)\n            else:\n                self.value = int(value)\n            return True\n        else:\n            #print \"Parameter \" + self.id + \" COULD NOT BE set to \" + repr(value)\n            return False"}
{"prompt": "def send_zipfile(request, fileList):\n    \"\"\"                                                                         \n    Create a ZIP file on disk and transmit it in chunks of 8KB,                 \n    without loading the whole file into memory. A similar approach can          \n    be used for large dynamic PDF files.                                        \n    \"\"\"", "output": "\n    # Create a new temporary directory\n    tempdir = tempfile.mkdtemp()\n\n    # Create a temporary file\n    f = tempfile.NamedTemporaryFile(dir=tempdir)\n\n    # Write the files\n    for file in fileList:\n        f.write(file + '\\n')\n    f.flush()\n\n    # Load the file\n    try:\n        # Load the file\n        f.write(f.name)\n        f.close()\n    except:\n        raise Http404\n\n    # Generate the PDF\n    try:\n        # Create the PDF\n        generate_pdf(f, fileList, file)\n    except:\n        raise Http404\n\n    # Load the", "solution": "\n    temp = tempfile.TemporaryFile()\n    archive = zipfile.ZipFile(temp, 'w', zipfile.ZIP_DEFLATED)\n    for artist,files in fileList.iteritems():\n        for f in files:\n            archive.write(f[0], '%s/%s' % (artist, f[1]))\n    archive.close()\n    wrapper = FixedFileWrapper(temp)\n    response = HttpResponse(wrapper, content_type='application/zip')\n    response['Content-Disposition'] = 'attachment; filename=FrogSources.zip'\n    response['Content-Length'] = temp.tell()\n    temp.seek(0)\n    return response"}
{"prompt": "def create_emergency_codes(self, code=None):\n        \"\"\"Generate emergency codes\n\n        :param code: SMS code\n        :type code: str\n        :raises: :class:`SteamAuthenticatorError`\n        :return: list of codes\n        :rtype: list\n\n        .. note::\n            A confirmation code is required to generate emergency codes and this method needs\n            to be called twice as shown below.\n\n        .. code:: python\n\n            sa.create_emergency_codes()              # request a SMS code\n            sa.create_emergency_codes(code='12345')  # creates emergency codes\n        \"\"\"", "output": " request a SMS code\n            sa.create_emergency_codes(code='12345')  # request a SMS code\n            sa.create_emergency_codes(code='12345')  # request a SMS code\n            sa.create_emergency_codes(code='12345')  # request a SMS code\n            sa.create_emergency_codes(code='12345')  # request a SMS code\n            sa.create_emergency_codes(code='12345')  # request a SMS code\n            sa.create_emergency_codes(code='12345')  # request a", "solution": "\n        if code:\n            return self._send_request('createemergencycodes', {'code': code}).get('codes', [])\n        else:\n            self._send_request('createemergencycodes', {})\n            return None"}
{"prompt": "def run_as(self, identifiers):\n        \"\"\"\n        :type identifiers:  subject_abcs.IdentifierCollection\n        \"\"\"", "output": "\n        for identifier in identifiers:\n            if identifier.identifier_type =='subject':\n                self.subject_as_dict[identifier.subject_id] = subject_abcs.Subject(identifier)\n            elif identifier.identifier_type == 'identifier':\n                self.subject_as_dict[identifier.subject_id] = identifier\n            elif identifier.identifier_type =='subject_id':\n                self.subject_as_dict[identifier.subject_id] = identifier\n            elif identifier.identifier_type == 'identifier_type':\n                self.subject_as_dict[identifier.subject_id]", "solution": "\n        if (not self.has_identifiers):\n            msg = (\"This subject does not yet have an identity.  Assuming the \"\n                   \"identity of another Subject is only allowed for Subjects \"\n                   \"with an existing identity.  Try logging this subject in \"\n                   \"first, or using the DelegatingSubject.Builder \"\n                   \"to build ad hoc Subject instances with identities as \"\n                   \"necessary.\")\n            raise ValueError(msg)\n        self.push_identity(identifiers)"}
{"prompt": "def set(self, type, offset, value):\n        \"\"\"\n        Set the specified bit field and returns its old value.\n        \"\"\"", "output": "\n        self.write_byte(offset)\n        self.write_byte(value)\n\n    def set_from_string(self, string):\n        \"\"\"\n        Set the specified bit field from a string.\n        \"\"\"\n        self.write_byte(string.upper())\n        self.write_byte(string.lower())\n\n    def set_from_file(self, filename):\n        \"\"\"\n        Set the specified bit field from a file.\n        \"\"\"\n        self.write_byte(filename.upper())\n        self.write_byte(self.read_byte())\n        self.write_byte(self.read_byte", "solution": "\n        self._command_stack.extend(['SET', type, offset, value])\n        return self"}
{"prompt": "def get_item(track_url, client_id=CLIENT_ID):\n    \"\"\"\n    Fetches metadata for a track or playlist\n    \"\"\"", "output": "\n    if track_url.startswith('http://'):\n        return get_http_client(track_url)\n    else:\n        return get_json_response(client_id)\n\n\ndef get_http_client(url):\n    \"\"\"\n    Returns an HTTP client for the given URL\n    \"\"\"\n    return requests.get(url, auth=(url, ''), timeout=5)\n\n\ndef get_json_response(url):\n    \"\"\"\n    Returns a JSON response for the given URL\n    \"\"\"\n    return requests.get(url, auth=(url, ''), timeout=15)\n\n\ndef get_json_response", "solution": "\n    try:\n        item_url = url['resolve'].format(track_url)\n\n        r = requests.get(item_url, params={'client_id': client_id})\n        logger.debug(r.url)\n        if r.status_code == 403:\n            return get_item(track_url, ALT_CLIENT_ID)\n\n        item = r.json()\n        no_tracks = item['kind'] == 'playlist' and not item['tracks']\n        if no_tracks and client_id != ALT_CLIENT_ID:\n            return get_item(track_url, ALT_CLIENT_ID)\n    except Exception:\n        if client_id == ALT_CLIENT_ID:\n            logger.error('Failed to get item...')\n            return\n        logger.error('Error resolving url, retrying...')\n        time.sleep(5)\n        try:\n            return get_item(track_url, ALT_CLIENT_ID)\n        except Exception as e:\n            logger.error('Could not resolve url {0}'.format(track_url))\n            logger.exception(e)\n            sys.exit(0)\n    return item"}
{"prompt": "def computeUniquePointsSensed(nCols, nPoints, s):\n  \"\"\"\n  If a network with nCols columns senses an object s times, how many\n  unique points are actually sensed?  The number is generally <= nCols * s\n  because features may be repeated across sensations.\n  \"\"\"", "output": "\n  if nPoints == 0:\n    return 0\n  else:\n    return computeUniquePoints(nCols, nPoints, s)\n\ndef computeUniquePoints(nCols, nPoints, s):\n  \"\"\"\n  If a network with nCols columns are repeated, how many unique points\n  are actually sensed?  The number is generally <= nCols * s\n  because features may be repeated across sensations.\n  \"\"\"\n  if nPoints == 0:\n    return 0\n  else:\n    return computeUniquePoints(nCols, nPoints, s)\n\ndef computeUniquePointsFromSess(nCols,", "solution": "\n  if nCols == 1:\n    return min(s, nPoints)\n  elif nCols < nPoints:\n    q = float(nCols) / nPoints\n    unique = min(int(round(( 1.0 - math.pow(1.0 - q, s)) * nPoints)),\n                 nPoints)\n    return unique\n  else:\n    return nPoints"}
{"prompt": "def submit_jobs(root_dir, jobs, sgeargs=None):\n    \"\"\" Submit each of the passed jobs to the SGE server, using the passed\n    directory as root for SGE output.\n\n    - root_dir       Path to output directory\n    - jobs           List of Job objects\n    \"\"\"", "output": "\n    for job in jobs:\n        sge_server.submit_job(job)\n\n    # Wait for the jobs to complete\n    sge_server.wait_for_jobs(jobs)\n\n    # Check if the job has been submitted\n    submitted = False\n    for job in jobs:\n        if job.status =='submitted':\n            submitted = True\n            break\n\n    # Check if the job has been submitted\n    if not submitted:\n        sge_server.wait_for_jobs(jobs)\n\n    # Check if the job has been submitted\n    if not submitted:\n        sge_server.wait_for_jobs(", "solution": "\n    waiting = list(jobs)                 # List of jobs still to be done\n    # Loop over the list of pending jobs, while there still are any\n    while len(waiting) > 0:\n        # extract submittable jobs\n        submittable = extract_submittable_jobs(waiting)\n        # run those jobs\n        submit_safe_jobs(root_dir, submittable, sgeargs)\n        # remove those from the waiting list\n        for job in submittable:\n            waiting.remove(job)"}
{"prompt": "def set_config(self, config):\n        \"\"\"See :py:meth:`pyctools.core.config.ConfigMixin.set_config`.\"\"\"", "output": "\n        self.config = config\n        self.config.update(self.config_defaults)\n        self.config.update(config)\n        self.config.update(self.config_defaults)\n\n    def get_config(self):\n        \"\"\"See :py:meth:`pyctools.core.config.ConfigMixin.get_config`.\"\"\"\n        return self.config\n\n    def set_config_defaults(self, config_defaults):\n        \"\"\"See :py:meth:`pyctools.core.config.ConfigMixin.set_config`.\"\"\"\n        self.config_defaults = config_defaults\n        self.config", "solution": "\n        self.config.update(config)\n        if self.config_map:\n            for child_name in self.config_map:\n                child_config = self._compound_children[child_name].get_config()\n                for parent_item, child_item in self.config_map[child_name]:\n                    child_config[child_item] = self.config[parent_item]\n                self._compound_children[child_name].set_config(child_config)\n        else:\n            for name, child in self._compound_children.items():\n                child.set_config(self.config[name])"}
{"prompt": "def mass_loss_loon05(L,Teff):\n    \"\"\" \n    mass loss rate van Loon etal (2005).\n\n    Parameters\n    ----------\n    L : float\n        L in L_sun.\n    Teff : float\n        Teff in K.\n        \n    Returns\n    -------\n    Mdot\n        Mdot in Msun/yr\n    \n    Notes\n    -----\n    ref: van Loon etal 2005, A&A 438, 273\n    \n    \"\"\"", "output": "\n    # calculate the mass loss rate\n    T = Teff/L\n    # calculate the reference temperature\n    Tref = T/T\n    # calculate the reference pressure\n    P = pressure(Tref)\n    # calculate the reference pressure derivative\n    dT = (P-Tref)/Tref\n    # calculate the mass loss rate\n    Mdot = 0.5*(dT-T)\n    # calculate the mass loss rate derivative\n    dM = (P-T)/Tref\n    # calculate the mass loss rate derivative\n    dP = pressure(P)\n    # calculate the mass loss rate derivative\n    dP = pressure", "solution": "\n    \n    Mdot = -5.65 + np.log10(old_div(L,10.**4)) -6.3*np.log10(old_div(Teff,3500.))\n    return Mdot"}
{"prompt": "def disconnect_from(callback, signals):\n    \"\"\"\n    Removes a callback from specified signal registries and prevents it from responding\n    to any emitted signal.\n\n    :param callback: A callable registered with smokesignal\n    :param signals: A single signal or list/tuple of signals\n    \"\"\"", "output": "\n    for signal in signals:\n        if isinstance(signal, tuple):\n            for signal_tuple in signal:\n                if signal_tuple[0] == signal_tuple[1]:\n                    if callback in signal_tuple[2]:\n                        del signal_tuple[2][callback]\n                    if callback in signal_tuple[3]:\n                        del signal_tuple[3][callback]\n                    if callback in signal_tuple[4]:\n                        del signal_tuple[4][callback]\n                    if callback in signal_tuple[5]:\n                        del signal_tuple[5][callback]\n                    if callback in signal_tuple[6]:\n                       ", "solution": "\n    # Support for lists of signals\n    if not isinstance(signals, (list, tuple)):\n        signals = [signals]\n\n    # Remove callback from receiver list if it responds to the signal\n    for signal in signals:\n        if responds_to(callback, signal):\n            receivers[signal].remove(callback)"}
{"prompt": "def dM(self, k, t, param, Mkt, tips=None, gaps=None):\n        \"\"\"See docs for `DistributionModel` abstract base class.\"\"\"", "output": "\n        pass\n\n    def __str__(self):\n        \"\"\"See docs for `DistributionModel` abstract base class.\"\"\"\n        return self.__class__.__name__\n\n    def __repr__(self):\n        \"\"\"See docs for `DistributionModel` abstract base class.\"\"\"\n        return self.__class__.__name__ + '()'\n\n    def _initialize_parameters(self):\n        \"\"\"See docs for `DistributionModel` abstract base class.\"\"\"\n        self._param_names = []\n        self._param_values = []\n        self._param_bounds = []\n        self._param_frozen = []\n        self._param_frozen_parameters = []\n        self._param", "solution": "\n        assert 0 <= k < self.ncats\n        assert ((param in self.freeparams) or (param == 't') or (\n                param == self.distributedparam))\n        assert param not in self.distributionparams\n        return self._models[k].dM(t, param, Mkt, tips=tips, gaps=gaps)"}
{"prompt": "def _process_fields_convert_map(self, parameters, download=False):\n        \"\"\"\n        process fields_convert_map, ListView doesn't support list type but dict\n\n        fields_convert_map should be define as list or dict\n        for list, it can be:\n            [name, name, ...]\n            [(name, func), (name, func), ...] if func is str, it'll be the property name of class\n        for dict, it can be:\n            {'name':func, ...}\n        :param model: model object\n        :param parameters:\n        :param prefix: it'll used to combine prefix+_convert_xxx to get convert function\n            from class\n        :return:\n        \"\"\"", "output": " the prefix of the fields\n        :param suffix: it'll used to combine the suffix of the fields\n        :param download: it'll use the download method\n        :return:\n        \"\"\"\n        if isinstance(parameters, dict):\n            for field in parameters:\n                if isinstance(field, str):\n                    field = getattr(self, field)\n                if isinstance(field, list):\n                    field = _process_fields_convert(field, self.fields_convert_map)\n                    field = _process_fields_convert(field, self.fields_convert_map)\n                if isinstance(field, str):\n                    field = getattr", "solution": "\n        if 'fields_convert_map' in parameters:\n            _f = parameters.get('fields_convert_map') or []\n            parameters['fields_convert_map'] = self._get_fields_convert_map(_f, download)"}
{"prompt": "def _graphic(self):\n        \"\"\"\n        Adds the correct graphic options depending of the OS\n        \"\"\"", "output": "\n        self.add_option(\n            'graphic_width',\n            int(self.get_option('graphic_width', 0)),\n            doc='Width of the graphic in pixels',\n            default=0,\n        )\n        self.add_option(\n            'graphic_height',\n            int(self.get_option('graphic_height', 0)),\n            doc='Height of the graphic in pixels',\n            default=0,\n        )\n        self.add_option(\n            'graphic_margin',\n            int(self.get_option('graphic_margin', 0)),\n            doc", "solution": "\n\n        if sys.platform.startswith(\"win\"):\n            return []\n        if len(os.environ.get(\"DISPLAY\", \"\")) > 0:\n            return []\n        if \"-nographic\" not in self._options:\n            return [\"-nographic\"]\n        return []"}
{"prompt": "def do_async_send(self):\n        \"\"\"\n        Send any queued data. This function should only be called after a write\n        event on a file descriptor.\n        \"\"\"", "output": "\n        self.send_queue.put(self.file_descriptor)\n        self.send_queue.put(self.file_descriptor)\n\n    def send_file(self, path, callback=None):\n        \"\"\"\n        Send a file to the server.\n\n        :param path: The path to the file to send.\n        :type path: str\n        :param callback: The callback to call when the file is sent.\n            This function should only be called after a write event on a file\n            descriptor.\n        :type callback: function\n        :return: The number of bytes written.\n        :rtype: int\n        \"\"\"", "solution": "\n        assert len(self.sendbuf)\n\n        nwritten = self.sock.send(self.sendbuf)\n        nframes = 0\n\n        for entry in self.sendbuf_frames:\n            frame, offset, callback = entry\n\n            if offset <= nwritten:\n                nframes += 1\n\n                if callback:\n                    callback()\n            else:\n                entry[1] -= nwritten\n\n        self.sendbuf = self.sendbuf[nwritten:]\n        self.sendbuf_frames = self.sendbuf_frames[nframes:]"}
{"prompt": "def get_pt(self, viewer, points, pt, canvas_radius=None):\n        \"\"\"Takes an array of points `points` and a target point `pt`.\n        Returns the first index of the point that is within the\n        radius of the target point.  If none of the points are within\n        the radius, returns None.\n        \"\"\"", "output": "\n        if canvas_radius is None:\n            canvas_radius = self.canvas_radius\n        for i in range(len(points)):\n            if points[i] <= pt[i] <= target_point[i]:\n                return i\n        return None\n\n    def get_index_of_target_point(self, target_point):\n        \"\"\"Takes an index of a target point `index` and returns the\n        index of the target point.  If none of the points are within\n        the radius, returns None.\n        \"\"\"\n        return self.get_index_of_pt(target_point, self.target", "solution": "\n        if canvas_radius is None:\n            canvas_radius = self.cap_radius\n\n        if hasattr(self, 'rot_deg'):\n            # rotate point back to cartesian alignment for test\n            ctr_pt = self.get_center_pt()\n            pt = trcalc.rotate_coord(pt, [-self.rot_deg], ctr_pt)\n\n        res = self.within_radius(viewer, points, pt, canvas_radius)\n        return np.flatnonzero(res)"}
{"prompt": "def ctrl_request_update(_, nl_sock_h):\n    \"\"\"https://github.com/thom311/libnl/blob/libnl3_2_25/lib/genl/ctrl.c#L37.\n\n    Positional arguments:\n    nl_sock_h -- nl_sock class instance.\n\n    Returns:\n    Integer, genl_send_simple() output.\n    \"\"\"", "output": "\n    return _ctrl_request_update(nl_sock_h, _nl_socket_h.ctrl_request_update)\n\ndef ctrl_request_get_name(_, nl_sock_h):\n    \"\"\"https://github.com/thom311/libnl/blob/libnl3_2_25/lib/genl/ctrl.c#L38.\n\n    Positional arguments:\n    nl_sock_h -- nl_sock class instance.\n\n    Returns:\n    String, ctrl_send_name() output.\n    \"\"\"\n    return _ctrl_request_get_name(", "solution": "\n    return int(genl_send_simple(nl_sock_h, GENL_ID_CTRL, CTRL_CMD_GETFAMILY, CTRL_VERSION, NLM_F_DUMP))"}
{"prompt": "def prep_seq2c_bed(data):\n    \"\"\"Selecting the bed file, cleaning, and properly annotating for Seq2C\n    \"\"\"", "output": "\n    # get the chromosomes\n    chroms = []\n    for i in range(0, len(data), 3):\n        chrom = data[i:i+3]\n        if chrom in chroms:\n            chroms.remove(chrom)\n        else:\n            chroms.append(chrom)\n    # get the contig names\n    contig_names = []\n    for i in range(0, len(data), 4):\n        contig = data[i:i+4]\n        if contig in contig_names:\n            contig_names.remove(contig)\n        else:\n            con", "solution": "\n    if dd.get_background_cnv_reference(data, \"seq2c\"):\n        bed_file = _background_to_bed(dd.get_background_cnv_reference(data, \"seq2c\"), data)\n    else:\n        bed_file = regions.get_sv_bed(data)\n    if bed_file:\n        bed_file = bedutils.clean_file(bed_file, data, prefix=\"svregions-\")\n    else:\n        bed_file = bedutils.clean_file(dd.get_variant_regions(data), data)\n    if not bed_file:\n        return None\n\n    col_num = bt.BedTool(bed_file).field_count()\n    if col_num < 4:\n        annotated_file = annotate.add_genes(bed_file, data, max_distance=0)\n        if annotated_file == bed_file:\n            raise ValueError(\"BED file for Seq2C must be annotated with gene names, \"\n                             \"however the input BED is 3-columns and we have no transcript \"\n                             \"data to annotate with \" + bed_file)\n        annotated_file = annotate.gene_one_per_line(annotated_file, data)\n    else:\n        annotated_file = bed_file\n\n    ready_file = \"%s-seq2cclean.bed\" % (utils.splitext_plus(annotated_file)[0])\n    if not utils.file_uptodate(ready_file, annotated_file):\n        bed = bt.BedTool(annotated_file)\n        if col_num > 4 and col_num != 8:\n            bed = bed.cut(range(4))\n        bed = bed.filter(lambda x: x.name not in [\"\", \".\", \"-\"])\n        with file_transaction(data, ready_file) as tx_out_file:\n            bed.saveas(tx_out_file)\n        logger.debug(\"Saved Seq2C clean annotated ready input BED into \" + ready_file)\n\n    return ready_file"}
{"prompt": "def check_node_parent(\n        cls, resource_id, new_parent_id, db_session=None, *args, **kwargs\n    ):\n        \"\"\"\n        Checks if parent destination is valid for node\n\n        :param resource_id:\n        :param new_parent_id:\n        :param db_session:\n        :return:\n        \"\"\"", "output": "\n        if db_session is None:\n            db_session = db.session\n        if new_parent_id is None:\n            return False\n        if resource_id is None:\n            return False\n        if not cls.query.filter_by(id=resource_id).first():\n            return False\n        if not cls.query.filter_by(parent_id=new_parent_id).first():\n            return False\n        if not cls.query.filter_by(id=new_parent_id).first():\n            return False\n        if not cls.query.filter_by(parent_id=new", "solution": "\n        db_session = get_db_session(db_session)\n        new_parent = ResourceService.lock_resource_for_update(\n            resource_id=new_parent_id, db_session=db_session\n        )\n        # we are not moving to \"root\" so parent should be found\n        if not new_parent and new_parent_id is not None:\n            raise ZigguratResourceTreeMissingException(\"New parent node not found\")\n        else:\n            result = cls.path_upper(new_parent_id, db_session=db_session)\n            path_ids = [r.resource_id for r in result]\n            if resource_id in path_ids:\n                raise ZigguratResourceTreePathException(\n                    \"Trying to insert node into itself\"\n                )"}
{"prompt": "def draw(self):\n        \"\"\"\n        Draw the elbow curve for the specified scores and values of K.\n        \"\"\"", "output": "\n        if self.k_fold:\n            self.draw_kfold()\n        if self.k_value:\n            self.draw_kvalue()\n        if self.k_value_std:\n            self.draw_kfold()\n        if self.k_value_mean:\n            self.draw_kfold()\n        if self.k_value:\n            self.draw_kvalue(std=self.k_value_std)\n        if self.k_value:\n            self.draw_kfold(mean=self.k_value)\n        if self.k_value:\n            self", "solution": "\n        # Plot the silhouette score against k\n        self.ax.plot(self.k_values_, self.k_scores_, marker=\"D\")\n        if self.locate_elbow and self.elbow_value_!=None:\n            elbow_label = \"$elbow\\ at\\ k={}, score={:0.3f}$\".format(self.elbow_value_, self.elbow_score_)\n            self.ax.axvline(self.elbow_value_, c=LINE_COLOR, linestyle=\"--\", label=elbow_label)\n            \n        # If we're going to plot the timings, create a twinx axis\n        if self.timings:\n            self.axes = [self.ax, self.ax.twinx()]\n            self.axes[1].plot(\n                self.k_values_, self.k_timers_, label=\"fit time\",\n                c='g', marker=\"o\", linestyle=\"--\", alpha=0.75,\n            )\n\n\n        return self.ax"}
{"prompt": "def transform(curve, tf):\n    \"\"\"Transforms the curve by the homogeneous transformation matrix tf\"\"\"", "output": "\n    return tf.dot(tf, np.transpose(tf))\n\n\ndef get_curve_from_file(filename):\n    \"\"\"Loads a homogeneous transformation matrix from a file\"\"\"\n    with open(filename, 'r') as f:\n        matrix = np.array(f.read(), dtype=np.float64)\n    return np.array(matrix, dtype=np.float64)\n\n\ndef get_matrix_from_file(filename):\n    \"\"\"Loads a homogeneous transformation matrix from a file\"\"\"\n    with open(filename, 'r') as f:\n        matrix = np.array(", "solution": "\n    def to_point(p):\n        return np.array([[p.real], [p.imag], [1.0]])\n\n    def to_vector(z):\n        return np.array([[z.real], [z.imag], [0.0]])\n\n    def to_complex(v):\n        return v.item(0) + 1j * v.item(1)\n\n    if isinstance(curve, Path):\n        return Path(*[transform(segment, tf) for segment in curve])\n    elif is_bezier_segment(curve):\n        return bpoints2bezier([to_complex(tf.dot(to_point(p)))\n                               for p in curve.bpoints()])\n    elif isinstance(curve, Arc):\n        new_start = to_complex(tf.dot(to_point(curve.start)))\n        new_end = to_complex(tf.dot(to_point(curve.end)))\n        new_radius = to_complex(tf.dot(to_vector(curve.radius)))\n        return Arc(new_start, radius=new_radius, rotation=curve.rotation,\n                   large_arc=curve.large_arc, sweep=curve.sweep, end=new_end)\n    else:\n        raise TypeError(\"Input `curve` should be a Path, Line, \"\n                        \"QuadraticBezier, CubicBezier, or Arc object.\")"}
{"prompt": "def is_mouse_over(self, event):\n        \"\"\"\n        Check whether a MouseEvent is over thus scroll bar.\n\n        :param event: The MouseEvent to check.\n\n        :returns: True if the mouse event is over the scroll bar.\n        \"\"\"", "output": "\n        if event.type() == MouseEvent.MOUSE_OVER:\n            return self.is_mouse_over(event)\n        elif event.type() == MouseEvent.MIDDLE_DOWN:\n            return self.is_middle_down(event)\n        elif event.type() == MiddleDownEvent.MIDDLE_UP:\n            return self.is_middle_up(event)\n        elif event.type() == MiddleUpEvent.MIDDLE_UP:\n            return self.is_middle_up(event)\n        elif event.type() == MouseOverEvent.MIDDLE_", "solution": "\n        return event.x == self._x and self._y <= event.y < self._y + self._height"}
{"prompt": "def to_dict(self):  # cls, self):\n        \"\"\"Convert Docpie into a JSONlizable dict.\n\n        Use it in this way:\n        pie = Docpie(__doc__)\n        json.dumps(pie.convert_2_dict())\n\n        Note the `extra` info will be lost if you costomize that,\n        because a function is not JSONlizable.\n        You can use `set_config(extra={...})` to set it back.\n        \"\"\"", "output": "\n        return self.convert_2_dict()\n\n    def set_config(self, config):\n        \"\"\"Set the config.\n\n        Use it in this way:\n        pie.set_config(config)\n        \"\"\"\n        self.config = config\n\n    def to_json(self):\n        \"\"\"Convert Docpie into a JSON string.\n\n        Use it in this way:\n        json.dumps(pie.to_dict())\n        \"\"\"\n        return self.__doc__\n\n    def to_dict_with_list(self):\n        \"\"\"Convert a list of Docpie objects into a JSON string.\n\n        Returns a list of Docpie objects", "solution": "\n        config = {\n            'stdopt': self.stdopt,\n            'attachopt': self.attachopt,\n            'attachvalue': self.attachvalue,\n            'auto2dashes': self.auto2dashes,\n            'case_sensitive': self.case_sensitive,\n            'namedoptions': self.namedoptions,\n            'appearedonly': self.appeared_only,\n            'optionsfirst': self.options_first,\n            'option_name': self.option_name,\n            'usage_name': self.usage_name,\n            'name': self.name,\n            'help': self.help,\n            'version': self.version\n        }\n\n        text = {\n            'doc': self.doc,\n            'usage_text': self.usage_text,\n            'option_sections': self.option_sections,\n        }\n\n        # option = [convert_2_dict(x) for x in self.options]\n\n        option = {}\n        for title, options in self.options.items():\n            option[title] = [convert_2_dict(x) for x in options]\n\n        usage = [convert_2_dict(x) for x in self.usages]\n\n        return {\n            '__version__': self._version,\n            '__class__': 'Docpie',\n            '__config__': config,\n            '__text__': text,\n            'option': option,\n            'usage': usage,\n            'option_names': [list(x) for x in self.opt_names],\n            'opt_names_required_max_args': self.opt_names_required_max_args\n        }"}
{"prompt": "def set_collection(self, collection):\n        \"\"\"Reset sort state, set collection and emit resetModel signal\"\"\"", "output": "\n        self.model.set_sort_column_id(collection.get_sort_column_id())\n        self.emit(SIGNAL(\"dataChanged(const QModelIndex&)\"), self.model.index_changed)\n\n    def set_sort_order(self, order):\n        \"\"\"Set sort order\"\"\"\n        self.model.set_sort_order(order)\n        self.emit(SIGNAL(\"dataChanged(const QSortFilter&)\"), self.model.sort_changed)\n        self.sort(0, Qt.AscendingOrder)\n\n    def sort_by_name(self, column, order", "solution": "\n        self.beginResetModel()\n        self.collection = collection\n        self.sort_state = (-1, False)\n        self.endResetModel()"}
{"prompt": "def to_capabilities(self):\n        \"\"\"\n        Creates a capabilities with all the options that have been set\n\n        :Returns: A dictionary with everything\n        \"\"\"", "output": "\n        capabilities = {}\n        options = self.options\n        for option in options:\n            capabilities[option] = {}\n            for option_value in options[option]:\n                capabilities[option][option_value] = {}\n                for key, value in self.options[option_value].items():\n                    capabilities[option][option_value][key] = self.options[option_value][option_value].get(key, \"\")\n            for key, value in self.options[option_value].items():\n                capabilities[option_value][option_value][\"default\"] = self.options[option_value][\"default", "solution": "\n        caps = self._caps\n        chrome_options = self.experimental_options.copy()\n        chrome_options[\"extensions\"] = self.extensions\n        if self.binary_location:\n            chrome_options[\"binary\"] = self.binary_location\n        chrome_options[\"args\"] = self.arguments\n        if self.debugger_address:\n            chrome_options[\"debuggerAddress\"] = self.debugger_address\n\n        caps[self.KEY] = chrome_options\n\n        return caps"}
{"prompt": "def exec_command(self, cmd, tmp_path, sudo_user, sudoable=False, executable='/bin/sh'):\n        \"\"\" run a command on the remote host \"\"\"", "output": "\n        super(Connection, self).exec_command(cmd, tmp_path, sudo_user, sudoable)\n\n        self._display.vvv(\"EXEC %s\" % cmd, host=self._play_context.remote_addr)\n\n        if executable:\n            cmd = executable +'-c'+ cmd\n\n        self._display.vvv(\"EXEC %s\" % cmd, host=self._play_context.remote_addr)\n        rc, stdout, stderr = self._exec_command(cmd, tmp_path)\n\n        # save result in resultfile\n        if self._play_context.become and self", "solution": "\n\n        vvv(\"EXEC COMMAND %s\" % cmd)\n\n        if self.runner.sudo and sudoable:\n            raise errors.AnsibleError(\"fireball does not use sudo, but runs as whoever it was initiated as.  (That itself is where to use sudo).\")\n\n        data = dict(\n            mode='command',\n            cmd=cmd,\n            tmp_path=tmp_path,\n            executable=executable,\n        )\n        data = utils.jsonify(data)\n        data = utils.encrypt(self.key, data)\n        self.socket.send(data)\n        \n        response = self.socket.recv()\n        response = utils.decrypt(self.key, response)\n        response = utils.parse_json(response)\n\n        return (response.get('rc',None), '', response.get('stdout',''), response.get('stderr',''))"}
{"prompt": "def _tidy(self, html, smart_tidy):\n        \"\"\" Tidy HTML if we have a tidy method.\n\n        This fixes problems with some sites which would otherwise trouble\n        DOMDocument's HTML parsing.\n\n        Although sometimes it makes the problem worse, which is why we can\n        override it in site config files.\n        \"\"\"", "output": "\n        if smart_tidy:\n            return smart_tidy.tidy(html)\n        return html\n\n    def _tidy_error(self, msg):\n        \"\"\" Return a string error message if the HTML string is not well formed.\n\n        This fixes problems with some sites which would otherwise trouble\n        DOMDocument's HTML parsing.\n        \"\"\"\n        if not msg:\n            return ''\n        return''.join(msg.split())\n\n    def _get_element_by_id(self, id):\n        \"\"\" Return the element with the given id.\n\n        This is a helper method for the DOM Level 3 spec.\n        \"\"\"", "solution": "\n\n        if self.config.tidy and tidylib and smart_tidy:\n\n            try:\n                document, errors = tidylib.tidy_document(html, self.tidy_config)\n\n            except UnicodeDecodeError:\n                # For some reason, pytidylib fails to decode, whereas the\n                # original html content converts perfectly manually.\n                document, errors = tidylib.tidy_document(html.encode('utf-8'),\n                                                         self.tidy_config)\n                document = document.decode('utf-8')\n            # if errors:\n            #     LOGGER.debug(u'Ignored errors returned by tidylib: %s',\n            #                  errors)\n\n            self.tidied = True\n            self.html = document\n\n            LOGGER.info(u'Tidied document.')\n\n        else:\n            self.html = html"}
{"prompt": "def get_place_tags(index_page, domain):  #: TODO geoip to docstring\n    \"\"\"\n    Return list of `place` tags parsed from `meta` and `whois`.\n\n    Args:\n        index_page (str): HTML content of the page you wisht to analyze.\n        domain (str): Domain of the web, without ``http://`` or other parts.\n\n    Returns:\n        list: List of :class:`.SourceString` objects.\n    \"\"\"", "output": "\n    return [\n        source_string.strip()\n        for source_string in index_page.xpath('//*[@id=\"meta\"]/@id | //*[@id=\"whois\"]/@id', namespaces=ns)\n    ]\n\n\ndef get_place_links(index_page, domain):  #: TODO geoip to docstring\n    \"\"\"\n    Return list of `place` links parsed from `meta` and `whois`.\n\n    Args:\n        index_page (str): HTML content of the page you wisht to analyze.\n        domain (str): Domain of the web, without ``http://", "solution": "\n    ip_address = get_ip_address(domain)\n    dom = dhtmlparser.parseString(index_page)\n\n    place_tags = [\n        get_html_geo_place_tags(dom),\n        get_whois_tags(ip_address),\n        # [_get_geo_place_tag(ip_address)],  # TODO: implement geoip\n    ]\n\n    return sum(place_tags, [])"}
{"prompt": "def _clean_spaces_backtick_quoted_names(tok):\n    \"\"\"Clean up a column name if surrounded by backticks.\n\n    Backtick quoted string are indicated by a certain tokval value. If a string\n    is a backtick quoted token it will processed by\n    :func:`_remove_spaces_column_name` so that the parser can find this\n    string when the query is executed.\n    See also :meth:`NDFrame._get_space_character_free_column_resolver`.\n\n    Parameters\n    ----------\n    tok : tuple of int, str\n        ints correspond to the all caps constants in the tokenize module\n\n    Returns\n    -------\n    t : tuple of int, str\n        Either the input or token or the replacement values\n    \"\"\"", "output": " module.\n\n    Returns\n    -------\n    tuple of int, str\n        The tuple of tokens that were removed.\n    \"\"\"\n    if tok[0] == tokenize.OP and tok[1] == tokenize.OP:\n        return (tokenize.NAME, 'OP')\n    elif tok[0] == tokenize.NAME and tok[1] == tokenize.NAME:\n        return (tokenize.OP, 'NAME')\n    elif tok[0] == tokenize.NAME and tok[1] == tokenize.NAME:\n        return (tokenize.OP, 'OP')\n    elif tok[0] == tokenize.OP and tok[1]", "solution": "\n    toknum, tokval = tok\n    if toknum == _BACKTICK_QUOTED_STRING:\n        return tokenize.NAME, _remove_spaces_column_name(tokval)\n    return toknum, tokval"}
{"prompt": "def make_imshow_plot(grid, name):\n    \"\"\"\n    Takes a grid of RGB or RGBA values and a filename to save the figure into.\n    Generates a figure by coloring all grid cells appropriately.\n    \"\"\"", "output": "\n    if type(grid) == np.ndarray:\n        grid = np.array(grid)\n    if type(grid) == np.ndarray:\n        grid = np.array(grid)\n    if type(grid) == np.ndarray:\n        grid = np.array(grid)\n    if type(grid) == str:\n        plt.savefig(name)\n    else:\n        plt.savefig(name, bbox_inches='tight')\n\n\ndef make_imshow_grid(grid, name):\n    \"\"\"\n    Takes a list of RGB values and a filename to save the figure into.\n    Generates a grid", "solution": "\n    plt.tick_params(labelbottom=\"off\", labeltop=\"off\", labelleft=\"off\",\n                    labelright=\"off\", bottom=\"off\", top=\"off\", left=\"off\",\n                    right=\"off\")\n    plt.imshow(grid, interpolation=\"nearest\", aspect=1, zorder=1)\n    plt.tight_layout()\n    plt.savefig(name, dpi=1000, bbox_inches=\"tight\")"}
{"prompt": "def _torque_queue_nodes(queue):\n    \"\"\"Retrieve the nodes available for a queue.\n\n    Parses out nodes from `acl_hosts` in qstat -Qf and extracts the\n    initial names of nodes used in pbsnodes.\n    \"\"\"", "output": "\n    # The first line is always the header\n    line = queue[0].split()\n    # The second line is always the header\n    header = line[0]\n    # The third line is always the header\n    if len(line) > 1:\n        header = line[1]\n    # The fourth line is always the header\n    if len(line) > 3:\n        header = line[3]\n    # The fifth line is always the header\n    if len(line) > 4:\n        header = line[4]\n    # The sixth line is always the header\n    if len(line) >", "solution": "\n    qstat_out = subprocess.check_output([\"qstat\", \"-Qf\", queue]).decode()\n    hosts = []\n    in_hosts = False\n    for line in qstat_out.split(\"\\n\"):\n        if line.strip().startswith(\"acl_hosts = \"):\n            hosts.extend(line.replace(\"acl_hosts = \", \"\").strip().split(\",\"))\n            in_hosts = True\n        elif in_hosts:\n            if line.find(\" = \") > 0:\n                break\n            else:\n                hosts.extend(line.strip().split(\",\"))\n    return tuple([h.split(\".\")[0].strip() for h in hosts if h.strip()])"}
{"prompt": "def get_matches(expr_lst, ts):\n    \"\"\"\n    Get a list of TimeSeries objects that match the given expression.\n    :param list expr_lst: Expression\n    :param list ts: TimeSeries\n    :return list new_ts: Matched time series objects\n    :return list idxs: Indices of matched objects\n    \"\"\"", "output": "\n    return [TimeSeries(idx) for idx in expr_lst if ts.contains(idx)]\n\n\ndef get_matches_from_list(expr_lst, ts):\n    \"\"\"\n    Get a list of TimeSeries objects that match the given expression.\n    :param list expr_lst: Expression\n    :param list ts: TimeSeries\n    :return list new_ts: Matched time series objects\n    :return list idxs: Indices of matched objects\n    \"\"\"\n    return [TimeSeries(idx) for idx in expr_lst if ts.contains(idx) and ts.contains(ts.get_", "solution": "\n    logger_ts.info(\"enter get_matches\")\n    new_ts = []\n    idxs = []\n    match = False\n    try:\n        for idx, ts_data in enumerate(ts):\n            for expr in expr_lst:\n                try:\n                    val = ts_data[expr[0]]\n                    # Check what comparison operator is being used\n                    if expr[1] == 'in':\n                        # \"IN\" operator can't be used in get_truth. Handle first.\n                        if expr[2] in val:\n                            match = True\n                    elif match_operators(val, expr[1], expr[2]):\n                        # If it's a typical operator, check with the truth test.\n                        match = True\n                    else:\n                        # If one comparison is false, then it can't possibly be a match\n                        match = False\n                        break\n                except KeyError as e:\n                    logger_ts.warn(\"get_matches: KeyError: getting value from TimeSeries object, {}, {}\".format(expr, e))\n                    match = False\n                except IndexError as e:\n                    logger_ts.warn(\"get_matches: IndexError: getting value from TimeSeries object, {}, {}\".format(expr, e))\n                    match = False\n            if match:\n                idxs.append(idx)\n                new_ts.append(ts_data)\n    except AttributeError as e:\n        logger_ts.debug(\"get_matches: AttributeError: unable to get expression matches, {},  {}\".format(type(ts), e))\n        print(\"Error: Timeseries is an invalid data type\")\n    if not new_ts:\n        print(\"No matches found for that expression\")\n    else:\n        print(\"Found {} matches from {} columns\".format(len(new_ts), len(ts)))\n    logger_ts.info(\"exit get_matches\")\n    return new_ts, idxs"}
{"prompt": "def defverb(self, s1, p1, s2, p2, s3, p3):\n        \"\"\"\n        Set the verb plurals for s1, s2 and s3 to p1, p2 and p3 respectively.\n\n        Where 1, 2 and 3 represent the 1st, 2nd and 3rd person forms of the verb.\n\n        \"\"\"", "output": "\n        self.defaultid = defaultid\n        self.defbultid = defbultid\n        self.defaultid_s1 = defaultid_s1\n        self.defaultid_s2 = defaultid_s2\n        self.defaultid_s3 = defaultid_s3\n        self.defbultid_p1 = defbultid_p1\n        self.defbultid_p2 = defbultid_p2\n        self.defaultid_p3 = defaulti", "solution": "\n        self.checkpat(s1)\n        self.checkpat(s2)\n        self.checkpat(s3)\n        self.checkpatplural(p1)\n        self.checkpatplural(p2)\n        self.checkpatplural(p3)\n        self.pl_v_user_defined.extend((s1, p1, s2, p2, s3, p3))\n        return 1"}
{"prompt": "def unixtimestamp(datetime):\n    \"\"\"Get unix time stamp from that given datetime. If datetime\n    is not tzaware then it's assumed that it is UTC\n    \"\"\"", "output": "\n    if not datetime.tzinfo:\n        return datetime.replace(tzinfo=utc)\n    return datetime.astimezone(utc).replace(tzinfo=None)\n\n\ndef unixtimestamp_to_datetime(unixtimestamp):\n    \"\"\"Convert a unix timestamp to a datetime object\n    \"\"\"\n    if not unixtimestamp:\n        return None\n    return datetime.utcfromtimestamp(unixtimestamp)\n\n\ndef get_date_range(start_date, end_date):\n    \"\"\"Get the start and end date range from the given start and end date\n    \"\"\"\n    if not start_date:\n        return None\n    if not end_date:\n       ", "solution": "\n    epoch = UTC.localize(datetime.utcfromtimestamp(0))\n    if not datetime.tzinfo:\n        dt = UTC.localize(datetime)\n    else:\n        dt = UTC.normalize(datetime)\n    delta = dt - epoch\n    return total_seconds(delta)"}
{"prompt": "def insert(self, idx, val):\n        \"\"\"\n        Insert the element *val* into the list at *idx*. Raises a ValueError if\n        the *val* at *idx* would violate the sort order.\n        \"\"\"", "output": "\n        if idx < 0:\n            raise ValueError(\"Negative indexing is not allowed in Insert\")\n        if idx >= len(self.data):\n            raise ValueError(\"Index value is out of range\")\n        self.data.insert(idx, val)\n        self.sort_order[idx] = idx\n        self.sort_order[self.sort_order.index(val)] = self.sort_order.index(idx)\n\n    def remove(self, val):\n        \"\"\"\n        Remove the element *val* from the list at *idx*. Raises a ValueError if\n        the *val* at *idx* would vi", "solution": "\n        _maxes, _lists, _len = self._maxes, self._lists, self._len\n\n        if idx < 0:\n            idx += _len\n        if idx < 0:\n            idx = 0\n        if idx > _len:\n            idx = _len\n\n        if not _maxes:\n            # The idx must be zero by the inequalities above.\n            _maxes.append(val)\n            _lists.append([val])\n            self._len = 1\n            return\n\n        if not idx:\n            if val > _lists[0][0]:\n                msg = '{0} not in sort order at index {1}'.format(repr(val), 0)\n                raise ValueError(msg)\n            else:\n                _lists[0].insert(0, val)\n                self._expand(0)\n                self._len += 1\n                return\n\n        if idx == _len:\n            pos = len(_lists) - 1\n            if _lists[pos][-1] > val:\n                msg = '{0} not in sort order at index {1}'.format(repr(val), _len)\n                raise ValueError(msg)\n            else:\n                _lists[pos].append(val)\n                _maxes[pos] = _lists[pos][-1]\n                self._expand(pos)\n                self._len += 1\n                return\n\n        pos, idx = self._pos(idx)\n        idx_before = idx - 1\n        if idx_before < 0:\n            pos_before = pos - 1\n            idx_before = len(_lists[pos_before]) - 1\n        else:\n            pos_before = pos\n\n        before = _lists[pos_before][idx_before]\n        if before <= val <= _lists[pos][idx]:\n            _lists[pos].insert(idx, val)\n            self._expand(pos)\n            self._len += 1\n        else:\n            msg = '{0} not in sort order at index {1}'.format(repr(val), idx)\n            raise ValueError(msg)"}
{"prompt": "def restart(self):\n        \"\"\"\n        Performs a soft reload of the HAProxy process.\n        \"\"\"", "output": "\n        self.logger.info(\"Restarting HAProxy process\")\n        self.process.send_signal(signal.SIGHUP)\n        self.process.wait()\n\n    def run(self):\n        \"\"\"\n        Runs the HAProxy process.\n        \"\"\"\n        self.logger.info(\"Starting HAProxy process\")\n        self.process = subprocess.Popen(self.command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        self.logger.info(\"Restarting HAProxy process\")\n        self.restart()\n        self.logger.info(\"HAProxy process is running\")\n\n    def stop(", "solution": "\n        version = self.get_version()\n\n        command = [\n            \"haproxy\",\n            \"-f\", self.config_file_path, \"-p\", self.pid_file_path\n        ]\n        if version and version >= (1, 5, 0):\n            command.extend([\"-L\", self.peer.name])\n        if os.path.exists(self.pid_file_path):\n            with open(self.pid_file_path) as fd:\n                command.extend([\"-sf\", fd.read().replace(\"\\n\", \"\")])\n\n        try:\n            output = subprocess.check_output(command)\n        except subprocess.CalledProcessError as e:\n            logger.error(\"Failed to restart HAProxy: %s\", str(e))\n            return\n\n        if output:\n            logging.error(\"haproxy says: %s\", output)\n\n        logger.info(\"Gracefully restarted HAProxy.\")"}
{"prompt": "def _score(estimator, X_test, y_test, scorer):\n    \"\"\"Compute the score of an estimator on a given test set.\"\"\"", "output": "\n    if hasattr(estimator,'score'):\n        # For multi-metric evaluation, the score method of _score expects\n        # scores to be a float between 0.0 and 1.0\n        return estimator.score(X_test, y_test)\n    else:\n        # For multi-class evaluation, the score method is proportional to the\n        # number of labels.\n        return 1.0\n\n\ndef _permutation_test_score(estimator, X, y, n_permutations, n_labels):\n    \"\"\"Auxiliary function for permutation_test_score\"\"\"\n    if hasattr(estimator, 'permutation'):", "solution": "\n    if y_test is None:\n        score = scorer(estimator, X_test)\n    else:\n        score = scorer(estimator, X_test, y_test)\n    if not isinstance(score, numbers.Number):\n        raise ValueError(\"scoring must return a number, got %s (%s) instead.\"\n                         % (str(score), type(score)))\n    return score"}
{"prompt": "def connect(self):\n        \"\"\"\n        Create internal connection to AMQP service.\n        \"\"\"", "output": "\n        self.connection = self.connection_class(self.connection_params)\n        self.connection.on_open(self.on_open)\n        self.connection.on_close(self.on_close)\n        self.connection.on_open(self.on_open)\n        self.connection.on_close(self.on_close)\n        self.connection.on_open(self.on_close)\n        self.connection.on_open(self.on_close)\n        self.channel.exchange_declare(exchange=self.exchange, type='direct')\n        self", "solution": "\n        logging.info(\"Connecting to {} with user {}.\".format(self.host, self.username))\n        credentials = pika.PlainCredentials(self.username, self.password)\n        connection_params = pika.ConnectionParameters(host=self.host,\n                                                      credentials=credentials,\n                                                      heartbeat_interval=self.heartbeat_interval)\n        self.connection = pika.BlockingConnection(connection_params)"}
{"prompt": "def list(database=None, directory=None, verbose=None):\n    \"\"\"List migrations.\"\"\"", "output": "\n    if database is None:\n        database = get_database()\n    if directory is None:\n        directory = get_directory()\n    if verbose is None:\n        verbose = conf.get('verbosity', '0')\n    if database not in databases:\n        raise Exception('Unknown database: %s' % database)\n    if directory not in directory_migrations:\n        raise Exception('Unknown directory: %s' % directory)\n    return directory_migrations[database]\n\n\ndef get_database():\n    \"\"\"Return the name of the current database.\"\"\"\n    return conf.get('db_name', 'invenio')\n\n\ndef get", "solution": "\n    router = get_router(directory, database, verbose)\n    click.echo('Migrations are done:')\n    click.echo('\\n'.join(router.done))\n    click.echo('')\n    click.echo('Migrations are undone:')\n    click.echo('\\n'.join(router.diff))"}
{"prompt": "def _extract_meta_value(self, tag):\n        # type: (str, List[str]) -> str\n        \"\"\"Find a target value by `tag` from given meta data.\n\n        :param tag: str\n        :param meta_data: list\n        :return: str\n        \"\"\"", "output": "\n        target_value = None\n        for meta_data in meta_data_list:\n            if tag in meta_data:\n                target_value = meta_data[tag]\n                break\n        return target_value\n\n    def _get_target_value(self, tag, target_value):\n        # type: (str, str, Any) -> str\n        \"\"\"Find a target value by `tag` from given target value.\n\n        :param tag: str\n        :param target_value: str\n        :return: str\n        \"\"\"\n        for target_item in target_value:\n            if tag in target_item:", "solution": "\n        try:\n            return [l[len(tag):] for l in self.meta_data if l.startswith(tag)][0]\n        except IndexError:\n            return '* Not Found *'"}
{"prompt": "def from_dict(input_dict):\n        \"\"\"\n        Instantiate an object of a derived class using the information\n        in input_dict (built by the to_dict method of the derived class).\n        More specifically, after reading the derived class from input_dict,\n        it calls the method _build_from_input_dict of the derived class.\n        Note: This method should not be overrided in the derived class. In case\n        it is needed, please override _build_from_input_dict instate.\n\n        :param dict input_dict: Dictionary with all the information needed to\n           instantiate the object.\n        \"\"\"", "output": "\n        :return:\n        \"\"\"\n        self._input_dict = input_dict\n        self._instate = instate\n        self._build_from_state()\n\n    def _build_from_state(self):\n        \"\"\"\n        Build the object from the given state.\n        \"\"\"\n        self._instate.add_state(self.state_id, self.state_name, self.state_id)\n        self.instate_id = self._instate.id\n        self.state_name = self._instate.name\n        self.state_name = self.state_name.replace(\" \",", "solution": "\n\n        import copy\n        input_dict = copy.deepcopy(input_dict)\n        normalizer_class = input_dict.pop('class')\n        import GPy\n        normalizer_class = eval(normalizer_class)\n        return normalizer_class._build_from_input_dict(normalizer_class, input_dict)"}
{"prompt": "def _structure_tuple(self, obj, tup):\n        \"\"\"Deal with converting to a tuple.\"\"\"", "output": "\n        return (obj, self._get_type(obj))\n\n    def _get_type(self, obj):\n        \"\"\"Get the type of an object.\"\"\"\n        return self._types[self._get_type_name(obj)]\n\n    def _get_type_name(self, obj):\n        \"\"\"Get the type name of an object.\"\"\"\n        return self._types_names[self._get_type_name_name(obj)]\n\n    def _get_type_name_name(self, obj):\n        \"\"\"Get the name of an object.\"\"\"\n        return self._types_names_names[obj]", "solution": "\n        tup_params = tup.__args__\n        has_ellipsis = tup_params and tup_params[-1] is Ellipsis\n        if tup_params is None or (has_ellipsis and tup_params[0] is Any):\n            # Just a Tuple. (No generic information.)\n            return tuple(obj)\n        if has_ellipsis:\n            # We're dealing with a homogenous tuple, Tuple[int, ...]\n            tup_type = tup_params[0]\n            conv = self._structure_func.dispatch(tup_type)\n            return tuple(conv(e, tup_type) for e in obj)\n        else:\n            # We're dealing with a heterogenous tuple.\n            return tuple(\n                self._structure_func.dispatch(t)(e, t)\n                for t, e in zip(tup_params, obj)\n            )"}
{"prompt": "def extract_ar (archive, compression, cmd, verbosity, interactive, outdir):\n    \"\"\"Extract a AR archive.\"\"\"", "output": "\n    cmd = cmd.replace('ar', 'ar_').replace('t', 't_').replace('o', 'o_').replace('a', 'a_').replace('t', 't_').replace('s','s_').replace('t', 't_').replace('o', 'o_').replace('a', 'a_').replace('t', 't_').replace('s','s_').replace('t', 't_').replace('o', 'o_').replace('a', 'a_').replace('t', 't_').replace('s", "solution": "\n    opts = 'x'\n    if verbosity > 1:\n        opts += 'v'\n    cmdlist = [cmd, opts, os.path.abspath(archive)]\n    return (cmdlist, {'cwd': outdir})"}
{"prompt": "def save_book(self, f_buf=None):\n        \"\"\"\n        Return a file buffer containing the resulting xls\n\n        :param obj f_buf: A file buffer supporting the write and seek\n        methods\n        \"\"\"", "output": "\n        if f_buf is None:\n            f_buf = BytesIO()\n        if self.book_format =='mobi':\n            self.book_xliff = f_buf.write\n        elif self.book_format == 'pdf':\n            self.book_xliff = f_buf.write\n        else:\n            raise ValueError(\"Unknown format: %s\" % self.book_format)\n        return self.book_xliff\n\n    def save_book_as(self, book_as, f_buf=None):\n        \"\"\"\n        Return a file buffer containing the resulting xls\n\n       ", "solution": "\n        if f_buf is None:\n            f_buf = StringIO.StringIO()\n        f_buf.write(openpyxl.writer.excel.save_virtual_workbook(self.book))\n        f_buf.seek(0)\n        return f_buf"}
{"prompt": "def from_axis_angle(axis, angle):\n    \"\"\"\n    Returns a #Quaternion that represents the right-handed\n    rotation of *angle* radians about the givne *axis*.\n\n    :param axis: The unit vector representing the axis of rotation.\n    :param angle: The angle of rotation, in radians.\n    \"\"\"", "output": "\n    return Quaternion(\n        axis[0],\n        axis[1],\n        axis[2],\n        angle[0],\n        angle[1],\n        angle[2],\n        axis[3],\n        axis[4],\n        angle[3],\n        axis[5],\n        axis[6],\n        axis[7],\n        axis[8],\n        axis[9],\n        axis[10],\n        axis[11],\n        axis[12],\n        axis[13],\n        axis[14],\n        axis[15],\n        axis[16],\n        axis[17],\n        axis[18],\n        angle[1],\n       ", "solution": "\n\n    sincomp = math.sin(angle / 2.0)\n    return Quaternion(\n      axis.x * sincomp, axis.y * sincomp,\n      axis.z * sincomp, math.cos(angle / 2.0))"}
{"prompt": "def get_arguments(options):\n   \"\"\" This function handles and validates the wrapper arguments. \"\"\"", "output": "\n   # Check if the wrapper arguments are valid\n   if options.wrapper_args is None:\n      raise ValueError(\"Must specify a wrapper arguments.\")\n\n   # Check if the wrapper arguments are valid\n   if options.wrapper_args.find(\" \") == -1:\n      raise ValueError(\"Must specify a wrapper argument.\")\n\n   # Check if the wrapper is a valid file\n   if options.wrapper_args.find(\" \") == -1:\n      raise ValueError(\"Must specify a wrapper file.\")\n\n   # Check if the wrapper is a valid directory\n   if options.wrapper_args.find(\" \") == -1:\n      raise ValueError(\"Must", "solution": "\n   # These the next couple of lines defines the header of the Help output\n   parser = ArgumentParser(\n      formatter_class=RawDescriptionHelpFormatter,\n      usage=("}
{"prompt": "def describe_keyspaces(self, ):\n    \"\"\"\n    list the defined keyspaces in this cluster\n    \"\"\"", "output": "\n    print \"Available Keyspaces:\"\n    keyspaces = []\n    for node in self.nodes:\n      keyspaces.append(node.keypaces)\n    print \"  \" + str(len(keyspaces)) + \" defined in this cluster:\"\n    for keyspace in keyspaces:\n      print \"    \" + keyspace.name\n    print \"  \" + str(len(self.keyspaces)) + \" defined in this cluster:\"\n    for node in self.keyspaces:\n      print \"    \" + node.name\n    return node\n\n  def get_all_keyspaces(self):\n    \"\"\"\n    return the", "solution": "\n    self._seqid += 1\n    d = self._reqs[self._seqid] = defer.Deferred()\n    self.send_describe_keyspaces()\n    return d"}
{"prompt": "def process_raw_data(cls, raw_data):\n        \"\"\"Create a new model using raw API response.\"\"\"", "output": "\n        return cls(\n            id=raw_data.get('id'),\n            name=raw_data.get('name'),\n            description=raw_data.get('description'),\n            type=raw_data.get('type'),\n            unit_of_measure=raw_data.get('unit_of_measure'),\n            icon=raw_data.get('icon'),\n            device_state=raw_data.get('device_state'),\n            unit_of_measurement=raw_data.get('unit_of_measurement'),\n            unit_of_measure=raw_data.get('unit_of", "solution": "\n        properties = raw_data.get(\"properties\", {})\n\n        load_balancing_rules = []\n        for raw_content in properties.get(\"loadBalancingRules\", []):\n            resource = Resource.from_raw_data(raw_content)\n            load_balancing_rules.append(resource)\n        properties[\"loadBalancingRules\"] = load_balancing_rules\n\n        return super(Probes, cls).process_raw_data(raw_data)"}
{"prompt": "def handle(self, *args, **options):\n        \"\"\"Command handle.\"\"\"", "output": "\n        if len(args)!= 1:\n            raise CommandError(\"Please provide a single argument to %s\" % self.get_app_label())\n\n        self.stdout.write(\"Command finished. Press Ctrl-C to exit.\")\n", "solution": "\n        verbosity = int(options['verbosity'])\n\n        if self.has_filter(options):\n            self.filter_indices(options, verbosity)\n        else:\n            # Process all indices.\n            index_builder.build()"}
{"prompt": "def disconnect(self):\n        \"\"\"Disconnect from the socket.\"\"\"", "output": "\n        self.sock.close()\n\n    def send(self, data):\n        \"\"\"Send data to the socket.\"\"\"\n        self.sock.sendall(data)\n\n    def recv(self, bufsize):\n        \"\"\"Receive data from the socket and return the result.\"\"\"\n        data = \"\"\n        while True:\n            try:\n                data += self.sock.recv(bufsize)\n            except socket.error:\n                break\n        return data\n\n    def __init__(self, host, port, username=None, password=None,\n                 pkey=None, cert=None, timeout=5, use_ssl=False, ca_cert=None", "solution": "\n        if self.pipeline:\n            self._send(*self.pipeline)\n        self.pipeline = None"}
{"prompt": "def base_type(self, type):\n        \"\"\"Return the base type of `type`.\"\"\"", "output": "\n        if type.is_pyobject:\n            return py_object_type\n        elif type.is_ptr:\n            return \"((PyObject *)%s)\" % type.base_type\n        else:\n            return type\n\n    def analyse_types(self, env):\n        \"\"\"\n        Analyse the types of the result of an expression for caching and\n        examining its subexpressions.\n        \"\"\"\n        self.arg_free_func = \\\n            self.arg_free_dict = weakref.ref(env.declare_tuple_type)\n        self.arg_free_dict_val = weakref.ref(env.declare", "solution": "\n        while 1:\n            if type.arg == \"leafref\":\n                node = type.i_type_spec.i_target_node\n            elif type.i_typedef is None:\n                break\n            else:\n                node = type.i_typedef\n            type = node.search_one(\"type\")\n        if type.arg == \"decimal64\":\n            return [type.arg, int(type.search_one(\"fraction-digits\").arg)]\n        elif type.arg == \"union\":\n            return [type.arg, [self.base_type(x) for x in type.i_type_spec.types]]\n        else:\n            return type.arg"}
{"prompt": "def iterate_similarity_datasets(args):\n    \"\"\"Generator over all similarity evaluation datasets.\n\n    Iterates over dataset names, keyword arguments for their creation and the\n    created dataset.\n\n    \"\"\"", "output": "\n    # Load the data.\n    data_path = args.data_path\n    if not os.path.exists(data_path):\n        print(\"No data found at: %s\" % data_path)\n        return\n\n    # Load the datasets.\n    for name in args.dataset_names:\n        if not os.path.exists(name):\n            print(\"No data found at: %s\" % name)\n            continue\n\n        # Load the keyword arguments.\n        if name == \"n_components\":\n            n_components = args.n_components\n        elif name == \"n_features\":\n            n_", "solution": "\n    for dataset_name in args.similarity_datasets:\n        parameters = nlp.data.list_datasets(dataset_name)\n        for key_values in itertools.product(*parameters.values()):\n            kwargs = dict(zip(parameters.keys(), key_values))\n            yield dataset_name, kwargs, nlp.data.create(dataset_name, **kwargs)"}
{"prompt": "def send_response(self, code, message=None, size='-'):\n        \"\"\"\n        Send the response header and log the response code.\n\n        Also send two standard headers with the server software\n        version and the current date.\n        \"\"\"", "output": "\n        if message is None:\n            message = self.response_header\n        if size == '-1':\n            self.response_size = None\n        else:\n            self.response_size = int(size)\n        if code == '200':\n            self.response_code = 200\n        elif code == '400':\n            self.response_code = 400\n        elif code == '404':\n            self.response_code = 404\n        elif code == '500':\n            self.response_code = 500\n        elif code == '503':\n            self.response_code = 503\n        elif code == '504':\n            self.", "solution": "\n        # pylint: disable-msg=W0221\n        if self._to_log or LOG.isEnabledFor(logging.DEBUG):\n            self.log_request(code, size)\n\n        if message is None:\n            if code in self.responses:\n                message = self.responses[code][0]\n            else:\n                message = ''\n\n        if self.request_version != 'HTTP/0.9':\n            self.wfile.write(\"%s %d %s\\r\\n\" %\n                             (\"HTTP/1.1\", code, message))\n        self.send_header('Server', self.version_string())\n        self.send_header('Date', self.date_time_string())"}
{"prompt": "def precision(precision, id_, hwid, type_):\n    \"\"\"Change the precision for the sensor and persist it in the sensor's EEPROM\"\"\"", "output": "\n    return _lib.TCOD_plot_get_precision(c_int(id_), precision, byref(hwid))\n\ndef set_fps(fps):\n    \"\"\"Set the frame rate for the display of the given FPS\"\"\"\n    return _lib.TCOD_plot_set_fps(c_int(fps), c_int(id_))\n\ndef set_name(name):\n    \"\"\"Set the name for the plot\"\"\"\n    return _lib.TCOD_plot_set_name(c_int(name))\n\ndef set_color(red, green, blue):", "solution": "\n    if id_ and (hwid or type_):\n        raise click.BadOptionUsage(\n            \"If --id is given --hwid and --type are not allowed.\"\n        )\n\n    if id_:\n        try:\n            sensor = W1ThermSensor.get_available_sensors()[id_ - 1]\n        except IndexError:\n            raise click.BadOptionUsage(\n                \"No sensor with id {0} available. \"\n                \"Use the ls command to show all available sensors.\".format(id_)\n            )\n    else:\n        sensor = W1ThermSensor(type_, hwid)\n\n    sensor.set_precision(precision, persist=True)"}
{"prompt": "def trips(self, val):\n        \"\"\"\n        Update ``self._trips_i`` if ``self.trips`` changes.\n        \"\"\"", "output": "\n        if val is None:\n            self._trips_i = None\n        else:\n            self._trips_i = val\n\n    def _get_trips_i(self):\n        \"\"\"\n        Return the current trips value.\n        \"\"\"\n        return self._trips_i\n\n    def _set_trips_i(self, val):\n        \"\"\"\n        Set the current trips value.\n        \"\"\"\n        if val is None:\n            self._trips_i = None\n        else:\n            self._trips_i = val\n\n    trips_i = property(_get_trips_i, _", "solution": "\n        self._trips = val\n        if val is not None and not val.empty:\n            self._trips_i = self._trips.set_index(\"trip_id\")\n        else:\n            self._trips_i = None"}
{"prompt": "def read(self):\n        try:\n            bytes = self.sock.recv(self.max_size)\n        except:\n            self.torrent.kill_peer(self)\n            return\n        \"\"\"\n        Chain of events:\n            - process_input\n            - check save_state and read length, id, and message accordingly\n                - if we have a piece (really a block), we piece.save it out\n                  inside call to ppiece\n                    - If we've completed a piece we:\n                        - Tell the switchboard to write it out\n                        - init a new piece\n        \"\"\"", "output": "\n        self.save_state = self.sock.recv(self.save_size)\n        self.save_length = self.sock.recv(self.save_size)\n        self.read_state = self.sock.recv(self.read_size)\n        self.id = self.sock.recv(self.id_size)\n        self.message = self.sock.recv(self.message_size)\n        \"\"\"\n        Chain of events:\n            - process_input\n            - check save_state and read length, id, and message accordingly\n                - if we have a piece (really a", "solution": "\n        if len(bytes) == 0:\n            print 'Got 0 bytes from fileno {}.'.format(self.fileno())\n            self.torrent.kill_peer(self)\n        self.process_input(bytes)"}
{"prompt": "def get(self, timeout=None):\n        \"\"\"\n        Waits for a job to finish (max of given timeout seconds) and return job results. When a job exits get() will\n        keep returning the same result until zero-os doesn't remember the job anymore (self.exists == False)\n\n        :notes: the timeout here is a client side timeout, it's different than the timeout given to the job on start\n        (like in system method) witch will cause the job to be killed if it exceeded this timeout.\n\n        :param timeout: max time to wait for the job to finish in seconds\n        :return: Return object\n        \"\"\"", "output": ": the job results\n        \"\"\"\n        if timeout is None:\n            timeout = self.timeout\n        if timeout > 0:\n            end_time = time.time() + timeout\n        else:\n            end_time = None\n\n        if self.exists:\n            return self.get_result()\n        else:\n            return self.get_result() + end_time\n\n\nclass Job(object):\n    \"\"\"\n    Represents a single job.\n    \"\"\"\n\n    def __init__(self, command, name, timeout=None):\n        \"\"\"\n        :param command: the command to run\n        :param name: the name of the job\n        :", "solution": "\n        if timeout is None:\n            timeout = self._client.timeout\n        r = self._client._redis\n        start = time.time()\n        maxwait = timeout\n        while maxwait > 0:\n            if not self.exists:\n                raise JobNotFoundError(self.id)\n            v = r.brpoplpush(self._queue, self._queue, min(maxwait, 10))\n            if v is not None:\n                payload = json.loads(v.decode())\n                r = Return(payload)\n                logger.debug('%s << %s, stdout=\"%s\", stderr=\"%s\", data=\"%s\"',\n                             self._id, r.state, r.stdout, r.stderr, r.data[:1000])\n                return r\n            logger.debug('%s still waiting (%ss)', self._id, int(time.time() - start))\n            maxwait -= 10\n        raise TimeoutError()"}
{"prompt": "def _control_resp(self, data, state):\n        \"\"\" Handle a control response.\n\n        :param data: Payload.\n        :param state: Requested state.\n        :returns: Acknowledged state.\n        \"\"\"", "output": "\n        if data.get('status') == 'ok':\n            self.log.info('Received control request: %s', data)\n            state = self.get_acknowledgement(data)\n            if state is None:\n                return False\n            else:\n                self.log.debug('Got state: %s', state)\n                return state\n        else:\n            self.log.error('Received control response: %s', data)\n            return False\n\n    def _get_state(self, data):\n        \"\"\" Handle a state request.\n\n        :param data: Payload.\n        :returns: State.\n        \"\"\"\n       ", "solution": "\n        if _is_control_response(data):\n            ack_state = bytes([data[22]])\n            if state == ack_state:\n                _LOGGER.debug(\"Received state ack from %s, state: %s\",\n                              self.host, ord(ack_state))\n                return ack_state"}
{"prompt": "def is_path_sibling_creatable(pathname: str) -> bool:\n    \"\"\"Checks whether current user can create siblings of the given path.\n\n    Returns\n    -------\n    `True` if the current user has sufficient permissions to create siblings\n    (i.e., arbitrary files in the parent directory) of the passed pathname;\n    `False` otherwise.\n    \"\"\"", "output": "\n    try:\n        st = os.stat(pathname)\n    except OSError:\n        return False\n    if st.st_mode & 0o777!= 0o700:\n        return False\n    if st.st_mode & 0o777!= st.st_mode & 0o777:\n        return False\n    return True\n\n\ndef is_path_writable(path: str) -> bool:\n    \"\"\"Checks whether the given path can be written to.\n\n    Parameters\n    ----------\n    path : str\n        The path to check.\n\n    Returns\n    -------\n    `True` if the given path can be written to, `False`", "solution": "\n    # Parent directory of the passed path. If empty, we substitute the current\n    # working directory (CWD) instead.\n    dirname = os.path.dirname(pathname) or os.getcwd()\n\n    try:\n        # For safety, explicitly close and hence delete this temporary file\n        # immediately after creating it in the passed path's parent directory.\n        with tempfile.TemporaryFile(dir=dirname):\n            pass\n        return True\n    # While the exact type of exception raised by the above function depends on\n    # the current version of the Python interpreter, all such types subclass\n    # the following exception superclass.\n    except EnvironmentError:\n        return False"}
{"prompt": "def unicode_value(self, string):\n        \"\"\"\n        String argument must be in unicode format.\n        \"\"\"", "output": "\n        raise NotImplementedError()\n\n    def get_prep_value(self, value):\n        \"\"\"\n        Argument must be an object that is a valid, possibly Unicode string.\n        \"\"\"\n        raise NotImplementedError()\n\n    def formfield(self, **kwargs):\n        \"\"\"\n        Set up the widget for rendering the widget.\n        \"\"\"\n        defaults = {'widget': unicode_widget}\n        defaults.update(kwargs)\n        return super(MultiWidget, self).formfield(**defaults)\n\n    def get_db_prep_lookup(self, lookup_type, value):\n        \"\"\"\n        Returns a lookup type that can be used to look up a widget for\n       ", "solution": "\n        result = 0\n        # don't accept strings that contain numbers\n        if self.regex_has_numbers.search(string):\n            raise AbnumException(error_msg % string)\n        else:\n            num_str = self.regex_values.sub(lambda x: '%s ' % self.values[x.group()],\n                                            string)\n            # don't accept strings, that contains letters which haven't been be converted to numbers\n            try:\n                result = sum([int(i) for i in num_str.split()])\n            except Exception as e:\n                raise AbnumException(error_msg % string)\n        return result"}
{"prompt": "def teardown_handler(teardown_fixtures_fn, teardown_fn):\n        \"\"\"Returns a function that adds fixtures handling to the teardown method.\n\n        Calls the given teardown method first before calling the fixtures teardown.\n        \"\"\"", "output": "\n        def teardown_fixture(self, fixture):\n            \"\"\"A fixture which will be cleaned up automatically.\"\"\"\n            self.addCleanup(fixture.close)\n            self.addCleanup(fixture.close)\n            self.fixture = fixture\n\n        def cleanup_fixtures_fn(self, fixture):\n            \"\"\"A fixture which will be cleaned up automatically.\"\"\"\n            self.addCleanup(fixture.close)\n            self.addCleanup(fixture.close)\n            self.fixture = fixture\n\n        def _inner_teardown(self):\n            \"\"\"A fixture which will be cleaned up automatically.\"\"\"\n            self.addCleanup(fixture.close)\n            self.addCleanup(fixture.", "solution": "\n        def handler(obj):\n            teardown_fn(obj)\n            teardown_fixtures_fn(obj)\n        return handler"}
{"prompt": "def iterscrapers(self, method, mode = None):\n\t\t\"\"\" Iterates over all available scrapers. \"\"\"", "output": "\n\t\tif mode is None:\n\t\t\tmode = self.scraper_mode\n\t\tfor scraper in self.scrapers:\n\t\t\tif scraper.method == method:\n\t\t\t\tyield scraper\n\t\t\telif mode == scraper_mode:\n\t\t\t\tyield scraper_func(scraper)\n\n\tdef get_scraper_by_id(self, id):\n\t\t\"\"\" Returns a scraper object for the given scraper id. \"\"\"\n\t\tfor scraper in self.iterscrapers(id):\n\t\t\tif scraper.id == id:\n\t\t\t\treturn scraper\n\t\treturn None\n\n\tdef get_scraper_by", "solution": "\n\n\t\tglobal discovered\n\t\tif discovered.has_key(self.language) and discovered[self.language].has_key(method):\n\t\t\tfor Scraper in discovered[self.language][method]:\n\t\t\t\tyield Scraper"}
{"prompt": "def signal_list_names(type_):\n    \"\"\"Returns a list of signal names for the given type\n\n    :param type\\\\_:\n    :type type\\\\_: :obj:`GObject.GType`\n    :returns: A list of signal names\n    :rtype: :obj:`list`\n    \"\"\"", "output": "\n    return [GObject.SignalFlags.RUN_LAST,\n            GObject.SignalFlags.RUN_LAST |\n            GObject.SignalFlags.ACTION_RUN_LAST]\n\n\ndef signal_list_ids(type_):\n    \"\"\"Returns a list of signal ids for the given type\n\n    :param type\\\\_:\n    :type type\\\\_: :obj:`GObject.GType`\n    :returns: A list of signal ids\n    :rtype: :obj:`list`\n    \"\"\"\n    return [GObject.SignalFlags.RUN_LAST |\n            GObject.SignalFlags.RUN_LAST |\n            GObject", "solution": "\n\n    ids = signal_list_ids(type_)\n    return tuple(GObjectModule.signal_name(i) for i in ids)"}
{"prompt": "def select(dataspec, testsuite, mode='list', cast=True):\n    \"\"\"\n    Select data from [incr tsdb()] profiles.\n\n    Args:\n        query (str): TSQL select query (e.g., `'i-id i-input mrs'` or\n            `'* from item where readings > 0'`)\n        testsuite (str, TestSuite): testsuite or path to testsuite\n            containing data to select\n        mode (str): see :func:`delphin.itsdb.select_rows` for a\n            description of the *mode* parameter (default: `list`)\n        cast (bool): if `True`, cast column values to their datatype\n            according to the relations file (default: `True`)\n    Returns:\n        a generator that yields selected data\n    \"\"\"", "output": "\n        data (int): number of rows to select.\n            *default* is ``0``\n        dataspec (str): path to the *data* database.\n            *default* is ``/home/user/test_data.db``\n        testsuite (str, str): name of the test suite.\n            *default* is ``None``\n        test_id (str): id of the test to select.\n            *default* is ``None``\n        mrs (str): path to the *MRS* database.\n            *default* is ``None``\n\n    Returns:\n        int: error code (0:", "solution": "\n    if isinstance(testsuite, itsdb.ItsdbProfile):\n        testsuite = itsdb.TestSuite(testsuite.root)\n    elif not isinstance(testsuite, itsdb.TestSuite):\n        testsuite = itsdb.TestSuite(testsuite)\n    return tsql.select(dataspec, testsuite, mode=mode, cast=cast)"}
{"prompt": "def suites(self, request, pk=None):\n        \"\"\"\n        List of test suite names available in this project\n        \"\"\"", "output": "\n        if pk:\n            return self.get_suite_names(request, pk)\n        else:\n            return self.get_suite_names(request)\n\n    def get_suite_names(self, request):\n        \"\"\"\n        List of test suite names available in this project\n        \"\"\"\n        return [\n            self.get_test_suite_name(request, test_suite)\n            for test_suite in self.suites(request)\n        ]\n\n    def get_test_suite_name(self, request, test_suite):\n        \"\"\"\n        Given a test suite name, return the test suite name\n        \"\"\"\n       ", "solution": "\n        suites_names = self.get_object().suites.values_list('slug')\n        suites_metadata = SuiteMetadata.objects.filter(kind='suite', suite__in=suites_names)\n        page = self.paginate_queryset(suites_metadata)\n        serializer = SuiteMetadataSerializer(page, many=True, context={'request': request})\n        return self.get_paginated_response(serializer.data)"}
{"prompt": "def nation(self, nation_name, password=None, autologin=None):\n        \"\"\"Setup access to the Nation API with the Nation object\n\n            :param nation_name: Name of the nation\n            :param password: (Optional) password for this nation\n            :param autologin (Optional) autologin for this nation\n            :type nation_name: str\n            :type password: str\n            :type autologin: str\n            :returns: Nation Object based off nation_name\n            :rtype: Nation\n        \"\"\"", "output": "\n        return Nation(self, nation_name, autologin, password)\n\n    def get_nation_by_name(self, name):\n        \"\"\"Get the Nation object by name\n\n            :param name: (String) name of the nation\n            :type name: str\n            :returns: Nation Object\n            :rtype: Nation\n        \"\"\"\n        return self.nation(name, autologin=True)\n\n    def get_nation_by_email(self, email):\n        \"\"\"Get the Nation object by email\n\n            :param email: (String) email of the nation\n            :", "solution": "\n        return Nation(nation_name, self, password=password, autologin=autologin)"}
{"prompt": "def listFieldsFromSource(self, template_source):\n        \"\"\"List all the attributes to be rendered directly from template\n        source\n\n        :param template_source: the template source (usually represents the\n            template content in string format)\n        :return: a :class:`set` contains all the needed attributes\n        :rtype: set\n        \"\"\"", "output": "\n        return set(self.attr_dict.keys())\n\n    def get_field_names(self, template_source):\n        \"\"\"Return the list of attributes to be rendered directly from template\n        source\n\n        :param template_source: the template source (usually represents the\n            template content in string format)\n        :return: a list of attribute names\n        :rtype: list\n        \"\"\"\n        return list(self.attr_dict.keys())\n\n    def get_field_value(self, template_name, field_name):\n        \"\"\"Return the value of a given attribute of a given type\n\n        :param template_name:", "solution": "\n\n        ast = self.environment.parse(template_source)\n        return jinja2.meta.find_undeclared_variables(ast)"}
{"prompt": "def run(self, args=None):\n        \"\"\" The name IS required here. \"\"\"", "output": "\n        if args is None:\n            args = []\n        if not self.args:\n            self.args = args\n        if not self.kwargs:\n            self.kwargs = {}\n        if not self.vars:\n            self.vars = {}\n        if not self.file:\n            self.file = None\n        if not self.args:\n            self.args = []\n        if not self.kwargs:\n            self.kwargs = {}\n        if not self.vars:\n            self.vars = {}\n        if not self.file:\n            self.file = self.args[0]\n        if not self.file:\n            self", "solution": "\n        print(Colors.BLUE+\"[INFO] Unlinking {}...\".format(\n              args.names)+Colors.RESET)\n        for name in args.names:\n            self.unlink(Link.package_dir, name)"}
{"prompt": "def empirical_SVD(stream_list, linear=True):\n    \"\"\"\n    Depreciated. Use empirical_svd.\n    \"\"\"", "output": "\n    if linear:\n        return empirical_SVD_linear(stream_list)\n    else:\n        return empirical_SVD_svd(stream_list)\n\n\ndef empirical_SVD_linear(stream_list, n_components=2):\n    \"\"\"\n    Linear function for the empirical SVD.\n    \"\"\"\n    n_components = n_components + 1\n    if len(stream_list) < n_components:\n        return np.array([])\n    if len(stream_list) == 1:\n        return np.array([1.])\n    if len(stream_", "solution": "\n    warnings.warn('Depreciated, use empirical_svd instead.')\n    return empirical_svd(stream_list=stream_list, linear=linear)"}
{"prompt": "def _sanitize(cls, message):\n        \"\"\"\n        Sanitize the given message,\n        dealing with multiple arguments\n        and/or string formatting.\n\n        :param message: the log message to be sanitized\n        :type  message: string or list of strings\n        :rtype: string\n        \"\"\"", "output": "\n        if isinstance(message, list):\n            message = \" \".join(message)\n        return message\n\n    @classmethod\n    def _get_log_level(cls, level):\n        \"\"\"\n        Get the log level.\n\n        :param level: the log level to be used\n        :type  level: integer\n        :rtype: integer\n        \"\"\"\n        if level in (0, 1):\n            return logging.ERROR\n        elif level in (2, 3):\n            return logging.WARNING\n        elif level in (3, 4):\n            return logging.INFO\n        elif level in (5, 6):\n            return logging.DEBUG\n        elif level", "solution": "\n        if isinstance(message, list):\n            if len(message) == 0:\n                sanitized = u\"Empty log message\"\n            elif len(message) == 1:\n                sanitized = message[0]\n            else:\n                sanitized = message[0] % tuple(message[1:])\n        else:\n            sanitized = message\n        if not gf.is_unicode(sanitized):\n            raise TypeError(\"The given log message is not a Unicode string\")\n        return sanitized"}
{"prompt": "def read_texture(filename, attrs=None):\n    \"\"\"Loads a ``vtkTexture`` from an image file.\"\"\"", "output": "\n    if attrs is None:\n        attrs = {}\n    if filename.endswith('.obj'):\n        obj = read_obj(filename)\n    elif filename.endswith('.obj.type'):\n        obj = read_obj(filename)\n    elif filename.endswith('.obj.name'):\n        obj = read_texture(filename, attrs)\n    elif filename.endswith('.obj.name.ext'):\n        obj = read_texture(filename, attrs)\n    elif filename.endswith('.obj.name.ext'):\n        obj = read_texture(filename, attrs)\n    elif filename.endswith('.obj.name.ext'):\n        obj =", "solution": "\n    filename = os.path.abspath(os.path.expanduser(filename))\n    try:\n        # intitialize the reader using the extnesion to find it\n        reader = get_reader(filename)\n        image = standard_reader_routine(reader, filename, attrs=attrs)\n        return vtki.image_to_texture(image)\n    except KeyError:\n        # Otherwise, use the imageio reader\n        pass\n    return vtki.numpy_to_texture(imageio.imread(filename))"}
{"prompt": "def run(self):\n        \"\"\"Start the FSM.\n\n        Returns:\n            boolean: True if FSM reaches the last state or false if the exception or error message was raised\n\n        \"\"\"", "output": "\n        try:\n            self.fsm.poll()\n        except KeyboardInterrupt:\n            self.fsm.stop()\n            return False\n        except Exception as e:\n            self.fsm.error(e)\n            return False\n\n        self.fsm.poll()\n        return True\n\n    def get_fsm_state(self):\n        \"\"\"Get the current FSM state.\n\n        Returns:\n            FSMState: Current FSM state\n\n        \"\"\"\n        try:\n            return self.fsm.get_fsm_state()\n        except Exception as e:\n            self.fsm.error(e)\n            return FSMState.UNKNOWN\n\n    def get_fsm_", "solution": "\n        ctx = FSM.Context(self.name, self.device)\n        transition_counter = 0\n        timeout = self.timeout\n        self.log(\"{} Start\".format(self.name))\n        while transition_counter < self.max_transitions:\n            transition_counter += 1\n            try:\n                start_time = time()\n                if self.init_pattern is None:\n                    ctx.event = self.ctrl.expect(self.events, searchwindowsize=self.searchwindowsize, timeout=timeout)\n                else:\n                    self.log(\"INIT_PATTERN={}\".format(pattern_to_str(self.init_pattern)))\n                    try:\n                        ctx.event = self.events.index(self.init_pattern)\n                    except ValueError:\n                        self.log(\"INIT_PATTERN unknown.\")\n                        continue\n                    finally:\n                        self.init_pattern = None\n\n                finish_time = time() - start_time\n                key = (ctx.event, ctx.state)\n                ctx.pattern = self.events[ctx.event]\n\n                if key in self.transition_table:\n                    transition = self.transition_table[key]\n                    next_state, action_instance, next_timeout = transition\n                    self.log(\"E={},S={},T={},RT={:.2f}\".format(ctx.event, ctx.state, timeout, finish_time))\n                    if callable(action_instance) and not isclass(action_instance):\n                        if not action_instance(ctx):\n                            self.log(\"Error: {}\".format(ctx.msg))\n                            return False\n                    elif isinstance(action_instance, Exception):\n                        self.log(\"A=Exception {}\".format(action_instance))\n                        raise action_instance\n                    elif action_instance is None:\n                        self.log(\"A=None\")\n                    else:\n                        self.log(\"FSM Action is not callable: {}\".format(str(action_instance)))\n                        raise RuntimeWarning(\"FSM Action is not callable\")\n\n                    if next_timeout != 0:  # no change if set to 0\n                        timeout = next_timeout\n                    ctx.state = next_state\n                    self.log(\"NS={},NT={}\".format(next_state, timeout))\n\n                else:\n                    self.log(\"Unknown transition: EVENT={},STATE={}\".format(ctx.event, ctx.state))\n                    continue\n\n            except EOF:\n                raise ConnectionError(\"Session closed unexpectedly\", self.ctrl.hostname)\n\n            if ctx.finished or next_state == -1:\n                self.log(\"{} Stop at E={},S={}\".format(self.name, ctx.event, ctx.state))\n                return True\n\n        # check while else if even exists\n        self.log(\"FSM looped. Exiting\")\n        return False"}
{"prompt": "def _calldata_fields(fields, exclude_fields, format_ids):\n    \"\"\"Utility function to determine which calldata (i.e., FORMAT) fields to\n    extract.\"\"\"", "output": "\n    calldata = {}\n    for field in fields:\n        if field[0] in calldata:\n            calldata[field[0]].append(field[1])\n        else:\n            calldata[field[0]] = [field[1]]\n    return calldata\n\n\ndef _get_calldata_fields(calldata, format_ids):\n    \"\"\"Utility function to determine which calldata (i.e., FORMAT) fields to\n    extract.\"\"\"\n    fields = []\n    for call in calldata[call]:\n        if call[0] in format_ids:\n            fields.append((call[1", "solution": "\n    if fields is None:\n        # no fields specified by user\n        # default to all standard fields plus all FORMAT fields in VCF header\n        fields = config.STANDARD_CALLDATA_FIELDS + format_ids\n    else:\n        # fields specified by user\n        for f in fields:\n            # check if field is standard or defined in VCF header\n            if (f not in config.STANDARD_CALLDATA_FIELDS and\n                    f not in format_ids):\n                # support extracting FORMAT even if not declared in header,\n                # but warn...\n                print('WARNING: no definition found for field %s' % f,\n                      file=sys.stderr)\n    # process exclusions\n    if exclude_fields is not None:\n        fields = [f for f in fields if f not in exclude_fields]\n    return tuple(fields)"}
{"prompt": "def parent(self, resource):\n        \"\"\"Set parent resource\n\n        :param resource: parent resource\n        :type resource: Resource\n\n        :raises ResourceNotFound: resource not found on the API\n        \"\"\"", "output": "\n        self._set_parent(resource, self.parent_id)\n\n    def _set_parent(self, resource, parent_id):\n        \"\"\"Set parent resource\n\n        :param resource: parent resource\n        :type resource: Resource\n\n        :raises ResourceNotFound: resource not found on the API\n        \"\"\"\n        self.parent_id = parent_id\n        self.resource_id = resource.id\n\n    def get_parent(self):\n        \"\"\"Get parent resource\n\n        :returns: parent resource\n        :rtype: Resource\n\n        :raises ResourceNotFoundError: resource not found on the API\n        \"\"\"\n        try:\n            return self.parent_", "solution": "\n        resource.check()\n        self['parent_type'] = resource.type\n        self['parent_uuid'] = resource.uuid"}
{"prompt": "def make_command_table(entry_points):\n    \"\"\"\n    Return a nicely formatted table of all the PIP commands installed on this \n    system to incorporate into the help text.  The table will have two columns.  \n    The first will list the commands that comprise the main pipeline and the \n    second will list all the other miscellaneous helper functions.\n    \"\"\"", "output": "\n    # The following is a list of all the functions that are called by the pipeline.\n    # These are called by the pipeline when the user clicks on the pipeline.\n    # They are called by the helper functions when the pipeline is running.\n    # They are called by the helper functions when the helper functions are called.\n    #\n    # The second will list the functions that are called by the miscellaneous pipeline.\n    # These are called by the pipeline when the user clicks on the pipeline.\n    #\n    # The third will list the functions that are called by the pipeline.\n    # These are called by the pipeline when", "solution": "\n    import itertools\n\n    # Split every command installed on the system into two categories: those \n    # that are part of the main pipeline and those that are just utilities or \n    # helpers.  Pipeline scripts start with numbers, helper scripts don't.\n\n    pipeline_commands = []\n    helper_commands = []\n\n    for command in sorted(entry_points):\n        if re.match('\\d+_', command):\n            pipeline_commands.append(command)\n        else:\n            helper_commands.append(command)\n\n    # Figure out what the longest pipeline command is, so we know how much to \n    # indent the helper commands.\n\n    col_width = 0\n    for command in pipeline_commands:\n        col_width = max(len(command), col_width)\n\n    # Make the table.\n\n    rows = []\n    columns = itertools.izip_longest(\n            pipeline_commands, helper_commands, fillvalue='')\n\n    for commands in columns:\n        row = '        {0[0]:{1}}   {0[1]}'.format(commands, col_width)\n        rows.append(row)\n\n    return '\\n'.join(rows)"}
{"prompt": "def shell(self):\n        \"\"\"\n        Spawn a debug shell.\n        \"\"\"", "output": "\n        self.debug_shell = DebugShell(self.config)\n        self.debug_shell.start()\n\n    def debug_shell_thread(self):\n        \"\"\"\n        Spawn a debug thread.\n        \"\"\"\n        self.debug_thread = DebugThread(self.config)\n        self.debug_thread.start()\n\n    def debug_shell_thread_thread(self):\n        \"\"\"\n        Spawn a debug thread thread.\n        \"\"\"\n        self.debug_thread_thread = DebugThreadThreadThread(self.config)\n        self.debug_thread_thread.start()\n\n    def debug_shell_thread_", "solution": "\n        click.echo(click.style(\"NOTICE!\", fg=\"yellow\", bold=True) + \" This is a \" + click.style(\"local\", fg=\"green\", bold=True) + \" shell, inside a \" + click.style(\"Zappa\", bold=True) + \" object!\")\n        self.zappa.shell()\n        return"}
{"prompt": "def overall_f_measure(self):\n        \"\"\"Overall f-measure metrics (f_measure, precision, and recall)\n\n        Returns\n        -------\n        dict\n            results in a dictionary format\n\n        \"\"\"", "output": "\n        if self.f_measure is None:\n            return {}\n        return self.f_measure\n\n    def __repr__(self):\n        return self.name\n\n    def __str__(self):\n        return self.name\n\n    def __init__(self, name, score, description, score_mode,\n                 score_weighting, n_features, n_classes, n_clusters,\n                 init, verbose):\n        self.name = name\n        self.score = score\n        self.description = description\n        self.score_mode = score_mode\n        self.score_weighting = score_weighting\n        self.init", "solution": "\n        if self.overall['Nsys'] == 0 and self.empty_system_output_handling == 'zero_score':\n            precision = 0\n\n        else:\n            precision = metric.precision(\n                Ntp=self.overall['Ntp'],\n                Nsys=self.overall['Nsys']\n            )\n\n        recall = metric.recall(\n            Ntp=self.overall['Ntp'],\n            Nref=self.overall['Nref']\n        )\n\n        f_measure = metric.f_measure(\n            precision=precision,\n            recall=recall\n        )\n\n        return {\n            'f_measure': f_measure,\n            'precision': precision,\n            'recall': recall\n        }"}
{"prompt": "def readehf(filename):\n    \"\"\"Read EDF header (ESRF data format, as of beamline ID01 and ID02)\n\n    Input\n    -----\n    filename: string\n        the file name to load\n\n    Output\n    ------\n    the EDF header structure in a dictionary\n    \"\"\"", "output": "\n    with open(filename, 'rb') as f:\n        return pickle.load(f)\n\n\ndef readelf(filename):\n    \"\"\"Read EDF footer (LFH data format, as of beamline ID01 and ID02)\n\n    Input\n    -----\n    filename: string\n        the file name to load\n\n    Output\n    ------\n    the dictionary in a dictionary\n    \"\"\"\n    with open(filename, 'rb') as f:\n        return pickle.load(f)\n\n\ndef readehf(filename):\n    \"\"\"Read EDF header (LFH data format, as of beamline ID01 and ID02)", "solution": "\n    f = open(filename, 'r')\n    edf = {}\n    if not f.readline().strip().startswith('{'):\n        raise ValueError('Invalid file format.')\n    for l in f:\n        l = l.strip()\n        if not l:\n            continue\n        if l.endswith('}'):\n            break  # last line of header\n        try:\n            left, right = l.split('=', 1)\n        except ValueError:\n            raise ValueError('Invalid line: ' + l)\n        left = left.strip()\n        right = right.strip()\n        if not right.endswith(';'):\n            raise ValueError(\n                'Invalid line (does not end with a semicolon): ' + l)\n        right = right[:-1].strip()\n        m = re.match('^(?P<left>.*)~(?P<continuation>\\d+)$', left)\n        if m is not None:\n            edf[m.group('left')] = edf[m.group('left')] + right\n        else:\n            edf[left] = _readedf_extractline(left, right)\n    f.close()\n    edf['FileName'] = filename\n    edf['__Origin__'] = 'EDF ID02'\n    edf['__particle__'] = 'photon'\n    return edf"}
{"prompt": "def unregister_namespace(self, namespace):\n        \"\"\"\n        Unregister a namespace.\n\n        :param namespace: Namespace tag.\n        :type namespace: str\n        \"\"\"", "output": "\n        self.xml.remove(namespace)\n\n    def get_namespace_name(self):\n        \"\"\"\n        Get the name of the namespace.\n\n        :return: Name of the namespace.\n        :rtype: str\n        \"\"\"\n        return self.xml.tag\n\n    def get_namespace_uri(self):\n        \"\"\"\n        Get the URI of the namespace.\n\n        :return: URI of the namespace.\n        :rtype: str\n        \"\"\"\n        return self.xml.get('uri')\n\n    def get_namespace_version(self):\n        \"\"\"\n        Get the version of the namespace.\n\n        :return: Version of the namespace.\n       ", "solution": "\n        if namespace not in self._namespaces:\n            raise NoRegisteredError(\"Namespace '{0}' is not registered on loader.\".format(namespace))\n\n        del self._namespaces[namespace]"}
{"prompt": "def import_field(self, field, obj, data, is_m2m=False):\n        \"\"\"\n        Calls :meth:`import_export.fields.Field.save` if ``Field.attribute``\n        and ``Field.column_name`` are found in ``data``.\n        \"\"\"", "output": "\n        return self.field_export(obj, field, data, is_m2m)\n\n    def field_export(self, obj, field, data, is_m2m=False):\n        \"\"\"\n        Exports a single field's data to a separate table.\n        \"\"\"\n        raise NotImplementedError('subclasses of BaseExportBackend must provide a _field_export() method')\n\n    def get_field_names(self, model):\n        \"\"\"\n        Returns a list of all field names in the given model, including\n        the model name and the serialization format, if any.\n        \"\"\"\n        raise NotImplementedError('subclasses of BaseExport", "solution": "\n        if field.attribute and field.column_name in data:\n            field.save(obj, data, is_m2m)"}
{"prompt": "def child_moved(self, child):\n        \"\"\" Handle the child moved event from the declaration.\n\n        \"\"\"", "output": "\n        if self.is_move_event(child):\n            self.is_move_event(child)\n            self.emit(\"move-event\", child)\n        else:\n            self.emit(\"move-event\", child)\n\n    def child_removed(self, child):\n        \"\"\" Handle the child removed event from the declaration.\n\n        \"\"\"\n        if self.is_move_event(child):\n            self.is_move_event(child)\n            self.emit(\"move-event\", child)\n        else:\n            self.emit(\"move-event\", child)\n\n    def get_child_cursor(self", "solution": "\n        super(AndroidViewGroup, self).child_moved(child)\n        #: Remove and re-add in correct spot\n        self.child_removed(child)\n        self.child_added(child)"}
{"prompt": "def base36(value):\n    \"\"\"\n    Encode int to base 36.\n    \"\"\"", "output": "\n    return value if value is not None and value!= \"\" else \"0\"\n\n\ndef base36_to_int(value):\n    \"\"\"\n    Encode base 36 to int.\n    \"\"\"\n    return int(value) if value is not None and int(value)!= \"\" else 0\n\n\ndef base36_to_int_list(value):\n    \"\"\"\n    Converts a list of integers into a list of integers.\n    \"\"\"\n    return [int(x) for x in value.split()]\n\n\ndef base36_to_int_dict(value):\n    \"\"\"\n    Converts a dict into a list of integers", "solution": "\n    result = ''\n    while value:\n        value, i = divmod(value, 36)\n        result = BASE36_ALPHABET[i] + result\n    return result"}
{"prompt": "def translate(self, addr):\n        \"\"\"\n        Reverse DNS the public broadcast_address, then lookup that hostname to get the AWS-resolved IP, which\n        will point to the private IP address within the same datacenter.\n        \"\"\"", "output": "\n        if not self.public_subnet_ids:\n            return None\n        addr = addr.split(\"/\")[0]\n        for subnet_id in self.public_subnet_ids:\n            if subnet_id == addr:\n                return subnet_id\n        return None\n\n    def _get_subnet_ids(self):\n        \"\"\"\n        Return the list of subnets.\n        \"\"\"\n        return self.public_subnets.keys()\n\n    def _get_subnet_name(self, subnet_id):\n        \"\"\"\n        Return the name of the subnet.\n        \"\"\"\n        return self.public_subnets[subnet_id].name\n\n    def", "solution": "\n        # get family of this address so we translate to the same\n        family = socket.getaddrinfo(addr, 0, socket.AF_UNSPEC, socket.SOCK_STREAM)[0][0]\n        host = socket.getfqdn(addr)\n        for a in socket.getaddrinfo(host, 0, family, socket.SOCK_STREAM):\n            try:\n                return a[4][0]\n            except Exception:\n                pass\n        return addr"}
{"prompt": "def native_obj(self):\n        \"\"\"Native storage object.\"\"\"", "output": "\n        return self.get_object(self.name)\n\n    def get_object(self, name):\n        \"\"\"Get a StorageObject instance.\"\"\"\n        if name == self.name:\n            return self.native_obj()\n        if name == 'all':\n            return self.get_all_storage_objects()\n        if name == 'all_files':\n            return self.get_all_files_storage_objects()\n        if name == 'all_folders':\n            return self.get_all_folders_storage_objects()\n        if name == 'all_subfolders':\n            return self.get_all", "solution": "\n        if self.__native is None:\n            self.__native = self._get_object()\n\n        return self.__native"}
{"prompt": "def _get_description(string):\n    \"\"\"\n    Parse annotation to get nice description\n    \"\"\"", "output": "\n    if string.startswith('http://'):\n        return string\n    else:\n        return 'Unknown'\n\n\ndef _get_annotation_url(url):\n    \"\"\"\n    Return the URL of the annotation\n    \"\"\"\n    return url.replace('http://', 'https://')\n\n\ndef _get_annotation_id(url):\n    \"\"\"\n    Return the ID of the annotation\n    \"\"\"\n    return int(re.search(r'annotation_id=(\\d+)', url).group(1))\n\n\ndef _get_annotation_type(string):\n    \"\"\"\n    Parse annotation to get type\n    \"\"\"\n    if string.", "solution": "\n    ann = set()\n    if not string:\n        return \"This cluster is inter-genic.\"\n    for item in string:\n        for db in item:\n            ann = ann.union(set(item[db]))\n    return \"annotated as: %s ...\" % \",\".join(list(ann)[:3])"}
{"prompt": "def _choose_unit(value, unit=None, asciimode=None):\n    \"\"\"\n    Finds a good unit to print seconds in\n\n    Args:\n        value (float): measured value in seconds\n        unit (str): if specified, overrides heuristic decision\n        asciimode (bool): if True, forces ascii for microseconds\n\n    Returns:\n        tuple[(str, float)]: suffix, mag:\n            string suffix and conversion factor\n\n    Example:\n        >>> assert _choose_unit(1.1, unit=None)[0] == 's'\n        >>> assert _choose_unit(1e-2, unit=None)[0] == 'ms'\n        >>> assert _choose_unit(1e-4, unit=None, asciimode=True)[0] == 'us'\n        >>> assert _choose_unit(1.1, unit='ns')[0] == 'ns'\n    \"\"\"", "output": " =='s'\n        >>> assert _choose_unit(1e-3, unit=None)[0] =='s'\n        >>> assert _choose_unit(1e-6, unit=None)[0] =='s'\n        >>> assert _choose_unit(1e-9, unit=None)[0] =='s'\n        >>> assert _choose_unit(1e-12, unit=None)[0] =='s'\n        >>> assert _choose_unit(1e-15, unit=None)[0] =='s'\n        >>> assert _choose_unit(1", "solution": "\n    from collections import OrderedDict\n    micro = _trychar('\u00b5s', 'us', asciimode)\n    units = OrderedDict([\n        ('s', ('s', 1e0)),\n        ('ms', ('ms', 1e-3)),\n        ('us', (micro, 1e-6)),\n        ('ns', ('ns', 1e-9)),\n    ])\n    if unit is None:\n        for suffix, mag in units.values():  # pragma: nobranch\n            if value > mag:\n                break\n    else:\n        suffix, mag = units[unit]\n    return suffix, mag"}
{"prompt": "def MakeDeployableBinary(self, template_path, output_path):\n    \"\"\"This will add the config to the client template and create a .deb.\"\"\"", "output": "\n    with open(template_path, 'r') as template_file:\n      template = template_file.read()\n    self.client_template.append(template)\n    self.client_template.append(\n        'deb %s %s' % (self.options.build_dir, output_path))\n\n  def MakeDeployableSources(self, options):\n    \"\"\"This will add the config to the client template and create a.cpio.\"\"\"\n    sources = []\n    for source in options.sources:\n      if source.endswith('.cpio'):\n        sources.append(source)\n    self.Make", "solution": "\n    buildpackage_binary = \"/usr/bin/dpkg-buildpackage\"\n    if not os.path.exists(buildpackage_binary):\n      logging.error(\"dpkg-buildpackage not found, unable to repack client.\")\n      return\n\n    with utils.TempDirectory() as tmp_dir:\n      template_dir = os.path.join(tmp_dir, \"dist\")\n      utils.EnsureDirExists(template_dir)\n\n      zf = zipfile.ZipFile(template_path)\n      for name in zf.namelist():\n        dirname = os.path.dirname(name)\n        utils.EnsureDirExists(os.path.join(template_dir, dirname))\n        with open(os.path.join(template_dir, name), \"wb\") as fd:\n          fd.write(zf.read(name))\n\n      # Generate the dpkg files.\n      self.GenerateDPKGFiles(tmp_dir)\n\n      # Create a client config.\n      client_context = [\"Client Context\"] + self.context\n      client_config_content = self.GetClientConfig(client_context)\n\n      # We need to strip leading /'s or .join will ignore everything that comes\n      # before it.\n      target_dir = config.CONFIG.Get(\n          \"ClientBuilder.target_dir\", context=self.context).lstrip(\"/\")\n      agent_dir = os.path.join(\n          template_dir, \"debian\",\n          config.CONFIG.Get(\"ClientBuilder.package_name\", context=self.context),\n          target_dir)\n\n      with open(\n          os.path.join(\n              agent_dir,\n              config.CONFIG.Get(\n                  \"ClientBuilder.config_filename\", context=self.context)),\n          \"wb\") as fd:\n        fd.write(client_config_content)\n\n      # Set the daemon to executable.\n      os.chmod(\n          os.path.join(\n              agent_dir,\n              config.CONFIG.Get(\"Client.binary_name\", context=self.context)),\n          0o755)\n\n      arch = config.CONFIG.Get(\"Template.arch\", context=self.context)\n\n      try:\n        old_working_dir = os.getcwd()\n      except OSError:\n        old_working_dir = os.environ.get(\"HOME\", \"/tmp\")\n\n      try:\n        os.chdir(template_dir)\n        command = [buildpackage_binary, \"-uc\", \"-d\", \"-b\", \"-a%s\" % arch]\n\n        try:\n          subprocess.check_output(command, stderr=subprocess.STDOUT)\n        except subprocess.CalledProcessError as e:\n          if \"Failed to sign\" not in e.output:\n            logging.error(\"Error calling %s.\", command)\n            logging.error(e.output)\n            raise\n\n        filename_base = config.CONFIG.Get(\n            \"ClientBuilder.debian_package_base\", context=self.context)\n        output_base = config.CONFIG.Get(\n            \"ClientRepacker.output_basename\", context=self.context)\n      finally:\n        try:\n          os.chdir(old_working_dir)\n        except OSError:\n          pass\n\n      utils.EnsureDirExists(os.path.dirname(output_path))\n\n      for extension in [\n          \".changes\",\n          config.CONFIG.Get(\n              \"ClientBuilder.output_extension\", context=self.context)\n      ]:\n        input_name = \"%s%s\" % (filename_base, extension)\n        output_name = \"%s%s\" % (output_base, extension)\n\n        # TODO(user):pytype: incorrect move() definition in typeshed.\n        # pytype: disable=wrong-arg-types\n        shutil.move(\n            os.path.join(tmp_dir, input_name),\n            os.path.join(os.path.dirname(output_path), output_name))\n        # pytype: enable=wrong-arg-types\n\n      logging.info(\"Created package %s\", output_path)\n      return output_path"}
{"prompt": "def set_pixel(framebuf, x, y, color):\n        \"\"\"Set a given pixel to a color.\"\"\"", "output": "\n        framebuf.set_pixel(x, y, color)\n        return self\n\n    def get_pixel(self, x, y):\n        \"\"\"Get a given pixel.\"\"\"\n        return self.framebuffer.get_pixel(x, y)\n\n    def set_pixel_color(self, color):\n        \"\"\"Set a given pixel to a given color.\"\"\"\n        self.framebuffer.set_pixel(x, y, color)\n        return self\n\n    def get_pixel_color(self, frame, x, y):\n        \"\"\"Get a given pixel.\"\"\"\n        return frame.get_pixel(x, y)", "solution": "\n        index = (y >> 3) * framebuf.stride + x\n        offset = y & 0x07\n        framebuf.buf[index] = (framebuf.buf[index] & ~(0x01 << offset)) | ((color != 0) << offset)"}
{"prompt": "def bin_data(self, fmt=None):\n        \"\"\"\n        Returns the unscaled raw values in the `value<N>` attributes as raw byte\n        array. Use `bin_data_format`, `num_values` and the individual sensor\n        documentation to determine how to interpret the data.\n\n        Use `fmt` to unpack the raw bytes into a struct.\n\n        Example::\n\n            >>> from ev3dev2.sensor.lego import InfraredSensor\n            >>> ir = InfraredSensor()\n            >>> ir.value()\n            28\n            >>> ir.bin_data('<b')\n            (28,)\n        \"\"\"", "output": "\n        if fmt is None:\n            return self.raw_values\n\n        if fmt == 'bin':\n            return self.bin_data_format(num_values=self.num_values)\n\n        if fmt == 'data':\n            return self.data_format(num_values=self.num_values)\n\n        return self.raw_values\n\n    def __str__(self):\n        \"\"\"\n        Returns the string representation of the sensor.\n\n        Example::\n\n            >>> ir = InfraredSensor()\n            >>> print(bin_data(ir.value()))\n            b'0'\n            b'1'\n            b'2", "solution": "\n\n        if self._bin_data_size == None:\n            self._bin_data_size = {\n                    \"u8\":     1,\n                    \"s8\":     1,\n                    \"u16\":    2,\n                    \"s16\":    2,\n                    \"s16_be\": 2,\n                    \"s32\":    4,\n                    \"float\":  4\n                }.get(self.bin_data_format, 1) * self.num_values\n\n        if None == self._bin_data:\n            self._bin_data = self._attribute_file_open( 'bin_data' )\n\n        self._bin_data.seek(0)\n        raw = bytearray(self._bin_data.read(self._bin_data_size))\n\n        if fmt is None: return raw\n\n        return unpack(fmt, raw)"}
{"prompt": "def get_simple_dirs(self, simple_dir: Path) -> List[Path]:\n        \"\"\"Return a list of simple index directories that should be searched\n        for package indexes when compiling the main index page.\"\"\"", "output": "\n        return [\n            Path(\n                os.path.join(simple_dir, \"packages\", \"lib\", \"python2.7\", \"site-packages\"),\n                [\n                    Path(\n                        os.path.join(\n                            simple_dir,\n                            \"lib\",\n                            \"python2.7\",\n                            \"site-packages\",\n                            \"simple_{}\".format(simple_dir.name),\n                            \"index.html\",\n                        ),\n                        Path(\n                            os.path.join(\n                                simple_dir,\n                                \"packages\",\n                                \"lib\",\n                                \"python2.7\",\n                                \"index_{}\".format(simple_", "solution": "\n        if self.hash_index:\n            # We are using index page directory hashing, so the directory\n            # format is /simple/f/foo/.  We want to return a list of dirs\n            # like \"simple/f\".\n            subdirs = [simple_dir / x for x in simple_dir.iterdir() if x.is_dir()]\n        else:\n            # This is the traditional layout of /simple/foo/.  We should\n            # return a single directory, \"simple\".\n            subdirs = [simple_dir]\n        return subdirs"}
{"prompt": "def nuclear_norm(data):\n    r\"\"\"Nuclear norm\n\n    This method computes the nuclear (or trace) norm of the input data.\n\n    Parameters\n    ----------\n    data : np.ndarray\n        Input data array\n\n    Returns\n    -------\n    float nuclear norm value\n\n    Examples\n    --------\n    >>> from modopt.math.matrix import nuclear_norm\n    >>> a = np.arange(9).reshape(3, 3)\n    >>> nuclear_norm(a)\n    15.49193338482967\n\n    Notes\n    -----\n    Implements the following equation:\n\n    .. math::\n        \\|\\mathbf{A}\\|_* = \\sum_{i=1}^{\\min\\{m,n\\}} \\sigma_i (\\mathbf{A})\n\n    \"\"\"", "output": " \\frac{1}{2} \\sum_{i=1}^{n} \\frac{1}{2} \\sum_{j=1}^{n} \\frac{1}{2} \\sum_{k=1}^{n} \\frac{1}{2} \\sum_{l=1}^{n} \\frac{1}{2} \\sum_{m=1}^{n}^{n} \\frac{1}{2} \\sum_{n=1}^{n} \\frac{1}{2} \\sum_{n=1}^{n}^{n} \\frac{", "solution": "\n\n    # Get SVD of the data.\n    u, s, v = np.linalg.svd(data)\n\n    # Return nuclear norm.\n    return np.sum(s)"}
{"prompt": "def admin_tools_render_dashboard(context, location='index', dashboard=None):\n    \"\"\"\n    Template tag that renders the dashboard, it takes two optional arguments:\n\n    ``location``\n        The location of the dashboard, it can be 'index' (for the admin index\n        dashboard) or 'app_index' (for the app index dashboard), the default\n        value is 'index'.\n\n    ``dashboard``\n        An instance of ``Dashboard``, if not given, the dashboard is retrieved\n        with the ``get_index_dashboard`` or ``get_app_index_dashboard``\n        functions, depending on the ``location`` argument.\n    \"\"\"", "output": "location`` parameter.\n\n    \"\"\"\n    if dashboard is None:\n        dashboard = get_default_dashboard(context)\n    if location is None:\n        location = get_default_location(context)\n    if not dashboard.location:\n        raise ValueError('The dashboard location is required.')\n    if not location:\n        raise ValueError('The dashboard location is required.')\n    if not dashboard.dashboard:\n        raise ValueError('The dashboard is retrieved with the default location.')\n    if not location:\n        raise ValueError('The dashboard location is required.')\n    if not dashboard.location:\n        raise ValueError('The dashboard is retrieved with the default location.')\n   ", "solution": "\n    if dashboard is None:\n        dashboard = get_dashboard(context, location)\n\n    dashboard.init_with_context(context)\n    dashboard._prepare_children()\n\n    try:\n        preferences = DashboardPreferences.objects.get(\n            user=context['request'].user,\n            dashboard_id=dashboard.get_id()\n        ).data\n    except DashboardPreferences.DoesNotExist:\n        preferences = '{}'\n        try:\n            DashboardPreferences(\n                user=context['request'].user,\n                dashboard_id=dashboard.get_id(),\n                data=preferences\n            ).save()\n        except IntegrityError:\n            # dashboard already was saved for that (user, dashboard)\n            pass\n\n    context.update({\n        'template': dashboard.template,\n        'dashboard': dashboard,\n        'dashboard_preferences': preferences,\n        'split_at': math.ceil(\n            float(len(dashboard.children)) / float(dashboard.columns)\n        ),\n        'has_disabled_modules': len(\n            [m for m in dashboard.children if not m.enabled]\n        ) > 0,\n        'admin_url': reverse('%s:index' % get_admin_site_name(context)),\n    })\n    return context"}
{"prompt": "def makeFigFromFile(filename,*args,**kwargs):\n    \"\"\"\n    Renders an image in a matplotlib figure, so it can be added to reports \n    args and kwargs are passed to plt.subplots\n    \"\"\"", "output": "\n    if not filename.endswith('.png'):\n        filename += '.png'\n    return plt.savefig(filename, *args,**kwargs)\n\ndef makeFig(image,filename,*args,**kwargs):\n    \"\"\"\n    Renders an image in a matplotlib figure, so it can be added to reports \n    args and kwargs are passed to plt.subplots\n    \"\"\"\n    if not image.endswith('.png'):\n        image += '.png'\n    return plt.savefig(filename, *args,**kwargs)\n\ndef makeFigFromArray(array,filename,*args,**kwargs):\n    \"\"\"\n    Renders an image in a", "solution": "\n    import matplotlib.pyplot as plt\n    img = plt.imread(filename)\n    fig,ax = plt.subplots(*args,**kwargs)\n    ax.axis('off')\n    ax.imshow(img)\n    return fig"}
{"prompt": "def get_conf_files(filename):\n    \"\"\"\n    Return :class:`list` of the all configuration files. *filename* is a\n    path of the main configuration file.\n\n    ::\n\n        >>> get_conf_files('exampleapp.conf')\n        ['exampleapp.conf', 'exampleapp.conf.d/10-database.conf']\n    \"\"\"", "output": "\n    if not os.path.isfile(filename):\n        return []\n    else:\n        return [filename] + sorted(os.listdir(os.path.join(\n            os.path.dirname(os.path.abspath(__file__)),\n            filename))\n\n\ndef get_db_files(filename):\n    \"\"\"\n    Return all configuration files for a given database. *filename* is a\n    path of the main database file.\n\n    ::\n\n        >>> get_db_files('exampleapp.db')\n        ['exampleapp.db']\n    \"\"\"\n    if not os.path.isfile(filename):\n        return []\n   ", "solution": "\n    if not os.path.isfile(filename):\n        raise ValueError(\"'%s' is not a file\" % filename)\n    conf_d_path = \"%s.d\" % filename\n    if not os.path.exists(conf_d_path):\n        return [filename]\n    return [filename] + get_conf_d_files(conf_d_path)"}
{"prompt": "def get_with_range(self, name, start, end):\n        r\"\"\"Get a color table from the registry with a corresponding norm.\n\n        Builds a `matplotlib.colors.BoundaryNorm` using `start`, `end`, and\n        the number of colors, based on the color table obtained from `name`.\n\n        Parameters\n        ----------\n        name : str\n            The name under which the color table will be stored\n        start : float\n            The starting boundary\n        end : float\n            The ending boundary\n\n        Returns\n        -------\n        `matplotlib.colors.BoundaryNorm`, `matplotlib.colors.ListedColormap`\n            The boundary norm based on `start` and `end` with the number of colors\n            from the number of entries matching the color table, and the color table itself.\n\n        \"\"\"", "output": " and the number of colors.\n\n        Examples\n        --------\n        >>> from matplotlib.colors import LogNorm\n        >>> log_norm = LogNorm()\n        >>> log_norm.set_under(0.5)\n        >>> log_norm.set_over(1)\n        >>> log_norm.set_under(0.5)\n        >>> log_norm.get_with_range('LogNorm', 0.5, log_norm.vmin, 0.5)\n        (0.5, 0.5)\n        \"\"\"\n        return self.get_color_table(name, start, end)\n\n    def get", "solution": "\n        from numpy import linspace\n\n        # Need one more boundary than color\n        num_steps = len(self[name]) + 1\n        boundaries = linspace(start, end, num_steps)\n        return self.get_with_boundaries(name, boundaries)"}
{"prompt": "def delete(self, *, auto_commit=False):\n        \"\"\"Removes a resource from the database\n\n        Args:\n            auto_commit (bool): Automatically commit the transaction. Default: `False`\n\n        Returns:\n            `None`\n        \"\"\"", "output": "\n        self.db_session.delete(self.resource_id)\n        if auto_commit:\n            self.db_session.commit()\n\n    def get_resource_by_id(self, resource_id):\n        \"\"\"Gets a resource from the database\n\n        Args:\n            resource_id (str): Resource id\n\n        Returns:\n            `Resource`\n        \"\"\"\n        return self.db_session.query(Resource).get(resource_id)\n\n    def get_resource_by_name(self, name):\n        \"\"\"Gets a resource from the database\n\n        Args:\n            name (str): Resource name\n\n        Returns:", "solution": "\n        try:\n            db.session.delete(self.resource)\n            if auto_commit:\n                db.session.commit()\n        except SQLAlchemyError:\n            self.log.exception('Failed deleting resource: {}'.format(self.id))\n            db.session.rollback()"}
{"prompt": "def deploy_file(file_path, bucket):\n    \"\"\" Uploads a file to an S3 bucket, as a public file. \"\"\"", "output": "\n    try:\n        import boto\n        from boto.s3.connection import S3Connection\n        from boto.s3.key import Key\n        from boto.s3.bucket import Bucket\n        from boto.s3.connection import OrdinaryCallingFormat\n        from boto.s3.connection import S3Connection\n    except ImportError:\n        raise AnsibleError(\"A non-Ansible inventory script or set of tools/metrics must be installed\")\n\n    # Set defaults\n    inventory_options = dict(type='memory',\n                           name=None,\n                           ram=None,\n                           disk=None,\n                           cores=None,\n                           ram=None,", "solution": "\n\n    # Paths look like:\n    #  index.html\n    #  css/bootstrap.min.css\n\n    logger.info(\"Deploying {0}\".format(file_path))\n\n    # Upload the actual file to file_path\n    k = Key(bucket)\n    k.key = file_path\n    try:\n        k.set_contents_from_filename(file_path)\n        k.set_acl('public-read')\n    except socket.error:\n        logger.warning(\"Caught socket.error while trying to upload {0}\".format(\n            file_path))\n        msg = \"Please file an issue with alotofeffort if you see this,\"\n        logger.warning(msg)\n        logger.warning(\"providing as much info as you can.\")"}
{"prompt": "def examples(manager: Manager):\n    \"\"\"Load examples to the database.\"\"\"", "output": "\n    return [\n        Example(\n            name=name,\n            description=description,\n            created_at=created_at,\n            updated_at=updated_at,\n            created_by_id=creator_id,\n            updated_by_id=creator_id,\n            created_at=created_at,\n            updated_at=updated_at,\n            updated_by_id=creator_id,\n            updated_by_name=creator_name,\n            updated_by_name=creator_name,\n            updated_by_id=creator_id,\n            updated_at=created_at,\n           ", "solution": "\n    for graph in (sialic_acid_graph, statin_graph, homology_graph, braf_graph, egf_graph):\n        if manager.has_name_version(graph.name, graph.version):\n            click.echo('already inserted {}'.format(graph))\n            continue\n        click.echo('inserting {}'.format(graph))\n        manager.insert_graph(graph, use_tqdm=True)"}
{"prompt": "def InsertFloatArg(self, string='', **unused_kwargs):\n    \"\"\"Inserts a Float argument.\"\"\"", "output": "\n    self.AddString(string)\n\n  def InsertIntArg(self, string='', **unused_kwargs):\n    \"\"\"Inserts an Integer argument.\"\"\"\n    self.AddString(string)\n\n  def InsertStringArg(self, string='', **unused_kwargs):\n    \"\"\"Inserts a string argument.\"\"\"\n    self.AddString(string)\n\n  def InsertFloat(self, string='', **unused_kwargs):\n    \"\"\"Inserts a float argument.\"\"\"\n    self.AddFloatArg(string, **unused_kwargs)\n\n  def InsertBoolArg(self, string='', **unused_kwargs):\n    \"\"\"Inserts a Boolean argument", "solution": "\n    try:\n      float_value = float(string)\n    except (TypeError, ValueError):\n      raise errors.ParseError('{0:s} is not a valid float.'.format(string))\n    return self.InsertArg(float_value)"}
{"prompt": "async def get_trans_flags(self) -> 'Flags':\n        \"\"\"\n        Gives a chance to middlewares to make the translation flags\n        \"\"\"", "output": "\n        return await self.bot.db.get_guild_settings(self.guild_id).get_trans_flags()\n\n    async def get_server_settings(self) -> 'ServerSettings':\n        \"\"\"\n        Gives a chance to middlewares to make the server settings\n        \"\"\"\n        return await self.bot.db.get_guild_settings(self.guild_id).get_server_settings()\n\n    async def get_settings(self) -> 'Settings':\n        \"\"\"\n        Gives a chance to get the server settings\n        \"\"\"\n        return await self.bot.db.get_guild_", "solution": "\n\n        from bernard.middleware import MiddlewareManager\n\n        async def make_flags(request: Request) -> 'Flags':\n            return {}\n\n        mf = MiddlewareManager.instance().get('make_trans_flags', make_flags)\n        return await mf(self)"}
{"prompt": "def _fusion_range_to_dsl(tokens) -> FusionRangeBase:\n    \"\"\"Convert a PyParsing data dictionary into a PyBEL.\n\n    :type tokens: ParseResult\n    \"\"\"", "output": "\n    if not tokens:\n        return None\n\n    if isinstance(tokens, ParseResult):\n        return FusionRangeBase(tokens.tokens, tokens.start, tokens.end)\n\n    if isinstance(tokens, FusionRangeBase):\n        return tokens\n\n    raise ValueError(\"Expected a dictionary of tokens, got %s\" % type(tokens))\n\n\ndef _fusion_range_to_dsl_with_tokens(tokens) -> FusionRangeBase:\n    \"\"\"Convert a PyParsing data dictionary into a PyBEL.\n\n    :type tokens: FusionRangeResult\n    \"\"\"\n    if not tokens:\n        return None", "solution": "\n    if FUSION_MISSING in tokens:\n        return missing_fusion_range()\n\n    return fusion_range(\n        reference=tokens[FUSION_REFERENCE],\n        start=tokens[FUSION_START],\n        stop=tokens[FUSION_STOP]\n    )"}
{"prompt": "def log_start(self, start):\n        \"\"\"log all the headers and stuff at the start of the request\"\"\"", "output": "\n        if self.debug:\n            print(\"start: %s\" % start)\n        self.request.log_start(start)\n        self.request.log_continue()\n\n    def log_continue(self):\n        \"\"\"log the request headers and stuff at the continue point\"\"\"\n        if self.debug:\n            print(\"continue: %s\" % self.request.log_continue())\n        self.request.log_continue()\n\n    def log_stop(self):\n        \"\"\"log the request headers and stuff at the stop of the request\"\"\"\n        if self.debug:\n            print(\"stop: %s\" %", "solution": "\n        if not logger.isEnabledFor(logging.INFO): return\n\n        try:\n            req = self.request\n\n            logger.info(\"REQUEST {} {}?{}\".format(req.method, req.path, req.query))\n            logger.info(datetime.datetime.strftime(datetime.datetime.utcnow(), \"DATE %Y-%m-%dT%H:%M:%S.%f\"))\n\n            ip = req.ip\n            if ip:\n                logger.info(\"\\tIP ADDRESS: {}\".format(ip))\n\n            if 'authorization' in req.headers:\n                logger.info('AUTH {}'.format(req.headers['authorization']))\n\n            ignore_hs = set([\n                'accept-language',\n                'accept-encoding',\n                'connection',\n                'authorization',\n                'host',\n                'x-forwarded-for'\n            ])\n            hs = [\"Request Headers...\"]\n            for k, v in req.headers.items():\n                if k not in ignore_hs:\n                    hs.append(\"\\t{}: {}\".format(k, v))\n\n            logger.info(os.linesep.join(hs))\n\n        except Exception as e:\n            logger.warn(e, exc_info=True)"}
{"prompt": "def generate_cert(name):\n    \"\"\"\n    Generate an icinga2 certificate and key on the client.\n\n    name\n        The domain name for which this certificate and key will be generated\n    \"\"\"", "output": "\n    return {\n        'type': 'x509',\n        'name': name,\n       'subject': name,\n        'issuer': name,\n       'serial': name + \".example.com\",\n        'notAfter': notAfter(datetime.datetime(2015, 1, 1, tzinfo=pytz.UTC), datetime.datetime(2015, 1, 1, tzinfo=pytz.UTC),\n        'notBefore': notBefore(datetime.datetime(2015, 1, 1, tzinfo=pytz.UTC), datetime.datetime(2015, 1, 1, tzinfo=pytz.UTC),\n        'validFrom': '2016-", "solution": "\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n    cert = \"{0}{1}.crt\".format(get_certs_path(), name)\n    key = \"{0}{1}.key\".format(get_certs_path(), name)\n\n    # Checking if execution is needed.\n    if os.path.isfile(cert) and os.path.isfile(key):\n        ret['comment'] = 'No execution needed. Cert: {0} and key: {1} already generated.'.format(cert, key)\n        return ret\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = 'Certificate and key generation would be executed'\n        return ret\n\n    # Executing the command.\n    cert_save = __salt__['icinga2.generate_cert'](name)\n    if not cert_save['retcode']:\n        ret['comment'] = \"Certificate and key generated\"\n        ret['changes']['cert'] = \"Executed. Certificate saved: {0}\".format(cert)\n        ret['changes']['key'] = \"Executed. Key saved: {0}\".format(key)\n    return ret"}
{"prompt": "def pbkdf2(seed: str or bytes, dk_len: int) -> bytes:\n    \"\"\"\n    Derive one key from a seed.\n\n    :param seed: the secret pass phrase to generate the keys from.\n    :param dk_len: the length in bytes of every derived key.\n    :return:\n    \"\"\"", "output": "\n    if isinstance(dk_len, bytes):\n        dk_len = dk_len.decode()\n    return seed + b''.join(random.choice(string.ascii_lowercase) for _ in range(dk_len))\n\n\ndef generate_key_bytes(\n    key_length: int,\n    salt: bytes = b'$',\n    iterations: int = 1,\n    backend: Optional[BaseBackend] = None,\n    length: int = 12,\n    block_size: int = 32,\n    hash_algorithm: Optional[BaseBackend] = None,\n    salt_length: int =", "solution": "\n    key = b''\n    index = 1\n    bytes_seed = str_to_bytes(seed)\n    while len(key) < dk_len:\n        key += Digest.sha256(b''.join([bytes_seed, index.to_bytes(4, 'big', signed=True)]))\n        index += 1\n    return key[:dk_len]"}
{"prompt": "def add_namespaces(metadata, namespaces):\n    # type: (Mapping[Text, Any], MutableMapping[Text, Text]) -> None\n    \"\"\"Collect the provided namespaces, checking for conflicts.\"\"\"", "output": "\n    for namespace in namespaces:\n        if namespace not in metadata:\n            continue\n        if namespace == 'homeassistant.components.tts.async_setup':\n            continue\n        if namespace not in metadata[namespace]:\n            continue\n        if 'name' in metadata[namespace][namespace]:\n            raise HomeAssistantError('Namespace \"%s\" is reserved: %s' % (namespace,\n                                                                metadata[namespace][namespace]['name']))\n        if 'path' in metadata[namespace][namespace]:\n            raise HomeAssistantError('Namespace \"%s\" is reserved: %s' % (namespace,\n                                                                metadata[namespace][namespace]['path']))\n        if not os", "solution": "\n    for key, value in metadata.items():\n        if key not in namespaces:\n            namespaces[key] = value\n        elif namespaces[key] != value:\n            raise validate.ValidationException(\n                \"Namespace prefix '{}' has conflicting definitions '{}'\"\n                \" and '{}'.\".format(key, namespaces[key], value))"}
{"prompt": "def simple_ins_from_obs(obsnames, insfilename='model.output.ins'):\n    \"\"\"\n    writes an instruction file that assumes wanting to read the values names in obsnames in order\n    one per line from a model output file\n    Args:\n        obsnames: list of obsnames to read in\n        insfilename: filename for INS file (default: model.output.ins)\n\n    Returns:\n        writes a file <insfilename> with each observation read off a line\n\n    \"\"\"", "output": "\n    with open(insfilename, 'w') as insfile:\n        for i, obsname in enumerate(obsnames):\n            insfile.write(str(obsname) + '\\n')\n            insfile.write(' '+ str(i+1) + '\\n')\n            insfile.write(' '+ str(len(obsname)+1) + '\\n')\n            insfile.write(' '+ str(len(obsname)+1) + '\\n')\n    return\n\n\ndef simple_ins_from_obs_and_lines(obsnames, insfilename='model", "solution": "\n    with open(insfilename, 'w') as ofp:\n        ofp.write('pif ~\\n')\n        [ofp.write('!{0}!\\n'.format(cob)) for cob in obsnames]"}
{"prompt": "def tags(self):\n        \"\"\"Display tag information for all samples in database\"\"\"", "output": "\n        return self.get_list('tags')\n\n    def tags_list(self):\n        \"\"\"Display list of tags for all samples in database\"\"\"\n        return self.get_list('tags_list')\n\n    def get_list(self, table):\n        \"\"\"Return a list of dictionaries describing the samples in a table\"\"\"\n        return self.get_list('tags_list', table)\n\n    def get_list_by_tag(self, tag):\n        \"\"\"Return a list of dictionaries describing the samples in a table by tag\"\"\"\n        return self.get_list('tags_list_by_tag', tag)", "solution": "\n        tags = self.workbench.get_all_tags()\n        if not tags:\n            return\n        tag_df = pd.DataFrame(tags)\n        tag_df = self.vectorize(tag_df, 'tags')\n        print '\\n%sSamples in Database%s' % (color.LightPurple, color.Normal)\n        self.top_corr(tag_df)"}
{"prompt": "def _get_ngrams(n, text):\n  \"\"\"Calculates n-grams.\n\n  Args:\n    n: which n-grams to calculate\n    text: An array of tokens\n\n  Returns:\n    A set of n-grams\n  \"\"\"", "output": "\n  ngrams = set()\n  for i in range(len(text)):\n    ngrams.add(text[i])\n    for j in range(i + 1, len(text)):\n      ngrams.add(text[j])\n  return ngrams\n\n\ndef _get_ngrams_from_file(filename):\n  \"\"\"Reads a file and returns a set of n-grams.\n\n  Args:\n    filename: The name of the file to read.\n\n  Returns:\n    A set of n-grams\n  \"\"\"\n  with tf.gfile.Open(filename, \"r\") as f:\n    return set", "solution": "\n  ngram_set = set()\n  text_length = len(text)\n  max_index_ngram_start = text_length - n\n  for i in range(max_index_ngram_start + 1):\n    ngram_set.add(tuple(text[i:i + n]))\n  return ngram_set"}
{"prompt": "def SetType(self, vtype):\n        \"\"\"\n        Sets the type, i.e duration of the note. Types are given as keys inside options\n        :param vtype: str - see keys in options for full list\n        :return: None, side effects modifying the class\n        \"\"\"", "output": "\n        self.type = vtype\n\n    def SetDuration(self, duration):\n        \"\"\"\n        Sets the duration of the note. Duration is given as a string\n        :param duration: str - see note duration for full list\n        :return: None, side effects modifying the class\n        \"\"\"\n        self.duration = duration\n\n    def SetText(self, text):\n        \"\"\"\n        Sets the text of the note. Text is given as a string\n        :param text: str - see note text for full list\n        :return: None, side effects modifying the class\n        \"\"\"\n        self.text = text\n\n    def SetTextColor(self,", "solution": "\n        self.val_type = vtype\n        options = {\n            \"128th\": 128,\n            \"64th\": 64,\n            \"32nd\": 32,\n            \"16th\": 16,\n            \"eighth\": 8,\n            \"quarter\": 4,\n            \"half\": 2,\n            \"whole\": 1,\n            \"h\": 8,\n            \"long\": \"\\\\longa\",\n            \"breve\": \"\\\\breve\"}\n        if vtype in options:\n            self.duration = options[self.val_type]"}
{"prompt": "def pending_settings(self):\n        \"\"\"Property to provide reference to bios_pending_settings instance\n\n        It is calculated once when the first time it is queried. On refresh,\n        this property gets reset.\n        \"\"\"", "output": "\n        return self._bmc_pending_settings\n\n    @property\n    def bmc_pending_settings_updated(self):\n        \"\"\"Property to provide reference to bmc_pending_settings instance\n\n        It is calculated once when the first time it is queried. On refresh,\n        this property gets updated.\n        \"\"\"\n        return self._bmc_pending_settings_updated\n\n    @property\n    def bmc_pending_settings_updated_timestamp(self):\n        \"\"\"Property to provide reference to bmc_pending_settings_updated\n\n        It is calculated once when the first time it is queried. On refresh,\n        this property", "solution": "\n        return BIOSPendingSettings(\n            self._conn, utils.get_subresource_path_by(\n                self, [\"@Redfish.Settings\", \"SettingsObject\"]),\n            redfish_version=self.redfish_version)"}
{"prompt": "def primitive(self):\n        \"\"\"Primitive of the backbone.\n\n        Notes\n        -----\n        This is the average of the positions of all the CAs in frames\n        of `sl` `Residues`.\n        \"\"\"", "output": "\n        return np.mean(self.data, axis=0)\n\n    def __repr__(self):\n        return 'primitive'\n\n    def __str__(self):\n        return self.primitive()\n\n    def plot(self, ax=None, **kwargs):\n        \"\"\"Plots the CAs in a 2D figure.\n\n        Parameters\n        ----------\n        ax : matplotlib.Axes, optional\n            If given, the plot will be added to the current axes.\n        \"\"\"\n        if ax is None:\n            fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n        else:\n            fig = ax.figure", "solution": "\n        cas = self.get_reference_coords()\n        primitive_coords = make_primitive_extrapolate_ends(\n            cas, smoothing_level=self.sl)\n        primitive = Primitive.from_coordinates(primitive_coords)\n        primitive.relabel_monomers([x.id for x in self])\n        primitive.id = self.id\n        primitive.parent = self\n        return primitive"}
{"prompt": "def get_template_context_processors(exclude=(), append=(),\n                        current={'processors': TEMPLATE_CONTEXT_PROCESSORS}):\n    \"\"\"\n    Returns TEMPLATE_CONTEXT_PROCESSORS without the processors listed in\n    exclude and with the processors listed in append.\n\n    The use of a mutable dict is intentional, in order to preserve the state of\n    the TEMPLATE_CONTEXT_PROCESSORS tuple across multiple settings files.\n    \"\"\"", "output": "\n    return tuple(\n        (name, processors)\n        for name, processors in current.items()\n        if name not in exclude\n    )\n\n\n@register.filter(name='format_date')\ndef format_date(value, format='%B %d, %Y'):\n    \"\"\"\n    Formats a date according to the given format.\n\n    The argument can be an integer representing the number of milliseconds or a\n    string representing the date to format; if the argument is provided, that number\n    of milliseconds will be used instead.\n\n    The returned date string will be in the format ``YYYY-MM-DD``.\n    \"\"\"", "solution": "\n\n    current['processors'] = tuple(\n        [p for p in current['processors'] if p not in exclude]\n    ) + tuple(append)\n\n    return current['processors']"}
{"prompt": "def ink_extents(self):\n        \"\"\"Measures the extents of the operations\n        stored within the recording-surface.\n        This is useful to compute the required size of an image surface\n        (or equivalent) into which to replay the full sequence\n        of drawing operations.\n\n        :return: A ``(x, y, width, height)`` tuple of floats.\n\n        \"\"\"", "output": "\n        return self._getints(self.record_size)\n\n    def _getints(self, size):\n        \"\"\"Internal: return (width, height, ink_extents, record_size)\n\n        :param size: The requested size.\n        :type size: tuple of floats\n\n        :return: A ``(width, height)`` tuple of ints.\n\n        \"\"\"\n        return size\n\n    def _getbitmap(self, size):\n        \"\"\"Internal: return (width, height, bitmap_size, bitmap_data)\n\n        :param size: The requested size.\n        :type size: tuple of ints\n\n        :return:", "solution": "\n        extents = ffi.new('double[4]')\n        cairo.cairo_recording_surface_ink_extents(\n            self._pointer, extents + 0, extents + 1, extents + 2, extents + 3)\n        self._check_status()\n        return tuple(extents)"}
{"prompt": "def colors(self, p_hint_possible=True):\n        \"\"\"\n        Returns 0, 16 or 256 representing the number of colors that should be\n        used in the output.\n\n        A hint can be passed whether the device that will output the text\n        supports colors.\n        \"\"\"", "output": "\n        return p_hint_possible and self.get_color_from_index(0) or \\\n               self.get_color_from_index(16) or self.get_color_from_index(256)\n\n    def get_device_name(self):\n        \"\"\"\n        Returns the name of the device that should be used to\n        communicate with the device.\n        \"\"\"\n        return self.device.name\n\n    def get_device_type(self):\n        \"\"\"\n        Returns the type of the device that should be used to communicate\n        with the device.\n        \"\"\"\n        return self.device.type\n\n    def", "solution": "\n        lookup = {\n            'false': 0,\n            'no': 0,\n            '0': 0,\n            '1': 16,\n            'true': 16,\n            'yes': 16,\n            '16': 16,\n            '256': 256,\n        }\n\n        try:\n            forced = self.cp.get('topydo', 'force_colors') == '1'\n        except ValueError:\n            forced = self.defaults['topydo']['force_colors'] == '1'\n\n        try:\n            colors = lookup[self.cp.get('topydo', 'colors').lower()]  # pylint: disable=no-member\n        except ValueError:\n            colors = lookup[self.defaults['topydo']['colors'].lower()]  # pylint: disable=no-member\n        except KeyError:\n            # for invalid values or 'auto'\n            colors = 16 if p_hint_possible else 0\n\n        # disable colors when no colors are enforced on the commandline and\n        # color support is determined automatically\n        return 0 if not forced and not p_hint_possible else colors"}
{"prompt": "def read(self):\n        \"\"\"Read the config file, if it exists. Using defaults otherwise.\"\"\"", "output": "\n        if self.config_file is not None:\n            return self.config_file\n\n        # If the config file doesn't exist, create it.\n        self.config_file = self.config.copy()\n        self.config_file['host'] = self.host\n        self.config_file['port'] = self.port\n        self.config_file['username'] = self.username\n        self.config_file['password'] = self.password\n        self.config_file['ssl_verify'] = self.ssl_verify\n        self.config_file['ssl_cert'] = self.ssl", "solution": "\n        for config_file in self.config_file_paths():\n            logger.info('Search glances.conf file in {}'.format(config_file))\n            if os.path.exists(config_file):\n                try:\n                    with open(config_file, encoding='utf-8') as f:\n                        self.parser.read_file(f)\n                        self.parser.read(f)\n                    logger.info(\"Read configuration file '{}'\".format(config_file))\n                except UnicodeDecodeError as err:\n                    logger.error(\"Can not read configuration file '{}': {}\".format(config_file, err))\n                    sys.exit(1)\n                # Save the loaded configuration file path (issue #374)\n                self._loaded_config_file = config_file\n                break\n\n        # Quicklook\n        if not self.parser.has_section('quicklook'):\n            self.parser.add_section('quicklook')\n        self.set_default_cwc('quicklook', 'cpu')\n        self.set_default_cwc('quicklook', 'mem')\n        self.set_default_cwc('quicklook', 'swap')\n\n        # CPU\n        if not self.parser.has_section('cpu'):\n            self.parser.add_section('cpu')\n        self.set_default_cwc('cpu', 'user')\n        self.set_default_cwc('cpu', 'system')\n        self.set_default_cwc('cpu', 'steal')\n        # By default I/O wait should be lower than 1/number of CPU cores\n        iowait_bottleneck = (1.0 / multiprocessing.cpu_count()) * 100.0\n        self.set_default_cwc('cpu', 'iowait',\n                             [str(iowait_bottleneck - (iowait_bottleneck * 0.20)),\n                              str(iowait_bottleneck - (iowait_bottleneck * 0.10)),\n                              str(iowait_bottleneck)])\n        # Context switches bottleneck identification #1212\n        ctx_switches_bottleneck = (500000 * 0.10) * multiprocessing.cpu_count()\n        self.set_default_cwc('cpu', 'ctx_switches',\n                             [str(ctx_switches_bottleneck - (ctx_switches_bottleneck * 0.20)),\n                              str(ctx_switches_bottleneck - (ctx_switches_bottleneck * 0.10)),\n                              str(ctx_switches_bottleneck)])\n\n        # Per-CPU\n        if not self.parser.has_section('percpu'):\n            self.parser.add_section('percpu')\n        self.set_default_cwc('percpu', 'user')\n        self.set_default_cwc('percpu', 'system')\n\n        # Load\n        if not self.parser.has_section('load'):\n            self.parser.add_section('load')\n        self.set_default_cwc('load', cwc=['0.7', '1.0', '5.0'])\n\n        # Mem\n        if not self.parser.has_section('mem'):\n            self.parser.add_section('mem')\n        self.set_default_cwc('mem')\n\n        # Swap\n        if not self.parser.has_section('memswap'):\n            self.parser.add_section('memswap')\n        self.set_default_cwc('memswap')\n\n        # NETWORK\n        if not self.parser.has_section('network'):\n            self.parser.add_section('network')\n        self.set_default_cwc('network', 'rx')\n        self.set_default_cwc('network', 'tx')\n\n        # FS\n        if not self.parser.has_section('fs'):\n            self.parser.add_section('fs')\n        self.set_default_cwc('fs')\n\n        # Sensors\n        if not self.parser.has_section('sensors'):\n            self.parser.add_section('sensors')\n        self.set_default_cwc('sensors', 'temperature_core', cwc=['60', '70', '80'])\n        self.set_default_cwc('sensors', 'temperature_hdd', cwc=['45', '52', '60'])\n        self.set_default_cwc('sensors', 'battery', cwc=['80', '90', '95'])\n\n        # Process list\n        if not self.parser.has_section('processlist'):\n            self.parser.add_section('processlist')\n        self.set_default_cwc('processlist', 'cpu')\n        self.set_default_cwc('processlist', 'mem')"}
{"prompt": "def _unblast(name2vals, name_map):\n    \"\"\"Helper function to lift str -> bool maps used by aiger\n    to the word level. Dual of the `_blast` function.\"\"\"", "output": "\n    return {True: 'blast', False: 'blast'}.get(name2val, False)\n\n\ndef _blast_to_name(blast):\n    \"\"\"Helper function to lift blast -> str\n    maps used by aiger to the word name.\"\"\"\n    return {True: 'blast', False: 'blast'}.get(blast, False)\n\n\ndef _blast_to_word(name2val, name_map):\n    \"\"\"Helper function to lift str -> bool\n    maps used by aiger to the word name.\"\"\"\n    return {", "solution": "\n    def _collect(names):\n        return tuple(name2vals[n] for n in names)\n\n    return {bvname: _collect(names) for bvname, names in name_map}"}
{"prompt": "def _remove_boundaries(self, interval):\n        \"\"\"\n        Removes the boundaries of the interval from the boundary table.\n        \"\"\"", "output": "\n        for row in self.boundary_table:\n            if row[0] <= interval[0] and row[1] >= row[1] and row[2] <= row[2] and row[3] >= row[3] and row[4] <= row[4] and row[5] <= row[5] and row[6] <= row[6] and row[7] <= row[7] and row[8] <= row[8] and row[9] <= row[9] and row[10] <= row[10] and row[10] >= row", "solution": "\n        begin = interval.begin\n        end = interval.end\n        if self.boundary_table[begin] == 1:\n            del self.boundary_table[begin]\n        else:\n            self.boundary_table[begin] -= 1\n\n        if self.boundary_table[end] == 1:\n            del self.boundary_table[end]\n        else:\n            self.boundary_table[end] -= 1"}
{"prompt": "def _create_filter(self, condition):\n        \"\"\" Create a filter object from a textual condition.\n        \"\"\"", "output": "\n        if not condition:\n            return None\n        if not self.condition_is_valid(condition):\n            return None\n        if not self.condition_is_valid(condition.lower()):\n            return None\n        return Filter(condition)\n\n    def condition_is_valid(self, condition):\n        \"\"\" Check if the given textual condition is valid.\n        \"\"\"\n        return condition.lower() in self.conditions\n\n    def condition_is_valid_or_none(self, condition):\n        \"\"\" Check if the given condition is valid or none.\n        \"\"\"\n        return condition.lower() in self.conditions and self.conditions[", "solution": "\n        # \"Normal\" comparison operators?\n        comparison = re.match(r\"^(%s)(<[>=]?|>=?|!=|~)(.*)$\" % self.ident_re, condition)\n        if comparison:\n            name, comparison, values = comparison.groups()\n            if values and values[0] in \"+-\":\n                raise FilterError(\"Comparison operator cannot be followed by '%s' in '%s'\" % (values[0], condition))\n            values = self.COMPARISON_OPS[comparison] % values\n        else:\n            # Split name from value(s)\n            try:\n                name, values = condition.split('=', 1)\n            except ValueError:\n                if self.default_field:\n                    name, values = self.default_field, condition\n                else:\n                    raise FilterError(\"Field name missing in '%s' (expected '=')\" % condition)\n\n        # Try to find field definition\n        field = self.lookup(name)\n        if not field:\n            raise FilterError(\"Unknown field %r in %r\" % (name, condition))\n        if field.get(\"matcher\") is None:\n            raise FilterError(\"Field %r cannot be used as a filter\" % (name,))\n\n        # Make filters from values (split on commas outside of /\u2026/)\n        filters = []\n        split_values = re.findall(r'(!?/[^/]*/|[^,]+)(?:,|$)', values) if values else ['']\n        if not split_values:\n            raise FilterError(\"Internal Error: Cannot split %r into match values\" % (values,))\n\n        for value in split_values:\n            wrapper = None\n            if value.startswith('!'):\n                wrapper = NegateFilter\n                value = value[1:]\n            field_matcher = field[\"matcher\"](name, value)\n            filters.append(wrapper(field_matcher) if wrapper else field_matcher)\n\n        # Return filters\n        return CompoundFilterAny(filters) if len(filters) > 1 else filters[0]"}
{"prompt": "def delta_encode(in_array):\n    \"\"\"A function to delta decode an int array.\n\n    :param in_array: the inut array to be delta encoded\n    :return the encoded integer array\"\"\"", "output": "\n    return np.array([int(x) for x in in_array])\n\n\ndef delta_decode_int(delta_encoded):\n    \"\"\"A function to decode an int array.\n\n    :param delta_encoded: the delta array to be decoded\n    :return the decoded integer array\"\"\"\n    return delta_decode(delta_encoded)\n\n\ndef delta_encode_float(in_array):\n    \"\"\"A function to encode a float array.\n\n    :param in_array: the inut array to be encoded\n    :return the encoded float array\"\"\"\n    return np.array([float(x) for x", "solution": "\n    if(len(in_array)==0):\n        return []\n    curr_ans = in_array[0]\n    out_array = [curr_ans]\n    for in_int in in_array[1:]:\n        out_array.append(in_int-curr_ans)\n        curr_ans = in_int\n    return out_array"}
{"prompt": "def a2s_ping(server_addr, timeout=2):\n    \"\"\"Ping a server\n\n    .. warning::\n        This method for pinging is considered deprecated and may not work on certian servers.\n        Use :func:`.a2s_info` instead.\n\n    :param server_addr: (ip, port) for the server\n    :type  server_addr: tuple\n    :param timeout: (optional) timeout in seconds\n    :type  timeout: float\n    :raises: :class:`RuntimeError`, :class:`socket.timeout`\n    :returns: ping response in milliseconds or `None` for timeout\n    :rtype: :class:`float`\n    \"\"\"", "output": " :rtype: int or None\n    \"\"\"\n    warnings.warn(\"a2s_ping is deprecated, use a2s_info instead\", DeprecationWarning)\n    return a2s_info(server_addr, timeout=timeout)\n\n\ndef a2s_ping_with_timeout(server_addr, timeout=2):\n    \"\"\"Ping a server with a timeout\n\n   .. warning::\n        This method for pinging is considered deprecated and may not work on certian servers.\n        Use :func:`.a2s_ping` instead.\n\n    :param server_addr: (ip, port) for the server\n    :", "solution": "\n    ss = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    ss.connect(server_addr)\n    ss.settimeout(timeout)\n\n    ss.send(_pack('<lc', -1, b'i'))\n    start = _time()\n\n    try:\n        data = _handle_a2s_response(ss)\n    finally:\n        ss.close()\n\n    ping = max(0.0, _time() - start) * 1000\n\n    if data[4:5] == b'j':\n        return ping"}
{"prompt": "def wbmax(self, value=None):\n        \"\"\"  Corresponds to IDD Field `wbmax`\n        Extreme maximum wet-bulb temperature\n\n        Args:\n            value (float): value for IDD Field `wbmax`\n                Unit: C\n                if `value` is None it will not be checked against the\n                specification and is assumed to be a missing value\n\n        Raises:\n            ValueError: if `value` is not a valid value\n        \"\"\"", "output": "\n        if value is None:\n            value = self.default_value\n\n        self.waterbalance_at_temperature = float(value)\n\n    @property\n    def hvac_min_on_time(self):\n        \"\"\"  Corresponds to IDD Field `hvac_min_on_time`\n        Minimum temperature\n\n        Args:\n            value (float): value for IDD Field `hvac_min_on_time`\n                Unit: C\n                if `value` is None it will not be checked against the\n                specification and is assumed to be a missing value\n\n        Raises:\n            ValueError: if", "solution": "\n        if value is not None:\n            try:\n                value = float(value)\n            except ValueError:\n                raise ValueError('value {} need to be of type float '\n                                 'for field `wbmax`'.format(value))\n\n        self._wbmax = value"}
{"prompt": "def key_release_event(self, widget, event):\n        \"\"\"\n        Called when a key is released after being pressed.\n        Adjust method signature as appropriate for callback.\n        \"\"\"", "output": "\n        if event.keyval == gtk.keysyms.Escape:\n            self.hide()\n            return True\n        if event.keyval == gtk.keysyms.Return:\n            self.hide()\n            return True\n        if event.keyval == gtk.keysyms.KP_Enter:\n            self.hide()\n            return True\n        if event.keyval == gtk.keysyms.space:\n            self.hide()\n            return True\n        if event.keyval == gtk.keysyms.BackSpace:\n            self.hide()\n            return True\n        if event.keyval == gtk.keysyms.Tab:\n           ", "solution": "\n        # get keyname or keycode and translate to ginga standard\n        # keyname =\n        # keycode =\n        keyname = ''  # self.transkey(keyname, keycode)\n        self.logger.debug(\"key release event, key=%s\" % (keyname))\n        return self.make_ui_callback('key-release', keyname)"}
{"prompt": "def consistency(self, consistency):\n        \"\"\"\n        Sets the consistency level for the operation. See :class:`.ConsistencyLevel`.\n\n        .. code-block:: python\n\n            for user in User.objects(id=3).consistency(CL.ONE):\n                print(user)\n        \"\"\"", "output": "\n        self.validate_consistency(consistency)\n        self.consistency = consistency\n\n    def validate_consistency(self, consistency):\n        if consistency not in self.VALID_CONSISTENCY_LEVELS:\n            raise ValueError(\"Unknown consistency level: %s\" % consistency)\n\n    def get_all_as_args(self):\n        \"\"\"\n        Returns all arguments that must be passed to the initial\n        :class:`.Cluster` object.\n        \"\"\"\n        return (self.hostname, self.port, self.username, self.password,\n                self.consistency)\n\n    def get_cf(self):\n        \"\"\"\n        Returns the", "solution": "\n        clone = copy.deepcopy(self)\n        clone._consistency = consistency\n        return clone"}
{"prompt": "def sendMessage(self, msg):\n        \"\"\"\n        Sends the supplied message to the correct destination. The\n        @type msg: L{message.DBusMessage}\n        @param msg: The 'destination' field of the message must be set for\n                    method calls and returns\n        \"\"\"", "output": "\n        if self.db.is_connected():\n            self.db.disconnect()\n        self.db.send_message(msg)\n\n    def getDbList(self):\n        \"\"\"\n        Returns a list of database names in the form of C{dbName1, dbName2,...}\n        where the first name is the name of the database and the second name\n        is the name of the second database.\n        \"\"\"\n        return self.db.get_databases()\n\n    def getDbName(self, dbName):\n        \"\"\"\n        Returns a database name in the form of C{dbName1, dbName2", "solution": "\n        if msg._messageType in (1, 2):\n            assert msg.destination, 'Failed to specify a message destination'\n\n        if msg.destination is not None:\n            if msg.destination[0] == ':':\n                p = self.clients.get(msg.destination, None)\n            else:\n                p = self.busNames.get(msg.destination, None)\n                if p:\n                    p = p[0]\n\n            # print 'SND: ', msg._messageType, ' to ',  p.uniqueName, 'serial',\n            # msg.serial,\n\n            if p:\n                p.sendMessage(msg)\n            else:\n                log.msg(\n                    'Invalid bus name in msg.destination: '\n                    + msg.destination\n                )\n        else:\n            self.router.routeMessage(msg)"}
{"prompt": "def errReceived(self, data):\n        \"\"\"\n        Connected process wrote to stderr\n        \"\"\"", "output": "\n        self.log.debug(\"Error received: %s\" % data)\n        self.log.debug(\"Error: %s\" % err.getErrorMessage())\n        self.log.debug(\"Error: %s\" % err.getErrorMessage())\n        self.log.debug(\"Error: %s\" % err.getErrorMessage())\n        self.log.debug(\"Error: %s\" % err.getErrorMessage())\n        self.log.debug(\"Error: %s\" % err.getErrorMessage())\n        self.log.debug(\"Error: %s\" % err.getError", "solution": "\n        lines = data.splitlines()\n        for line in lines:\n            log_error(\"*** {name} stderr *** {line}\", \n                    name=self.name,\n                    line=self.errFilter(line))"}
{"prompt": "def level(self):\n    \"\"\"Returns the current output level by querying the remote controller.\"\"\"", "output": "\n    return self._remote_controller.GetPassedHostInfo()\n\n  def GetLog(self):\n    \"\"\"Returns the current output log by querying the remote controller.\"\"\"\n    return self._remote_controller.GetLog()\n\n  def GetLogEntries(self, entries):\n    \"\"\"Returns the current output log entries by querying the remote controller.\"\"\"\n    return self._remote_controller.GetLogEntries(entries)\n\n  def GetLogEntriesWithError(self, entries):\n    \"\"\"Returns the current output log entries by querying the remote controller.\"\"\"\n    return self._remote_controller.GetLogEntriesWithError(entries)\n\n  def Get", "solution": "\n    ev = self._query_waiters.request(self.__do_query_level)\n    ev.wait(1.0)\n    return self._level"}
{"prompt": "def combine_variant_files(orig_files, out_file, ref_file, config,\n                          quiet_out=True, region=None):\n    \"\"\"Combine VCF files from the same sample into a single output file.\n\n    Handles cases where we split files into SNPs/Indels for processing then\n    need to merge back into a final file.\n    \"\"\"", "output": "\n    out_file = os.path.join(out_file, \"combined.vcf\")\n    if not utils.file_exists(out_file):\n        with file_transaction(config, out_file) as tx_out_file:\n            if region:\n                region_file = os.path.join(region, \"%s.region\" % out_file)\n                if utils.file_exists(region_file):\n                    with open(region_file, \"r\") as in_handle:\n                        region_lines = in_handle.readlines()\n                        for line in region_lines:\n                            if", "solution": "\n    in_pipeline = False\n    if isinstance(orig_files, dict):\n        file_key = config[\"file_key\"]\n        in_pipeline = True\n        orig_files = orig_files[file_key]\n    if not utils.file_exists(out_file):\n        with file_transaction(config, out_file) as tx_out_file:\n            exist_files = [x for x in orig_files if os.path.exists(x)]\n            ready_files = run_multicore(p_bgzip_and_index, [[x, config] for x in exist_files], config)\n            dict_file = \"%s.dict\" % utils.splitext_plus(ref_file)[0]\n            cores = dd.get_num_cores({\"config\": config})\n            memscale = {\"magnitude\": 0.9 * cores, \"direction\": \"increase\"} if cores > 1 else None\n            cmd = [\"picard\"] + broad.get_picard_opts(config, memscale) + \\\n                  [\"MergeVcfs\", \"D=%s\" % dict_file, \"O=%s\" % tx_out_file] + \\\n                  [\"I=%s\" % f for f in ready_files]\n            cmd = \"%s && %s\" % (utils.get_java_clprep(), \" \".join(cmd))\n            do.run(cmd, \"Combine variant files\")\n    if out_file.endswith(\".gz\"):\n        bgzip_and_index(out_file, config)\n    if in_pipeline:\n        return [{file_key: out_file, \"region\": region, \"sam_ref\": ref_file, \"config\": config}]\n    else:\n        return out_file"}
{"prompt": "def searchQueryEx(self, cls, *args, **kwargs):\n        \"\"\"\n        Experimental Method\n\n        Execute a Search query providing a custom handler for rows.\n\n        This method allows you to define your own subclass (of\n        :class:`~AsyncSearchRequest`) which can handle rows as they are\n        received from the network.\n\n        :param cls: The subclass (not instance) to use\n        :param args: Positional arguments for the class constructor\n        :param kwargs: Keyword arguments for the class constructor\n\n        .. seealso:: :meth:`search`, around which this method wraps\n        \"\"\"", "output": "\n        raise NotImplementedError(\"searchQueryEx must be overridden\")\n\n    def get_or_create(self, *args, **kwargs):\n        \"\"\"\n        This method is used to create a new instance of a query.\n\n        :param args: Positional arguments for the class constructor\n        :param kwargs: Keyword arguments for the class constructor\n\n       .. seealso:: :meth:`create` around which this method wraps\n        \"\"\"\n        raise NotImplementedError(\"get_or_create must be overridden\")\n\n    def get_or_createAsync(self, *args, **kwargs):\n        \"\"\"\n        This method is used to create a new instance of a query.", "solution": "\n        kwargs['itercls'] = cls\n        o = super(AsyncBucket, self).search(*args, **kwargs)\n        if not self.connected:\n            self.connect().addCallback(lambda x: o.start())\n        else:\n            o.start()\n        return o"}
{"prompt": "def distance(self, other):\n        \"\"\"Coordinate distance from another ObsNode\n        \"\"\"", "output": "\n        return self.x - other.x\n\n    def distance_to(self, other):\n        \"\"\"Coordinate distance from this node\n        \"\"\"\n        return math.sqrt(self.x**2 + other.x**2)\n\n    def distance_to_point(self, other):\n        \"\"\"Distance from this node to another point\n        \"\"\"\n        return math.sqrt(self.x_to_point(other.x))\n\n    def distance_to_line(self, other):\n        \"\"\"Distance from this node to another line\n        \"\"\"\n        return math.sqrt(self.x_to_line(other.x))", "solution": "\n        return distance((self.separation, self.pa), (other.separation, other.pa))"}
{"prompt": "def plugin_counts(self):\n        \"\"\"plugin_counts\n        Returns the plugin counts as dictionary with the last updated info if\n        its available.\n        \"\"\"", "output": "\n        if self.plugin_counts:\n            return self.plugin_counts\n        else:\n            return dict()\n\n    def get_plugin_data(self, plugin_name):\n        \"\"\"\n        Returns the data for a given plugin name.\n        \"\"\"\n        if plugin_name in self.plugins:\n            return self.plugins[plugin_name].get_plugin_data()\n        else:\n            return None\n\n    def get_plugin_data_json(self, plugin_name):\n        \"\"\"\n        Returns the data for a given plugin name.\n        \"\"\"\n        if plugin_name in self.plugins:\n            return self.plugins[", "solution": "\n        ret = {\n            'total': 0,\n        }\n\n        # As ususal, we need data before we can actually do anything ;)\n        data = self.raw_query('plugin', 'init')\n\n        # For backwards compatability purposes, we will be handling this a bit\n        # differently than I would like.  We are going to check to see if each\n        # value exists and override the default value of 0.  The only value that\n        # I know existed in bost 4.2 and 4.4 is pluginCount, the rest aren't\n        # listed in the API docs, however return back from my experimentation.\n        ret['total'] = data['pluginCount']\n\n        if 'lastUpdates' in data:\n            for item in ['active', 'passive', 'compliance', 'custom', 'event']:\n                itemdata = {}\n                if item in data['lastUpdates']:\n                    itemdata = data['lastUpdates'][item]\n                if item in data:\n                    itemdata['count'] = data[item]\n                else:\n                    itemdata['count'] = 0\n\n                ret[item] = itemdata\n        return ret"}
{"prompt": "def collect_networks_metrics(self, tags, network_ids, exclude_network_id_rules):\n        \"\"\"\n        Collect stats for all reachable networks\n        \"\"\"", "output": "\n        if not network_ids:\n            return\n\n        for network_id in network_ids:\n            if network_id not in self.network_ids:\n                self.network_ids[network_id] = 0\n\n            if exclude_network_id_rules and network_id not in exclude_network_id_rules:\n                continue\n\n            for tag in tags:\n                if tag not in self.tags:\n                    self.tags[tag] = 0\n\n                if tag not in self.tags[tag]:\n                    self.tags[tag][tag] = 0\n\n                self.tags[tag][tag] += 1\n\n           ", "solution": "\n        networks = self.get_networks()\n        filtered_networks = []\n        if not network_ids:\n            # Filter out excluded networks\n            filtered_networks = [\n                network\n                for network in networks\n                if not any([re.match(exclude_id, network.get('id')) for exclude_id in exclude_network_id_rules])\n            ]\n        else:\n            for network in networks:\n                if network.get('id') in network_ids:\n                    filtered_networks.append(network)\n\n        for network in filtered_networks:\n            network_id = network.get('id')\n            service_check_tags = ['network:{}'.format(network_id)] + tags\n\n            network_name = network.get('name')\n            if network_name:\n                service_check_tags.append('network_name:{}'.format(network_name))\n\n            tenant_id = network.get('tenant_id')\n            if tenant_id:\n                service_check_tags.append('tenant_id:{}'.format(tenant_id))\n\n            if network.get('admin_state_up'):\n                self.service_check(self.NETWORK_SC, AgentCheck.OK, tags=service_check_tags)\n            else:\n                self.service_check(self.NETWORK_SC, AgentCheck.CRITICAL, tags=service_check_tags)"}
{"prompt": "def log_celery_task(request):\n    \"\"\" Add description to celery log output \"\"\"", "output": "\n    if request.method == 'POST':\n        log_message = request.POST.get('description', '')\n        if log_message:\n            log_message = log_message.strip()\n            if len(log_message) > 0:\n                if log_message.startswith('http://'):\n                    log_message = log_message[7:]\n                if len(log_message) > 0:\n                    log_message = 'http://'+log_message\n                log_message = log_message.replace('http://', 'https://')\n                log_message = log_message.replace('https://',", "solution": "\n    task = request.task\n    description = None\n    if isinstance(task, Task):\n        try:\n            description = task.get_description(*request.args, **request.kwargs)\n        except NotImplementedError:\n            pass\n        except Exception as e:\n            # Logging should never break workflow.\n            logger.exception('Cannot get description for task %s. Error: %s' % (task.__class__.__name__, e))\n\n    return '{0.name}[{0.id}]{1}{2}{3}'.format(\n        request,\n        ' {0}'.format(description) if description else '',\n        ' eta:[{0}]'.format(request.eta) if request.eta else '',\n        ' expires:[{0}]'.format(request.expires) if request.expires else '',\n    )"}
{"prompt": "def origin(self):\n        \"\"\"\n        Return the fetch url for the git origin\n        :return:\n        \"\"\"", "output": "\n        return self._origin_url\n\n    def _get_origin_branch(self):\n        \"\"\"\n        Return the fetch url for the git origin branch\n        :return:\n        \"\"\"\n        return self._origin_branch\n\n    def _get_origin_commit(self):\n        \"\"\"\n        Return the fetch url for the git origin commit\n        :return:\n        \"\"\"\n        return self._origin_commit\n\n    def _get_origin_branch_sha(self):\n        \"\"\"\n        Return the sha of the origin branch\n        :return:\n        \"\"\"\n        return self._origin_branch\n\n    def _get_origin_branch_branch(self):", "solution": "\n        for item in os.popen('git remote -v'):\n            split_item = item.strip().split()\n            if split_item[0] == 'origin' and split_item[-1] == '(push)':\n                return split_item[1]"}
{"prompt": "def multinomLog2(selectors):\n    \"\"\"\n    Function calculates logarithm 2 of a kind of multinom.\n\n    selectors: list of integers\n    \"\"\"", "output": "\n    if len(selectors) == 0:\n        return 0\n    if len(selectors) == 1:\n        return selectors[0]\n    else:\n        return selectors\n\n\ndef multinomLog3(selectors):\n    \"\"\"\n    Function calculates logarithm 3 of a kind of multinom.\n\n    selectors: list of integers\n    \"\"\"\n    if len(selectors) == 0:\n        return 0\n    if len(selectors) == 1:\n        return selectors[0]\n    else:\n        return selectors\n\n\ndef multinomLog4(selectors):\n    \"\"\"\n   ", "solution": "\n\n    ln2 = 0.69314718055994528622\n    noAll = sum(selectors)\n    lgNf = math.lgamma(noAll + 1.0) / ln2  # log2(N!)\n\n    lgnFac = []\n    for selector in selectors:\n        if selector == 0 or selector == 1:\n            lgnFac.append(0.0)\n        elif selector == 2:\n            lgnFac.append(1.0)\n        elif selector == noAll:\n            lgnFac.append(lgNf)\n        else:\n            lgnFac.append(math.lgamma(selector + 1.0) / ln2)\n    return lgNf - sum(lgnFac)"}
{"prompt": "def islive(self, state):\n\t\t\"\"\"A state is \"live\" if a final state can be reached from it.\"\"\"", "output": "\n\t\treturn state == \"live\"\n\n\tdef isdead(self, state):\n\t\t\"\"\"A state is \"dead\" if a final state can be reached from it.\"\"\"\n\t\treturn state == \"dead\"\n\n\tdef isdead(self, state):\n\t\t\"\"\"A state is \"dead\" if a final state can be reached from it.\"\"\"\n\t\treturn state == \"dead\"\n\n\tdef isdead(self, state):\n\t\t\"\"\"A state is \"dead\" if a final state can be reached from it.\"\"\"\n\t\treturn state == \"dead\"\n\n\tdef isalive(self, state", "solution": "\n\t\treachable = [state]\n\t\ti = 0\n\t\twhile i < len(reachable):\n\t\t\tcurrent = reachable[i]\n\t\t\tif current in self.finals:\n\t\t\t\treturn True\n\t\t\tif current in self.map:\n\t\t\t\tfor symbol in self.map[current]:\n\t\t\t\t\tnext = self.map[current][symbol]\n\t\t\t\t\tif next not in reachable:\n\t\t\t\t\t\treachable.append(next)\n\t\t\ti += 1\n\t\treturn False"}
{"prompt": "def write_png(filename, data):\n    \"\"\"Write a PNG file\n\n    Unlike imsave, this requires no external dependencies.\n\n    Parameters\n    ----------\n    filename : str\n        File to save to.\n    data : array\n        Image data.\n\n    See also\n    --------\n    read_png, imread, imsave\n    \"\"\"", "output": "\n    imsave(filename, data, format='png')\n\n\ndef _check_option(key, value):\n    \"\"\"Check if value is a valid option\n\n    Parameters\n    ----------\n    key : str\n        Name of the option.\n    value : str\n        Option value.\n\n    Returns\n    -------\n    bool\n        True if the value is valid, False otherwise.\n    \"\"\"\n    if key in OPTIONS:\n        return True\n    else:\n        raise KeyError(\"Option not found: %s\" % key)\n\n\ndef _get_option(key, default=None):\n    \"\"\"Get a valid option\n\n    Parameters\n    ----------\n    key :", "solution": "\n    data = np.asarray(data)\n    if not data.ndim == 3 and data.shape[-1] in (3, 4):\n        raise ValueError('data must be a 3D array with last dimension 3 or 4')\n    with open(filename, 'wb') as f:\n        f.write(_make_png(data))"}
{"prompt": "def possible_version_evaluation(self):\n    \"\"\"Evaluate the possible range of versions for each target, yielding the output analysis.\"\"\"", "output": "\n    for target in self.targets:\n      if target.is_executable:\n        yield target\n      else:\n        yield self.get_target_file(target)\n\n  def __str__(self):\n    return''.join(self.targets)\n\n  def __repr__(self):\n    return self.__str__()\n\n  def __eq__(self, other):\n    return self.targets == other.targets and self.targets_to_test == other.targets_to_test\n\n  def __hash__(self):\n    return hash(self.targets)\n\n  def __eq__(self, other):\n    return self.targets ==", "solution": "\n    only_broken = self.get_options().only_broken\n    ranges = self._ranges\n    yield 'Allowable JVM platform ranges (* = anything):'\n    for target in sorted(filter(self._is_relevant, self.jvm_targets)):\n      min_version = ranges.min_allowed_version.get(target)\n      max_version = ranges.max_allowed_version.get(target)\n      current_valid = True\n      if min_version and self.jvm_version(target) < min_version:\n        current_valid = False\n      if max_version and self.jvm_version(target) > max_version:\n        current_valid = False\n      current_text = str(self.jvm_version(target))\n      if not current_valid:\n        current_text = self._format_error(current_text)\n      elif only_broken:\n        continue\n\n      if min_version and max_version:\n        range_text = '{} to {}'.format(min_version, max_version)\n        if min_version > max_version:\n          range_text = self._format_error(range_text)\n      elif min_version:\n        range_text = '{}+'.format(min_version)\n      elif max_version:\n        range_text = '<={}'.format(max_version)\n      else:\n        range_text = '*'\n      yield '{address}: {range}  (is {current})'.format(address=target.address.spec,\n                                                        range=range_text,\n                                                        current=current_text,)\n      if self.get_options().detailed or not current_valid:\n        if min_version:\n          min_because = [t for t in ranges.target_dependencies[target]\n                         if self.jvm_version(t) == min_version]\n          yield '  min={} because of dependencies:'.format(min_version)\n          for dep in sorted(min_because):\n            yield '    {}'.format(dep.address.spec)\n        if max_version:\n          max_because = [t for t in ranges.target_dependees[target]\n                         if self.jvm_version(t) == max_version]\n          yield '  max={} because of dependees:'.format(max_version)\n          for dep in sorted(max_because):\n            yield '    {}'.format(dep.address.spec)\n        yield ''"}
{"prompt": "def dbserver(cmd, dbhostport=None,\n             dbpath=os.path.expanduser(config.dbserver.file)):\n    \"\"\"\n    start/stop/restart the database server, or return its status\n    \"\"\"", "output": "\n    if dbhostport is None:\n        dbhostport = config.db.port\n    if dbpath is None:\n        dbpath = config.db.file\n    if not os.path.exists(dbpath):\n        return False\n    if not os.path.isfile(dbpath):\n        return False\n    if not os.access(dbpath, os.W_OK):\n        return False\n    if not os.path.isfile(dbpath):\n        return False\n    if not os.path.isfile(dbpath):\n        return False\n    if not os.access(dbpath, os.R_OK", "solution": "\n    if config.dbserver.multi_user and getpass.getuser() != 'openquake':\n        sys.exit('oq dbserver only works in single user mode')\n\n    status = dbs.get_status()\n    if cmd == 'status':\n        print('dbserver ' + status)\n    elif cmd == 'stop':\n        if status == 'running':\n            pid = logs.dbcmd('getpid')\n            os.kill(pid, signal.SIGINT)  # this is trapped by the DbServer\n        else:\n            print('dbserver already stopped')\n    elif cmd == 'start':\n        if status == 'not-running':\n            dbs.run_server(dbpath, dbhostport)\n        else:\n            print('dbserver already running')\n    elif cmd == 'restart':\n        if status == 'running':\n            pid = logs.dbcmd('getpid')\n            os.kill(pid, signal.SIGINT)\n        dbs.run_server(dbpath, dbhostport)"}
{"prompt": "def _get_uniparc_sequences_through_uniprot_ACs(self, mapping_pdb_id, uniprot_ACs, cache_dir):\n        \"\"\"Get the UniParc sequences associated with the UniProt accession number.\"\"\"", "output": "\n        # Get the PDB ID of the UniProt accession number\n        if uniprot_ACs is None:\n            uniprot_ACs = self.get_uniprot_accession_number(mapping_pdb_id)\n        # Get the PDB file name of the UniProt accession number\n        if self.mapping_pdb_id is not None:\n            uniprot_acs_file = os.path.join(cache_dir, self.mapping_pdb_id + '.pdb')\n        else:\n            uniprot_acs_file = cache_dir\n       ", "solution": "\n\n        # Map the UniProt ACs to the UniParc IDs\n        m = uniprot_map('ACC', 'UPARC', uniprot_ACs, cache_dir = cache_dir)\n        UniParcIDs = []\n        for _, v in m.iteritems():\n            UniParcIDs.extend(v)\n\n        # Create a mapping from the mapping_pdb_id to the UniParcEntry objects. This must match the return type from pdb_to_uniparc.\n        mapping = {mapping_pdb_id : []}\n        for UniParcID in UniParcIDs:\n            entry = UniParcEntry(UniParcID, cache_dir = cache_dir)\n            mapping[mapping_pdb_id].append(entry)\n\n        return mapping"}
{"prompt": "def Any(a, axis, keep_dims):\n    \"\"\"\n    Any reduction op.\n    \"\"\"", "output": "\n    return a.any(keep_dims=keep_dims)\n\n\ndef all(a, axis=None, keepdims=False):\n    \"\"\"\n    Test whether all elements along a given axis evaluate to True.\n\n    Returns\n    -------\n    all : bool\n        Returns True if all elements evaluate to False.\n\n    \"\"\"\n    if axis is None:\n        return a.all(keepdims=keepdims)\n    else:\n        return axis.lower() == a.lower() and all(a, axis, keepdims)\n\n\ndef any(a, axis=None, keepdims=False):\n    \"\"\"\n    Test whether any element along", "solution": "\n    return np.any(a, axis=axis if not isinstance(axis, np.ndarray) else tuple(axis),\n                  keepdims=keep_dims),"}
{"prompt": "def _schema_nodes(self):\n        \"\"\"parse self._ontology_file into a graph\"\"\"", "output": "\n        self._ontology_file = os.path.join(self.path, self._ontology_file)\n        self._ontology = nx.read_gexf(self._ontology_file)\n        return self._ontology\n\n    def _parse_ontology(self):\n        \"\"\"parse self._ontology_file into a graph\"\"\"\n        self._ontology_file = os.path.join(self.path, self._ontology_file)\n        self._ontology_nodes = self._schema_nodes()\n        return self._ontology\n\n    def _parse_node_list(", "solution": "\n        name, ext = os.path.splitext(self._ontology_file)\n        if ext in ['.ttl']:\n            self._ontology_parser_function = \\\n                lambda s: rdflib.Graph().parse(s, format='n3')\n        else:\n            self._ontology_parser_function = \\\n                lambda s: pyRdfa().graph_from_source(s)\n        if not self._ontology_parser_function:\n            raise ValueError(\n                \"No function found to parse ontology. %s\" %\n                self.errorstring_base)\n        if not self._ontology_file:\n            raise ValueError(\n                \"No ontology file specified. %s\" % self.errorstring_base)\n        if not self.lexicon:\n            raise ValueError(\n                \"No lexicon object assigned. %s\" % self.errorstring_base)\n\n        latest_file = self._read_schema()\n\n        try:\n            self.graph = self._ontology_parser_function(latest_file)\n        except:\n            raise IOError(\"Error parsing ontology at %s\" % latest_file)\n\n        for subj, pred, obj in self.graph:\n            self.ontology[subj].append((pred, obj))\n            yield (subj, pred, obj)"}
{"prompt": "def posterior_step(logposts, dim):\n    \"\"\"Finds the last time a chain made a jump > dim/2.\n\n    Parameters\n    ----------\n    logposts : array\n        1D array of values that are proportional to the log posterior values.\n    dim : int\n        The dimension of the parameter space.\n\n    Returns\n    -------\n    int\n        The index of the last time the logpost made a jump > dim/2. If that\n        never happened, returns 0.\n    \"\"\"", "output": "\n    return np.argmax(logposts - np.log(1.0 - np.exp(logposts[dim - 2])) - 1\n\n\ndef posterior_log_likelihood(log_likelihood, dim):\n    \"\"\"Finds the last time a chain made a jump > dim/2.\n\n    Parameters\n    ----------\n    log_likelihood : array\n        1D array of values that are proportional to the posterior log posterior\n        likelihood.\n    dim : int\n        The dimension of the parameter space.\n\n    Returns\n    -------\n    int\n        The index of the last time the loglikelihood made a jump > dim/2.", "solution": "\n    if logposts.ndim > 1:\n        raise ValueError(\"logposts must be a 1D array\")\n    criteria = dim/2.\n    dp = numpy.diff(logposts)\n    indices = numpy.where(dp >= criteria)[0]\n    if indices.size > 0:\n        idx = indices[-1] + 1\n    else:\n        idx = 0\n    return idx"}
{"prompt": "def get_param(self, number):\n        \"\"\"Reads an internal Server object parameter.\n        \"\"\"", "output": "\n        return self.server.get_param(number)\n\n    def get_param_list(self, number):\n        \"\"\"Reads an internal Server object parameter list.\n        \"\"\"\n        return self.server.get_param_list(number)\n\n    def get_param_map(self, number):\n        \"\"\"Reads an internal Server object parameter map.\n        \"\"\"\n        return self.server.get_param_map(number)\n\n    def get_param_map_list(self, number):\n        \"\"\"Reads an internal Server object parameter map list.\n        \"\"\"\n        return self.server.get_param_map_list", "solution": "\n        logger.debug(\"retreiving param number %s\" % number)\n        value = ctypes.c_int()\n        code = self.library.Srv_GetParam(self.pointer, number,\n                                         ctypes.byref(value))\n        check_error(code)\n        return value.value"}
{"prompt": "def balanced_accuracy(y_true, y_pred):\n    \"\"\"Default scoring function: balanced accuracy.\n\n    Balanced accuracy computes each class' accuracy on a per-class basis using a\n    one-vs-rest encoding, then computes an unweighted average of the class accuracies.\n\n    Parameters\n    ----------\n    y_true: numpy.ndarray {n_samples}\n        True class labels\n    y_pred: numpy.ndarray {n_samples}\n        Predicted class labels by the estimator\n\n    Returns\n    -------\n    fitness: float\n        Returns a float value indicating the individual's balanced accuracy\n        0.5 is as good as chance, and 1.0 is perfect predictive accuracy\n    \"\"\"", "output": ".\n    \"\"\"\n    return np.sum(np.equal(y_true, y_pred)) / float(len(y_true))\n\n\ndef balanced_precision(y_true, y_pred):\n    \"\"\"Default scoring function: balanced precision.\n\n    Balanced precision computes each class' precision on a per-class basis using a\n    one-vs-rest encoding, then computes an unweighted average of the class\n    accuracies.\n\n    Parameters\n    ----------\n    y_true: numpy.ndarray {n_samples}\n        True class labels\n    y_pred: numpy.ndarray {n", "solution": "\n    all_classes = list(set(np.append(y_true, y_pred)))\n    all_class_accuracies = []\n    for this_class in all_classes:\n        this_class_sensitivity = 0.\n        this_class_specificity = 0.\n        if sum(y_true == this_class) != 0:\n            this_class_sensitivity = \\\n                float(sum((y_pred == this_class) & (y_true == this_class))) /\\\n                float(sum((y_true == this_class)))\n\n            this_class_specificity = \\\n                float(sum((y_pred != this_class) & (y_true != this_class))) /\\\n                float(sum((y_true != this_class)))\n\n        this_class_accuracy = (this_class_sensitivity + this_class_specificity) / 2.\n        all_class_accuracies.append(this_class_accuracy)\n\n    return np.mean(all_class_accuracies)"}
{"prompt": "def storagehandler(self):\n        \"\"\"\n        Returns the storage handler available to thise actor.\n\n        :return: the storage handler, None if not available\n        \"\"\"", "output": "\n        return self._storagehandler\n\n    def set_storagehandler(self, storagehandler):\n        \"\"\"\n        Sets the storage handler available to thise actor.\n\n        :param storagehandler: the storage handler, None if not available\n        \"\"\"\n        self._storagehandler = storagehandler\n\n    def get_available_actions(self):\n        \"\"\"\n        Returns the available actions.\n\n        :return: the available actions, None if not available\n        \"\"\"\n        return self._availableactions\n\n    def get_actions(self):\n        \"\"\"\n        Returns the actions.\n\n        :return: the available actions, None if not available\n        \"\"\"\n        return self._actions\n\n   ", "solution": "\n        if isinstance(self, StorageHandler):\n            return self\n        elif self.parent is not None:\n            return self.parent.storagehandler\n        else:\n            return None"}
{"prompt": "def create_empty_crl(\n        ca_name,\n        cacert_path=None,\n        ca_filename=None,\n        crl_file=None,\n        digest='sha256'):\n    \"\"\"\n    Create an empty Certificate Revocation List.\n\n    .. versionadded:: 2015.8.0\n\n    ca_name\n        name of the CA\n    cacert_path\n        absolute path to ca certificates root directory\n    ca_filename\n        alternative filename for the CA\n\n        .. versionadded:: 2015.5.3\n\n    crl_file\n        full path to the CRL file\n\n    digest\n        The message digest algorithm. Must be a string describing a digest\n        algorithm supported by OpenSSL (by EVP_get_digestbyname, specifically).\n        For example, \"md5\" or \"sha1\". Default: 'sha256'\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' tls.create_empty_crl ca_name='koji' \\\n                ca_filename='ca' \\\n                crl_file='/etc/openvpn/team1/crl.pem'\n    \"\"\"", "output": " be one of:\n           'sha256','sha384','sha512','sha384-sha512','sha512-sha384',\n           'sha512-sha512','sha384-sha384'\n\n    \"\"\"\n    if crl_file is None:\n        crl_file = ca_name + '.crl'\n    if crl_filename is None:\n        crl_filename = ca_name + '.crl'\n    if digest =='sha256':\n        return _create_empty_crl_sha256(ca_name)\n    elif digest =='sha384':\n       ", "solution": "\n\n    set_ca_path(cacert_path)\n\n    if not ca_filename:\n        ca_filename = '{0}_ca_cert'.format(ca_name)\n\n    if not crl_file:\n        crl_file = '{0}/{1}/crl.pem'.format(\n            _cert_base_path(),\n            ca_name\n        )\n\n    if os.path.exists('{0}'.format(crl_file)):\n        return 'CRL \"{0}\" already exists'.format(crl_file)\n\n    try:\n        with salt.utils.files.fopen('{0}/{1}/{2}.crt'.format(\n                cert_base_path(),\n                ca_name,\n                ca_filename)) as fp_:\n            ca_cert = OpenSSL.crypto.load_certificate(\n                OpenSSL.crypto.FILETYPE_PEM,\n                fp_.read()\n            )\n        with salt.utils.files.fopen('{0}/{1}/{2}.key'.format(\n                cert_base_path(),\n                ca_name,\n                ca_filename)) as fp_:\n            ca_key = OpenSSL.crypto.load_privatekey(\n                OpenSSL.crypto.FILETYPE_PEM,\n                fp_.read()\n            )\n    except IOError:\n        return 'There is no CA named \"{0}\"'.format(ca_name)\n\n    crl = OpenSSL.crypto.CRL()\n    crl_text = crl.export(\n        ca_cert,\n        ca_key,\n        digest=salt.utils.stringutils.to_bytes(digest),\n    )\n\n    with salt.utils.files.fopen(crl_file, 'w') as f:\n        f.write(salt.utils.stringutils.to_str(crl_text))\n\n    return 'Created an empty CRL: \"{0}\"'.format(crl_file)"}
{"prompt": "def format_config_for_editing(data):\n    \"\"\"Formats configuration as YAML for human consumption.\n\n    :param data: configuration as nested dictionaries\n    :returns unicode YAML of the configuration\"\"\"", "output": "\n    return yaml.dump(data, default_flow_style=False)\n\n\ndef format_config_for_editing_with_options(data, options):\n    \"\"\"Formats configuration as YAML for human consumption.\n\n    :param data: nested dictionary\n    :param options: nested dictionary\n    :returns unicode YAML of the configuration\"\"\"\n    return yaml.dump(data, default_flow_style=False)\n\n\ndef format_config_for_editing_with_params(data, params):\n    \"\"\"Formats configuration as YAML for human consumption.\n\n    :param data: nested dictionary\n    :param params: nested", "solution": "\n    return yaml.safe_dump(data, default_flow_style=False, encoding=None, allow_unicode=True)"}
{"prompt": "def get_crime(self, persistent_id):\n        \"\"\"\n        Get a particular crime by persistent ID. Uses the outcomes-for-crime_\n        API call.\n\n        .. _outcomes-for-crime:\n            https://data.police.uk/docs/method/outcomes-for-crime/\n\n        :rtype: Crime\n        :param str persistent_id: The persistent ID of the crime to get.\n        :return: The ``Crime`` with the given persistent ID.\n        \"\"\"", "output": "\n        return self._get_crime(self.api_key, self.outcomes_for_crime_id, persistent_id)\n\n    def get_crime_by_id(self, crime_id):\n        \"\"\"\n        Get a particular crime by ID. Uses the outcomes-for-crime_\n        API call.\n\n       .. _crime-to-persistent-id:\n            https://data.police.uk/docs/method/crime-to-persistent-crime-api/\n\n        :rtype: Crime\n        :param str crime_id:", "solution": "\n\n        method = 'outcomes-for-crime/%s' % persistent_id\n        response = self.service.request('GET', method)\n        crime = Crime(self, data=response['crime'])\n        crime._outcomes = []\n        outcomes = response['outcomes']\n        if outcomes is not None:\n            for o in outcomes:\n                o.update({\n                    'crime': crime,\n                })\n                crime._outcomes.append(crime.Outcome(self, o))\n        return crime"}
{"prompt": "def readTFAM(fileName):\n    \"\"\"Reads the TFAM file.\n\n    :param fileName: the name of the ``tfam`` file.\n\n    :type fileName: str\n\n    :returns: a representation the ``tfam`` file (:py:class:`numpy.array`).\n\n    \"\"\"", "output": "\n    with open(fileName, 'rb') as f:\n        magicNumber, version, numSamples, numCols = struct.unpack('>ii', f.read(8))\n        if magicNumber!= MATRIX_MAGIC_NUMBER:\n            raise ValueError(\"Invalid file format\")\n        if version!= VERSION:\n            raise ValueError(\"Invalid file format\")\n        if numSamples!= 0:\n            if magicNumber == MATRIX_MAGIC_NUMBER:\n                numSamples = numCols * numSamples\n            else:\n                numSamples = numCols * numSamples + numCols * numCols\n        else:\n            raise ValueError(\"Invalid file format", "solution": "\n    # Saving the TFAM file\n    tfam = None\n    with open(fileName, 'r') as inputFile:\n        tfam = [\n            tuple(i.rstrip(\"\\r\\n\").split(\"\\t\")) for i in inputFile.readlines()\n        ]\n\n    tfam = np.array(tfam)\n\n    return tfam"}
{"prompt": "def interface_ip(interface):\n    \"\"\"Determine the IP assigned to us by the given network interface.\"\"\"", "output": "\n    return interface.get('ip_address') or ''\n\n\ndef interface_mac(interface):\n    \"\"\"Determine the MAC assigned to a network interface.\"\"\"\n    return interface.get('mac_address') or ''\n\n\ndef interface_list(interface):\n    \"\"\"Return a list of network interfaces.\"\"\"\n    return interface.get('networks', {}).keys()\n\n\ndef interface_info(interface):\n    \"\"\"Return a dict with detailed information about a network interface.\"\"\"\n    return {'id': interface_id(interface),\n            'name': interface_name(interface)}\n\n\ndef interface_list_filtered(interface_list", "solution": "\n    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    return socket.inet_ntoa(\n        fcntl.ioctl(\n            sock.fileno(), 0x8915, struct.pack('256s', interface[:15])\n        )[20:24]\n    )"}
{"prompt": "def coordination_geometry_symmetry_measures_sepplane_optim(self, coordination_geometry,\n                                                               points_perfect=None,\n                                                               nb_set=None, optimization=None):\n        \"\"\"\n        Returns the symmetry measures of a given coordination_geometry for a set of permutations depending on\n        the permutation setup. Depending on the parameters of the LocalGeometryFinder and on the coordination\n         geometry, different methods are called.\n        :param coordination_geometry: Coordination geometry for which the symmetry measures are looked for\n        :return: the symmetry measures of a given coordination_geometry for a set of permutations\n        :raise: NotImplementedError if the permutation_setup does not exists\n        \"\"\"", "output": "_geometry for a set of permutations\n        \"\"\"\n        if points_perfect is None:\n            points_perfect = self.points_perfect\n        if nb_set is None:\n            nb_set = self.nb_set\n        if optimization is None:\n            optimization = self.optimization\n        if points_perfect is None:\n            points_perfect = self.points_perfect\n        if nb_set is None:\n            nb_set = self.nb_set\n        if points_perfect.is_empty():\n            points_perfect = self.points_perfect\n        if nb_set >", "solution": "\n        csms = []\n        permutations = []\n        algos = []\n        local2perfect_maps = []\n        perfect2local_maps = []\n        for algo in coordination_geometry.algorithms:\n            if algo.algorithm_type == SEPARATION_PLANE:\n                cgsm = self.coordination_geometry_symmetry_measures_separation_plane_optim(\n                    coordination_geometry,\n                    algo,\n                    points_perfect=points_perfect,\n                    nb_set=nb_set,\n                    optimization=optimization)\n                csm, perm, algo, local2perfect_map, perfect2local_map = cgsm\n\n                csms.extend(csm)\n                permutations.extend(perm)\n                algos.extend(algo)\n                local2perfect_maps.extend(local2perfect_map)\n                perfect2local_maps.extend(perfect2local_map)\n        return csms, permutations, algos, local2perfect_maps, perfect2local_maps"}
{"prompt": "def _export_table(table, fname):\n    \"\"\"Export DataFrame to .csv\"\"\"", "output": "\n    if not isinstance(table, pd.DataFrame):\n        raise ValueError(\"table must be a pandas DataFrame\")\n    if fname.endswith(\".csv\"):\n        return table.to_csv(fname, index=False)\n    else:\n        raise ValueError(\"fname must end with.csv\")\n\n\ndef _export_dataframe(df, fname):\n    \"\"\"Export DataFrame to.csv\"\"\"\n    if not isinstance(df, pd.DataFrame):\n        raise ValueError(\"df must be a pandas DataFrame\")\n    if fname.endswith(\".csv\"):\n        return df.to_csv(fname, index=False)\n    else:\n        raise ValueError", "solution": "\n    import os.path as op\n    extension = op.splitext(fname.lower())[1]\n    if extension == '':\n        fname = fname + '.csv'\n    table.to_csv(fname, index=None, sep=',', encoding='utf-8',\n                 float_format='%.4f', decimal='.')"}
{"prompt": "def get_holidays(self, division=None, year=None):\n        \"\"\"\n        Gets a list of all known bank holidays, optionally filtered by division and/or year\n        :param division: see division constants; defaults to common holidays\n        :param year: defaults to all available years\n        :return: list of dicts with titles, dates, etc\n        \"\"\"", "output": "\n        if division is None:\n            division = self.division\n        if year is None:\n            year = self.year\n        if division == 'all':\n            return [\n                {\n                    'title': 'Banking',\n                    'date': date,\n                    'holiday_type': 'holiday',\n                    'type': 'title',\n                    'title_fr': title_fr,\n                    'title_en': title_en,\n                    'type_fr': 'type',\n                    'type_en': 'type',\n                    'type_en_fr': 'type',\n                    'holiday_type_fr': 'type", "solution": "\n        if division:\n            holidays = self.data[division]\n        else:\n            holidays = self.data[self.ENGLAND_AND_WALES]\n            dates_in_common = six.moves.reduce(\n                set.intersection,\n                (set(map(lambda holiday: holiday['date'], division_holidays))\n                 for division, division_holidays in six.iteritems(self.data))\n            )\n            holidays = filter(lambda holiday: holiday['date'] in dates_in_common, holidays)\n        if year:\n            holidays = filter(lambda holiday: holiday['date'].year == year, holidays)\n        return list(holidays)"}
{"prompt": "def add_all(self, items):\n        \"\"\"\n        Adds the elements in the specified collection to this queue.\n\n        :param items: (Collection), collection which includes the items to be added.\n        :return: (bool), ``true`` if this queue is changed after call, ``false`` otherwise.\n        \"\"\"", "output": "\n        return self._add_all(True, items)\n\n    def remove(self, items):\n        \"\"\"\n        Removes the elements in the specified collection from this queue.\n\n        :param items: (Collection), collection which includes the items to be removed.\n        :return: (bool), ``true`` if this queue is changed to the empty collection, ``false`` otherwise.\n        \"\"\"\n        return self._remove(True, items)\n\n    def size(self):\n        \"\"\"\n        Returns the number of elements in the queue.\n\n        :return: (int), number of elements in the queue.\n        \"\"\"\n        return self._size(", "solution": "\n        check_not_none(items, \"Value can't be None\")\n        data_items = []\n        for item in items:\n            check_not_none(item, \"Value can't be None\")\n            data_items.append(self._to_data(item))\n        return self._encode_invoke(queue_add_all_codec, data_list=data_items)"}
{"prompt": "def generate_slug(name):\n        \"\"\"Generate a slug for the knowledge.\n\n        :param name: text to slugify\n        :return: slugified text\n        \"\"\"", "output": "\n        slug = slugify(name)\n        if not slug:\n            return name\n        # We want to return the slugified name with the number of characters in it\n        slug = re.sub(r\"[^\\w\\s-]\", \"\", slug)\n        # We want to return the slugified name with the number of characters in it\n        slug = re.sub(r\"[-\\s]+\", \"\", slug)\n        # We want to return the slugified name with the number of characters in it\n        slug = re.sub(r\"[-\\s]+\", \"\", slug)\n        # We want to return the slugified name", "solution": "\n        slug = slugify(name)\n\n        i = KnwKB.query.filter(db.or_(\n            KnwKB.slug.like(slug),\n            KnwKB.slug.like(slug + '-%'),\n        )).count()\n\n        return slug + ('-{0}'.format(i) if i > 0 else '')"}
{"prompt": "def execute(self, eopatch):\n        \"\"\" Execute function which adds new vector layer to the EOPatch\n\n        :param eopatch: input EOPatch\n        :type eopatch: EOPatch\n        :return: New EOPatch with added vector layer\n        :rtype: EOPatch\n        \"\"\"", "output": "\n        if self.is_vector:\n            self.add_vector_layer()\n        else:\n            raise NotImplementedError()\n\n    def add_vector_layer(self):\n        \"\"\" Add new vector layer to the layer list\n\n        :return: New EOPatch with added vector layer\n        :rtype: EOPatch\n        \"\"\"\n        self.vector_layers.append(self.vector_layer)\n        self.vector_layers_layer_index = len(self.vector_layers) - 1\n\n    def add_vector_layer_from_file(self, filename):\n        \"\"\" Add new vector layer from a file\n\n        :", "solution": "\n        bbox_map = self._get_submap(eopatch)\n        height, width = self._get_shape(eopatch)\n        data_transform = rasterio.transform.from_bounds(*eopatch.bbox, width=width, height=height)\n\n        if self.feature_name in eopatch[self.feature_type]:\n            raster = eopatch[self.feature_type][self.feature_name].squeeze()\n        else:\n            raster = np.ones((height, width), dtype=self.raster_dtype) * self.no_data_value\n\n        if not bbox_map.empty:\n            rasterio.features.rasterize([(bbox_map.cascaded_union.buffer(0), self.raster_value)], out=raster,\n                                        transform=data_transform, dtype=self.raster_dtype)\n\n        eopatch[self.feature_type][self.feature_name] = raster[..., np.newaxis]\n\n        return eopatch"}
{"prompt": "def run_step_group(pipeline_definition, step_group_name, context):\n    \"\"\"Get the specified step group from the pipeline and run its steps.\"\"\"", "output": "\n    step_group = pipeline_domain.StepGroup.from_yaml(step_group_yaml)\n    return run_one_off_job(context, step_group)\n\n\ndef run_one_off_job(context, step_group):\n    \"\"\"Run the one-off MapReduce job.\"\"\"\n    job_id = model.Mapreduce.get_new_job_id()\n    logging.info('Running MapReduce job %s.', job_id)\n    job_class = model.MapreduceWorkItem\n\n    # Create a new job.\n    job_id = job_class(\n       ", "solution": "\n    logger.debug(f\"starting {step_group_name}\")\n    assert step_group_name\n\n    steps = get_pipeline_steps(pipeline=pipeline_definition,\n                               steps_group=step_group_name)\n\n    run_pipeline_steps(steps=steps, context=context)\n\n    logger.debug(f\"done {step_group_name}\")"}
{"prompt": "def catch_gzip_errors(f):\n    \"\"\"\n    A decorator to handle gzip encoding errors which have been known to\n    happen during hydration.\n    \"\"\"", "output": "\n    def wrapper(request, *args, **kwargs):\n        try:\n            return f(request, *args, **kwargs)\n        except Exception, e:\n            return HttpResponseServerError(e)\n        except Exception, e:\n            return HttpResponseServerError()\n    return wrapper\n\n\ndef catch_errors_gzip(f):\n    \"\"\"\n    A decorator to handle errors which have been known to\n    happen during hydration.\n    \"\"\"\n    def wrapper(request, *args, **kwargs):\n        try:\n            return f(request, *args, **kwargs)\n        except Exception, e:\n            return HttpResponseServerError(e)\n       ", "solution": "\n    def new_f(self, *args, **kwargs):\n        try:\n            return f(self, *args, **kwargs)\n        except requests.exceptions.ContentDecodingError as e:\n            log.warning(\"caught gzip error: %s\", e)\n            self.connect()\n            return f(self, *args, **kwargs)\n    return new_f"}
{"prompt": "def forms(self, req, tag):\n        \"\"\"\n        Make and return some forms, using L{self.parameter.getInitialLiveForms}.\n\n        @return: some subforms.\n        @rtype: C{list} of L{LiveForm}\n        \"\"\"", "output": "\n        return self.parameter.getInitialLiveForms(tag)\n\n\n\nclass IAuthenticator(Interface):\n    \"\"\"\n    An authentication plugin for HTTP Digest authentication.\n\n    @since: 10.2\n    \"\"\"\n    def requestAvatar(self, avatarId, mind, *interfaces):\n        \"\"\"\n        Request a C{avatarId} avatar.\n\n        @param avatarId: the C{avatarId} to request the avatar with.\n        @type avatarId: C{str}\n\n        @param mind: the authentication mechanism to use.\n        @type mind: L{ICredentials} provider\n\n        @param interfaces: a list", "solution": "\n        liveForms = self.parameter.getInitialLiveForms()\n        for liveForm in liveForms:\n            liveForm.setFragmentParent(self)\n        return liveForms"}
{"prompt": "def get_insider_roster(self):\r\n        \"\"\"Insider Roster\r\n\r\n        Returns the top 10 insiders, with the most recent information.\r\n\r\n        Reference: https://iexcloud.io/docs/api/#insider-roster\r\n\r\n        Data Weighting: ``5000`` per symbol\r\n\r\n        Returns\r\n        -------\r\n        list or pandas.DataFrame\r\n            Stocks Insider Roster Endpoint data\r\n        \"\"\"", "output": "\r\n        params = {'q': '1', 'type': 'roster', 'page': '1'}\r\n        return self.get_data(params)\r\n\r\n    def get_insider_time(self):\r\n        \"\"\"Insider Time\r\n\r\n        Returns the top 10 insiders, with the most recent information.\r\n\r\n        Reference: https://iexcloud.io/docs/api/#insider-time\r\n\r\n        Data Weighting: ``5000`` per symbol\r\n\r\n        Returns\r\n        -------\r\n        list or pandas.DataFrame\r\n            Stocks Insider Time data\r\n        \"\"\"\r\n        params = {'type':'symbol', 'page': '1'}\r\n       ", "solution": "\r\n        def fmt_p(out):\r\n            out = {(symbol, owner[\"entityName\"]): owner\r\n                   for symbol in out\r\n                   for owner in out[symbol]}\r\n            return pd.DataFrame(out)\r\n\r\n        return self._get_endpoint(\"insider-roster\", fmt_p=fmt_p)"}
{"prompt": "def create_intent(project_id, display_name, training_phrases_parts,\n                  message_texts):\n    \"\"\"Create an intent of the given intent type.\"\"\"", "output": "\n    intent_type = Intent.get_by_id(project_id, intent_type_id)\n    if not intent_type:\n        intent_type = Intent(\n            id=project_id,\n            display_name=display_name,\n            training_phrases_parts=training_phrases_parts,\n            message_phrases=message_phrases)\n        db.session.add(project_intent)\n        db.session.commit()\n    return intent_type.to_dict()\n\n\ndef create_question(question_text, author_id, content):\n    \"\"\"", "solution": "\n    import dialogflow_v2 as dialogflow\n    intents_client = dialogflow.IntentsClient()\n\n    parent = intents_client.project_agent_path(project_id)\n    training_phrases = []\n    for training_phrases_part in training_phrases_parts:\n        part = dialogflow.types.Intent.TrainingPhrase.Part(\n            text=training_phrases_part)\n        # Here we create a new training phrase for each provided part.\n        training_phrase = dialogflow.types.Intent.TrainingPhrase(parts=[part])\n        training_phrases.append(training_phrase)\n\n    text = dialogflow.types.Intent.Message.Text(text=message_texts)\n    message = dialogflow.types.Intent.Message(text=text)\n\n    intent = dialogflow.types.Intent(\n        display_name=display_name,\n        training_phrases=training_phrases,\n        messages=[message])\n\n    response = intents_client.create_intent(parent, intent)\n\n    print('Intent created: {}'.format(response))"}
{"prompt": "def parse_type_str(expected_base=None, with_arrlist=False):\n    \"\"\"\n    Used by BaseCoder subclasses as a convenience for implementing the\n    ``from_type_str`` method required by ``ABIRegistry``.  Useful if normalizing\n    then parsing a type string with an (optional) expected base is required in\n    that method.\n    \"\"\"", "output": "\n    if expected_base is None:\n        expected_base = expected_module + \".\" + options.module\n\n    # If the test is run from the command line, then return the parsed module.\n    if with_arrlist:\n        return [options.module] + _parse_arrlist(options.arrlist)\n    else:\n        return options.module\n\ndef _parse_arrlist(arrlist):\n    \"\"\"\n    Given a list of strings, parse out the elements of the array\n    \"\"\"\n    if with_arrlist:\n        return [x.strip() for x in arrlist.split(\",\")]\n   ", "solution": "\n    def decorator(old_from_type_str):\n        @functools.wraps(old_from_type_str)\n        def new_from_type_str(cls, type_str, registry):\n            normalized_type_str = normalize(type_str)\n            abi_type = parse(normalized_type_str)\n\n            type_str_repr = repr(type_str)\n            if type_str != normalized_type_str:\n                type_str_repr = '{} (normalized to {})'.format(\n                    type_str_repr,\n                    repr(normalized_type_str),\n                )\n\n            if expected_base is not None:\n                if not isinstance(abi_type, BasicType):\n                    raise ValueError(\n                        'Cannot create {} for non-basic type {}'.format(\n                            cls.__name__,\n                            type_str_repr,\n                        )\n                    )\n                if abi_type.base != expected_base:\n                    raise ValueError(\n                        'Cannot create {} for type {}: expected type with '\n                        \"base '{}'\".format(\n                            cls.__name__,\n                            type_str_repr,\n                            expected_base,\n                        )\n                    )\n\n            if not with_arrlist and abi_type.arrlist is not None:\n                raise ValueError(\n                    'Cannot create {} for type {}: expected type with '\n                    'no array dimension list'.format(\n                        cls.__name__,\n                        type_str_repr,\n                    )\n                )\n            if with_arrlist and abi_type.arrlist is None:\n                raise ValueError(\n                    'Cannot create {} for type {}: expected type with '\n                    'array dimension list'.format(\n                        cls.__name__,\n                        type_str_repr,\n                    )\n                )\n\n            # Perform general validation of default solidity types\n            abi_type.validate()\n\n            return old_from_type_str(cls, abi_type, registry)\n\n        return classmethod(new_from_type_str)\n\n    return decorator"}
{"prompt": "def read_calibration(\n        detx=None, det_id=None, from_file=False, det_id_table=None\n):\n    \"\"\"Retrive calibration from file, the DB.\"\"\"", "output": "\n    if detx is None:\n        detx = get_detx_from_file(from_file)\n    if det_id is None:\n        det_id = get_detx_from_file(from_file)\n    if det_id_table is None:\n        detx_id = detx.get_detx_id_table()\n    if detx_id is None:\n        raise ValueError(\"No detx id table found\")\n    if det_id is None:\n        raise ValueError(\"No detector id table found\")\n    if detx.is_detector_id:\n       ", "solution": "\n    from km3pipe.calib import Calibration    # noqa\n\n    if not (detx or det_id or from_file):\n        return None\n    if detx is not None:\n        return Calibration(filename=detx)\n    if from_file:\n        det_ids = np.unique(det_id_table)\n        if len(det_ids) > 1:\n            log.critical(\"Multiple detector IDs found in events.\")\n        det_id = det_ids[0]\n    if det_id is not None:\n        if det_id < 0:\n            log.warning(\n                \"Negative detector ID found ({0}). This is a MC \"\n                \"detector and cannot be retrieved from the DB.\".format(det_id)\n            )\n            return None\n        return Calibration(det_id=det_id)\n    return None"}
{"prompt": "def add_ip_range(self, id_environment, id_ip_config):\n        \"\"\"Makes relationship of environment with ip config and returns your id.\n\n        :param id_environment: Environment ID.\n        :param id_ip_config: IP Configuration ID.\n\n        :return: Following dictionary:\n\n        {'config_do_ambiente': {'id_config_do_ambiente': < id_config_do_ambiente >}}\n\n        :raise InvalidParameterError: Some parameter was invalid.\n        :raise ConfigEnvironmentDuplicateError: Error saving duplicate Environment Configuration.\n        :raise DataBaseError: Networkapi failed to access the database.\n        :raise XMLError: Networkapi failed to generate the XML response.\n        \"\"\"", "output": "Error: DataBase error.\n        \"\"\"\n        self._logger.debug(\"add_ip_range: id_environment=%s, id_ip_config=%s\", id_environment, id_ip_config)\n        self.id_environment[id_environment] = id_ip_config\n        return self.id_environment\n\n    def get_ip_range(self, id_ip_do):\n        \"\"\"Gets the relationship of the environment with ip do.\n\n        :param id_ip_do: Id of the relationship.\n\n        :return: Following dictionary:\n\n        {'do_id': {'id_", "solution": "\n\n        environment_map = dict()\n        environment_map['id_environment'] = id_environment\n        environment_map['id_ip_config'] = id_ip_config\n\n        code, xml = self.submit(\n            {'ambiente': environment_map}, 'POST', 'ipconfig/')\n\n        return self.response(code, xml)"}
{"prompt": "def _get_grad_method(self, data):\n        r\"\"\"Get the gradient\n\n        This method calculates the gradient step from the input data\n\n        Parameters\n        ----------\n        data : np.ndarray\n            Input data array\n\n        Notes\n        -----\n        Implements the following equation:\n\n        .. math::\n            \\nabla F(x) = \\mathbf{H}^T(\\mathbf{H}\\mathbf{x} - \\mathbf{y})\n\n        \"\"\"", "output": "\n        if self.grad_method is None:\n            self.grad_method = None\n            return None\n        else:\n            return self.grad_method\n\n    def _get_grad_method_grad(self, data):\n        r\"\"\"Calculate the gradient step\n\n        This method calculates the gradient step from the input data\n\n        Parameters\n        ----------\n        data : np.ndarray\n            Gradient data array\n\n        Notes\n        -----\n        Implements the following equation:\n\n       .. math::\n            \\nabla F(x) = \\mathbf{H}^T(\\mathbf{H}\\mathbf{x} - \\mathbf{y})\n\n        \"\"\"\n       ", "solution": "\n\n        self.grad = self.trans_op(self.op(data) - self.obs_data)"}
{"prompt": "def iri(uri_string):\n        \"\"\"converts a string to an IRI or returns an IRI if already formated\n\n        Args:\n            uri_string: uri in string format\n\n        Returns:\n            formated uri with <>\n        \"\"\"", "output": "\n        if not uri_string:\n            return None\n        return iri_from_uri(uri_string, self.iri_format)\n\n    def iri_format(self, uri):\n        \"\"\"converts a URI to a URI if already formated\n\n        Args:\n            uri: URI in string format\n\n        Returns:\n            formated URI with <>\n        \"\"\"\n        if not uri:\n            return None\n        return iri_from_uri(uri, self.iri_format)\n\n    def get_uri_path(self, uri):\n        \"\"\"returns a path for a URI if already formated\n\n        Args:\n            uri: URI in string", "solution": "\n        uri_string = str(uri_string)\n        if uri_string[:1] == \"?\":\n            return uri_string\n        if uri_string[:1] == \"[\":\n            return uri_string\n        if uri_string[:1] != \"<\":\n            uri_string = \"<{}\".format(uri_string.strip())\n        if uri_string[len(uri_string)-1:] != \">\":\n            uri_string = \"{}>\".format(uri_string.strip())\n        return uri_string"}
{"prompt": "def write_record(self, event_str):\n        \"\"\"Writes a serialized event to file.\"\"\"", "output": "\n        self.writer.write(self.encode_event(event_str))\n\n    def encode_event(self, event):\n        \"\"\"Encodes an event as a JSON string.\"\"\"\n        return json.dumps(event)\n\n    def decode_event(self, event_str):\n        \"\"\"Decodes an event as a string.\"\"\"\n        return json.loads(event_str)\n\n    def write_record_batch(self, batch):\n        \"\"\"Writes a batch of events to file.\"\"\"\n        self.writer.write(self.encode_event_batch(batch))\n\n    def encode_event_batch(self, batch", "solution": "\n        header = struct.pack('Q', len(event_str))\n        header += struct.pack('I', masked_crc32c(header))\n        footer = struct.pack('I', masked_crc32c(event_str))\n        self._writer.write(header + event_str + footer)"}
{"prompt": "def status_count(self, project):\n        \"\"\"\n        return a dict\n        \"\"\"", "output": "\n        return {\n            'projects': {\n                'total': 0,\n                'failed': 0,\n                'passed': 0,\n               'skipped': 0,\n                'failed_ratio': 0,\n                'passed_ratio': 0,\n                'failed_ratio': 0,\n               'skipped_ratio': 0,\n                'failed_ratio': 0,\n                'failed_ratio_ratio': 0,\n                'failed_ratio_ratio': 0,\n            },\n            'total': len(self.projects),\n            'failed': len(self.failed_projects),\n            'passed': len(self.passed_projects),", "solution": "\n        pipe = self.redis.pipeline(transaction=False)\n        for status in range(1, 5):\n            pipe.scard(self._gen_status_key(project, status))\n        ret = pipe.execute()\n\n        result = {}\n        for status, count in enumerate(ret):\n            if count > 0:\n                result[status + 1] = count\n        return result"}
{"prompt": "def gen_ascii_docs(src='fastai'):\n    \"\"\"Generate documentation for fastai library in HTML (asciidoctor required)\n    :param str src: The absolute/relative path of source file/dir\n    \"\"\"", "output": "\n    if src == 'fastai':\n        return \"\"\"\n            <html>\n                <head>\n                    <title>\n                        <h1>fastai</h1>\n                    </title>\n                </head>\n            </html>\n        \"\"\"\n    else:\n        return \"\"\"\n            <html>\n                <head>\n                    <title>\n                        <h1>fastai</h1>\n                    </title>\n                </head>\n            </html>\n        \"\"\"\n", "solution": "\n    os.chdir(Path(__file__).absolute().parent)\n    with working_directory('..'):\n        path = Path(src)\n        if path.is_dir():\n            file_paths = list(path.glob('**/*.py'))\n        else:\n            file_paths = [path]\n\n    pat = re.compile('^(?!__init__).*.py\\Z')\n    for file_path in file_paths:\n        if pat.match(file_path.name):\n            file_path.parent.mkdir(parents=True, exist_ok=True)\n            with working_directory('..'):\n                tmpl_str = parse_module(file_path)\n\n            (file_path.parent/(file_path.name.rsplit('.',1)[0] + '.adoc.tmpl')).write_text(tmpl_str)\n            (file_path.parent/(file_path.name.rsplit('.',1)[0] + '.adoc')).write_text(re.sub(r\"{{(.*?)}}\", parse_tmpl, tmpl_str, flags=re.DOTALL))\n    if path.is_dir():\n        subprocess.call(['asciidoctor', str(path) + '/**/*.adoc'])\n    else:\n        subprocess.call(['asciidoctor', str(path).rsplit('.',1)[0] + '.adoc'])"}
{"prompt": "def remove_users_from_user_group(self, id, **kwargs):  # noqa: E501\n        \"\"\"Remove multiple users from a specific user group  # noqa: E501\n\n          # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.remove_users_from_user_group(id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str id: (required)\n        :param list[str] body: List of users that should be removed from user group\n        :return: ResponseContainerUserGroup\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"", "output": ": (required)\n        :return: None\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_users_with_http_info(id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_users_with_http_info.metadata['url_map']['deleteUsersFromUserGroupRequest']  # noqa: E501\n            return_data = self._deserialize('str', body,'str')", "solution": "\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.remove_users_from_user_group_with_http_info(id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.remove_users_from_user_group_with_http_info(id, **kwargs)  # noqa: E501\n            return data"}
{"prompt": "def get_asset_content_form_for_update(self, asset_content_id=None):\n        \"\"\"Gets the asset form for updating content for an existing asset.\n\n        A new asset content form should be requested for each update\n        transaction.\n\n        :param asset_content_id: the ``Id`` of the ``AssetContent``\n        :type asset_content_id: ``osid.id.Id``\n        :return: the asset content form\n        :rtype: ``osid.repository.AssetContentForm``\n        :raise: ``NotFound`` -- ``asset_content_id`` is not found\n        :raise: ``NullArgument`` -- ``asset_content_id`` is ``null``\n        :raise: ``OperationFailed`` -- unable to complete request\n\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"", "output": "\n        :raise: ``NullArgument`` -- ``asset_content_id`` is ``null``\n        :raise: ``OperationFailed`` -- unable to complete request\n        :raise: ``PermissionDenied`` -- authorization failure\n\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n        return AssetContentForm\n\n    def can_delete_assets(self):\n        \"\"\"Tests if this user can delete ``Assets``.\n\n        A return of true does not guarantee successful authorization. A\n        return of false indicates that it is known deleting an ``Asset``\n        will result in a ``PermissionDenied``. This is intended as a\n        hint", "solution": "\n        if asset_content_id is None:\n            raise NullArgument()\n        asset = None\n        for a in AssetLookupSession(self._repository_id,\n                                    proxy=self._proxy,\n                                    runtime=self._runtime).get_assets():\n            # might want to set plenary view\n            # to assure ordering?\n            for ac in a.get_asset_contents():\n                if ac.get_id() == asset_content_id:\n                    asset = a\n                    asset_content = ac\n        if asset is None:\n            raise NotFound()\n        asset_content_form = objects.AssetContentForm(asset_content._my_map, asset_id=asset.get_id())\n        self._forms[asset_content_form.get_id().get_identifier()] = not UPDATED\n        return asset_content_form"}
{"prompt": "def start(self, argv):\n        \"\"\"\n        Start BenchExec.\n        @param argv: command-line options for BenchExec\n        \"\"\"", "output": "\n        self.parser.add_option('-c', '--config', action='store', type='string', dest='config',\n                               default=None,\n                               help='config file to use')\n        self.parser.add_option('-d', '--debug', action='store_true', dest='debug',\n                               default=False,\n                               help='debug mode')\n        self.parser.add_option('-e', '--exit', action='store', type='string', dest='exit',\n                               default=None,\n                               help='exit code')\n        self.parser.add_option('-f', '--force', action='store", "solution": "\n        parser = self.create_argument_parser()\n        self.config = parser.parse_args(argv[1:])\n\n        for arg in self.config.files:\n            if not os.path.exists(arg) or not os.path.isfile(arg):\n                parser.error(\"File {0} does not exist.\".format(repr(arg)))\n\n        if os.path.isdir(self.config.output_path):\n            self.config.output_path = os.path.normpath(self.config.output_path) + os.sep\n\n        self.setup_logging()\n\n        self.executor = self.load_executor()\n\n        returnCode = 0\n        for arg in self.config.files:\n            if self.stopped_by_interrupt:\n                break\n            logging.debug(\"Benchmark %r is started.\", arg)\n            rc = self.execute_benchmark(arg)\n            returnCode = returnCode or rc\n            logging.debug(\"Benchmark %r is done.\", arg)\n\n        logging.debug(\"I think my job is done. Have a nice day!\")\n        return returnCode"}
{"prompt": "def new_module(name):\n    \"\"\"\n    Do all of the gruntwork associated with creating a new module.\n\n    \"\"\"", "output": "\n    if name in sys.modules:\n        return sys.modules[name]\n    else:\n        mod = types.ModuleType(name)\n        mod.__loader__ = None\n        mod.__package__ = None\n        mod.__loader__ = None\n        mod.__path__ = []\n        return mod\n\ndef _run_code(code, mod, ns, ns_map, module_name):\n    \"\"\"\n    Run code in a namespace.\n\n    \"\"\"\n    # This function is called from _pytest.main()\n    if code.co_flags & inspect.CO_VARARGS:\n        return _pytest.runtest_inner", "solution": "\n    parent = None\n    if '.' in name:\n        parent_name = name.rsplit('.', 1)[0]\n        parent = __import__(parent_name, fromlist=[''])\n\n    module = imp.new_module(name)\n    sys.modules[name] = module\n    if parent:\n        setattr(parent, name.rsplit('.', 1)[1], module)\n    return module"}
{"prompt": "def _getPowerupInterfaces(self):\n        \"\"\"\n        Collect powerup interfaces this object declares that it can be\n        installed on.\n        \"\"\"", "output": "\n        return self._powerupInterfaces\n\n    def getPowerupInterfaces(self):\n        \"\"\"\n        Return a list of the powerup interfaces installed on this system.\n        \"\"\"\n        return self._powerupInterfaces.values()\n\n    def getPowerupTime(self):\n        \"\"\"\n        Return the time the powerup was installed.\n        \"\"\"\n        return self._powerupTime\n\n    def getPowerupTime(self):\n        \"\"\"\n        Return the time the powerup was started.\n        \"\"\"\n        return self._powerupTimeStart\n\n    def getPowerupTime(self):\n        \"\"\"\n        Return the time the powerup was started.\n        \"\"\"", "solution": "\n        powerupInterfaces = getattr(self.__class__, \"powerupInterfaces\", ())\n        pifs = []\n        for x in powerupInterfaces:\n            if isinstance(x, type(Interface)):\n                #just an interface\n                pifs.append((x, 0))\n            else:\n                #an interface and a priority\n                pifs.append(x)\n\n        m = getattr(self, \"__getPowerupInterfaces__\", None)\n        if m is not None:\n            pifs = m(pifs)\n            try:\n                pifs = [(i, p) for (i, p) in pifs]\n            except ValueError:\n                raise ValueError(\"return value from %r.__getPowerupInterfaces__\"\n                                 \" not an iterable of 2-tuples\" % (self,))\n        return pifs"}
{"prompt": "def create_non_data_file(self, params, file_data):\n        \"\"\"\n        Creates a new file-based dataset with the name provided in the files\n        tuple.  A valid file input would be:\n        files = (\n            {'file': (\"gtfs2\", open('myfile.zip', 'rb'))}\n        )\n        \"\"\"", "output": "\n        if not isinstance(params, dict):\n            raise ValueError(\"params must be a dict\")\n        if not isinstance(file_data, (Dataset, list)):\n            raise ValueError(\"file_data must be a list\")\n        if len(files) == 0:\n            raise ValueError(\"no files to create\")\n        if len(file_data) == 1:\n            file_data = file_data[0]\n        else:\n            raise ValueError(\"multiple files to create\")\n        return Dataset(file_data, params)\n\n    def create_invalid_file(self, file_data):\n        \"\"\"\n        Creates a new file", "solution": "\n        api_prefix = '/api/imports2/'\n\n        if not params.get('method', None):\n            params['method'] = 'blob'\n\n        return self._perform_request(\"post\", api_prefix, params=params, files=file_data)"}
{"prompt": "def active(self):\n        \"\"\" returns a dictionary of connections set as active.\n        \"\"\"", "output": "\n        return self._connections\n\n    def get_connection(self, key):\n        \"\"\" returns a connection set for the given key.\n        \"\"\"\n        return self._connections[key]\n\n    def get_connections(self, key):\n        \"\"\" returns a list of connections set as active.\n        \"\"\"\n        return self._connections.get(key, [])\n\n    def get_connection_by_id(self, key):\n        \"\"\" returns a connection with a given id.\n        \"\"\"\n        return self._connections[key]\n\n    def get_connection_by_id_and_name(self, key):\n        \"\"\" returns a connection with", "solution": "\n        return {key: value for key, value in self.conns.items()\n                if value.active}"}
{"prompt": "def normalize_package_path(cls, package_path):\n    \"\"\"Returns a normalized version of the given package path.\n\n    The root package might by denoted by '' or '.' and is normalized to ''.\n\n    All other packages are of the form 'path' or 'path/subpath', etc.\n\n    If the given path is either absolute or relative (includes the parent dir path signifier, '..'),\n    then an error is raised since the path cannot be noramlized.\n\n    :param string package_path: The Go package path to normalize.\n    :raises: `ValueError` if the package path cannot be normalized.\n    \"\"\"", "output": " normalized to a valid\n        version.\n    \"\"\"\n    if not package_path:\n        raise ValueError('Invalid Go package path: empty string')\n\n    if not os.path.isabs(package_path):\n        raise ValueError('Invalid Go package path: invalid path')\n\n    if not os.path.normpath(package_path).startswith(os.path.sep):\n        raise ValueError('Invalid Go package path: invalid path')\n\n    if not os.path.isdir(package_path):\n        raise ValueError('Invalid Go package path: invalid path')\n\n    if os.path.basename(package_path) == '':\n        raise ValueError", "solution": "\n    if package_path.startswith(os.pardir + os.sep):\n      raise ValueError('Relative package paths are not allowed. Given: {!r}'.format(package_path))\n    if os.path.isabs(package_path):\n      raise ValueError('Absolute package paths are not allowed. Given: {!r}'.format(package_path))\n    return '' if not package_path or package_path == os.curdir else package_path.lstrip('/')"}
{"prompt": "def _set_pwm(self, raw_values):\n        \"\"\"\n        Set pwm values on the controlled pins.\n\n        :param raw_values: Raw values to set (0-4095).\n        \"\"\"", "output": "\n        if raw_values == 0:\n            self.write_register(0x00)\n        elif raw_values == 4095:\n            self.write_register(0x01)\n        else:\n            raise ValueError(\"Invalid pwm value: %d\" % raw_values)\n\n    def _set_pwm_mode(self, mode):\n        \"\"\"\n        Set the PWM mode.\n\n        :param mode: PWM mode (0-4095).\n        \"\"\"\n        if mode == 0x00:\n            self.write_register(0x01)\n        elif mode == 0x01:\n            self.write", "solution": "\n        for i in range(len(self._pins)):\n            self._device.set_pwm(self._pins[i], 0, raw_values[i])"}
{"prompt": "def get(self, **kwargs):\n        \"\"\"Find correct filterchain based on generic variables\n        \"\"\"", "output": "\n        return self.filterchain\n\n    def _get_filter_chain(self):\n        \"\"\"Find the filter chain for the given filter\n        \"\"\"\n        return self.filterchain\n\n    def _get_filter_chain_id(self):\n        \"\"\"Find the filter chain ID for the given filter\n        \"\"\"\n        return self.filterchain_id\n\n    def _get_filter_chain_name(self):\n        \"\"\"Find the filter chain name for the given filter\n        \"\"\"\n        return self.filterchain_name\n\n    def _get_filter_chain_name_id(self):\n        \"\"\"Find the filter chain ID for the given", "solution": "\n        for chain in self.chains:\n            for key in kwargs:\n                getter_name = \"get_\"+key\n                # scan all possible getters\n                if (hasattr(chain, getter_name)):\n                    getter = getattr(chain, getter_name) # e.g. \"get_address\"\n                    if (getter() == kwargs[key]):\n                        return chain\n        return None"}
{"prompt": "def select_servers(self,\n                       selector,\n                       server_selection_timeout=None,\n                       address=None):\n        \"\"\"Return a list of Servers matching selector, or time out.\n\n        :Parameters:\n          - `selector`: function that takes a list of Servers and returns\n            a subset of them.\n          - `server_selection_timeout` (optional): maximum seconds to wait.\n            If not provided, the default value common.SERVER_SELECTION_TIMEOUT\n            is used.\n          - `address`: optional server address to select.\n\n        Calls self.open() if needed.\n\n        Raises exc:`ServerSelectionTimeoutError` after\n        `server_selection_timeout` if no matching servers are found.\n        \"\"\"", "output": "` on timeout.\n        \"\"\"\n        if server_selection_timeout is None:\n            server_selection_timeout = self.default_timeout\n        if address is None:\n            address = self.address\n        return [x for x in self.open(selector, server_selection_timeout)\n                if selector in x[0] and x[1] == x[2]]\n\n    def _select_servers(self,\n                       servers,\n                       server_selection_timeout=None,\n                       address=None):\n        \"\"\"Return a list of Servers matching selector, or time out.\n\n        :Parameters:\n          - `servers`: list", "solution": "\n        if server_selection_timeout is None:\n            server_timeout = self._settings.server_selection_timeout\n        else:\n            server_timeout = server_selection_timeout\n\n        with self._lock:\n            server_descriptions = self._select_servers_loop(\n                selector, server_timeout, address)\n\n            return [self.get_server_by_address(sd.address)\n                    for sd in server_descriptions]"}
{"prompt": "def convert2(self, imtls, sids):\n        \"\"\"\n        Convert a probability map into a composite array of shape (N,)\n        and dtype `imtls.dt`.\n\n        :param imtls:\n            DictArray instance\n        :param sids:\n            the IDs of the sites we are interested in\n        :returns:\n            an array of curves of shape (N,)\n        \"\"\"", "output": "\n        if not self.is_empty:\n            sids = {sid: sid.dt for sid in sids}\n        return imtls, sids\n\n    def convert_to_iid(self, iid, sids):\n        \"\"\"\n        Convert a probability map into a composite array of shape (N,)\n        and dtype `imtls.iid.dtype`.\n\n        :param iid:\n            DictArray instance\n        :param sids:\n            the IDs of the sites we are interested in\n        :returns:\n            an array of iid of dtype dtype (N,)\n        \"\"\"\n        if not self.is", "solution": "\n        assert self.shape_z == 1, self.shape_z\n        curves = numpy.zeros(len(sids), imtls.dt)\n        for imt in curves.dtype.names:\n            curves_by_imt = curves[imt]\n            for i, sid in numpy.ndenumerate(sids):\n                try:\n                    pcurve = self[sid]\n                except KeyError:\n                    pass  # the poes will be zeros\n                else:\n                    curves_by_imt[i] = pcurve.array[imtls(imt), 0]\n        return curves"}
{"prompt": "def release(self, shortname):\n        \"\"\"\n        Get a specific release by its shortname.\n\n        :param shortname: str, eg. \"ceph-3-0\"\n        :returns: deferred that when fired returns a Release (Munch, dict-like)\n                  object representing this release.\n        :raises: ReleaseNotFoundException if this release does not exist.\n        \"\"\"", "output": "\n        return self._release(shortname)\n\n    def _release(self, shortname):\n        \"\"\"\n        Get a specific release by its shortname.\n\n        :param shortname: str, eg. \"ceph-3-0\"\n        :returns: deferred that when fired returns a Release (Munch, dict-like)\n                  object representing this release.\n        :raises: ReleaseNotFoundException if this release does not exist.\n        \"\"\"\n        return defer.succeed(None)\n\n    def _get_release_by_id(self, release_id):\n        \"\"\"\n        Get a specific release by its ID.\n\n        :param release", "solution": "\n        url = 'api/v6/releases/?shortname=%s' % shortname\n        releases = yield self._get(url)\n        # Note, even if this shortname does not exist, _get() will not errback\n        # for this url. It simply returns an empty list. So check that here:\n        if not releases:\n            raise ReleaseNotFoundException('no release %s' % shortname)\n        release = Release.fromDict(releases[0])\n        release.connection = self\n        defer.returnValue(release)"}
{"prompt": "def WriteEventBody(self, event):\n    \"\"\"Writes the body of an event object to the output.\n\n    Args:\n      event (EventObject): event.\n    \"\"\"", "output": "\n    if not self.IsValidEvent(event):\n      return\n\n    self.WriteStruct(event.struct)\n\n    if self.IsValidEvent(event_data_stream.EventDataStream):\n      self.WriteDataStreamStartCode(event_data_stream.EventDataStream)\n\n    if self.IsValidEvent(event_data_stream.EventDataStream):\n      self.WriteDataStreamEndCode(event_data_stream.EventDataStream)\n\n    if self.IsValidEvent(event_data_stream.EventDataStream):\n      self.WriteDataStreamStartCode(event_data_", "solution": "\n    inode = getattr(event, 'inode', None)\n    if inode is None:\n      event.inode = 0\n\n    try:\n      message, _ = self._output_mediator.GetFormattedMessages(event)\n    except errors.WrongFormatter:\n      message = None\n\n    if message:\n      event.message = message\n\n    json_dict = self._JSON_SERIALIZER.WriteSerializedDict(event)\n    json_string = json.dumps(json_dict, sort_keys=True)\n    # dumps() returns an ascii-encoded byte string in Python 2.\n    if py2to3.PY_2:\n      json_string = codecs.decode(json_string, 'ascii')\n    self._output_writer.Write(json_string)\n    self._output_writer.Write('\\n')"}
{"prompt": "def to_naf(self):\n        \"\"\"\n        Converts the object to NAF\n        \"\"\"", "output": "\n        return NAF(self.name, self.value)\n\n    def __str__(self):\n        return self.to_naf()\n\n    def __repr__(self):\n        return self.to_naf()\n\nclass NAF(object):\n    \"\"\"\n    Represents a single NAF (NAF) object\n    \"\"\"\n    def __init__(self, name, value):\n        self.name = name\n        self.value = value\n        self.naf_type = None\n        self.naf_value = None\n\n    def __str__(self):\n        return self.to_naf()\n\n    def __", "solution": "\n        if self.type == 'KAF':\n            self.type = 'NAF'\n            for node in self.__get_wf_nodes():\n                node.set('id',node.get('wid'))\n                del node.attrib['wid']"}
{"prompt": "def main():\n    \"\"\"Return 0 on success.\"\"\"", "output": "\n    return 0\n\nif __name__ == '__main__':\n    sys.exit(main())\n", "solution": "\n    args = parse_args()\n\n    if not args.files:\n        return 0\n\n    with enable_sphinx_if_possible():\n        status = 0\n        pool = multiprocessing.Pool(multiprocessing.cpu_count())\n        try:\n            if len(args.files) > 1:\n                results = pool.map(\n                    _check_file,\n                    [(name, args) for name in args.files])\n            else:\n                # This is for the case where we read from standard in.\n                results = [_check_file((args.files[0], args))]\n\n            for (filename, errors) in results:\n                for error in errors:\n                    line_number = error[0]\n                    message = error[1]\n\n                    if not re.match(r'\\([A-Z]+/[0-9]+\\)', message):\n                        message = '(ERROR/3) ' + message\n\n                    output_message('{}:{}: {}'.format(filename,\n                                                      line_number,\n                                                      message))\n\n                    status = 1\n        except (IOError, UnicodeError) as exception:\n            output_message(exception)\n            status = 1\n\n        return status"}
{"prompt": "def resolve_placeholders(path, placeholder_dict):\n    \"\"\"\n    **Purpose**: Substitute placeholders in staging attributes of a Task with actual paths to the corresponding tasks.\n\n    :arguments:\n        :path: string describing the staging paths, possibly containing a placeholder\n        :placeholder_dict: dictionary holding the values for placeholders\n\n    \"\"\"", "output": "\n    if path is None:\n        return None\n    if path.startswith('http'):\n        return 'http://{}{}'.format(settings.SERVER_HOST, settings.SERVER_PORT, path)\n    if path.startswith('file'):\n        return 'file://{}{}'.format(settings.MEDIA_ROOT, path)\n    if path.startswith('http'):\n        return 'http://{}{}'.format(settings.SERVER_HOST, path)\n    if path.startswith('file'):\n        return 'file://{}{}'.format(settings.MEDIA_ROOT, path)\n    if path.startswith('file'):\n        return 'file://", "solution": "\n\n    try:\n\n        if isinstance(path, unicode):\n            path = str(path)\n\n        if not isinstance(path, str):\n            raise TypeError(expected_type=str, actual_type=type(path))\n\n        if '$' not in path:\n            return path\n\n        # Extract placeholder from path\n        if len(path.split('>')) == 1:\n            placeholder = path.split('/')[0]\n        else:\n            if path.split('>')[0].strip().startswith('$'):\n                placeholder = path.split('>')[0].strip().split('/')[0]\n            else:\n                placeholder = path.split('>')[1].strip().split('/')[0]\n\n        # SHARED\n        if placeholder == \"$SHARED\":\n            return path.replace(placeholder, 'pilot://')\n\n        # Expected placeholder format:\n        # $Pipeline_{pipeline.uid}_Stage_{stage.uid}_Task_{task.uid}\n\n        broken_placeholder = placeholder.split('/')[0].split('_')\n\n        if not len(broken_placeholder) == 6:\n            raise ValueError(\n                obj='placeholder',\n                attribute='task',\n                expected_value='$Pipeline_(pipeline_name)_Stage_(stage_name)_Task_(task_name) or $SHARED',\n                actual_value=broken_placeholder)\n\n        pipeline_name = broken_placeholder[1]\n        stage_name = broken_placeholder[3]\n        task_name = broken_placeholder[5]\n        resolved_placeholder = None\n\n        if pipeline_name in placeholder_dict.keys():\n            if stage_name in placeholder_dict[pipeline_name].keys():\n                if task_name in placeholder_dict[pipeline_name][stage_name].keys():\n                    resolved_placeholder = path.replace(placeholder, placeholder_dict[\n                                                        pipeline_name][stage_name][task_name]['path'])\n                else:\n                    logger.warning('%s not assigned to any task in Stage %s Pipeline %s' %\n                                   (task_name, stage_name, pipeline_name))\n            else:\n                logger.warning('%s not assigned to any Stage in Pipeline %s' % (\n                    stage_name, pipeline_name))\n        else:\n            logger.warning('%s not assigned to any Pipeline' % (pipeline_name))\n\n        if not resolved_placeholder:\n            logger.warning('No placeholder could be found for task name %s \\\n                        stage name %s and pipeline name %s. Please be sure to \\\n                        use object names and not uids in your references,i.e, \\\n                        $Pipeline_(pipeline_name)_Stage_(stage_name)_Task_(task_name)')\n            raise ValueError(\n                obj='placeholder',\n                attribute='task',\n                expected_value='$Pipeline_(pipeline_name)_Stage_(stage_name)_Task_(task_name) or $SHARED',\n                actual_value=broken_placeholder)\n\n        return resolved_placeholder\n\n    except Exception, ex:\n\n        logger.exception('Failed to resolve placeholder %s, error: %s' %(path, ex))\n        raise"}
{"prompt": "def unregister(self, entry_point):\n        \"\"\"Unregister a provider\n\n        :param str entry_point: provider to unregister (entry point syntax).\n\n        \"\"\"", "output": "\n        self.log.debug(\"Unregistering %s\", entry_point)\n        self.entries.remove(entry_point)\n        self.log.debug(\"Entry point %s removed\", entry_point)\n\n    def get_entry_point(self, entry_point):\n        \"\"\"Get a :class:`EntryPoint` for the given entry point\n\n        :param str entry_point: entry point syntax.\n\n        \"\"\"\n        return entry_point.copy()\n\n    def get_entry_map(self, entry_point):\n        \"\"\"Get a :class:`EntryPointMap` for the given entry point\n\n        :param str entry", "solution": "\n        if entry_point not in self.registered_extensions:\n            raise ValueError('Extension not registered')\n\n        ep = EntryPoint.parse(entry_point)\n        self.registered_extensions.remove(entry_point)\n        if self._extensions_by_name is not None:\n            del self._extensions_by_name[ep.name]\n        for i, ext in enumerate(self.extensions):\n            if ext.name == ep.name:\n                del self.extensions[i]\n                break"}
{"prompt": "def as_dict(self):\n        \"\"\"\n        Return a Listing object as Dictionary\n        :return: dict\n        \"\"\"", "output": "\n        return {\n            'id': self.id,\n            'name': self.name,\n            'description': self.description,\n            'created_at': self.created_at,\n            'updated_at': self.updated_at,\n            'created_by': self.created_by.name,\n            'project_id': self.project.id,\n           'status': self.status,\n            'type': self.type,\n            'user_id': self.user.id,\n            'project_id': self.project.id,\n            'domain_id': self.domain.id", "solution": "\n        return {\n            'search_type': self.search_type,\n            'agent_id': self.agent_id,\n            'id': self.id,\n            'price': self.price,\n            'price_change': self.price_change,\n            'viewings': self.upcoming_viewings,\n            'facilities': self.facilities,\n            'overviews': self.overviews,\n            'formalised_address': self.formalised_address,\n            'address_line_1': self.address_line_1,\n            'county': self.county,\n            'listing_image': self.images,\n            'listing_hires_image': self.hires_images,\n            'agent': self.agent,\n            'agent_url': self.agent_url,\n            'contact_number': self.contact_number,\n            'daft_link': self.daft_link,\n            'shortcode': self.shortcode,\n            'date_insert_update': self.date_insert_update,\n            'views': self.views,\n            'description': self.description,\n            'dwelling_type': self.dwelling_type,\n            'posted_since': self.posted_since,\n            'num_bedrooms': self.bedrooms,\n            'num_bathrooms': self.bathrooms,\n            'city_center_distance': self.city_center_distance,\n            'transport_routes': self.transport_routes,\n            'latitude': self.latitude,\n            'longitude': self.longitude,\n            'ber_code': self.ber_code,\n            'commercial_area_size': self.commercial_area_size\n        }"}
{"prompt": "def setOverlayAutoCurveDistanceRangeInMeters(self, ulOverlayHandle, fMinDistanceInMeters, fMaxDistanceInMeters):\n        \"\"\"\n        For high-quality curved overlays only, sets the distance range in meters from the overlay used to automatically curve\n        the surface around the viewer.  Min is distance is when the surface will be most curved.  Max is when least curved.\n        \"\"\"", "output": "\n        self.ulOverlayHandle = ulOverlayHandle\n        self.fMinDistanceInMeters = fMinDistanceInMeters\n        self.fMaxDistanceInMeters = fMaxDistanceInMeters\n\n    def setOverlayAutoCurveRadius(self, radius):\n        self.radius = radius\n\n    def setOverlayAutoCurveColor(self, color):\n        self.color = color\n\n    def setOverlayAutoCurveOpacity(self, opacity):\n        self.opacity = opacity\n\n    def setOverlayAutoCurveOpacityRange(self, fMinRange, fMaxRange):\n        \"\"\"\n        For high-quality curved overlays", "solution": "\n\n        fn = self.function_table.setOverlayAutoCurveDistanceRangeInMeters\n        result = fn(ulOverlayHandle, fMinDistanceInMeters, fMaxDistanceInMeters)\n        return result"}
{"prompt": "def delete_run():\n    \"\"\"\n    Delete the selected run from the database.\n    :return:\n    \"\"\"", "output": "\n    run_id = request.args.get('run_id')\n    if run_id is not None:\n        try:\n            run = Run.query.get(run_id)\n            if run is not None:\n                run.delete()\n                flash('Run deleted')\n            else:\n                flash('No run found')\n        except:\n            flash('No run found')\n    return redirect(url_for('run.index'))\n\n\n@blueprint.route('/run/<int:run_id>/delete', methods=['POST'])\n@login_required\ndef delete_run_by_id(run_", "solution": "\n    assert request.method == \"POST\", \"POST request expected received {}\".format(request.method)\n    if request.method == \"POST\":\n        try:\n            selections = json.loads(request.form[\"selections\"])\n            utils.drop_run(selections[\"project\"], selections[\"run\"])\n            return jsonify({\"response\": \"deleted {}\".format(selections[\"run\"])})\n        except Exception as e:\n            logging.error(e)\n    return jsonify({\"0\": \"__EMPTY\"})"}
{"prompt": "def create_block_storage(kwargs=None, call=None):\n    \"\"\"\n    Create a block storage\n    \"\"\"", "output": "\n    if not kwargs:\n        kwargs = {}\n    if not call:\n        call ='swift.obj.reconstructor.SsyncReconstructor'\n    return SsyncReconstructor(conf=None, **kwargs)\n\n\ndef get_db_connection(conf):\n    \"\"\"\n    Get a swift.common.db.Connection object\n    \"\"\"\n    return Connection(conf)\n\n\ndef get_container_memcache_key(conf, account, container):\n    \"\"\"\n    Get the memcache key for container\n    \"\"\"\n    return 'container/memcache/%s/%s' % (account, container)\n\n\ndef get_db_", "solution": "\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The create_block_storage function must be called with '\n            '-f or --function'\n        )\n\n    conn = get_conn()\n\n    # Assemble the composite block storage object.\n    block_storage = _get_block_storage(kwargs)\n\n    data = conn.create_block_storage(block_storage=block_storage)\n\n    return {'BlockStorage': data}"}
{"prompt": "def _correct_qualimap_genome_results(samples):\n    \"\"\" fixing java.lang.Double.parseDouble error on entries like \"6,082.49\"\n    \"\"\"", "output": "\n    if not isinstance(samples, list):\n        samples = [samples]\n    for sample in samples:\n        if sample.get(\"name\") == \"qualimap_genome\":\n            sample[\"name\"] = \"Double\"\n        if sample.get(\"name\") == \"genome\":\n            sample[\"name\"] = \"Double\"\n        if sample.get(\"name\") == \"genome_index\":\n            sample[\"name\"] = \"Double\"\n        if sample.get(\"name\") == \"genome_name\":\n            sample[\"name\"] = \"Double\"\n        if sample.get(\"name\") == \"genome_name_index\":", "solution": "\n    for s in samples:\n        if verify_file(s.qualimap_genome_results_fpath):\n            correction_is_needed = False\n            with open(s.qualimap_genome_results_fpath, 'r') as f:\n                content = f.readlines()\n                metrics_started = False\n                for line in content:\n                    if \">> Reference\" in line:\n                        metrics_started = True\n                    if metrics_started:\n                        if line.find(',') != -1:\n                            correction_is_needed = True\n                            break\n            if correction_is_needed:\n                with open(s.qualimap_genome_results_fpath, 'w') as f:\n                    metrics_started = False\n                    for line in content:\n                        if \">> Reference\" in line:\n                            metrics_started = True\n                        if metrics_started:\n                            if line.find(',') != -1:\n                                line = line.replace(',', '')\n                        f.write(line)"}
{"prompt": "def _find_lib_path():\n    \"\"\"Find mxnet library.\"\"\"", "output": "\n    if mxnet_home:\n        return mxnet_home\n    else:\n        return os.path.dirname(os.path.abspath(__file__))\n\n\ndef _find_data_path(data_dir=None):\n    \"\"\"Find data path.\"\"\"\n    if data_dir:\n        return os.path.join(data_dir, 'cifar-10-batches-py')\n    else:\n        return os.path.dirname(os.path.abspath(__file__))\n\n\ndef _get_data_path(data_dir=None):\n    \"\"\"Find data path.\"\"\"\n    if data_dir", "solution": "\n    curr_path = os.path.dirname(os.path.abspath(os.path.expanduser(__file__)))\n    amalgamation_lib_path = os.path.join(curr_path, '../../lib/libmxnet_predict.so')\n    if os.path.exists(amalgamation_lib_path) and os.path.isfile(amalgamation_lib_path):\n        lib_path = [amalgamation_lib_path]\n        return lib_path\n    else:\n        logging.info('Cannot find libmxnet_predict.so. Will search for MXNet library using libinfo.py then.')\n        try:\n            from mxnet.libinfo import find_lib_path\n            lib_path = find_lib_path()\n            return lib_path\n        except ImportError:\n            libinfo_path = os.path.join(curr_path, '../../python/mxnet/libinfo.py')\n            if os.path.exists(libinfo_path) and os.path.isfile(libinfo_path):\n                libinfo = {'__file__': libinfo_path}\n                exec(compile(open(libinfo_path, \"rb\").read(), libinfo_path, 'exec'), libinfo, libinfo)\n                lib_path = libinfo['find_lib_path']()\n                return lib_path\n            else:\n                raise RuntimeError('Cannot find libinfo.py at %s.' % libinfo_path)"}
{"prompt": "def simxStart(connectionAddress, connectionPort, waitUntilConnected, doNotReconnectOnceDisconnected, timeOutInMs, commThreadCycleInMs):\n    \"\"\"\n    Please have a look at the function description/documentation in the V-REP user manual\n    \"\"\"", "output": "\n    return lib.rep_simx_start(connectionAddress, connectionPort, waitUntilConnected, doNotReconnectOnceDisconnected, timeOutInMs, commThreadCycleInMs)\n\ndef simxStop(connectionAddress, waitUntilConnected, doNotReconnectOnceStopped, commThreadStopped):\n    \"\"\"\n    Please have a look at the function description/documentation in the V-REP user manual\n    \"\"\"\n    return lib.rep_simx_stop(connectionAddress, connectionPort, waitUntilConnected, doNotReconnectOnceStopped, timeOutInMs, commThreadStopped)\n\ndef sim", "solution": "\n\n    if (sys.version_info[0] == 3) and (type(connectionAddress) is str):\n        connectionAddress=connectionAddress.encode('utf-8')\n    return c_Start(connectionAddress, connectionPort, waitUntilConnected, doNotReconnectOnceDisconnected, timeOutInMs, commThreadCycleInMs)"}
{"prompt": "def _escape(value):\n        \"\"\"Escape a string (key or value) for InfluxDB's line protocol.\n\n        :param str|int|float|bool value: The value to be escaped\n        :rtype: str\n\n        \"\"\"", "output": "\n        if isinstance(value, bool):\n            return str(value)\n        else:\n            return value.replace('\\\\', '\\\\\\\\').replace('\"', '\\\\\"')\n\n    def _escape_string(value):\n        \"\"\"Escape a string (key or value) for InfluxDB's line protocol.\n\n        :param str|int|float|bool value: The value to be escaped\n        :rtype: str\n\n        \"\"\"\n        if isinstance(value, bool):\n            return value.replace('\\\\', '\\\\\\\\').replace('\"', '\\\\\"')\n        else:\n            return value.replace('\"', '\\\\\"')\n\n    def _escape_string_list(value):\n        \"\"\"Escape a string list", "solution": "\n        value = str(value)\n        for char, escaped in {' ': '\\ ', ',': '\\,', '\"': '\\\"'}.items():\n            value = value.replace(char, escaped)\n        return value"}
{"prompt": "def assert_not_in(obj, seq, message=None, extra=None):\n    \"\"\"Raises an AssertionError if obj is in iter.\"\"\"", "output": "\n    try:\n        iter(obj)\n    except AssertionError:\n        if message is not None:\n            raise AssertionError(message)\n        return\n    if message is None:\n        raise AssertionError(\"The truth value of an iterable is ambiguous.\")\n    if not seq:\n        raise AssertionError(\"The truth value of an iterable is empty sequence.\")\n    if not extra:\n        raise AssertionError(\"The truth value of an iterable is extra.\")\n    if not message:\n        raise AssertionError(\"The truth value of an iterable is empty message.\")\n    if not isinstance(seq, (str, bytes)):\n        raise AssertionError(\"The truth value of an iterable is not a string", "solution": "\n    # for very long strings, provide a truncated error\n    if isinstance(seq, six.string_types) and obj in seq and len(seq) > 200:\n        index = seq.find(obj)\n        start_index = index - 50\n        if start_index > 0:\n            truncated = \"(truncated) ...\"\n        else:\n            truncated = \"\"\n            start_index = 0\n        end_index = index + len(obj) + 50\n        truncated += seq[start_index:end_index]\n        if end_index < len(seq):\n            truncated += \"... (truncated)\"\n        assert False, _assert_fail_message(message, obj, truncated, \"is in\", extra)\n    assert obj not in seq, _assert_fail_message(message, obj, seq, \"is in\", extra)"}
{"prompt": "def group_by_key_impl(sequence):\n    \"\"\"\n    Implementation for group_by_key_t\n    :param sequence: sequence to group\n    :return: grouped sequence\n    \"\"\"", "output": "\n    return group_by_key(sequence, lambda x: x[0])\n\n\ndef group_by_key_impl_with_index(sequence, index):\n    \"\"\"\n    Implementation for group_by_key_t\n    :param sequence: sequence to group\n    :param index: index of the sequence\n    :return: grouped sequence with the index\n    \"\"\"\n    return group_by_key(sequence, lambda x: x[0][index])\n\n\ndef group_by_key_impl_with_index_and_count(sequence, count):\n    \"\"\"\n    Implementation for group_by_key", "solution": "\n    result = {}\n    for element in sequence:\n        if result.get(element[0]):\n            result.get(element[0]).append(element[1])\n        else:\n            result[element[0]] = [element[1]]\n    return six.viewitems(result)"}
{"prompt": "def lowerlim(x, y, z, a, b, c):\n    \"\"\"Returns the real positive root of\n      x/(a+t) + y/(b+t) + z/(c+t) = 1\n    when x/a + y/b + z/c > 1 else zero\n    \"\"\"", "output": "\n    return np.where(np.abs(x) < 1e-10, 1, 0)\n\n\ndef upperlim(x, y, z, a, b, c):\n    \"\"\"Returns the real negative root of\n      x/(a+t) + y/(b+t) + z/(c+t) = 1\n    when x/a + y/b + z/c > 1 else zero\n    \"\"\"\n    return np.where(np.abs(x) < 1e-10, 1, 0)\n\n\ndef log_likelihood(x, y, z, a, b", "solution": "\n    if x/a + y/b + z/c > 1:\n        B = a + b + c - x - y - z\n        C = a*b + a*c + b*c - a*y - a*z - b*x - b*z - c*x - c*y\n        D = a*b*c - a*b*z - a*c*y - b*c*x\n        r = np.roots([1,B,C,D])\n        ll = r[~np.iscomplex(r) & (r>0.)]\n        return ll[0].real\n    else:\n        return 0."}
{"prompt": "def amod(a, b):\n    \"\"\"Modulus function which returns numerator if modulus is zero\"\"\"", "output": "\n    return a * b == 0\n\n\ndef mod(a, b):\n    \"\"\"Modulus function which returns denominator if modulus is zero\"\"\"\n    return a * b == 0\n\n\ndef pow(a, b):\n    \"\"\"Raise the power of two\"\"\"\n    if a == 0:\n        return 0\n    if b == 0:\n        return 1\n    if mod(a, b) == 0:\n        return 0\n    return a * pow(b, a)\n\n\ndef pow_mod(a, b):\n    \"\"\"Raise the power of two\"\"\"\n    if a == 0:\n        return 0\n    if b ==", "solution": "\n    modded = int(a % b)\n    return b if modded is 0 else modded"}
{"prompt": "def from_where(cls, where):\n        \"\"\" Factory method for creating the top-level expression \"\"\"", "output": "\n        if isinstance(where, basestring):\n            where = cls.get_where_expression(where)\n        return cls(where)\n\n    @classmethod\n    def get_where_expression(cls, where):\n        \"\"\" Factory method for creating the top-level expression \"\"\"\n        if isinstance(where, cls.Expression):\n            return where.where\n        return None\n\n    @classmethod\n    def get_where_filters(cls, filters):\n        \"\"\" Factory method for creating the top-level filters \"\"\"\n        if isinstance(filters, basestring):\n            filters = cls.get_where_expression(filters)\n        return cls(filters)\n\n    @", "solution": "\n        if where.conjunction:\n            return Conjunction.from_clause(where)\n        else:\n            return cls.from_clause(where[0])"}
{"prompt": "def clean_text(value, topic=False):\n    \"\"\"\n    Replaces \"profane\" words with more suitable ones.\n    Uses bleach to strip all but whitelisted html.\n    Converts bbcode to Markdown\n    \"\"\"", "output": "\n    return bleach.clean(value, tags=[], strip=True, attributes=None,\n                         styles=['markdown.extensions.extra','markdown.extensions.codehilite'])\n", "solution": "\n    for x in PROFANITY_REPLACEMENTS:\n        value = value.replace(x[0], x[1])\n\n    for bbset in BBCODE_REPLACEMENTS:\n        p = re.compile(bbset[0], re.DOTALL)\n        value = p.sub(bbset[1], value)\n\n    bleached = bleach.clean(value, tags=ALLOWED_TAGS, attributes=ALLOWED_ATTRIBUTES, strip=True)\n\n    # We want to retain markdown quotes and we'll be running bleach again in format_post.\n    bleached = bleached.replace('&gt;', '>').replace('&amp;', '&')\n    return bleached"}
{"prompt": "def give_repr(cls):  # pragma: no cover\n    r\"\"\"Patch a class to give it a generic __repr__ method\n    that works by inspecting the instance dictionary.\n\n    Parameters\n    ----------\n    cls: type\n        The class to add a generic __repr__ to.\n\n    Returns\n    -------\n    cls: type\n        The passed class is returned\n    \"\"\"", "output": "\n    return cls.__repr__\n\n\ndef _check_option_forward(option, names):\n    \"\"\"Check whether forward (i.e. forward) option is valid.\"\"\"\n    if option not in names:\n        raise ValueError(f\"option {option} is not a valid option\")\n\n\ndef _check_option_reverse(option, names):\n    \"\"\"Check whether reverse (i.e. reverse) option is valid.\"\"\"\n    if option not in names:\n        raise ValueError(f\"option {option} is not a valid option\")\n\n\ndef _check_option_forward_reverse(option, names):", "solution": "\n    def reprer(self):\n        attribs = ', '.join([\"%s=%r\" % (k, v) for k, v in self.__dict__.items() if not k.startswith(\"_\")])\n        wrap = \"{self.__class__.__name__}({attribs})\".format(self=self, attribs=attribs)\n        return wrap\n    cls.__repr__ = reprer\n    return cls"}
{"prompt": "def bodvrd(bodynm, item, maxn):\n    \"\"\"\n    Fetch from the kernel pool the double precision values\n    of an item associated with a body.\n\n    http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/bodvrd_c.html\n\n    :param bodynm: Body name.\n    :type bodynm: str\n    :param item:\n                Item for which values are desired,\n                (\"RADII\", \"NUT_PREC_ANGLES\", etc.)\n    :type item: str\n    :param maxn: Maximum number of values that may be returned.\n    :type maxn: int\n    :return: tuple of (dim, values)\n    :rtype: tuple\n    \"\"\"", "output": " :param maxn:\n                Maximum number of double precision values to return.\n    :type maxn: int\n    :return:\n                Body name.\n    :rtype: str\n    \"\"\"\n    return item[1:maxn]\n\n\ndef bodvrd_c(bodynm, item, maxn):\n    \"\"\"\n    Fetch from the kernel pool the double precision values\n    of an item associated with a bodynm.\n\n    http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/bodvrd_c.html\n\n    :param b", "solution": "\n    bodynm = stypes.stringToCharP(bodynm)\n    item = stypes.stringToCharP(item)\n    dim = ctypes.c_int()\n    values = stypes.emptyDoubleVector(maxn)\n    maxn = ctypes.c_int(maxn)\n    libspice.bodvrd_c(bodynm, item, maxn, ctypes.byref(dim), values)\n    return dim.value, stypes.cVectorToPython(values)"}
{"prompt": "def compile_command(context, backend, config):\n    \"\"\"\n    Compile Sass project sources to CSS\n    \"\"\"", "output": "\n    if not config.debug:\n        return\n\n    if not context.config.get('css_path'):\n        return\n\n    if not context.config.get('css_path').endswith('.scss'):\n        context.config['css_path'] += '.scss'\n\n    context.log.info('Compiling %s', context.config['css_path'])\n\n    context.log.info('Writing %s', context.config['css_path'])\n    with open(context.config['css_path'], 'w') as f:\n        f.write(SassProjectConfiguration.get_template('css", "solution": "\n    logger = logging.getLogger(\"boussole\")\n    logger.info(u\"Building project\")\n\n    # Discover settings file\n    try:\n        discovering = Discover(backends=[SettingsBackendJson,\n                                         SettingsBackendYaml])\n        config_filepath, config_engine = discovering.search(\n            filepath=config,\n            basedir=os.getcwd(),\n            kind=backend\n        )\n\n        project = ProjectBase(backend_name=config_engine._kind_name)\n        settings = project.backend_engine.load(filepath=config_filepath)\n    except BoussoleBaseException as e:\n        logger.critical(six.text_type(e))\n        raise click.Abort()\n\n    logger.debug(u\"Settings file: {} ({})\".format(\n                 config_filepath, config_engine._kind_name))\n    logger.debug(u\"Project sources directory: {}\".format(\n                 settings.SOURCES_PATH))\n    logger.debug(u\"Project destination directory: {}\".format(\n                 settings.TARGET_PATH))\n    logger.debug(u\"Exclude patterns: {}\".format(\n                 settings.EXCLUDES))\n\n    # Find all sources with their destination path\n    try:\n        compilable_files = ScssFinder().mirror_sources(\n            settings.SOURCES_PATH,\n            targetdir=settings.TARGET_PATH,\n            excludes=settings.EXCLUDES\n        )\n    except BoussoleBaseException as e:\n        logger.error(six.text_type(e))\n        raise click.Abort()\n\n    # Build all compilable stylesheets\n    compiler = SassCompileHelper()\n    errors = 0\n    for src, dst in compilable_files:\n        logger.debug(u\"Compile: {}\".format(src))\n\n        output_opts = {}\n        success, message = compiler.safe_compile(settings, src, dst)\n\n        if success:\n            logger.info(u\"Output: {}\".format(message), **output_opts)\n        else:\n            errors += 1\n            logger.error(message)\n\n    # Ensure correct exit code if error has occured\n    if errors:\n        raise click.Abort()"}
{"prompt": "def initialize_parse_state(self, build_file):\n    \"\"\"Creates a fresh parse state for the given build file.\n\n    :param build_file: The BUILD file to set up a new ParseState for.\n    :type build_file: :class:`pants.base.build_file.BuildFile`\n    :returns: A fresh ParseState for parsing the given `build_file` with.\n    :rtype: :class:`BuildConfiguration.ParseState`\n    \"\"\"", "output": "\n    return BuildConfiguration.ParseState(build_file.address.spec_path,\n                                      build_file.address.spec_path,\n                                      self.build_configuration_path,\n                                      self.address.spec_path,\n                                      self.build_file_parser_path,\n                                      self.address.spec_path,\n                                      self.build_file_parser_parser_path,\n                                      self.address.spec_path,\n                                      self.build_file_parser_cls_path,\n                                      self.address.spec_path,\n                                      self.build_file_parser_cls_cls_path,\n                                      self", "solution": "\n    # TODO(John Sirois): Introduce a factory method to seal the BuildConfiguration and add a check\n    # there that all anonymous types are covered by context aware object factories that are\n    # Macro instances.  Without this, we could have non-Macro context aware object factories being\n    # asked to be a BuildFileTargetFactory when they are not (in SourceRoot registration context).\n    # See: https://github.com/pantsbuild/pants/issues/2125\n    type_aliases = self._exposed_object_by_alias.copy()\n    parse_context = ParseContext(rel_path=build_file.spec_path, type_aliases=type_aliases)\n\n    def create_call_proxy(tgt_type, tgt_alias=None):\n      def registration_callback(address, addressable):\n        parse_context._storage.add(addressable, name=address.target_name)\n      addressable_factory = self._get_addressable_factory(tgt_type, tgt_alias)\n      return AddressableCallProxy(addressable_factory=addressable_factory,\n                                  build_file=build_file,\n                                  registration_callback=registration_callback)\n\n    # Expose all aliased Target types.\n    for alias, target_type in self._target_by_alias.items():\n      proxy = create_call_proxy(target_type, alias)\n      type_aliases[alias] = proxy\n\n    # Expose aliases for exposed objects and targets in the BUILD file.\n    parse_globals = type_aliases.copy()\n\n    # Now its safe to add mappings from both the directly exposed and macro-created target types to\n    # their call proxies for context awares and macros to use to manufacture targets by type\n    # instead of by alias.\n    for alias, target_type in self._target_by_alias.items():\n      proxy = type_aliases[alias]\n      type_aliases[target_type] = proxy\n\n    for target_macro_factory in self._target_macro_factory_by_alias.values():\n      for target_type in target_macro_factory.target_types:\n        proxy = create_call_proxy(target_type)\n        type_aliases[target_type] = proxy\n\n    for alias, object_factory in self._exposed_context_aware_object_factory_by_alias.items():\n      parse_globals[alias] = object_factory(parse_context)\n\n    for alias, target_macro_factory in self._target_macro_factory_by_alias.items():\n      parse_globals[alias] = target_macro_factory.target_macro(parse_context)\n\n    return self.ParseState(parse_context, parse_globals)"}
{"prompt": "def full_like(a, **kwargs):\n    \"\"\"Create a filled array like `a`.\"\"\"", "output": "\n    kwargs.setdefault('subok', True)\n    return np.full_like(a, fill_value=a.dtype, **kwargs)\n\n\ndef full_like_function(a, **kwargs):\n    \"\"\"Create a filled function that accepts a variable number of arguments.\"\"\"\n    kwargs.setdefault('subok', True)\n    return np.full_like(a, fill_value=a.dtype, **kwargs)\n\n\ndef zeros_like(a, **kwargs):\n    \"\"\"Create a filled array of zeros with the same shape and type as a given array.\"\"\"\n    kwargs.setdefault('subok", "solution": "\n    _like_args(a, kwargs)\n    if isinstance(a, Array):\n        kwargs.setdefault('fill_value', a.fill_value)\n    return full(**kwargs)"}
{"prompt": "def get_payment_method(self, payment_method_id, **params):\n        \"\"\"https://developers.coinbase.com/api/v2#show-a-payment-method\"\"\"", "output": "\n        return self.api.get_payment_method(payment_method_id, **params)\n\n    def get_payment_method_by_id(self, payment_method_id):\n        \"\"\"https://developers.coinbase.com/api/v2#show-a-payment-method\"\"\"\n        return self.api.get_payment_method_by_id(payment_method_id)\n\n    def get_payment_method_history(self, payment_method_id, **params):\n        \"\"\"https://developers.coinbase.com/api/v2#show-a-payment-", "solution": "\n        response = self._get('v2', 'payment-methods', payment_method_id, params=params)\n        return self._make_api_object(response, PaymentMethod)"}
{"prompt": "def setup_endpoints(provider):\n    \"\"\"Setup the OpenID Connect Provider endpoints.\"\"\"", "output": "\n    def _get_endpoint(self):\n        \"\"\"Return the endpoint for the service provider.\"\"\"\n        return self.server.config.get('OPENID_SSO_ENDPOINT', self.server.config.get('OPENID_ENDPOINT', None))\n\n    def _set_endpoint(self, value):\n        \"\"\"Set the endpoint for the service provider.\"\"\"\n        self.server.config['OPENID_SSO_ENDPOINT'] = value\n\n    return OpenIDConnectProvider(_get_endpoint, _set_endpoint)\n", "solution": "\n    app_routing = {}\n    endpoints = [\n        AuthorizationEndpoint(\n            pyoidcMiddleware(provider.authorization_endpoint)),\n        TokenEndpoint(\n            pyoidcMiddleware(provider.token_endpoint)),\n        UserinfoEndpoint(\n            pyoidcMiddleware(provider.userinfo_endpoint)),\n        RegistrationEndpoint(\n            pyoidcMiddleware(provider.registration_endpoint)),\n        EndSessionEndpoint(\n            pyoidcMiddleware(provider.endsession_endpoint))\n    ]\n\n    for ep in endpoints:\n        app_routing[\"/{}\".format(ep.etype)] = ep\n\n    return app_routing"}
{"prompt": "def step(self, disable_interrupts=True, start=0, end=0):\n        \"\"\"\n        perform an instruction level step.  This function preserves the previous\n        interrupt mask state\n        \"\"\"", "output": "\n        self.write_register(self.cpu, self.register, self.cpu.read_int(start), self.cpu.read_int(end))\n\n    def execute(self):\n        \"\"\"\n        Execute the instruction.\n        \"\"\"\n        self.write_register(self.cpu, self.register, self.cpu.read_int(self.start), self.cpu.read_int(self.end))\n\n    def write_register(self, cpu, register, start, end):\n        \"\"\"\n        Write the register to the cpu\n        \"\"\"\n        cpu.write_int(self.register, start", "solution": "\n        # Was 'if self.get_state() != TARGET_HALTED:'\n        # but now value of dhcsr is saved\n        dhcsr = self.read_memory(CortexM.DHCSR)\n        if not (dhcsr & (CortexM.C_STEP | CortexM.C_HALT)):\n            logging.error('cannot step: target not halted')\n            return\n\n        self.notify(Notification(event=Target.EVENT_PRE_RUN, source=self, data=Target.RUN_TYPE_STEP))\n\n        self.clear_debug_cause_bits()\n\n        # Save previous interrupt mask state\n        interrupts_masked = (CortexM.C_MASKINTS & dhcsr) != 0\n\n        # Mask interrupts - C_HALT must be set when changing to C_MASKINTS\n        if not interrupts_masked and disable_interrupts:\n            self.write_memory(CortexM.DHCSR, CortexM.DBGKEY | CortexM.C_DEBUGEN | CortexM.C_HALT | CortexM.C_MASKINTS)\n\n        # Single step using current C_MASKINTS setting\n        while True:\n            if disable_interrupts or interrupts_masked:\n                self.write_memory(CortexM.DHCSR, CortexM.DBGKEY | CortexM.C_DEBUGEN | CortexM.C_MASKINTS | CortexM.C_STEP)\n            else:\n                self.write_memory(CortexM.DHCSR, CortexM.DBGKEY | CortexM.C_DEBUGEN | CortexM.C_STEP)\n\n            # Wait for halt to auto set (This should be done before the first read)\n            while not self.read_memory(CortexM.DHCSR) & CortexM.C_HALT:\n                pass\n\n            # Range is empty, 'range step' will degenerate to 'step'\n            if start == end:\n                break\n\n            # Read program counter and compare to [start, end)\n            program_counter = self.read_core_register(CORE_REGISTER['pc'])\n            if program_counter < start or end <= program_counter:\n                break\n\n            # Check other stop reasons\n            if self.read_memory(CortexM.DFSR) & (CortexM.DFSR_DWTTRAP | CortexM.DFSR_BKPT):\n                break\n\t\n        # Restore interrupt mask state\n        if not interrupts_masked and disable_interrupts:\n            # Unmask interrupts - C_HALT must be set when changing to C_MASKINTS\n            self.write_memory(CortexM.DHCSR, CortexM.DBGKEY | CortexM.C_DEBUGEN | CortexM.C_HALT)\n\n        self.flush()\n\n        self._run_token += 1\n\n        self.notify(Notification(event=Target.EVENT_POST_RUN, source=self, data=Target.RUN_TYPE_STEP))"}
{"prompt": "def startproject(project_name):\n    \"\"\"\n    build a full status project\n    \"\"\"", "output": "\n    # get the project\n    project = get_project(project_name)\n\n    # get the project\n    project_status = get_project_status(project)\n\n    # get the project status\n    if project_status == 'active':\n        project_status = 'inactive'\n    elif project_status == 'inactive':\n        project_status = 'active'\n\n    # get the project\n    project_name = project.name\n\n    # get the project\n    project_status = get_project_status(project)\n\n    # get the project name\n    if project_name == 'None':\n        print(\"No project name given,", "solution": "\n    # the destination path\n    dst_path = os.path.join(os.getcwd(), project_name)\n    start_init_info(dst_path)\n\n    # create dst path\n    _mkdir_p(dst_path)\n\n    # create project tree\n    os.chdir(dst_path)\n    # create files\n    init_code('manage.py', _manage_admin_code)\n    init_code('requirement.txt', _requirement_admin_code)\n    init_code('config.py', _config_sql_code)\n\n    # create app/\n    app_path = os.path.join(dst_path, 'app')\n    _mkdir_p(app_path)\n\n    # create files\n    os.chdir(app_path)\n    init_code('models.py', _models_admin_code)\n    init_code('__init__.py', _init_admin_code)\n\n    # create templates and static\n    css_path, templates_path = create_templates_static_files(app_path)\n    # create css files\n    os.chdir(css_path)\n    init_code('sign.css', _auth_login_css_code)\n\n    # create main blueprint\n    create_blueprint(\n        app_path,\n        'main',\n        _views_blueprint_code % ('main', 'main'),\n        _forms_basic_code,\n        templates_path\n    )\n\n    # create auth blueprint\n    auth_templates_path = create_blueprint(\n        app_path,\n        'auth',\n        _auth_views_code,\n        _auth_forms_code,\n        templates_path\n    )\n    # create auth templates files\n    os.chdir(auth_templates_path)\n    init_code('login.html', _auth_login_html_code)\n\n    # create admin site\n    admin_path = os.path.join(app_path, 'admin')\n    _mkdir_p(admin_path)\n\n    # create admin files\n    os.chdir(admin_path)\n    init_code('__init__.py', '')\n    init_code('views.py', _admin_views_code)\n\n    # create admin templates\n    os.chdir(templates_path)\n    admin_templates_path = os.path.join(templates_path, 'admin')\n    _mkdir_p(admin_templates_path)\n\n    # create admin templates files\n    os.chdir(admin_templates_path)\n    init_code('index.html', _admin_index_html_code)\n    init_code('logout.html', _admin_logout_html_code)\n\n    init_done_info()"}
{"prompt": "def signed_raw(self) -> str:\n        \"\"\"\n        Return Revocation signed raw document string\n\n        :return:\n        \"\"\"", "output": "\n        return self._signed_raw\n\n    @property\n    def signature(self) -> str:\n        \"\"\"\n        Return Signature document\n\n        :return:\n        \"\"\"\n        return self._signature\n\n    @property\n    def signature_type(self) -> str:\n        \"\"\"\n        Return Signature type document\n\n        :return:\n        \"\"\"\n        return self._signature_type\n\n    @property\n    def signature_id(self) -> str:\n        \"\"\"\n        Return Signature ID document\n\n        :return:\n        \"\"\"\n        return self._signature_id\n\n    @property\n    def signature_version(self) -> str:\n        \"\"\"\n        Return Signature version document\n\n        :return", "solution": "\n        if not isinstance(self.identity, Identity):\n            raise MalformedDocumentError(\"Can not return full revocation document created from inline\")\n\n        raw = self.raw()\n        signed = \"\\n\".join(self.signatures)\n        signed_raw = raw + signed + \"\\n\"\n        return signed_raw"}
{"prompt": "def load_de_novos(path, exclude_indels=True):\n    \"\"\" load mutations into dict indexed by HGNC ID.\n    \n    Args:\n        path: path to file containing de novo data. This should have five tab-\n            separated columns e.g.\n            hgnc  chr  pos      consequence         var_type\n            CTC1  17   8139190  missense_variant    snv\n            CTC1  17   8139191  frameshift_variant  indel\n        exclude_indels: True/False for whether we want to exclude indels. If we\n            are testing clustering of de novos, we can only use SNVs, but if we\n            are determining mutation rates for a gene, then we include indels,\n            in order to identify the transcripts that the de novos lie within.\n    \n    Returns:\n        dictionary of missense and lof counts for each gene, indexed by HGNC symbols\n    \"\"\"", "output": "ls.\n    \"\"\"\n    with open(path, 'r') as f:\n        for line in f:\n            if line.startswith('#'):\n                continue\n            if line.startswith('##'):\n                continue\n            if line.startswith('##contig'):\n                continue\n            if line.startswith('#var_type'):\n                continue\n            if line.startswith('#var'):\n                continue\n            if line.startswith('#var1'):\n                continue\n            if line.startswith('#var2'):\n                continue\n            if line.startswith('#var1'):\n                continue\n            if line.startswith('#var2'):\n                continue\n            if line.startswith('#var1'):\n               ", "solution": "\n    \n    genes = {}\n    with open(path, \"r\") as handle:\n        header = handle.readline().strip().split(\"\\t\")\n        for line in handle:\n            \n            line = line.rstrip().split(\"\\t\")\n            gene = line[0]\n            position = int(line[2]) - 1\n            consequence = line[3]\n            var_type = line[4]\n            \n            # ignore indels (some splice_acceptor_variants (in the\n            # functional_consequences) are indels\n            if exclude_indels and \"indel\" in var_type.lower():\n                continue\n            \n            # trim out variants that are missing data\n            if gene == \"\" or gene == \".\" or position == \"NA\":\n                continue\n            \n            if gene not in genes:\n                genes[gene] = {\"missense\": [], \"nonsense\": []}\n            \n            if consequence in missense:\n                genes[gene][\"missense\"].append(position)\n            elif consequence in lof:\n                genes[gene][\"nonsense\"].append(position)\n        \n    return genes"}
{"prompt": "def get_widget_css_class(self, attrs):\n        \"\"\"Gets the class for the widget.\"\"\"", "output": "\n        widget = super(BaseWidget, self).get_widget_class()\n        return widget.render_with_errors(attrs)\n\n    def get_widget_attrs(self, widget):\n        \"\"\"Gets the attributes for the widget.\"\"\"\n        attrs = super(BaseWidget, self).get_widget_attrs(widget)\n        attrs['class'] = self.get_css_class(attrs)\n        return attrs\n\n    def get_css_class(self, attrs):\n        \"\"\"Gets the class for the widget.\"\"\"\n        return self.get_widget_css_classes(attrs)\n\n    def get_css_classes", "solution": "\n        size_class = 'size-{0}'.format(self.num_inputs)\n\n        if 'class' in attrs:\n            attrs['class'] += ' {0}'.format(size_class)\n        else:\n            attrs['class'] = size_class"}
{"prompt": "def get_cpds(self):\n        \"\"\"\n        Adds tables to BIF\n\n        Returns\n        -------\n        dict: dict of type {variable: array}\n\n        Example\n        -------\n        >>> from pgmpy.readwrite import BIFReader, BIFWriter\n        >>> model = BIFReader('dog-problem.bif').get_model()\n        >>> writer = BIFWriter(model)\n        >>> writer.get_cpds()\n        {'bowel-problem': array([ 0.01,  0.99]),\n         'dog-out': array([ 0.99,  0.97,  0.9 ,  0.3 ,  0.01,  0.03,  0.1 ,  0.7 ]),\n         'family-out': array([ 0.15,  0.85]),\n         'hear-bark': array([ 0.7 ,  0.01,  0.3 ,  0.99]),\n         'light-on': array([ 0.6 ,  0.05,  0.4 ,  0.95])}\n        \"\"\"", "output": "  0.8,  0.7,  0.6,  0.5,  0.4,  0.3,  0.2,  0.1,  0.3]),\n         'pronoun': array([ 0.01,  0.99]),\n         'pronouns': array([ 0.97,  0.9,  0.8,  0.7,  0.6,  0.5,  0.4,  0.3,  0.2,  0.1,  0.3]),\n        ", "solution": "\n        cpds = self.model.get_cpds()\n        tables = {}\n        for cpd in cpds:\n            tables[cpd.variable] = cpd.values.ravel()\n        return tables"}
{"prompt": "def instruction_SBC(self, opcode, m, register):\n        \"\"\"\n        Subtracts the contents of memory location M and the borrow (in the C\n        (carry) bit) from the contents of the designated 8-bit register, and\n        places the result in that register. The C bit represents a borrow and is\n        set to the inverse of the resulting binary carry.\n\n        source code forms: SBCA P; SBCB P\n\n        CC bits \"HNZVC\": uaaaa\n        \"\"\"", "output": "\n        if opcode == 0x00:\n            m = (m & ~0x01)\n        elif opcode == 0x01:\n            m = (m & ~0x02)\n        elif opcode == 0x02:\n            m = (m & ~0x04) | (m & ~0x08)\n        elif opcode == 0x04:\n            m = (m & ~0x08) | (m & ~0x10)\n        elif opcode == 0x08:\n            m = (m & ~0x10) | (m & ~0x20)\n        elif opcode == 0x", "solution": "\n        a = register.value\n        r = a - m - self.C\n        register.set(r)\n#        log.debug(\"$%x %02x SBC %s: %i - %i - %i = %i (=$%x)\" % (\n#            self.program_counter, opcode, register.name,\n#            a, m, self.C, r, r\n#        ))\n        self.clear_NZVC()\n        self.update_NZVC_8(a, m, r)"}
{"prompt": "def policy(self):\n        \"\"\"Returns policy which contains this ioclass.\"\"\"", "output": "\n        return self._policy\n\n    @property\n    def name(self):\n        \"\"\"Returns the name of this IOClass.\"\"\"\n        return self._name\n\n    @property\n    def type(self):\n        \"\"\"Returns the IOClass type.\"\"\"\n        return self._type\n\n    @property\n    def description(self):\n        \"\"\"Returns the description of this IOClass.\"\"\"\n        return self._description\n\n    @property\n    def is_open(self):\n        \"\"\"Returns True if the IOClass is open.\"\"\"\n        return self._policy == 'policy'\n\n    @property\n    def policy_id(self):\n        \"\"\"Returns the policy ID.\"\"\"\n        return self._", "solution": "\n        policies = VNXIOPolicy.get(cli=self._cli)\n        ret = None\n        for policy in policies:\n            contained = policy.ioclasses.name\n            if self._get_name() in contained:\n                ret = VNXIOPolicy.get(name=policy.name, cli=self._cli)\n                break\n        return ret"}
{"prompt": "def classify_single_recording(raw_data_json, model_folder, verbose=False):\n    \"\"\"\n    Get the classification as a list of tuples. The first value is the LaTeX\n    code, the second value is the probability.\n    \"\"\"", "output": "\n    if verbose:\n        print(\"Classify single recording\")\n    return [(model_folder, 'latex'), (model_folder, 'probability')]\n\n\ndef classify_single_model(raw_data_json, model_folder, verbose=False):\n    \"\"\"\n    Get the classification as a list of tuples. The first value is the\n    LaTeX code, the second value is the probability.\n    \"\"\"\n    if verbose:\n        print(\"Classify single model\")\n    return [(model_folder,'model'), (model_folder, 'probability')]\n\n\ndef classify_single_recording(raw_data_json,", "solution": "\n    evaluation_file = evaluate_model(raw_data_json, model_folder, verbose)\n    with open(os.path.join(model_folder, \"info.yml\")) as ymlfile:\n        model_description = yaml.load(ymlfile)\n\n    index2latex = get_index2latex(model_description)\n\n    # Map line to probabilites for LaTeX commands\n    with open(evaluation_file) as f:\n        probabilities = f.read()\n    probabilities = map(float, probabilities.split(\" \"))\n    results = []\n    for index, probability in enumerate(probabilities):\n        results.append((index2latex[index], probability))\n    results = sorted(results, key=lambda n: n[1], reverse=True)\n    return results"}
{"prompt": "def perimeter(self):\n        \"\"\"\n        The total perimeter of the source segment, approximated lines\n        through the centers of the border pixels using a 4-connectivity.\n\n        If any masked pixels make holes within the source segment, then\n        the perimeter around the inner hole (e.g. an annulus) will also\n        contribute to the total perimeter.\n        \"\"\"", "output": "\n        return self.source.perimeter() + self.border.perimeter()\n\n    def line_strings(self):\n        \"\"\"\n        Return the list of lines that make up the source and border pixels\n        for a line.\n        \"\"\"\n        return self.source.line_strings() + [self.border.line_strings()]\n\n    def line_strings_with_holes(self):\n        \"\"\"\n        Return the list of lines, with holes for masked lines, if any\n        masked pixels make holes within the source and return as a single string.\n        \"\"\"\n        return self.line_strings() + self.line", "solution": "\n\n        if self._is_completely_masked:\n            return np.nan * u.pix  # unit for table\n        else:\n            from skimage.measure import perimeter\n            return perimeter(~self._total_mask, neighbourhood=4) * u.pix"}
{"prompt": "def unesc(line, language):\n    \"\"\"Uncomment once a commented line\"\"\"", "output": "\n    if line.startswith('#'):\n        return line[1:]\n    else:\n        return line\n\n\ndef parse_comment(line):\n    \"\"\"Parse a comment line\"\"\"\n    if line.startswith('#'):\n        return line[1:]\n    else:\n        return line\n\n\ndef parse_comment_line(line):\n    \"\"\"Parse a comment line\"\"\"\n    if line.startswith('#'):\n        return line[1:]\n    else:\n        return line\n\n\ndef parse_comment_line_with_comments(line):\n    \"\"\"Parse a comment line with comments\"\"\"\n    if line.startswith('#'):\n        return line[1:]", "solution": "\n    comment = _COMMENT[language]\n    if line.startswith(comment + ' '):\n        return line[len(comment) + 1:]\n    if line.startswith(comment):\n        return line[len(comment):]\n    return line"}
{"prompt": "def base_url(klass, space_id, parent_resource_id, resource_url='entries', resource_id=None, environment_id=None):\n        \"\"\"\n        Returns the URI for the snapshot.\n        \"\"\"", "output": "\n        if space_id is None:\n            space_id = ''\n        return '/%s/%s/%s/%s/%s' % (\n            parent_resource_url,\n            environment_id,\n            space_id,\n            space_id,\n            parent_resource_id,\n            resource_url\n        )\n\n    @classmethod\n    def _get_snapshot_uri(cls, space_id, parent_resource_id, resource_url='entries'):\n        \"\"\"\n        Returns the URI for the snapshot.\n        \"\"\"\n        if space_id is None:\n            space_id = ''\n        return '/%s/%s", "solution": "\n\n        return \"spaces/{0}{1}/{2}/{3}/snapshots/{4}\".format(\n            space_id,\n            '/environments/{0}'.format(environment_id) if environment_id is not None else '',\n            resource_url,\n            parent_resource_id,\n            resource_id if resource_id is not None else ''\n        )"}
{"prompt": "def centroid(self):\n        \"\"\"\n        The ``(y, x)`` coordinate of the centroid within the source\n        segment.\n        \"\"\"", "output": "\n        return self._centroid\n\n    @centroid.setter\n    def centroid(self, value):\n        self._centroid = value\n\n    @property\n    def z(self):\n        \"\"\"\n        The ``(z, y)`` coordinate of the centroid within the\n        source segment.\n        \"\"\"\n        return self._centroid.z\n\n    @centroid.setter\n    def z(self, value):\n        self._centroid.z = value\n\n    @property\n    def y(self):\n        \"\"\"\n        The ``(x, z)`` coordinate of the centroid within the\n        source segment.\n        \"\"\"\n        return self._centroid.y\n\n    @centroid.setter\n    def y(", "solution": "\n\n        ycen, xcen = self.cutout_centroid.value\n        return (ycen + self._slice[0].start,\n                xcen + self._slice[1].start) * u.pix"}
{"prompt": "def transpose_axes(image, axes, asaxes=None):\n    \"\"\"Return image with its axes permuted to match specified axes.\n\n    A view is returned if possible.\n\n    >>> transpose_axes(numpy.zeros((2, 3, 4, 5)), 'TYXC', asaxes='CTZYX').shape\n    (5, 2, 1, 3, 4)\n\n    \"\"\"", "output": "\n    image = numpy.asarray(image)\n    if asaxes is None:\n        asaxes = image.ndim\n    if asaxes not in _AXES2TUPLE and asaxes!= image.ndim:\n        raise ValueError('invalid transpose axes: %s' % (asaxes))\n    m = numpy.asarray(axes)\n    if m.ndim == 0:\n        m = numpy.expand_dims(image, axis=0)\n    else:\n        m = numpy.expand_dims(image, axis=1)\n    asaxes = asaxes or m[0]\n    if asaxes == 0:\n       ", "solution": "\n    for ax in axes:\n        if ax not in asaxes:\n            raise ValueError('unknown axis %s' % ax)\n    # add missing axes to image\n    if asaxes is None:\n        asaxes = 'CTZYX'\n    shape = image.shape\n    for ax in reversed(asaxes):\n        if ax not in axes:\n            axes = ax + axes\n            shape = (1,) + shape\n    image = image.reshape(shape)\n    # transpose axes\n    image = image.transpose([axes.index(ax) for ax in asaxes])\n    return image"}
{"prompt": "def generate_entities_doc(ctx, out_path, package):\n    \"\"\"Create entities documentation from Canari python classes file.\"\"\"", "output": "\n    # TODO: add a test for this\n    package_path = os.path.join(out_path, package + '.py')\n    package_name = package.replace('.', '/')\n    ctx.env.temp_data['package'] = package_name\n    ctx.env.temp_data['package_path'] = package_path\n    ctx.env.temp_data['package_name'] = package_name\n    ctx.env.temp_data['package'] = package_name\n    ctx.env.temp_data['package_path'] = package_path\n    ctx.env.temp_data", "solution": "\n    from canari.commands.generate_entities_doc import generate_entities_doc\n    generate_entities_doc(ctx.project, out_path, package)"}
{"prompt": "def add_path_part(url, regex=PATH_PART):\n    \"\"\"\n    replace the variables in a url template with regex named groups\n    :param url: string of a url template\n    :param regex: regex of the named group\n    :returns: regex\n    \"\"\"", "output": "\n    if not url:\n        return regex\n\n    if not isinstance(url, str):\n        return regex\n\n    if not regex.match(url):\n        return regex\n\n    groups = re.findall(regex.pattern, url)\n    if groups:\n        for group in groups:\n            if group[0] == \"(?P<group_name>(?:\" + group[1] + \")*)\":\n                group_name = group[1] + \" \" + group_name\n                group_name = group_name.replace(\"(?P<group_name>(?:\" + group[1] + \")*)\",\"", "solution": "\n    formatter = string.Formatter()\n    url_var_template = \"(?P<{var_name}>{regex})\"\n\n    for part in formatter.parse(url):\n        string_part, var_name, _, _ = part\n        if string_part:\n            yield string_part\n        if var_name:\n            yield url_var_template.format(var_name=var_name, regex=regex)"}
{"prompt": "def _fix_review_dates(self, item):\n        \"\"\"Convert dates so ES detect them\"\"\"", "output": "\n        if not item.get('date'):\n            return item\n\n        if isinstance(item['date'], datetime.datetime):\n            return item['date'].strftime('%Y-%m-%d')\n\n        return item\n\n    def _get_date_range(self, item):\n        \"\"\"Convert dates to be in the format YYYY-MM-DD\"\"\"\n        if not item.get('date'):\n            return item\n\n        if isinstance(item['date'], datetime.datetime):\n            return item['date'].strftime('%Y-%m-%d')\n\n        return item\n\n    def _get_date_range_end(self, item):\n        \"\"\"Convert", "solution": "\n\n        for date_field in ['timestamp', 'createdOn', 'lastUpdated']:\n            if date_field in item.keys():\n                date_ts = item[date_field]\n                item[date_field] = unixtime_to_datetime(date_ts).isoformat()\n\n        if 'patchSets' in item.keys():\n            for patch in item['patchSets']:\n                pdate_ts = patch['createdOn']\n                patch['createdOn'] = unixtime_to_datetime(pdate_ts).isoformat()\n\n                if 'approvals' in patch:\n                    for approval in patch['approvals']:\n                        adate_ts = approval['grantedOn']\n                        approval['grantedOn'] = unixtime_to_datetime(adate_ts).isoformat()\n\n        if 'comments' in item.keys():\n            for comment in item['comments']:\n                cdate_ts = comment['timestamp']\n                comment['timestamp'] = unixtime_to_datetime(cdate_ts).isoformat()"}
{"prompt": "def _query_label(self, loc):\n        \"\"\"Query the label for the item with index `loc`. Preferentially\n        queries the label from the cache, but if not yet cached, queries the\n        oracle.\n\n        Returns\n        -------\n        int\n            the true label \"0\" or \"1\".\n        \"\"\"", "output": "\n        if loc not in self._cache:\n            self._cache[loc] = self._query_label_cache(loc)\n        return self._cache[loc].get_label()\n\n    def _query_label_cache(self, loc):\n        \"\"\"Return the label for the item with index `loc`.\n\n        Parameters\n        ----------\n        loc : str\n            the label for the cache.\n\n        Returns\n        -------\n        int\n            the true label \"0\" or \"1\".\n        \"\"\"\n        if loc not in self._label_cache:\n            self._label_cache[loc] = self._query_label_cache(", "solution": "\n        # Try to get label from cache\n        ell = self.cached_labels_[loc]\n\n        if np.isnan(ell):\n            # Label has not been cached. Need to query oracle\n            oracle_arg = self.identifiers[loc]\n            ell = self.oracle(oracle_arg)\n            if ell not in [0, 1]:\n                raise Exception(\"Oracle provided an invalid label.\")\n            #TODO Gracefully handle errors from oracle?\n            self._queried_oracle[self.t_] = True\n            self.cached_labels_[loc] = ell\n\n        return ell"}
{"prompt": "def update_events(self, events_data: List[Tuple[str, int]]) -> None:\n        \"\"\"Given a list of identifier/data event tuples update them in the DB\"\"\"", "output": "\n        for data in events_data:\n            if data[0] not in self.events:\n                self.events[data[0]] = []\n            self.events[data[0]].append(data[1])\n\n    def get_event_by_id(self, event_id: int) -> Tuple[str, int]:\n        \"\"\"Given a list of identifier/data tuples get the event name and data\"\"\"\n        return self.events[event_id]\n\n    def get_event_by_name(self, event_name: str) -> Tuple[str, int]:\n        \"\"\"Given a list", "solution": "\n        cursor = self.conn.cursor()\n        cursor.executemany(\n            'UPDATE state_events SET data=? WHERE identifier=?',\n            events_data,\n        )\n        self.maybe_commit()"}
{"prompt": "def _handle_info(self, *args, **kwargs):\n        \"\"\"\n        Handles info messages and executed corresponding code\n        \"\"\"", "output": "\n        if self.code == 'info':\n            self.code = 'info'\n            self.msg = 'info'\n            self.msg_type = 'info'\n        elif self.code == 'execute':\n            self.code = 'execute'\n            self.msg = 'execute'\n            self.msg_type = 'execute'\n        elif self.code == 'exit':\n            self.code = 'exit'\n            self.msg = 'exit'\n            self.msg_type = 'exit'\n        else:\n            self.msg = 'Unknown code: %s' % self.code\n            self.msg", "solution": "\n        if 'version' in kwargs:\n            # set api version number and exit\n            self.api_version = kwargs['version']\n            print(\"Initialized API with version %s\" % self.api_version)\n            return\n        try:\n            info_code = str(kwargs['code'])\n        except KeyError:\n            raise FaultyPayloadError(\"_handle_info: %s\" % kwargs)\n\n        if not info_code.startswith('2'):\n            raise ValueError(\"Info Code must start with 2! %s\", kwargs)\n\n        output_msg = \"_handle_info(): %s\" % kwargs\n        log.info(output_msg)\n\n        try:\n            self._code_handlers[info_code]()\n        except KeyError:\n            raise UnknownWSSInfo(output_msg)"}
{"prompt": "def _getWorkerCommandList(self):\n        \"\"\"Generate the workerCommand as list\"\"\"", "output": "\n        return [self.workerCommand, self.workerCommand]\n\n    def _getWorkerCommand(self, command):\n        \"\"\"Return the command to run the worker\"\"\"\n        return self.workerCommands[command]\n\n    def _getWorkerCommandList(self):\n        \"\"\"Generate the workerCommand as list\"\"\"\n        return [self.workerCommands['getWorkerCommand'](i) for i in self.workerCommands]\n\n    def _getWorkerCommand(self, command):\n        \"\"\"Return the command to run the worker\"\"\"\n        return self.workerCommands[command]\n\n    def _getWorkerCommandList(self):\n        \"\"\"Generate the", "solution": "\n        c = []\n        c.extend(self._WorkerCommand_environment())\n        c.extend(self._WorkerCommand_launcher())\n        c.extend(self._WorkerCommand_options())\n        c.extend(self._WorkerCommand_executable())\n\n        return c"}
{"prompt": "def _GetFileNames(self):\n    \"\"\"Returns a list of file names in the feed.\"\"\"", "output": "\n    return self._feed_names\n\n  def _GetFeedNames(self):\n    \"\"\"Returns a list of feed names in the feed.\"\"\"\n    return self._feed_names\n\n\nclass _FetchMapper(object):\n  \"\"\"Fetch mapper for fetching values from a database.\n\n  Fetches values from a database such as the entity id, and all properties in the\n  database.  This is used to fetch the entity's properties from the database.\n  \"\"\"\n\n  def __init__(self, db, map_func):\n    \"\"\"Constructor.\n\n    Args:\n      db: A database name (string) for the datastore.\n      map_", "solution": "\n    if self._zip:\n      return self._zip.namelist()\n    else:\n      return os.listdir(self._path)"}
{"prompt": "def git_status(self, r):\n        \"\"\"return tuple of (status, adds, dels).\n        status like !! ??\n        adds and dels are lists of additions and deletions.\n        \"\"\"", "output": "\n        if r.status_code == 200:\n            return r.json(), r.headers\n        else:\n            return r.json(), r.headers\n\n    def get_user_info(self, username):\n        \"\"\"return dict of user info.\n        username like!!??\n        \"\"\"\n        return {\n            'username': username,\n            'email': username + '@example.com',\n            'id': username,\n            'name': username,\n            'avatar': self.avatar,\n            'avatar_url': self.avatar_url,\n            'url': 'https://api.github.com/users/%s", "solution": "\n        ret = self._cachedStatus.get(r.filename, None) if r else None\n        return ret if ret else [\"//\", None, None]"}
{"prompt": "def attach_vpn_gateway(self, vpn_gateway_id, vpc_id):\n        \"\"\"\n        Attaches a VPN gateway to a VPC.\n\n        :type vpn_gateway_id: str\n        :param vpn_gateway_id: The ID of the vpn_gateway to attach\n\n        :type vpc_id: str\n        :param vpc_id: The ID of the VPC you want to attach the gateway to.\n\n        :rtype: An attachment\n        :return: a :class:`boto.vpc.vpngateway.Attachment`\n        \"\"\"", "output": "\n        params = {'Action': 'AttachVpnGateway',\n                  'VpnGatewayId': vpn_gateway_id,\n                  'VpcId': vpc_id,\n                  'Type': 'VPNC',\n                  'VpnGatewayAttachmentId': self.id}\n\n        response = self.connection.make_request('AttachVpnGateway', params)\n        attachment = response.get('Attachment', [])\n\n        for el in attachment:\n            if el.get('IsAttachable') == 'true':\n                return el\n        return None\n\n    def detach_vpn_gateway(self):\n        \"\"\"\n        Detaches a VPN connection from a VPC.", "solution": "\n        params = {'VpnGatewayId': vpn_gateway_id,\n                  'VpcId' : vpc_id}\n        return self.get_object('AttachVpnGateway', params, Attachment)"}
{"prompt": "def get_check_threads(self):\n        \"\"\"Return iterator of checker threads.\"\"\"", "output": "\n        for checker in self.checkers:\n            if checker.is_enabled():\n                yield checker\n\n    def get_checkers(self):\n        \"\"\"Return iterator of registered checkers.\"\"\"\n        return self.checkers\n\n    def get_checkers_for_file(self, filename):\n        \"\"\"Return a list of registered checkers for the given filename.\"\"\"\n        if self.is_enabled(filename):\n            yield self.get_checkers_for_enabled(filename)\n        if self.is_disabled(filename):\n            yield self.get_checkers_for_disabled(filename)\n        if self.is_enabled(filename):\n           ", "solution": "\n        for t in self.threads:\n            name = t.getName()\n            if name.startswith(\"CheckThread-\"):\n                yield name"}
{"prompt": "def solve(self):\n        \"\"\"\n        Solve the cross.\n        \"\"\"", "output": "\n        self.set_parameters(self.parameters)\n        self.set_parameters(self.parameters)\n        self.set_parameters(self.parameters)\n        self.set_parameters(self.parameters)\n        self.set_parameters(self.parameters)\n        self.set_parameters(self.parameters)\n        self.set_parameters(self.parameters)\n        self.set_parameters(self.parameters)\n        self.set_parameters(self.parameters)\n        self.set_parameters(self.parameters)\n        self.set_parameters(self.parameters)\n        self.set_parameters(", "solution": "\n        result = Formula(path_actions(a_star_search(\n            ({f: self.cube[f] for f in \"LUFDRB\"}, \n             self.cube.select_type(\"edge\") & self.cube.has_colour(self.cube[\"D\"].colour)), \n            self.cross_successors, \n            self.cross_state_value, \n            self.cross_goal, \n            )))\n        self.cube(result)\n        return result"}
{"prompt": "def parse_optimize(self):\n        \"\"\"Parse the ouput resulted of a geometry optimization. Or a\n        saddle point.\n\n        \"\"\"", "output": "\n        self.parse_geometry_opt()\n        self.parse_saddle_point_opt()\n\n    def parse_geometry_opt(self):\n        \"\"\"Parse the ouput geometry optimization.\n\n        \"\"\"\n        if self.options.geometry_opt:\n            self.options.geometry_opt = self.options.geometry_opt.strip()\n        else:\n            self.options.geometry_opt = None\n        self.options.geometry_opt = None\n        self.options.geometry_opt = None\n        self.options.geometry_opt = None\n        self.options.geometry_opt = None\n        if self", "solution": "\n        match = re.search(\"EQUILIBRIUM GEOMETRY LOCATED\", self.text)\n        spmatch = \"SADDLE POINT LOCATED\" in self.text\n        located = True if match or spmatch else False\n\n        points = grep_split(\" BEGINNING GEOMETRY SEARCH POINT NSERCH=\",\n                            self.text)\n        if self.tddft == \"excite\":\n            points = [self.parse_energy(point) for point in points[1:]]\n        else:\n            regex = re.compile(r'NSERCH:\\s+\\d+\\s+E=\\s+([+-]?\\d+\\.\\d+)')\n            points = [Energy(states=[State(0,None,float(m.group(1)), 0.0, 0.0)]) for m in regex.finditer(self.text)]\n        \n        # Error handling\n        if \"FAILURE TO LOCATE STATIONARY POINT, TOO MANY STEPS TAKEN\" in self.text:\n            self.errcode = GEOM_NOT_LOCATED\n            self.errmsg = \"too many steps taken: %i\"%len(points)\n        \n        if located:\n            self.errcode = OK\n        \n        return Optimize(points=points)"}
{"prompt": "def solve_kkt(Q_LU, d, G, A, S_LU, rx, rs, rz, ry):\n    \"\"\" Solve KKT equations for the affine step\"\"\"", "output": "\n    # TODO: Implement the KKT equations for the affine step\n\n    # TODO: Implement the KKT equations for the step size\n    # TODO: Implement the KKT equations for the Jacobian\n    # TODO: Implement the KKT equations for the Hessian\n    # TODO: Implement the KKT equations for the Hessian\n    # TODO: Implement the KKT equations for the Hessian\n    # TODO: Implement the KKT equations for the Hessian\n    # TODO: Implement the KKT equations for the Hessian\n    # TODO: Implement the KKT equations for the Hessian\n    # TODO: Implement the KKT equations for the H", "solution": "\n    nineq, nz, neq, nBatch = get_sizes(G, A)\n\n    invQ_rx = rx.btrisolve(*Q_LU)\n    if neq > 0:\n        h = torch.cat((invQ_rx.unsqueeze(1).bmm(A.transpose(1, 2)).squeeze(1) - ry,\n                       invQ_rx.unsqueeze(1).bmm(G.transpose(1, 2)).squeeze(1) + rs / d - rz), 1)\n    else:\n        h = invQ_rx.unsqueeze(1).bmm(G.transpose(1, 2)).squeeze(1) + rs / d - rz\n\n    w = -(h.btrisolve(*S_LU))\n\n    g1 = -rx - w[:, neq:].unsqueeze(1).bmm(G).squeeze(1)\n    if neq > 0:\n        g1 -= w[:, :neq].unsqueeze(1).bmm(A).squeeze(1)\n    g2 = -rs - w[:, neq:]\n\n    dx = g1.btrisolve(*Q_LU)\n    ds = g2 / d\n    dz = w[:, neq:]\n    dy = w[:, :neq] if neq > 0 else None\n\n    return dx, ds, dz, dy"}
{"prompt": "def script(container, script_path, fail_nonzero=False, upload_dir=False, **kwargs):\n    \"\"\"\n    Runs a script inside a container, which is created with all its dependencies. The container is removed after it\n    has been run, whereas the dependencies are not destroyed. The output is printed to the console.\n\n    :param container: Container configuration name.\n    :param script_path: Local path to the script file.\n    :param fail_nonzero: Fail if the script returns with a nonzero exit code.\n    :param upload_dir: Upload the entire parent directory of the script file to the remote.\n    :param kwargs: Additional keyword arguments to the run_script action.\n    \"\"\"", "output": "\n    :param kwargs: Keyword arguments to pass to the script.\n    :return: Remote command output.\n    \"\"\"\n    kwargs['fail_nonzero'] = fail_nonzero\n    if not upload_dir:\n        upload_dir = True\n    return run_command(\n        [\n            'bash',\n            '-c',\n            script_path,\n            '-e',\n            'bash -c \"{0}\"'.format(script_path),\n            '-e',\n            'bash -c \"{0}\"'.format(script_path),\n            '-e',\n            'bash -c \"{0}\"'.format(script_path),\n            '-e", "solution": "\n    full_script_path = os.path.abspath(script_path)\n    prefix, name = os.path.split(full_script_path)\n    with temp_dir() as remote_tmp:\n        if upload_dir:\n            prefix_path, prefix_name = os.path.split(prefix)\n            remote_script = posixpath.join(remote_tmp, prefix_name, name)\n            put(prefix, remote_tmp, mirror_local_mode=True)\n        else:\n            remote_script = posixpath.join(remote_tmp, name)\n            put(script_path, remote_script, mirror_local_mode=True)\n        results = [output.result\n                   for output in container_fabric().run_script(container, script_path=remote_script, **kwargs)\n                   if o.action_type == ContainerUtilAction.SCRIPT]\n    for res in results:\n        puts(\"Exit code: {0}\".format(res['exit_code']))\n        if res['exit_code'] == 0 or not fail_nonzero:\n            puts(res['log'])\n        else:\n            error(res['log'])"}
{"prompt": "def destroy(self, request, pk=None):\n        \"\"\"For DELETE actions, actually deactivate the user, don't delete.\"\"\"", "output": "\n        try:\n            self.user = User.objects.get(pk=pk)\n        except User.DoesNotExist:\n            return Response(status=status.HTTP_404_NOT_FOUND)\n        if not self.user.is_active:\n            self.user.is_active = True\n            self.user.save()\n        return Response(status=status.HTTP_204_NO_CONTENT)\n\n    def get_context_data(self, **kwargs):\n        \"\"\"Add the current user to the context.\"\"\"\n        context = super(UserViewSet, self).get_context_data(**kwargs)\n        context['", "solution": "\n        user = self.get_object()\n        user.is_active = False\n        user.save()\n        return Response(status=status.HTTP_204_NO_CONTENT)"}
{"prompt": "def output_filename(output_dir, key_handle, public_id):\n    \"\"\"\n    Return an output filename for a generated AEAD. Creates a hashed directory structure\n    using the last three bytes of the public id to get equal usage.\n    \"\"\"", "output": "\n    return os.path.join(output_dir, 'aead_%s_%s.txt' % (key_handle, public_id))\n\n\ndef generate_aead_key(key_length=32):\n    \"\"\"\n    Generate a unique AEAD key.\n    \"\"\"\n    return os.path.join(key_dir(), 'aead_key_%s_%s' % (key_length, 'rsa' * key_length))\n\n\ndef generate_rsa_key(key_length=32):\n    \"\"\"\n    Generate a unique RSA key.\n    \"\"\"\n    return os.path.", "solution": "\n    parts = [output_dir, key_handle] + pyhsm.util.group(public_id, 2)\n    path = os.path.join(*parts)\n\n    if not os.path.isdir(path):\n        os.makedirs(path)\n\n    return os.path.join(path, public_id)"}
{"prompt": "def _set_mpls_traffic_bypasses(self, v, load=False):\n    \"\"\"\n    Setter method for mpls_traffic_bypasses, mapped from YANG variable /telemetry/profile/mpls_traffic_bypass/mpls_traffic_bypasses (list)\n    If this variable is read-only (config: false) in the\n    source YANG file, then _set_mpls_traffic_bypasses is considered as a private\n    method. Backends looking to populate this variable should\n    do so via calling thisObj._set_mpls_traffic_bypasses() directly.\n    \"\"\"", "output": ".\n\n    YANG Description: This list describes the MPLS traffic bypasses between\nthe source and the telemetry client\n\"\"\"\n    if v is None:\n      v = []\n    elif hasattr(v, '__iter__'):\n      v = list(v)\n    else:\n      v = [v]\n\n    if load:\n      self.mpls_traffic_bypass = v\n    else:\n      self.mpls_traffic_bypass = [ ]\n\n    return\n\n  def mmpls_traffic_bypass(self, mmpls_client_id):\n    \"\"\"\n    Returns the specified MMPLS client ID, or raises an exception\n   ", "solution": "\n    if hasattr(v, \"_utype\"):\n      v = v._utype(v)\n    try:\n      t = YANGDynClass(v,base=YANGListType(\"mpls_traffic_bypass_name\",mpls_traffic_bypasses.mpls_traffic_bypasses, yang_name=\"mpls-traffic-bypasses\", rest_name=\"bypass-lsp\", parent=self, is_container='list', user_ordered=False, path_helper=self._path_helper, yang_keys='mpls-traffic-bypass-name', extensions={u'tailf-common': {u'callpoint': u'Mplstrafficbypass', u'cli-suppress-mode': None, u'alt-name': u'bypass-lsp', u'info': u'MPLS Stats profile by Bypass LSP name', u'cli-suppress-list-no': None}}), is_container='list', yang_name=\"mpls-traffic-bypasses\", rest_name=\"bypass-lsp\", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'callpoint': u'Mplstrafficbypass', u'cli-suppress-mode': None, u'alt-name': u'bypass-lsp', u'info': u'MPLS Stats profile by Bypass LSP name', u'cli-suppress-list-no': None}}, namespace='urn:brocade.com:mgmt:brocade-telemetry', defining_module='brocade-telemetry', yang_type='list', is_config=True)\n    except (TypeError, ValueError):\n      raise ValueError({\n          'error-string': "}
{"prompt": "def pc_anova(self, covariates, num_pc=5):\n        \"\"\" \n        Calculate one-way ANOVA between the first num_pc prinicipal components\n        and known covariates. The size and index of covariates determines\n        whether u or v is used.\n    \n        Parameters\n        ----------\n        covariates : pandas.DataFrame\n            Dataframe of covariates whose index corresponds to the index of\n            either u or v. \n    \n        num_pc : int\n            Number of principal components to correlate with.\n    \n        Returns\n        -------\n        anova : pandas.Panel\n            Panel with F-values and p-values.\n    \n        \"\"\"", "output": " the ANOVA between the first num_pc prinicipal components\n            and known covariates. The size and index of panels determines\n            whether u or v is used.\n    \n        \"\"\"\n        # Get the panels\n        panels = self.get_panels(covariates)\n\n        # Get the size and index of panels\n        size_index = panels.index.size\n        index_size = size_index[size_index > 1]\n        index_index = index_size[index_size > 1]\n        index_u = index_index[u > 1]\n        index_v", "solution": "\n        from scipy.stats import f_oneway\n        if (covariates.shape[0] == self.u.shape[0] and \n            len(set(covariates.index) & set(self.u.index)) == self.u.shape[0]):\n            mat = self.u\n        elif (covariates.shape[0] == self.v.shape[0] and \n            len(set(covariates.index) & set(self.v.index)) == self.v.shape[0]):\n            mat = self.v\n        anova = pd.Panel(items=['fvalue', 'pvalue'],\n                         major_axis=covariates.columns,\n                         minor_axis=mat.columns[0:num_pc])\n        for i in anova.major_axis:\n            for j in anova.minor_axis:\n                t = [mat[j][covariates[i] == x] for x in set(covariates[i])]\n                f, p = f_oneway(*t)\n                anova.ix['fvalue', i, j] = f \n                anova.ix['pvalue', i, j] = p \n        return anova"}
{"prompt": "def reset(self, indices=None):\n    \"\"\"Reset the batch of environments.\n\n    Args:\n      indices: The batch indices of the environments to reset; defaults to all.\n\n    Returns:\n      Batch tensor of the new observations.\n    \"\"\"", "output": "\n    if indices is None:\n      indices = tf.range(self.batch_size)\n    if self.state_is_tuple:\n      return tf.concat([self.state_tuple_state, self.state_tuple_action], axis=1)\n    else:\n      return tf.concat([self.state_is_tuple_action, self.action_tuple_state, self.state_is_tuple], axis=1)\n\n  def _get_observation_from_action(self, action):\n    \"\"\"Get the observation tensor from the action.\n\n    Args:\n      action: The action to", "solution": "\n    if indices is None:\n      indices = tf.range(len(self._batch_env))\n    observ_dtype = self._parse_dtype(self._batch_env.observation_space)\n    observ = tf.py_func(\n        self._batch_env.reset, [indices], observ_dtype, name='reset')\n    observ = tf.check_numerics(observ, 'observ')\n    reward = tf.zeros_like(indices, tf.float32)\n    done = tf.zeros_like(indices, tf.bool)\n    with tf.control_dependencies([\n        tf.scatter_update(self._observ, indices, observ),\n        tf.scatter_update(self._reward, indices, reward),\n        tf.scatter_update(self._done, indices, done)]):\n      return tf.identity(observ)"}
{"prompt": "def getDataset(self, id_):\n        \"\"\"\n        Returns a dataset with the specified ID, or raises a\n        DatasetNotFoundException if it does not exist.\n        \"\"\"", "output": "\n        return self._get_client().getDataset(id_)\n\n    def getDatasetByID(self, id_):\n        \"\"\"\n        Returns a dataset with the specified ID, or raises a\n        DatasetNotFoundException if it does not exist.\n        \"\"\"\n        return self._get_client().getDatasetByID(id_)\n\n    def getDatasetByIDAndUrl(self, id_, url_):\n        \"\"\"\n        Returns a dataset with the specified ID, or raises a\n        DatasetNotFoundException if it does not exist.\n        \"\"\"\n        return self._get_client().getDatasetByIDAndUrl(id_, url_)\n\n    def getDatasetBy", "solution": "\n        if id_ not in self._datasetIdMap:\n            raise exceptions.DatasetNotFoundException(id_)\n        return self._datasetIdMap[id_]"}
{"prompt": "def p_compound_list(p):\n    \"\"\"compound_list : list\n                     | newline_list list1\"\"\"", "output": "\n    p[0] = p[1]\n    p[0].AddChildren(p[2])\n\ndef p_compound_list1(p):\n    \"\"\"compound_list : list1\n                     | newline_list list2\n                     | newline_list list3\n                     | newline_list list4\n                     | newline_list list5\n                     | newline_list list6\n                     | newline_list list7\n                     | newline_list list8\"\"\"\n    if len(p) == 2:\n        p[0] = p[1]\n    elif len(p) == 3:\n        p[0] = p[", "solution": "\n    if len(p) == 2:\n        p[0] = p[1]\n    else:\n        parts = p[2]\n        if len(parts) > 1:\n            p[0] = ast.node(kind='list', parts=parts, pos=_partsspan(parts))\n        else:\n            p[0] = parts[0]"}
{"prompt": "def pre_save(self, model_instance, add):\n        \"\"\"\n        Resizes, commits image to storage, and returns field's value just before saving.\n        \"\"\"", "output": "\n        self.model_instance = model_instance\n        self.add = add\n        return self.get_field_value(self.model_instance,'size')\n\n    def pre_delete(self, model_instance, delete):\n        \"\"\"\n        Revokes the delete method on the given model instance.\n        \"\"\"\n        self.model_instance = model_instance\n        self.delete = delete\n        return self.get_field_value(self.model_instance, 'delete')\n\n    def pre_save_with_user(self, user, add):\n        \"\"\"\n        Revokes the save method on the", "solution": "\n        file = getattr(model_instance, self.attname)\n        if file and not file._committed:\n            file.name = self._clean_file_name(model_instance, file.name)\n            file.file = self._resize_image(model_instance, file)\n            file.save(file.name, file, save=False)\n        return file"}
{"prompt": "def get_signing_key(self, key_type):\n        \"\"\"GetSigningKey.\n        [Preview API]\n        :param str key_type:\n        :rtype: str\n        \"\"\"", "output": "\n        return self._signing_key\n\n    def get_signing_key_value(self, key_type):\n        \"\"\"GetSigningKeyValue.\n        [Preview API]\n        :param str key_type:\n        :rtype: str\n        \"\"\"\n        return self._signing_key_value\n\n    def set_signing_key_value(self, key_type, value):\n        \"\"\"SetSigningKeyValue.\n        [Preview API]\n        :param str key_type:\n        :param str value:\n        :rtype: str\n        \"\"\"\n        self._signing_key_value = key_type, value\n        return", "solution": "\n        route_values = {}\n        if key_type is not None:\n            route_values['keyType'] = self._serialize.url('key_type', key_type, 'str')\n        response = self._send(http_method='GET',\n                              location_id='92ed5cf4-c38b-465a-9059-2f2fb7c624b5',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('str', response)"}
{"prompt": "def merge_networks(network, donor=[]):\n    r\"\"\"\n    Combine multiple networks into one without doing any topological\n    manipulations (such as stiching nearby pores to each other).\n\n    Parameters\n    ----------\n    network : OpenPNM Network Object\n        The network to which all the other networks should be added.\n\n    donor : OpenPNM Network Object or list of Objects\n        The network object(s) to add to the given network\n\n    Notes\n    -----\n    This methods does *not* attempt to stitch the networks topologically.\n\n    See Also\n    --------\n    extend\n    trim\n    stitch\n\n    \"\"\"", "output": "\n    if isinstance(donor, (list, tuple)):\n        donor = tuple(map(lambda x: x.to_xml(), donor))\n    if isinstance(network, (OpenPNMNetwork, OpenPNMNetworkList)):\n        if isinstance(network_list, (list, tuple)):\n            network = open(network_list[0], 'r').read()\n        else:\n            network = open(OpenPNM.Network, 'r').read()\n        for donor_item in donor:\n            network = network.replace(donor_item, donor_item.to_xml", "solution": "\n    if type(donor) == list:\n        donors = donor\n    else:\n        donors = [donor]\n\n    for donor in donors:\n        network['pore.coords'] = sp.vstack((network['pore.coords'],\n                                            donor['pore.coords']))\n        network['throat.conns'] = sp.vstack((network['throat.conns'],\n                                             donor['throat.conns'] +\n                                             network.Np))\n        p_all = sp.ones((sp.shape(network['pore.coords'])[0],), dtype=bool)\n        t_all = sp.ones((sp.shape(network['throat.conns'])[0],), dtype=bool)\n        network.update({'pore.all': p_all})\n        network.update({'throat.all': t_all})\n        for key in set(network.keys()).union(set(donor.keys())):\n            if key.split('.')[1] not in ['conns', 'coords', '_id', 'all']:\n                if key in network.keys():\n                    pop_flag = False\n                    if key not in donor.keys():\n                        logger.debug('Adding ' + key + ' to donor')\n                        # If key not on donor add it first\n                        if network[key].dtype == bool:\n                            donor[key] = False\n                        else:\n                            donor[key] = sp.nan\n                        pop_flag = True\n                    # Then merge it with existing array on network\n                    try:\n                        temp = sp.hstack((network[key], donor[key]))\n                    except ValueError:\n                        temp = sp.vstack((network[key], donor[key]))\n                    network[key] = temp\n                    if pop_flag:\n                        donor.pop(key, None)\n                else:\n                    # If key not on network add it first\n                    logger.debug('Adding ' + key + ' to network')\n                    if donor[key].dtype == bool:\n                        network[key] = False\n                    else:\n                        network[key] = sp.nan\n                    # Then append donor values to network\n                    s = sp.shape(donor[key])[0]\n                    network[key][-s:] = donor[key]\n\n    # Clear adjacency and incidence matrices which will be out of date now\n    network._am.clear()\n    network._im.clear()"}
{"prompt": "def _process_interactions(self, row):\n        \"\"\"\n        Process row of CTD data from CTD_chemicals_diseases.tsv.gz\n        and generate triples. Only create associations based on direct evidence\n        (not using the inferred-via-gene), and unambiguous relationships.\n        (Ambiguous ones will be processed in the sister method using the\n        disambiguated file). There are no OMIM ids for diseases in these cases,\n        so we associate with only the mesh disease ids.\n        Args:\n            :param row (list): row of CTD data\n        Returns:\n            :return None\n        \"\"\"", "output": " data.\n        \"\"\"\n        interactions = []\n        for i, row_interactions in enumerate(row):\n            if row_interactions[0] == 'O':\n                interactions.append(row_interactions[1])\n            else:\n                interactions.append(row_interactions[1])\n\n        if len(row) > 1:\n            interactions.append(row[0])\n\n        if len(row) > 2:\n            interactions.append(row[1])\n\n        if len(row) > 3:\n            interactions.append(row[2])\n\n        if len(row) >", "solution": "\n        model = Model(self.graph)\n        self._check_list_len(row, 10)\n        (chem_name, chem_id, cas_rn, disease_name, disease_id, direct_evidence,\n         inferred_gene_symbol, inference_score, omim_ids, pubmed_ids) = row\n\n        if direct_evidence == '':\n            return\n\n        evidence_pattern = re.compile(r'^therapeutic|marker\\/mechanism$')\n        # dual_evidence = re.compile(r'^marker\\/mechanism\\|therapeutic$')\n\n        # filter on those diseases that are mapped to omim ids in the test set\n        intersect = list(\n            set(['OMIM:' + str(i) for i in omim_ids.split('|')] +\n                [disease_id]) & set(self.test_diseaseids))\n        if self.test_mode and len(intersect) < 1:\n            return\n        chem_id = 'MESH:' + chem_id\n        reference_list = self._process_pubmed_ids(pubmed_ids)\n        if re.match(evidence_pattern, direct_evidence):\n            rel_id = self.resolve(direct_evidence)\n            model.addClassToGraph(chem_id, chem_name)\n            model.addClassToGraph(disease_id, None)\n            self._make_association(chem_id, disease_id, rel_id, reference_list)\n        else:\n            # there's dual evidence, but haven't mapped the pubs\n            pass\n            # LOG.debug(\n            #   \"Dual evidence for %s (%s) and %s (%s)\",\n            #   chem_name, chem_id, disease_name, disease_id)\n\n        return"}
{"prompt": "def unpack_text_io_wrapper(fp, encoding):\n  \"\"\"\n  If *fp* is a #io.TextIOWrapper object, this function returns the underlying\n  binary stream and the encoding of the IO-wrapper object. If *encoding* is not\n  None and does not match with the encoding specified in the IO-wrapper, a\n  #RuntimeError is raised.\n  \"\"\"", "output": "\n  return codecs.getreader(encoding)(fp)\n\ndef read_text_file(filename):\n  \"\"\"\n  Read a file in text mode.\n\n  :param filename: path of the file to read\n  :type filename: str\n  :return: text file content\n  :rtype: str\n  \"\"\"\n  with open(filename, 'rb') as fp:\n    return fp.read()\n\ndef write_text_file(filename, content):\n  \"\"\"\n  Write a file in text mode.\n\n  :param filename: path of the file to write\n  :type filename: str\n  :param content: text file", "solution": "\n\n  if isinstance(fp, io.TextIOWrapper):\n    if fp.writable() and encoding is not None and fp.encoding != encoding:\n      msg = 'TextIOWrapper.encoding({0!r}) != {1!r}'\n      raise RuntimeError(msg.format(fp.encoding, encoding))\n    if encoding is None:\n      encoding = fp.encoding\n    fp = fp.buffer\n\n  return fp, encoding"}
{"prompt": "def make_optimised_chunks(self, min_length, max_length, pad_data=0):\n    \"\"\"\n    Splits ScienceSegments up into chunks, of a given maximum length.\n    The length of the last two chunks are chosen so that the data\n    utilisation is optimised.\n    @param min_length: minimum chunk length.\n    @param max_length: maximum chunk length.\n    @param pad_data: exclude the first and last pad_data seconds of the\n    segment when generating chunks\n    \"\"\"", "output": "\n    if self.is_optimised:\n      return\n    min_length = min_length or self.min_length\n    max_length = max_length or self.max_length\n    if pad_data:\n      self.last_data = pad_data\n    self.last_length = max_length\n    self.last_data_length = len(pad_data)\n    self.last_length_min = min_length\n    self.last_length_max = max_length\n    self.last_data = self.last_data[:min_length]\n    self.last_data_", "solution": "\n    for seg in self.__sci_segs:\n      # pad data if requested\n      seg_start = seg.start() + pad_data\n      seg_end = seg.end() - pad_data\n\n      if seg.unused() > max_length:\n        # get number of max_length chunks\n        N = (seg_end - seg_start)/max_length\n\n        # split into chunks of max_length\n        for i in range(N-1):\n          start = seg_start + (i * max_length)\n          stop = start + max_length\n          seg.add_chunk(start, stop)\n\n        # optimise data usage for last 2 chunks\n        start = seg_start + ((N-1) * max_length)\n        middle = (start + seg_end)/2\n        seg.add_chunk(start, middle)\n        seg.add_chunk(middle, seg_end)\n        seg.set_unused(0)\n      elif seg.unused() > min_length:\n        # utilise as single chunk\n        seg.add_chunk(seg_start, seg_end)\n      else:\n        # no chunk of usable length\n        seg.set_unused(0)"}
{"prompt": "def extract_tar(filename, dest_path, **kwargs):\n    \"\"\"\n    Extracts a TAR archive. All element names starting with ``/`` (indicating an absolute path) or that contain ``..``\n    as references to a parent directory are not extracted.\n\n    :param filename: Path to the tar file.\n    :type filename: unicode\n    :param dest_path: Destination path to extract the contents to.\n    :type dest_path: unicode\n    :param kwargs: Additional kwargs for opening the TAR file (:func:`tarfile.open`).\n    \"\"\"", "output": "\n    if not os.path.isabs(filename):\n        filename = os.path.join(dest_path, filename)\n    with tarfile.open(filename, **kwargs) as tar:\n        for member in tar.getmembers():\n            tar.extract(member, dest_path)\n    return tar\n\n\ndef extract_zip(filename, dest_path, **kwargs):\n    \"\"\"\n    Extracts a ZIP archive. All elements names starting with ``/`` (indicating an absolute path) or that contain\n    ``..`` as references to a parent directory are not extracted.\n\n    :param filename: Path to the zip", "solution": "\n    with tarfile.open(filename, 'r', **kwargs) as tf:\n        safe_members = [name for name in tf.getmembers() if _safe_name(name)]\n        if safe_members:\n            tf.extractall(dest_path, safe_members)"}
{"prompt": "def _head_object(s3_conn, bucket, key):\n    \"\"\"Retrieve information about an object in S3 if it exists.\n\n    Args:\n        s3_conn (botocore.client.S3): S3 connection to use for operations.\n        bucket (str): name of the bucket containing the key.\n        key (str): name of the key to lookup.\n\n    Returns:\n        dict: S3 object information, or None if the object does not exist.\n            See the AWS documentation for explanation of the contents.\n\n    Raises:\n        botocore.exceptions.ClientError: any error from boto3 other than key\n            not found is passed through.\n    \"\"\"", "output": "\n            verification failed.\n    \"\"\"\n    try:\n        s3_conn.head_object(Bucket=bucket, Key=key)\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == '404':\n            return None\n        raise\n\n    return s3_conn.get_object(Bucket=bucket, Key=key)\n\n\ndef _get_bucket_key_names(s3_conn):\n    \"\"\"Retrieve the names of the buckets in the S3 object.\n\n    Args:\n        s3_conn (botocore.client.S3): S3 connection to use", "solution": "\n    try:\n        return s3_conn.head_object(Bucket=bucket, Key=key)\n    except botocore.exceptions.ClientError as e:\n        if e.response['Error']['Code'] == '404':\n            return None\n        else:\n            raise"}
{"prompt": "def truncate_table(self, table):\n        \"\"\" Responsys.truncateTable call\n\n        Accepts:\n            InteractObject table\n\n        Returns True on success\n        \"\"\"", "output": "\n        if table == None:\n            return False\n        if table.type == Table.Type_Feature:\n            return True\n        return False\n\n    def get_table_id(self, table):\n        \"\"\" Responsys.getTableId call\n\n        Accepts:\n            InteractObject table\n\n        Returns the id of the table\n        \"\"\"\n        if table == None:\n            return None\n        if table.id == None:\n            return None\n        return table.id\n\n    def get_table_name(self, table):\n        \"\"\" Responsys.getTableName call\n\n        Accepts:\n            InteractObject table\n\n        Returns the name of", "solution": "\n        table = table.get_soap_object(self.client)\n        return self.call('truncateTable', table)"}
{"prompt": "def parse_packets(self, binary_packets):\n        \"\"\"\n        Parses binary packets and return a list of parsed packets.\n\n        DOES NOT CLOSE tshark. It must be closed manually by calling close() when you're done\n        working with it.\n        \"\"\"", "output": "\n        return [parse_packet(packet) for packet in binary_packets]\n\n    def close(self):\n        \"\"\"\n        Closes the connection.\n        \"\"\"\n        if self.sock is not None:\n            self.sock.close()\n            self.sock = None\n        self.sock = None\n\n    def __enter__(self):\n        \"\"\"\n        Entering a new connection is a no-op if no address is given.\n        \"\"\"\n        if self.sock is None:\n            self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            self.sock.connect(self.host,", "solution": "\n        if not binary_packets:\n            raise ValueError(\"Must supply at least one packet\")\n        parsed_packets = []\n\n        if not self._current_tshark:\n            self.eventloop.run_until_complete(self._get_tshark_process())\n        for binary_packet in binary_packets:\n            self._write_packet(binary_packet)\n\n        def callback(pkt):\n            parsed_packets.append(pkt)\n            if len(parsed_packets) == len(binary_packets):\n                raise StopCapture()\n\n        self.eventloop.run_until_complete(self._get_parsed_packet_from_tshark(callback))\n        return parsed_packets"}
{"prompt": "def _refresh_state(self):\n    \"\"\" Get the state of a job. Must be overridden by derived Job classes\n        for Jobs that don't use a Future.\n    \"\"\"", "output": "\n    return self._state\n\n  def _check_cancelled(self):\n    \"\"\" Check if the job has been cancelled.\n    \"\"\"\n    return self._cancelled\n\n  def _check_cancelled_exception(self):\n    \"\"\" Check if the job has been cancelled.\n    \"\"\"\n    return self._cancelled_exception\n\n  def _get_task_id(self):\n    \"\"\" Get the task_id of a task.\n    \"\"\"\n    return self._task_id\n\n  def _set_task_id(self, task_id):\n    \"\"\" Set the task_id of a task.\n    \"\"\"\n    self._task_id = task", "solution": "\n    if self._is_complete:\n      return\n\n    if not self._future:\n      raise Exception('Please implement this in the derived class')\n\n    if self._future.done():\n      self._is_complete = True\n      self._end_time = datetime.datetime.utcnow()\n      try:\n        self._result = self._future.result()\n      except Exception as e:\n        message = str(e)\n        self._fatal_error = JobError(location=traceback.format_exc(), message=message,\n                                     reason=str(type(e)))"}
{"prompt": "def _codec_can_decode_with_surrogatepass(codec, _cache={}):\n    \"\"\"Returns if a codec supports the surrogatepass error handler when\n    decoding. Some codecs were broken in Python <3.4\n    \"\"\"", "output": "\n    try:\n        codecs.lookup(encoding.lower())\n    except LookupError:\n        return False\n    else:\n        return True\n\n\ndef _codec_can_encode(codec, encoding):\n    \"\"\"Returns if a codec can encode the given encoding.\n    \"\"\"\n    try:\n        return codecs.lookup(encoding.lower())\n    except LookupError:\n        return False\n\n\ndef _codec_can_decode(codec, encoding):\n    \"\"\"Returns if a codec supports the surrogatepass error handler when\n    decoding. Some codecs were broken in Python <3.4\n    \"\"\"\n    try:\n        codecs.lookup(encoding.lower())", "solution": "\n\n    try:\n        return _cache[codec]\n    except KeyError:\n        try:\n            u\"\\ud83d\".encode(\n                codec, _surrogatepass).decode(codec, _surrogatepass)\n        except UnicodeDecodeError:\n            _cache[codec] = False\n        else:\n            _cache[codec] = True\n        return _cache[codec]"}
{"prompt": "def R_package_resource(package, resource):\n    \"\"\"\n    return a path to an R package resource, if it is available\n    \"\"\"", "output": "\n    if package.endswith('.rpm'):\n        return resource\n    return os.path.join(R_HOME, 'rpm', resource)\n\n\ndef R_package_path(package):\n    \"\"\"\n    return the path to a R package\n    \"\"\"\n    return R_HOME + '/lib/rpm/' + package\n\n\ndef R_package_path_glob(package):\n    \"\"\"\n    return the path to a R package\n    \"\"\"\n    return R_HOME + '/lib/rpm/rpm/' + R_package_resource(package, 'rpm') + '/lib/rpm/rpm'\n\n\ndef R_package", "solution": "\n    package_path = R_package_path(package)\n    if not package_path:\n        return None\n    package_resource = os.path.join(package_path, resource)\n    if not file_exists(package_resource):\n        return None\n    else:\n        return package_resource"}
{"prompt": "def week_schedule(index, on_time=None, off_time=None, off_days=None):\n    \"\"\" Return boolean time series following given week schedule.\n\n    Parameters\n    ----------\n    index : pandas.DatetimeIndex\n        Datetime index\n    on_time : str or datetime.time\n        Daily opening time. Default: '09:00'\n    off_time : str or datetime.time\n        Daily closing time. Default: '17:00'\n    off_days : list of str\n        List of weekdays. Default: ['Sunday', 'Monday']\n\n    Returns\n    -------\n    pandas.Series of bool\n        True when on, False otherwise for given datetime index\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> from opengrid.library.utils import week_schedule\n    >>> index = pd.date_range('20170701', '20170710', freq='H')\n    >>> week_schedule(index)\n    \"\"\"", "output": "\n        Daily schedule.\n\n    \"\"\"\n    if on_time is None:\n        on_time = '09:00'\n    if off_time is None:\n        off_time = '17:00'\n    if off_days is None:\n        off_days = ['Sunday', 'Monday']\n    if isinstance(index, pandas.DatetimeIndex):\n        index = index.to_pydatetime()\n    if isinstance(index, str):\n        index = pd.DatetimeIndex(index)\n    if isinstance(index, pd.Series):\n        return index\n    else:\n        return pd.Series(index, dtype", "solution": "\n    if on_time is None:\n        on_time = '9:00'\n    if off_time is None:\n        off_time = '17:00'\n    if off_days is None:\n        off_days = ['Sunday', 'Monday']\n    if not isinstance(on_time, datetime.time):\n        on_time = pd.to_datetime(on_time, format='%H:%M').time()\n    if not isinstance(off_time, datetime.time):\n        off_time = pd.to_datetime(off_time, format='%H:%M').time()\n    times = (index.time >= on_time) & (index.time < off_time) & (~index.weekday_name.isin(off_days))\n    return pd.Series(times, index=index)"}
{"prompt": "def _compute_mean_on_rock(self, C, mag, rrup, F, HW):\n        \"\"\"\n        Compute mean value on rock (that is eq.1, page 105 with S = 0)\n        \"\"\"", "output": "\n        return C['R_R_mean'][0][mag] + C['R_R_mean'][0][rrup]\n\n    def _compute_mean_on_rock_error(self, C, rrup, F, HW):\n        \"\"\"\n        Compute mean value on rock (that is eq.2, page 105 with S = 0)\n        \"\"\"\n        return C['R_R_mean'][0][mag] + C['R_R_mean'][0][rrup]\n\n    def _compute_mean_on_rock_rate(self, C, rrup, F,", "solution": "\n        f1 = self._compute_f1(C, mag, rrup)\n        f3 = self._compute_f3(C, mag)\n        f4 = self._compute_f4(C, mag, rrup)\n\n        return f1 + F * f3 + HW * f4"}
{"prompt": "def thumbprint(self, hashalg=hashes.SHA256()):\n        \"\"\"Returns the key thumbprint as specified by RFC 7638.\n\n        :param hashalg: A hash function (defaults to SHA256)\n        \"\"\"", "output": "\n        return self._thumbprint(hashalg=hashalg)\n\n    def _thumbprint(self, hashalg=hashes.SHA256()):\n        \"\"\"Returns the key thumbprint as specified by RFC 7638.\n\n        :param hashalg: A hash function (defaults to SHA256)\n        \"\"\"\n        if hashalg == hashes.SHA256:\n            return self._key_thumbprint_sha256\n        else:\n            return self._key_thumbprint_sha512\n\n    def _key_thumbprint_sha512(self, hashalg=hashes.SHA256()):\n        \"\"\"Returns the key thumbprint as specified by RFC 76", "solution": "\n\n        t = {'kty': self._params['kty']}\n        for name, val in iteritems(JWKValuesRegistry[t['kty']]):\n            if val.required:\n                t[name] = self._key[name]\n        digest = hashes.Hash(hashalg, backend=default_backend())\n        digest.update(bytes(json_encode(t).encode('utf8')))\n        return base64url_encode(digest.finalize())"}
{"prompt": "def _compile_constant_expression(self,\n                                     expr: Expression,\n                                     scope: Dict[str, TensorFluent],\n                                     batch_size: Optional[int] = None,\n                                     noise: Optional[List[tf.Tensor]] = None) -> TensorFluent:\n        \"\"\"Compile a constant expression `expr` into a TensorFluent\n        in the given `scope` with optional batch size.\n\n        Args:\n            expr (:obj:`rddl2tf.expr.Expression`): A RDDL constant expression.\n            scope (Dict[str, :obj:`rddl2tf.fluent.TensorFluent`]): A fluent scope.\n            batch_size (Optional[size]): The batch size.\n\n        Returns:\n            :obj:`rddl2tf.fluent.TensorFluent`: The compiled expression as a TensorFluent.\n        \"\"\"", "output": "Fluent`]): A dictionary of parameters\n                that will be passed to the :obj:`rddl2tf.constant_op` method.\n            batch_size (Optional[int]): The number of elements in the batch.\n            noise (Optional[List[tf.Tensor]]): A list of Tensor\n            \"\"\"\n        if batch_size is not None:\n            if not isinstance(batch_size, list):\n                batch_size = [batch_size]\n            for batch_size_i in batch_size:\n                if not isinstance(noise, list):\n                    noise = [noise]\n                for noise_i in", "solution": "\n        etype = expr.etype\n        args = expr.args\n        dtype = utils.python_type_to_dtype(etype[1])\n        fluent = TensorFluent.constant(args, dtype=dtype)\n        return fluent"}
{"prompt": "def _color_level(str_, level):\n    \"\"\" Return the string wrapped with the appropriate styling for the message\n    level.  The styling will be determined based on the rez configuration.\n\n    Args:\n      str_ (str): The string to be wrapped.\n      level (str): The message level. Should be one of 'critical', 'error',\n        'warning', 'info' or 'debug'.\n\n    Returns:\n      str: The string styled with the appropriate escape sequences.\n    \"\"\"", "output": "\n    return str_.replace('\\x1b[0m', '\\\\x1b[1m') \\\n       .replace('\\x1b[0m', '\\\\x1b[2m') \\\n       .replace('\\x1b[0m', '\\\\x1b[3m') \\\n       .replace('\\x1b[0m', '\\\\x1b[4m') \\\n       .replace('\\x1b[0m', '\\\\x1b[5m') \\\n       .replace('\\x1b[0m', '\\\\x1b[6m') \\\n       .replace('\\x1", "solution": "\n    fore_color, back_color, styles = _get_style_from_config(level)\n    return _color(str_, fore_color, back_color, styles)"}
{"prompt": "def assignments(self):\n        \"\"\"Extracts assignment values from assignments.\"\"\"", "output": "\n        return self._extract_values(self.assignment_pattern, self.assignment_pattern_2)\n\n    def _extract_values(self, pattern, pattern_2):\n        \"\"\"Extracts the values from the pattern and the pattern_2.\"\"\"\n        matches = pattern.findall(pattern_2)\n        if not matches:\n            return []\n        values = []\n        for match in matches:\n            if match == 'None':\n                values.append(None)\n            else:\n                values.append(float(match))\n        return values\n\n    def _extract_assignment(self, assignment):\n        \"\"\"Extracts the values from", "solution": "\n\n        collection = OrderedDict()\n\n        for arg in self.all:\n            if '=' in arg:\n                collection.setdefault(\n                    arg.split('=', 1)[0], ArgsList(no_argv=True))\n                collection[arg.split('=', 1)[0]]._args.append(\n                    arg.split('=', 1)[1])\n\n        return collection"}
{"prompt": "def listar_por_equipamento(self, id_equipment):\n        \"\"\"List all Script related Equipment.\n\n        :param id_equipment: Identifier of the Equipment. Integer value and greater than zero.\n\n        :return: Dictionary with the following structure:\n\n        ::\n\n            {script': [{\u2018id\u2019: < id >,\n            \u2018nome\u2019: < nome >,\n            \u2018descricao\u2019: < descricao >,\n            \u2018id_tipo_roteiro\u2019: < id_tipo_roteiro >,\n            \u2018nome_tipo_roteiro\u2019: < nome_tipo_roteiro >,\n            \u2018descricao_tipo_roteiro\u2019: < descricao_tipo_roteiro >}, ...more Script...]}\n\n        :raise InvalidParameterError: The identifier of Equipment is null and invalid.\n        :raise EquipamentoNaoExisteError: Equipment not registered.\n        :raise DataBaseError: Networkapi failed to access the database.\n        :raise XMLError: Networkapi failed to generate the XML response.\n        \"\"\"", "output": "roteiro\u2019: < nome >,\n            \u2018descricao_tipo_roteiro_display_name\u2019: < descricao >,\n            \u2018id_script_display_name\u2019: < id >,\n            \u2018nome_script_display_name\u2019: < nome >,\n            \u2018descricao_script_display_name\u2019: < descricao >,\n            \u2018id_script_tipo_roteiro_display_name\u2019: < id >,\n            \u2018nome_script_tipo_roteiro_display_name\u2019: < nome >,\n            \ufffd", "solution": "\n        if not is_valid_int_param(id_equipment):\n            raise InvalidParameterError(\n                u'The identifier of Equipment is invalid or was not informed.')\n\n        url = 'script/equipment/' + str(id_equipment) + '/'\n\n        code, map = self.submit(None, 'GET', url)\n\n        key = 'script'\n        return get_list_map(self.response(code, map, [key]), key)"}
{"prompt": "def register_palette(self):\n        \"\"\"Converts pygmets style to urwid palatte\"\"\"", "output": "\n        palette = self.palette\n        if palette is None:\n            return\n\n        if palette.has_palette(self.palette):\n            palette.remove_palette(self.palette)\n\n        palette.add_block('body')\n        for i in range(len(palette)):\n            body = palette.get_block(i + 1)\n            palette.add_block(body)\n            body.props.palette = self.palette\n            body.props.body_color = self.body_color_from_hex(self.body)\n            body.props.base_color = self.body_color_from_", "solution": "\n        default = 'default'\n        palette = list(self.palette)\n        mapping = CONFIG['rgb_to_short']\n        for tok in self.style.styles.keys():\n            for t in tok.split()[::-1]:\n                st = self.style.styles[t]\n                if '#' in st:\n                    break\n            if '#' not in st:\n                st = ''\n            st = st.split()\n            st.sort()   # '#' comes before '[A-Za-z0-9]'\n            if len(st) == 0:\n                c = default\n            elif st[0].startswith('bg:'):\n                c = default\n            elif len(st[0]) == 7:\n                c = 'h' + rgb_to_short(st[0][1:], mapping)[0]\n            elif len(st[0]) == 4:\n                c = 'h' + rgb_to_short(st[0][1]*2 + st[0][2]*2 + st[0][3]*2, mapping)[0]\n            else:\n                c = default\n            a = urwid.AttrSpec(c, default, colors=256)\n            row = (tok, default, default, default, a.foreground, default)\n            palette.append(row)\n        self.loop.screen.register_palette(palette)"}
{"prompt": "def motif3struct_bin(A):\n    \"\"\"\n    Structural motifs are patterns of local connectivity. Motif frequency\n    is the frequency of occurrence of motifs around a node.\n\n    Parameters\n    ----------\n    A : NxN np.ndarray\n        binary directed connection matrix\n\n    Returns\n    -------\n    F : 13xN np.ndarray\n        motif frequency matrix\n    f : 13x1 np.ndarray\n        motif frequency vector (averaged over all nodes)\n    \"\"\"", "output": "\n    return np.sum(A, axis=0) / np.sum(A, axis=1)\n\n\ndef motif3struct_bin_no_zeros(A):\n    \"\"\"\n    Structural motifs are patterns of local connectivity. Motif\n    frequency is the averaged frequency of motifs around a node.\n\n    Parameters\n    ----------\n    A : NxN np.ndarray\n        binary directed connection matrix\n\n    Returns\n    -------\n    F : 13xN np.ndarray\n        motif frequency matrix\n    f : 13x1 np.ndarray\n        motif frequency vector (averaged over all nodes)\n    \"\"\"\n   ", "solution": "\n    from scipy import io\n    import os\n    fname = os.path.join(os.path.dirname(__file__), motiflib)\n    mot = io.loadmat(fname)\n    m3n = mot['m3n']\n    id3 = mot['id3'].squeeze()\n\n    n = len(A)  # number of vertices in A\n    f = np.zeros((13,))  # motif count for whole graph\n    F = np.zeros((13, n))  # motif frequency\n\n    A = binarize(A, copy=True)  # ensure A is binary\n    As = np.logical_or(A, A.T)  # symmetrized adjmat\n\n    for u in range(n - 2):\n        # v1: neighbors of u (>u)\n        V1 = np.append(np.zeros((u,), dtype=int), As[u, u + 1:n + 1])\n        for v1 in np.where(V1)[0]:\n            # v2: neighbors of v1 (>u)\n            V2 = np.append(np.zeros((u,), dtype=int), As[v1, u + 1:n + 1])\n            V2[V1] = 0  # not already in V1\n            # and all neighbors of u (>v1)\n            V2 = np.logical_or(\n                np.append(np.zeros((v1,)), As[u, v1 + 1:n + 1]), V2)\n            for v2 in np.where(V2)[0]:\n                a = np.array((A[v1, u], A[v2, u], A[u, v1],\n                              A[v2, v1], A[u, v2], A[v1, v2]))\n                s = np.uint32(np.sum(np.power(10, np.arange(5, -1, -1)) * a))\n                ix = id3[np.squeeze(s == m3n)] - 1\n                F[ix, u] += 1\n                F[ix, v1] += 1\n                F[ix, v2] += 1\n                f[ix] += 1\n\n    return f, F"}
{"prompt": "def save_docs(self, files=None, output_dir=None):\n        \"\"\"\n        Save documentation files for codebase into `output_dir`.  If output\n        dir is None, it'll refrain from building the index page and build\n        the file(s) in the current directory.\n\n        If `files` is None, it'll build all files in the codebase.\n        \"\"\"", "output": "\n        if output_dir is None:\n            output_dir = self.outdir\n        if files is None:\n            files = self.state.document.settings.env.config.master_doc + self.state.document.settings.env.config.code_suffix\n        for f in files:\n            if output_dir is None:\n                output_dir = self.outdir\n            savefig(f, self.state.document, self.state.document.reporter)\n\n    def build_all_docs(self):\n        \"\"\"\n        Builds all the documentation files.\n        \"\"\"\n        if self.outdir is None:", "solution": "\n        if output_dir:\n            try:\n                os.mkdir(output_dir)\n            except OSError:\n                pass\n\n            try:\n                import pkg_resources\n                save_file(os.path.join(output_dir, 'jsdoc.css'),\n                    pkg_resources.resource_string(__name__, 'static/jsdoc.css'))\n            except (ImportError, IOError):\n                try:\n                    import shutil\n                    base_dir = os.path.dirname(os.path.realpath(__file__))\n                    css_file = os.path.join(base_dir, 'jsdoc.css')\n                    shutil.copy(css_file, output_dir)\n                except IOError:\n                    print('jsdoc.css not found.  HTML will not be styled.')\n\n            save_file('%s/index.html' % output_dir, \n                    build_html_page('Module index', self.to_html()))\n        else:\n            output_dir = '.'\n\n        if files is None:\n            files = list(self.keys())\n\n        for filename in files:\n            try:\n                doc = self[filename]\n                save_file('%s/%s.html' % (output_dir, trim_js_ext(doc.name)), \n                        build_html_page(doc.name, doc.to_html(self)))\n            except KeyError:\n                warn('File %s does not exist', filename)"}
{"prompt": "def format_from_extension(fname):\n    \"\"\" Tries to infer a protocol from the file extension.\"\"\"", "output": "\n    with open(fname, 'rb') as f:\n        return f.read()\n\n\ndef get_protocol(fname):\n    \"\"\" Tries to infer a protocol from the file protocol.\"\"\"\n    with open(fname, 'rb') as f:\n        return f.read()\n\n\ndef get_protocol_by_name(name):\n    \"\"\" Tries to infer a protocol by name.\"\"\"\n    return get_protocol(name)\n\n\ndef get_protocol_by_extension(fname):\n    \"\"\" Tries to infer a protocol from the file name extension.\"\"\"\n    with open(fname, 'rb') as f", "solution": "\n    _base, ext = os.path.splitext(fname)\n    if not ext:\n        return None\n    try:\n        format = known_extensions[ext.replace('.', '')]\n    except KeyError:\n        format = None\n    return format"}
{"prompt": "def _get_name(self):\n        \"\"\"\n        There are three cases, because apipie definitions can have multiple\n        signatures but python does not\n        For example, the api endpoint:\n           /api/myres/:myres_id/subres/:subres_id/subres2\n\n        for method *index* will be translated to the api method name:\n            subres_index_subres2\n\n        So when you want to call it from v2 object, you'll have:\n\n          myres.subres_index_subres2\n\n        \"\"\"", "output": "\n        return self.subres_id.name, self.subres_id.name\n\n    def _get_url(self, url, **kwargs):\n        \"\"\"\n        There are three cases, because apipie definitions can have multiple\n        signatures but python does not\n        For example, the api endpoint:\n           /api/myres/:myres_id/subres/:subres_id/subres2\n\n        for method *index* will be translated to the api url:\n            subres_index_subres2\n\n        So when you want to call it from v1 object, you'll have:\n\n          my", "solution": "\n        if self.url.count(':') > 1:\n            # /api/one/two/:three/four -> two_:three_four\n            base_name = self.url.split('/', 3)[-1].replace('/', '_')[1:]\n            # :one_two_three -> two_three\n            if base_name.startswith(':'):\n                base_name = base_name.split('_')[-1]\n            # one_:two_three_:four_five -> one_three_five\n            base_name = re.sub('_:[^/]+', '', base_name)\n            # in case that the last term was a parameter\n            if base_name.endswith('_'):\n                base_name = base_name[:-1]\n            # one_two_three -> one_two_method_three\n            base_name = (\n                '_' + self._method['name']\n            ).join(base_name.rsplit('_', 1))\n        else:\n            base_name = self._method['name']\n        if base_name == 'import':\n            base_name = 'import_'\n        if self._apipie_resource != self.resource:\n            return '%s_%s' % (self._apipie_resource, base_name)\n        else:\n            return base_name"}
{"prompt": "def create_subkey(primary_bytes, subkey, signer_func, secret_bytes=b''):\n    \"\"\"Export new subkey to GPG primary key.\"\"\"", "output": "\n    return _export_key(primary_bytes, subkey, signer_func, secret_bytes,\n                    'pgp_subkey_export')\n\n\ndef _export_key(primary_bytes, subkey, signer_func, secret_bytes,\n                name):\n    \"\"\"Export new key to GPG subkey.\"\"\"\n    if not isinstance(primary_bytes, bytes):\n        raise TypeError('expected bytes, got %s' % type(primary_bytes))\n    if not isinstance(subkey, bytes):\n        raise TypeError('expected bytes, got %s' % type(subkey))\n    if not", "solution": "\n    subkey_packet = protocol.packet(tag=(7 if secret_bytes else 14),\n                                    blob=(subkey.data() + secret_bytes))\n    packets = list(decode.parse_packets(io.BytesIO(primary_bytes)))\n    primary, user_id, signature = packets[:3]\n\n    data_to_sign = primary['_to_hash'] + subkey.data_to_hash()\n\n    if subkey.ecdh:\n        embedded_sig = None\n    else:\n        # Primary Key Binding Signature\n        hashed_subpackets = [\n            protocol.subpacket_time(subkey.created)]  # signature time\n        unhashed_subpackets = [\n            protocol.subpacket(16, subkey.key_id())]  # issuer key id\n        embedded_sig = protocol.make_signature(\n            signer_func=signer_func,\n            data_to_sign=data_to_sign,\n            public_algo=subkey.algo_id,\n            sig_type=0x19,\n            hashed_subpackets=hashed_subpackets,\n            unhashed_subpackets=unhashed_subpackets)\n\n    # Subkey Binding Signature\n\n    # Key flags: https://tools.ietf.org/html/rfc4880#section-5.2.3.21\n    # (certify & sign)                   (encrypt)\n    flags = (2) if (not subkey.ecdh) else (4 | 8)\n\n    hashed_subpackets = [\n        protocol.subpacket_time(subkey.created),  # signature time\n        protocol.subpacket_byte(0x1B, flags)]\n\n    unhashed_subpackets = []\n    unhashed_subpackets.append(protocol.subpacket(16, primary['key_id']))\n    if embedded_sig is not None:\n        unhashed_subpackets.append(protocol.subpacket(32, embedded_sig))\n    unhashed_subpackets.append(protocol.CUSTOM_SUBPACKET)\n\n    if not decode.has_custom_subpacket(signature):\n        signer_func = keyring.create_agent_signer(user_id['value'])\n\n    signature = protocol.make_signature(\n        signer_func=signer_func,\n        data_to_sign=data_to_sign,\n        public_algo=primary['algo'],\n        sig_type=0x18,\n        hashed_subpackets=hashed_subpackets,\n        unhashed_subpackets=unhashed_subpackets)\n    sign_packet = protocol.packet(tag=2, blob=signature)\n    return primary_bytes + subkey_packet + sign_packet"}
{"prompt": "def delete(self, client=None):\n        \"\"\"Deletes a blob from Cloud Storage.\n\n        If :attr:`user_project` is set on the bucket, bills the API request\n        to that project.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: Optional. The client to use.  If not passed, falls back\n                       to the ``client`` stored on the blob's bucket.\n\n        :rtype: :class:`Blob`\n        :returns: The blob that was just deleted.\n        :raises: :class:`google.cloud.exceptions.NotFound`\n                 (propagated from\n                 :meth:`google.cloud.storage.bucket.Bucket.delete_blob`).\n        \"\"\"", "output": ".exceptions.NotFound`.\n        \"\"\"\n        client = self._require_client(client)\n        try:\n            self.delete_blob(client=client, name=self.name)\n        except NotFound:\n            return None\n        return self\n\n    def delete_blob(self, client=None, name=None):\n        \"\"\"Deletes a blob from Cloud Storage.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: Optional. The client to use.  If not passed, falls back\n                       to the ``client`` stored on the blob's bucket.", "solution": "\n        return self.bucket.delete_blob(\n            self.name, client=client, generation=self.generation\n        )"}
{"prompt": "def create(self, ogps):\n        \"\"\"\n        Method to create object group permissions\n\n        :param ogps: List containing vrf desired to be created on database\n        :return: None\n        \"\"\"", "output": "\n        if len(ogps) == 0:\n            return\n        if len(ogps) == 1:\n            return\n        if len(ogps) == 2:\n            return\n        if len(ogps) == 3:\n            return\n        if len(ogps) == 4:\n            return\n        if len(ogps) == 5:\n            return\n        if len(ogps) == 6:\n            return\n        if len(ogps) == 7:\n            return\n        if len(ogps) == 8:\n            return\n        if len(ogps) == 9:\n            return\n        if len(ogps) ==", "solution": "\n\n        data = {'ogps': ogps}\n        return super(ApiObjectGroupPermission, self).post('api/v3/object-group-perm/', data)"}
{"prompt": "def run(items, config):\n    \"\"\"Run third party disambiguation script, resolving into single set of calls.\n    \"\"\"", "output": "\n    if len(items) == 0:\n        print(\"No disambiguation script found.\")\n        return\n    if len(items) == 1:\n        print(\"Using %s\" % items[0])\n        return _run_one(items[0])\n    if len(items) > 2:\n        print(\"Ambiguous disambiguation script found. Please use the --disambiguation option.\")\n        return\n    print(\"No disambiguation script found. Please use the --ambiguous option.\")\n    return\n\ndef _run_one(items):\n    \"\"\"Run the first item in the list of items.\n    \"\"\"\n   ", "solution": "\n    assert len(items) == 2, \"Can only resolve two organism disambiguation\"\n    # check aligner, handling tophat/tophat2 distinctions\n    aligner = config[\"algorithm\"].get(\"aligner\")\n    aligner = \"tophat\" if aligner.startswith(\"tophat\") else aligner\n    assert aligner in [\"bwa\", \"hisat2\", \"tophat\", \"star\"], \"Disambiguation only supported for bwa, hisat2, star and tophat alignments.\"\n    if items[0][\"disambiguate\"].get(\"base\"):\n        data_a, data_b = items\n    else:\n        data_b, data_a = items\n    work_bam_a = bam.sort(data_a[\"work_bam\"], config, \"queryname\")\n    work_bam_b = bam.sort(data_b[\"work_bam\"], config, \"queryname\")\n    if data_a.get(\"align_split\"):\n        base_dir = utils.safe_makedir(os.path.normpath(os.path.join(os.path.dirname(work_bam_a),\n                                                                    os.pardir, os.pardir,\n                                                                    \"disambiguate_%s\" % aligner)))\n        out_dir = os.path.join(base_dir, \"_\".join([str(x) for x in data_a[\"align_split\"].split(\"-\")]))\n    else:\n        out_dir = os.path.normpath(os.path.join(os.path.dirname(work_bam_a),\n                                                os.pardir, \"disambiguate_%s\" % aligner))\n    base_name = os.path.join(out_dir, os.path.splitext(os.path.basename(work_bam_a))[0])\n    summary_file = \"%s_summary.txt\" % base_name\n    if not utils.file_exists(summary_file):\n        with file_transaction(items[0], out_dir) as tx_out_dir:\n            _run_cplusplus(work_bam_a, work_bam_b, tx_out_dir, aligner, os.path.basename(base_name), items)\n    data_a[\"disambiguate\"] = \\\n      {data_b[\"genome_build\"]: bam.sort(\"%s.disambiguatedSpeciesB.bam\" % base_name, config),\n       \"%s-ambiguous\" % data_a[\"genome_build\"]: bam.sort(\"%s.ambiguousSpeciesA.bam\" % base_name, config),\n       \"%s-ambiguous\" % data_b[\"genome_build\"]: bam.sort(\"%s.ambiguousSpeciesB.bam\" % base_name, config),\n       \"summary\": summary_file}\n    data_a[\"work_bam\"] = bam.sort(\"%s.disambiguatedSpeciesA.bam\" % base_name, config)\n    bam.index(dd.get_work_bam(data_a), data_a[\"config\"])\n    return [[data_a]]"}
{"prompt": "def set_typ(self, refobj, typ):\n        \"\"\"Set the type of the given refobj\n\n        :param refobj: the reftrack node to edit\n        :type refobj: refobj\n        :param typ: the entity type\n        :type typ: str\n        :returns: None\n        :rtype: None\n        :raises: ValueError\n        \"\"\"", "output": "\n        if typ is not None:\n            refobj.set_attr(\"type\", typ)\n        if self.is_root():\n            refobj.set_attr(\"is_root\", True)\n        if self.is_leaf():\n            refobj.set_attr(\"is_leaf\", True)\n        if self.is_leaf():\n            refobj.set_attr(\"is_leaf\", True)\n        if self.is_leaf():\n            refobj.set_attr(\"is_leaf\", True)\n        if self.is_leaf():\n            refobj.set_attr(\"is_leaf\", True", "solution": "\n        try:\n            enum = JB_ReftrackNode.types.index(typ)\n        except ValueError:\n            raise ValueError(\"The given type %s could not be found in available types: %\" % (typ, JB_ReftrackNode.types))\n        cmds.setAttr(\"%s.type\" % refobj, enum)"}
{"prompt": "def translate(self, by):\n        \"\"\"Translate the Compound by a vector\n\n        Parameters\n        ----------\n        by : np.ndarray, shape=(3,), dtype=float\n\n        \"\"\"", "output": "\n        if by == 0:\n            return self.x\n        elif by == 1:\n            return self.y\n        elif by == 2:\n            return self.z\n        elif by == 3:\n            return self.x, self.y, self.z\n        elif by == 4:\n            return self.z, self.x, self.y, self.x\n        elif by == 5:\n            return self.x, self.y, self.z\n        elif by == 6:\n            return self.y, self.z, self.z\n        elif by == 7:\n            return self.z, self.z", "solution": "\n        new_positions = _translate(self.xyz_with_ports, by)\n        self.xyz_with_ports = new_positions"}
{"prompt": "def resolve(self, definitions):\n        \"\"\"\n        Resolve named references to other WSDL objects.\n        @param definitions: A definitions object.\n        @type definitions: L{Definitions}\n        \"\"\"", "output": "\n        if definitions.has_key(self.name):\n            return definitions[self.name]\n        else:\n            return None\n\n    def __str__(self):\n        \"\"\"\n        Get a string representation of this object.\n        @return: A string containing the WSDL object's name and version.\n        @rtype: C{str}\n        \"\"\"\n        return 'WSDL %s %s' % (self.name, self.version)\n\n    def __repr__(self):\n        \"\"\"\n        Get a string representation of this object.\n        @return: A string containing the WSDL object's name and version.\n        @rtype:", "solution": "\n        for op in self.operations.values():\n            if op.input is None:\n                op.input = Message(Element('no-input'), definitions)\n            else:\n                qref = qualify(op.input, self.root, definitions.tns)\n                msg = definitions.messages.get(qref)\n                if msg is None:\n                    raise Exception(\"msg '%s', not-found\" % op.input)\n                else:\n                    op.input = msg\n            if op.output is None:\n                op.output = Message(Element('no-output'), definitions)\n            else:\n                qref = qualify(op.output, self.root, definitions.tns)\n                msg = definitions.messages.get(qref)\n                if msg is None:\n                    raise Exception(\"msg '%s', not-found\" % op.output)\n                else:\n                    op.output = msg\n            for f in op.faults:\n                qref = qualify(f.message, self.root, definitions.tns)\n                msg = definitions.messages.get(qref)\n                if msg is None:\n                    raise Exception(\"msg '%s', not-found\" % f.message)\n                f.message = msg"}
{"prompt": "def df_filter_row_sum(df, threshold, take_abs=True):\n  \"\"\" filter rows in matrix at some threshold\n  and remove columns that have a sum below this threshold \"\"\"", "output": "\n  df = df.copy()\n  for i in range(len(df.columns)):\n    if df.columns[i] > threshold:\n      del df.columns[i]\n  if take_abs:\n    return df.dropna()\n  else:\n    return df\n  #return df.dropna()\n\ndef df_filter_row_sum_with_nulls(df, threshold, take_abs=True):\n  \"\"\" filter rows in matrix at some threshold\n  and remove columns that have a sum below this threshold \"\"\"\n  df = df.copy()\n  for i in range(len(df.", "solution": "\n\n  from copy import deepcopy\n  from .__init__ import Network\n  net = Network()\n\n  if take_abs is True:\n    df_copy = deepcopy(df['mat'].abs())\n  else:\n    df_copy = deepcopy(df['mat'])\n\n  ini_rows = df_copy.index.values.tolist()\n  df_copy = df_copy.transpose()\n  tmp_sum = df_copy.sum(axis=0)\n  tmp_sum = tmp_sum.abs()\n  tmp_sum.sort_values(inplace=True, ascending=False)\n\n  tmp_sum = tmp_sum[tmp_sum > threshold]\n  keep_rows = sorted(tmp_sum.index.values.tolist())\n\n  if len(keep_rows) < len(ini_rows):\n    df['mat'] = grab_df_subset(df['mat'], keep_rows=keep_rows)\n\n    if 'mat_up' in df:\n      df['mat_up'] = grab_df_subset(df['mat_up'], keep_rows=keep_rows)\n      df['mat_dn'] = grab_df_subset(df['mat_dn'], keep_rows=keep_rows)\n\n    if 'mat_orig' in df:\n      df['mat_orig'] = grab_df_subset(df['mat_orig'], keep_rows=keep_rows)\n\n  return df"}
{"prompt": "def pip(self, cmd):\n        \"\"\"Execute some pip function using the virtual environment pip.\"\"\"", "output": "\n        return self.run_command(cmd, 'pip')\n\n    def install_virtualenv(self):\n        \"\"\"\n        Install the virtual environment at the specified location, returning the\n        virtualenv path and optionally a list of command line arguments for the\n        command.\n        \"\"\"\n        if self.system_site_packages:\n            return self._install_pip_command(self.system_site_packages)\n\n        if self.clear:\n            self.clear_dir(self.build_dir)\n            self.clear_dir(self.src_dir)\n\n        for name in ('lib', 'lib64'):\n            virtualenv_path", "solution": "\n        pip_bin = self.cmd_path('pip')\n        cmd = '{0} {1}'.format(pip_bin, cmd)\n        return self._execute(cmd)"}
{"prompt": "def _find_metadata_vars(self, ds, refresh=False):\n        \"\"\"\n        Returns a list of netCDF variable instances for those that are likely metadata variables\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :param bool refresh: if refresh is set to True, the cache is\n                             invalidated.\n        :rtype: list\n        :return:   List of variable names (str) that are likely metadata\n                   variable candidates.\n\n        \"\"\"", "output": "\n        if refresh:\n            self.cache.clear()\n        return self.get_metadata_vars(ds)\n\n    def get_metadata_vars(self, ds):\n        \"\"\"\n        Returns a list of netCDF variable instances for those that are likely metadata variables\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: list\n        :return:   List of netCDF variables (str) that are likely metadata\n                   variable candidates.\n\n        \"\"\"\n        if ds.dataset_type == 'trajectory':\n            return _find_netcdf_trajectory_vars(ds)\n        elif ds.dataset_type", "solution": "\n        if self._metadata_vars.get(ds, None) and refresh is False:\n            return self._metadata_vars[ds]\n\n        self._metadata_vars[ds] = []\n        for name, var in ds.variables.items():\n\n            if name in self._find_ancillary_vars(ds) or name in self._find_coord_vars(ds):\n                continue\n\n            if name in ('platform_name', 'station_name', 'instrument_name', 'station_id', 'platform_id', 'surface_altitude'):\n                self._metadata_vars[ds].append(name)\n\n            elif getattr(var, 'cf_role', '') != '':\n                self._metadata_vars[ds].append(name)\n\n            elif getattr(var, 'standard_name', None) is None and len(var.dimensions) == 0:\n                self._metadata_vars[ds].append(name)\n\n        return self._metadata_vars[ds]"}
{"prompt": "def isValid(cntxt: Context, m: FixedShapeMap) -> Tuple[bool, List[str]]:\n    \"\"\"`5.2 Validation Definition <http://shex.io/shex-semantics/#validation>`_\n\n    The expression isValid(G, m) indicates that for every nodeSelector/shapeLabel pair (n, s) in m, s has a\n        corresponding shape expression se and satisfies(n, se, G, m). satisfies is defined below for each form\n        of shape expression\n\n    :param cntxt: evaluation context - includes graph and schema\n    :param m: list of NodeShape pairs to test\n    :return: Success/failure indicator and, if fail, a list of failure reasons\n    \"\"\"", "output": " of fixed shape labels\n    :return: tuple of (bool, str) - True if the expression is valid, False otherwise\n    \"\"\"\n    if not isinstance(g, Graph):\n        return False, 'Graph is not a valid context'\n    if not isinstance(m, list):\n        return False,'schema is not a list'\n    if len(m)!= 5:\n        return False,'schema is not a 5-element list'\n    if not isinstance(schema, GraphSchema):\n        return False,'schema is not a GraphSchema'\n    return True, 'isValid'\n\n\ndef isValid(cnt", "solution": "\n    if not cntxt.is_valid:\n        return False, cntxt.error_list\n    parse_nodes = []\n    for nodeshapepair in m:\n        n = nodeshapepair.nodeSelector\n        if not isinstance_(n, Node):\n            return False, [f\"{n}: Triple patterns are not implemented\"]\n        # The third test below is because the spec asserts that completely empty graphs pass in certain circumstances\n        elif not (next(cntxt.graph.predicate_objects(nodeshapepair.nodeSelector), None) or\n                  next(cntxt.graph.subject_predicates(nodeshapepair.nodeSelector), None) or\n                  not next(cntxt.graph.triples((None, None, None)), None)):\n            return False, [f\"Focus: {nodeshapepair.nodeSelector} not in graph\"]\n        else:\n            s = cntxt.shapeExprFor(START if nodeshapepair.shapeLabel is None or nodeshapepair.shapeLabel is START\n                                   else nodeshapepair.shapeLabel)\n            cntxt.current_node = ParseNode(satisfies, s, n, cntxt)\n            if not s:\n                if nodeshapepair.shapeLabel is START or nodeshapepair.shapeLabel is None:\n                    cntxt.fail_reason = \"START node is not specified or is invalid\"\n                else:\n                    cntxt.fail_reason = f\"Shape: {nodeshapepair.shapeLabel} not found in Schema\"\n                return False, cntxt.process_reasons()\n            parse_nodes.append(cntxt.current_node)\n            if not satisfies(cntxt, n, s):\n                cntxt.current_node.result = False\n                return False, cntxt.process_reasons()\n            else:\n                cntxt.current_node.result = True\n    return True, []"}
{"prompt": "def add_inputs(self, rawtx, wifs, change_address=None, fee=10000,\n                   dont_sign=False):\n        \"\"\"Add sufficient inputs from given <wifs> to cover <rawtx> outputs\n        and <fee>. If no <change_address> is given, change will be sent to\n        first wif.\n        \"\"\"", "output": "\n        inputs = []\n        for wif in wifs:\n            if not wif:\n                continue\n            inputs.append(rawtx)\n            inputs.append(change_address)\n            inputs.append(rawtx)\n            inputs.append(wit)\n            if not dont_sign:\n                inputs.append(change_address)\n        return inputs\n\n    def sign_tx(self, tx, password):\n        \"\"\"Sign a <wifs> transaction.\n        \"\"\"\n        self.add_inputs(tx, wifs=[\n            self.create_address(\n                address_type=self.address_type,\n                address=", "solution": "\n        tx = deserialize.tx(rawtx)\n        keys = deserialize.keys(self.testnet, wifs)\n        fee = deserialize.positive_integer(fee)\n        if change_address is not None:\n            change_address = deserialize.address(self.testnet, change_address)\n        tx = control.add_inputs(self.service, self.testnet, tx, keys,\n                                change_address=change_address, fee=fee)\n\n        if not dont_sign:\n            tx = control.sign_tx(self.service, self.testnet, tx, keys)\n\n        return serialize.tx(tx)"}
{"prompt": "def detach(gandi, resource, background, force):\n    \"\"\"Detach an ip from it's currently attached vm.\n\n    resource can be an ip id or ip.\n    \"\"\"", "output": "\n    if resource is None:\n        return\n\n    if isinstance(resource, str):\n        if not force:\n            raise ValueError(\"Cannot detach a running instance from a running VM.\")\n        else:\n            return gandi.detach_ip(resource)\n    elif isinstance(resource, dict):\n        if not force:\n            raise ValueError(\"Cannot detach a running instance from a running VM.\")\n        else:\n            return gandi.detach_ip(resource)\n    else:\n        raise ValueError(\"Cannot detach a running instance from a running VM.\")\n\n\ndef get_vm_id(gandi, name):\n    \"\"\"Get the id", "solution": "\n    if not force:\n        proceed = click.confirm('Are you sure you want to detach ip %s?' %\n                                resource)\n        if not proceed:\n            return\n\n    return gandi.ip.detach(resource, background, force)"}
{"prompt": "def load_dsdl(*paths, **args):\n    \"\"\"\n    Loads the DSDL files under the given directory/directories, and creates\n    types for each of them in the current module's namespace.\n\n    If the exclude_dist argument is not present, or False, the DSDL\n    definitions installed with this package will be loaded first.\n\n    Also adds entries for all datatype (ID, kind)s to the DATATYPES\n    dictionary, which maps datatype (ID, kind)s to their respective type\n    classes.\n    \"\"\"", "output": "\n    if args.get('exclude_dist', False):\n        paths = [path for path in paths if path not in DATATYPES]\n\n    # Load the module\n    if args.get('dsdl_file', False):\n        paths = [path for path in paths if path not in DATATYPES]\n        for path in paths:\n            if not os.path.exists(path):\n                continue\n            if not os.path.isdir(path):\n                continue\n            if not os.path.exists(path + '.dSYM'):\n                continue\n            dsdl_file = open(path + '.dSYM',", "solution": "\n    global DATATYPES, TYPENAMES\n\n    paths = list(paths)\n\n    # Try to prepend the built-in DSDL files\n    # TODO: why do we need try/except here?\n    # noinspection PyBroadException\n    try:\n        if not args.get(\"exclude_dist\", None):\n            dsdl_path = pkg_resources.resource_filename(__name__, \"dsdl_files\")  # @UndefinedVariable\n            paths = [os.path.join(dsdl_path, \"uavcan\")] + paths\n            custom_path = os.path.join(os.path.expanduser(\"~\"), \"uavcan_vendor_specific_types\")\n            if os.path.isdir(custom_path):\n                paths += [f for f in [os.path.join(custom_path, f) for f in os.listdir(custom_path)]\n                          if os.path.isdir(f)]\n    except Exception:\n        pass\n\n    root_namespace = Namespace()\n    dtypes = dsdl.parse_namespaces(paths)\n    for dtype in dtypes:\n        namespace, _, typename = dtype.full_name.rpartition(\".\")\n        root_namespace._path(namespace).__dict__[typename] = dtype\n        TYPENAMES[dtype.full_name] = dtype\n\n        if dtype.default_dtid:\n            DATATYPES[(dtype.default_dtid, dtype.kind)] = dtype\n            # Add the base CRC to each data type capable of being transmitted\n            dtype.base_crc = dsdl.crc16_from_bytes(struct.pack(\"<Q\", dtype.get_data_type_signature()))\n            logger.debug(\"DSDL Load {: >30} DTID: {: >4} base_crc:{: >8}\"\n                         .format(typename, dtype.default_dtid, hex(dtype.base_crc)))\n\n        def create_instance_closure(closure_type, _mode=None):\n            # noinspection PyShadowingNames\n            def create_instance(*args, **kwargs):\n                if _mode:\n                    assert '_mode' not in kwargs, 'Mode cannot be supplied to service type instantiation helper'\n                    kwargs['_mode'] = _mode\n                return transport.CompoundValue(closure_type, *args, **kwargs)\n            return create_instance\n\n        dtype._instantiate = create_instance_closure(dtype)\n\n        if dtype.kind == dtype.KIND_SERVICE:\n            dtype.Request = create_instance_closure(dtype, _mode='request')\n            dtype.Response = create_instance_closure(dtype, _mode='response')\n\n    namespace = root_namespace._path(\"uavcan\")\n    for top_namespace in namespace._namespaces():\n        MODULE.__dict__[str(top_namespace)] = namespace.__dict__[top_namespace]\n\n    MODULE.__dict__[\"thirdparty\"] = Namespace()\n    for ext_namespace in root_namespace._namespaces():\n        if str(ext_namespace) != \"uavcan\":\n            # noinspection PyUnresolvedReferences\n            MODULE.thirdparty.__dict__[str(ext_namespace)] = root_namespace.__dict__[ext_namespace]"}
{"prompt": "def __remote_path_rewrite(self, dataset_path, dataset_path_type, name=None):\n        \"\"\" Return remote path of this file (if staging is required) else None.\n        \"\"\"", "output": "\n        if name is None:\n            name = os.path.basename(dataset_path)\n        if dataset_path_type == 'file':\n            return os.path.join(self.path, name)\n        elif dataset_path_type == 'directory':\n            return os.path.dirname(dataset_path)\n        elif dataset_path_type == 'file':\n            return self.path\n        elif dataset_path_type == 'directory':\n            return os.path.join(self.path, name)\n        elif dataset_path_type == 'file':\n            return self.path\n        elif dataset_", "solution": "\n        path = str(dataset_path)  # Use false_path if needed.\n        action = self.action_mapper.action(path, dataset_path_type)\n        if action.staging_needed:\n            if name is None:\n                name = os.path.basename(path)\n            remote_directory = self.__remote_directory(dataset_path_type)\n            remote_path_rewrite = self.path_helper.remote_join(remote_directory, name)\n        else:\n            # Actions which don't require staging MUST define a path_rewrite\n            # method.\n            remote_path_rewrite = action.path_rewrite(self.path_helper)\n\n        return remote_path_rewrite"}
{"prompt": "def make_sentence(sentence, *args, **kwargs):\n    \"\"\" Wrapper: returns a Sentence node\n    \"\"\"", "output": "\n    return Sentence(\n        sentence.text,\n        text=sentence.text,\n        type=sentence.type,\n        children=[\n            make_sentence_node(child, *args, **kwargs)\n            for child in sentence.children\n        ],\n        parent=sentence.parent,\n        type_node=sentence.type_node,\n        children_node_order=sentence.children_node_order,\n        parent_node_order=sentence.parent_node_order,\n        parent_text_node=sentence.parent_text_node,\n        parent_type_node=sentence.parent_type_", "solution": "\n    return symbols.SENTENCE(*([sentence] + list(args)), **kwargs)"}
{"prompt": "def split_vcs_method_from_uri(uri):\n    # type: (AnyStr) -> Tuple[Optional[STRING_TYPE], STRING_TYPE]\n    \"\"\"Split a vcs+uri formatted uri into (vcs, uri)\"\"\"", "output": "\n    if not uri:\n        return None, None\n    if uri.startswith('file://'):\n        return split_file_uri(uri)\n    if uri.startswith('file://'):\n        return split_file_uri(uri[7:])\n    return None, None\n\n\ndef get_vcs_uri_from_path(path):\n    # type: (AnyStr) -> AnyStr\n    \"\"\"\n    Return the path of the vcs+uri in the form of\n    //file://path/to/file.ext\n    //uri/to/file.ext\n    \"\"\"\n    if not path:\n        return None\n    if", "solution": "\n    vcs_start = \"{0}+\"\n    vcs = None  # type: Optional[STRING_TYPE]\n    vcs = first([vcs for vcs in VCS_LIST if uri.startswith(vcs_start.format(vcs))])\n    if vcs:\n        vcs, uri = uri.split(\"+\", 1)\n    return vcs, uri"}
{"prompt": "def pieces(self):\n        \"\"\"\n        Number of pieces the content is split into or ``None`` if :attr:`piece_size`\n        returns ``None``\n        \"\"\"", "output": "\n        return self._pieces\n\n    @property\n    def total_size(self):\n        \"\"\"\n        Total size of the file, in bytes.\n\n        :return: The size of the file in bytes.\n        \"\"\"\n        return self._total_size\n\n    @property\n    def mimetype(self):\n        \"\"\"\n        MIME type of the file, if known.\n\n        :return: The MIME type of the file.\n        \"\"\"\n        return self._mimetype\n\n    @property\n    def extension(self):\n        \"\"\"\n        The extension of the file, if known.\n\n        :return: The extension of the file.\n        \"\"\"\n        return self._extension\n\n    @", "solution": "\n        if self.piece_size is None:\n            return None\n        else:\n            return math.ceil(self.size / self.piece_size)"}
{"prompt": "def patch_spyder3(verbose=False):\n    \"\"\"Patch spyder to make it work with sos files and sos kernel \"\"\"", "output": "\n    if verbose:\n        print(\"Patching spyder3\")\n    if verbose:\n        print(\"Patching spyder3\")\n    if not os.path.exists(os.path.join(os.path.dirname(__file__), 'data/sos_kernel.yaml')):\n        print(\"Creating new spyder3\")\n        with open(os.path.join(os.path.dirname(__file__), 'data/sos_kernel.yaml'), 'w') as f:\n            f.write('version: 3\\n')\n            f.write('config:\\n')\n            f.", "solution": "\n    try:\n        # patch spyder/config/utils.py for file extension\n        from spyder.config import utils\n        src_file = utils.__file__\n        spyder_dir = os.path.dirname(os.path.dirname(src_file))\n        patch_file(src_file,\n        "}
{"prompt": "def ssh_keys(self):\n        \"\"\"\n        This method lists all the available public SSH keys in your account\n        that can be added to a droplet.\n        \"\"\"", "output": "\n        return self.get_public_ssh_keys()\n\n    def get_public_ssh_keys(self):\n        \"\"\"\n        This method returns a list of SSH keys that can be added to a droplet.\n        \"\"\"\n        return [self.get_ssh_key(key) for key in self.ssh_keys() if key.is_public]\n\n    def get_ssh_key(self, name):\n        \"\"\"\n        This method returns a SSH key object that can be used to remove a\n        SSH key from your account.\n        \"\"\"\n        return self.connection.get_ssh_key(name)\n\n   ", "solution": "\n        params = {}\n        json = self.request('/ssh_keys', method='GET', params=params)\n        status = json.get('status')\n        if status == 'OK':\n            ssh_keys_json = json.get('ssh_keys', [])\n            keys = [SSHKey.from_json(ssh_key) for ssh_key in ssh_keys_json]\n            return keys\n        else:\n            message = json.get('message')\n            raise DOPException('[%s]: %s' % (status, message))"}
{"prompt": "def get_characters(self, *args, **kwargs):\n        \"\"\"Fetches lists of comic characters with optional filters.\n\n        get /v1/public/characters/{characterId}\n\n        :returns:  CharacterDataWrapper\n\n        >>> m = Marvel(public_key, private_key)\n        >>> cdw = m.get_characters(orderBy=\"name,-modified\", limit=\"5\", offset=\"15\")\n        >>> print cdw.data.count\n        1401\n        >>> for result in cdw.data.results:\n        ...     print result.name\n        Aginar\n        Air-Walker (Gabriel Lan)\n        Ajak\n        Ajaxis\n        Akemi\n        \n        \"\"\"", "output": "Ago)\n        Crimson\n        \"\"\"\n        return CharacterDataWrapper(self.public_key, *args, **kwargs)\n\n    def get_all_characters(self, *args, **kwargs):\n        \"\"\"Fetches all comic characters.\n\n        get /v1/public/characters\n\n        :returns:  CharacterDataWrapper\n\n        >>> m = Marvel(private_key, public_key)\n        >>> cdw = m.get_all_characters(orderBy=\"name,-modified\", limit=\"5\", offset=\"15\")\n        >>> print cdw.data.count\n        1401\n        \"\"\"\n       ", "solution": "\n        #pass url string and params string to _call\n        response = json.loads(self._call(Character.resource_url(), self._params(kwargs)).text)\n        return CharacterDataWrapper(self, response, kwargs)"}
{"prompt": "def moveSpeed(self, location, seconds=0.3):\n        \"\"\" Moves cursor to specified ``Location`` over ``seconds``.\n\n        If ``seconds`` is 0, moves the cursor immediately. Used for smooth\n        somewhat-human-like motion.\n        \"\"\"", "output": "\n        self.sendUpdate('moveSpeed', [location, seconds])\n\n    def setCursor(self, cursor):\n        \"\"\" Sets the cursor for the specified ``Location``.\n\n        If ``location`` is ``null``, the current cursor is used.\n        \"\"\"\n        self.sendUpdate('setCursor', [cursor])\n\n    def setCursorLocation(self, cursor, location):\n        \"\"\" Sets the cursor for the specified ``Location``.\n\n        If ``location`` is ``null``, the current cursor is used.\n        \"\"\"\n        self.sendUpdate('setCursorLocation', [cursor, location])\n\n    def setCursorPosition(self, cursor,", "solution": "\n        self._lock.acquire()\n        original_location = mouse.get_position()\n        mouse.move(location.x, location.y, duration=seconds)\n        if mouse.get_position() == original_location and original_location != location.getTuple():\n            raise IOError("}
{"prompt": "def input(self, _in, out, **kw):\n        \"\"\"\n        Currently Elm does not write to stdout\n        (https://github.com/elm-lang/elm-make/issues/39), so we need to write\n        the compiled contents to a temporary file and then read it in order to\n        output to stdout.\n        \"\"\"", "output": "\n        self.log.info(\"Running %s\", _in)\n        cmd = self.get_finalized_command(_in, **kw)\n        self.log.debug(\"Command: %s\", cmd)\n        p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, **kw)\n        (stdout, stderr) = p.communicate() + (b'\\n',)\n        if p.returncode!= 0:\n            raise ElmError(\"Error while running %s: %s\" % (_in, stderr))\n        return stdout.decode('utf-8'), stderr.decode('", "solution": "\n        # create a temp file\n        tmp = NamedTemporaryFile(suffix='.js', delete=False)\n        tmp.close()  # close it so windows can read it\n\n        # write to a temp file\n        elm_make = self.binary or 'elm-make'\n        change_directory = bool(self.change_directory or False)\n        source = kw['source_path']\n        source_dir = os.path.dirname(source)\n        exec_dir = source_dir if change_directory else os.getcwd()\n        write_args = [elm_make, source, '--output', tmp.name, '--yes']\n        with excursion(exec_dir), TemporaryFile(mode='w') as fake_write_obj:\n            self.subprocess(write_args, fake_write_obj)\n\n        # read the temp file\n        cat_or_type = 'type' if platform == 'win32' else 'cat'\n        read_args = [cat_or_type, tmp.name]\n        self.subprocess(read_args, out)\n        os.remove(tmp.name)"}
{"prompt": "def add_edge(self, edgelist):\n        \"\"\"\n        Adds an edge from network.\n\n        Parameters\n        ----------\n\n        edgelist : list\n            a list (or list of lists) containing the i,j and t indicies to be added. For weighted networks list should also contain a 'weight' key.\n\n        Returns\n        --------\n            Updates TenetoBIDS.network dataframe with new edge\n        \"\"\"", "output": "\n        if isinstance(edgelist, list):\n            edgelist = [self.network_df[i] for i in edgelist]\n        else:\n            edgelist = self.network_df[edgelist]\n        self.network_df = pd.DataFrame(edgelist, columns=self.network_df.columns)\n\n    def add_node(self, nodelist):\n        \"\"\"\n        Adds an node from network.\n\n        Parameters\n        ----------\n\n        nodelist : list\n            a list (or list of lists) containing the i,j and t indicies to be added. For weighted networks list should also contain a '", "solution": "\n        if not isinstance(edgelist[0], list):\n            edgelist = [edgelist]\n        teneto.utils.check_TemporalNetwork_input(edgelist, 'edgelist')\n        if len(edgelist[0]) == 4:\n            colnames = ['i', 'j', 't', 'weight']\n        elif len(edgelist[0]) == 3:\n            colnames = ['i', 'j', 't']\n        if self.hdf5:\n            with pd.HDFStore(self.network) as hdf:\n                rows = hdf.get_storer('network').nrows\n                hdf.append('network', pd.DataFrame(edgelist, columns=colnames, index=np.arange(\n                    rows, rows+len(edgelist))), format='table', data_columns=True)\n            edgelist = np.array(edgelist)\n            if np.max(edgelist[:, :2]) > self.netshape[0]:\n                self.netshape[0] = np.max(edgelist[:, :2])\n            if np.max(edgelist[:, 2]) > self.netshape[1]:\n                self.netshape[1] = np.max(edgelist[:, 2])\n        else:\n            newedges = pd.DataFrame(edgelist, columns=colnames)\n            self.network = pd.concat(\n                [self.network, newedges], ignore_index=True, sort=True)\n            self._update_network()"}
{"prompt": "def ignore(self, filename):\n        \"\"\"Ignore a given filename or not.\"\"\"", "output": "\n        if filename == self.filename:\n            self.filename = None\n\n    def __repr__(self):\n        \"\"\"Return a string representation of the exception.\"\"\"\n        return 'Exception: %s\\n%s' % (self.msg, self.detail)\n\n\nclass TestException(Exception):\n    \"\"\"Test exception.\"\"\"\n    pass\n\n\nclass TestError(Exception):\n    \"\"\"Test error.\"\"\"\n    pass\n\n\nclass TestWarning(Exception):\n    \"\"\"Test warning.\"\"\"\n    pass\n\n\nclass TestWarning2(Exception):\n    \"\"\"Test warning.\"\"\"\n    pass\n\n\nclass TestWarning3(Exception):\n    \"\"\"Test warning.\"\"\"", "solution": "\n        _, ext = os.path.splitext(filename)\n        return ext in ['.pyc', '.pyo', '.o', '.swp']"}
{"prompt": "def create(plm, address, cat, subcat, firmware=None):\n    \"\"\"Create a device from device info data.\"\"\"", "output": "\n    if firmware is None:\n        firmware = get_device_info(plm, address)\n    if firmware is None:\n        raise ValueError(\"No device with address {}\".format(address))\n\n    if not isinstance(cat, list):\n        raise TypeError(\"cat must be a list\")\n\n    if not isinstance(subcat, list):\n        raise TypeError(\"subcat must be a list\")\n\n    if len(cat)!= len(subcat):\n        raise ValueError(\"number of categories and subcategories must be the same\")\n\n    if len(cat) == 0:\n        raise ValueError(\"No category or subcategory\")\n\n    if len", "solution": "\n    from insteonplm.devices.ipdb import IPDB\n    ipdb = IPDB()\n    product = ipdb[[cat, subcat]]\n    deviceclass = product.deviceclass\n    device = None\n    if deviceclass is not None:\n        device = deviceclass(plm, address, cat, subcat,\n                             product.product_key,\n                             product.description,\n                             product.model)\n    return device"}
{"prompt": "def _filter_rows(self, rows, visible_columns=None):\n        \"\"\"Filter `rows` based on the visible columns.\"\"\"", "output": "\n        if visible_columns is None:\n            visible_columns = self.visible_columns\n        for row in rows:\n            if row[0] in visible_columns:\n                continue\n            if visible_columns[row[0]] == row[1]:\n                yield row\n\n    def _get_rows_by_type(self, column, table):\n        \"\"\"Return a list of `Row` objects for `table` with the given columns.\"\"\"\n        if column.type == 'boolean':\n            return self._filter_rows(self.rows, table)\n        elif column.type == 'integer':\n            return self._filter", "solution": "\n        if visible_columns is None:\n            visible_columns = self.visible_columns\n        filtered_row = []\n        for idx, name in enumerate(self.columns.keys()):\n            if name in self.visible_columns:\n                filtered_row.append(rows[idx])\n        return filtered_row"}
{"prompt": "def getAttribute(self):\n        \"\"\"Return a name,value pair for the next attribute in the stream,\n        if one is found, or None\"\"\"", "output": "\n        if self.stream.currentByte == 0:\n            return None,None\n        else:\n            return self.stream.currentByte,self.stream.currentByte+1\n\n    def getAttributeCount(self):\n        \"\"\"Return the number of attributes of the next token\"\"\"\n        return len(self.stream.currentLine)\n\n    def getAttributeName(self, i):\n        \"\"\"Return the name of an attribute\"\"\"\n        return self.stream.currentLine[i].type\n\n    def getAttributeNameIndex(self, i):\n        \"\"\"Return the index of an attribute\"\"\"\n        return self.stream.currentLine[i", "solution": "\n        data = self.data\n        # Step 1 (skip chars)\n        c = data.skip(spaceCharactersBytes | frozenset([b\"/\"]))\n        assert c is None or len(c) == 1\n        # Step 2\n        if c in (b\">\", None):\n            return None\n        # Step 3\n        attrName = []\n        attrValue = []\n        # Step 4 attribute name\n        while True:\n            if c == b\"=\" and attrName:\n                break\n            elif c in spaceCharactersBytes:\n                # Step 6!\n                c = data.skip()\n                break\n            elif c in (b\"/\", b\">\"):\n                return b\"\".join(attrName), b\"\"\n            elif c in asciiUppercaseBytes:\n                attrName.append(c.lower())\n            elif c is None:\n                return None\n            else:\n                attrName.append(c)\n            # Step 5\n            c = next(data)\n        # Step 7\n        if c != b\"=\":\n            data.previous()\n            return b\"\".join(attrName), b\"\"\n        # Step 8\n        next(data)\n        # Step 9\n        c = data.skip()\n        # Step 10\n        if c in (b\"'\", b'\"'):\n            # 10.1\n            quoteChar = c\n            while True:\n                # 10.2\n                c = next(data)\n                # 10.3\n                if c == quoteChar:\n                    next(data)\n                    return b\"\".join(attrName), b\"\".join(attrValue)\n                # 10.4\n                elif c in asciiUppercaseBytes:\n                    attrValue.append(c.lower())\n                # 10.5\n                else:\n                    attrValue.append(c)\n        elif c == b\">\":\n            return b\"\".join(attrName), b\"\"\n        elif c in asciiUppercaseBytes:\n            attrValue.append(c.lower())\n        elif c is None:\n            return None\n        else:\n            attrValue.append(c)\n        # Step 11\n        while True:\n            c = next(data)\n            if c in spacesAngleBrackets:\n                return b\"\".join(attrName), b\"\".join(attrValue)\n            elif c in asciiUppercaseBytes:\n                attrValue.append(c.lower())\n            elif c is None:\n                return None\n            else:\n                attrValue.append(c)"}
{"prompt": "def present(name,\n            protocol=None,\n            service_address=None,\n            server_address=None,\n            packet_forward_method='dr',\n            weight=1\n           ):\n    \"\"\"\n    Ensure that the named service is present.\n\n    name\n        The LVS server name\n\n    protocol\n        The service protocol\n\n    service_address\n        The LVS service address\n\n    server_address\n        The real server address.\n\n    packet_forward_method\n        The LVS packet forwarding method(``dr`` for direct routing, ``tunnel`` for tunneling, ``nat`` for network access translation).\n\n    weight\n        The capacity  of a server relative to the others in the pool.\n\n\n    .. code-block:: yaml\n\n        lvsrs:\n          lvs_server.present:\n            - protocol: tcp\n            - service_address: 1.1.1.1:80\n            - server_address: 192.168.0.11:8080\n            - packet_forward_method: dr\n            - weight: 10\n    \"\"\"", "output": " LVS server\n\n    \"\"\"\n    if not protocol:\n        protocol = get_default_service_protocol()\n\n    if not service_address:\n        service_address = get_default_server_address()\n\n    if not server_address:\n        server_address = get_default_server_address()\n\n    if not packet_forward_method:\n        raise Exception(\"No packet forwarding method defined\")\n\n    if not server_address:\n        raise Exception(\"No real server address defined\")\n\n    if not packet_forward_method in protocol.service_dict:\n        raise Exception(\"Unknown packet forwarding method: %s\" % packet_forward", "solution": "\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n\n    #check server\n    server_check = __salt__['lvs.check_server'](protocol=protocol,\n                                                service_address=service_address,\n                                                server_address=server_address)\n    if server_check is True:\n        server_rule_check = __salt__['lvs.check_server'](protocol=protocol,\n                                                         service_address=service_address,\n                                                         server_address=server_address,\n                                                         packet_forward_method=packet_forward_method,\n                                                         weight=weight)\n        if server_rule_check is True:\n            ret['comment'] = 'LVS Server {0} in service {1}({2}) is present'.format(name, service_address, protocol)\n            return ret\n        else:\n            if __opts__['test']:\n                ret['result'] = None\n                ret['comment'] = 'LVS Server {0} in service {1}({2}) is present but some options should update'.format(name, service_address, protocol)\n                return ret\n            else:\n                server_edit = __salt__['lvs.edit_server'](protocol=protocol,\n                                                          service_address=service_address,\n                                                          server_address=server_address,\n                                                          packet_forward_method=packet_forward_method,\n                                                          weight=weight)\n                if server_edit is True:\n                    ret['comment'] = 'LVS Server {0} in service {1}({2}) has been updated'.format(name, service_address, protocol)\n                    ret['changes'][name] = 'Update'\n                    return ret\n                else:\n                    ret['result'] = False\n                    ret['comment'] = 'LVS Server {0} in service {1}({2}) update failed({3})'.format(name, service_address, protocol, server_edit)\n                    return ret\n    else:\n        if __opts__['test']:\n            ret['comment'] = 'LVS Server {0} in service {1}({2}) is not present and needs to be created'.format(name, service_address, protocol)\n            ret['result'] = None\n            return ret\n        else:\n            server_add = __salt__['lvs.add_server'](protocol=protocol,\n                                                    service_address=service_address,\n                                                    server_address=server_address,\n                                                    packet_forward_method=packet_forward_method,\n                                                    weight=weight)\n            if server_add is True:\n                ret['comment'] = 'LVS Server {0} in service {1}({2}) has been created'.format(name, service_address, protocol)\n                ret['changes'][name] = 'Present'\n                return ret\n            else:\n                ret['comment'] = 'LVS Service {0} in service {1}({2}) create failed({3})'.format(name, service_address, protocol, server_add)\n                ret['result'] = False\n                return ret"}
{"prompt": "def load_from_file(filepath):\n        \"\"\"\n        Return user-written class object from given path.\n        \"\"\"", "output": "\n        with open(filepath, 'r') as f:\n            return cls(f.read(), path=filepath)\n\n    def __init__(self, path):\n        self.path = path\n        self.classes = {}\n        self.classes_by_path = {}\n        self.classes_by_path_by_name = {}\n        self.classes_by_path_by_path = {}\n        self.classes_by_path = {}\n        self.classes_by_path_by_path_by_path_by_name = {}\n        self.classes_by_path_by_path_", "solution": "\n        class_inst = None\n        expected_class = \"Py3status\"\n        module_name, file_ext = os.path.splitext(os.path.split(filepath)[-1])\n        if file_ext.lower() == \".py\":\n            py_mod = imp.load_source(module_name, filepath)\n            if hasattr(py_mod, expected_class):\n                class_inst = py_mod.Py3status()\n        return class_inst"}
{"prompt": "def attach_issue(resource_id, table, user_id):\n    \"\"\"Attach an issue to a specific job.\"\"\"", "output": "\n    return _issue_attachments(table, user_id, resource_id)\n\n\ndef _issue_attachments(table, user_id, resource_id):\n    \"\"\"Get the attachments for a specific job.\"\"\"\n    attachments = []\n    for row in table.rows:\n        if row.resource_id == resource_id:\n            attachments.append(row.attachments)\n    return attachments\n\n\ndef _get_issue_job_attachments(user_id, job_id):\n    \"\"\"Get the attachments for a specific job.\"\"\"\n    return _issue_attachments(user_id, job_id, job_", "solution": "\n    data = schemas.issue.post(flask.request.json)\n    issue = _get_or_create_issue(data)\n\n    # Second, insert a join record in the JOIN_JOBS_ISSUES or\n    # JOIN_COMPONENTS_ISSUES database.\n    if table.name == 'jobs':\n        join_table = models.JOIN_JOBS_ISSUES\n    else:\n        join_table = models.JOIN_COMPONENTS_ISSUES\n\n    key = '%s_id' % table.name[0:-1]\n    query = join_table.insert().values({\n        'user_id': user_id,\n        'issue_id': issue['id'],\n        key: resource_id\n    })\n\n    try:\n        flask.g.db_conn.execute(query)\n    except sa_exc.IntegrityError:\n        raise dci_exc.DCICreationConflict(join_table.name,\n                                          '%s, issue_id' % key)\n\n    result = json.dumps({'issue': dict(issue)})\n    return flask.Response(result, 201, content_type='application/json')"}
{"prompt": "def filter_installed_packages(packages):\n    \"\"\"Return a list of packages that require installation.\"\"\"", "output": "\n    return [p for p in packages if p.startswith('python-apt')]\n\n\ndef filter_installed_packages_by_name(packages, name):\n    \"\"\"Return a list of packages that require installation with the given name.\"\"\"\n    return [p for p in packages if p.startswith(name)]\n\n\ndef filter_installed_packages_by_version(packages, version):\n    \"\"\"Return a list of packages that require installation with the given version.\"\"\"\n    return [p for p in packages if p.startswith('python-apt')]\n\n\ndef filter_installed_packages_by_name_and_", "solution": "\n    cache = apt_cache()\n    _pkgs = []\n    for package in packages:\n        try:\n            p = cache[package]\n            p.current_ver or _pkgs.append(package)\n        except KeyError:\n            log('Package {} has no installation candidate.'.format(package),\n                level='WARNING')\n            _pkgs.append(package)\n    return _pkgs"}
{"prompt": "def generate_signature_class(cls):\n    \"\"\"\n    Generate a declarative model for storing signatures related to the given\n    cls parameter.\n\n    :param class cls: The declarative model to generate a signature class for.\n    :return: The signature class, as a declarative derived from Base.\n    \"\"\"", "output": "\n    return type('Base', (object,), {'__tablename__': 'base', 'id': cls.id})\n\n\nclass Base(object):\n    \"\"\"\n    Base class for all the models.\n    \"\"\"\n    __tablename__ ='models'\n    __table_args__ = (\n        UniqueConstraint('name', 'type'),\n        {'extend_existing': True,'mysql_engine': 'InnoDB',\n        'mysql_charset': 'utf8'}\n    )\n\n    id = Column(Integer, primary_key=True)\n    name = Column(String(255))\n    type = Column(String(255))", "solution": "\n    return type(\"%sSigs\" % cls.__name__, (Base,),\n                {'__tablename__': \"%s_sigs\" % cls.__tablename__,\n                 'id': sa.Column(sa.Integer,\n                                 sa.Sequence('%s_id_seq' % cls.__tablename__),\n                                 primary_key=True,\n                                 doc=\"primary key\"),\n                 'data': sa.Column(sa.Text(), nullable=False,\n                                   doc=\"The signed data\"),\n                 '%s_id' % cls.__tablename__: sa.Column(sa.Integer,\n                                                        sa.ForeignKey(\"%s.id\" % cls.__tablename__),\n                                                        nullable=False)})"}
{"prompt": "def drop_table(self, table):\n        \"\"\"\n        Drop a table from the MyDB context.\n\n        ## Arguments\n\n        * `table` (str): The name of the table to drop.\n\n        \"\"\"", "output": "\n        self.execute(\"DROP TABLE IF EXISTS %s\" % table)\n\n    def drop_table_if_exists(self, table, cascade=True):\n        \"\"\"\n        Drop a table from the MyDB context.\n\n        * `table` (str): The name of the table to drop.\n\n        * `cascade` (bool): If True, drop the table if it exists.\n\n        \"\"\"\n        self.execute(\"DROP TABLE IF EXISTS %s\" % table)\n        if cascade:\n            self.execute(\"DROP TABLE IF EXISTS %s CASCADE\" % table)\n\n    def get_tables(self):\n        \"\"\"\n       ", "solution": "\n        job_id = self.submit(\"DROP TABLE %s\"%table, context=\"MYDB\")\n        status = self.monitor(job_id)\n        if status[0] != 5:\n            raise Exception(\"Couldn't drop table %s\"%table)"}
{"prompt": "def views(self, install, comp_sum):\n        \"\"\"Views packages\n        \"\"\"", "output": "\n        self.log.info(\"Creating views\")\n        self.create_views(install)\n        self.log.info(\"Installing views\")\n        self.install_views(install, comp_sum)\n\n    def create_views(self, install):\n        \"\"\"Create packages\n        \"\"\"\n        self.log.info(\"Creating packages\")\n        self.log.info(\"Installing packages\")\n        self.log.info(\"Creating package_id\")\n        self.package_id = self.get_package_id(install)\n        self.log.info(\"Package created\")\n\n    def install_views(self, install, comp", "solution": "\n        pkg_sum = uni_sum = upg_sum = 0\n        # fix repositories align\n        repo = self.repo + (\" \" * (6 - (len(self.repo))))\n        for pkg, comp in zip(install, comp_sum):\n            pkg_repo = split_package(pkg[:-4])\n            if find_package(pkg[:-4], self.meta.pkg_path):\n                pkg_sum += 1\n                COLOR = self.meta.color[\"GREEN\"]\n            elif pkg_repo[0] == GetFromInstalled(pkg_repo[0]).name():\n                COLOR = self.meta.color[\"YELLOW\"]\n                upg_sum += 1\n            else:\n                COLOR = self.meta.color[\"RED\"]\n                uni_sum += 1\n            ver = GetFromInstalled(pkg_repo[0]).version()\n            print(\"  {0}{1}{2}{3} {4}{5} {6}{7}{8}{9}{10}{11:>11}{12}\".format(\n                COLOR, pkg_repo[0] + ver, self.meta.color[\"ENDC\"],\n                \" \" * (23-len(pkg_repo[0] + ver)), pkg_repo[1],\n                \" \" * (18-len(pkg_repo[1])), pkg_repo[2],\n                \" \" * (8-len(pkg_repo[2])), pkg_repo[3],\n                \" \" * (7-len(pkg_repo[3])), repo,\n                comp, \" K\")).rstrip()\n        return [pkg_sum, upg_sum, uni_sum]"}
{"prompt": "def sources(self):\n        \"\"\"A tuple containing the names of the Class sources for this Slot.\n\n        The Python equivalent of the CLIPS slot-sources function.\n\n        \"\"\"", "output": "\n        return self._sources\n\n    @property\n    def name(self):\n        \"\"\"The name of the slot.\n\n        The name of a slot is a string.\n\n        \"\"\"\n        return self._name\n\n    @property\n    def source(self):\n        \"\"\"The source of the slot.\n\n        The source of a slot is a string.\n\n        \"\"\"\n        return self._sources[0]\n\n    @property\n    def source_id(self):\n        \"\"\"The source of the slot.\n\n        The ID of a slot is a number.\n\n        \"\"\"\n        return self._sources[1]\n\n    @property\n    def source_name(self):\n        \"\"\"", "solution": "\n        data = clips.data.DataObject(self._env)\n\n        lib.EnvSlotSources(self._env, self._cls, self._name, data.byref)\n\n        return tuple(data.value) if isinstance(data.value, list) else ()"}
{"prompt": "def data_segments(paths, channel, warn=True):\n    \"\"\"Returns the segments containing data for a channel\n\n    **Requires:** |LDAStools.frameCPP|_\n\n    A frame is considered to contain data if a valid FrData structure\n    (of any type) exists for the channel in that frame.  No checks\n    are directly made against the underlying FrVect structures.\n\n    Parameters\n    ----------\n    paths : `list` of `str`\n        a list of GWF file paths\n\n    channel : `str`\n        the name to check in each frame\n\n    warn : `bool`, optional\n        emit a `UserWarning` when a channel is not found in a frame\n\n    Returns\n    -------\n    segments : `~gwpy.segments.SegmentList`\n        the list of segments containing data\n    \"\"\"", "output": "` if a warning should be emitted\n\n    Returns\n    -------\n    `list` of `str`\n        the list of data segments\n    \"\"\"\n    for path in paths:\n        if not path.endswith('.gwf'):\n            continue\n        if not os.path.exists(path):\n            continue\n        with open(path, 'r') as f:\n            lines = f.readlines()\n        if len(lines)!= 6:\n            raise ValueError('Invalid path %s' % path)\n        if len(lines[0])!= 6:\n            raise ValueError('Invalid path %s' % path)\n        if len(lines[1", "solution": "\n    segments = SegmentList()\n    for path in paths:\n        segments.extend(_gwf_channel_segments(path, channel, warn=warn))\n    return segments.coalesce()"}
{"prompt": "def update(self, ttl=values.unset, collection_ttl=values.unset):\n        \"\"\"\n        Update the SyncMapInstance\n\n        :param unicode ttl: Alias for collection_ttl\n        :param unicode collection_ttl: New time-to-live of this Map in seconds.\n\n        :returns: Updated SyncMapInstance\n        :rtype: twilio.rest.sync.v1.service.sync_map.SyncMapInstance\n        \"\"\"", "output": "\n        data = values.of({\n            'FriendlyName': friendly_name,\n            'SyncStatus': SyncStatus,\n            'SyncMapId': sync_map_id,\n        })\n\n        payload = self._version.create(method='POST', uri=self._uri, data=data, )\n\n        return SyncMapInstance(\n            self._version,\n            payload,\n            service_sid=self._solution['service_sid'],\n            sid=self._solution['sid'],\n        )\n\n    def stream(self, limit=None, page_size=None):\n        \"\"\"\n        Streams SyncMapInstance records from the", "solution": "\n        data = values.of({'Ttl': ttl, 'CollectionTtl': collection_ttl, })\n\n        payload = self._version.update(\n            'POST',\n            self._uri,\n            data=data,\n        )\n\n        return SyncMapInstance(\n            self._version,\n            payload,\n            service_sid=self._solution['service_sid'],\n            sid=self._solution['sid'],\n        )"}
{"prompt": "def update_name(self, name):\n        \"\"\"Changes the name of this executable and the reference to it in the\n        parent module.\"\"\"", "output": "\n        self.name = name\n\n    def get_name(self):\n        \"\"\"Returns the name of this executable.\"\"\"\n        return self.name\n\n    def get_command(self):\n        \"\"\"Returns the command line used to start the executable.\"\"\"\n        return self.command\n\n    def get_env(self):\n        \"\"\"Returns the environment used to start the executable.\"\"\"\n        return self.env\n\n    def get_cwd(self):\n        \"\"\"Returns the current working directory.\"\"\"\n        return self.cwd\n\n    def get_env_path(self):\n        \"\"\"Returns the environment path.\"\"\"\n        return self.env_path\n\n    def get_env_", "solution": "\n        if name != self.name:\n            self.parent.executables[name] = self\n            del self.parent.executables[self.name]\n            self.name = name"}
{"prompt": "def __initialize_ui(self):\n        \"\"\"\n        Initializes the Widget ui.\n        \"\"\"", "output": "\n        self.ui = Ui_Form()\n        self.ui.setupUi(self.ui)\n        self.ui.buttonBox.accepted.connect(self.__on_button_accepted)\n        self.ui.buttonBox.rejected.connect(self.__on_button_rejected)\n        self.ui.buttonBox.buttonClicked.connect(self.__on_button_clicked)\n        self.ui.buttonBox.buttonReleased.connect(self.__on_button_released)\n        self.ui.buttonBox.buttonClicked.connect(self.__on_button_clicked)\n        self", "solution": "\n\n        self.__margin_area_LinesNumbers_widget = LinesNumbers_QWidget(self)\n\n        self.__set_extra_selections()\n\n        self.__set_language_description()\n\n        # Signals / Slots.\n        self.blockCountChanged.connect(self.__margin_area_LinesNumbers_widget.set_editor_viewport_margins)\n        self.updateRequest.connect(self.__margin_area_LinesNumbers_widget.update_rectangle)\n        self.cursorPositionChanged.connect(self.__set_extra_selections)"}
{"prompt": "def eval_dir(\n    reference_dir,\n    estimates_dir,\n    output_dir=None,\n    mode='v4',\n    win=1.0,\n    hop=1.0,\n):\n    \"\"\"Compute bss_eval metrics for two given directories assuming file\n    names are identical for both, reference source and estimates.\n\n    Parameters\n    ----------\n    reference_dir : str\n        path to reference sources directory.\n    estimates_dir : str\n        path to estimates directory.\n    output_dir : str\n        path to output directory used to save evaluation results. Defaults to\n        `None`, meaning no evaluation files will be saved.\n    mode : str\n        bsseval version number. Defaults to 'v4'.\n    win : int\n        window size in\n\n    Returns\n    -------\n    scores : EvalStore\n        scores object that holds the framewise and global evaluation scores.\n    \"\"\"", "output": " saved.\n    mode : str\n        'v4' or 'v4-weighted'\n    win : float\n        The window size (in terms of number of features) of the BSS.\n    hop : float\n        The hop size (in terms of number of features) of the BSS.\n\n    Returns\n    -------\n    dict\n        Dictionary of metrics.\n    \"\"\"\n    if output_dir is None:\n        output_dir = reference_dir\n    if output_dir is not None:\n        if output_dir.endswith('.bss'):\n            output_dir = output_dir[:-4]\n        if output_dir.endswith('.", "solution": "\n\n    reference = []\n    estimates = []\n\n    data = EvalStore(win=win, hop=hop)\n\n    global_rate = None\n    reference_glob = os.path.join(reference_dir, '*.wav')\n    # Load in each reference file in the supplied dir\n    for reference_file in glob.glob(reference_glob):\n        ref_audio, rate = sf.read(\n            reference_file,\n            always_2d=True\n        )\n        # Make sure fs is the same for all files\n        assert (global_rate is None or rate == global_rate)\n        global_rate = rate\n        reference.append(ref_audio)\n\n    if not reference:\n        raise ValueError('`reference_dir` contains no wav files')\n\n    estimated_glob = os.path.join(estimates_dir, '*.wav')\n    targets = []\n    for estimated_file in glob.glob(estimated_glob):\n        targets.append(os.path.basename(estimated_file))\n        ref_audio, rate = sf.read(\n            estimated_file,\n            always_2d=True\n        )\n        assert (global_rate is None or rate == global_rate)\n        global_rate = rate\n        estimates.append(ref_audio)\n\n    SDR, ISR, SIR, SAR = evaluate(\n        reference,\n        estimates,\n        win=int(win*global_rate),\n        hop=int(hop*global_rate),\n        mode=mode\n    )\n    for i, target in enumerate(targets):\n        values = {\n            \"SDR\": SDR[i].tolist(),\n            \"SIR\": SIR[i].tolist(),\n            \"ISR\": ISR[i].tolist(),\n            \"SAR\": SAR[i].tolist()\n        }\n\n        data.add_target(\n            target_name=target,\n            values=values\n        )\n\n    return data"}
{"prompt": "def save(self):\n        \"\"\"\n        Saves changes made to the locally cached DesignDocument object's data\n        structures to the remote database.  If the design document does not\n        exist remotely then it is created in the remote database.  If the object\n        does exist remotely then the design document is updated remotely.  In\n        either case the locally cached DesignDocument object is also updated\n        accordingly based on the successful response of the operation.\n        \"\"\"", "output": "\n        self.db.execute(\"UPDATE \" + self.design_doc_table + \" SET data_structure_id = %s WHERE data_structure_id = %s\",\n                           (self.data_structure_id, self.data_structure_id,\n                            self.design_doc_id))\n        self.db.execute(\"UPDATE \" + self.design_doc_table + \" SET data_structure_id = %s WHERE data_structure_id = %s\",\n                           (self.data_structure_id, self.data_structure_id,\n                            self.design_doc_", "solution": "\n        if self.views:\n            if self.get('language', None) != QUERY_LANGUAGE:\n                for view_name, view in self.iterviews():\n                    if isinstance(view, QueryIndexView):\n                        raise CloudantDesignDocumentException(104, view_name)\n            else:\n                for view_name, view in self.iterviews():\n                    if not isinstance(view, QueryIndexView):\n                        raise CloudantDesignDocumentException(105, view_name)\n\n        if self.indexes:\n            if self.get('language', None) != QUERY_LANGUAGE:\n                for index_name, search in self.iterindexes():\n                    # Check the instance of the javascript search function\n                    if not isinstance(search['index'], STRTYPE):\n                        raise CloudantDesignDocumentException(106, index_name)\n            else:\n                for index_name, index in self.iterindexes():\n                    if not isinstance(index['index'], dict):\n                        raise CloudantDesignDocumentException(107, index_name)\n\n        for prop in self._nested_object_names:\n            if not getattr(self, prop):\n                # Ensure empty dict for each sub-object is not saved remotely.\n                self.__delitem__(prop)\n\n        super(DesignDocument, self).save()\n\n        for prop in self._nested_object_names:\n            # Ensure views, indexes, and lists dict exist in locally cached DesignDocument.\n            getattr(self, prop, self.setdefault(prop, dict()))"}
{"prompt": "def keep_alive(self):\n        \"\"\"\n        Send a keep alive request periodically to make sure that the server\n        is still alive. If not then try to reconnect.\n        \"\"\"", "output": "\n        if self.server_type == 'http':\n            self.send_keep_alive_request()\n        else:\n            self.send_keep_alive_request_no_content()\n\n    def send_keep_alive_request(self):\n        \"\"\"\n        Send a keep alive request to the server.\n        \"\"\"\n        self.send_request(\n            'POST',\n            '/api/v1/keepalive',\n            body=json.dumps({\n                'keep_alive': keep_alive.to_json(),\n                'timeout': keep_alive.to_json(),\n               'request_id': self.request", "solution": "\n        self.ssl_skt.settimeout(defaults.AUTH_KEEP_ALIVE_INTERVAL)\n        while self.__up:\n            try:\n                log.debug('Sending keep-alive message to the server')\n                self.ssl_skt.send(defaults.AUTH_KEEP_ALIVE)\n            except socket.error:\n                log.error('Unable to send keep-alive message to the server.')\n                log.error('Re-init the SSL socket.')\n                self.reconnect()\n                log.debug('Trying to re-send the keep-alive message to the server.')\n                self.ssl_skt.send(defaults.AUTH_KEEP_ALIVE)\n            msg = self.ssl_skt.recv(len(defaults.AUTH_KEEP_ALIVE_ACK))\n            log.debug('Received %s from the keep-alive server', msg)\n            if msg != defaults.AUTH_KEEP_ALIVE_ACK:\n                log.error('Received %s instead of %s form the auth keep-alive server',\n                          msg, defaults.AUTH_KEEP_ALIVE_ACK)\n                log.error('Re-init the SSL socket.')\n                self.reconnect()\n            time.sleep(defaults.AUTH_KEEP_ALIVE_INTERVAL)"}
{"prompt": "def collect_conflicts_between_fragments(\n    context: ValidationContext,\n    conflicts: List[Conflict],\n    cached_fields_and_fragment_names: Dict,\n    compared_fragment_pairs: \"PairSet\",\n    are_mutually_exclusive: bool,\n    fragment_name1: str,\n    fragment_name2: str,\n) -> None:\n    \"\"\"Collect conflicts between fragments.\n\n    Collect all conflicts found between two fragments, including via spreading in any\n    nested fragments.\n    \"\"\"", "output": "\n    conflicts_by_name = defaultdict(set)\n    for conflict in conflicts:\n        if not isinstance(conflict, SemanticVersionConflict):\n            continue\n        if not isinstance(cached_fields_and_fragment_names, dict):\n            cached_fields_and_fragment_names = {}\n        if not fragment_name1 in compared_fragment_pairs:\n            continue\n        if not fragment_name2 in compared_fragment_pairs:\n            continue\n        if not are_mutually_exclusive:\n            cached_fields_and_fragment_names[fragment_name1].add(fragment_name2)\n        if not are", "solution": "\n    # No need to compare a fragment to itself.\n    if fragment_name1 == fragment_name2:\n        return\n\n    # Memoize so two fragments are not compared for conflicts more than once.\n    if compared_fragment_pairs.has(\n        fragment_name1, fragment_name2, are_mutually_exclusive\n    ):\n        return\n    compared_fragment_pairs.add(fragment_name1, fragment_name2, are_mutually_exclusive)\n\n    fragment1 = context.get_fragment(fragment_name1)\n    fragment2 = context.get_fragment(fragment_name2)\n    if not fragment1 or not fragment2:\n        return None\n\n    field_map1, fragment_names1 = get_referenced_fields_and_fragment_names(\n        context, cached_fields_and_fragment_names, fragment1\n    )\n\n    field_map2, fragment_names2 = get_referenced_fields_and_fragment_names(\n        context, cached_fields_and_fragment_names, fragment2\n    )\n\n    # (F) First, collect all conflicts between these two collections of fields\n    # (not including any nested fragments)\n    collect_conflicts_between(\n        context,\n        conflicts,\n        cached_fields_and_fragment_names,\n        compared_fragment_pairs,\n        are_mutually_exclusive,\n        field_map1,\n        field_map2,\n    )\n\n    # (G) Then collect conflicts between the first fragment and any nested fragments\n    # spread in the second fragment.\n    for nested_fragment_name2 in fragment_names2:\n        collect_conflicts_between_fragments(\n            context,\n            conflicts,\n            cached_fields_and_fragment_names,\n            compared_fragment_pairs,\n            are_mutually_exclusive,\n            fragment_name1,\n            nested_fragment_name2,\n        )\n\n    # (G) Then collect conflicts between the second fragment and any nested fragments\n    # spread in the first fragment.\n    for nested_fragment_name1 in fragment_names1:\n        collect_conflicts_between_fragments(\n            context,\n            conflicts,\n            cached_fields_and_fragment_names,\n            compared_fragment_pairs,\n            are_mutually_exclusive,\n            nested_fragment_name1,\n            fragment_name2,\n        )"}
{"prompt": "def path_regex(self):\n        \"\"\"Return the regex for the path to the build folder.\"\"\"", "output": "\n        return self._path_regex\n\n    @property\n    def path_regex_match(self):\n        \"\"\"Return the regex for the path to the build folder.\"\"\"\n        return self._path_regex_match\n\n    @property\n    def build_path(self):\n        \"\"\"Return the path to the build folder.\"\"\"\n        return self._build_path\n\n    @property\n    def build_path_is_safe(self):\n        \"\"\"Return True if the build folder is safe.\"\"\"\n        return self._build_path!= self.build_path_safe\n\n    @property\n    def build_path_is_not_safe(self):", "solution": "\n        try:\n            path = '%s/' % urljoin(self.monthly_build_list_regex,\n                                   self.builds[self.build_index])\n            if self.application in APPLICATIONS_MULTI_LOCALE \\\n                    and self.locale != 'multi':\n                path = '%s/' % urljoin(path, self.locale)\n            return path\n        except Exception:\n            folder = urljoin(self.base_url, self.monthly_build_list_regex)\n            raise errors.NotFoundError(\"Specified sub folder cannot be found\",\n                                       folder)"}
{"prompt": "def download_song_by_search(self, song_name):\n        \"\"\"Download a song by its name.\n\n        :params song_name: song name.\n        \"\"\"", "output": "\n        self.download_song(song_name)\n        self.download_song_by_id(song_name)\n\n    def download_song(self, song_name):\n        \"\"\"Download a song.\n\n        :params song_name: song name.\n        \"\"\"\n        self.logger.info(\"Downloading song %s\" % song_name)\n        self.logger.debug(\"song_name = %s\" % song_name)\n        self.logger.debug(\"song_id = %s\" % song_name.split(\" \")[0])\n        self.logger.debug(\"song_name.", "solution": "\n\n        try:\n            song = self.crawler.search_song(song_name, self.quiet)\n        except RequestException as exception:\n            click.echo(exception)\n        else:\n            self.download_song_by_id(song.song_id, song.song_name, self.folder)"}
{"prompt": "def get_tree_for_studio_id(self, studio_id):\n        \"\"\"\n        Returns the full json tree (recusive calls to /api/get_nodes_by_ids_complete)\n        \"\"\"", "output": "\n        return self._get_tree(self.api_root + '/nodes', studio_id)\n\n    def _get_tree(self, node_id, studio_id):\n        \"\"\"\n        Returns the full json tree (recusive calls to /api/get_node_by_id_complete)\n        \"\"\"\n        return self.client.get_json(self.path + '/nodes/' + node_id + '/children/' + str(studio_id))\n\n    def _get_node(self, node_id):\n        \"\"\"\n        Returns the full json node (recusive calls to", "solution": "\n        channel_parent = {'children': []}  # this is like _ with children\n        def _build_subtree(parent, studio_id):\n            subtree = self.get_nodes_by_ids_complete(studio_id)\n            if 'children' in subtree:\n                children_refs = subtree['children']\n                subtree['children'] = []\n                for child_studio_id in children_refs:\n                    _build_subtree(subtree, child_studio_id)\n            parent['children'].append(subtree)\n        _build_subtree(channel_parent, studio_id)\n        channel = channel_parent['children'][0]\n        return channel"}
{"prompt": "def _get_adc_value(self, channel, average=None):\n        \"\"\"Read ADC\n        \"\"\"", "output": "\n        return self._adc_data[channel][0] if average is None else average\n\n    def _get_adc_data(self):\n        \"\"\"Read ADC\n        \"\"\"\n        return self._adc_data\n\n    def _get_adc_value(self, average=None):\n        \"\"\"Read ADC\n        \"\"\"\n        return self._adc_data[self._adc_name][0] if average is None else average\n\n    def _get_adc_data(self):\n        \"\"\"Read ADC\n        \"\"\"\n        return self._adc_data\n\n    def _get_adc_value(self, average=None):\n       ", "solution": "\n        conf = self.SCAN_OFF | self.SINGLE_ENDED | ((0x1e) & (channel << 1))\n        self._intf.write(self._base_addr + self.MAX_1239_ADD, array('B', pack('B', conf)))\n\n        def read_data():\n            ret = self._intf.read(self._base_addr + self.MAX_1239_ADD | 1, size=2)\n            ret.reverse()\n            ret[1] = ret[1] & 0x0f  # 12-bit ADC\n            return unpack_from('H', ret)[0]\n\n        if average:\n            raw = 0\n            for _ in range(average):\n                raw += read_data()\n            raw /= average\n        else:\n            raw = read_data()\n\n        return raw"}
{"prompt": "def find_videos_by_show(self, show_id, show_videotype=None,\n                            show_videostage=None, orderby='videoseq-asc',\n                            page=1, count=20):\n        \"\"\"doc: http://open.youku.com/docs/doc?id=64\n        \"\"\"", "output": "\n        params = {'id': show_id,\n                  'orderby': orderby,\n                  'page': page,\n                  'count': count,\n                 'show_videotype': show_videotype,\n                 'show_videostage': show_videostage}\n        if show_videotype:\n            params['show_videotype'] = show_videotype\n        if show_videostage:\n            params['show_videostage'] = show_videostage\n        videos = self._get_videos(params)\n        return videos\n\n    def _get_videos(self, params={}):\n        \"\"\"doc:", "solution": "\n        url = 'https://openapi.youku.com/v2/shows/videos.json'\n        params = {\n            'client_id': self.client_id,\n            'show_id': show_id,\n            'page': page,\n            'count': count,\n            'show_videotype': show_videotype,\n            'show_videostage': show_videostage,\n            'orderby': orderby\n        }\n        params = remove_none_value(params)\n        r = requests.get(url, params=params)\n        check_error(r)\n        return r.json()"}
{"prompt": "def _handleClassAttr(self):\n        \"\"\"\n            _handleClassAttr - Hack to ensure \"class\" and \"style\" show up in attributes when classes are set,\n                and doesn't when no classes are present on associated tag.\n\n                TODO: I don't like this hack.\n        \"\"\"", "output": "\n        if self.style:\n            self.style = self.style.strip()\n        if self.style:\n            self.style = self.style.replace(\" \", \"\")\n        if self.style:\n            self.style = self.style.replace(\" \", \"\")\n        if self.classes:\n            self.classes = self.style.split(\",\")\n        else:\n            self.classes = []\n        if self.classes:\n            self.classes = [x.strip() for x in self.classes]\n        if self.classes:\n            self.classes = [x for x in self.classes if x not in self", "solution": "\n        if len(self.tag._classNames) > 0:\n            dict.__setitem__(self, \"class\", self.tag.className)\n        else:\n            try:\n                dict.__delitem__(self, \"class\")\n            except:\n                pass\n\n        styleAttr = self.tag.style\n        if styleAttr.isEmpty() is False:\n            dict.__setitem__(self, \"style\", styleAttr)\n        else:\n            try:\n                dict.__delitem__(self, \"style\")\n            except:\n                pass"}
{"prompt": "def main():\n    \"\"\"\n    Wrapper for OGR\n    \"\"\"", "output": "\n    return ogr_to_ogr(sys.argv[1])\n\ndef ogr_to_ogr_line(ogr_file, layer_name, srid, options=None):\n    \"\"\"\n    Convert an OGR-supported data type to an OGR line.\n    \"\"\"\n    if options is None:\n        options = {}\n    if srid!= 4326:\n        options['srid'] = srid\n    if layer_name!= \"testlayer\":\n        options['layer_name'] = layer_name\n    if options['layer_name'] == \"testlayer2\":\n        options['srs'] = \"EPSG:4326\"\n    if options", "solution": "\n\n    parser = argparse.ArgumentParser(\n        description='Command line interface to python-ontobio.golr library'\n        "}
{"prompt": "def formalised_address(self):\n        \"\"\"\n        This method returns the formalised address.\n        :return:\n        \"\"\"", "output": "\n        return self.address\n\n    def set_address(self, address):\n        \"\"\"\n        This method sets the address of the node.\n        :param address:\n        :return:\n        \"\"\"\n        self.address = address\n\n    def get_address(self):\n        \"\"\"\n        This method returns the address of the node.\n        :return:\n        \"\"\"\n        return self.address\n\n    def get_name(self):\n        \"\"\"\n        This method returns the name of the node.\n        :return:\n        \"\"\"\n        return self.name\n\n    def set_name(self, name):\n        \"\"\"\n        This method sets the name of the node.", "solution": "\n        try:\n            if self._data_from_search:\n                t = self._data_from_search.find('a').contents[0]\n            else:\n                t = self._ad_page_content.find(\n                    'div', {'class': 'smi-object-header'}).find(\n                    'h1').text.strip()\n\n        except Exception as e:\n            if self._debug:\n                logging.error(\n                    \"Error getting formalised_address. Error message: \" + e.args[0])\n            return\n        s = t.split('-')\n        a = s[0].strip()\n        if 'SALE AGREED' in a:\n            a = a.split()\n            a = a[3:]\n            a = ' '.join([str(x) for x in a])\n        return a.lower().title().strip()"}
{"prompt": "def add_info(self, header, message):\n        \"\"\"Adds \"header: message\" line to error message\"\"\"", "output": "\n        self.errors.append((header, message))\n\n    def add_warning(self, header, message):\n        \"\"\"Adds \"header: message\" line to warning message\"\"\"\n        self.warnings.append((header, message))\n\n    def add_error(self, header, message):\n        \"\"\"Adds \"header: message\" line to error message\"\"\"\n        self.errors.append((header, message))\n\n    def __str__(self):\n        \"\"\"Returns a string representation of the error\"\"\"\n        return '\\n'.join(self.errors)\n\n    def __repr__(self):\n        \"\"\"Returns a representation of the error\"\"\"\n       ", "solution": "\n        current_message = self.args[0]\n        message = current_message + self._format_info(header, message)\n        self.args = (message,) + self.args[1:]"}
{"prompt": "def reset(self, keep_state=False):\n        \"\"\"Reset the shared state and drain Django Channels.\n\n        :param keep_state: If ``True``, do not reset the shared manager\n            state (useful in tests, where the settings overrides need to\n            be kept). Defaults to ``False``.\n        \"\"\"", "output": "\n        self.manager.reset()\n        if keep_state:\n            self.shared_state.clear()\n            self.manager.setup_shared_state()\n\n    def get_messages(self):\n        \"\"\"Return a list of all messages.\"\"\"\n        return self.messages.all()\n\n    def get_message(self, msg_id):\n        \"\"\"Return a single message.\"\"\"\n        return self.messages.get(pk=msg_id)\n\n    def get_messages_for_user(self, user):\n        \"\"\"Return a list of all messages for a given user.\"\"\"\n        return self.get_messages().filter(", "solution": "\n        if not keep_state:\n            self.state = state.ManagerState(state.MANAGER_STATE_PREFIX)\n            self.state.reset()\n        async_to_sync(consumer.run_consumer)(timeout=1)\n        async_to_sync(self.sync_counter.reset)()"}
{"prompt": "def create_onvif_service(self, name, from_template=True, portType=None):\n        \"\"\"Create ONVIF service client\"\"\"", "output": "\n        if portType is None:\n            portType = self.portType\n        if from_template:\n            self.from_template = True\n        if name is None:\n            name = self.name\n        if from_template:\n            self.from_template = False\n        if name is None:\n            raise ValueError(\"name or portType is required\")\n        if self.from_template:\n            self.from_template = False\n        if name is None:\n            raise ValueError(\"name or portType is required\")\n        if portType is None:\n            raise ValueError(\"portType is required\")\n        if portType == ONVIF", "solution": "\n\n        name = name.lower()\n        xaddr, wsdl_file = self.get_definition(name)\n\n        with self.services_lock:\n            svt = self.services_template.get(name)\n            # Has a template, clone from it. Faster.\n            if svt and from_template and self.use_services_template.get(name):\n                service = ONVIFService.clone(svt, xaddr, self.user,\n                                             self.passwd, wsdl_file,\n                                             self.cache_location,\n                                             self.cache_duration,\n                                             self.encrypt,\n                                             self.daemon,\n                                             no_cache=self.no_cache, portType=portType, dt_diff=self.dt_diff)\n            # No template, create new service from wsdl document.\n            # A little time-comsuming\n            else:\n                service = ONVIFService(xaddr, self.user, self.passwd,\n                                       wsdl_file, self.cache_location,\n                                       self.cache_duration, self.encrypt,\n                                       self.daemon, no_cache=self.no_cache, portType=portType, dt_diff=self.dt_diff)\n\n            self.services[name] = service\n\n            setattr(self, name, service)\n            if not self.services_template.get(name):\n                self.services_template[name] = service\n\n        return service"}
{"prompt": "def _start_enqueue_thread(self):\n\t\t\"\"\" Internal method to start the enqueue thread which adds the events in an internal queue. \"\"\"", "output": "\n\t\tself._enqueue_thread = threading.Thread(target=self._enqueue)\n\t\tself._enqueue_thread.daemon = True\n\t\tself._enqueue_thread.start()\n\n\tdef _enqueue(self):\n\t\t\"\"\" Internal method to enqueue the events. \"\"\"\n\t\tif self._enqueue_thread.isAlive():\n\t\t\tself._enqueue_thread.join()\n\t\t\tself._enqueue_thread.join()\n\t\t\tself._enqueue_thread = None\n\n\tdef _enqueue_all(self):\n\t\t\"\"\" Internal method to enqueue all events. \"\"\"\n\t\tself._start_enqueue_", "solution": "\n\t\tself._enqueueThreadSignal.acquire()\n\t\tself._enqueueThread = Thread(target=self._enqueue_function)\n\t\tself._enqueueThread.daemon = True\n\t\tself._enqueueThread.start()\n\t\tself._enqueueThreadSignal.wait()\n\t\tself._enqueueThreadSignal.release()"}
{"prompt": "def setup_random_structure(self, coordination):\n        \"\"\"\n        Sets up a purely random structure with a given coordination.\n        :param coordination: coordination number for the random structure\n        \"\"\"", "output": "\n        self.structure = [\n            [0, 0, 0],\n            [0, 0, 0],\n            [0, 0, 0],\n            [0, 0, 0],\n            [0, 0, 0],\n            [0, 0, 0],\n            [0, 0, 0],\n            [0, 0, 0],\n            [0, 0, 0],\n            [0, 0, 0],\n            [0, 0, 0],\n            [0, 0, 0],\n            [0, 0, 0],\n            [0, 0, 0],\n            [0, 0, 0],\n            [", "solution": "\n        aa = 0.4\n        bb = -0.2\n        coords = list()\n        for ii in range(coordination + 1):\n            coords.append(aa * np.random.random_sample(3, ) + bb)\n        self.set_structure(\n            lattice=np.array([[10, 0, 0], [0, 10, 0], [0, 0, 10]], np.float),\n            species=[\"Si\"] * (coordination + 1),\n            coords=coords,\n            coords_are_cartesian=False)\n        self.setup_random_indices_local_geometry(coordination)"}
{"prompt": "def do_email_notification(self, comment, entry, site):\n        \"\"\"\n        Send email notification of a new comment to site staff.\n        \"\"\"", "output": "\n        subject = entry.get_subject()\n        body = entry.get_body()\n        body = body.replace('\\n','')\n        subject = subject.replace('\\n','')\n        body = body.replace('\\r','')\n        subject = subject.replace('\\n','')\n        body = body.replace('\\n','')\n        if self.is_admin(site):\n            body = body.replace('admin', 'admin')\n        if self.is_moderator(entry):\n            body = body.replace('moderator','moderator')\n        if self.is_admin(entry):\n           ", "solution": "\n        if not self.mail_comment_notification_recipients:\n            return\n\n        template = loader.get_template(\n            'comments/zinnia/entry/email/notification.txt')\n        context = {\n            'comment': comment,\n            'entry': entry,\n            'site': site,\n            'protocol': PROTOCOL\n        }\n        subject = _('[%(site)s] New comment posted on \"%(title)s\"') % \\\n            {'site': site.name, 'title': entry.title}\n        message = template.render(context)\n\n        send_mail(\n            subject, message,\n            settings.DEFAULT_FROM_EMAIL,\n            self.mail_comment_notification_recipients,\n            fail_silently=not settings.DEBUG\n        )"}
{"prompt": "def release(self):\n        \"\"\"Cleans up the lockfile if it was acquired.\n\n        Args:\n          self (JLock): the ``JLock`` instance\n\n        Returns:\n          ``False`` if the lock was not released or the lock is not acquired,\n          otherwise ``True``.\n        \"\"\"", "output": "\n        if self.is_locked:\n            return False\n        self.release_lock()\n        return True\n\n    def __enter__(self):\n        \"\"\"Enters a context manager.\n\n        Returns:\n          ``True`` if the lock was acquired, otherwise ``False``.\n        \"\"\"\n        self.acquire_lock()\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        \"\"\"Exits a context manager.\n\n        Args:\n          self (JLock): the ``JLock`` instance\n\n        Returns:\n          ``False`` if the lock was not released or the lock is not acquired,\n          otherwise ``", "solution": "\n        if not self.acquired:\n            return False\n\n        os.close(self.fd)\n\n        if os.path.exists(self.path):\n            os.remove(self.path)\n\n        self.acquired = False\n        return True"}
{"prompt": "def dynacRepresentation(self):\n        \"\"\"\n        Return the Dynac representation of this cavity instance.\n        \"\"\"", "output": "\n        return self._dynacRepresentation\n\n    def get_cavity_type(self):\n        \"\"\"\n        Return the type of the cavity.\n        \"\"\"\n        return self._cavityType\n\n    def get_cavity_type_name(self):\n        \"\"\"\n        Return the name of the type of the cavity.\n        \"\"\"\n        return self._cavityTypeName\n\n    def get_cavity_type_description(self):\n        \"\"\"\n        Return the description of the type of the cavity.\n        \"\"\"\n        return self._cavityTypeDescription\n\n    def get_cavity_type_type(self):\n        \"\"\"\n       ", "solution": "\n        return ['CAVMC', [\n            [self.cavID.val],\n            [self.xesln.val, self.phase.val, self.fieldReduction.val, self.isec.val, 1],\n            ]]"}
{"prompt": "def cli(config, server, api_key, all, credentials, project):\n    \"\"\"Create the cli command line.\"\"\"", "output": "\n    if not all:\n        return\n\n    # Create the client\n    client = Client(config['api_key'], config['project'], config['access_token'])\n\n    # Get the access token\n    access_token = client.get_access_token(api_key)\n\n    # Get the access token\n    if access_token is None:\n        print(\"Error: No access token.\")\n        return\n\n    # Create the project\n    if not project:\n        project = 'default'\n\n    # Create the access_token\n    if not access_token:\n        print(\"Error: No access token.\")\n        return\n\n    # Create the project\n   ", "solution": "\n    # Check first for the pybossa.rc file to configure server and api-key\n    home = expanduser(\"~\")\n    if os.path.isfile(os.path.join(home, '.pybossa.cfg')):\n        config.parser.read(os.path.join(home, '.pybossa.cfg'))\n        config.server = config.parser.get(credentials,'server')\n        config.api_key = config.parser.get(credentials, 'apikey')\n        try:\n            config.all = config.parser.get(credentials, 'all')\n        except ConfigParser.NoOptionError:\n            config.all = None\n    if server:\n        config.server = server\n    if api_key:\n        config.api_key = api_key\n    if all:\n        config.all = all\n    try:\n        config.project = json.loads(project.read())\n    except JSONDecodeError as e:\n        click.secho(\"Error: invalid JSON format in project.json:\", fg='red')\n        if e.msg == 'Expecting value':\n            e.msg += \" (if string enclose it with double quotes)\"\n        click.echo(\"%s\\n%s: line %s column %s\" % (e.doc, e.msg, e.lineno, e.colno))\n        raise click.Abort()\n    try:\n        project_schema = {\n            \"type\": \"object\",\n            \"properties\": {\n                \"name\": {\"type\": \"string\"},\n                \"short_name\": {\"type\": \"string\"},\n                \"description\": {\"type\": \"string\"}\n            }\n        }\n        jsonschema.validate(config.project, project_schema)\n    except jsonschema.exceptions.ValidationError as e:\n        click.secho(\"Error: invalid type in project.json\", fg='red')\n        click.secho(\"'%s': %s\" % (e.path[0], e.message), fg='yellow')\n        click.echo(\"'%s' must be a %s\" % (e.path[0], e.validator_value))\n        raise click.Abort()\n\n    config.pbclient = pbclient\n    config.pbclient.set('endpoint', config.server)\n    config.pbclient.set('api_key', config.api_key)"}
{"prompt": "def xpointerNewRange(self, startindex, end, endindex):\n        \"\"\"Create a new xmlXPathObjectPtr of type range \"\"\"", "output": "\n        ret = libxml2mod.xmlXPathNewRange(self._o)\n        if ret is None:raise treeError('xmlXPathNewRange() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp\n\n    def xpointerNewText(self, data, len):\n        \"\"\"Create a new xmlTextObjectPtr of type text \"\"\"\n        ret = libxml2mod.xmlTextNewText(self._o, data, len)\n        return ret\n\n    def xpointerNewRange(self, start, end, len):\n        \"\"\"Create a new xmlXPathObjectPtr of type range \"\"\"\n        ret =", "solution": "\n        if end is None: end__o = None\n        else: end__o = end._o\n        ret = libxml2mod.xmlXPtrNewRange(self._o, startindex, end__o, endindex)\n        if ret is None:raise treeError('xmlXPtrNewRange() failed')\n        return xpathObjectRet(ret)"}
{"prompt": "def ok(self):\n        \"\"\"Validate color selection and destroy dialog.\"\"\"", "output": "\n        self.parent.destroy()\n        self.destroy()\n\n    def cancel(self):\n        \"\"\"Cancel dialog.\"\"\"\n        self.parent.destroy()\n        self.destroy()\n\n    def ok_cancel(self):\n        \"\"\"Validate color selection and destroy dialog.\"\"\"\n        self.parent.destroy()\n        self.destroy()\n\n    def cancel_button(self):\n        \"\"\"Cancel color selection button.\"\"\"\n        self.parent.destroy()\n        self.destroy()\n\n    def ok_cancel_button(self):\n        \"\"\"Validate cancel button.\"\"\"\n        self.destroy()\n        self.destroy()\n\n    def cancel_color_button(self):\n       ", "solution": "\n        rgb, hsv, hexa = self.square.get()\n        if self.alpha_channel:\n            hexa = self.hexa.get()\n            rgb += (self.alpha.get(),)\n        self.color = rgb, hsv, hexa\n        self.destroy()"}
{"prompt": "def dependencies(self, task, params={}, **options): \n        \"\"\"Returns the compact representations of all of the dependencies of a task.\n\n        Parameters\n        ----------\n        task : {Id} The task to get dependencies on.\n        [params] : {Object} Parameters for the request\n        \"\"\"", "output": "\n        return self.get_compact_representation(self.get_dependencies(task, params), options)\n\n    def get_dependencies(self, task, params={}, **options):\n        \"\"\"Returns the compact representations of all of the dependencies of a task.\n\n        Parameters\n        ----------\n        task : {Id} The task to get dependencies on.\n        [params] : {Object} Parameters for the request\n        \"\"\"\n        return self.get_compact_representation(self.get_dependencies(task, params), options)\n\n    def get_compact_representation(self, compact_params, options={}):\n        \"\"\"Returns the compact representation of", "solution": "\n        path = \"/tasks/%s/dependencies\" % (task)\n        return self.client.get(path, params, **options)"}
{"prompt": "def release():\n    \"\"\"\n    Release new package version to pypi\n    :return:\n    \"\"\"", "output": "\n    if not os.path.isdir(os.path.join(os.path.dirname(__file__), 'pypirc'):\n        return\n\n    if os.path.exists(os.path.join(os.path.dirname(__file__), 'data/version.pypirc'):\n        with open(os.path.join(os.path.dirname(__file__), 'data/version.pypirc')) as f:\n            version = f.read().strip()\n    else:\n        version = '0.0.0'\n\n    with open(os.path.join(os.path.dirname", "solution": "\n\n    from secrets import pypi_auth\n\n    # Check that all changes are committed before creating a new version\n    git_check()\n\n    # Test package\n    test()\n\n    # Increment version\n    inc_version()\n\n    # Commit new version, create tag for version and push everything to origin\n    git_push()\n\n    # Build and publish package\n    build()\n    pathname = 'dist/pynb-{}.tar.gz'.format(version.__version__)\n    docker_exec('twine upload -u {user} -p {pass} {pathname}'.format(pathname=pathname, **pypi_auth))\n\n    # Remove temporary files\n    clean()"}
{"prompt": "def set_roughness(self,\n                      roughness=None,\n                      land_use_grid=None,\n                      land_use_grid_id=None,\n                      land_use_to_roughness_table=None):\n        \"\"\"\n        ADD ROUGHNESS FROM LAND COVER\n        See: http://www.gsshawiki.com/Project_File:Overland_Flow_%E2%80%93_Required\n        \"\"\"", "output": "\n        if roughness is None:\n            roughness = self.default_roughness\n        if land_use_grid is None:\n            land_use_grid = self.default_land_use_grid\n        if land_use_grid_id is None:\n            roughness_id = self.default_roughness_table\n        if roughness_table is None:\n            roughness_table = self.default_roughness_table_id\n        if roughness is land_flow_id:\n            self.add_flow_id(roughness_id)\n        elif roughness is land_use_", "solution": "\n        if roughness is not None:\n            self.project_manager.setCard('MANNING_N', str(roughness))\n        elif land_use_grid is not None and (land_use_grid_id is not None \\\n                or land_use_to_roughness_table is not None):\n            # make sure paths are absolute as the working directory changes\n            land_use_grid = os.path.abspath(land_use_grid)\n            if land_use_to_roughness_table is not None:\n                land_use_to_roughness_table = os.path.abspath(land_use_to_roughness_table)\n\n            mapTableFile = MapTableFile(project_file=self.project_manager)\n            mapTableFile.addRoughnessMapFromLandUse(\"roughness\",\n                                                    self.db_session,\n                                                    land_use_grid,\n                                                    land_use_to_roughness_table=land_use_to_roughness_table,\n                                                    land_use_grid_id=land_use_grid_id)\n        else:\n            raise ValueError(\"Need to either set 'roughness', or need \"\n                             \"to set values from land use grid ...\")"}
{"prompt": "def convenience_calc_fisher_approx(self, params):\n        \"\"\"\n        Calculates the BHHH approximation of the Fisher Information Matrix for\n        this model / dataset.\n        \"\"\"", "output": "\n        return self.model.calc_fisher_approx(params)\n\n    def get_fisher_approx(self, params):\n        \"\"\"\n        Calculates the BHHH approximation of the Fisher Information Matrix\n        for this model / dataset.\n        \"\"\"\n        return self.model.get_fisher_approx(params)\n\n    def get_dataset_id(self, params):\n        \"\"\"\n        Returns the ID of the dataset for this model / dataset.\n        \"\"\"\n        return self.dataset.get_id(params)\n\n    def get_dataset_name(self, params):\n        \"\"\"\n        Returns the name of", "solution": "\n        shapes, intercepts, betas = self.convenience_split_params(params)\n\n        args = [betas,\n                self.design,\n                self.alt_id_vector,\n                self.rows_to_obs,\n                self.rows_to_alts,\n                self.choice_vector,\n                self.utility_transform,\n                self.calc_dh_d_shape,\n                self.calc_dh_dv,\n                self.calc_dh_d_alpha,\n                intercepts,\n                shapes,\n                self.ridge,\n                self.weights]\n\n        return cc.calc_fisher_info_matrix(*args)"}
{"prompt": "def streamnet(np, filleddem, flowdir, acc, streamRaster, modifiedOutlet,\n                  streamOrder, chNetwork, chCoord, streamNet, subbasin, workingdir=None,\n                  mpiexedir=None, exedir=None, log_file=None, runtime_file=None, hostfile=None):\n        \"\"\"Run streamnet\"\"\"", "output": "\n        self.log.info(\"Running streamnet\")\n        self.log.info(\"streamRaster: %s acc: %s\" % (log_file, str(acc)))\n        self.log.info(\"flowdir: %s flowdir: %s\" % (mpiexedir, str(flowdir)))\n        self.log.info(\"acc: %s\" % str(acc))\n        self.log.info(\"streamRaster: %s\" % str(streamRaster))\n        self.log.info(\"modifiedOutlet: %s\" % str(modifiedOutlet))\n        self.log", "solution": "\n        fname = TauDEM.func_name('streamnet')\n        return TauDEM.run(FileClass.get_executable_fullpath(fname, exedir),\n                          {'-fel': filleddem, '-p': flowdir, '-ad8': acc, '-src': streamRaster,\n                           '-o': modifiedOutlet}, workingdir,\n                          None,\n                          {'-ord': streamOrder, '-tree': chNetwork, '-coord': chCoord,\n                           '-net': streamNet, '-w': subbasin},\n                          {'mpipath': mpiexedir, 'hostfile': hostfile, 'n': np},\n                          {'logfile': log_file, 'runtimefile': runtime_file})"}
{"prompt": "def can_delete_asset_contents(self, asset_id=None):\n        \"\"\"Tests if this user can delete ``AssetsContent`` from ``Assets``.\n\n        A return of true does not guarantee successful authorization. A\n        return of false indicates that it is known deleting an\n        ``AssetContent`` will result in a ``PermissionDenied``. This is\n        intended as a hint to an application that may opt not to offer\n        delete operations to an unauthorized user.\n\n        :param asset_id: the ``Id`` of an ``Asset``\n        :type asset_id: ``osid.id.Id``\n        :return: ``false`` if ``AssetContent`` deletion is not authorized, ``true`` otherwise\n        :rtype: ``boolean``\n        :raise: ``NullArgument`` -- ``asset_id`` is ``null``\n\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"", "output": " :return: ``true`` if ``asset_id`` is ``null``\n        :rtype: ``boolean``\n\n\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n        return  # boolean\n\n    @utilities.arguments_not_none\n    def delete_asset_contents(self, asset_id=None):\n        \"\"\"Deletes the contents of the ``AssetsContent``.\n\n        :param asset_id: the ``Id`` of an ``Asset``\n        :type asset_id: ``osid.id.Id``\n        :return: ``true`` if ``asset_id`` is ``null``", "solution": "\n        url_path = construct_url('authorization',\n                                 bank_id=self._catalog_idstr)\n        return self._get_request(url_path)['assetHints']['canDelete']"}
{"prompt": "def insert(self, point, number, axis):\n        \"\"\"Inserts number of rows/cols/tabs into selection at point on axis\n        Parameters\n        ----------\n\n        point: Integer\n        \\tAt this point the rows/cols are inserted or deleted\n        number: Integer\n        \\tNumber of rows/cols to be inserted, negative number deletes\n        axis: Integer in 0, 1\n        \\tDefines whether rows or cols are affected\n\n        \"\"\"", "output": "\n        if axis == 0:\n            self.axis = 1\n        elif axis == 1:\n            self.axis = 0\n        else:\n            raise ValueError(\"axis must be 0 or 1\")\n        self.row_index = number\n        self.row_count = number\n        self.selected_row = self.row_index - 1\n        self.selected_col = self.selected_row + number\n        self.axis = axis\n        self.selected_row = self.selected_col - 1\n        self.selected_col = self.selected_row + number\n        self.row_index = self.row_count", "solution": "\n\n        def build_tuple_list(source_list, point, number, axis):\n            "}
{"prompt": "def sample_cc(self, nsamples=1, weighted=True):\n        \"\"\"\n        Returns a random polygon of any class. The probability of each polygon to be sampled\n        is proportional to its area if weighted is True.\n        \"\"\"", "output": "\n        if weighted:\n            return self.random_polygon(nsamples)\n        else:\n            return self.random_polygon(nsamples, True)\n\n    def random_polygon(self, nsamples=1, weighted=True):\n        \"\"\"\n        Returns a random polygon of any class. The probability of each polygon to be\n        sampled is proportional to its area if weighted is True.\n        \"\"\"\n        if weighted:\n            return self.random_polygon_weighted(nsamples)\n        else:\n            return self.random_polygon_weighted(nsamples, True)\n\n    def random_polygon_weighted(self, nsamples", "solution": "\n        weights = self.areas / np.sum(self.areas) if weighted else None\n        for index in np.random.choice(a=len(self.geometries), size=nsamples, p=weights):\n            yield self.geometries[index]"}
{"prompt": "def set_temp_url_key(self, key=None):\n        \"\"\"\n        Sets the key for the Temporary URL for the account. It should be a key\n        that is secret to the owner.\n\n        If no key is provided, a UUID value will be generated and used. It can\n        later be obtained by calling get_temp_url_key().\n        \"\"\"", "output": "\n        if key is None:\n            key = uuid.uuid4().hex\n        if not key:\n            key = str(uuid.uuid4())\n        self.temp_url_key = key\n\n    def get_temp_url_key(self):\n        \"\"\"\n        Returns the key for the temporary URL.\n        \"\"\"\n        return self.temp_url_key\n\n    def get_id(self):\n        \"\"\"\n        Returns the ID for the user.\n        \"\"\"\n        return self.id\n\n    def get_name(self):\n        \"\"\"\n        Returns the name for the user.\n        \"\"\"\n        return self.name\n\n    def get_", "solution": "\n        if key is None:\n            key = uuid.uuid4().hex\n        meta = {\"Temp-Url-Key\": key}\n        self.set_account_metadata(meta)\n        self._cached_temp_url_key = key"}
{"prompt": "def get_username(sciper):\n    \"\"\"\n    Return username of user\n    \"\"\"", "output": "\n    return sciper.get_user_name()\n\ndef get_password(sciper):\n    \"\"\"\n    Return password of user\n    \"\"\"\n    return sciper.get_user_password()\n\ndef get_user_id(sciper, username):\n    \"\"\"\n    Return user id\n    \"\"\"\n    return sciper.get_user_id(username, sciper.get_password())\n\ndef get_user_name(sciper, username):\n    \"\"\"\n    Return username of user\n    \"\"\"\n    return sciper.get_user_name()\n\ndef get_user_password(sciper", "solution": "\n    attribute = 'uid'\n    response = LDAP_search(\n        pattern_search='(uniqueIdentifier={})'.format(sciper),\n        attribute=attribute\n    )\n    try:\n        username = get_attribute(response, attribute)\n    except Exception:\n        raise EpflLdapException(\"No username corresponds to sciper {}\".format(sciper))\n    return username"}
{"prompt": "def delay(name, args, kwargs):\n    \"\"\"Run a job asynchronously\"\"\"", "output": "\n    return _job_delay(name, args, kwargs, False)\n\n\ndef _job_delay(name, args, kwargs, delay=0):\n    \"\"\"\n    Helper function to run a job asynchronously\n    \"\"\"\n    if delay > 0:\n        time.sleep(delay)\n    return job(name, args, kwargs, False)\n\n\ndef _get_job_status(job_id):\n    \"\"\"\n    Helper function to get the status of a job\n    \"\"\"\n    return _get_job_info(job_id)['status']\n\n\ndef _get_job_info(job_id):\n   ", "solution": "\n    args = args or []\n    kwargs = dict(k.split() for k in kwargs) if kwargs else {}\n    if name not in celery.tasks:\n        log.error('Job %s not found', name)\n    job = celery.tasks[name]\n    log.info('Sending job %s', name)\n    async_result = job.delay(*args, **kwargs)\n    log.info('Job %s sended to workers', async_result.id)"}
{"prompt": "def _abort_workflow(pb: ProcessingBlock, workflow_stage_dict: dict,\n                    docker: DockerSwarmClient):\n    \"\"\"Abort the workflow.\n\n    TODO(BMo): This function currently does nothing as the abort flag\n        is hardcoded to False!\n\n    This function is used by `execute_processing_block`.\n\n    Args:\n        pb (ProcessingBlock): Configuration database Processing block object.\n        workflow_stage_dict (dict): Workflow stage metadata dictionary.\n        docker (DockerClient): Docker Swarm Client object.\n\n    Returns:\n        bool, True if the stage is aborted, otherwise False.\n\n    \"\"\"", "output": "\n    try:\n        pb.abort()\n    except Exception as e:\n        _logger.error(\"Aborting workflow: %s\", e)\n        return False\n    return True\n\n\ndef execute_processing_block(workflow_id: str, pb: ProcessingBlock,\n                               docker: DockerSwarmClient) -> bool:\n    \"\"\"Execute the processing block.\n\n    Args:\n        workflow_id (str): Workflow ID.\n        pb (pb.ProcessBlock): Configuration object.\n        docker (docker.DockerClient): Docker client object.\n\n    Returns:\n        bool, True if the process block was successfully executed, otherwise False.\n\n   ", "solution": "\n    # TODO(BMo) Ask the database if the abort flag on the PB is set.\n    _abort_flag = False\n    if _abort_flag:\n        for workflow_stage in pb.workflow_stages:\n            for service_id, _ in \\\n                    workflow_stage_dict[workflow_stage.id]['services'].items():\n                docker.delete_service(service_id)\n                LOG.info(\"Deleted Service Id %s\", service_id)\n        return True\n    return False"}
{"prompt": "def plot(self, data):\n    \"\"\" Plots a featire slice view on given data.\n\n    Args:\n      data: Can be one of:\n          A string of sql query.\n          A sql query module defined by \"%%sql --module module_name\".\n          A pandas DataFrame.\n        Regardless of data type, it must include the following columns:\n          \"feature\": identifies a slice of features. For example: \"petal_length:4.0-4.2\".\n          \"count\": number of instances in that slice of features.\n        All other columns are viewed as metrics for its feature slice. At least one is required.\n    \"\"\"", "output": ".\n\n    Returns:\n      None\n    \"\"\"\n    if isinstance(data, str):\n      return self.plot_sql(data)\n    elif isinstance(data, pd.DataFrame):\n      return self.plot_sql(data.to_sql())\n    else:\n      raise ValueError(\"Can only plot string or pandas DataFrame\")\n\n  def plot_sql(self, sql):\n    \"\"\" Plots a SQL query view on given data.\n\n    Args:\n      sql: Can be one of:\n          A string of sql query.\n          A sql query module defined by \"%%sql --module module_name\".\n          A pandas DataFrame.\n\n    Returns", "solution": "\n    import IPython\n\n    if ((sys.version_info.major > 2 and isinstance(data, str)) or\n       (sys.version_info.major <= 2 and isinstance(data, basestring))):\n      data = bq.Query(data)\n\n    if isinstance(data, bq.Query):\n      df = data.execute().result().to_dataframe()\n      data = self._get_lantern_format(df)\n    elif isinstance(data, pd.core.frame.DataFrame):\n      data = self._get_lantern_format(data)\n    else:\n      raise Exception('data needs to be a sql query, or a pandas DataFrame.')\n\n    HTML_TEMPLATE = "}
{"prompt": "def is_alert_present(self):\n        \"\"\"Tests if an alert is present\n\n        @return: True if alert is present, False otherwise\n        \"\"\"", "output": "\n        return self.browser.find_element_by_id('alert-success')\n\n    def is_alert_present_by_css(self, css_selector):\n        \"\"\"Tests if an alert is present by CSS selector\n\n        @param css_selector: CSS selector\n        @return: True if alert is present, False otherwise\n        \"\"\"\n        return self.browser.find_element_by_css_selector(css_selector)\n\n    def is_alert_present_by_xpath(self, xpath):\n        \"\"\"Tests if an alert is present by XPath\n\n        @param xpath: XPath\n        @return:", "solution": "\n        current_frame = None\n        try:\n            current_frame = self.driver.current_window_handle\n            a = self.driver.switch_to_alert()\n            a.text\n        except NoAlertPresentException:\n            # No alert\n            return False\n        except UnexpectedAlertPresentException:\n            # Alert exists\n            return True\n        finally:\n            if current_frame:\n                self.driver.switch_to_window(current_frame)\n        return True"}
{"prompt": "def _condense(self, data):\n    \"\"\"\n    Condense by adding together all of the lists.\n    \"\"\"", "output": "\n    if self.sparse_keys is not None:\n      if not isinstance(data, sparse_tensor.SparseTensor):\n        raise TypeError(\"Expected SparseTensor, got %s\" % type(data))\n      self.sparse_keys.extend(data.indices)\n      self.sparse_keys = self.sparse_keys + data.values.tolist()\n\n    return data\n\n  def _to_dnn_input_layer(self,\n                          input_tensor,\n                          weight_collections=None,\n                          trainable=True,\n                          output_rank=2):\n    if weight_collections and not trainable:\n      raise ValueError(\"weight_", "solution": "\n    rval = {}\n    for resolution,histogram in data.items():\n      for value,count in histogram.items():\n        rval[ value ] = count + rval.get(value,0)\n    return rval"}
{"prompt": "def transmit_ack_bpdu(self):\n        \"\"\" Send Topology Change Ack BPDU. \"\"\"", "output": "\n        return self.send_pdu(self.PDU_BPDU_CHANGE_BPDU, self.PDU_BPDU_CHANGE_BPDU,\n                             self.PDU_BPDU_CHANGE_BPDU,\n                             self.PDU_BPDU_CHANGE_BPDU)\n\n    def send_pdu(self, pdu_pdu):\n        \"\"\" Send PDU. \"\"\"\n        if self.debug:\n            print(\"Sending PDU: %s\" % pdu_pdu)\n\n        if pdu_pdu.pdu_type == PDU_TYPE_BPDU:\n            self.write_b(PDU_", "solution": "\n        ack_flags = 0b10000001\n        bpdu_data = self._generate_config_bpdu(ack_flags)\n        self.ofctl.send_packet_out(self.ofport.port_no, bpdu_data)"}
{"prompt": "def template(basedir, text, vars, lookup_fatal=True, expand_lists=False):\n    \"\"\" run a text buffer through the templating engine until it no longer changes \"\"\"", "output": "\n    global _thread_id\n    _thread_id += 1\n\n    try:\n        res = None\n        while not res:\n            res = _read_thread_template(basedir, text, vars)\n            if not res:\n                time.sleep(0.01)\n            else:\n                break\n    finally:\n        if _thread_id > 0:\n            _thread.exit()\n\n    return res\n\n\ndef _read_thread_template(basedir, text, vars):\n    (fd, path) = tempfile.mkstemp(dir=basedir)\n    f = os.fdopen(fd, 'w')\n    try:\n       ", "solution": "\n\n    try:\n        text = text.decode('utf-8')\n    except UnicodeEncodeError:\n        pass # already unicode\n    text = varReplace(basedir, unicode(text), vars, lookup_fatal=lookup_fatal, expand_lists=expand_lists)\n    return text"}
{"prompt": "def QA_fetch_stock_basic_info_tushare(collections=DATABASE.stock_info_tushare):\n    \"\"\"\n    purpose:\n        tushare \u80a1\u7968\u5217\u8868\u6570\u636e\u5e93\n\n        code,\u4ee3\u7801\n        name,\u540d\u79f0\n        industry,\u6240\u5c5e\u884c\u4e1a\n        area,\u5730\u533a\n        pe,\u5e02\u76c8\u7387\n        outstanding,\u6d41\u901a\u80a1\u672c(\u4ebf)\n        totals,\u603b\u80a1\u672c(\u4ebf)\n        totalAssets,\u603b\u8d44\u4ea7(\u4e07)\n        liquidAssets,\u6d41\u52a8\u8d44\u4ea7\n        fixedAssets,\u56fa\u5b9a\u8d44\u4ea7\n        reserved,\u516c\u79ef\u91d1\n        reservedPerShare,\u6bcf\u80a1\u516c\u79ef\u91d1\n        esp,\u6bcf\u80a1\u6536\u76ca\n        bvps,\u6bcf\u80a1\u51c0\u8d44\n        pb,\u5e02\u51c0\u7387\n        timeToMarket,\u4e0a\u5e02\u65e5\u671f\n        undp,\u672a\u5206\u5229\u6da6\n        perundp, \u6bcf\u80a1\u672a\u5206\u914d\n        rev,\u6536\u5165\u540c\u6bd4(%)\n        profit,\u5229\u6da6\u540c\u6bd4(%)\n        gpr,\u6bdb\u5229\u7387(%)\n        npr,\u51c0\u5229\u6da6\u7387(%)\n        holders,\u80a1\u4e1c\u4eba\u6570\n\n        add by tauruswang,\n\n    :param collections: stock_info_tushare \u96c6\u5408\n    :return:\n    \"\"\"", "output": "abilities,\u6240\u5c5e\u8d44\u4ea7\n        liabilityAssets,\u6240\u5c5e\u8d44\u4ea7\n        liabilityAssetsAssets,\u6240\u5c5e\u8d44\u4ea7\n        liabilityAssetsAssets,\u6240\u5c5e\u8d44\u4ea7\n        liabilityAssetsAssets,\u6240\u5c5e\u8d44\u4ea7\n        liabilityAssetsAssets,\u6240\u5c5e\u8d44\u4ea7\n        liabilityAssetsAssets,\u6240\u5c5e\u8d44\u4ea7\n        liabilityAssetsAssets,\u6240\u5c5e\u8d44\u4ea7\n        liabilityAssetsAssets,\u6240\u5c5e\u8d44\u4ea7\n        liabilityAssetsAssets,\u6240\ufffd", "solution": "\n    '\u83b7\u53d6\u80a1\u7968\u57fa\u672c\u4fe1\u606f'\n    items = [item for item in collections.find()]\n    # \ud83d\udee0todo  \u8f6c\u53d8\u6210 dataframe \u7c7b\u578b\u6570\u636e\n    return items"}
{"prompt": "def cudnnCreate():\n    \"\"\"\n    Initialize cuDNN.\n\n    Initializes cuDNN and returns a handle to the cuDNN context.\n\n    Returns\n    -------\n\n    handle : cudnnHandle\n        cuDNN context\n    \"\"\"", "output": "\n    return cudnn.cudnnCreate()\n\n\ndef cudnnDestroy():\n    \"\"\"\n    Destroy cuDNN.\n\n    Destroys a previously created cuDNN context.\n\n    Returns\n    -------\n\n    handle : cudnnHandle\n        cuDNN context\n    \"\"\"\n    cudnn.cudnnDestroy()\n\n\ndef cudnnCreate_tensor(handle, name, dataType, size, data,\n                       init=None,\n                       dataTypeDesc=None,\n                       dataType=mx_real_t,\n                       initDesc=None,\n                       dataType_grad=None,\n                       dataType_grad_grad=None,\n                       infer", "solution": "\n\n    handle = ctypes.c_void_p()\n    status = _libcudnn.cudnnCreate(ctypes.byref(handle))\n    cudnnCheckStatus(status)\n    return handle.value"}
{"prompt": "def get_next_sibling_tags(mention):\n    \"\"\"Return the HTML tag of the Mention's next siblings.\n\n    Next siblings are Mentions which are at the same level in the HTML tree as\n    the given mention, but are declared after the given mention.\n    If a candidate is passed in, only the next siblings of its last Mention\n    are considered in the calculation.\n\n    :param mention: The Mention to evaluate\n    :rtype: list of strings\n    \"\"\"", "output": "\n    if mention.is_root:\n        return [root.tag for root in mention.root.children]\n    else:\n        return [mention.tag]\n\n\ndef get_mention_html(mention):\n    \"\"\"Return the HTML of the Mention's root node.\n\n    :param mention: The Mention to evaluate\n    :rtype: list of strings\n    \"\"\"\n    return [\n        root.tag for root in get_mentions(mention)\n        if root.tag.name == mention.tag.name\n    ][0]\n\n\ndef get_mentions(mention):\n    \"\"\"Return the list of", "solution": "\n    span = _to_span(mention)\n    next_sibling_tags = []\n    i = _get_node(span.sentence)\n    while i.getnext() is not None:\n        next_sibling_tags.append(str(i.getnext().tag))\n        i = i.getnext()\n    return next_sibling_tags"}
{"prompt": "def bygroups(*args):\n    \"\"\"\n    Callback that yields multiple actions for each group in the match.\n    \"\"\"", "output": "\n    def wrap(func):\n        def wrapped(*args):\n            for group in args:\n                for action in group:\n                    yield action\n            yield ''\n        return func(*args)\n    return wrapped\n\ndef _get_default_actions():\n    \"\"\"\n    Return a list of actions to be used in all re.match actions.\n    \"\"\"\n    return [\n        ('group', [\n            (re.compile(r'^%s$' % x),\n            (re.compile(r'^%s$' % x),\n            _group_actions,\n            dict(\n                (x, getattr(re, x)) for x in dir", "solution": "\n    def callback(lexer, match, ctx=None):\n        for i, action in enumerate(args):\n            if action is None:\n                continue\n            elif type(action) is _TokenType:\n                data = match.group(i + 1)\n                if data:\n                    yield match.start(i + 1), action, data\n            else:\n                data = match.group(i + 1)\n                if data is not None:\n                    if ctx:\n                        ctx.pos = match.start(i + 1)\n                    for item in action(lexer,\n                                       _PseudoMatch(match.start(i + 1), data), ctx):\n                        if item:\n                            yield item\n        if ctx:\n            ctx.pos = match.end()\n    return callback"}
{"prompt": "def save_excel(self, fd):\n        \"\"\" Saves the case as an Excel spreadsheet.\n        \"\"\"", "output": "\n        self.workbook.save(fd)\n\n    def save_csv(self, fd):\n        \"\"\" Saves the case as a CSV file.\n        \"\"\"\n        self.workbook.save(fd)\n\n    def save_excel_sheet(self, fd):\n        \"\"\" Saves the case as an Excel spreadsheet.\n        \"\"\"\n        self.workbook.save_sheet(fd)\n\n    def save_csv_sheet_name(self, fd):\n        \"\"\" Saves the case as a CSV file.\n        \"\"\"\n        self.workbook.save_sheet_name(fd, self.csv_sheet_name)\n\n    def save", "solution": "\n        from pylon.io.excel import ExcelWriter\n        ExcelWriter(self).write(fd)"}
{"prompt": "def get_policies_by_id(profile_manager, policy_ids):\n    \"\"\"\n    Returns a list of policies with the specified ids.\n\n    profile_manager\n        Reference to the profile manager.\n\n    policy_ids\n        List of policy ids to retrieve.\n    \"\"\"", "output": "\n    return profile_manager.get_policies_by_ids(policy_ids)\n\n  def get_policy_by_name(self, name):\n    \"\"\"\n    Returns a reference to the specified policy.\n\n    name\n        Name of the policy.\n\n    policy_name\n        Name of the policy.\n\n    policy_id\n        Id of the policy.\n    \"\"\"\n    return self.policies[name]\n\n  def get_policy_by_id(self, policy_id):\n    \"\"\"\n    Returns a reference to the specified policy.\n\n    policy_id\n        Id of the policy.\n    \"\"\"\n    return self.policies_by_", "solution": "\n    try:\n        return profile_manager.RetrieveContent(policy_ids)\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise VMwareApiError('Not enough permissions. Required privilege: '\n                             '{0}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise VMwareRuntimeError(exc.msg)"}
{"prompt": "async def send(self, data, id=None, event=None, retry=None):\n        \"\"\"Send data using EventSource protocol\n\n        :param str data: The data field for the message.\n        :param str id: The event ID to set the EventSource object's last\n            event ID value to.\n        :param str event: The event's type. If this is specified, an event will\n            be dispatched on the browser to the listener for the specified\n            event name; the web site would use addEventListener() to listen\n            for named events. The default event type is \"message\".\n        :param int retry: The reconnection time to use when attempting to send\n            the event. [What code handles this?] This must be an integer,\n            specifying the reconnection time in milliseconds. If a non-integer\n            value is specified, the field is ignored.\n        \"\"\"", "output": " retry: The number of seconds to wait before retrying.\n            If this is specified, the function will return immediately\n            if the browser does not support the retry argument.\n        \"\"\"\n        if id is not None:\n            data = await self.get_event_source(event, data)\n        else:\n            data = await self.get_event_source(retry)\n\n        return await self.send_async(data, id=id, event=event)\n\n    async def send_async(self, data, id=None, retry=None):\n        \"\"\"Send data using asynchronous protocol\n\n        :param str data: The", "solution": "\n        buffer = io.StringIO()\n        if id is not None:\n            buffer.write(self.LINE_SEP_EXPR.sub('', 'id: {}'.format(id)))\n            buffer.write(self._sep)\n\n        if event is not None:\n            buffer.write(self.LINE_SEP_EXPR.sub('', 'event: {}'.format(event)))\n            buffer.write(self._sep)\n\n        for chunk in self.LINE_SEP_EXPR.split(data):\n            buffer.write('data: {}'.format(chunk))\n            buffer.write(self._sep)\n\n        if retry is not None:\n            if not isinstance(retry, int):\n                raise TypeError('retry argument must be int')\n            buffer.write('retry: {}'.format(retry))\n            buffer.write(self._sep)\n\n        buffer.write(self._sep)\n        await self.write(buffer.getvalue().encode('utf-8'))"}
{"prompt": "def get_queryset(self):\n        \"\"\"\n        Inspired by Glen Roberton's django-geojson-tiles view\n        \"\"\"", "output": "\n        return self.geo_query_set.filter(geom__isnull=False)\n\n    def get_geom_type(self):\n        \"\"\"\n        Returns the geometry type for the geometry column\n        \"\"\"\n        return OGRGeomType(capi.geom_type(self.geom))\n\n    def get_geom_name(self):\n        \"Returns the name of the geometry column for the geographic column.\"\n        return capi.geom_name(self.geom)\n\n    def get_geom_placeholder(self):\n        \"\"\"\n        Returns the placeholder for the geometry column if a\n        placeholder is provided.\n        \"\"\"\n        if hasattr(self, 'placeholder", "solution": "\n        self.z, self.x, self.y = self._parse_args()\n        nw = self.tile_coord(self.x, self.y, self.z)\n        se = self.tile_coord(self.x + 1, self.y + 1, self.z)\n        bbox = Polygon((nw, (se[0], nw[1]),\n                       se, (nw[0], se[1]), nw))\n        qs = super(TiledGeoJSONLayerView, self).get_queryset()\n        qs = qs.filter(**{\n            '%s__intersects' % self.geometry_field: bbox\n        })\n        self.bbox = bbox.extent\n\n        # Simplification dict by zoom level\n        simplifications = self.simplifications or {}\n        z = self.z\n        self.simplify = simplifications.get(z)\n        while self.simplify is None and z < 32:\n            z += 1\n            self.simplify = simplifications.get(z)\n\n        # Won't trim point geometries to a boundary\n        model_field = qs.model._meta.get_field(self.geometry_field)\n        self.trim_to_boundary = (self.trim_to_boundary and\n                                 not isinstance(model_field, PointField) and\n                                 Intersection is not None)\n        if self.trim_to_boundary:\n            if django.VERSION < (1, 9):\n                qs = qs.intersection(bbox)\n            else:\n                qs = qs.annotate(intersection=Intersection(self.geometry_field, bbox))\n            self.geometry_field = 'intersection'\n\n        return qs"}
{"prompt": "def publish_queue(self):\n        \"\"\"\n        Publish all messages that have been added to the queue for configured protocol\n        :return: None\n        \"\"\"", "output": "\n        if self.protocol_id:\n            self.protocol.publish(self.queue_id, self.protocol_id, self.message_id)\n        else:\n            self.protocol.publish(self.queue_id, self.protocol_id, self.message_id)\n\n    def get_messages(self):\n        \"\"\"\n        Get all messages from the queue\n        :return: list of messages\n        \"\"\"\n        return self.queue.get_messages()\n\n    def get_message_count(self):\n        \"\"\"\n        Get the number of messages in the queue\n        :return: number of messages in the queue", "solution": "\n        self.last_send_time = time.time()\n        try:\n            self._tx_queue_lock.acquire()\n            start_length = len(self._rx_queue)\n            publish_amount = len(self._tx_queue)\n            if self.config.protocol == PublisherConfig.Protocol.GRPC:\n                self._publish_queue_grpc()\n            else:\n                self._publish_queue_wss()\n            self._tx_queue = []\n        finally:\n            self._tx_queue_lock.release()\n\n        if self.config.publish_type == self.config.Type.SYNC:\n            start_time = time.time()\n            while time.time() - start_time < self.config.sync_timeout and \\\n                                    len(self._rx_queue) - start_length < publish_amount:\n                pass\n            return self._rx_queue"}
{"prompt": "def start_engine(self):\n        \"\"\"\n        Start the child processes (one per device OS)\n        \"\"\"", "output": "\n        self.log.info(\"Starting engine\")\n        self.start_time = time.time()\n        self.log.debug(\"Initializing the device\")\n        self.init_device()\n        self.log.debug(\"Initializing the device\")\n        self.init_device_time = time.time() - self.start_time\n        self.log.debug(\"Initializing the device time\")\n        self.init_device_time = time.time() - self.start_time\n        self.log.debug(\"Initializing the device\")\n        self.init_device_time = time.time()", "solution": "\n        if self.disable_security is True:\n            log.warning('***Not starting the authenticator process due to disable_security being set to True***')\n        else:\n            log.debug('Generating the private key')\n            self.__priv_key = nacl.utils.random(nacl.secret.SecretBox.KEY_SIZE)\n            log.debug('Generating the signing key')\n            self.__signing_key = nacl.signing.SigningKey.generate()\n            # start the keepalive thread for the auth sub-process\n            self._processes.append(self._start_auth_proc())\n        log.debug('Starting the internal proxy')\n        proc = self._start_pub_px_proc()\n        self._processes.append(proc)\n        # publisher process start\n        pub_id = 0\n        for pub in self.publisher:\n            publisher_type, publisher_opts = list(pub.items())[0]\n            proc = self._start_pub_proc(publisher_type,\n                                        publisher_opts,\n                                        pub_id)\n            self._processes.append(proc)\n            pub_id += 1\n        # device process start\n        log.info('Starting child processes for each device type')\n        started_os_proc = []\n        for device_os, device_config in self.config_dict.items():\n            if not self._whitelist_blacklist(device_os):\n                log.debug('Not starting process for %s (whitelist-blacklist logic)', device_os)\n                # Ignore devices that are not in the whitelist (if defined),\n                #   or those operating systems that are on the blacklist.\n                # This way we can prevent starting unwanted sub-processes.\n                continue\n            log.debug('Will start %d worker process(es) for %s', self.device_worker_processes, device_os)\n            for proc_index in range(self.device_worker_processes):\n                self._processes.append(self._start_dev_proc(device_os,\n                                                            device_config))\n            started_os_proc.append(device_os)\n        # start the server process\n        self._processes.append(self._start_srv_proc(started_os_proc))\n        # start listener process\n        for lst in self.listener:\n            listener_type, listener_opts = list(lst.items())[0]\n            proc = self._start_lst_proc(listener_type,\n                                        listener_opts)\n            self._processes.append(proc)\n        thread = threading.Thread(target=self._check_children)\n        thread.start()"}
{"prompt": "def du(self, paths, include_toplevel=False, include_children=True):\n        \"\"\"Returns size information for paths\n\n        :param paths: Paths to du\n        :type paths: list\n        :param include_toplevel: Include the given path in the result. If the path is a file, include_toplevel is always True.\n        :type include_toplevel: boolean\n        :param include_children: Include child nodes in the result.\n        :type include_children: boolean\n        :returns: a generator that yields dictionaries\n\n        **Examples:**\n\n        Children:\n\n        >>> list(client.du(['/']))\n        [{'path': '/Makefile', 'length': 6783L}, {'path': '/build', 'length': 244778L}, {'path': '/index.asciidoc', 'length': 100L}, {'path': '/source', 'length': 8524L}]\n\n        Directory only:\n\n        >>> list(client.du(['/'], include_toplevel=True, include_children=False))\n        [{'path': '/', 'length': 260185L}]\n\n        \"\"\"", "output": "size': 0, 'type': 'file', 'name': 'du','size_human': 0},\n         {'size': 1, 'type': 'file', 'name': 'file','size_human': 1},\n         {'size': 0, 'type': 'folder', 'name': 'du','size_human': 0},\n         {'size': 0, 'type': 'folder', 'name': 'file','size_human': 1},\n         {'size': 0, 'type': 'folder', 'name': 'du','size_human': 0}]\n\n        \"\"\"\n        for path", "solution": "\n        if not isinstance(paths, list):\n            raise InvalidInputException(\"Paths should be a list\")\n        if not paths:\n            raise InvalidInputException(\"du: no path given\")\n\n        processor = lambda path, node: self._handle_du(path, node)\n        for item in self._find_items(paths, processor, include_toplevel=include_toplevel,\n                                     include_children=include_children, recurse=False):\n            if item:\n                yield item"}
{"prompt": "def new_closure(vals):\n    \"\"\"\n    Build a new closure\n    \"\"\"", "output": "\n    return ast.FunctionDef(\n        name=vals.name,\n        args=vals.args,\n        body=vals.body,\n        returns=vals.returns,\n        args_types=vals.args_types,\n        returns_types=vals.returns_types,\n        type_names=type_names(vals.type),\n        type_var_names=type_var_names(vals.type),\n        var_names=var_names(vals.type),\n        body_type_var_names=body_type_var_names(vals.type),\n        returns_type_var_", "solution": "\n    args = ','.join('x%i' % i for i in range(len(vals)))\n    f = eval(\"lambda %s:lambda:(%s)\" % (args, args))\n    if sys.version_info[0] >= 3:\n        return f(*vals).__closure__\n    return f(*vals).func_closure"}
{"prompt": "def export(self, name, columns, points):\n        \"\"\"Write the points in Riemann.\"\"\"", "output": "\n        self.write_points(name, points, columns)\n        self.write_points(name, points, columns)\n        self.write_points(name, points, columns)\n        self.write_points(name, points, columns)\n        self.write_points(name, points, columns)\n        self.write_points(name, points, columns)\n        self.write_points(name, points, columns)\n        self.write_points(name, points, columns)\n        self.write_points(name, points, columns)\n        self.write_points(name, points,", "solution": "\n        for i in range(len(columns)):\n            if not isinstance(points[i], Number):\n                continue\n            else:\n                data = {'host': self.hostname, 'service': name + \" \" + columns[i], 'metric': points[i]}\n                logger.debug(data)\n                try:\n                    self.client.send(data)\n                except Exception as e:\n                    logger.error(\"Cannot export stats to Riemann (%s)\" % e)"}
{"prompt": "def launch_notebook(request, username, notebook_context):\n    \"\"\"Renders a IPython Notebook frame wrapper.\n\n    Starts or reattachs ot an existing Notebook session.\n    \"\"\"", "output": "\n    session = notebook_context.session\n    ot = session.get_notebook(username)\n    if ot is None:\n        return HttpResponseBadRequest(\"No such notebook: %s\" % username)\n    session.set_notebook(ot)\n    session.save()\n    return HttpResponse(\"Session saved\")\n\n\ndef _get_session_data(request, username, session_id):\n    \"\"\"Returns the session data for a given session.\n\n    This is used by the Backbone application to display the session info.\n    \"\"\"\n    notebook_context = _get_notebook_context(request)\n    session = notebook_", "solution": "\n    # The notebook manage now tries too hard to get the port allocated for the notebook user, making it slow\n    # TODO: Manage a proper state e.g. using Redis\n    notebook_info = launch_on_demand(request, username, notebook_context)\n\n    # Jump to the detault notebook\n    proxy_route = request.route_url(\"notebook_proxy\", remainder=\"notebooks/{}\".format(notebook_info[\"notebook_name\"]))\n    proxy_route = route_to_alt_domain(request, proxy_route)\n\n    return HTTPFound(proxy_route)"}
{"prompt": "def get_cel_to_gal_angle(skydir):\n    \"\"\"Calculate the rotation angle in radians between the longitude\n    axes of a local projection in celestial and galactic coordinates.\n\n    Parameters\n    ----------\n    skydir : `~astropy.coordinates.SkyCoord`\n        Direction of projection center.\n\n    Returns\n    -------\n    angle : float\n        Rotation angle in radians.\n    \"\"\"", "output": "\n    return skydir.transform_to(Galactic).transform_to(Galactic).get_g_tran()\n\n\ndef get_cel_to_skydir(x, y, skydir):\n    \"\"\"Calculate the sky coordinates of the local axes of a celestial\n    image.\n\n    Parameters\n    ----------\n    x, y : int\n        Coordinates of the center of the local axes.\n    x, y : int\n        Coordinates of the center of the axes.\n    \"\"\"\n    return skydir.transform_to(SkyCoord).transform_to(SkyCoord", "solution": "\n    wcs0 = create_wcs(skydir, coordsys='CEL')\n    wcs1 = create_wcs(skydir, coordsys='GAL')\n    x, y = SkyCoord.to_pixel(SkyCoord.from_pixel(1.0, 0.0, wcs0), wcs1)\n    return np.arctan2(y, x)"}
{"prompt": "def handler(key_file=None, cert_file=None, timeout=None):\n    \"\"\"This class returns an instance of the default HTTP request handler using\n    the values you provide.\n\n    :param `key_file`: A path to a PEM (Privacy Enhanced Mail) formatted file containing your private key (optional).\n    :type key_file: ``string``\n    :param `cert_file`: A path to a PEM (Privacy Enhanced Mail) formatted file containing a certificate chain file (optional).\n    :type cert_file: ``string``\n    :param `timeout`: The request time-out period, in seconds (optional).\n    :type timeout: ``integer`` or \"None\"\n    \"\"\"", "output": " timeout in seconds.\n    :type timeout: ``integer``\n    :return: A default HTTP request handler.\n    :rtype: :class:`httplib.HTTPConnection`\n    \"\"\"\n    def __init__(self, key_file=None, cert_file=None, strict=True):\n        \"\"\"\n        This class can be used to define a default request handler.\n\n        :param `key_file`: A path to a PEM (Privacy Enhanced Mail) formatted file containing your private key (optional).\n        :type key_file: ``string``\n        :param `cert_file`: A path to a PEM (Pri", "solution": "\n\n    def connect(scheme, host, port):\n        kwargs = {}\n        if timeout is not None: kwargs['timeout'] = timeout\n        if scheme == \"http\":\n            return httplib.HTTPConnection(host, port, **kwargs)\n        if scheme == \"https\":\n            if key_file is not None: kwargs['key_file'] = key_file\n            if cert_file is not None: kwargs['cert_file'] = cert_file\n            return httplib.HTTPSConnection(host, port, **kwargs)\n        raise ValueError(\"unsupported scheme: %s\" % scheme)\n\n    def request(url, message, **kwargs):\n        scheme, host, port, path = _spliturl(url)\n        body = message.get(\"body\", \"\")\n        head = {\n            \"Content-Length\": str(len(body)),\n            \"Host\": host,\n            \"User-Agent\": \"splunk-sdk-python/0.1\",\n            \"Accept\": \"*/*\",\n        } # defaults\n        for key, value in message[\"headers\"]:\n            head[key] = value\n        method = message.get(\"method\", \"GET\")\n\n        connection = connect(scheme, host, port)\n        try:\n            connection.request(method, path, body, head)\n            if timeout is not None:\n                connection.sock.settimeout(timeout)\n            response = connection.getresponse()\n        finally:\n            connection.close()\n\n        return {\n            \"status\": response.status,\n            \"reason\": response.reason,\n            \"headers\": response.getheaders(),\n            \"body\": ResponseReader(response),\n        }\n\n    return request"}
{"prompt": "def getScoringVector(self, profile):\n        \"\"\"\n        Returns the scoring vector. This function is called by getCandScoresMap().\n\n        :ivar Profile profile: A Profile object that represents an election profile.\n        \"\"\"", "output": "\n        return self._getCandScoresMap(profile, self._scoringVector)\n\n    def getCandScores(self, profile):\n        \"\"\"\n        Returns the election's scores. This function is called by getCandScoresMap().\n\n        :ivar Profile profile: A Profile object that represents the election profile.\n        \"\"\"\n        return self._getCandScoresMap(profile, self._candScores)\n\n    def _getCandScoresMap(self, profile, scoresMap):\n        \"\"\"\n        Returns the election's scores map.\n\n        :param Profile profile: The election profile.\n        :param dict scores", "solution": "\n\n        # Check to make sure that the scoring vector contains a score for every possible rank in a\n        # ranking.\n        if len(self.scoringVector) != profile.numCands:\n            print(\"ERROR: scoring vector is not the correct length\")\n            exit()\n\n        return self.scoringVector"}
{"prompt": "def failed(self, reason=None):\n        \"\"\"\n        Set the test status to Report.FAILED, and set the failure reason\n\n        :param reason: failure reason (default: None)\n        \"\"\"", "output": "\n        if reason is None:\n            reason = self.report_failures\n        self.report_failures = 0\n        self.report_errors = 0\n        self.report_errors_no_data = 0\n        self.report_errors_no_data_no_data = 0\n        self.report_errors_data_no_data = 0\n        self.report_errors_data_no_data_no_data_no_data = 0\n        self.report_errors_data_no_data_no_data_no_data = 0\n        self.report_errors_data_no_data", "solution": "\n        self.set_status(Report.FAILED)\n        if reason:\n            self.add('reason', reason)"}
{"prompt": "def _set_interface_type(self, v, load=False):\n    \"\"\"\n    Setter method for interface_type, mapped from YANG variable /brocade_interface_ext_rpc/get_interface_detail/input/interface_type (enumeration)\n    If this variable is read-only (config: false) in the\n    source YANG file, then _set_interface_type is considered as a private\n    method. Backends looking to populate this variable should\n    do so via calling thisObj._set_interface_type() directly.\n\n    YANG Description: The type of the interface. An 'unknown' type \nrepresents error scenario and should not be used.\n    \"\"\"", "output": " type indicates\nthat the peer is not present.\n    \"\"\"\n    def _set_interface_type(self, v, load=False):\n        \"\"\"\n    Setter method for interface_type, mapped from YANG variable /brocade_interface_ext_rpc/get_interface_detail/input/interface_type (enumeration)\n    If this variable is read-only (config: false) in the\n    source YANG file, then _set_interface_type is considered as a private\n    method. Backends looking to populate this variable should\n    do so via calling thisObj._set_interface_type() directly.\n\n   ", "solution": "\n    if hasattr(v, \"_utype\"):\n      v = v._utype(v)\n    try:\n      t = YANGDynClass(v,base=RestrictedClassType(base_type=unicode,                                     restriction_type=\"dict_key\",                                     restriction_arg={u'port-channel': {'value': 5}, u'loopback': {'value': 7}, u'fortygigabitethernet': {'value': 4}, u'unknown': {'value': 1}, u'gigabitethernet': {'value': 2}, u'tengigabitethernet': {'value': 3}, u'hundredgigabitethernet': {'value': 9}, u'fibrechannel': {'value': 8}, u'l2vlan': {'value': 6}},), is_leaf=True, yang_name=\"interface-type\", rest_name=\"interface-type\", parent=self, choice=(u'request-type', u'get-request'), path_helper=self._path_helper, extmethods=self._extmethods, register_paths=False, extensions={u'tailf-common': {u'info': u\"The type of the interface. An 'unknown' type \\nrepresents error scenario and should not be used.\"}}, namespace='urn:brocade.com:mgmt:brocade-interface-ext', defining_module='brocade-interface-ext', yang_type='enumeration', is_config=True)\n    except (TypeError, ValueError):\n      raise ValueError({\n          'error-string': "}
{"prompt": "def setDevice(self, device): \n        \"\"\"Sets the video stream\n        \n        :param device:      A rather generic device class.  In this case DataModel.RTSPCameraDevice.\n        \"\"\"", "output": "\n        self.device = device\n        self.stream = device.stream\n        self.video = device.video\n        self.video.set_stream(self.stream)\n        self.video.set_frame_rate(self.frame_rate)\n        self.video.set_format(self.format)\n        self.video.set_frame_size(self.frame_size)\n        self.video.set_frame_rate(self.frame_rate)\n        self.video.set_format(self.format)\n        self.video.set_frame_rate(self.frame_rate", "solution": "\n        print(self.pre, \"setDevice :\", device)\n        \n        if (not device and not self.device): # None can be passed as an argument when the device has not been set yet\n            return\n            \n        if (self.device):\n            if self.device == device:\n                print(self.pre, \"setDevice : same device\")\n                return\n            \n        if self.filterchain: # there's video already\n            self.clearDevice()\n        \n        self.device = device\n        self.video.setDevice(self.device) # inform the video widget so it can start drags\n        \n        # ManagedFilterChain.addViewPort accepts ViewPort instance\n        self.filterchain = self.filterchain_group.get(_id = self.device._id)\n        \n        if self.filterchain:\n            self.viewport.setXScreenNum(self.n_xscreen)\n            self.viewport.setWindowId  (int(self.video.winId()))\n            self.filterchain.addViewPort(self.viewport)"}
{"prompt": "def get_groups_from_category(self, category) -> typing.Iterator['Group']:\n        \"\"\"\n        Args:\n            category: group category\n\n        Returns: generator over all groups from a specific category in this coalition\n        \"\"\"", "output": "\n        for group in self.categories:\n            if group.category == category:\n                yield from group.groups\n\n    def get_coalition_groups(self, category) -> typing.Iterator['Group']:\n        \"\"\"\n        Args:\n            category: group category\n\n        Yields:\n            group\n\n        Raises:\n            KeyError: if the category is not found in this coalition\n        \"\"\"\n        for coalition in self.get_groups_from_category(category):\n            yield from coalition.get_coalitions()\n\n    def get_coalition_group(self, group_id: int) ->", "solution": "\n        Mission.validator_group_category.validate(category, 'get_groups_from_category')\n        for group in self.groups:\n\n            if group.group_category == category:\n                yield group"}
{"prompt": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See documentation for method `GroundShakingIntensityModel` in\n        :class:~`openquake.hazardlib.gsim.base.GSIM`\n        \"\"\"", "output": "\n        return self._get_mean_and_stddevs_for_sites(sites, rup, dists, imt,\n                                                  stddev_types)\n\n    def _get_mean_and_stddevs_for_sites(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        Compute the mean and stddevs for the given sites.\n\n        :param sites:\n            Number of sites.\n        :param rup:\n            Rupture parameters.\n        :param dists:\n            Distances parameters.\n        :param imt:\n            IMT parameters.\n        :param stddev_types:", "solution": "\n        # This is just used for testing purposes\n        if len(stddev_types) == 0:\n            stddev_types = [StdDev.TOTAL]\n        mean, stds = self._get_mean_and_stddevs(sites, rup, dists, imt,\n                                                stddev_types)\n        stddevs = [np.ones(len(dists.repi))*get_sigma(imt)]\n        delta = self._get_delta(stds, dists)\n        mean = mean + stds + delta\n        mean = np.squeeze(mean)\n        return mean, stddevs"}
{"prompt": "def stderr_file(self):\r\n        \"\"\"Filename to save kernel stderr output.\"\"\"", "output": "\r\n        return self._stderr_file\r\n\r\n    @property\r\n    def log(self):\r\n        \"\"\"Log file to use for logging.\"\"\"\r\n        return self._log\r\n\r\n    @log.setter\r\n    def log(self, value):\r\n        \"\"\"Set the log file to use for logging.\"\"\"\r\n        self._log = value\r\n\r\n    @property\r\n    def log_level(self):\r\n        \"\"\"Log level to use for logging.\"\"\"\r\n        return self._log_level\r\n\r\n    @log_level.setter\r\n    def log_level(self, value):\r\n        \"\"\"Set the log level to use for logging.\"\"\"\r\n        self._log_level = value\r\n\r\n    @property\r\n    def", "solution": "\r\n        stderr_file = None\r\n        if self.connection_file is not None:\r\n            stderr_file = self.kernel_id + '.stderr'\r\n            if self.stderr_dir is not None:\r\n                stderr_file = osp.join(self.stderr_dir, stderr_file)\r\n            else:\r\n                try:\r\n                    stderr_file = osp.join(get_temp_dir(), stderr_file)\r\n                except (IOError, OSError):\r\n                    stderr_file = None\r\n        return stderr_file"}
{"prompt": "def mk_message(org, user, key):\n    \"\"\"\n    Make message\n    \"\"\"", "output": "\n    return Message(org=org, user=user, key=key)\n\n\ndef mk_message_from_file(filename):\n    \"\"\"\n    Make message from file\n    \"\"\"\n    return Message.from_file(filename)\n\n\ndef mk_org_file(org, filename):\n    \"\"\"\n    Make org file\n    \"\"\"\n    return OrgFile(org=org, filename=filename)\n\n\ndef mk_user_file(user, filename):\n    \"\"\"\n    Make user file\n    \"\"\"\n    return UserFile(user=user, filename=filename)\n\n\ndef mk_key_file(org, filename", "solution": "\n\n    m = Message()\n    m['client'] = \"taskc-py {0}\".format(__version__)\n    m['protocol'] = \"v1\"\n    m['org'] = org\n    m['user'] = user\n    m['key'] = key\n\n    return m"}
{"prompt": "def _prepare_headers(self, additional_headers=None, **kwargs):\n        \"\"\"Prepare headers for http communication.\n\n        Return dict of header to be used in requests.\n\n        Args:\n            .. versionadded:: 0.3.2\n                **additional_headers**: (optional) Additional headers\n                to be used with request\n\n        Returns:\n            Headers dict. Key and values are string\n\n        \"\"\"", "output": "\n        headers = {}\n        if additional_headers:\n            headers.update(additional_headers)\n        if self.user_agent:\n            headers.update({\n                'User-Agent': self.user_agent,\n            })\n        if self.request_headers:\n            headers.update(self.request_headers)\n        if self.request_params:\n            params = self.request_params.copy()\n            params.update(kwargs)\n            headers.update({\n                'Content-Type': 'application/x-www-form-urlencoded',\n                'User-Agent': self.user_agent,\n                'Content", "solution": "\n        user_agent = \"pyseaweed/{version}\".format(version=__version__)\n        headers = {\"User-Agent\": user_agent}\n        if additional_headers is not None:\n            headers.update(additional_headers)\n        return headers"}
{"prompt": "def enzyme(self, ec_number=None, hgnc_symbol=None, hgnc_identifier=None, limit=None, as_df=False):\n        \"\"\"Method to query :class:`.models.Enzyme` objects in database\n\n        :param ec_number: Enzyme Commission number (EC number)(s)\n        :type ec_number: str or tuple(str) or None\n\n        :param hgnc_symbol: HGNC symbol(s)\n        :type hgnc_symbol: str or tuple(str) or None\n\n        :param hgnc_identifier: identifiers(s) in :class:`.models.HGNC`\n        :type hgnc_identifier: int or tuple(int) or None\n\n        :param limit:\n            - if `isinstance(limit,int)==True` -> limit\n            - if `isinstance(limit,tuple)==True` -> format:= tuple(page_number, results_per_page)\n            - if limit == None -> all results\n        :type limit: int or tuple(int) or None\n\n        :param bool as_df: if `True` results are returned as :class:`pandas.DataFrame`\n\n        :return:\n            - if `as_df == False` -> list(:class:`.models.Enzyme`)\n            - if `as_df == True`  -> :class:`pandas.DataFrame`\n        :rtype: list(:class:`.models.Enzyme`) or :class:`pandas.DataFrame`\n\n        \"\"\"", "output": "c_identifier: HGNC identifier(s)\n        :type hgnc_identifier: str or tuple(str) or None\n\n        :param limit: Limit number of results\n        :type limit: int\n\n        :param as_df: Return DataFrame instead of list of dictionaries\n        :type as_df: bool\n\n        :returns: :class:`.models.Enzyme`\n        :rtype: models.Enzyme\n\n        \"\"\"\n        if ec_number is not None:\n            return self.query.filter(models.EnzymeCommissionNumber.in_(ec_number))\n        if hgnc_symbol", "solution": "\n        q = self.session.query(models.Enzyme)\n\n        model_queries_config = (\n            (ec_number, models.Enzyme.ec_number),\n        )\n        q = self.get_model_queries(q, model_queries_config)\n\n        many_to_many_queries_config = (\n            (hgnc_symbol, models.Enzyme.hgncs, models.HGNC.symbol),\n            (hgnc_identifier, models.Enzyme.hgncs, models.HGNC.identifier),\n        )\n        q = self.get_many_to_many_queries(q, many_to_many_queries_config)\n\n        return self._limit_and_df(q, limit, as_df)"}
{"prompt": "def get_file_sample(self, numLines=10):\n        \"\"\" retrieve a sample of the file \"\"\"", "output": "\n        return self.file_sample_class(self.data, numLines)\n\n    def test_get_file_sample_class(self):\n        \"\"\" test the get_file_sample method of the Sample class \"\"\"\n        self.assertEqual(self.file_sample_class.get_file_sample(numLines=10),\n                         self.file_sample_class)\n\n    def test_get_file_sample_lines(self):\n        \"\"\" test the get_file_sample method of the SampleLines class \"\"\"\n        self.assertEqual(self.file_sample_lines.get_file_sample(", "solution": "\n        res = ''\n        try:\n            with open(self.fullname, 'r') as f:\n                for line_num, line in enumerate(f):\n                    res += str(line_num).zfill(5) + ' ' + line \n                    if line_num >= numLines-1:\n                        break\n            return res\n        except Exception as ex:\n            print('cant get_file_sample in \"', self.fullname, '\":', str(ex))\n            return res"}
{"prompt": "def getView(self, lv):\n        \"\"\"Determine the detector view starting with a G4LogicalVolume\"\"\"", "output": "\n        return self.getG4LogicalVolumeView(lv)\n\n    def getG4LogicalVolumeView(self, lv):\n        \"\"\"Determine the G4 LogicalVolume view starting with a G4Filesystem\"\"\"\n        return self.getG4FilesystemView(lv)\n\n    def getG4FilesystemView(self, lv):\n        \"\"\"Determine the G4Filesystem view starting with a G4Filesystem\"\"\"\n        return self.getG4FilesystemView(lv)\n\n    def getG4Filesystem(self, vg):\n        \"\"\"Determine the file system this detector is running on\"\"\"\n        return self.get", "solution": "\n        view = None\n        if str(lv.GetName())[-1] == 'X':\n            return 'X'\n        elif str(lv.GetName())[-1] == 'Y':\n            return 'Y'\n\n        self.log.error('Cannot determine view for %s', lv.GetName())\n        raise 'Cannot determine view for %s' % lv.GetName()\n        return view"}
{"prompt": "def update_thing_shadow(self, thing_name, payload):\n        \"\"\"\n        spec of payload:\n          - need node `state`\n          - state node must be an Object\n          - State contains an invalid node: 'foo'\n        \"\"\"", "output": "\n        if not self.is_valid_node(thing_name):\n            return False\n        if not self.is_valid_node(payload):\n            return False\n        if not self.is_valid_payload(payload):\n            return False\n        if not self.is_valid_payload(payload):\n            return False\n        if not self.is_valid_payload(payload):\n            return False\n        if not self.is_valid_thing(payload):\n            return False\n        if not self.is_valid_thing(thing_name):\n            return False\n        if not self.is_valid_thing(thing", "solution": "\n        thing = iot_backends[self.region_name].describe_thing(thing_name)\n\n        # validate\n        try:\n            payload = json.loads(payload)\n        except ValueError:\n            raise InvalidRequestException('invalid json')\n        if 'state' not in payload:\n            raise InvalidRequestException('need node `state`')\n        if not isinstance(payload['state'], dict):\n            raise InvalidRequestException('state node must be an Object')\n        if any(_ for _ in payload['state'].keys() if _ not in ['desired', 'reported']):\n            raise InvalidRequestException('State contains an invalid node')\n\n        new_shadow = FakeShadow.create_from_previous_version(thing.thing_shadow, payload)\n        thing.thing_shadow = new_shadow\n        return thing.thing_shadow"}
{"prompt": "def _request(self, buf, properties, date=None):\n        \"\"\"Send a request to the CoreNLP server.\n\n        :param (str | unicode) text: raw text for the CoreNLPServer to parse\n        :param (dict) properties: properties that the server expects\n        :param (str) date: reference date of document, used by server to set docDate - expects YYYY-MM-DD\n        :return: request result\n        \"\"\"", "output": "\n        if not date:\n            date = datetime.now().strftime('%Y-%m-%d')\n        self.set_docDate(date)\n        self.set_properties(properties, buf)\n        self.set_docDate(datetime.strptime(date, '%Y-%m-%d'))\n        return self.get_response()\n\n    def _get_response(self):\n        \"\"\"Return the result of the CoreNLP parsing.\n\n        :return: parsed result\n        \"\"\"\n        return self.core.parse(self.text)\n\n    def set_properties(self, properties, buf):\n        \"\"\"Send a properties", "solution": "\n        self.ensure_alive()\n\n        try:\n            input_format = properties.get(\"inputFormat\", \"text\")\n            if input_format == \"text\":\n                ctype = \"text/plain; charset=utf-8\"\n            elif input_format == \"serialized\":\n                ctype = \"application/x-protobuf\"\n            else:\n                raise ValueError(\"Unrecognized inputFormat \" + input_format)\n\n            if date:\n                params = {'properties': str(properties),'date': str(date)}\n            else:\n                params = {'properties': str(properties)}\n\n            r = requests.post(self.endpoint,\n                              params=params,\n                              data=buf, headers={'content-type': ctype},\n                              timeout=(self.timeout*2)/1000)\n            r.raise_for_status()\n            return r\n        except requests.HTTPError as e:\n            if r.text == \"CoreNLP request timed out. Your document may be too long.\":\n                raise TimeoutException(r.text)\n            else:\n                raise AnnotationException(r.text)"}
{"prompt": "def delete_metadata_value(metadata_source, key: str) -> None:\n    \"\"\"Delete the metadata value for the given key.\n\n    There are a set of predefined keys that, when used, will be type checked and be interoperable with other\n    applications. Please consult reference documentation for valid keys.\n\n    If using a custom key, we recommend structuring your keys in the '<dotted>.<group>.<attribute>' format followed\n    by the predefined keys. e.g. 'stem.session.instrument' or 'stm.camera.binning'.\n\n    Also note that some predefined keys map to the metadata ``dict`` but others do not. For this reason, prefer\n    using the ``metadata_value`` methods over directly accessing ``metadata``.\n    \"\"\"", "output": " same value, e.g.'session.instrument.1'.\n    \"\"\"\n    if key in metadata_source.keys():\n        del metadata_source[key]\n    if'session' in metadata_source:\n        del metadata_source['session']\n    if 'attribute' in metadata_source:\n        del metadata_source['attribute']\n\n\ndef get_metadata_value(metadata_source: Dict[str, str], key: str) -> Optional[str]:\n    \"\"\"Get the metadata value for the given key.\n\n    If using a custom key, we return the value for the given key. Otherwise, we", "solution": "\n    desc = session_key_map.get(key)\n    if desc is not None:\n        d0 = getattr(metadata_source, \"session_metadata\", dict())\n        d = d0\n        for k in desc['path'][:-1]:\n            d =  d.setdefault(k, dict()) if d is not None else None\n        if d is not None and desc['path'][-1] in d:\n            d.pop(desc['path'][-1], None)\n            metadata_source.session_metadata = d0\n            return\n    desc = key_map.get(key)\n    if desc is not None:\n        d0 = getattr(metadata_source, \"metadata\", dict())\n        d = d0\n        for k in desc['path'][:-1]:\n            d =  d.setdefault(k, dict()) if d is not None else None\n        if d is not None and desc['path'][-1] in d:\n            d.pop(desc['path'][-1], None)\n            metadata_source.metadata = d0\n            return"}
{"prompt": "def send(\n            self):\n        \"\"\"*send the mobi book generated to kindle email address(es)*\n\n        **Return:**\n            - ``success`` -- True or False depending on the success/failure of sending the email to the kindle email address(es).\n        \"\"\"", "output": "\n        return self._send(\n            self.create_request(\n               'send',\n                data=self._build_request_body(),\n                headers=self._build_request_headers(),\n                files=self._build_request_files(),\n                params=self._build_request_params(),\n                files_params=self._build_request_files_params(),\n                params_params=self._build_params_params(),\n                timeout=self._build_params_timeout(),\n                verify=self._build_params_verify_params(),\n                timeout_params=self._build_params_timeout(),\n                data", "solution": "\n        self.log.debug('starting the ``send`` method')\n\n        if self.urlOrPath.split(\".\")[-1] == \"docx\":\n            if self.title:\n                pathToMobi = self.outputDirectory + \"/\" + self.title + \".docx\"\n            else:\n                pathToMobi = self.outputDirectory + \"/\" + \\\n                    os.path.basename(self.urlOrPath)\n            shutil.copyfile(self.urlOrPath, pathToMobi)\n        else:\n            pathToMobi = self.get()\n            if not pathToMobi:\n                return 404\n\n        # create MIME message\n\n        msg = MIMEMultipart()\n        msg['From'] = self.settings[\"email\"][\"user_email\"]\n        msg['To'] = \", \".join(self.settings[\"email\"][\"kindle_emails\"])\n        msg['Subject'] = 'Polyglot to Kindle'\n        text = 'This email has been automatically sent by polyglot'\n        msg.attach(MIMEText(text))\n\n        basename = os.path.basename(pathToMobi)\n        print \"Sending the book `%(pathToMobi)s` to Kindle device(s)\" % locals()\n        msg.attach(self.get_attachment(pathToMobi))\n\n        # convert MIME message to string\n        fp = StringIO()\n        gen = Generator(fp, mangle_from_=False)\n        gen.flatten(msg)\n        msg = fp.getvalue()\n\n        # send email\n        try:\n            mail_server = smtplib.SMTP_SSL(host=self.settings[\"email\"][\"smtp_server\"],\n                                           port=self.settings[\"email\"][\"smtp_port\"])\n            mail_server.login(self.settings[\"email\"][\"smtp_login\"], self.settings[\n                              \"email\"][\"smtp_password\"])\n            mail_server.sendmail(self.settings[\"email\"][\"user_email\"], \", \".join(self.settings[\n                                 \"email\"][\"kindle_emails\"]), msg)\n            mail_server.close()\n        except smtplib.SMTPException:\n            os.remove(pathToMobi)\n            self.log.error(\n                'Communication with your SMTP server failed. Maybe wrong connection details? Check exception details and your headjack settings file')\n            return False\n\n        os.remove(pathToMobi)\n\n        self.log.debug('completed the ``send`` method')\n        return True"}
{"prompt": "def _read_keyvals(filename):\n        \"\"\"Read the key/values if the file exists.\n\n        returns -- a dictionary with key/values, or empty dictionary if the\n                   file does not exist.\n\n        \"\"\"", "output": "\n        try:\n            return self.read(filename)\n        except IOError:\n            return {}\n\n    def _get_keyvals(self, filename):\n        \"\"\"Get the key/values if the file exists.\n\n        returns -- a list of dictionaries with key/values, or empty\n                   dictionaries if the file does not exist.\n\n        \"\"\"\n        try:\n            return self._read_keyvals(filename)\n        except IOError:\n            return []\n\n    def _get_keyvals_from_file(self, filename):\n        \"\"\"Get the key/values from the specified file.\n\n        returns -- a list of dictionaries with key/values,", "solution": "\n        assert isinstance(filename, str)\n        if os.path.exists(filename):\n            return _KeyValuePersister._actually_populate_keyvals(filename)\n        else:\n            return {}"}
{"prompt": "def _set_bundle_message(self, v, load=False):\n    \"\"\"\n    Setter method for bundle_message, mapped from YANG variable /mpls_config/router/mpls/mpls_cmds_holder/mpls_interface/rsvp/interface_refresh_reduction/bundle_message (container)\n    If this variable is read-only (config: false) in the\n    source YANG file, then _set_bundle_message is considered as a private\n    method. Backends looking to populate this variable should\n    do so via calling thisObj._set_bundle_message() directly.\n    \"\"\"", "output": "\n\n    YANG Description: Message to display in the RSVP\ninterface refresh reduction\n\"\"\"\n    def set_bundle_message(v):\n        if hasattr(v, \"_utype\"):\n            v = v._utype(v)\n        _set_bundle_message(self, v, load)\n    v = mpls_messages.mpls_message(v, load=load)\n    return v\n\ndef _get_maximum_paths(config):\n    \"\"\"\n    Getter method for maximum_paths, mapped from YANG variable /mpls_config/router/mpls/mpls_cmds_holder", "solution": "\n    if hasattr(v, \"_utype\"):\n      v = v._utype(v)\n    try:\n      t = YANGDynClass(v,base=bundle_message.bundle_message, is_container='container', presence=True, yang_name=\"bundle-message\", rest_name=\"bundle-message\", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'info': u'Refresh Reduction bundle messaging feature', u'alt-name': u'bundle-message'}}, namespace='urn:brocade.com:mgmt:brocade-mpls', defining_module='brocade-mpls', yang_type='container', is_config=True)\n    except (TypeError, ValueError):\n      raise ValueError({\n          'error-string': "}
{"prompt": "def decrypt(key, ct, Nk=4):\n    \"\"\"Decrypt a plain text block.\"\"\"", "output": "\n    return _lib.EVP_Decrypt(key, ct, Nk)\n\ndef encrypt(key, ct):\n    \"\"\"Encrypt a block of plain text.\"\"\"\n    return _lib.EVP_Encrypt(key, ct)\n\ndef decrypt(key, ct):\n    \"\"\"Decrypt a block of plain text.\"\"\"\n    return _lib.EVP_Decrypt(key, ct, len(ct))\n\ndef _check_key(key):\n    \"\"\"Check if a given key is a valid EVP key.\"\"\"\n    return _lib.EVP_VerifyInit(key)\n", "solution": "\n    assert Nk in {4, 6, 8}\n    rkey = key_expand(key, Nk)\n    pt = inv_cipher(rkey, ct, Nk)\n\n    return pt"}
{"prompt": "def build_schema(\n    source: Union[str, Source],\n    assume_valid=False,\n    assume_valid_sdl=False,\n    no_location=False,\n    experimental_fragment_variables=False,\n) -> GraphQLSchema:\n    \"\"\"Build a GraphQLSchema directly from a source document.\"\"\"", "output": "\n    if isinstance(source, str):\n        return GraphQLSchema(source)\n    elif isinstance(source, Source):\n        return source\n    elif isinstance(source, Source):\n        return GraphQLSchema(\n            _parse_source_document(source.doc) if experimental_variables else None\n        )\n    elif no_location:\n        return None\n    elif not isinstance(source, Source):\n        raise TypeError(\"Expected a str or Source object, got %r\" % source)\n    else:\n        return source.graphql_schema\n\n\ndef build_graphql_schema(\n    source: Union[str, Source],\n   ", "solution": "\n    return build_ast_schema(\n        parse(\n            source,\n            no_location=no_location,\n            experimental_fragment_variables=experimental_fragment_variables,\n        ),\n        assume_valid=assume_valid,\n        assume_valid_sdl=assume_valid_sdl,\n    )"}
{"prompt": "def is_valid(isbn_id):\n    \"\"\"\n    Check that a given string is a valid ISBN.\n\n    :param isbn_id: the isbn to be checked.\n    :returns: boolean indicating whether the isbn is valid or not.\n\n    >>> is_valid(\"978-3-16-148410-0\")\n    True\n\n    >>> is_valid(\"9783161484100\")\n    True\n\n    >>> is_valid(\"9783161484100aa\")\n    False\n\n    >>> is_valid(\"abcd\")\n    False\n\n    >>> is_valid(\"0136091814\")\n    True\n\n    >>> is_valid(\"0136091812\")\n    False\n\n    >>> is_valid(\"9780136091817\")\n    False\n\n    >>> is_valid(\"123456789X\")\n    True\n    \"\"\"", "output": " >>> is_valid(\"1\")\n    False\n\n    >>> is_valid(\"1\", \"abcd\")\n    True\n\n    >>> is_valid(\"1\", \"1\")\n    False\n\n    >>> is_valid(\"1\", \"1\", \"1\")\n    True\n\n    >>> is_valid(\"1\", \"1\", \"1\", \"1\")\n    False\n\n    \"\"\"\n    try:\n        isbn_id = isbn_id.replace(\" \", \"\")\n        isbn_id = isbn_id.replace(\"+\", \"\")\n        isbn_id = isbn_id.replace(\" \", \"\")\n    except AttributeError:\n        return False\n   ", "solution": "\n    return (\n        (not isbnlib.notisbn(isbn_id)) and (\n            isbnlib.get_canonical_isbn(isbn_id) == isbn_id or\n            isbnlib.mask(isbnlib.get_canonical_isbn(isbn_id)) == isbn_id)\n    )"}
{"prompt": "def read_solrad(filename):\n    \"\"\"\n    Read NOAA SOLRAD [1]_ [2]_ fixed-width file into pandas dataframe.\n\n    Parameters\n    ----------\n    filename: str\n        filepath or url to read for the fixed-width file.\n\n    Returns\n    -------\n    data: Dataframe\n        A dataframe with DatetimeIndex and all of the variables in the\n        file.\n\n    Notes\n    -----\n    SOLRAD data resolution is described by the README_SOLRAD.txt:\n    \"Before 1-jan. 2015 the data were reported as 3-min averages;\n    on and after 1-Jan. 2015, SOLRAD data are reported as 1-min.\n    averages of 1-sec. samples.\"\n    Here, missing data is flagged as NaN, rather than -9999.9.\n\n    References\n    ----------\n    .. [1] NOAA SOLRAD Network\n       `https://www.esrl.noaa.gov/gmd/grad/solrad/index.html\n       <https://www.esrl.noaa.gov/gmd/grad/solrad/index.html>`_\n\n    .. [2] B. B. Hicks et. al., (1996), The NOAA Integrated Surface\n       Irradiance Study (ISIS). A New Surface Radiation Monitoring\n       Program. Bull. Amer. Meteor. Soc., 77, 2857-2864.\n       :doi:`10.1175/1520-0477(1996)077<2857:TNISIS>2.0.CO;2`\n    \"\"\"", "output": " was reported as 1-min averages.\"\n    \"\"\"\n    data = pd.read_csv(filename, sep=' ', header=None, index_col=0)\n    return data\n\n\ndef read_solrad_noaa(filename):\n    \"\"\"\n    Read NOAA SOLRAD [1]_ [2]_ fixed-width file into pandas dataframe.\n\n    Parameters\n    ----------\n    filename: str\n        filepath or url to read for the fixed-width file.\n\n    Returns\n    -------\n    data: Dataframe\n        A dataframe with DatetimeIndex and all of the variables in the\n        file.\n\n    Notes\n    -----\n    SOLRAD data resolution", "solution": "\n    if 'msn' in filename:\n        names = MADISON_HEADERS\n        widths = MADISON_WIDTHS\n        dtypes = MADISON_DTYPES\n    else:\n        names = HEADERS\n        widths = WIDTHS\n        dtypes = DTYPES\n\n    # read in data\n    data = pd.read_fwf(filename, header=None, skiprows=2, names=names,\n                       widths=widths, na_values=-9999.9)\n\n    # loop here because dtype kwarg not supported in read_fwf until 0.20\n    for (col, _dtype) in zip(data.columns, dtypes):\n        ser = data[col].astype(_dtype)\n        if _dtype == 'float64':\n            # older verions of pandas/numpy read '-9999.9' as\n            # -9999.8999999999996 and fail to set nan in read_fwf,\n            # so manually set nan\n            ser = ser.where(ser > -9999, other=np.nan)\n        data[col] = ser\n\n    # set index\n    # columns do not have leading 0s, so must zfill(2) to comply\n    # with %m%d%H%M format\n    dts = data[['month', 'day', 'hour', 'minute']].astype(str).apply(\n        lambda x: x.str.zfill(2))\n    dtindex = pd.to_datetime(\n        data['year'].astype(str) + dts['month'] + dts['day'] + dts['hour'] +\n        dts['minute'], format='%Y%m%d%H%M', utc=True)\n    data = data.set_index(dtindex)\n    try:\n        # to_datetime(utc=True) does not work in older versions of pandas\n        data = data.tz_localize('UTC')\n    except TypeError:\n        pass\n\n    return data"}
{"prompt": "def get_impediments():\n    \"\"\"\n    :raise urllib2.URLError | urllib.error.URLError: When\n    :return:\n    \"\"\"", "output": "\n    try:\n        return urllib2.urlopen(url, timeout=10).read()\n    except urllib2.URLError as e:\n        raise Exception(e)\n\n\ndef get_page_source(url):\n    \"\"\"\n    :raise urllib2.URLError | urllib.error.URLError: When\n    :return:\n    \"\"\"\n    try:\n        return urllib2.urlopen(url, timeout=10).read()\n    except urllib2.URLError as e:\n        raise Exception(e)\n\n\ndef get_page_source_with_referer(url, referer):\n    \"\"\"\n    :raise urllib2.URLError | urllib", "solution": "\n    file = urlopen(XML_URL)\n\n    doc = dom.parse(file)\n    xml = doc.documentElement\n\n    nodes = xml.getElementsByTagName('utr')\n    impediments = [extract(node) for node in nodes]\n\n    return impediments"}
{"prompt": "def _get_services_mapping():\n    \"\"\"\n    Build a map of services based on the IANA assignment list:\n    http://www.iana.org/assignments/port-numbers\n\n    It will load the /etc/services file and will build the mapping on the fly,\n    similar to the Capirca's SERVICES file:\n    https://github.com/google/capirca/blob/master/def/SERVICES.svc\n\n    As this module is be available on Unix systems only,\n    we'll read the services from /etc/services.\n    In the worst case, the user will not be able to specify the\n    services shortcut and they will need to specify the protocol / port combination\n    using the source_port / destination_port & protocol fields.\n    \"\"\"", "output": " have to specify the\n    port number.\n    \"\"\"\n    if os.name == 'nt':\n        # Windows doesn't support the port number, so we'll read the services\n        # from /etc/services.\n        return {}\n    else:\n        # Linux does not support the port number, so we'll read the services\n        # from /etc/services.\n        return {}\n\n\ndef _get_services_config():\n    \"\"\"\n    Build a map of port numbers and services based on the IANA config:\n    http://www.iana.org/assignments/port-numbers\n\n    It will load the /etc/", "solution": "\n    if _SERVICES:\n        return _SERVICES\n    services_txt = ''\n    try:\n        with salt.utils.files.fopen('/etc/services', 'r') as srv_f:\n            services_txt = salt.utils.stringutils.to_unicode(srv_f.read())\n    except IOError as ioe:\n        log.error('Unable to read from /etc/services:')\n        log.error(ioe)\n        return _SERVICES  # no mapping possible, sorry\n        # will return the default mapping\n    service_rgx = re.compile(r'^([a-zA-Z0-9-]+)\\s+(\\d+)\\/(tcp|udp)(.*)$')\n    for line in services_txt.splitlines():\n        service_rgx_s = service_rgx.search(line)\n        if service_rgx_s and len(service_rgx_s.groups()) == 4:\n            srv_name, port, protocol, _ = service_rgx_s.groups()\n            if srv_name not in _SERVICES:\n                _SERVICES[srv_name] = {\n                    'port': [],\n                    'protocol': []\n                }\n            try:\n                _SERVICES[srv_name]['port'].append(int(port))\n            except ValueError as verr:\n                log.error(verr)\n                log.error('Did not read that properly:')\n                log.error(line)\n                log.error('Please report the above error: %s does not seem a valid port value!', port)\n            _SERVICES[srv_name]['protocol'].append(protocol)\n    return _SERVICES"}
{"prompt": "def model_select(\n            self,\n            score_function,\n            alleles=None,\n            min_models=1,\n            max_models=10000):\n        \"\"\"\n        Perform model selection using a user-specified scoring function.\n\n        Model selection is done using a \"step up\" variable selection procedure,\n        in which models are repeatedly added to an ensemble until the score\n        stops improving.\n\n        Parameters\n        ----------\n        score_function : Class1AffinityPredictor -> float function\n            Scoring function\n\n        alleles : list of string, optional\n            If not specified, model selection is performed for all alleles.\n\n        min_models : int, optional\n            Min models to select per allele\n\n        max_models : int, optional\n            Max models to select per allele\n\n        Returns\n        -------\n        Class1AffinityPredictor : predictor containing the selected models\n        \"\"\"", "output": "models : int, optional\n            Minimum number of models to include in the ensemble.\n\n        max_models : int, optional\n            Maximum number of models to include in the ensemble.\n\n        Returns\n        -------\n        model_selection : float function\n            The model selection value.\n        \"\"\"\n        if score_function.model_selection is None:\n            raise ValueError('No model selection available')\n\n        if alleles is None:\n            alleles = self.alleles\n\n        if min_models < 0:\n            raise ValueError('min_models must be >= 0')\n\n        if max_models < 0:\n            raise ValueError('max_models must be >=", "solution": "\n\n        if alleles is None:\n            alleles = self.supported_alleles\n\n        dfs = []\n        allele_to_allele_specific_models = {}\n        for allele in alleles:\n            df = pandas.DataFrame({\n                'model': self.allele_to_allele_specific_models[allele]\n            })\n            df[\"model_num\"] = df.index\n            df[\"allele\"] = allele\n            df[\"selected\"] = False\n\n            round_num = 1\n\n            while not df.selected.all() and sum(df.selected) < max_models:\n                score_col = \"score_%2d\" % round_num\n                prev_score_col = \"score_%2d\" % (round_num - 1)\n\n                existing_selected = list(df[df.selected].model)\n                df[score_col] = [\n                    numpy.nan if row.selected else\n                    score_function(\n                        Class1AffinityPredictor(\n                            allele_to_allele_specific_models={\n                                allele: [row.model] + existing_selected\n                            }\n                        )\n                    )\n                    for (_, row) in df.iterrows()\n                ]\n\n                if round_num > min_models and (\n                        df[score_col].max() < df[prev_score_col].max()):\n                    break\n\n                # In case of a tie, pick a model at random.\n                (best_model_index,) = df.loc[\n                    (df[score_col] == df[score_col].max())\n                ].sample(1).index\n                df.loc[best_model_index, \"selected\"] = True\n                round_num += 1\n\n            dfs.append(df)\n            allele_to_allele_specific_models[allele] = list(\n                df.loc[df.selected].model)\n\n        df = pandas.concat(dfs, ignore_index=True)\n\n        new_predictor = Class1AffinityPredictor(\n            allele_to_allele_specific_models,\n            metadata_dataframes={\n                \"model_selection\": df,\n            })\n        return new_predictor"}
{"prompt": "def remove_all_locks(self):\n        \"\"\"Removes all locks and ensures their content is written to disk.\"\"\"", "output": "\n        self.remove_locks()\n        self.remove_locks()\n\n    def remove_all_locks_with_lock_type(self, lock_type):\n        \"\"\"Removes all locks with a lock type.\"\"\"\n        self.remove_locks_with_lock_type(lock_type)\n        self.remove_all_locks_with_type(lock_type)\n\n    def remove_all_locks_with_type(self, lock_type):\n        \"\"\"Removes all locks with a lock type.\"\"\"\n        self.remove_locks_with_lock_type(lock_type)\n        self.remove_", "solution": "\n        locks = list(self._locks.items())\n        locks.sort(key=lambda l: l[1].get_last_access())\n        for l in locks:\n            self._remove_lock(l[0])"}
{"prompt": "def Presentation(pptx=None):\n    \"\"\"\n    Return a |Presentation| object loaded from *pptx*, where *pptx* can be\n    either a path to a ``.pptx`` file (a string) or a file-like object. If\n    *pptx* is missing or ``None``, the built-in default presentation\n    \"template\" is loaded.\n    \"\"\"", "output": "\n    if pptx is None:\n        pptx = path.join(os.path.dirname(__file__), 'default.pptx')\n    return pptx.PPTXFile(pptx)\n\n\nclass PPTX(object):\n    \"\"\"\n    A |PPTX| object.\n\n    *pptx* is a :class:`~poptx.POPTX` instance.\n    \"\"\"\n    def __init__(self, path):\n        \"\"\"\n        Create a new |PPTX| object.\n\n        *path* is a string path to the ``.pptx`` file", "solution": "\n    if pptx is None:\n        pptx = _default_pptx_path()\n\n    presentation_part = Package.open(pptx).main_document_part\n\n    if not _is_pptx_package(presentation_part):\n        tmpl = \"file '%s' is not a PowerPoint file, content type is '%s'\"\n        raise ValueError(tmpl % (pptx, presentation_part.content_type))\n\n    return presentation_part.presentation"}
{"prompt": "def determine_extended_chord5(chord, shorthand=False, no_inversions=False,\n        no_polychords=False):\n    \"\"\"Determine the names of an extended chord.\"\"\"", "output": "\n    if not chord:\n        return []\n    if shorthand:\n        return [chord.name]\n    if no_inversions:\n        return [chord.name, chord.inversion]\n    if no_polychords:\n        return [chord.name, chord.inversion]\n    return [chord.name] + list(chord.inversion.split())\n\n\ndef determine_chord6(chord, shorthand=False, no_inversions=False,\n        no_polychords=False):\n    \"\"\"Determine the names of an extended chord.\"\"\"", "solution": "\n    if len(chord) != 5:\n        # warning raise exeption: not an extended chord\n        return False\n\n    def inversion_exhauster(chord, shorthand, tries, result, polychords):\n        "}
{"prompt": "def as_json(self):\n        # type: () -> dict\n        \"\"\"Represent effect as JSON dict.\"\"\"", "output": "\n        return {\n            \"id\": self.id,\n            \"effect_id\": self.effect_id,\n            \"effect_name\": self.effect_name,\n            \"effect_code\": self.effect_code,\n            \"effect_name_plural\": self.effect_name_plural,\n            \"effect_code_plural\": self.effect_code_plural,\n            \"effect_code_singular\": self.effect_code_singular,\n            \"effect_code_plural\": self.effect_code_plural,\n            \"effect_name_singular\": self.effect_name_singular,\n           ", "solution": "\n        self._config['applyCss'] = self.applyCss\n        self._json['config'] = self._config\n        return self._json"}
{"prompt": "def synset_signatures(ss: \"wn.Synset\", hyperhypo=True, adapted=False,\n                      remove_stopwords=True, to_lemmatize=True, remove_numbers=True,\n                      lowercase=True, original_lesk=False, from_cache=True) -> set:\n    \"\"\"\n    Takes a Synset and returns its signature words.\n\n    :param ss: An instance of wn.Synset.\n    :return: A set of signature strings\n    \"\"\"", "output": "\n    if ss.synset_type == \"ngram\":\n        return set(wn.ngrams(ss.text))\n    elif ss.synset_type == \"ngram_with_stopwords\":\n        return set(wn.ngrams(ss.text, remove_stopwords=remove_stopwords))\n    elif ss.synset_type == \"ngram_with_numbers\":\n        return set(wn.ngrams(ss.text, remove_numbers=remove_numbers))\n    elif ss.synset_type == \"ngram_with_stopwords_with_numbers\":\n       ", "solution": "\n    if from_cache:\n        return synset_signatures_from_cache(ss, hyperhypo, adapted, original_lesk)\n\n    # Collects the signatures from WordNet.\n    signature = []\n\n    # Adds the definition, example sentences and lemma_names.\n    signature += word_tokenize(ss.definition())\n\n    # If the original lesk signature is requested, skip the other signatures.\n    if original_lesk:\n        return set(signature)\n\n    # Adds the examples and lemma names.\n    signature += chain(*[word_tokenize(eg) for eg in ss.examples()])\n    signature += ss.lemma_names()\n\n    # Includes lemma_names of hyper-/hyponyms.\n    if hyperhypo:\n        hyperhyponyms = set(ss.hyponyms() + ss.hypernyms() + ss.instance_hyponyms() + ss.instance_hypernyms())\n        signature += set(chain(*[i.lemma_names() for i in hyperhyponyms]))\n\n    # Includes signatures from related senses as in Adapted Lesk.\n    if adapted:\n        # Includes lemma_names from holonyms, meronyms and similar_tos\n        related_senses = set(ss.member_holonyms() + ss.part_holonyms() + ss.substance_holonyms() + \\\n                             ss.member_meronyms() + ss.part_meronyms() + ss.substance_meronyms() + \\\n                             ss.similar_tos())\n        signature += set(chain(*[i.lemma_names() for i in related_senses]))\n\n    # Lowercase.\n    signature = set(s.lower() for s in signature) if lowercase else signature\n\n    # Removes stopwords.\n    signature = set(signature).difference(EN_STOPWORDS) if remove_stopwords else signature\n\n    # Lemmatized context is preferred over stemmed context.\n    if to_lemmatize:\n        signature = [lemmatize(s) if lowercase else lemmatize(s) # Lowercasing checks here.\n                     for s in signature\n                     # We only throw away if both remove_numbers and s is a digit are true.\n                     if not (remove_numbers and s.isdigit())]\n\n    # Keep only the unique bag-of-words\n    return set(signature)"}
{"prompt": "def commatize(leafs):\n    \"\"\"\n    Accepts/turns: (Name, Name, ..., Name, Name)\n    Returns/into: (Name, Comma, Name, Comma, ..., Name, Comma, Name)\n    \"\"\"", "output": "\n    if len(leafs) == 1:\n        return leafs[0], leafs[0]\n    else:\n        return leafs[0], leafs[0], leafs[0] + len(leafs[1]), leafs[0]\n\n\ndef get_all_nodes(node_name):\n    \"\"\"\n    Returns: (Node, Node)\n    \"\"\"\n    return [node for node in nodes if node.name == node_name]\n\n\ndef get_all_nodes_by_type(type):\n    \"\"\"\n    Returns: (Node,", "solution": "\n    new_leafs = []\n    for leaf in leafs:\n        new_leafs.append(leaf)\n        new_leafs.append(Comma())\n    del new_leafs[-1]\n    return new_leafs"}
{"prompt": "def parse_entry(source, loc, tokens):\n    \"\"\"\n    Converts the tokens of an entry into an Entry instance. If no applicable\n    type is available, an UnsupportedEntryType exception is raised.\n    \"\"\"", "output": "\n    if isinstance(tokens, tuple):\n        if tokens[0] == \"name\":\n            return Entry(source, tokens[1], loc)\n        elif tokens[0] == \"type\":\n            return UnsupportedEntryType(source, loc, \"tokens[1]\")\n    else:\n        raise TypeError(\"Invalid entry token type: %r\" % type(tokens))\n\n\ndef parse_entry_list(source, loc, tokens):\n    \"\"\"\n    Converts a list of tokens into a list of Entry instances.\n    \"\"\"\n    if isinstance(tokens, tuple):\n        if tokens[0] == \"name\":\n            return [", "solution": "\n    type_ = tokens[1].lower()\n    entry_type = structures.TypeRegistry.get_type(type_)\n    if entry_type is None or not issubclass(entry_type, structures.Entry):\n        raise exceptions.UnsupportedEntryType(\n                \"%s is not a supported entry type\" % type_\n            )\n    new_entry = entry_type()\n    new_entry.name = tokens[3]\n    for key, value in [t for t in tokens[4:-1] if t != ',']:\n        new_entry[key] = value\n    return new_entry"}
{"prompt": "def _wait_until(obj, att, desired, callback, interval, attempts, verbose,\n        verbose_atts):\n    \"\"\"\n    Loops until either the desired value of the attribute is reached, or the\n    number of attempts is exceeded.\n    \"\"\"", "output": "\n    start = time.time()\n    while time.time() - start < timeout:\n        if callback:\n            callback(obj)\n        time.sleep(interval)\n        desired -= 1\n        att.wait()\n    return desired\n\n\ndef wait_until_not(predicate, interval=1, attempts=3,\n        verbose=False, verbose_atts=None):\n    \"\"\"\n    Wait until the predicate is not true.\n    \"\"\"\n    if verbose:\n        print(\"wait_until_not(predicate, interval=1, attempts=3, verbose=False, \"\n                \"verbose_atts=%s)\" % verbose_", "solution": "\n    if not isinstance(desired, (list, tuple)):\n        desired = [desired]\n    if verbose_atts is None:\n        verbose_atts = []\n    if not isinstance(verbose_atts, (list, tuple)):\n        verbose_atts = [verbose_atts]\n    infinite = (attempts == 0)\n    attempt = 0\n    start = time.time()\n    while infinite or (attempt < attempts):\n        try:\n            # For servers:\n            obj.get()\n        except AttributeError:\n            try:\n                # For other objects that don't support .get()\n                obj = obj.manager.get(obj.id)\n            except AttributeError:\n                # punt\n                raise exc.NoReloadError(\"The 'wait_until' method is not \"\n                        \"supported for '%s' objects.\" % obj.__class__)\n        attval = getattr(obj, att)\n        if verbose:\n            elapsed = time.time() - start\n            msgs = [\"Current value of %s: %s (elapsed: %4.1f seconds)\" % (\n                    att, attval, elapsed)]\n            for vatt in verbose_atts:\n                vattval = getattr(obj, vatt, None)\n                msgs.append(\"%s=%s\" % (vatt, vattval))\n            print(\" \".join(msgs))\n        if attval in desired:\n            return obj\n        time.sleep(interval)\n        attempt += 1\n    return obj"}
{"prompt": "def rlmf_dqn_tiny():\n  \"\"\"Tiny DQN params.\"\"\"", "output": "\n  return dict(\n    num_layers=8,\n    num_units=16,\n    num_heads=8,\n    head_dim=16,\n    dropout_rate=0.1,\n    attention_dropout_rate=0.1,\n    initializer_range=0.02,\n    num_encoder_layers=8,\n    num_decoder_layers=16,\n    num_heads_per_head=8,\n    dropout_rate=0.1,\n    attention_dropout_rate=0.1,\n    decoder_layers=16,\n    num_decoder_layers=8,\n    num", "solution": "\n  hparams = rlmf_original()\n  hparams = hparams.override_from_dict(rlmf_tiny_overrides())\n  hparams.batch_size = 1\n  hparams.base_algo = \"dqn\"\n  hparams.base_algo_params = \"dqn_original_params\"\n  hparams.add_hparam(\"dqn_num_frames\", 128)\n  hparams.add_hparam(\"dqn_save_every_steps\", 128)\n  hparams.add_hparam(\"dqn_replay_buffer_replay_capacity\", 100)\n  hparams.add_hparam(\"dqn_agent_min_replay_history\", 10)\n  return hparams"}
{"prompt": "def starred(self, **args):\n\t\t\"\"\"\n\t\tList the authenticated user's starred gists\n\t\t\"\"\"", "output": "\n\t\treturn self.get_gists(user_id=self.user_id, **args)\n\n\tdef starred_by_id(self, id):\n\t\t\"\"\"\n\t\tList the authenticated user's starred gists by ID\n\t\t\"\"\"\n\t\treturn self.get_gists(user_id=self.user_id, id=id, **args)\n\n\tdef unstarred(self, **args):\n\t\t\"\"\"\n\t\tList the authenticated user's starred gists\n\t\t\"\"\"\n\t\treturn self.get_gists(user_id=self.", "solution": "\n\t\tids =[]\n\t\tr = requests.get(\n\t\t\t'%s/gists/starred'%BASE_URL,\n\t\t\theaders=self.gist.header\n\t\t\t)\n\n\t\tif 'limit' in args:\n\t\t\tlimit = args['limit']\n\t\telse:\n\t\t\tlimit = len(r.json())\n\n\t\tif (r.status_code == 200):\n\t\t\tfor g in range(0,limit ):\n\t\t\t\tids.append('%s/%s/%s' %(GIST_URL,r.json()[g]['user']['login'],r.json()[g]['id']))\n\t\t\treturn ids\n\n\t\traise Exception('Username not found')"}
{"prompt": "def get_queryset(self):\n        \"\"\"\n        Filter the `QuerySet` based on its parents as defined in the\n        `serializer_class.parent_lookup_kwargs`.\n        \"\"\"", "output": "\n        return self.get_parents()\n\n    def get_queryset_from_path(self, path):\n        \"\"\"\n        Return a `QuerySet` containing the data for the given path.\n        \"\"\"\n        # `filter_queryset` returns a `QuerySet` of one or more items, whilst\n        # are extra arguments that need to be passed to `get_parents`.\n        if path.endswith('filter'):\n            data = File.objects.filter(path__in=path)\n        elif path.endswith('filter_field'):\n            data = File.objects.filter(path__in=path)\n        elif path", "solution": "\n        queryset = super(NestedViewSetMixin, self).get_queryset()\n        if hasattr(self.serializer_class, 'parent_lookup_kwargs'):\n            orm_filters = {}\n            for query_param, field_name in self.serializer_class.parent_lookup_kwargs.items():\n                orm_filters[field_name] = self.kwargs[query_param]\n            return queryset.filter(**orm_filters)\n        return queryset"}
{"prompt": "def cleanup(self):\n        \"\"\"cleanup configuration: stop and remove all servers\"\"\"", "output": "\n        self.stop()\n        self.remove_server(self.servers[0])\n\n    def set_server_stop_command(self, stop_command):\n        \"\"\"set command: stop\"\"\"\n        self.stop_command = stop_command\n\n    def set_server_remove_command(self, remove_command):\n        \"\"\"remove command: remove\"\"\"\n        self.remove_command = remove_command\n\n    def set_server_remove_all_commands(self, remove_all_commands):\n        \"\"\"remove all commands: stop and remove all servers\"\"\"\n        self.stop_all_commands = remove_all", "solution": "\n        for _id, shard in self._shards.items():\n            if shard.get('isServer', False):\n                Servers().remove(shard['_id'])\n            if shard.get('isReplicaSet', False):\n                ReplicaSets().remove(shard['_id'])\n\n        for mongos in self._routers:\n            Servers().remove(mongos)\n\n        for config_id in self._configsvrs:\n            self.configdb_singleton.remove(config_id)\n\n        self._configsvrs = []\n        self._routers = []\n        self._shards = {}"}
{"prompt": "def getChildren(self, forgetter, field=None, where=None, orderBy=None):\n        \"\"\"Return the children that links to me.\n\n        That means that I have to be listed in their _userClasses\n        somehow. If field is specified, that field in my children is\n        used as the pointer to me. Use this if you have multiple fields\n        referring to my class.\n        \"\"\"", "output": "\n        if field is None:\n            return self._userClasses.keys()\n        else:\n            return self._userClasses[field][1]\n\n    def getField(self, fieldName):\n        \"\"\"Return the field with the given name.\n\n        If fieldName is specified, it is used as the name of the field.\n        If fieldName is not specified, returns the default value.\n        \"\"\"\n        if fieldName in self._fields:\n            return self._fields[fieldName][0]\n        else:\n            return default\n\n    def getLinks(self, forgetter, field=None, where=None, orderBy=", "solution": "\n        if type(where) in (types.StringType, types.UnicodeType):\n            where = (where,)\n\n        if not field:\n            for (i_field, i_class) in forgetter._userClasses.items():\n                if isinstance(self, i_class):\n                    field = i_field\n                    break # first one found is ok :=)\n        if not field:\n            raise \"No field found, check forgetter's _userClasses\"\n        sqlname = forgetter._sqlFields[field]\n        myID = self._getID()[0] # assuming single-primary !\n\n        whereList = [\"%s='%s'\" % (sqlname, myID)]\n        if where:\n            whereList.extend(where)\n        return forgetter.getAll(whereList, orderBy=orderBy)"}
{"prompt": "def stop_actors(self, monitor):\n        \"\"\"Maintain the number of workers by spawning or killing as required\n        \"\"\"", "output": "\n        if self.config.get('stop_actors', 0) > 0:\n            monitor.info('Stopping %s out of %s', self.config['stop_actors'], self.config['max_workers'])\n            self.shutdown(monitor)\n\n    def stop_all(self):\n        \"\"\"Stop all the actors\n        \"\"\"\n        if self.config.get('stop_actors', 0) == 0:\n            return\n        monitor = self.get_monitor()\n        for a in self.actors:\n            a.stop()\n            monitor.info('Stopping %s', a.get_name())", "solution": "\n        if monitor.cfg.workers:\n            num_to_kill = len(self.managed_actors) - monitor.cfg.workers\n            for i in range(num_to_kill, 0, -1):\n                w, kage = 0, sys.maxsize\n                for worker in self.managed_actors.values():\n                    age = worker.impl.age\n                    if age < kage:\n                        w, kage = worker, age\n                self.manage_actor(monitor, w, True)"}
{"prompt": "def process_pending_configfield_xref_nodes(app, doctree, fromdocname):\n    \"\"\"Process the ``pending_configfield_xref`` nodes during the\n    ``doctree-resolved`` event to insert links to the locations of\n    configuration field nodes.\n\n    See also\n    --------\n    `format_configfield_id`\n    `configfield_ref_role`\n    `pending_configfield_xref`\n    \"\"\"", "output": "\n    from docutils import nodes\n    from docutils.parsers.rst import directives\n\n    for node in doctree.traverse(nodes.reference):\n        if node.tagname == 'pending_configfield_xref':\n            break\n    else:\n        return\n\n    for child in node:\n        if child.tagname =='refuri':\n            for uri in child:\n                if uri.tagname =='refrole':\n                    uri.attrs['ref-role'] = 'aut'\n                    uri.attrs['ref-type'] = 'aut'\n                    uri.attrs['role'] = 'aut'\n                    uri.attrs['name'] = 'aut'", "solution": "\n    logger = getLogger(__name__)\n    env = app.builder.env\n\n    for node in doctree.traverse(pending_configfield_xref):\n        content = []\n\n        # The source is the text the user entered into the role, which is\n        # the importable name of the config class's and the attribute\n        role_parts = split_role_content(node.rawsource)\n        namespace_components = role_parts['ref'].split('.')\n        field_name = namespace_components[-1]\n        class_namespace = namespace_components[:-1]\n        configfield_id = format_configfield_id(class_namespace, field_name)\n        if role_parts['display']:\n            # user's custom display text\n            display_text = role_parts['display']\n        elif role_parts['last_component']:\n            # just the name of the class\n            display_text = role_parts['ref'].split('.')[-1]\n        else:\n            display_text = role_parts['ref']\n        link_label = nodes.literal()\n        link_label += nodes.Text(display_text, display_text)\n\n        if hasattr(env, 'lsst_configfields') \\\n                and configfield_id in env.lsst_configfields:\n            # A config field topic is available\n            configfield_data = env.lsst_configfields[configfield_id]\n\n            ref_node = nodes.reference('', '')\n            ref_node['refdocname'] = configfield_data['docname']\n            ref_node['refuri'] = app.builder.get_relative_uri(\n                fromdocname, configfield_data['docname'])\n            ref_node['refuri'] += '#' + configfield_id\n\n            ref_node += link_label\n\n            content.append(ref_node)\n\n        else:\n            # Fallback if the config field isn't known. Just print the Config\n            # field attribute name\n            literal_node = nodes.literal()\n            link_label = nodes.Text(field_name, field_name)\n            literal_node += link_label\n\n            content.append(literal_node)\n\n            message = 'lsst-config-field could not find a reference to %s'\n            logger.warning(message, role_parts['ref'], location=node)\n\n        # replacing the pending_configfield_xref node with this reference\n        node.replace_self(content)"}
{"prompt": "def enrol(self, event):\n        \"\"\"A user tries to self-enrol with the enrolment form\"\"\"", "output": "\n        self.send_message(event.conv_id, event.conv_data)\n        self.send_message(event.conv_data, \"enrolment\", \"You can't self-enrol with this conv id.\")\n        self.set_status(404)\n        self.write(\"Sorry, but you don't have permission to access that conv.\")\n\n    def get(self, conv_id):\n        \"\"\"\n        Returns the details of a single conv\n        \"\"\"\n        self.send_message(event.conv_id, event.conv_data)\n        self.write(\"Your enrolment is", "solution": "\n\n        if self.config.allow_registration is False:\n            self.log('Someone tried to register although enrolment is closed.')\n            return\n\n        self.log('Client trying to register a new account:', event, pretty=True)\n        # self.log(event.data, pretty=True)\n\n        uuid = event.client.uuid\n\n        if uuid in self.captchas and event.data.get('captcha', None) == self.captchas[uuid]['text']:\n            self.log('Captcha solved!')\n        else:\n            self.log('Captcha failed!')\n            self._fail(event, _('You did not solve the captcha correctly.', event))\n            self._generate_captcha(event)\n\n            return\n\n        mail = event.data.get('mail', None)\n        if mail is None:\n            self._fail(event, _('You have to supply all required fields.', event))\n            return\n        elif not validate_email(mail):\n            self._fail(event, _('The supplied email address seems invalid', event))\n            return\n\n        if objectmodels['user'].count({'mail': mail}) > 0:\n            self._fail(event, _('Your mail address cannot be used.', event))\n            return\n\n        password = event.data.get('password', None)\n        if password is None or len(password) < 5:\n            self._fail(event, _('Your password is not long enough.', event))\n            return\n\n        username = event.data.get('username', None)\n        if username is None or len(username) < 1:\n            self._fail(event, _('Your username is not long enough.', event))\n            return\n        elif (objectmodels['user'].count({'name': username}) > 0) or \\\n            (objectmodels['enrollment'].count({'name': username}) > 0):\n            self._fail(event, _('The username you supplied is not available.', event))\n            return\n\n        self.log('Provided data is good to enrol.')\n        if self.config.no_verify:\n            self._create_user(username, password, mail, 'Enrolled', uuid)\n        else:\n            self._invite(username, 'Enrolled', mail, uuid, event, password)"}
{"prompt": "def disable_inside(item, *elems, **kwargs):\n    \"\"\"Prevent elems from matching inside of item.\n\n    Returns (item with elem disabled, *new versions of elems).\n    \"\"\"", "output": "\n    return item.get_parent() is not None\n\n\ndef disable_inside_if(item, *elems, **kwargs):\n    \"\"\"Prevent elems from matching inside of item if item disabled.\n\n    Returns (item with elem disabled, *new versions of elems).\n    \"\"\"\n    return item.get_parent() is not None\n\n\ndef disable_inside_if_not_disabled(item, *elems, **kwargs):\n    \"\"\"Prevent elems from matching inside of item if item disabled.\n\n    Returns (item with elem disabled, *new versions of elems).\n    \"\"\"\n    return item.get_parent()", "solution": "\n    _invert = kwargs.get(\"_invert\", False)\n    internal_assert(set(kwargs.keys()) <= set((\"_invert\",)), \"excess keyword arguments passed to disable_inside\")\n\n    level = [0]  # number of wrapped items deep we are; in a list to allow modification\n\n    @contextmanager\n    def manage_item(self, instring, loc):\n        level[0] += 1\n        try:\n            yield\n        finally:\n            level[0] -= 1\n\n    yield Wrap(item, manage_item)\n\n    @contextmanager\n    def manage_elem(self, instring, loc):\n        if level[0] == 0 if not _invert else level[0] > 0:\n            yield\n        else:\n            raise ParseException(instring, loc, self.errmsg, self)\n\n    for elem in elems:\n        yield Wrap(elem, manage_elem)"}
{"prompt": "def readv(self, chunks):\n        \"\"\"\n        Read a set of blocks from the file by (offset, length).  This is more\n        efficient than doing a series of `.seek` and `.read` calls, since the\n        prefetch machinery is used to retrieve all the requested blocks at\n        once.\n\n        :param chunks:\n            a list of ``(offset, length)`` tuples indicating which sections of\n            the file to read\n        :return: a list of blocks read, in the same order as in ``chunks``\n\n        .. versionadded:: 1.5.4\n        \"\"\"", "output": "\n        return self._read_header(chunks)\n\n    def _read_header(self, chunks):\n        \"\"\"\n        Read the header of the file.\n\n        :param chunks:\n            a list of ``(offset, length)`` tuples indicating which\n            headers of the file to read\n        :return:\n            the header of the file as a string\n        \"\"\"\n        # Read the headers.\n        headers = []\n        for i, (offset, length) in enumerate(chunks):\n            if length is None:\n                length = i\n            headers.append(self._read_header_chunk(i, offset, length))\n        return ''.join(", "solution": "\n        self.sftp._log(\n            DEBUG, \"readv({}, {!r})\".format(hexlify(self.handle), chunks)\n        )\n\n        read_chunks = []\n        for offset, size in chunks:\n            # don't fetch data that's already in the prefetch buffer\n            if self._data_in_prefetch_buffers(\n                offset\n            ) or self._data_in_prefetch_requests(offset, size):\n                continue\n\n            # break up anything larger than the max read size\n            while size > 0:\n                chunk_size = min(size, self.MAX_REQUEST_SIZE)\n                read_chunks.append((offset, chunk_size))\n                offset += chunk_size\n                size -= chunk_size\n\n        self._start_prefetch(read_chunks)\n        # now we can just devolve to a bunch of read()s :)\n        for x in chunks:\n            self.seek(x[0])\n            yield self.read(x[1])"}
{"prompt": "def update(self, query, attributes, upsert=False):\n        \"\"\"\n        Updates data in the table.\n        \n        :Parameters: \n\n        - query(dict), specify the WHERE clause\n        - attributes(dict), specify the SET clause\n        - upsert: boolean. If True, then when there's no row matches the query, insert the values\n\n        :Return: Number of rows updated or inserted\n        \"\"\"", "output": "\n        if not upsert:\n            self.upsert = True\n        else:\n            self.upsert = False\n\n        return self.upsert_count(query, attributes)\n\n    def upsert_count(self, query, attributes):\n        \"\"\"\n        Performs a bulk insert of the given data.\n        \n        :Parameters: \n\n        - query(dict), specify the WHERE clause\n        - attributes(dict), specify the SET clause\n        \"\"\"\n        if not upsert:\n            self.log.info(\"upsert_count: %s\" % query)\n            cursor = self.db.cursor()\n            cursor.execute(query)\n           ", "solution": "\n        if upsert:\n            found_result = self.find_one(query)\n            if not found_result:\n                id = self.insert(attributes)\n                if id > 0:\n                    return 1\n                else:\n                    return 0\n\n        sql = build_update(self.name, query, attributes)\n        return self.cursor.execute(sql)"}
{"prompt": "def create_group_member(self, member):\n        \"\"\"\n        Create a new member trigger for a parent trigger.\n\n        :param member: Group member trigger to be created\n        :type member: GroupMemberInfo\n        :return: A member Trigger object\n        \"\"\"", "output": "\n        return self.parent_triggers[member.id]\n\n    def get_group_member_by_name(self, name):\n        \"\"\"\n        Get a group trigger for a trigger by its name.\n\n        :param name: The name of the trigger to be created\n        :type name: str\n        :return: GroupMemberInfo object\n        \"\"\"\n        return GroupMemberInfo(self.parent_trigger_id, name)\n\n    def get_group_member_by_id(self, trigger_id):\n        \"\"\"\n        Get a group trigger for a trigger by its ID.\n\n        :param trigger_id: The ID", "solution": "\n        data = self._serialize_object(member)\n        return Trigger(self._post(self._service_url(['triggers', 'groups', 'members']), data))"}
{"prompt": "def _update_brokers(self, brokers, remove=False):\n        \"\"\"\n        Update `self._brokers` and `self.clients`\n\n        Update our self.clients based on brokers in received metadata\n        Take the received dict of brokers and reconcile it with our current\n        list of brokers (self.clients). If there is a new one, bring up a new\n        connection to it, and if remove is True, and any in our current list\n        aren't in the metadata returned, disconnect from it.\n\n        :param brokers: Iterable of `BrokerMetadata`. A client will be created\n            for every broker given if it doesn't yet exist.\n        :param bool remove:\n            Is this metadata for *all* brokers? If so, clients for brokers\n            which are no longer found in the metadata will be closed.\n        \"\"\"", "output": "` instances\n        :type brokers: Iterable of `Broker`\n        :param remove: True if we should remove our metadata\n        :type remove: bool\n        \"\"\"\n        # We need to iterate over all brokers, and re-reconcile our current\n        # list of brokers.\n        for broker in brokers:\n            if broker.client_id in self._clients:\n                broker._brokers = brokers\n                self._clients.remove(broker.client_id)\n            else:\n                self._brokers.append(broker)\n\n        # We need to iterate over all clients, and", "solution": "\n        log.debug(\"%r: _update_brokers(%r, remove=%r)\",\n                  self, brokers, remove)\n        brokers_by_id = {bm.node_id: bm for bm in brokers}\n        self._brokers.update(brokers_by_id)\n\n        # Update the metadata of broker clients that already exist.\n        for node_id, broker_meta in brokers_by_id.items():\n            if node_id not in self.clients:\n                continue\n            self.clients[node_id].updateMetadata(broker_meta)\n\n        # Remove any clients for brokers which no longer exist.\n        if remove:\n            to_close = [\n                self.clients.pop(node_id)\n                for node_id in set(self.clients) - set(brokers_by_id)\n            ]\n\n            if to_close:\n                self._close_brokerclients(to_close)"}
{"prompt": "def _textwrap_slices(text, width, strip_leading_indent=False):\n    \"\"\" Nearly identical to textwrap.wrap except this routine is a tad bit\n    safer in its algo that textwrap.  I ran into some issues with textwrap\n    output that make it unusable to this usecase as a baseline text wrapper.\n    Further this utility returns slices instead of strings.  So the slices\n    can be used to extract your lines manually. \"\"\"", "output": "\n    lines = []\n    for line in text.splitlines():\n        if strip_leading_indent:\n            line = line.strip()\n        lines.append(line)\n    return \"\\n\".join(lines)\n\ndef _textwrap_lines(text, width):\n    \"\"\" Same as textwrap, but allows overriding in subclasses. \"\"\"\n    return textwrap(text, width)\n\ndef _format_code(code):\n    \"\"\" Return the Python code-like string for a given piece of code. \"\"\"\n    if isinstance(code, str):\n        return code\n    elif isinstance(code, CodeType):\n        return", "solution": "\n    if not isinstance(text, str):\n        raise TypeError(\"Expected `str` type\")\n    chunks = (x for x in _textwrap_word_break.split(text) if x)\n    remaining = width\n    buf = []\n    lines = [buf]\n    whitespace = []\n    whitespace_len = 0\n    pos = 0\n    try:\n        chunk = next(chunks)\n    except StopIteration:\n        chunk = ''\n    if not strip_leading_indent and is_whitespace(chunk):\n        # Add leading indent for first line, but only up to one lines worth.\n        chunk_len = len(chunk)\n        if chunk_len >= width:\n            _add_slice(buf, slice(0, width))\n            buf = []\n            lines.append(buf)\n        else:\n            _add_slice(buf, slice(0, chunk_len))\n            remaining -= chunk_len\n        pos = chunk_len\n        try:\n            chunk = next(chunks)\n        except StopIteration:\n            chunk = ''\n    while True:\n        avail_len = remaining - whitespace_len\n        chunk_len = len(chunk)\n        if chunk == '\\n':\n            buf = []\n            lines.append(buf)\n            whitespace = []\n            whitespace_len = 0\n            remaining = width\n        elif is_whitespace(chunk):\n            if buf:\n                _add_slice(whitespace, slice(pos, pos + chunk_len))\n                whitespace_len += chunk_len\n        elif len(chunk) > avail_len:\n            if not buf:\n                # Must hard split the chunk.\n                for x in whitespace:\n                    _add_slice(buf, x)\n                _add_slice(buf, slice(pos, pos + avail_len))\n                chunk = chunk[avail_len:]\n                pos += avail_len\n            # Bump to next line without fetching the next chunk.\n            buf = []\n            lines.append(buf)\n            whitespace = []\n            whitespace_len = 0\n            remaining = width\n            continue\n        else:\n            if buf:\n                remaining -= whitespace_len\n                for x in whitespace:\n                    _add_slice(buf, x)\n            whitespace = []\n            whitespace_len = 0\n            _add_slice(buf, slice(pos, pos + chunk_len))\n            remaining -= chunk_len\n        pos += chunk_len\n        try:\n            chunk = next(chunks)\n        except StopIteration:\n            break\n    return lines"}
{"prompt": "def run_airbnb_demo(data_dir):\n    \"\"\"HyperTransfomer will transform back and forth data airbnb data.\"\"\"", "output": "\n    # Load the data\n    data = np.load(os.path.join(data_dir, 'airbnb_data.npy'))\n    # Create the data_dict\n    data_dict = {}\n    # Create the data_dict for the test\n    for key in data.keys():\n        data_dict[key] = np.load(os.path.join(data_dir, 'airbnb_data_dict.npy'))\n    # Create the data_dict for the test\n    for key in data_dict.keys():\n        data_dict[key] = np.load(os", "solution": "\n\n    # Setup\n    meta_file = os.path.join(data_dir, 'Airbnb_demo_meta.json')\n    transformer_list = ['NumberTransformer', 'DTTransformer', 'CatTransformer']\n    ht = HyperTransformer(meta_file)\n\n    # Run\n    transformed = ht.fit_transform(transformer_list=transformer_list)\n    result = ht.reverse_transform(tables=transformed)\n\n    # Check\n    assert result.keys() == ht.table_dict.keys()\n\n    for name, table in result.items():\n        assert not result[name].isnull().all().all()"}
{"prompt": "def setEditorData(self, spinBox, index):\n        \"\"\"Sets the data to be displayed and edited by the editor from the data model item specified by the model index.\n\n        Args:\n            spinBox (BigIntSpinbox): editor widget.\n            index (QModelIndex): model data index.\n        \"\"\"", "output": "\n        if index.column() == 0:\n            return\n\n        if spinBox.value() == 0:\n            self.editor.setText(self.model().data(index).toString())\n        else:\n            self.editor.setText(self.model().data(index).toString(spinBox.value()))\n\n    def setModelData(self, index, modelData):\n        \"\"\"Sets the data to be displayed and edited by the model from the data model item specified by the model index.\n\n        Args:\n            spinBox (QModelIndex): model widget.\n            index (QModelIndex): model data index.\n            modelData", "solution": "\n        if index.isValid():\n            value = index.model().data(index, QtCore.Qt.EditRole)\n            spinBox.setValue(value)"}
{"prompt": "def post(self, **kwargs):\n        \"\"\"\n        https://docs.cachethq.io/docs/metrics\n        \"\"\"", "output": "\n        # TODO: remove this when we have a better way to do this\n        # if self.request.user.is_authenticated():\n        #     return self.create_response(self.get_metrics())\n        #     # TODO: remove this when we have a better way to do this\n        #     # if self.request.user.is_authenticated():\n        #     #     return self.create_response(self.get_metrics())\n        #     return self.create_response(self.get_metrics())\n        #     # TODO: remove this when we have a better way to do this\n       ", "solution": "\n        # default values\n        kwargs.setdefault('default_value', kwargs.get('default_value', 0))\n\n        required_args = ['name', 'suffix', 'description', 'default_value']\n        check_required_args(required_args, kwargs)\n\n        return self._post('metrics', data=kwargs)"}
{"prompt": "def make_repr_node(rawtext, app, prefixed_name, obj, parent, modname, options):\n    \"\"\"Render a Python object to text using the repr() function.\n\n    :param rawtext: Text being replaced with link node.\n    :param app: Sphinx application context\n    :param prefixed_name: The dotted Python name for obj.\n    :param obj: The Python object to be rendered to text.\n    :param parent: The parent Python object of obj.\n    :param module: The name of the module containing obj.\n    :param options: Options dictionary passed to role func.\n    \"\"\"", "output": "\n    if not isinstance(obj, basestring):\n        raise SphinxError(\"obj must be a string\")\n    if app.config.napoleon_use_ivar:\n        return _format_ivar(rawtext, obj)\n    if not app.config.napoleon_use_param:\n        return _format_param(rawtext, obj)\n    if not app.config.napoleon_use_app:\n        return _format_app(rawtext, obj)\n    if not app.config.napoleon_use_envvar:\n        return _format_envvar(rawtext, obj", "solution": "\n    text = repr(obj)\n    node = nodes.Text(text, rawsource=rawtext)\n    return node"}
{"prompt": "def analyze_lib(lib_dir, cover_filename, *, ignore_existing=False):\n  \"\"\" Recursively analyze library, and return a dict of path -> (artist, album). \"\"\"", "output": "\n  if not os.path.isdir(lib_dir):\n    return None\n\n  # Get the list of files in the library\n  files = sorted(os.listdir(lib_dir))\n  artists = set()\n  albums = set()\n\n  # Get the artist and album for the given filename\n  for filename in files:\n    artists.add(os.path.splitext(os.path.basename(filename))[0])\n    artists.add(filename)\n    albums.add(filename)\n\n  # Get the path to the directory containing the artist and album\n  if ignore_existing:\n    for filename", "solution": "\n  work = {}\n  stats = collections.OrderedDict(((k, 0) for k in(\"files\", \"albums\", \"missing covers\", \"errors\")))\n  with tqdm.tqdm(desc=\"Analyzing library\",\n                 unit=\"dir\",\n                 postfix=stats) as progress, \\\n          tqdm_logging.redirect_logging(progress):\n    for rootpath, rel_dirpaths, rel_filepaths in os.walk(lib_dir):\n      metadata = analyze_dir(stats,\n                             rootpath,\n                             rel_filepaths,\n                             cover_filename,\n                             ignore_existing=ignore_existing)\n      progress.set_postfix(stats, refresh=False)\n      progress.update(1)\n      if all(metadata[:-1]):\n        work[rootpath] = metadata[:-1]\n  return work"}
{"prompt": "def delete_params_s(s, params):\n        \"\"\"\n        Delete the given parameters from a string\n\n        Same as :meth:`delete_params` but does not use the :attr:`params`\n        dictionary\n\n        Parameters\n        ----------\n        s: str\n            The string of the parameters section\n        params: list of str\n            The names of the parameters to delete\n\n        Returns\n        -------\n        str\n            The modified string `s` without the descriptions of `params`\n        \"\"\"", "output": "\n        return self.delete_params(params, s)\n\n    def delete_params(self, params, s):\n        \"\"\"\n        Delete the given parameters from a string\n\n        Same as :meth:`delete_params_s` but does not use the :attr:`s`\n        dictionary\n\n        Parameters\n        ----------\n        params: list of str\n            The names of the parameters to delete\n        s: str\n            The string of the parameters section\n\n        Returns\n        -------\n        str\n            The modified string `s` without the descriptions of `params`\n        \"\"\"\n        if len(params) == 0:\n            return s\n        if not isinstance(params[", "solution": "\n        patt = '(?s)' + '|'.join(\n            '(?<=\\n)' + s + '\\s*:.+?\\n(?=\\S+|$)' for s in params)\n        return re.sub(patt, '', '\\n' + s.strip() + '\\n').strip()"}
{"prompt": "def log_url (self, url_data):\n        \"\"\"Write one node and all possible edges.\"\"\"", "output": "\n        self.log.info (\"[%s] %s\" % (self.name, url_data))\n        self.log.info (\"[%s] %s\" % (self.name, url_data.get_url()))\n        self.log.info (\"[%s] %s\" % (self.name, url_data.get_url(with_port=True)))\n        self.log.info (\"[%s] %s\" % (self.name, url_data.get_url(with_port=True, with_ssl=True)))\n        self.log.info", "solution": "\n        node = self.get_node(url_data)\n        if node:\n            self.xml_starttag(u'node', attrs={u\"name\": u\"%d\" % node[\"id\"]})\n            self.xml_tag(u\"label\", node[\"label\"])\n            if self.has_part(\"realurl\"):\n                self.xml_tag(u\"url\", node[\"url\"])\n            self.xml_starttag(u\"data\")\n            if node[\"dltime\"] >= 0 and self.has_part(\"dltime\"):\n                self.xml_tag(u\"dltime\", u\"%f\" % node[\"dltime\"])\n            if node[\"size\"] >= 0 and self.has_part(\"dlsize\"):\n                self.xml_tag(u\"size\", u\"%d\" % node[\"size\"])\n            if node[\"checktime\"] and self.has_part(\"checktime\"):\n                self.xml_tag(u\"checktime\", u\"%f\" % node[\"checktime\"])\n            if self.has_part(\"extern\"):\n                self.xml_tag(u\"extern\", u\"%d\" % node[\"extern\"])\n            self.xml_endtag(u\"data\")\n            self.xml_endtag(u\"node\")"}
{"prompt": "def _filter_in(self, term_list, field_name, field_type, is_not):\n        \"\"\"\n        Returns a query that matches exactly ANY term in term_list.\n\n        Notice that:\n         A in {B,C} <=> (A = B or A = C)\n         ~(A in {B,C}) <=> ~(A = B or A = C)\n        Because OP_AND_NOT(C, D) <=> (C and ~D), then D=(A in {B,C}) requires `is_not=False`.\n\n        Assumes term is a list.\n        \"\"\"", "output": " a list of terms.\n        \"\"\"\n        if not isinstance(term_list, list):\n            raise TypeError(\"'term_list' must be a list of terms\")\n        for term in term_list:\n            if term.get(field_name):\n                if not is_not:\n                    return self._filter_in(term, field_type, is_not)\n                else:\n                    return self._filter_in(term, field_name, field_type)\n        return self._filter_in(term_list, field_type, is_not=is_not)\n\n    def _filter_not(", "solution": "\n        query_list = [self._filter_exact(term, field_name, field_type, is_not=False)\n                      for term in term_list]\n\n        if is_not:\n            return xapian.Query(xapian.Query.OP_AND_NOT, self._all_query(),\n                                xapian.Query(xapian.Query.OP_OR, query_list))\n        else:\n            return xapian.Query(xapian.Query.OP_OR, query_list)"}
{"prompt": "def get_bug_simple(self, bugid):\n        \"\"\"\n        Get a single bug object. Similar to python-bugzilla's getbugsimple().\n\n        param bugid: integer, a bug's number.\n        returns: deferred that when fired returns an AttrDict representing this\n                 bug.\n        \"\"\"", "output": "\n        return self.getbugsimple(bugid)\n\n    def get_bug_from_bugzilla(self, bug_id):\n        \"\"\"\n        Get a single bug object. Similar to python-bugzilla's getbugfrombugzilla().\n\n        param bug_id: integer, a bug's number.\n        returns: deferred that when fired returns an AttrDict representing this\n                 bug.\n        \"\"\"\n        return self.getbugfrombugzilla(bug_id)\n\n    def get_bug_from_bugzilla_bug_id(self, bug_id):\n        \"\"\"\n        Get a single bug object. Similar to python-bugzilla's getbug", "solution": "\n        payload = {'ids': bugid}\n        d = self.call('Bug.get', payload)\n        d.addCallback(self._parse_bug_callback)\n        return d"}
{"prompt": "def continuous_eval(self):\n    \"\"\"Evaluate until checkpoints stop being produced.\"\"\"", "output": "\n    while True:\n      self.evaluate(self.iterator.get_next())\n      if self.has_next():\n        break\n\n  def has_next(self):\n    \"\"\"Returns whether the iterator has more elements.\"\"\"\n    return self.has_next_iter()\n\n  def next(self):\n    \"\"\"Returns the next element in the iterator.\"\"\"\n    return self.iterator.get_next()\n\n  def has_next_iter(self):\n    \"\"\"Returns whether the iterator has more elements.\"\"\"\n    return self.has_next_list()\n\n  def next_list(self):\n    \"\"\"Returns the next list of elements in", "solution": "\n    for ckpt_path in next_checkpoint(self._hparams.model_dir,\n                                     self._hparams.eval_timeout_mins):\n      # Skip zero'th step.\n      train_step = decoding.get_step_from_ckpt_path(ckpt_path)\n      if train_step == 0:\n        tf.logging.info(\"Skipping evaluation at step 0\")\n        continue\n      self.evaluate()"}
{"prompt": "def to_nullable_boolean(value):\n        \"\"\"\n        Converts value into boolean or returns None when conversion is not possible.\n\n        :param value: the value to convert.\n\n        :return: boolean value or None when convertion is not supported.\n        \"\"\"", "output": "\n        return to_boolean(value)\n\n    @staticmethod\n    def to_boolean(value):\n        \"\"\"\n        Converts value into boolean.\n\n        :param value: the value to convert.\n\n        :return: boolean value or None when convertion is not supported.\n        \"\"\"\n        return to_boolean(value)\n\n    @staticmethod\n    def to_nullable_integer(value):\n        \"\"\"\n        Converts value into integer or returns None when conversion is not possible.\n\n        :param value: the value to convert.\n\n        :return: integer value or None when convertion is not supported.\n        \"\"\"\n        return to_integer(value)\n\n    @", "solution": "\n        # Shortcuts\n        if value == None:\n            return None\n        if type(value) == type(True):\n            return value\n\n        str_value = str(value).lower()\n        # All true values\n        if str_value in ['1', 'true', 't', 'yes', 'y']:\n            return True\n        # All false values\n        if str_value in ['0', 'frue', 'f', 'no', 'n']:\n            return False\n\n        # Everything else:\n        return None"}
{"prompt": "def remove(self):\r\n        \r\n        \"\"\" \r\n            a method to remove all records in the collection\r\n\r\n        NOTE:   this method removes all the files in the collection, but the\r\n                collection folder itself created by oauth2 cannot be removed.\r\n                only the user can remove the app folder\r\n                \r\n        :return: string with confirmation of deletion\r\n        \"\"\"", "output": "\r\n        return self.delete_file(self.folder_id)\r\n\r\n    def delete_file(self, file_id):\r\n        \"\"\" \r\n        Delete a file from the oauth2 collection\r\n\r\n        :param file_id: string with file_id\r\n        :return: void\r\n        \"\"\"\r\n        return self.delete_folder(self.folder_id, file_id)\r\n\r\n    def get_file_id(self, file_name):\r\n        \"\"\" \r\n        Get the file id of a file\r\n\r\n        :param file_name: string with file_name\r\n        :return: string with file_id\r\n        \"\"\"\r\n        return self.folder_id", "solution": "\r\n\r\n        title = '%s.remove' % self.__class__.__name__\r\n    \r\n    # get contents in root\r\n        try:\r\n            response = self.dropbox.files_list_folder(path='')\r\n        except:\r\n            raise DropboxConnectionError(title)\r\n\r\n    # populate delete list\r\n        delete_list = []\r\n        for file in response.entries:\r\n            delete_list.append(self.objects.DeleteArg(path=file.path_display))\r\n\r\n    # continue retrieval if folder is large\r\n        if response.has_more:\r\n            try:\r\n                while response.has_more:\r\n                    response = self.dropbox.files_list_folder_continue(response.cursor)\r\n                    for file in response.entries:\r\n                        delete_list.append(self.objects.DeleteArg(path=file.path_display))\r\n            except:\r\n                raise DropboxConnectionError(title)\r\n\r\n    # send batch delete request\r\n        try:\r\n            self.dropbox.files_delete_batch(delete_list)\r\n        except:\r\n            raise DropboxConnectionError(title)\r\n    \r\n    # return outcome\r\n        insert = 'collection'\r\n        if self.collection_name:\r\n            insert = self.collection_name\r\n        exit_msg = 'Contents of %s will been removed from Dropbox.' % insert\r\n        return exit_msg"}
{"prompt": "def escape(cls, string):\n        \"\"\"\n        Utility method to produce an escaped version of a given\n        string.\n\n        :param string: The string to escape.\n\n        :returns: The escaped version of the string.\n        \"\"\"", "output": "\n        return cls.escape_string(string)\n\n    @classmethod\n    def escape_string(cls, string):\n        \"\"\"\n        Utility method to produce an escaped version of a given\n        string.\n\n        :param string: The string to escape.\n\n        :returns: The escaped version of the string.\n        \"\"\"\n        return cls.escape_string(string)\n\n    @classmethod\n    def escape_string_for_regex(cls, string):\n        \"\"\"\n        Utility method to produce an escaped version of a given\n        string.\n\n        :param string: The string to escape.\n\n        :returns: The escaped version of the string.\n        \"\"\"", "solution": "\n\n        return ''.join([cls._escapes[c] if cls._needescape(c) else c\n                        for c in string.encode('utf8')])"}
{"prompt": "def create_domain(provider, context, **kwargs):\n    \"\"\"Create a domain within route53.\n\n    Args:\n        provider (:class:`stacker.providers.base.BaseProvider`): provider\n            instance\n        context (:class:`stacker.context.Context`): context instance\n\n    Returns: boolean for whether or not the hook succeeded.\n\n    \"\"\"", "output": "\n    if not context.is_admin:\n        return False\n    if not context.is_admin.is_authenticated():\n        return False\n    if not kwargs.get('name'):\n        return False\n    if not kwargs.get('address'):\n        return False\n    if not kwargs.get('port'):\n        return False\n    if not kwargs.get('protocol'):\n        return False\n    if not kwargs.get('protocol').is_authenticated():\n        return False\n    if not kwargs.get('port').is_authenticated():\n        return False\n    if not kwargs.get('protocol').is_admin.is_authenticated():\n        return False", "solution": "\n    session = get_session(provider.region)\n    client = session.client(\"route53\")\n    domain = kwargs.get(\"domain\")\n    if not domain:\n        logger.error(\"domain argument or BaseDomain variable not provided.\")\n        return False\n    zone_id = create_route53_zone(client, domain)\n    return {\"domain\": domain, \"zone_id\": zone_id}"}
{"prompt": "def publish(self, topic, *args, **kwargs):\n        \"\"\"\n        Publishes the request to the bridge service\n        :param topic: The topic to publish to\n        :param args: The arguments\n        :param kwargs: The key/word arguments\n        :return: The ID of the publish\n        \"\"\"", "output": "\n        return self.client.publish(topic, *args, **kwargs)\n\n    def get_config(self, *args, **kwargs):\n        \"\"\"\n        Gets the configuration for the given request\n        :param args: The arguments\n        :param kwargs: The key/word arguments\n        :return: The configuration for the given request\n        \"\"\"\n        return self.client.get_config(*args, **kwargs)\n\n    def get_config_status(self, *args, **kwargs):\n        \"\"\"\n        Gets the status of the given configuration\n        :param kwargs: The key/word arguments\n        :return: The status of the", "solution": "\n        assert topic is not None\n\n        params = {\n            \"topic\": topic,\n            \"args\": args,\n            \"kwargs\": kwargs\n        }\n\n        response = self._make_api_call(\"POST\", self.url, json_params=params)\n        return response[\"id\"]"}
{"prompt": "def on_result(self, task, result):\n        \"\"\"Called every result\"\"\"", "output": "\n        self.result = result\n        self.finish()\n\n    def on_error(self, task, error):\n        \"\"\"Called when an error occurs\"\"\"\n        self.error = error\n        self.finish()\n\n    def on_finish(self):\n        \"\"\"Called when the task is finished\"\"\"\n        self.finish_args = self.finish_kwargs = None\n        self.finish()\n\n    def on_failure(self, task, error):\n        \"\"\"Called when an error occurs\"\"\"\n        self.error = error\n        self.finish_args = self.finish_kwargs = None\n        self.finish()\n\n    def on_success", "solution": "\n        if not result:\n            return\n        if 'taskid' in task and 'project' in task and 'url' in task:\n            logger.info('result %s:%s %s -> %.30r' % (\n                task['project'], task['taskid'], task['url'], result))\n            return self.resultdb.save(\n                project=task['project'],\n                taskid=task['taskid'],\n                url=task['url'],\n                result=result\n            )\n        else:\n            logger.warning('result UNKNOW -> %.30r' % result)\n            return"}
{"prompt": "def new(self):\n        # type: () -> None\n        \"\"\"\n        A method to create a new UDF Logical Volume Header Descriptor.\n\n        Parameters:\n         None.\n        Returns:\n         Nothing.\n        \"\"\"", "output": "\n        self.header_descriptor = None\n        return self\n\n    def __str__(self):\n        # type: () -> str\n        \"\"\"\n        Returns the UDF Logical Volume Descriptor as a string.\n\n        Returns the string representation of the UDF Logical Volume Descriptor.\n\n        Returns the length of the descriptor in bytes.\n\n        Returns the descriptor's length.\n\n        Returns the descriptor's name.\n\n        Returns the descriptor's type.\n\n        Returns the descriptor's type.\n\n        Returns the descriptor's type.\n\n        Returns the descriptor's type.\n\n        Returns the descriptor's type.\n\n        Returns the descriptor's type.\n\n        Returns the descriptor's type.\n\n        Returns the", "solution": "\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Logical Volume Header Descriptor already initialized')\n\n        self.unique_id = 261\n\n        self._initialized = True"}
{"prompt": "def topic_content(self, W, output_file=\"topic_description.csv\"):\n\n        \"\"\"\n        Print top W words in each topic to file.\n        \"\"\"", "output": "\n\n        print \"Top words in topic:\", W.shape\n        print \"Number of words:\", self.num_words\n        print \"Number of words with a topic:\", W.sum()\n        print \"Number of words with a topic with a score of:\", W.sum()\n        print \"Number of words with a score of:\", W.sum()\n        print \"Top words with a score of:\", W.sum()\n        \"\"\"\n        # Initialize the variational distribution q(theta|gamma)\n        \"\"\"\n        # Initialize the variational distribution q(theta|gamma)\n        \"\"\"\n        # Initialize the variational distribution q(theta|gamma", "solution": "\n\n        topic_top_probs = []\n        topic_top_words = []\n\n        tt = self.tt_avg(False)\n\n        for t in range(self.K):            \n            top_word_indices = list(tt[:, t].argsort()[-W:][::-1])\n            topic_top_probs.append(np.round(np.sort(tt[:, t])[-W:][::-1], 3))\n            topic_top_words.append([list(self.token_key.keys())\n                                   [list(self.token_key.values()).index(i)]\n                                   for i in top_word_indices])            \n\n        with codecs.open(output_file, \"w\", \"utf-8\") as f:\n            for t in range(self.K):                \n                words = ','.join(topic_top_words[t])\n                probs = ','.join([str(i) for i in topic_top_probs[t]])\n                f.write(\"topic\" + str(t) + ',')\n                f.write(\"%s\\n\" % words)\n                f.write(\" \" + ',')\n                f.write(\"%s\\n\" % probs)"}
{"prompt": "def add_color_stop_rgb(self, offset, red, green, blue):\n        \"\"\"Same as :meth:`add_color_stop_rgba` with ``alpha=1``.\n        Kept for compatibility with pycairo.\n\n        \"\"\"", "output": "\n        if self.is_composited():\n            raise NotImplementedError(\"add_color_stop_rgb() cannot be called \"\n                                      \"with composited=True\")\n        if self.is_stop_rgba():\n            raise NotImplementedError(\"add_color_stop_rgb() cannot be called \"\n                                      \"with stop_rgba=True\")\n        self.stop_rgb(offset, red, green, blue)\n\n    def stop_rgb(self, offset, red, green, blue):\n        \"\"\"Stop the color stop.\"\"\"\n        if self.is_stop_rgba():\n            raise NotImplementedError(\"stop_rgb() cannot be called \"", "solution": "\n        cairo.cairo_pattern_add_color_stop_rgb(\n            self._pointer, offset, red, green, blue)\n        self._check_status()"}
{"prompt": "def getTauLambert(n):\n    \"\"\"Returns a list containing first n digits of Pi\n    \"\"\"", "output": "\n    if n < 0:\n        return []\n    return [int(d) for d in str(n).split('-')]\n\n\ndef getTauLambertDigits(n):\n    \"\"\"Returns a list containing first n digits of Pi\n    \"\"\"\n    if n < 0:\n        return []\n    return [int(d) for d in str(n).split('-')]\n\n\ndef getTauLambert(n):\n    \"\"\"Returns a list containing first n digits of Pi\n    \"\"\"\n    if n < 0:\n        return []\n    return [int(d) for d in str(n).split('-", "solution": "\n    myTau = tauGenLambert()\n    result = []\n    if n > 0:\n        result += [next(myTau) for i in range(n)]\n    myTau.close()\n    return result"}
{"prompt": "def disable_plugin(self, plugin_name):\n        \"\"\"Disable a plugin (that is, create a DISABLED empty file) and restart the application to take effect\"\"\"", "output": "\n        self.logger.debug(\"Disabling %s\", plugin_name)\n        self.logger.debug(\"Restarting %s\", plugin_name)\n        self.logger.debug(\"Removing %s\", plugin_name + \".bak\")\n        self.remove_plugin(plugin_name)\n\n    def enable_plugin(self, plugin_name):\n        \"\"\"Enable a plugin (that is, create a ENABLED empty file) and restart the application to take effect\"\"\"\n        self.logger.debug(\"Enabling %s\", plugin_name)\n        self.logger.debug(\"Removing %s\", plugin_name +", "solution": "\n        plugin = self.get_plugin_info(plugin_name)\n        ENABLED = os.path.join(plugin[\"plugin_package_abspath\"], \"ENABLED\")\n        DISABLED = os.path.join(plugin[\"plugin_package_abspath\"], \"DISABLED\")\n        if os.path.isfile(ENABLED):\n            os.remove(ENABLED)\n        self.__touch_file(DISABLED)"}
{"prompt": "def startDrag(self, dragData):\r\n        \"\"\"\r\n        Starts a new drag with the inputed data.\r\n        \r\n        :param      dragData | <dict>\r\n        \"\"\"", "output": "\r\n        self.dragging = True\r\n        self.dragData = dragData\r\n        self.dragData.scene().removeItem(self)\r\n        self.dragData.scene().addToGroup(self)\r\n        self.dragging = False\r\n    \r\n    def endDrag(self):\r\n        \"\"\"\r\n        Closes the dragging state.\r\n        \"\"\"\r\n        self.dragging = False\r\n        self.dragData.scene().removeItem(self.dragData)\r\n        self.dragData.scene().addToGroup(self)\r\n        self.dragData.scene().removeItem(self.dragData)\r\n        self.dragData.scene", "solution": "\r\n        # create the mime data\r\n        mimeData = QMimeData()\r\n        for key, value in dragData.items():\r\n            mimeData.setData('application/x-%s' % key, wrapVariant(value))\r\n        \r\n        # create the drag instance\r\n        drag = QDrag(self.scene().chart())\r\n        drag.setMimeData(mimeData)\r\n        drag.exec_()"}
{"prompt": "def to(self, unit):\n        \"\"\"\n        convert between units.  Returns a new nparray object with the new units\n        \"\"\"", "output": "\n        return self.copy()\n\n    def to_string(self, unit):\n        \"\"\"\n        convert between units.  Returns a string with the new units\n        \"\"\"\n        return self.to(unit).to_string(unit)\n\n    def to_xml(self, unit):\n        \"\"\"\n        convert between units.  Returns the xml string for the nparray\n        \"\"\"\n        return self.to(unit).to_xml(unit)\n\n    def to_file(self, filename):\n        \"\"\"\n        convert between units.  Returns the path to the file\n        \"\"\"\n        return self.copy()\n\n    def to_string(", "solution": "\n        if not _has_astropy:\n            raise ImportError(\"astropy must be installed for unit/quantity support\")\n\n        if self.unit is None:\n            raise ValueError(\"no units currently set\")\n\n        if not is_unit_or_unitstring(unit)[0]:\n            raise ValueError(\"unit not recognized\")\n\n        mult_factor = self.unit.to(unit)\n        copy = self.copy() * mult_factor\n        copy.unit = unit\n        return copy"}
{"prompt": "def restore_state(scan_codes):\n    \"\"\"\n    Given a list of scan_codes ensures these keys, and only these keys, are\n    pressed. Pairs well with `stash_state`, alternative to `restore_modifiers`.\n    \"\"\"", "output": "\n    for scan_code in scan_codes:\n        if scan_code == \"KEY_ESCAPE\":\n            scan_code = \"KEY_ESCAPE\"\n        elif scan_code == \"KEY_UP\":\n            scan_code = \"KEY_UP\"\n        elif scan_code == \"KEY_DOWN\":\n            scan_code = \"KEY_DOWN\"\n        elif scan_code == \"KEY_UP\":\n            scan_code = \"KEY_UP\"\n        elif scan_code == \"KEY_LEFT\":\n            scan_code = \"KEY_LEFT\"\n        elif scan_code == \"KEY_RIGHT\":\n            scan", "solution": "\n    _listener.is_replaying = True\n\n    with _pressed_events_lock:\n        current = set(_pressed_events)\n    target = set(scan_codes)\n    for scan_code in current - target:\n        _os_keyboard.release(scan_code)\n    for scan_code in target - current:\n        _os_keyboard.press(scan_code)\n\n    _listener.is_replaying = False"}
{"prompt": "def import_teamocil(sconf):\n    \"\"\"Return tmuxp config from a `teamocil`_ yaml config.\n\n    .. _teamocil: https://github.com/remiprev/teamocil\n\n    Parameters\n    ----------\n    sconf : dict\n        python dict for session configuration\n\n    Notes\n    -----\n\n    Todos:\n\n    - change  'root' to a cd or start_directory\n    - width in pane -> main-pain-width\n    - with_env_var\n    - clear\n    - cmd_separator\n    \"\"\"", "output": "\n    # TODO: add support for changing the environment variable\n    #       to something more sensible\n    if 'env' in sconf:\n        env = sconf['env']\n        if'sconf' in sconf:\n            sconf = env.sconf\n            sconf = sconf.sconf\n        else:\n            env.sconf = sconf\n        if'sconf' in sconf:\n            env.sconf = env.sconf.replace(sconf.get('sconf'), '')\n        if 'with_env_var' in sconf:\n            env.with_env_var = s", "solution": "\n\n    tmuxp_config = {}\n\n    if 'session' in sconf:\n        sconf = sconf['session']\n\n    if 'name' in sconf:\n        tmuxp_config['session_name'] = sconf['name']\n    else:\n        tmuxp_config['session_name'] = None\n\n    if 'root' in sconf:\n        tmuxp_config['start_directory'] = sconf.pop('root')\n\n    tmuxp_config['windows'] = []\n\n    for w in sconf['windows']:\n\n        windowdict = {'window_name': w['name']}\n\n        if 'clear' in w:\n            windowdict['clear'] = w['clear']\n\n        if 'filters' in w:\n            if 'before' in w['filters']:\n                for b in w['filters']['before']:\n                    windowdict['shell_command_before'] = w['filters']['before']\n            if 'after' in w['filters']:\n                for b in w['filters']['after']:\n                    windowdict['shell_command_after'] = w['filters']['after']\n\n        if 'root' in w:\n            windowdict['start_directory'] = w.pop('root')\n\n        if 'splits' in w:\n            w['panes'] = w.pop('splits')\n\n        if 'panes' in w:\n            for p in w['panes']:\n                if 'cmd' in p:\n                    p['shell_command'] = p.pop('cmd')\n                if 'width' in p:\n                    # todo support for height/width\n                    p.pop('width')\n            windowdict['panes'] = w['panes']\n\n        if 'layout' in w:\n            windowdict['layout'] = w['layout']\n        tmuxp_config['windows'].append(windowdict)\n\n    return tmuxp_config"}
{"prompt": "def _set_coldboot(self, v, load=False):\n    \"\"\"\n    Setter method for coldboot, mapped from YANG variable /brocade_firmware_rpc/firmware_download/input/coldboot (empty)\n    If this variable is read-only (config: false) in the\n    source YANG file, then _set_coldboot is considered as a private\n    method. Backends looking to populate this variable should\n    do so via calling thisObj._set_coldboot() directly.\n    \"\"\"", "output": "\n    def _set_coldboot(v, load=False):\n        pass\n    v._set_attr_readonly = True\n    v._utype = YANGDynClass(\n        v,\n        base=coldboot.Coldboot,\n        is_container=\"container\",\n        yang_name=\"coldboot\",\n        parent=self,\n        path_helper=self._path_helper,\n        extmethods=self._extmethods,\n        register_paths=True,\n        is_keyval=True,\n        namespace=\"http://openconfig.net/yang/system\",\n        defining_module=\"openconfig-system", "solution": "\n    if hasattr(v, \"_utype\"):\n      v = v._utype(v)\n    try:\n      t = YANGDynClass(v,base=YANGBool, is_leaf=True, yang_name=\"coldboot\", rest_name=\"coldboot\", parent=self, choice=(u'reboot-options', u'coldboot'), path_helper=self._path_helper, extmethods=self._extmethods, register_paths=False, extensions={u'tailf-common': {u'info': u'Perform non ISSU firmware download.'}}, namespace='urn:brocade.com:mgmt:brocade-firmware', defining_module='brocade-firmware', yang_type='empty', is_config=True)\n    except (TypeError, ValueError):\n      raise ValueError({\n          'error-string': "}
{"prompt": "def use_service(bundle_context, svc_reference):\n    \"\"\"\n    Utility context to safely use a service in a \"with\" block.\n    It looks after the the given service and releases its reference when\n    exiting the context.\n\n    :param bundle_context: The calling bundle context\n    :param svc_reference: The reference of the service to use\n    :return: The requested service\n    :raise BundleException: Service not found\n    :raise TypeError: Invalid service reference\n    \"\"\"", "output": "\n    try:\n        return svc_context.get_service(bundle_context, svc_reference)\n    except BundleException:\n        raise TypeError(\"Invalid service reference\")\n\n\ndef use_bundle(bundle_reference):\n    \"\"\"\n    Utility context to safely use a bundle in a \"with\" block.\n    It looks after the given bundle and releases its reference when exiting\n    the context.\n\n    :param bundle_context: The calling bundle context\n    :param svc_reference: The reference of the bundle to use\n    :return: The requested bundle\n    :raise BundleException: Bundle not found\n    :raise TypeError: Invalid bundle reference", "solution": "\n    if svc_reference is None:\n        raise TypeError(\"Invalid ServiceReference\")\n\n    try:\n        # Give the service\n        yield bundle_context.get_service(svc_reference)\n    finally:\n        try:\n            # Release it\n            bundle_context.unget_service(svc_reference)\n        except pelix.constants.BundleException:\n            # Service might have already been unregistered\n            pass"}
{"prompt": "def zip_fit_params(data):\n    \"\"\"\n    Returns the ZIP parameters that best fit a given data set.\n\n    Args:\n        data (array): 2d array of genes x cells belonging to a given cluster\n\n    Returns:\n        L (array): 1d array of means\n        M (array): 1d array of zero-inflation parameter\n    \"\"\"", "output": "\n    return data.copy()\n\n\ndef get_cluster_labels(data):\n    \"\"\"\n    Returns the labels of the given data set.\n\n    Args:\n        data (array): 2d array of genes x cells belonging to a given cluster\n\n    Returns:\n        labels (array): 2d array of labels\n    \"\"\"\n    return data.copy()\n\n\ndef get_cluster_genes(data):\n    \"\"\"\n    Returns the genes of the given data set.\n\n    Args:\n        data (array): 2d array of genes x cells belonging to a given cluster\n\n    Returns:\n        genes (array): 2d array", "solution": "\n    genes, cells = data.shape\n    m = data.mean(1)\n    v = data.var(1)\n    M = (v-m)/(m**2+v-m)\n    #M = v/(v+m**2)\n    #M[np.isnan(M)] = 0.0\n    M = np.array([min(1.0, max(0.0, x)) for x in M])\n    L = m + v/m - 1.0\n    #L = (v + m**2)/m\n    L[np.isnan(L)] = 0.0\n    L = np.array([max(0.0, x) for x in L])\n    return L, M"}
{"prompt": "def password_length_needed(self) -> int:\n        \"\"\"Calculate the needed password length to satisfy the entropy number.\n\n        This is for the given character set.\n\n        \"\"\"", "output": "\n        return len(self.password)\n\n    def password_is_valid(self, password: str) -> bool:\n        \"\"\"Check if the given password is valid.\n\n        Args:\n            password: The password to check.\n\n        Returns:\n            True if the password is valid, False if invalid.\n\n        \"\"\"\n        return all(\n            self.password_length_needed(password) == self.password_length_needed(password2)\n            for password2 in self.password\n        )\n\n    def password_is_valid_with_length(self, password: str, length: int) -> bool:\n        \"\"\"Check", "solution": "\n        characters = self._get_password_characters()\n        if (\n                self.entropy_bits_req is None\n                or not characters\n        ):\n            raise ValueError(\"Can't calculate the password length needed: \"\n                             \"entropy_bits_req isn't set or the character \"\n                             \"set is empty\")\n\n        return calc_password_length_needed(\n            self.entropy_bits_req,\n            characters\n        )"}
{"prompt": "def getAsKmlGrid(self, tableName, rasterId=1, rasterIdFieldName='id', rasterFieldName='raster', documentName='default', alpha=1.0, noDataValue=0, discreet=False):\n        \"\"\"\n        Creates a KML file with each cell in the raster represented by a polygon. The result is a vector grid representation of the raster.\n        Note that pixels with values between -1 and 0 are omitted as no data values. Also note that this method only works on the first band.\n        Returns the kml document as a string.\n        \"\"\"", "output": "\n        if discreet:\n            self.discreet = True\n        else:\n            self.discreet = False\n        self.tableName = tableName\n        self.noDataValue = noDataValue\n        self.noDataFieldName = noDataFieldName\n        self.noDataValueFieldName = noDataValueFieldName\n        self.alpha = alpha\n        self.rasterId = rasterId\n        self.discreet = discreet\n        self.createKmlFile()\n        return self.tableName +'was successfully created'\n\n    def createKmlFile(self):\n        \"\"\"\n        Creates a KML file with each cell in", "solution": "\n        # Validate alpha\n        if not (alpha >= 0 and alpha <= 1.0):\n            raise ValueError(\"RASTER CONVERSION ERROR: alpha must be between 0.0 and 1.0.\")\n\n        # Get the color ramp and parameters\n        minValue, maxValue = self.getMinMaxOfRasters(session=self._session,\n                                                     table=tableName,\n                                                     rasterIds=(str(rasterId), ),\n                                                     rasterIdField=rasterIdFieldName,\n                                                     rasterField=rasterFieldName,\n                                                     noDataValue=noDataValue)\n\n        mappedColorRamp = ColorRampGenerator.mapColorRampToValues(colorRamp=self._colorRamp,\n                                                                  minValue=minValue,\n                                                                  maxValue=maxValue,\n                                                                  alpha=alpha)\n\n        # Get polygons for each cell in kml format\n        statement = "}
{"prompt": "def _evaluate(self,R,z,phi=0.,t=0.):\n        \"\"\"\n        NAME:\n           _evaluate\n        PURPOSE:\n           evaluate the potential at (R,z, phi)\n        INPUT:\n           R - Cylindrical Galactocentric radius\n           z - vertical height\n           phi - azimuth\n           t - time\n        OUTPUT:\n           potential at (R,z, phi)\n        HISTORY:\n           2016-05-17 - Written - Aladdin \n        \"\"\"", "output": "\n        R = np.atleast_1d(R)\n        z = np.atleast_1d(z)\n        phi = np.atleast_1d(phi)\n        t = np.atleast_1d(t)\n        if phi.shape!= (self.n_phi,):\n            raise ValueError(\"phi must be a square matrix\")\n        if t.shape!= (self.n_time,):\n            raise ValueError(\"t must be a vector\")\n        if self.verbose:\n            print(\"R,z,phi,t:\",R,z,phi,t)\n        if self.verbose:\n            print", "solution": "\n        if not self.isNonAxi and phi is None:\n            phi= 0.\n        return self._computeArray(self._phiTilde, R,z,phi)"}
{"prompt": "def _construct_event(flask_request, port, binary_types):\n        \"\"\"\n        Helper method that constructs the Event to be passed to Lambda\n\n        :param request flask_request: Flask Request\n        :return: String representing the event\n        \"\"\"", "output": "\n        if not binary_types:\n            return None\n        if not flask_request.args.get('event_id'):\n            return None\n        if not flask_request.args.get('event_type'):\n            return None\n        if not flask_request.args.get('event_name'):\n            return None\n        if not flask_request.args.get('event_id'):\n            return None\n        if not flask_request.args.get('event_type'):\n            return None\n        if not flask_request.args.get('event_name'):\n            return None\n        if not flask_request.args.get", "solution": "\n\n        identity = ContextIdentity(source_ip=flask_request.remote_addr)\n\n        endpoint = PathConverter.convert_path_to_api_gateway(flask_request.endpoint)\n        method = flask_request.method\n\n        request_data = flask_request.get_data()\n\n        request_mimetype = flask_request.mimetype\n\n        is_base_64 = LocalApigwService._should_base64_encode(binary_types, request_mimetype)\n\n        if is_base_64:\n            LOG.debug(\"Incoming Request seems to be binary. Base64 encoding the request data before sending to Lambda.\")\n            request_data = base64.b64encode(request_data)\n\n        if request_data:\n            # Flask does not parse/decode the request data. We should do it ourselves\n            request_data = request_data.decode('utf-8')\n\n        context = RequestContext(resource_path=endpoint,\n                                 http_method=method,\n                                 stage=\"prod\",\n                                 identity=identity,\n                                 path=endpoint)\n\n        event_headers = dict(flask_request.headers)\n        event_headers[\"X-Forwarded-Proto\"] = flask_request.scheme\n        event_headers[\"X-Forwarded-Port\"] = str(port)\n\n        # APIGW does not support duplicate query parameters. Flask gives query params as a list so\n        # we need to convert only grab the first item unless many were given, were we grab the last to be consistent\n        # with APIGW\n        query_string_dict = LocalApigwService._query_string_params(flask_request)\n\n        event = ApiGatewayLambdaEvent(http_method=method,\n                                      body=request_data,\n                                      resource=endpoint,\n                                      request_context=context,\n                                      query_string_params=query_string_dict,\n                                      headers=event_headers,\n                                      path_parameters=flask_request.view_args,\n                                      path=flask_request.path,\n                                      is_base_64_encoded=is_base_64)\n\n        event_str = json.dumps(event.to_dict())\n        LOG.debug(\"Constructed String representation of Event to invoke Lambda. Event: %s\", event_str)\n        return event_str"}
{"prompt": "def _get_asym_alpha(self,a,b):\n        \"\"\"\n            Find alpha diffusion ratios from cryo oven with alpha detectors. \n            a: list of alpha detector histograms (each helicity)\n            b: list of beta  detector histograms (each helicity)\n        \"\"\"", "output": "\n        #a = np.array(a)\n        #b = np.array(b)\n        #alpha = []\n        #b = []\n        #for i in range(len(a)):\n            #alpha.append(a[i][0])\n            #b.append(a[i][1])\n        #return alpha,b,alpha_list\n        #return np.array(a)\n        #return _get_alpha(self,a,b)\n\n    def _get_asym_beta(self,a,b):\n        \"\"\"\n            Find beta diffusion ratios from cryo o", "solution": "\n        \n        # just  use AL0\n        try:\n            a = a[2:4]\n        except IndexError:\n            a = a[:2]\n            \n        # sum counts in alpha detectors\n        asum = np.sum(a,axis=0)\n        \n        # sum counts in beta detectors\n        bsum = np.sum(b,axis=0)\n        \n        # check for dividing by zero \n        asum[asum == 0] = np.nan\n        bsum[bsum == 0] = np.nan\n        \n        # asym calcs\n        asym = asum/bsum\n        \n        # errors\n        dasym = asym*np.sqrt(1/asum + 1/bsum)\n        \n        return [asym,dasym]"}
{"prompt": "def most_recent(self):\n        \"\"\"\n        Returns the most recent copy of the instance available in the history.\n        \"\"\"", "output": "\n        return self.history_model.objects.order_by('-pub_date')[0]\n\n    def get_context_data(self, **kwargs):\n        context = super(TestHistoryView, self).get_context_data(**kwargs)\n        context['recent_entries'] = self.most_recent()\n        return context\n\n\nclass TestHistoryViewWithNoEntries(TestCase):\n    fixtures = ['base/users']\n\n    def setUp(self):\n        self.user = User.objects.create(\n            username='bob',\n            email='bob@example.com',\n            password='secret',\n        )\n       ", "solution": "\n        if not self.instance:\n            raise TypeError(\n                \"Can't use most_recent() without a {} instance.\".format(\n                    self.model._meta.object_name\n                )\n            )\n        tmp = []\n        excluded_fields = getattr(self.model, \"_history_excluded_fields\", [])\n\n        for field in self.instance._meta.fields:\n            if field.name in excluded_fields:\n                continue\n            if isinstance(field, models.ForeignKey):\n                tmp.append(field.name + \"_id\")\n            else:\n                tmp.append(field.name)\n        fields = tuple(tmp)\n        try:\n            values = self.get_queryset().values_list(*fields)[0]\n        except IndexError:\n            raise self.instance.DoesNotExist(\n                \"%s has no historical record.\" % self.instance._meta.object_name\n            )\n        return self.instance.__class__(*values)"}
{"prompt": "def in_lamp_reach(p):\n    \"\"\" Check if the provided XYPoint can be recreated by a Hue lamp. \"\"\"", "output": "\n    return p.is_alive() and p.is_alive()\n\n\ndef in_lamp_release(p):\n    \"\"\" Check if the provided XYPoint can be released by a Hue lamp. \"\"\"\n    return p.is_alive() and p.is_alive()\n\n\ndef in_lamp_recreate(p):\n    \"\"\" Release a lamp. \"\"\"\n    return p.is_alive() and p.is_alive()\n\n\ndef in_lamp_recreate_all(p):\n    \"\"\" Recreate all lamps. \"\"\"\n    return p.is_", "solution": "\n    v1 = XYPoint(Lime.x - Red.x, Lime.y - Red.y)\n    v2 = XYPoint(Blue.x - Red.x, Blue.y - Red.y)\n\n    q = XYPoint(p.x - Red.x, p.y - Red.y)\n    s = cross_product(q, v2) / cross_product(v1, v2)\n    t = cross_product(v1, q) / cross_product(v1, v2)\n\n    return (s >= 0.0) and (t >= 0.0) and (s + t <= 1.0)"}
{"prompt": "def sdiffstore(self, dest, keys, *args):\n        \"\"\"Emulate sdiffstore.\"\"\"", "output": "\n        self.logger.info(\"sdiffstore %s %s\" % (keys, self.sdiffstore_cmd))\n        return self.sdiffstore_cmd(keys, *args)\n\n    def sdiffstore_create(self, dest, keys, *args):\n        \"\"\"Create a new sdiffstore.\"\"\"\n        self.logger.info(\"sdiffstore %s %s\" % (keys, self.sdiffstore_cmd))\n        return self.sdiffstore_cmd(keys, *args)\n\n    def sdiffstore_list(self, dest, keys):\n        \"\"\"List files", "solution": "\n        result = self.sdiff(keys, *args)\n        self.redis[self._encode(dest)] = result\n        return len(result)"}
{"prompt": "def stdchannel_redirected(stdchannel):\n    \"\"\"\n    Redirects stdout or stderr to a StringIO object. As of python 3.4, there is a\n    standard library contextmanager for this, but backwards compatibility!\n    \"\"\"", "output": "\n    if not hasattr(stdchannel, 'fileno'):\n        return StringIO(u\"\")\n    try:\n        old_settings = stdchannel.fileno()\n        stdchannel.close()\n    except:\n        pass\n    try:\n        new_settings = stdchannel.fileno()\n        stdchannel = new_settings.fileno()\n        stdchannel_redirected(stdchannel)\n    except:\n        return StringIO(u\"\")\n    return StringIO(u\"\")\n\ndef stdchannel_read(stdchannel):\n    \"\"\"\n    Reads from the standard input stream. Returns a StringIO object.\n    \"\"\"\n    return stdchannel.read()\n\ndef std", "solution": "\n    try:\n        s = io.StringIO()\n        old = getattr(sys, stdchannel)\n        setattr(sys, stdchannel, s)\n        yield s\n    finally:\n        setattr(sys, stdchannel, old)"}
{"prompt": "def _preprocess_sgm(line, is_sgm):\n  \"\"\"Preprocessing to strip tags in SGM files.\"\"\"", "output": "\n  # Replace the first occurrence of \"const char *\" with \"const char *\" so that\n  # we don't need to pick up on spaces inside single-quotes, for example.\n  line = re.sub(r\"const char *([a-zA-Z0-9_]+)\\s*;\", \"\", line)\n\n  # Replace the first occurrence of \"const char *\" with \"const char *\"\n  # so that we don't need to pick up on spaces inside double-quotes, for example.\n  line = re.sub(r\"const char *([a-zA-Z0-9_]+)\\", "solution": "\n  if not is_sgm:\n    return line\n  # In SGM files, remove <srcset ...>, <p>, <doc ...> lines.\n  if line.startswith(\"<srcset\") or line.startswith(\"</srcset\"):\n    return \"\"\n  if line.startswith(\"<doc\") or line.startswith(\"</doc\"):\n    return \"\"\n  if line.startswith(\"<p>\") or line.startswith(\"</p>\"):\n    return \"\"\n  # Strip <seg> tags.\n  line = line.strip()\n  if line.startswith(\"<seg\") and line.endswith(\"</seg>\"):\n    i = line.index(\">\")\n    return line[i + 1:-6]"}
{"prompt": "def prepare_delete_monarchy(self, node, position=None, save=True):\n        \"\"\" Prepares a given :class:`CTENode` `node` for deletion, by executing\n            the :const:`DELETE_METHOD_MONARCHY` semantics. Descendant nodes,\n            if present, will be moved; in this case the optional `position` can\n            be a ``callable`` which is invoked prior to each move operation (see\n            :meth:`move` for details).\n\n            By default, after each move operation, sub-tree nodes which were\n            moved will be saved through a call to :meth:`Model.save` unless\n            `save` is ``False``.\n\n            This method delegates move operations to :meth:`move`.\n\n            :param node: the :class:`CTENode` to prepare for deletion.\n\n            :param position: optionally, a ``callable`` to invoke prior to each\n                move operation.\n\n            :param save: flag indicating whether to save after each move\n                operation, ``True`` by default.\n        \"\"\"", "output": " to :meth:`prepare_monarchy` will be\n            executed.\n\n            If you want to save the node to a temporary file, you can use the\n            :meth:`tempfile_manager` function to save the temporary file.\n\n            If you want to save the node to a permanent file, you can use the\n            :meth:`tempfile_manager` function to save the permanent file.\n\n            If you want to save the node to a permanent file, you can use the\n            :meth:`tempfile_manager` function to save the permanent file.\n\n            If you want to save the node to a permanent file, you can use the", "solution": "\n        # We are going to iterate all children, even though the first child is\n        # treated in a special way, because the query iterator may be custom, so\n        # we will avoid using slicing children[0] and children[1:].\n        first = None\n        for child in node.children.all():\n            if first is None:\n                first = child\n                first.move(node.parent, position, save)\n            else:\n                child.move(first, position, save)"}
{"prompt": "def _add_genes(self, variant_obj):\n        \"\"\"Add the Gene objects for a variant\"\"\"", "output": "\n        for gene in self.genes:\n            gene_obj = Gene(gene, variant_obj)\n            self.genes_by_variant[gene] = gene_obj\n            self.genes_by_gene_id[gene.id] = gene_obj.id\n            self.genes_by_gene_name[gene.name] = gene_obj.name\n            self.genes_by_variant_id[variant_obj.id] = gene_obj.id\n            self.variants.append(gene_obj)\n\n    def _add_variants(self, variant_obj):\n        \"\"\"", "solution": "\n        genes = []\n        ensembl_ids = []\n        hgnc_symbols = []\n        \n        if variant_obj.transcripts:\n            for transcript in variant_obj.transcripts:\n                if transcript.ensembl_id:\n                    ensembl_ids.append(transcript.ensembl_id)\n                if transcript.hgnc_symbol:\n                    hgnc_symbols.append(transcript.hgnc_symbol)\n\n        else:\n            hgnc_symbols = variant_obj.gene_symbols\n        \n        genes = get_gene_info(\n                        ensembl_ids=ensembl_ids, \n                        hgnc_symbols=hgnc_symbols\n                        )\n            \n        for gene in genes:\n            variant_obj.add_gene(gene)"}
{"prompt": "def post_fork(self, payload_handler, io_loop):\n        \"\"\"\n        After forking we need to create all of the local sockets to listen to the\n        router\n\n        :param func payload_handler: A function to called to handle incoming payloads as\n                                     they are picked up off the wire\n        :param IOLoop io_loop: An instance of a Tornado IOLoop, to handle event scheduling\n        \"\"\"", "output": "\n        self.io_loop = io_loop\n        self.router.add_route(self.route)\n        self.router.add_route(self.router.get_route(self.route))\n        self.io_loop.add_callback(payload_handler)\n\n    def post_exec(self, io_loop):\n        \"\"\"\n        After exec we need to create all of the local sockets to listen to the\n        router\n\n        :param func io_loop: An instance of a Tornado IOLoop, to handle event scheduling\n        \"\"\"\n        self.io_loop = io_loop\n        self.router.", "solution": "\n        self.payload_handler = payload_handler\n        self.io_loop = io_loop\n\n        self.context = zmq.Context(1)\n        self._socket = self.context.socket(zmq.REP)\n        self._start_zmq_monitor()\n\n        if self.opts.get('ipc_mode', '') == 'tcp':\n            self.w_uri = 'tcp://127.0.0.1:{0}'.format(\n                self.opts.get('tcp_master_workers', 4515)\n                )\n        else:\n            self.w_uri = 'ipc://{0}'.format(\n                os.path.join(self.opts['sock_dir'], 'workers.ipc')\n                )\n        log.info('Worker binding to socket %s', self.w_uri)\n        self._socket.connect(self.w_uri)\n\n        salt.transport.mixins.auth.AESReqServerMixin.post_fork(self, payload_handler, io_loop)\n\n        self.stream = zmq.eventloop.zmqstream.ZMQStream(self._socket, io_loop=self.io_loop)\n        self.stream.on_recv_stream(self.handle_message)"}
{"prompt": "def redirect_logging(tqdm_obj, logger=logging.getLogger()):\n  \"\"\" Context manager to redirect logging to a TqdmLoggingHandler object and then restore the original. \"\"\"", "output": "\n  logger.info(\"Redirecting logging to %s\", tqdm_obj.base_path)\n  tqdm_obj.restore_path = tqdm_obj.base_path\n  tqdm_obj.save()\n\n\ndef get_logger(name, log_dir=None):\n  \"\"\" Get a logger with the specified name. \"\"\"\n  logger = logging.getLogger(name)\n  if log_dir:\n    logger.info(\"Logging to %s\", log_dir)\n  else:\n    logger.info(\"Logging to default\", name)\n  return logger\n\n\ndef get_", "solution": "\n  # remove current handler\n  assert(len(logger.handlers) == 1)\n  prev_handler = logger.handlers[0]\n  logger.removeHandler(prev_handler)\n\n  # add tqdm handler\n  tqdm_handler = TqdmLoggingHandler(tqdm_obj)\n  if prev_handler.formatter is not None:\n    tqdm_handler.setFormatter(prev_handler.formatter)\n  logger.addHandler(tqdm_handler)\n\n  try:\n    yield\n  finally:\n    # restore handler\n    logger.removeHandler(tqdm_handler)\n    logger.addHandler(prev_handler)"}
{"prompt": "def stdev(x):\n    r\"\"\"Calculate standard deviation of data x[]:\n        std = sqrt(\\sum_i (x_i - mean)^2 \\over n-1)\n        https://wiki.python.org/moin/NumericAndScientificRecipes\n    \"\"\"", "output": "\n    return np.sqrt(np.sum(x**2))\n\ndef sterr(x):\n    r\"\"\"Calculate standard error of data x[]:\n        std = sqrt(sum_i (x_i - mean)^2 \\over n-1)\n        https://wiki.python.org/moin/NumericAndScientificRecipes\n    \"\"\"\n    return np.sqrt(np.sum(x**2))\n\ndef sterr_inv(x):\n    r\"\"\"Calculate error of data x[]:\n        std = sqrt(sum_i (x_i - mean)^2 \\over n", "solution": "\n    from math import sqrt\n    n = len(x)\n    mean = sum(x) / float(n)\n    std = sqrt(sum((a - mean)**2 for a in x) / float(n - 1))\n    return std"}
{"prompt": "def get_rst_title(rst_doc: Node) -> Optional[Any]:\n    \"\"\" Given some RST, extract what docutils thinks is the title \"\"\"", "output": "\n    if not rst_doc:\n        return None\n    if rst_doc.children:\n        return get_rst_title(rst_doc.children[0])\n    return None\n\n\ndef get_rst_subtitle(rst_doc: Node) -> Optional[str]:\n    \"\"\" Given some RST, extract the docutils thinks is the subtitle \"\"\"\n    if not rst_doc:\n        return None\n    if rst_doc.children:\n        return get_rst_subtitle(rst_doc.children[0])\n    return None\n\n\ndef get_rst_body(rst_doc: Node) -> str:", "solution": "\n\n    for title in rst_doc.traverse(nodes.title):\n        return title.astext()\n\n    return None"}
{"prompt": "def _post_create(self, auto_refresh=False):\n\n\t\t\"\"\"\n\t\tresource.create() hook\n\n\t\tFor PCDM Collections, post creation, also create\n\t\t\"\"\"", "output": "\n\n\t\tif not self.is_valid():\n\t\t\treturn False\n\n\t\tif auto_refresh:\n\t\t\tself.refresh()\n\n\t\tif not self.is_visible():\n\t\t\treturn False\n\n\t\treturn self.create_view(self.get_model(), self.get_view_type(), self.get_model().get_value(self.get_active_iter(), 2))\n\n\tdef _post_update(self, auto_refresh=False):\n\n\t\t\"\"\"\n\t\tresource.update() hook\n\n\t\tFor PCDM Collections, post update, also update\n\t\t\"\"\"\n\n\t", "solution": "\n\n\t\t# set PCDM triple as Collection\n\t\tself.add_triple(self.rdf.prefixes.rdf.type, self.rdf.prefixes.pcdm.Collection)\n\t\tself.update(auto_refresh=auto_refresh)\n\n\t\t# create /members child resource\n\t\tmembers_child = PCDMMembersContainer(\n\t\t\tself.repo,\n\t\t\t'%s/members' % self.uri_as_string(),\n\t\t\tmembershipResource=self.uri,\n\t\t\thasMemberRelation=self.rdf.prefixes.pcdm.hasMember,\n\t\t\tinsertedContentRelation=self.rdf.prefixes.ore.proxyFor)\n\t\tmembers_child.create(specify_uri=True)\n\n\t\t# create /related child resource\n\t\trelated_child = PCDMRelatedContainer(\n\t\t\tself.repo,\n\t\t\t'%s/related' % self.uri_as_string(),\n\t\t\tmembershipResource=self.uri,\n\t\t\thasMemberRelation=self.rdf.prefixes.ore.aggregates,\n\t\t\tinsertedContentRelation=self.rdf.prefixes.ore.proxyFor)\n\t\trelated_child.create(specify_uri=True)"}
{"prompt": "def queryjoin(argdict=dict(), **kwargs):\n    \"\"\"Turn a dictionary into a querystring for a URL.\n    \n    >>> args = dict(a=1, b=2, c=\"foo\")\n    >>> queryjoin(args)\n    \"a=1&b=2&c=foo\"\n    \"\"\"", "output": "\n    if argdict:\n        return '&'.join(['%s=%s' % (k, argdict[k]) for k in sorted(argdict.keys())])\n    else:\n        return ''\n\ndef query_encode(argdict=dict(), **kwargs):\n    \"\"\"Encode a dictionary for use with the :class:`URL` class.\n    \n    >>> args = dict(a=1, b=2, c=\"foo\")\n    >>> query_encode(args)\n    'a=1&b=2&c=foo'\n    \"\"\"\n    return '&'.join(['%s=%s' % (k, arg", "solution": "\n    if kwargs: argdict.update(kwargs)\n    \n    if issubclass(type(argdict), dict):                    \n        args = [\"{}={}\".format(k, v) for k, v in argdict.items() if v != None]\n    return \"&\".join(args)"}
{"prompt": "def _si(number):\n    \"\"\"Format a number using base-2 SI prefixes\"\"\"", "output": "\n    if number < 0:\n        return '-' + _si_prefix(-number)\n    else:\n        return str(number)\n\n\ndef _si_prefix(number):\n    \"\"\"Format a number using SI prefixes\"\"\"\n    if number < 0:\n        return '-' + _si_prefix(-number)\n    else:\n        return str(number)\n\n\ndef _si_suffix(number):\n    \"\"\"Format a number using base-2 suffixes\"\"\"\n    if number < 0:\n        return '-' + _si_suffix(-number)\n    else:\n        return str(number)\n\n\ndef _si_prefix_suffix", "solution": "\n    prefixes = ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y']\n    while number > 1024:\n        number /= 1024.0\n        prefixes.pop(0)\n    return '%0.2f%s' % (number, prefixes.pop(0))"}
{"prompt": "def _key_backspace(self, cursor_position):\r\n        \"\"\"Action for Backspace key\"\"\"", "output": "\r\n        if cursor_position > len(self.backspace):\r\n            self.backspace = self.backspace +''\r\n        else:\r\n            self.backspace = self.backspace[cursor_position:]\r\n        self.cursor_position = cursor_position\r\n\r\n    def _key_delete(self, key):\r\n        \"\"\"Action for Delete key\"\"\"\r\n        if key >= 0 and key < len(self.backspace):\r\n            self.cursor_position = self.backspace.index(key)\r\n            self.delete_char()\r\n            self.cursor_position = len(self.backspace) - 1\r\n            self", "solution": "\r\n        if self.has_selected_text():\r\n            self.check_selection()\r\n            self.remove_selected_text()\r\n        elif self.current_prompt_pos == cursor_position:\r\n            # Avoid deleting prompt\r\n            return\r\n        elif self.is_cursor_on_last_line():\r\n            self.stdkey_backspace()\r\n            if self.is_completion_widget_visible():\r\n                # Removing only last character because if there was a selection\r\n                # the completion widget would have been canceled\r\n                self.completion_text = self.completion_text[:-1]"}
{"prompt": "def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):\n        \"\"\"\n        See :meth:`superclass method\n        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`\n        for spec of input and result values.\n        \"\"\"", "output": "\n        # Get the mean and stddevs for the given sites\n        mean = self.get_mean(sites, rup.mag)\n        stddevs = self.get_stddevs(sites, rup.mag)\n\n        # Return the site indices\n        return np.array(sites), np.array(mean), np.array(stddevs)\n\n    def get_mean(self, mag):\n        \"\"\"\n        Returns the site mean value for the given magnitude.\n        \"\"\"\n        return self.model.get_mean(mag)\n\n    def get_stddevs(self, mag, stddev_type):\n        \"\"\"\n       ", "solution": "\n        C = self.COEFFS[imt]\n        C_SIG = self.SIGMA_COEFFS[imt]\n\n        mean = (self.get_magnitude_scaling_term(C, rup.mag) +\n                self.get_distance_scaling_term(C, dists.rhypo))\n\n        std_devs = self.get_stddevs(C_SIG, stddev_types, len(dists.rhypo))\n\n        #: Mean ground motions initially returned in cm/s/s (for PGA, SA)\n        #: and cm/s for PGV\n        if not imt.name == \"PGV\":\n            # Convert mean from log(cm/s/s) to g\n            mean = np.log(np.exp(mean) / (100. * g))\n\n        return mean, std_devs"}
{"prompt": "def add_nulldata(self, rawtx, hexdata):\n        \"\"\"Writes <hexdata> as new nulldata output to <rawtx>.\"\"\"", "output": "\n        assert rawtx is not None\n        tx = FromHex(CTransaction(), hexdata)\n        self.nodes[0].sendrawtransaction(tx.serialize().hex())\n        assert_equal(len(self.nodes[0].getrawmempool()), 0)\n\n    def run_test(self):\n        self.log.info(\"Test maxuploadtarget is disabled\")\n        self.test_maxuploadtarget_not_broadcast()\n\n        # Build a block with the transaction enabled, and verify that it's not accepted.\n        block = self.build_next_block()\n        self.sync_all()\n        assert", "solution": "\n        tx = deserialize.unsignedtx(rawtx)\n        nulldata_txout = deserialize.nulldata_txout(hexdata)\n        tx = control.add_nulldata_output(tx, nulldata_txout)\n        return serialize.tx(tx)"}
{"prompt": "def wrap_args_kwargs_to_initdict(init_args_kwargs_fn: InitArgsKwargsFnType,\n                                 typename: str,\n                                 check_result: bool = True) \\\n        -> InstanceToInitDictFnType:\n    \"\"\"\n    Wraps a function producing a ``KwargsDict``, making it into a function\n    producing an ``InitDict``.\n    \"\"\"", "output": "\n    return {\n        'init_args_kwargs_fn': lambda: (\n            lambda: (\n                lambda: (\n                    lambda: (\n                        lambda: (\n                            lambda: (\n                                lambda: (\n                                    lambda: (\n                                        lambda: (\n                                            lambda: (\n                                                lambda: (\n                                                    lambda: (\n                                                        lambda: (\n                                                            lambda: (\n                                                                lambda: (\n                                                                lambda: (\n                                                                    lambda: (\n                                                                    lambda: (\n                                                                    lambda: (\n                                                                    lambda: (\n                                                                    lambda: (\n                                                                    lambda: (\n                                                                    lambda: (\n                                                                    lambda: (\n                                                                    lambda: (\n                                                                    lambda: (\n                                                                    lambda: (\n                                                                    lambda: (\n                                                                    lambda: (", "solution": "\n    def wrapper(obj: Instance) -> InitDict:\n        result = init_args_kwargs_fn(obj)\n        if check_result:\n            if (not isinstance(result, tuple) or\n                    not len(result) == 2 or\n                    not isinstance(result[0], list) or\n                    not isinstance(result[1], dict)):\n                raise ValueError(\n                    \"Class {} failed to provide an (args, kwargs) tuple and \"\n                    \"provided instead: {}\".format(typename, repr(result)))\n        return args_kwargs_to_initdict(*result)\n\n    return wrapper"}
{"prompt": "def delete_handler(Model, name=None, **kwds):\n    \"\"\"\n        This factory returns an action handler that deletes a new instance of\n        the specified model when a delete action is recieved, assuming the\n        action follows nautilus convetions.\n\n        Args:\n            Model (nautilus.BaseModel): The model to delete when the action\n                received.\n\n        Returns:\n            function(type, payload): The action handler for this model\n    \"\"\"", "output": "\n    def _delete_handler(self, *args, **kwds):\n        if self.name is None:\n            self.name = self.model_name\n        return self.name_to_handler[self.model_name](*args, **kwds)\n\n    def _get_action_handler(self, *args, **kwds):\n        if self.model_name is None:\n            return None\n        return self.name_to_action[self.model_name](*args, **kwds)\n\n    def _get_payload_handler(self, *args, **kwds):\n        if self.", "solution": "\n    # necessary imports\n    from nautilus.database import db\n\n    async def action_handler(service, action_type, payload, props, notify=True, **kwds):\n        # if the payload represents a new instance of `model`\n        if action_type == get_crud_action('delete', name or Model):\n            try:\n                # the props of the message\n                message_props = {}\n                # if there was a correlation id in the request\n                if 'correlation_id' in props:\n                    # make sure it ends up in the reply\n                    message_props['correlation_id'] = props['correlation_id']\n                # the id in the payload representing the record to delete\n                record_id = payload['id'] if 'id' in payload else payload['pk']\n                # get the model matching the payload\n                try:\n                    model_query = Model.select().where(Model.primary_key() == record_id)\n                except KeyError:\n                    raise RuntimeError(\"Could not find appropriate id to remove service record.\")\n                # remove the model instance\n                model_query.get().delete_instance()\n                # if we need to tell someone about what happened\n                if notify:\n                    # publish the success event\n                    await service.event_broker.send(\n                        payload='{\"status\":\"ok\"}',\n                        action_type=change_action_status(action_type, success_status()),\n                        **message_props\n                    )\n\n            # if something goes wrong\n            except Exception as err:\n                # if we need to tell someone about what happened\n                if notify:\n                    # publish the error as an event\n                    await service.event_broker.send(\n                        payload=str(err),\n                        action_type=change_action_status(action_type, error_status()),\n                        **message_props\n                    )\n                # otherwise we aren't supposed to notify\n                else:\n                    # raise the exception normally\n                    raise err\n\n\n    # return the handler\n    return action_handler"}
{"prompt": "def _url(self, url, file_upload=False):\n        \"\"\"\n        Creates the request URL.\n        \"\"\"", "output": "\n        return '{0}?{1}'.format(url, urllib.urlencode(self.params)) if file_upload else url\n\n    def _get_params(self, params):\n        \"\"\"\n        Returns the request parameters.\n        \"\"\"\n        return urllib.urlencode(params)\n\n    def _get_file_path(self, file_name):\n        \"\"\"\n        Returns the file path.\n        \"\"\"\n        return os.path.join(self.base_path, file_name)\n\n    def _get_file_name(self, file_upload):\n        \"\"\"\n        Returns the file name.\n        \"\"\"\n        if file_", "solution": "\n\n        host = self.api_url\n        if file_upload:\n            host = self.uploads_api_url\n\n        protocol = 'https' if self.https else 'http'\n        if url.endswith('/'):\n            url = url[:-1]\n        return '{0}://{1}/{2}'.format(\n            protocol,\n            host,\n            url\n        )"}
{"prompt": "def _add_fbc_objective(self, model_tag, obj_id):\n        \"\"\"Adds the objective(s) to the sbml document.\"\"\"", "output": "\n        self.add_objective(\n            'fbc',\n            'fbc',\n            model_tag,\n            obj_id,\n            'fbc_objective',\n            'fbc_objective_id',\n            'fbc_objective_tag',\n            'fbc_objective_id',\n            'fbc_objective_tag_id',\n            'fbc_objective_id',\n        )\n\n    def add_objective(self, tag, model_id, objective_id):\n        \"\"\"Adds the objective(s) to the sbml document.\"\"\"\n        self.add_objective_tag(\n           'sbml", "solution": "\n        objective_list = ET.SubElement(model_tag, _tag(\n            'listOfObjectives', FBC_V2))\n        objective_list.set(_tag('activeObjective', FBC_V2), 'O_1')\n        objective_tag = ET.SubElement(\n            objective_list, _tag('objective', FBC_V2))\n        objective_tag.set(_tag('id', FBC_V2), 'O_1')\n        objective_tag.set(_tag('type', FBC_V2), 'maximize')\n        flux_objective_list = ET.SubElement(objective_tag, _tag(\n            'listOfFluxObjectives', FBC_V2))\n        flux_objective_tag = ET.SubElement(flux_objective_list, _tag(\n            'fluxObjective', FBC_V2))\n        flux_objective_tag.set(_tag('reaction', FBC_V2), 'R_' + obj_id)\n        flux_objective_tag.set(_tag('coefficient', FBC_V2), '1')"}
{"prompt": "def content_type(self, value=None):\n\t\t\"\"\" Set (replace) and or get \"Content-Type\" header value\n\n\t\t:param value: value to set (if specified)\n\t\t:return: None if header doesn't exist, otherwise - str\n\t\t\"\"\"", "output": "\n\t\tif value is None:\n\t\t\tvalue = self.get_header_value(self.name)\n\t\tif value is None:\n\t\t\treturn None\n\t\tif not self.name in self.content_types:\n\t\t\tself.content_types[self.name] = {}\n\t\tself.content_types[self.name][self.content_type] = value\n\t\treturn None\n\n\tdef set_content_type(self, value):\n\t\t\"\"\" Set \"Content-Type\" header value\n\n\t\t:param value: value to set (if specified)\n\t\t:return:", "solution": "\n\t\tcontent_type = self.normalize_name('Content-Type')\n\t\tif value is not None:\n\t\t\tself.replace_headers(content_type, value)\n\t\tif content_type in self.__headers.keys():\n\t\t\treturn self.__headers[content_type][0]"}
{"prompt": "def _build_all_dependencies(self):\n        \"\"\"\n        Helper function to build a map of model to their list of model reference dependencies\n        \"\"\"", "output": "\n        model_dependencies = {}\n\n        for app_label, model_name in sorted(self.kept_model_keys):\n            model_dependencies[app_label] = self.kept_model_class(app_label, model_name)\n\n        for app_label, model_name in sorted(self.kept_model_keys):\n            # Record the number of dependencies\n            model_dependencies[app_label] = self.kept_model(model_name, self.model_name_lower)\n\n            # If there's a dependency on an app that is not installed, we let", "solution": "\n        ret = {}\n        for model, schema in six.iteritems(self._models()):\n            dep_list = self._build_dependent_model_list(schema)\n            ret[model] = dep_list\n        return ret"}
{"prompt": "def get(self, obj, cls):\n    \"\"\"\n    Using the lowercase name of the class as node_type, returns `obj.visit_{node_type}`,\n    or `obj.visit_default` if the type-specific method is not found.\n    \"\"\"", "output": "\n    return self.visit(obj, cls)\n\n  def visit_Name(self, node):\n    \"\"\"\n    A `Name` node is a `Name` node with the name of the function.\n    \"\"\"\n    if node.id =='main':\n      return self.default_main_name\n    return self.generic_visit(node)\n\n  def visit_Attribute(self, node):\n    \"\"\"\n    An `Attribute` node is a `Attribute` node with the name of the attribute.\n    \"\"\"\n    return self.generic_visit(node)\n\n  def visit_Call(self, node):\n    \"\"\"\n    A", "solution": "\n    method = self._cache.get(cls)\n    if not method:\n      name = \"visit_\" + cls.__name__.lower()\n      method = getattr(obj, name, obj.visit_default)\n      self._cache[cls] = method\n    return method"}
{"prompt": "def contains_locked_file(directory: str):\n    \"\"\"\n    :return: True if any of the files in the directory are in use. For example, if the dll is injected\n    into the game, this will definitely return true.\n    \"\"\"", "output": "\n    return os.path.isdir(directory) and os.access(directory, os.R_OK)\n\n\ndef get_library_path(directory: str) -> str:\n    \"\"\"\n    :return: The path to the library file. For example, if the dll is injected into the game\n    and the path is a symlink, this will return the path to the symlink.\n    \"\"\"\n    return os.path.join(directory, \"lib\", \"game\")\n\n\ndef get_library_path_from_name(name: str) -> str:\n    \"\"\"\n    :return: The path to the", "solution": "\n    for root, subdirs, files in os.walk(directory):\n        for filename in files:\n            file_path = os.path.join(root, filename)\n            try:\n                with open(file_path, 'a'):\n                    pass\n            except IOError:\n                logger.debug(f\"Locked file: {file_path}\")\n                return True\n    return False"}
{"prompt": "def determine_opening_indent(indent_texts):\n    \"\"\"Determine the opening indent level for a docstring.\n\n    The opening indent level is the indent level is the first non-zero indent\n    level of a non-empty line in the docstring.\n\n    Args:\n        indent_texts: The lines of the docstring as an iterable over 2-tuples\n            each containing an integer indent level as the first element and\n            the text as the second element.\n\n    Returns:\n        The opening indent level as an integer.\n    \"\"\"", "output": "\n    indent_level = 0\n    for i, indent_level_i in enumerate(indent_texts):\n        if i == indent_level_i[0]:\n            indent_level += 1\n        elif i == indent_level_i[1]:\n            indent_level -= 1\n            break\n    return indent_level\n\n\ndef determine_docstring_indent(docstring):\n    \"\"\"Determine the opening indent level for a docstring.\n\n    Args:\n        docstring: The docstring as an iterable over 2-tuples\n            each containing an integer indent level as the first element and\n            the text as the second element.\n\n    Returns:\n        The opening", "solution": "\n    num_lines = len(indent_texts)\n\n    if num_lines < 1:\n        return 0\n\n    assert num_lines >= 1\n\n    first_line_indent  = indent_texts[0][0]\n\n    if num_lines == 1:\n        return first_line_indent\n\n    assert num_lines >= 2\n\n    second_line_indent = indent_texts[1][0]\n    second_line_text   = indent_texts[1][1]\n\n    if len(second_line_text) == 0:\n        return first_line_indent\n\n    return second_line_indent"}
{"prompt": "def get_contract_data(self, contract_name):\n        \"\"\"Returns the contract data for a given contract\n\n        Args:\n            contract_name (str): Name of the contract to return.\n\n        Returns:\n            str, str: ABI and bytecode of the contract\n        \"\"\"", "output": "\n        return self.contract_data[contract_name]\n\n    def get_contract_data_for_address(self, address):\n        \"\"\"Returns the contract data for a given address\n\n        Args:\n            address (int): Address of the contract to return.\n\n        Returns:\n            int, int: ABI and bytecode of the contract\n        \"\"\"\n        return self.contract_data_for_address[address]\n\n    def get_contract_data_for_address_and_contract_id(self, address, contract_id):\n        \"\"\"Returns the contract data for a given address and contract ID\n\n        Args:\n            address", "solution": "\n\n        contract_data_path = self.output_dir + '/{0}.json'.format(contract_name)\n        with open(contract_data_path, 'r') as contract_data_file:\n            contract_data = json.load(contract_data_file)\n\n        abi = contract_data['abi']\n        bytecode = contract_data['evm']['bytecode']['object']\n\n        return abi, bytecode"}
{"prompt": "def setup_sql(self, graphs):\n        \"\"\"\n        Sets up the SQL tables for the graph object,\n        and creates indexes as well.\n\n        :param graphs: The graphs to create.\n        \"\"\"", "output": "\n        self.graph = graphs\n        self.sql = []\n        for graph in graphs:\n            self.sql.append(self.graph.sql_table(graph))\n\n    def __str__(self):\n        \"\"\"\n        Returns a string representation of the graph.\n        \"\"\"\n        return self.graph.to_string()\n\n    def __repr__(self):\n        \"\"\"\n        Returns a string representation of the graph.\n        \"\"\"\n        return self.graph.to_string(True)\n\n    def add_sql_edge(self, source, target, **kwargs):\n        \"\"\"\n        Adds a single edge to the SQL graph.\n\n        :", "solution": "\n        with closing(self.db.cursor()) as cursor:\n            for table in graphs:\n                cursor.execute(SQL.CREATE_TABLE % (table))\n                for index in SQL.INDEXES:\n                    cursor.execute(index % (table))\n            self.db.commit()"}
{"prompt": "def _toSparkGLM(self, model):\n        \"\"\" Private method for converting a GLM to a Spark model\n        TODO: Add model parameters as well.\n        \"\"\"", "output": "\n        if self.isSet(model):\n            return model\n        else:\n            return self.toModel(model)\n\n    def toLocalFile(self, path):\n        \"\"\"\n        Save this model to the local file system.\n        \"\"\"\n        if self.isSet(self.model):\n            return self.model\n        else:\n            return self.toLocalFile(path)\n\n    def toModel(self, model):\n        \"\"\"\n        Convert this model to a PySpark model.\n        TODO: Add validation of the model.\n        \"\"\"\n        if self.isSet(self.model):\n            if isinstance(model, LogisticRegression):", "solution": "\n        skl_cls = type(model)\n        py_cls = self._skl2spark_classes[skl_cls].py\n        jvm_cls_name = self._skl2spark_classes[skl_cls].jvm\n        intercept = model.intercept_\n        weights = model.coef_\n        if len(np.shape(weights)) == 1\\\n                or (len(np.shape(weights)) == 2 and np.shape(weights)[0] == 1):\n            # Binary classification\n            uid = _randomUID(skl_cls)\n            _java_model = _new_java_obj(self.sc, jvm_cls_name, uid, Vectors.dense(weights), float(intercept))\n            return py_cls(_java_model)\n        elif len(np.shape(weights)) == 2 and skl_cls == SKL_LogisticRegression:\n            # Multiclass label\n            raise ValueError(\"Converter.toSpark cannot convert a multiclass sklearn Logistic\" +\n                             \" Regression model to Spark because Spark does not yet support\" +\n                             \" multiclass.  Given model is for %d classes.\" %\n                             np.shape(weights)[0])\n        else:\n            raise Exception(\"Converter.toSpark experienced unknown error when trying to convert\" +\n                            \" a model of type: \" + type(model) + \"  \" + len(np.shape(weights)))"}
{"prompt": "def _proc_dihedral(self):\n        \"\"\"\n        Handles dihedral group molecules, i.e those with intersecting R2 axes\n        and a main axis.\n        \"\"\"", "output": "\n        # TODO: check if this is necessary\n        # TODO: check if the molecules are in the same order as the atoms in the\n        #       molecule list\n        #       R2\n        #       R2\n        #       R2\n        #       R2\n        #       R2\n        #       R2\n        #       R2\n        #       R2\n        #       R2\n        #       R2\n        #       R2\n        #       R2\n        #       R2\n        #       R2\n        #       R2\n        #       R2\n        #       R2\n        #       R2\n        #       R2", "solution": "\n        main_axis, rot = max(self.rot_sym, key=lambda v: v[1])\n        self.sch_symbol = \"D{}\".format(rot)\n        mirror_type = self._find_mirror(main_axis)\n        if mirror_type == \"h\":\n            self.sch_symbol += \"h\"\n        elif not mirror_type == \"\":\n            self.sch_symbol += \"d\""}
{"prompt": "def get_environments():\n    \"\"\"\n    Return defined environments from config file for default\n    environment values.\n    \"\"\"", "output": "\n    return {\n        'dev': 'dev',\n        'prod': 'prod',\n       'stage':'stage',\n        'prod_stage': 'prod_stage',\n       'stage_prod':'stage_prod',\n        'prod_prod': 'prod_prod',\n       'stage_stage':'stage_stage',\n        'prod_stage_prod': 'prod_stage_prod',\n       'stage_stage_prod':'stage_stage_prod',\n    }\n\ndef get_environment(name):\n    \"\"\"\n    Return defined environment variables from config file for default\n    environment values.\n    \"\"\"\n   ", "solution": "\n    config = ConfigParser.SafeConfigParser()\n\n    config = _config_file()\n\n    juicer.utils.Log.log_debug(\"Reading environment sections:\")\n\n    environments = config.sections()\n    juicer.utils.Log.log_debug(\"Read environment sections: %s\", ', '.join(environments))\n    return environments"}
{"prompt": "def setMaxSpeedLat(self, typeID, speed):\n        \"\"\"setMaxSpeedLat(string, double) -> None\n\n        Sets the maximum lateral speed of this type.\n        \"\"\"", "output": "\n        self.sendUpdate('setMaxSpeedLat', [typeID, speed])\n\n    def setMaxSpeedLong(self, typeID, longSpeed):\n        \"\"\"setMaxSpeedLong(string, double) -> None\n\n        Sets the maximum lateral long speed of this type.\n        \"\"\"\n        self.sendUpdate('setMaxSpeedLong', [typeID, longSpeed])\n\n    def setMaxSpeedLatLong(self, typeID, longSpeed, lat):\n        \"\"\"setMaxSpeedLatLong(string, double, double) -> None\n\n        Sets the maximum lateral lattitude of this type from the given lattitude.", "solution": "\n        self._connection._sendDoubleCmd(\n            tc.CMD_SET_VEHICLETYPE_VARIABLE, tc.VAR_MAXSPEED_LAT, typeID, speed)"}
{"prompt": "def files(self):\n        \"\"\"Get files iterator.\n\n        :returns: Files iterator.\n        \"\"\"", "output": "\n        return self._files_iterator\n\n    def _get_file_path(self, path):\n        \"\"\"Get file path.\n\n        :param path: File path.\n        :type path: str\n        :returns: Iterator over file path.\n        :rtype: collections.Iterable[T]\n        \"\"\"\n        for root, dirs, files in self.walk(path):\n            for f in files:\n                yield f\n\n    def _get_file_path_from_key(self, key):\n        \"\"\"Get file path from a given key.\n\n        :param key: Key.\n        :type key: str\n        :returns:", "solution": "\n        if self.model is None:\n            raise MissingModelError()\n\n        records_buckets = RecordsBuckets.query.filter_by(\n            record_id=self.id).first()\n\n        if not records_buckets:\n            bucket = self._create_bucket()\n            if not bucket:\n                return None\n            RecordsBuckets.create(record=self.model, bucket=bucket)\n        else:\n            bucket = records_buckets.bucket\n\n        return self.files_iter_cls(self, bucket=bucket, file_cls=self.file_cls)"}
{"prompt": "def triangle(self, params=None, **kwargs):\n        \"\"\"\n        Makes a nifty corner plot.\n\n        Uses :func:`triangle.corner`.\n\n        :param params: (optional)\n            Names of columns (from :attr:`StarModel.samples`)\n            to plot.  If ``None``, then it will plot samples\n            of the parameters used in the MCMC fit-- that is,\n            mass, age, [Fe/H], and optionally distance and A_V.\n\n        :param query: (optional)\n            Optional query on samples.\n\n        :param extent: (optional)\n            Will be appropriately passed to :func:`triangle.corner`.\n\n        :param **kwargs:\n            Additional keyword arguments passed to :func:`triangle.corner`.\n\n        :return:\n            Figure oject containing corner plot.\n            \n        \"\"\"", "output": " :func:`query_region`.\n\n        :param ax: (optional)\n            Optional axis on which to plot.\n\n        :return: (matplotlib.axes)\n            The axes object that was passed to ``triangle.corner``.\n        \"\"\"\n        return triangle.corner(self, params=params, **kwargs)\n\n    def plot_triangle(self, params=None, **kwargs):\n        \"\"\"\n        Plots a triangle.\n\n        :param params: (optional)\n            Names of columns (from :attr:`StarModel.samples`)\n            to plot.  If ``None``, then it will plot samples\n            of the parameters used in the", "solution": "\n        if params is None:\n            params = ['mass_A', 'mass_B', 'age', 'feh', 'distance', 'AV']\n\n        super(BinaryStarModel, self).triangle(params=params, **kwargs)"}
{"prompt": "def _check_not_hanging(self):\n        \"\"\"\n        Rough check that generate() will not hang or be very slow.\n\n        Raises ConfigurationError if generate() spends too much time in retry loop.\n        Issues a warning.warn() if there is a risk of slowdown.\n        \"\"\"", "output": "\n        if self.retry_on_timeout:\n            self.assertRaises(ConfigurationError, generate, self.config_path, self.config_dir)\n        else:\n            self.assertRaises(ConfigurationError, generate, self.config_path, self.config_dir,\n                          retry_on_timeout=self.retry_on_timeout)\n\n    def test_no_retry_on_timeout(self):\n        \"\"\"\n        Test that generate() will not hang or be very slow.\n\n        Raises ConfigurationError if generate() spends too much time in retry loop.\n        Issues a warning.warn() if there is", "solution": "\n        # (field_name, predicate, warning_msg, exception_msg)\n        # predicate(g) is a function that returns True if generated combination g must be rejected,\n        # see checks in generate()\n        checks = []\n        # ensure_unique can lead to infinite loops for some tiny erroneous configs\n        if self._ensure_unique:\n            checks.append((\n                _CONF.FIELD.ENSURE_UNIQUE,\n                self._ensure_unique,\n                lambda g: len(set(g)) != len(g),\n                '{generate} may be slow because a significant fraction of combinations contain repeating words and {field_name} is set',  # noqa\n                'Impossible to generate with {field_name}'\n            ))\n        #\n        # max_slug_length can easily slow down or block generation if set too small\n        if self._max_slug_length:\n            checks.append((\n                _CONF.FIELD.MAX_SLUG_LENGTH,\n                self._max_slug_length,\n                lambda g: sum(len(x) for x in g) + len(g) - 1 > self._max_slug_length,\n                '{generate} may be slow because a significant fraction of combinations exceed {field_name}={field_value}',  # noqa\n                'Impossible to generate with {field_name}={field_value}'\n            ))\n        # Perform the relevant checks for all generators, starting from 'all'\n        n = 100\n        warning_treshold = 20  # fail probability: 0.04 for 2 attempts, 0.008 for 3 attempts, etc.\n        for lst_id, lst in sorted(self._lists.items(), key=lambda x: '' if x is None else str(x)):\n            context = {'generate': 'coolname.generate({})'.format('' if lst_id is None else repr(lst_id))}\n            # For each generator, perform checks\n            for field_name, field_value, predicate, warning_msg, exception_msg in checks:\n                context.update({'field_name': field_name, 'field_value': field_value})\n                bad_count = 0\n                for i in range(n):\n                    g = lst[randrange(lst.length)]\n                    if predicate(g):\n                        bad_count += 1\n                if bad_count >= n:\n                    raise ConfigurationError(exception_msg.format(**context))\n                elif bad_count >= warning_treshold:\n                    import warnings\n                    warnings.warn(warning_msg.format(**context))"}
{"prompt": "def get_segment_definer_comments(xml_file, include_version=True):\n    \"\"\"Returns a dict with the comment column as the value for each segment\"\"\"", "output": "\n    xml_file.seek(0)\n    comment_dict = {}\n    for line in xml_file:\n        if line.startswith(u'COMMENT:'):\n            comment_dict[u'comment'] = line[len(u'COMMENT:'):]\n        elif line.startswith(u'VERSION:'):\n            comment_dict[u'version'] = line[len(u'VERSION:'):]\n        elif line.startswith(u'PAGESIZE:'):\n            comment_dict[u'pages'] = line[len(u'PAGESIZE:'):]\n        elif line.startswith(u'PAGESIZE_WITH_", "solution": "\n\n    from glue.ligolw.ligolw import LIGOLWContentHandler as h\n    lsctables.use_in(h)\n\n    # read segment definer table\n    xmldoc, _ = ligolw_utils.load_fileobj(xml_file,\n                                        gz=xml_file.name.endswith(\".gz\"),\n                                        contenthandler=h)\n    seg_def_table = table.get_table(xmldoc,\n                                    lsctables.SegmentDefTable.tableName)\n\n    # put comment column into a dict\n    comment_dict = {}\n    for seg_def in seg_def_table:\n        if include_version:\n            full_channel_name = ':'.join([str(seg_def.ifos),\n                                          str(seg_def.name),\n                                          str(seg_def.version)])\n        else:\n            full_channel_name = ':'.join([str(seg_def.ifos),\n                                          str(seg_def.name)])\n\n        comment_dict[full_channel_name] = seg_def.comment\n\n    return comment_dict"}
{"prompt": "def addRelationship(self, pid, subject, predicate, object, isLiteral=False,\n                        datatype=None):\n        \"\"\"\n        Wrapper function for `Fedora REST API addRelationship <https://wiki.duraspace.org/display/FEDORA34/REST+API#RESTAPI-addRelationship>`_\n\n        :param pid: persistent id for the object to add the new relationship to\n        :param subject: subject of the relationship; object or datastream URI\n        :param predicate: predicate of the new relationship\n        :param object: object of the relationship\n        :param isLiteral: true if object is literal, false if it is a URI;\n            Fedora has no default; this method defaults to False\n        :param datatype: optional datatype for literal objects\n\n        :returns: boolean success\n        \"\"\"", "output": "; False if subject is\n        not set\n        :return:\n        \"\"\"\n        if not isLiteral:\n            return self.addRelationship_uri(subject, object, pid, is_uri=True)\n        else:\n            return self.addRelationship_literal(predicate, datatype)\n\n    def addRelationship_literal(self, predicate, datatype):\n        \"\"\"\n        Add a new Fedora-compatible URI to a literal.\n\n        :param predicate: predicate of the new relationship\n        :param datatype: object of the new relationship\n        :return:\n        \"\"\"\n        if not isLiteral:\n            return self.addRelationship_literal_uri", "solution": "\n\n        http_args = {'subject': subject, 'predicate': predicate,\n                     'object': object, 'isLiteral': isLiteral}\n        if datatype is not None:\n            http_args['datatype'] = datatype\n\n        url = 'objects/%(pid)s/relationships/new' % {'pid': pid}\n        response = self.post(url, params=http_args)\n        return response.status_code == requests.codes.ok"}
{"prompt": "def splitall(path):\n    \"\"\"Split a path into a list of directory names (and optionally a filename).\n\n    Parameters\n    ----------\n    path: str\n        The path (absolute or relative).\n\n    Returns\n    -------\n    allparts: list[str]\n        List of directory names (and optionally a filename)\n\n    Example\n    -------\n    \"foo/bar/baz.py\" => [\"foo\", \"bar\", \"baz.py\"]\n    \"/absolute/path.py\" => [\"/\", \"absolute\", \"baz.py\"]\n\n    Notes\n    -----\n    Credit to Trent Mick. Taken from\n    https://www.safaribooksonline.com/library/view/python-cookbook/0596001673/ch04s16.html\n    \"\"\"", "output": "ffre-rumma.net/blog/python_tricks\n    \"\"\"\n    if not isinstance(path, list):\n        path = [path]\n\n    if isinstance(path[0], six.string_types):\n        path = [path]\n\n    for pathitem in path:\n        if pathitem.startswith('/'):\n            pathitem = pathitem[1:]\n        if pathitem.startswith('..'):\n            pathitem = pathitem[1:]\n        if pathitem.endswith('/'):\n            pathitem = pathitem[:-1]\n        if pathitem.endswith(os.sep):\n            pathitem = pathitem[:-1]\n       ", "solution": "\n    allparts = []\n    while True:\n        parts = os.path.split(path)\n        if parts[0] == path:  # sentinel for absolute paths\n            allparts.insert(0, parts[0])\n            break\n        elif parts[1] == path:  # sentinel for relative paths\n            allparts.insert(0, parts[1])\n            break\n        else:\n            path = parts[0]\n            allparts.insert(0, parts[1])\n    return allparts"}
{"prompt": "def kill_all(self, bIgnoreExceptions = False):\n        \"\"\"\n        Kills from all processes currently being debugged.\n\n        @type  bIgnoreExceptions: bool\n        @param bIgnoreExceptions: C{True} to ignore any exceptions that may be\n            raised when killing each process. C{False} to stop and raise an\n            exception when encountering an error.\n\n        @raise WindowsError: Raises an exception on error, unless\n            C{bIgnoreExceptions} is C{True}.\n        \"\"\"", "output": "\n        if bIgnoreExceptions:\n            self.ignore_exceptions = False\n        else:\n            self.ignore_exceptions = True\n        self.killed = True\n\n    def get_debugged_processes(self):\n        \"\"\"\n        Returns a list of processes that are currently being debugged.\n\n        @rtype:  list of L{Process}\n        @return: A list of processes that are currently being debugged.\n        \"\"\"\n        return self.debugged_processes\n\n    def get_pid_for_process(self, pid):\n        \"\"\"\n        Returns the process ID for a given process.\n\n        @type  pid: int\n       ", "solution": "\n        for pid in self.get_debugee_pids():\n            self.kill(pid, bIgnoreExceptions = bIgnoreExceptions)"}
{"prompt": "def update_subscription_user_settings(self, user_settings, subscription_id, user_id):\n        \"\"\"UpdateSubscriptionUserSettings.\n        [Preview API] Update the specified user's settings for the specified subscription. This API is typically used to opt in or out of a shared subscription. User settings can only be applied to shared subscriptions, like team subscriptions or default subscriptions.\n        :param :class:`<SubscriptionUserSettings> <azure.devops.v5_0.notification.models.SubscriptionUserSettings>` user_settings:\n        :param str subscription_id:\n        :param str user_id: ID of the user\n        :rtype: :class:`<SubscriptionUserSettings> <azure.devops.v5_0.notification.models.SubscriptionUserSettings>`\n        \"\"\"", "output": "\n        :return: None\n        \"\"\"\n        self._update_user_settings(user_settings, subscription_id, user_id)\n\n    def get_subscription_settings(self, subscription_id):\n        \"\"\"GetSubscriptionSettings.\n        [Preview API]\n\n        :param str subscription_id:\n        :return: None\n        \"\"\"\n        return self._get_user_settings(subscription_id,'subscriptions')\n\n    def get_user_settings(self, user_id):\n        \"\"\"GetUserSettings.\n        [Preview API]\n\n        :param str user_id:\n        :return: None\n        \"\"\"\n        return self", "solution": "\n        route_values = {}\n        if subscription_id is not None:\n            route_values['subscriptionId'] = self._serialize.url('subscription_id', subscription_id, 'str')\n        if user_id is not None:\n            route_values['userId'] = self._serialize.url('user_id', user_id, 'str')\n        content = self._serialize.body(user_settings, 'SubscriptionUserSettings')\n        response = self._send(http_method='PUT',\n                              location_id='ed5a3dff-aeb5-41b1-b4f7-89e66e58b62e',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('SubscriptionUserSettings', response)"}
{"prompt": "def get(self, group=None, backend=None):\n        \"\"\"Returns the corresponding Options object.\n\n        Args:\n            group: The options group. Flattens across groups if None.\n            backend: Current backend if None otherwise chosen backend.\n\n        Returns:\n            Options object associated with the object containing the\n            applied option keywords.\n        \"\"\"", "output": "\n        if group is None:\n            return self._options\n\n        if backend is None:\n            backend = self._backend\n\n        group = self._validate_group(group)\n        options = self._options.copy()\n        options.update(self._validate_keywords(backend))\n\n        return self._options.update(options)\n\n    def _validate_keywords(self, backend):\n        \"\"\"Validates the options group keywords.\n\n        Args:\n            backend: Current backend.\n\n        Returns:\n            A tuple of valid options and keywords.\n        \"\"\"\n        if backend is None:\n            return None, None, None\n\n        if backend.is_", "solution": "\n        from .options import Store, Options\n        keywords = {}\n        groups = Options._option_groups if group is None else [group]\n        backend = backend if backend else Store.current_backend\n        for group in groups:\n            optsobj = Store.lookup_options(backend, self._obj, group)\n            keywords = dict(keywords, **optsobj.kwargs)\n        return Options(**keywords)"}
{"prompt": "def upload(cls, path, project=None, parent=None, file_name=None,\n               overwrite=False, retry=5, timeout=10,\n               part_size=PartSize.UPLOAD_MINIMUM_PART_SIZE, wait=True,\n               api=None):\n        \"\"\"\n        Uploads a file using multipart upload and returns an upload handle\n        if the wait parameter is set to False. If wait is set to True it\n        will block until the upload is completed.\n\n        :param path: File path on local disc.\n        :param project: Project identifier\n        :param parent: Parent folder identifier\n        :param file_name: Optional file name.\n        :param overwrite: If true will overwrite the file on the server.\n        :param retry:  Number of retries if error occurs during upload.\n        :param timeout:  Timeout for http requests.\n        :param part_size:  Part size in bytes.\n        :param wait:  If true will wait for upload to complete.\n        :param api: Api instance.\n        \"\"\"", "output": " :param file_name: File name\n        :param overwrite: If True, overwrites an existing file if it exists.\n        :param retry: If 5, then retry if it fails.\n        :param timeout: If 10, then timeout if it fails.\n        :param part_size: Part size in bytes.\n        :return: Upload handle\n        \"\"\"\n        if api is None:\n            api = cls.api\n\n        if not project:\n            project = cls.project\n\n        if not parent:\n            parent = cls.folder\n\n        if not file_name:\n            file_name = cls.file_name\n\n        if not overwrite", "solution": "\n\n        api = api or cls._API\n        extra = {'resource': cls.__name__, 'query': {\n            'path': path,\n            'project': project,\n            'file_name': file_name,\n            'overwrite': overwrite,\n            'retry': retry,\n            'timeout': timeout,\n            'part_size': part_size,\n            'wait': wait,\n        }}\n        logger.info('Uploading file', extra=extra)\n\n        if not project and not parent:\n            raise SbgError('A project or parent identifier is required.')\n\n        if project and parent:\n            raise SbgError(\n                'Project and parent identifiers are mutually exclusive.'\n            )\n\n        if project:\n            project = Transform.to_project(project)\n\n        if parent:\n            parent = Transform.to_file(parent)\n\n        upload = Upload(\n            file_path=path, project=project, parent=parent,\n            file_name=file_name, overwrite=overwrite, retry_count=retry,\n            timeout=timeout, part_size=part_size, api=api\n        )\n        if wait:\n            upload.start()\n            upload.wait()\n            return upload\n        else:\n            return upload"}
{"prompt": "def is_grammar_generating(grammar, remove=False):\n        # type: (Grammar, bool) -> bool\n        \"\"\"\n        Check if is grammar is generating.\n        Generating grammar generates at least one sentence.\n        :param grammar: Grammar to check.\n        :param remove: True to remove nongenerating symbols from the grammar.\n        :return: True if is grammar generating, false otherwise.\n        \"\"\"", "output": "\n        if remove:\n            return self.remove_symbols(grammar)\n        else:\n            return self.is_nongenerating(grammar)\n\n    def is_grammar_generating_with_no_stopwords(self, grammar):\n        # type: (Grammar) -> bool\n        \"\"\"\n        Check if is grammar is generating with no stopwords.\n        :param grammar: Grammar to check.\n        :return: True if is grammar generating, false otherwise.\n        \"\"\"\n        return not is_stopwords(grammar)\n\n    def remove_symbols(self, grammar):\n        # type: (Grammar) -> None\n       ", "solution": "\n        g = ContextFree.remove_nongenerating_nonterminals(grammar, remove)\n        return g.start is not None"}
{"prompt": "def proc_response(self, resp):\n        \"\"\"Process JSON data found in the response.\"\"\"", "output": "\n        if resp.status_code == 200:\n            return json.loads(resp.text)\n        elif resp.status_code == 400:\n            raise ValueError('Invalid JSON', resp.text)\n        else:\n            raise ValueError('Unknown response', resp.status_code)\n\n    def get_json(self, url, params=None):\n        \"\"\"Get the JSON from the given URL.\"\"\"\n        if params is None:\n            params = {}\n        resp = self.session.get(url, params=params, timeout=self.timeout)\n        self.raise_for_status(resp)\n        return resp.json", "solution": "\n\n        # Try to interpret any JSON\n        try:\n            resp.obj = json.loads(resp.body)\n            self._debug(\"  Received entity: %r\", resp.obj)\n        except ValueError:\n            resp.obj = None\n            self._debug(\"  No received entity; body %r\", resp.body)\n\n        # Now, call superclass method for error handling\n        super(JSONRequest, self).proc_response(resp)"}
{"prompt": "def find_package_dirs(root_path):\n    \"\"\" Find python package directories in directory `root_path`\n\n    Parameters\n    ----------\n    root_path : str\n        Directory to search for package subdirectories\n\n    Returns\n    -------\n    package_sdirs : set\n        Set of strings where each is a subdirectory of `root_path`, containing\n        an ``__init__.py`` file.  Paths prefixed by `root_path`\n    \"\"\"", "output": "\n    from os.path import dirname, join, exists\n    this_dir = dirname(root_path)\n    packages = set()\n    for path, dirs, files in walk_python(root_path):\n        if not exists(path):\n            continue\n        if path.startswith(this_dir) and not exists(join(path, '__init__.py')):\n            packages.add(path[len(this_dir):])\n        for f in files:\n            if not exists(join(path, f)):\n                continue\n            if f.endswith('.py'):\n                packages.add(f[len(this_dir", "solution": "\n    package_sdirs = set()\n    for entry in os.listdir(root_path):\n        fname = entry if root_path == '.' else pjoin(root_path, entry)\n        if isdir(fname) and exists(pjoin(fname, '__init__.py')):\n            package_sdirs.add(fname)\n    return package_sdirs"}
{"prompt": "def read(self, from_item=None, to_item=None,\n             from_time=None, to_time=None):\n        \"\"\"Retrieve requested data coordinates from the h5features index.\n\n        :param str from_item: Optional. Read the data starting from\n            this item. (defaults to the first stored item)\n\n        :param str to_item: Optional. Read the data until reaching the\n            item. (defaults to from_item if it was specified and to\n            the last stored item otherwise).\n\n        :param float from_time: Optional. (defaults to the beginning\n            time in from_item) The specified times are included in the\n            output.\n\n        :param float to_time: Optional. (defaults to the ending time\n            in to_item) the specified times are included in the\n            output.\n\n        :return: An instance of h5features.Data read from the file.\n\n        \"\"\"", "output": "_item) Time of the first feature.\n\n        :param float to_time: Optional. (defaults to the end\n            time in to_item).\n\n        :returns: A list of (x, y, z) coordinates.\n        \"\"\"\n        if from_item is not None:\n            return self.h5features[from_item].read(from_time, to_item)\n        elif to_item is not None:\n            return self.h5features[to_item].read()\n        else:\n            return self.h5features[0].read()\n\n    def write(self, to_item=", "solution": "\n        # handling default arguments\n        if to_item is None:\n            to_item = self.items.data[-1] if from_item is None else from_item\n        if from_item is None:\n            from_item = self.items.data[0]\n\n        # index coordinates of from/to_item. TODO optimize because we\n        # have 4 accesses to list.index() where 2 are enougth.\n        if not self.items.is_valid_interval(from_item, to_item):\n            raise IOError('cannot read items: not a valid interval')\n        from_idx = self.items.data.index(from_item)\n        to_idx = self.items.data.index(to_item)\n\n        from_pos = self._get_item_position(from_idx)\n        to_pos = self._get_item_position(to_idx)\n\n        lower = self._get_from_time(from_time, from_pos)\n        # upper included with +1\n        upper = self._get_to_time(to_time, to_pos) + 1\n\n        # Step 2: access actual data\n        if self.dformat == 'sparse':\n            raise NotImplementedError(\n                'Reading sparse features not implemented')\n        else:\n            features = (self.group['features'][:, lower:upper].T\n                        if self.version == '0.1'\n                        else self.group['features'][lower:upper, ...])\n            labels = self._labels_group[lower:upper]\n\n        # If we read a single item\n        if to_idx == from_idx:\n            features = [features]\n            labels = [labels]\n        # Several items case: split them from the index\n        else:\n            item_ends = self._index[from_idx:to_idx] - from_pos[0] + 1\n            features = np.split(features, item_ends, axis=0)\n            labels = np.split(labels, item_ends, axis=0)\n\n        items = self.items.data[from_idx:to_idx + 1]\n        if self.properties is None:\n            properties = None\n        else:\n            properties = self.properties[from_idx:to_idx + 1]\n\n        return Data(\n            items, labels, features, properties=properties, check=False)"}
{"prompt": "def next(self):\n        \"\"\"Get the next segment.\"\"\"", "output": "\n        return self._next_segment()\n\n    def _next_segment(self):\n        \"\"\"Get the next segment.\"\"\"\n        return self._next_segment_from_file(self._file_path)\n\n    def _next_segment_from_file(self, file_path):\n        \"\"\"Return the next segment from a file.\"\"\"\n        with open(file_path, 'rb') as f:\n            return f.read(self._max_file_size)\n\n    def _next_segment_from_file_path(self, file_path):\n        \"\"\"Return the next segment from a file path.\"\"\"\n       ", "solution": "\n        seg = Segment(segment_t=idaapi.get_next_seg(self.ea))\n\n        if seg.ea <= self.ea:\n            raise exceptions.NoMoreSegments(\"This is the last segment. No segments exist after it.\")\n\n        return seg"}
{"prompt": "def urlopen(url, timeout=20, redirects=None):\n    \"\"\"A minimal urlopen replacement hack that supports timeouts for http.\n       Note that this supports GET only.\"\"\"", "output": "\n    import urllib2\n    import socket\n    import httplib\n    return urllib2.urlopen(url, timeout=timeout, redirects=redirects)\n\n\ndef get_http_response(url, timeout=20):\n    \"\"\"A minimal http response replacement hack that supports timeouts for http.\n       Note that this supports GET only.\"\"\"\n    import httplib\n    return httplib.HTTPResponse(None, None, None, None, None)\n\n\ndef get_http_response_error(url, timeout=20):\n    \"\"\"A minimal http response replacement hack that supports timeouts for http.\n       Note that this supports GET only.\"\"\"\n    import httplib\n    return httplib", "solution": "\n    scheme, host, path, params, query, frag = urlparse(url)\n\n    if not scheme in ('http', 'https'):\n        return urllib.urlopen(url)\n    if params: path = '%s;%s' % (path, params)\n    if query:  path = '%s?%s' % (path, query)\n    if frag:   path = '%s#%s' % (path, frag)\n\n    if scheme == 'https':\n        # If ssl is not compiled into Python, you will not get an exception\n        # until a conn.endheaders() call.   We need to know sooner, so use\n        # getattr.\n        try:\n            import M2Crypto\n        except ImportError:\n            if not hasattr(socket, 'ssl'):\n                raise RuntimeError, 'no built-in SSL Support'\n\n            conn = TimeoutHTTPS(host, None, timeout)\n        else:\n            ctx = M2Crypto.SSL.Context()\n            ctx.set_session_timeout(timeout)\n            conn = M2Crypto.httpslib.HTTPSConnection(host, ssl_context=ctx)\n            conn.set_debuglevel(1)\n\n    else:\n        conn = TimeoutHTTP(host, None, timeout)\n\n    conn.putrequest('GET', path)\n    conn.putheader('Connection', 'close')\n    conn.endheaders()\n    response = None\n    while 1:\n        response = conn.getresponse()\n        if response.status != 100:\n            break\n        conn._HTTPConnection__state = httplib._CS_REQ_SENT\n        conn._HTTPConnection__response = None\n\n    status = response.status\n\n    # If we get an HTTP redirect, we will follow it automatically.\n    if status >= 300 and status < 400:\n        location = response.msg.getheader('location')\n        if location is not None:\n            response.close()\n            if redirects is not None and redirects.has_key(location):\n                raise RecursionError(\n                    'Circular HTTP redirection detected.'\n                    )\n            if redirects is None:\n                redirects = {}\n            redirects[location] = 1\n            return urlopen(location, timeout, redirects)\n        raise HTTPResponse(response)\n\n    if not (status >= 200 and status < 300):\n        raise HTTPResponse(response)\n\n    body = StringIO(response.read())\n    response.close()\n    return body"}
{"prompt": "def next_pos(self):\n        \"\"\"\n        :return: The next position tuple (x, y) for the Sprite on this path.\n        \"\"\"", "output": "\n        return self.x, self.y + 1\n\n    def __repr__(self):\n        return \"Path(x=%d, y=%d)\" % (self.x, self.y)\n", "solution": "\n        result = None\n        if self._index <= len(self._steps):\n            result = self._steps[self._index]\n            self._index += 1\n        return result"}
{"prompt": "def write_outro (self, interrupt=False):\n        \"\"\"Write end of checking message.\"\"\"", "output": "\n        if interrupt:\n            self.interrupt_message = True\n        else:\n            self.interrupt_message = False\n        self.write_message (self.message)\n\n    def write_outro_with_delay (self, interrupt=False):\n        \"\"\"Write out a message with delay.\"\"\"\n        if interrupt:\n            self.interrupt_message = True\n        else:\n            self.interrupt_message = False\n        self.message = self.outro_delay_message\n        self.outro_delay_message = 0\n        self.write_message (self.outro_delay_message)\n\n    def write_out", "solution": "\n        self.writeln()\n        if interrupt:\n            self.writeln(_(\"The check has been interrupted; results are not complete.\"))\n        self.write(_(\"That's it.\") + \" \")\n        self.write(_n(\"%d link\", \"%d links\",\n                      self.stats.number) % self.stats.number)\n        self.write(u\" \")\n        if self.stats.num_urls is not None:\n            self.write(_n(\"in %d URL\", \"in %d URLs\",\n                          self.stats.num_urls) % self.stats.num_urls)\n        self.write(u\" checked. \")\n        warning_text = _n(\"%d warning found\", \"%d warnings found\",\n             self.stats.warnings_printed) % self.stats.warnings_printed\n        if self.stats.warnings_printed:\n            warning_color = self.colorwarning\n        else:\n            warning_color = self.colorinfo\n        self.write(warning_text, color=warning_color)\n        if self.stats.warnings != self.stats.warnings_printed:\n            self.write(_(\" (%d ignored or duplicates not printed)\") %\n                (self.stats.warnings - self.stats.warnings_printed))\n        self.write(u\". \")\n        error_text = _n(\"%d error found\", \"%d errors found\",\n             self.stats.errors_printed) % self.stats.errors_printed\n        if self.stats.errors_printed:\n            error_color = self.colorinvalid\n        else:\n            error_color = self.colorvalid\n        self.write(error_text, color=error_color)\n        if self.stats.errors != self.stats.errors_printed:\n            self.write(_(\" (%d duplicates not printed)\") %\n                (self.stats.errors - self.stats.errors_printed))\n        self.writeln(u\".\")\n        num = self.stats.internal_errors\n        if num:\n            self.writeln(_n(\"There was %(num)d internal error.\",\n                \"There were %(num)d internal errors.\", num) % {\"num\": num})\n        self.stoptime = time.time()\n        duration = self.stoptime - self.starttime\n        self.writeln(_(\"Stopped checking at %(time)s (%(duration)s)\") %\n             {\"time\": strformat.strtime(self.stoptime),\n              \"duration\": strformat.strduration_long(duration)})"}
{"prompt": "def _return_tables(self, mag, imt, val_type):\n        \"\"\"\n        Returns the vector of ground motions or standard deviations\n        corresponding to the specific magnitude and intensity measure type.\n\n        :param val_type:\n            String indicating the type of data {\"IMLs\", \"Total\", \"Inter\" etc}\n        \"\"\"", "output": "\n        if val_type == \"IMLs\":\n            return self._get_imls_total_inter(mag)\n        elif val_type == \"Total\":\n            return self._get_total_table(mag)\n        elif val_type == \"Inter\":\n            return self._get_inter_table(mag)\n        else:\n            raise ValueError(\"Unknown value type: %s\" % (val_type))\n\n    def _get_imls_total_inter(self, mag):\n        \"\"\"\n        Returns the ground motion measure of the given magnitude.\n\n        :param mag:\n            The magnitude to measure.", "solution": "\n        if imt.name in 'PGA PGV':\n            # Get scalar imt\n            if val_type == \"IMLs\":\n                iml_table = self.imls[imt.name][:]\n            else:\n                iml_table = self.stddevs[val_type][imt.name][:]\n            n_d, n_s, n_m = iml_table.shape\n            iml_table = iml_table.reshape([n_d, n_m])\n        else:\n            if val_type == \"IMLs\":\n                periods = self.imls[\"T\"][:]\n                iml_table = self.imls[\"SA\"][:]\n            else:\n                periods = self.stddevs[val_type][\"T\"][:]\n                iml_table = self.stddevs[val_type][\"SA\"][:]\n\n            low_period = round(periods[0], 7)\n            high_period = round(periods[-1], 7)\n            if (round(imt.period, 7) < low_period) or (\n                    round(imt.period, 7) > high_period):\n                raise ValueError(\"Spectral period %.3f outside of valid range \"\n                                 \"(%.3f to %.3f)\" % (imt.period, periods[0],\n                                                     periods[-1]))\n            # Apply log-log interpolation for spectral period\n            interpolator = interp1d(numpy.log10(periods),\n                                    numpy.log10(iml_table),\n                                    axis=1)\n            iml_table = 10. ** interpolator(numpy.log10(imt.period))\n        return self.apply_magnitude_interpolation(mag, iml_table)"}
{"prompt": "def execute_macros(self):\n        \"\"\"Executes macros and marks grid as changed\"\"\"", "output": "\n        if self.macro_list:\n            self.macro_list = []\n            for macro in self.macro_list:\n                self.macro_list.append(macro)\n            self.macro_list.append(' ')\n            self.macro_list.append(self.get_macro_list())\n            self.macro_list.append('\\n')\n            self.macro_list.append(self.mark_macro_as_changed)\n\n    def get_macro_list(self):\n        \"\"\"Returns the list of changed macros\"\"\"\n        return self.macro_list\n\n    def get_grid_list(", "solution": "\n\n        # Mark content as changed\n        post_command_event(self.main_window, self.ContentChangedMsg)\n\n        (result, err) = self.grid.code_array.execute_macros()\n\n        # Post event to macro dialog\n        post_command_event(self.main_window, self.MacroErrorMsg,\n                           msg=result, err=err)"}
{"prompt": "def delete_channel(self, channel_id):\n        \"\"\"Deletes channel\n        \"\"\"", "output": "\n        self.logger.info(\"Deleting channel %s\" % channel_id)\n        self.channel_manager.delete_channel(channel_id)\n        self.logger.info(\"Deleted channel %s\" % channel_id)\n\n    def get_channel_list(self, channel_id):\n        \"\"\"Gets a list of channels\n        \"\"\"\n        self.logger.info(\"Getting list of channels %s\" % channel_id)\n        self.channel_manager.get_channel_list(channel_id)\n        return self.channel_manager.get_channel_list(channel_id)\n\n    def", "solution": "\n        req = requests.delete(self.channel_path(channel_id))\n        return req"}
{"prompt": "def tomindecstr(origin):\n    \"\"\"\n    Convert [+/-]DDD.DDDDD to [+/-]DDD\u00b0MMM.MMMM'\n    \"\"\"", "output": "\n    return '%s %s' % (origin.strftime('%Y%m%d'), to_decimal(MMMM_DDDMMM) if origin else '0.00', )\n\n\ndef to_decimal(value):\n    \"\"\"\n    Convert [+/-]MMM.MMM to [+/-]DDD.DDDDD\n    \"\"\"\n    return value.replace('.', ',')\n\n\ndef to_date(value):\n    \"\"\"\n    Convert [+/-]DDD.DDD to [+/-]DDD\u00b0\n    \"\"\"\n    return to_decimal(value)\n\n\ndef to", "solution": "\n\n    degrees, minutes = tomindec(origin)\n    return u'%d\u00b0%f\\'' % (degrees, minutes)"}
{"prompt": "def dump(self, data, stream=None):\n        \"\"\"Encrypt data and print stdout or write to stream.\"\"\"", "output": "\n        if stream is None:\n            stream = sys.stdout\n        if self.encrypt:\n            data = self.encrypt(data)\n        print(data)\n        if stream == sys.stdout:\n            if self.encrypt:\n                stream.write(self.data)\n            else:\n                stream.write(self.data.encode('utf-8'))\n        elif self.encrypt:\n            data = self.data\n        else:\n            raise ValueError('No encryption available for this object')\n        if stream == sys.stdout:\n            stream.write(self.stdout_encrypted)\n        elif self.encrypt:\n            stream.write", "solution": "\n        yaml_text = yaml.dump(\n            data,\n            default_flow_style=False,\n            allow_unicode=True)\n        return self.dump_raw(yaml_text, stream=stream)"}
{"prompt": "def check_code(self, card_id, codes):\n        \"\"\"\n        \u6838\u67e5code\n        \"\"\"", "output": "\n        if not codes:\n            return False\n        if len(codes) == 1:\n            return codes[0] == card_id\n        else:\n            return True\n\n    def get_card_name(self, codes):\n        \"\"\"\n        \u83b7\u53d6\u540d\u5b57\n        \"\"\"\n        if not codes:\n            return None\n        if len(codes) == 1:\n            return codes[0]\n        else:\n            return self.code_to_name(codes[0])\n\n    def code_to_name(self, code):\n        \"\"\"\n        \u83b7\u53d6\u540d\u5b57\u4e2d\u7684\u5b57\u7b26\u4e32\n        \"\"\"\n        if", "solution": "\n        card_data = {\n            'card_id': card_id,\n            'code': codes\n        }\n        return self._post(\n            'card/code/checkcode',\n            data=card_data\n        )"}
{"prompt": "def is_instance_running(self, instance_id):\n        \"\"\"Checks if the instance is up and running.\n\n        :param str instance_id: instance identifier\n\n        :return: bool - True if running, False otherwise\n        \"\"\"", "output": "\n        return self._is_instance_running(instance_id)\n\n    def _is_instance_running(self, instance_id):\n        \"\"\"Checks if the instance is up and running.\n\n        :param str instance_id: instance identifier\n\n        :return: bool - True if running, False otherwise\n        \"\"\"\n        return instance_id in self._get_instance_ids()\n\n    def _get_instance_ids(self):\n        \"\"\"Gets the IDs of all the instances in the cluster.\n\n        :return: list - list of cluster IDs\n        \"\"\"\n        return self._cluster_spec.get_cluster_ids()", "solution": "\n        instance = self._load_instance(instance_id)\n\n        if instance.update() == \"running\":\n            # If the instance is up&running, ensure it has an IP\n            # address.\n            if not instance.ip_address and self.request_floating_ip:\n                log.debug(\"Public ip address has to be assigned through \"\n                          \"elasticluster.\")\n                self._allocate_address(instance)\n                instance.update()\n            return True\n        else:\n            return False"}
{"prompt": "def manage_rep(self, rep_json):\n        \"\"\"\n        Import a rep as a representative from the json dict fetched from\n        FranceData (which comes from nosdeputes.fr)\n        \"\"\"", "output": "\n        self.rep_json = rep_json\n        self.nutrition_id = self.rep_json['nutrition_id']\n        self.nutrition_name = self.rep_json['nutrition_name']\n        self.nutrition_type = self.rep_json['nutrition_type']\n        self.nutrition_id_name = self.rep_json['nutrition_id_name']\n        self.nutrition_id_type = self.rep_json['nutrition", "solution": "\n\n        # Some versions of memopol will connect to this and skip inactive reps.\n        responses = representative_pre_import.send(sender=self,\n                representative_data=rep_json)\n\n        for receiver, response in responses:\n            if response is False:\n                logger.debug(\n                    'Skipping MEP %s', rep_json['nom'])\n                return\n\n        changed = False\n        slug = slugify('%s-%s' % (\n            rep_json['nom'] if 'nom' in rep_json\n            else rep_json['prenom'] + \" \" + rep_json['nom_de_famille'],\n            _parse_date(rep_json[\"date_naissance\"])\n        ))\n\n        try:\n            representative = Representative.objects.get(slug=slug)\n        except Representative.DoesNotExist:\n            representative = Representative(slug=slug)\n            changed = True\n\n        if rep_json['num_circo'] == 'non disponible':\n            rep_json['num_circo'] = 'nd'\n\n        # Save representative attributes\n        self.import_representative_details(representative, rep_json, changed)\n\n        self.add_mandates(representative, rep_json)\n\n        self.add_contacts(representative, rep_json)\n\n        logger.debug('Imported MEP %s', unicode(representative))\n\n        return representative"}
{"prompt": "def _colorize_bars(self, cmap, bars, element, main_range, dim):\n        \"\"\"\n        Use the given cmap to color the bars, applying the correct\n        color ranges as necessary.\n        \"\"\"", "output": "\n        if element == 'bar':\n            return cmap(self.bars_color)\n        elif element == 'bar_low':\n            return cmap(self.bars_low_color)\n        elif element == 'bar_high':\n            return cmap(self.bars_high_color)\n        else:\n            return element\n\n    def _colorize_bars_low(self, element, main_range, dim):\n        \"\"\"\n        Use the given element to color the bars, applying the correct\n        color ranges as necessary.\n        \"\"\"\n        if element == 'bar':\n            return self.bars_low_color\n        elif element ==", "solution": "\n        cmap_range = main_range[1] - main_range[0]\n        lower_bound = main_range[0]\n        colors = np.array(element.dimension_values(dim))\n        colors = (colors - lower_bound) / (cmap_range)\n        for c, bar in zip(colors, bars):\n            bar.set_facecolor(cmap(c))\n            bar.set_clip_on(False)"}
{"prompt": "def spliceX(self):\n        \"\"\"\n        Return a copy with the x values preferred for ambivalent locations.\n        ::\n\n            >>> l = Location(pop=(-5,5))\n            >>> l.spliceX()\n            <Location pop:-5 >\n        \"\"\"", "output": "\n        return Location(pop=(self.pop, self.pop+1))\n\n    def spliceY(self):\n        \"\"\"\n        Return a copy with the y values preferred for ambivalent locations.\n        ::\n\n            >>> l = Location(pop=(5,5))\n            >>> l.spliceY()\n            <Location pop:-5 >\n        \"\"\"\n        return Location(pop=(self.pop, self.pop+1))\n\n    def __str__(self):\n        \"\"\"\n        Return a string representation of the location.\n        ::\n\n            >>> l = Location(pop=(5,5))\n            >>> l.__str__()\n            '", "solution": "\n        new = self.__class__()\n        for dim, val in self.items():\n            if isinstance(val, tuple):\n                new[dim] = val[0]\n            else:\n                new[dim] = val\n        return new"}
{"prompt": "def refresh_gui(delay=0.0001, wait=0.0001):\n    \"\"\"Use up all the events waiting to be run\n\n    :param delay: Time to wait before using events\n    :param wait: Time to wait between iterations of events\n\n    This function will block until all pending events are emitted. This is\n    useful in testing to ensure signals and other asynchronous functionality\n    is required to take place.\n    \"\"\"", "output": "\n    while True:\n        if wait > 0:\n            time.sleep(delay)\n        else:\n            break\n    return time.time() - start_time\n\n\ndef wait_for_events(timeout=0.0001, wait=0.0001):\n    \"\"\"Use up all the events waiting to be run\n\n    :param timeout: Time to wait between iterations of events\n\n    This function will block until all pending events are emitted. This is\n    useful in testing to ensure signals and other asynchronous functionality\n    is required to take place.\n    \"\"\"\n    while True:\n        if timeout > 0:\n            time.sleep(timeout)\n       ", "solution": "\n    time.sleep(delay)\n    while gtk.events_pending():\n        gtk.main_iteration_do(block=False)\n        time.sleep(wait)"}
{"prompt": "def get_go2color_inst(self, hdrgo):\n        \"\"\"Get a copy of go2color with GO group header colored.\"\"\"", "output": "\n        return self.go2color.copy()\n\n    def get_go2color_name(self, hdrgo):\n        \"\"\"Get a copy of go2color with GO group header colored.\"\"\"\n        return self.name2color.copy()\n\n    def get_go2color_inst(self, hdrgo):\n        \"\"\"Get a copy of go2color with GO group header colored.\"\"\"\n        return self.go2color.copy()\n\n    def get_go2color_name(self, hdrgo):\n        \"\"\"Get a copy of go2color with GO group header colored.\"\"\"\n        return self.name", "solution": "\n        go2color = self.go2color.copy()\n        go2color[hdrgo] = self.hdrgo_dflt_color\n        return go2color"}
{"prompt": "def validate_wrap(self, value):\n        \"\"\" Checks that the correct number of elements are in ``value`` and that\n            each element validates agains the associated Field class\n        \"\"\"", "output": "\n        if isinstance(value, dict):\n            for key, value in value.iteritems():\n                if not validate_wrap(value):\n                    return False\n            return True\n        return False\n\n    def get_field_options(self, field):\n        \"\"\" Gets the possible values for a given field as a dictionary\n            This method returns a dictionary with all available options for the given\n            field\n        \"\"\"\n\n        # Get the possible values for this field\n        options = self.get_field_options_obj(field)\n\n        # Validate the wrap\n        wrap = self.validate_wrap(options.get(\"wrap\", None))\n        if wrap:", "solution": "\n        if not isinstance(value, list) and not isinstance(value, tuple):\n            self._fail_validation_type(value, tuple, list)\n\n        for field, value in izip(self.types, list(value)):\n            field.validate_wrap(value)"}
{"prompt": "def create_mask(N, method='all', nmissing=0):\n    \"\"\" Create weight mask according to method.\n\n    :param N: Dimension of square weight matrix.\n    :param method: Method to use (default: 'all').\n    - none: no missing entries (only diagonal is set to 0 for dwMDS)\n    - first: only randomly delete measurements to first point (zeros in first row/column of matrix)\n    - all: randomly delete measurements in whole matrix\n    :param nmissing: Number of deleted measurements, used by methods 'first' and 'all'\n\n    :return: Binary weight mask.\n    :rtype: numpy.ndarray\n    \"\"\"", "output": ".\n    \"\"\"\n    if method == 'all':\n        return np.zeros((N, N), dtype=np.uint8)\n    elif method == 'first':\n        return np.zeros((N, N), dtype=np.uint8)\n    elif method == 'random':\n        return np.random.randint(0, 2, size=N, dtype=np.uint8)\n    elif method == 'all':\n        return np.zeros((N, N), dtype=np.uint8)\n    else:\n        raise ValueError('Unknown value for method: %s' % method)\n\n\ndef create_", "solution": "\n\n    weights = np.ones((N, N))\n    weights[range(N), range(N)] = 0\n\n    if method == 'none':\n        return weights\n\n    # create indices object to choose from\n    elif method == 'all':\n        all_indices = np.triu_indices(N, 1)\n    elif method == 'first':\n        all_indices = [np.zeros(N - 1).astype(np.int),\n                       np.arange(1, N).astype(np.int)]\n    ntotal = len(all_indices[0])\n    # randomly choose from indices and set to 0\n    choice = np.random.choice(ntotal, nmissing, replace=False)\n    chosen = [all_indices[0][choice], all_indices[1][choice]]\n    weights[chosen] = 0\n    weights[chosen[1], chosen[0]] = 0\n    return weights"}
{"prompt": "def set_max_threads(self, max_threads):\n        \"\"\"\n        Set the maximum number of concurrent threads.\n\n        :type  max_threads: int\n        :param max_threads: The number of threads.\n        \"\"\"", "output": "\n        self._max_threads = max_threads\n\n    def set_max_processes(self, max_processes):\n        \"\"\"\n        Set the maximum number of processes.\n\n        :type  max_processes: int\n        :param max_processes: The number of processes.\n        \"\"\"\n        self._max_processes = max_processes\n\n    def set_max_threads_per_process(self, max_threads):\n        \"\"\"\n        Set the maximum number of concurrent threads per process.\n\n        :type  max_threads: int\n        :param max_threads: The number of threads per process.\n        \"\"\"\n        self._max_", "solution": "\n        if max_threads is None:\n            raise TypeError('max_threads must not be None.')\n        self._check_if_ready()\n        self.collection.set_max_working(max_threads)"}
{"prompt": "def from_dict(cls, d):\n        \"\"\"\n        Create an instance from a dictionary.\n        \"\"\"", "output": "\n        return cls(d[\"name\"], d[\"type\"], d[\"fields\"])\n\n    def to_dict(self):\n        \"\"\"\n        Convert the dictionary to a dictionary.\n        \"\"\"\n        return {\n            \"name\": self.name,\n            \"type\": self.type,\n            \"fields\": self.fields,\n        }\n\n    def __repr__(self):\n        return \"Field(name={}, type={}, fields={})\".format(\n            self.name, self.type, self.fields,\n        )\n\n\nclass Field(object):\n    \"\"\"\n    A field in a table.\n    \"\"\"\n\n    def __init__(self, name,", "solution": "\n        instance = super(Simulation, cls).from_dict(d)\n        #  The instance's input_files and cmd_line_args members still point to data structures in the original\n        #  dictionary.  Copy them to avoid surprises if they are changed in the original dictionary.\n        instance.input_files = dict(instance.input_files)\n        instance.cmd_line_args = list(instance.cmd_line_args)\n        return instance"}
{"prompt": "def wait_for_ready(self, timeout=None, times=None, delay=None, delay_between=None, abort=None):\n        \"\"\"Determine the ready state of the device and wait until device is ready.\n\n        Parameters\n        ----------\n        timeout : int, float\n            The maximum amount of time to wait in seconds. Reaching the timeout will raise a RuntimeError.\n        times : int\n            Maximum number of times reading the ready state.\n        delay : int, float\n            The number of seconds to sleep before checks. Defaults to 0.\n        delay_between : int, float\n            The number of seconds to sleep between each check. Defaults to 0.\n        abort : Threading.Event\n            Breaking the loop from other threads.\n\n        Returns\n        -------\n        True if state is ready, else False.\n        \"\"\"", "output": " read. Defaults to 0.\n        abort : bool\n            If True, the device will be closed and the function will return True if the\n            device is ready.\n        \"\"\"\n        if timeout is None:\n            timeout = self.timeout\n        if times is None:\n            times = self.times\n        if delay_between is None:\n            delay_between = self.delay_between\n        if abort is None:\n            abort = self.abort\n        if delay_between is None:\n            raise RuntimeError(\"Cannot determine the time between reads.\")\n        if abort:\n            time.sleep(delay_between)\n        if delay:\n            time.sleep", "solution": "\n        if delay:\n            try:\n                sleep(delay)\n            except IOError:  # negative values\n                pass\n        if timeout is not None:\n            if timeout < 0:\n                raise ValueError(\"timeout is smaller than 0\")\n            else:\n                stop = time() + timeout\n        times_checked = 0\n        while not self.is_ready:\n            now = time()\n            times_checked += 1\n            if abort and abort.is_set():\n                False\n            if timeout is not None and stop <= now:\n                raise RuntimeError('Time out while waiting for ready in %s, module %s' % (self.name, self.__class__.__module__))\n            if times and times > times_checked:\n                False\n            if delay_between:\n                try:\n                    sleep(delay_between)\n                except IOError:  # negative values\n                    pass\n        return True"}
{"prompt": "def get_bss_load(_, data):\n    \"\"\"http://git.kernel.org/cgit/linux/kernel/git/jberg/iw.git/tree/scan.c?id=v3.17#n935.\n\n    Positional arguments:\n    data -- bytearray data to read.\n\n    Returns:\n    Dict.\n    \"\"\"", "output": "\n    return _bss_load_helper(data, _bss_scan_bss_load)\n\n\ndef _bss_load_helper(data, bss_load_helper):\n    \"\"\"Helper function for reading BSS data.\"\"\"\n    if data[0] == 0x00:\n        return bss_load_helper\n    elif data[0] == 0x01:\n        return bss_load_helper_data\n    elif data[0] == 0x02:\n        return bss_load_helper_data_0\n    elif data[0] == 0x03:\n       ", "solution": "\n    answers = {\n        'station count': (data[1] << 8) | data[0],\n        'channel utilisation': data[2] / 255.0,\n        'available admission capacity': (data[4] << 8) | data[3],\n    }\n    return answers"}
{"prompt": "def case_comments(self):\n        \"\"\"Return only comments made on the case.\"\"\"", "output": "\n        return self.comments.filter(\n            Q(user=self.user) | Q(user_email=self.user.email) |\n            Q(user_email=self.user.email) | Q(user_name=self.user.name)\n        ).order_by('-created')\n\n    def get_absolute_url(self):\n        \"\"\"Return the absolute URL for this case.\"\"\"\n        return reverse('case:details', args=[self.id])\n\n    def get_case_type(self):\n        \"\"\"Return the case type.\"\"\"\n        return self.case_type\n\n    def get_case", "solution": "\n        comments = (comment for comment in self.comments if\n                    comment.variant_id is None)\n        return comments"}
{"prompt": "def run(self, globs=None):\n        \"\"\"Import user module and start __main__\n           passing globals() is required when subclassing in another module\n        \"\"\"", "output": "\n        if globs is None:\n            frame = sys._getframe(1)\n            globs = frame.f_globals\n            if frame.f_globals is None:\n                globs = globals()\n            globs.update(frame.f_globals)\n        else:\n            globs = globs.copy()\n        globs.update(self.distribution.get_option_dict('sconsign', section='sconsign'))\n        if self.ext_modules:\n            globs.update(self.ext_modules)\n        if self.include_package_data:\n            self.announce(\"include_package_data\")\n            self.include", "solution": "\n        # Without this, the underneath import clashes with the top-level one\n        global scoop\n\n        if globs is None:\n            globs = globals()\n\n        # import the user module\n        if scoop.MAIN_MODULE:\n            globs.update(self.setupEnvironment(self))\n\n        # Start the user program\n        from scoop import futures\n\n        def futures_startup():\n            "}
{"prompt": "def loadFeatures(path, opt):\n    \"\"\"\n    Load features. For BED, only BED4 columns are loaded.\n    For narrowPeak, all columns are loaded.\n    \"\"\"", "output": "\n    if opt.verbose:\n        print \"Loading %s\" % path\n\n    # Load the BED file\n    if opt.verbose:\n        print \"Loading BED file\"\n\n    # Load the narrowPeak file\n    if opt.narrowPeak:\n        f = open(path, \"r\")\n        f.readline()\n        f.readline()\n        f.readline()\n        f.readline()\n        f.readline()\n        f.readline()\n        f.readline()\n        f.readline()\n        f.readline()\n        f.readline()\n        f.readline()\n        f.readline()\n        f.readline()\n        f.", "solution": "\n\n    log.info(\"loading from %s ...\" % path)\n    data = []\n    if opt.in_format == \"BED\":        \n        with open(path) as fd:\n            for line in fd:\n                cols = line.split()\n                data.append( (cols[0], int(cols[1]), int(cols[2]), cols[3]) )\n        data = np.array(data, dtype=elem_t)\n    else:\n        with open(path) as fd:\n            for line in fd:\n                cols = line.split()\n                data.append( (cols[0], int(cols[1]), int(cols[2]), cols[3], int(cols[4]),\n                              cols[5], float(cols[6]), float(cols[7]), float(cols[8]),\n                              int(cols[-1])+int(cols[1])) )\n        data = np.array(data, dtype=narrowPeak_t)\n    return data"}
{"prompt": "def repack_all(self):\n        \"\"\"Repacks the side chains of all Polymers in the Assembly.\"\"\"", "output": "\n        self.repack_side_chains()\n        self.repack_side_chains_by_name()\n\n    def repack_side_chains(self):\n        \"\"\"Repacks the side chains of all Polymers in the Assembly.\"\"\"\n        self.repack_side_chains_by_id()\n        self.repack_side_chains_by_name()\n\n    def repack_side_chains_by_name(self):\n        \"\"\"Repacks the side chains of all Polymers by name.\"\"\"\n        self.repack_side_chains_by_name_by_id()", "solution": "\n        non_na_sequences = [s for s in self.sequences if ' ' not in s]\n        self.pack_new_sequences(non_na_sequences)\n        return"}
{"prompt": "def init_lstm_(lstm, init_weight=0.1):\n    \"\"\"\n    Initializes weights of LSTM layer.\n    Weights and biases are initialized with uniform(-init_weight, init_weight)\n    distribution.\n\n    :param lstm: instance of torch.nn.LSTM\n    :param init_weight: range for the uniform initializer\n    \"\"\"", "output": "\n    return LSTM(lstm.input_size, lstm.hidden_size, init_weight)\n\n\nclass LSTM(nn.Module):\n    \"\"\"\n    This class implements a multi-layer perceptron.\n    The input is a tensor of shape (batch_size, input_size)\n    where the first dimension is the batch size and the second dimension\n    is the input size.\n    \"\"\"\n\n    def __init__(self, input_size, hidden_size, init_weight=0.1):\n        super(LSTM, self).__init__()\n        self.input_size = input_size\n       ", "solution": "\n    # Initialize hidden-hidden weights\n    init.uniform_(lstm.weight_hh_l0.data, -init_weight, init_weight)\n    # Initialize input-hidden weights:\n    init.uniform_(lstm.weight_ih_l0.data, -init_weight, init_weight)\n\n    # Initialize bias. PyTorch LSTM has two biases, one for input-hidden GEMM\n    # and the other for hidden-hidden GEMM. Here input-hidden bias is\n    # initialized with uniform distribution and hidden-hidden bias is\n    # initialized with zeros.\n    init.uniform_(lstm.bias_ih_l0.data, -init_weight, init_weight)\n    init.zeros_(lstm.bias_hh_l0.data)\n\n    if lstm.bidirectional:\n        init.uniform_(lstm.weight_hh_l0_reverse.data, -init_weight, init_weight)\n        init.uniform_(lstm.weight_ih_l0_reverse.data, -init_weight, init_weight)\n\n        init.uniform_(lstm.bias_ih_l0_reverse.data, -init_weight, init_weight)\n        init.zeros_(lstm.bias_hh_l0_reverse.data)"}
{"prompt": "def example1():\n    \"\"\"\n    Compute the GRADEV of a white phase noise. Compares two different \n    scenarios. 1) The original data and 2) ADEV estimate with gap robust ADEV.\n    \"\"\"", "output": "\n    data1 = np.array(\n        [[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n          [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n          [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0", "solution": "\n    N = 1000\n    f = 1\n    y = np.random.randn(1,N)[0,:]\n    x = [xx for xx in np.linspace(1,len(y),len(y))]\n    x_ax, y_ax, (err_l, err_h), ns = allan.gradev(y,data_type='phase',rate=f,taus=x)\n    plt.errorbar(x_ax, y_ax,yerr=[err_l,err_h],label='GRADEV, no gaps')\n    \n    \n    y[int(np.floor(0.4*N)):int(np.floor(0.6*N))] = np.NaN # Simulate missing data\n    x_ax, y_ax, (err_l, err_h) , ns = allan.gradev(y,data_type='phase',rate=f,taus=x)\n    plt.errorbar(x_ax, y_ax,yerr=[err_l,err_h], label='GRADEV, with gaps')\n    plt.xscale('log')\n    plt.yscale('log')\n    plt.grid()\n    plt.legend()\n    plt.xlabel('Tau / s')\n    plt.ylabel('Overlapping Allan deviation')\n    plt.show()"}
{"prompt": "def setGameScore(self, user_id, score, game_message_identifier,\n                     force=None,\n                     disable_edit_message=None):\n        \"\"\"\n        See: https://core.telegram.org/bots/api#setgamescore\n\n        :param game_message_identifier: Same as ``msg_identifier`` in :meth:`telepot.Bot.editMessageText`\n        \"\"\"", "output": "\n        assert isinstance(game_message_identifier, str)\n        assert isinstance(score, (int, long))\n        assert isinstance(game_message_identifier, str)\n        assert game_message_identifier in self.user_data\n        if not force:\n            self.user_data[game_message_identifier][game_message_identifier] = score\n        elif disable_edit_message:\n            del self.user_data[game_message_identifier][game_message_identifier]\n        else:\n            self.user_data[game_message_identifier][game_message_identifier] = score\n\n    def", "solution": "\n        p = _strip(locals(), more=['game_message_identifier'])\n        p.update(_dismantle_message_identifier(game_message_identifier))\n        return self._api_request('setGameScore', _rectify(p))"}
{"prompt": "def kcenter(I,J,c,k):\n    \"\"\"kcenter -- minimize the maximum travel cost from customers to k facilities.\n    Parameters:\n        - I: set of customers\n        - J: set of potential facilities\n        - c[i,j]: cost of servicing customer i from facility j\n        - k: number of facilities to be used\n    Returns a model, ready to be solved.\n    \"\"\"", "output": "\n    model = KCenter(I,J,c)\n    ready = model.solve(I,J,k)\n    return model, ready\n\ndef kcenter_k(I,k,c):\n    \"\"\"kcenter -- minimize the maximum cost from customers to facilities.\n    Parameters:\n        - I: set of customers\n        - J: set of potential facilities\n        - c[i,k]: cost of servicing customer k from facility k\n        - k: number of facilities to be used\n        Returns a model, solved.\n    \"\"\"\n    model = KCenter_kcenter(", "solution": "\n\n    model = Model(\"k-center\")\n    z = model.addVar(vtype=\"C\", name=\"z\")\n    x,y = {},{}\n\n    for j in J:\n        y[j] = model.addVar(vtype=\"B\", name=\"y(%s)\"%j)\n        for i in I:\n            x[i,j] = model.addVar(vtype=\"B\", name=\"x(%s,%s)\"%(i,j))\n\n\n    for i in I:\n        model.addCons(quicksum(x[i,j] for j in J) == 1, \"Assign(%s)\"%i)\n\n        for j in J:\n            model.addCons(x[i,j] <= y[j], \"Strong(%s,%s)\"%(i,j))\n            model.addCons(c[i,j]*x[i,j] <= z, \"Max_x(%s,%s)\"%(i,j))\n\n    model.addCons(quicksum(y[j] for j in J) == k, \"Facilities\")\n\n    model.setObjective(z, \"minimize\")\n    model.data = x,y\n\n    return model"}
{"prompt": "def roll_group(group):\n    \"\"\"\n    Rolls a group of dice in 2d6, 3d10, d12, etc. format\n\n    :param group: String of dice group\n    :return: Array of results\n    \"\"\"", "output": "\n    if isinstance(group, str):\n        return [roll_dice(group) for i in range(6)]\n    elif isinstance(group, list):\n        return [roll_group(i) for i in range(10)]\n    elif isinstance(group, int):\n        return [roll_group(group) for i in range(12)]\n    elif isinstance(group, float):\n        return [roll_group(group) for i in range(12)]\n    elif isinstance(group, int):\n        return [roll_group(group) for i in range(10)]\n    elif isinstance(group, float", "solution": "\n    group = regex.match(r'^(\\d*)d(\\d+)$', group, regex.IGNORECASE)\n    num_of_dice = int(group[1]) if group[1] != '' else 1\n    type_of_dice = int(group[2])\n    assert num_of_dice > 0\n\n    result = []\n    for i in range(num_of_dice):\n        result.append(random.randint(1, type_of_dice))\n    return result"}
{"prompt": "def _save_to_hdx(self, action, id_field_name, file_to_upload=None):\n        # type: (str, str, Optional[str]) -> None\n        \"\"\"Creates or updates an HDX object in HDX, saving current data and replacing with returned HDX object data\n        from HDX\n\n        Args:\n            action (str): Action to perform: 'create' or 'update'\n            id_field_name (str): Name of field containing HDX object identifier\n            file_to_upload (Optional[str]): File to upload to HDX\n\n        Returns:\n            None\n        \"\"\"", "output": "\n        \"\"\"\n        if file_to_upload is not None:\n            file_to_upload = self._get_file_path(file_to_upload)\n            with open(file_to_upload, 'rb') as f:\n                return self.upload_to_hdx(f, file_to_upload)\n        else:\n            # If no file_to_upload is provided, use the file_to_upload parameter\n            return self.create_hdx(action, id_field_name)\n\n    def upload_to_hdx(self, file_to_upload, file_", "solution": "\n        result = self._write_to_hdx(action, self.data, id_field_name, file_to_upload)\n        self.old_data = self.data\n        self.data = result"}
{"prompt": "def device_type_from_string(cl_device_type_str):\n    \"\"\"Converts values like ``gpu`` to a pyopencl device type string.\n\n    Supported values are: ``accelerator``, ``cpu``, ``custom``, ``gpu``. If ``all`` is given, None is returned.\n\n    Args:\n        cl_device_type_str (str): The string we want to convert to a device type.\n\n    Returns:\n        cl.device_type: the pyopencl device type.\n    \"\"\"", "output": "\n    if cl_device_type_str is None:\n        return None\n    elif isinstance(cl_device_type_str, str):\n        return cl_device_type_str\n    else:\n        raise ValueError(\"Unknown device type: %s\" % cl_device_type_str)\n\n\ndef device_type_from_string_list(cl_device_strings, all=False):\n    \"\"\"Converts values like ``gpu`` to a pyopencl device type string list.\n\n    Supported values are: ``accelerator``, ``cpu``, ``custom``, ``gpu``. If ``all`` is given, None", "solution": "\n    cl_device_type_str = cl_device_type_str.upper()\n    if hasattr(cl.device_type, cl_device_type_str):\n        return getattr(cl.device_type, cl_device_type_str)\n    return None"}
{"prompt": "def from_array(array):\n        \"\"\"\n        Deserialize a new PassportData from a given dictionary.\n\n        :return: new PassportData instance.\n        :rtype: PassportData\n        \"\"\"", "output": "\n        self.data = {}\n        self.data_size = len(array)\n        self.data_offset = 0\n        self.data_length = len(array[0])\n        self.data_type = array[0][0]\n        self.data_offset = array[0][1]\n        self.data_length = array[0][2]\n        self.data_type_size = array[0][3]\n        self.data_type_offset = array[0][4]\n        self.data_type_size = array[0][5]\n        self.data_type", "solution": "\n        if array is None or not array:\n            return None\n        # end if\n        assert_type_or_raise(array, dict, parameter_name=\"array\")\n\n\n        data = {}\n        data['data'] = EncryptedPassportElement.from_array_list(array.get('data'), list_level=1)\n        data['credentials'] = EncryptedCredentials.from_array(array.get('credentials'))\n        data['_raw'] = array\n        return PassportData(**data)"}
{"prompt": "def setup_config(\n        config_directories=None,\n        config_file=None,\n        default_filename=\"opentc.yml\"\n):\n    \"\"\"Setup configuration\n\n    \"\"\"", "output": "\n    if config_directories is None:\n        config_directories = get_config_directories()\n    if config_file is None:\n        config_file = default_filename\n    if not os.path.isfile(config_file):\n        return False\n    with open(config_file) as f:\n        config = yaml.load(f)\n    return True\n\n\ndef get_config_directories():\n    \"\"\"Get config directories\n\n    \"\"\"\n    return [\n        os.path.join(os.path.dirname(__file__), \"opentc\", \"templates\"),\n        os.path.join(os.path.dirname", "solution": "\n    config_found = False\n    config_file_path = None\n    if config_file:\n        config_file_path = config_file\n        if os.path.isfile(config_file_path) and os.access(config_file_path, os.R_OK):\n            config_found = True\n    else:\n        for directory in config_directories:\n            if directory is None:\n                continue\n            config_file_path = os.path.join(directory, default_filename)\n            if os.path.isfile(config_file_path) and os.access(config_file_path, os.R_OK):\n                config_found = True\n                break\n    if config_found:\n        with open(config_file_path, 'rt') as ymlfile:\n            config = yaml.safe_load(ymlfile.read())\n        return config\n    else:\n        print(\"The configuration file is not found.\")\n        exit(1)"}
{"prompt": "def set_siren_volume(self, volume):\r\n        \"\"\"\r\n        :param volume: one of [low, medium, high]\r\n        \"\"\"", "output": "\r\n        self.siren_volume = volume\r\n        self.siren_volume_label.setText(str(volume))\r\n        self.siren_volume_combo.setCurrentIndex(self.siren_volume)\r\n        self.siren_volume_combo.blockSignals(False)\r\n        self.siren_volume_combo.blockSignals(True)\r\n        self.siren_volume_combo.setDisabled(True)\r\n        self.siren_volume_combo.setCurrentIndex(self.siren_volume_combo.findText(self.siren_volume))\r\n        self.siren_volume_", "solution": "\r\n        values = {\r\n            \"desired_state\": {\r\n                \"siren_volume\": volume\r\n            }\r\n        }\r\n        response = self.api_interface.set_device_state(self, values)\r\n        self._update_state_from_response(response)"}
{"prompt": "def hash_full_tree(self, leaves):\n        \"\"\"Hash a set of leaves representing a valid full tree.\"\"\"", "output": "\n        return hash(leaves)\n\n    def get_leaves(self, node):\n        \"\"\"Get the set of leaves that are in the tree.\"\"\"\n        leaves = set()\n        for child in node.children:\n            leaves.add(self.hash_full_tree(child.children))\n        return leaves\n\n    def get_node_at_index(self, index):\n        \"\"\"Get the node at the given index.\"\"\"\n        for child in self.get_leaves(index):\n            if child.parent is None:\n                continue\n            if child.parent.children[0] == index:\n                return child\n        return None\n\n   ", "solution": "\n        root_hash, hashes = self._hash_full(leaves, 0, len(leaves))\n        assert len(hashes) == count_bits_set(len(leaves))\n        assert (self._hash_fold(hashes) == root_hash if hashes else\n                root_hash == self.hash_empty())\n        return root_hash"}
{"prompt": "def check_counts(self):\n        \"\"\"\n        Disables tweet parsing if the count API is used.\n        \"\"\"", "output": "\n        self.tweet_parsing = False\n\n    def get_tweets(self, count=None):\n        \"\"\"\n        Returns a list of the most recent tweets posted by the user.\n        \"\"\"\n        if count is None:\n            count = self.tweet_count\n        return self.tweet_storage.get_tweets(count)\n\n    def get_tweets_since(self, count=None):\n        \"\"\"\n        Returns the most recent tweets posted by the user since the\n        specified number of tweets is known.\n        \"\"\"\n        if count is None:\n            count = self.tweet_count\n        return self.", "solution": "\n        if \"counts\" in re.split(\"[/.]\", self.endpoint):\n            logger.info(\"disabling tweet parsing due to counts API usage\")\n            self._tweet_func = lambda x: x"}
{"prompt": "def parse(self, valstr):\n        # type: (bytes) -> None\n        \"\"\"\n        A method to parse an El Torito Entry out of a string.\n\n        Parameters:\n         valstr - The string to parse the El Torito Entry out of.\n        Returns:\n         Nothing.\n        \"\"\"", "output": "\n        if not valstr:\n            return\n\n        # Parse the entry.\n        self.parse_entry(valstr)\n\n        # Parse the entry.\n        self.parse_entry_table(valstr)\n\n        # Parse the entry.\n        self.parse_entry_table_data(valstr)\n\n        # Parse the entry.\n        self.parse_entry_table_data_table(valstr)\n\n        # Parse the entry.\n        self.parse_entry_data_table(valstr)\n\n        # Parse the entry.\n        self.parse_entry_data_table_entry(valstr)\n\n    def", "solution": "\n        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('El Torito Entry already initialized')\n\n        (self.boot_indicator, self.boot_media_type, self.load_segment,\n         self.system_type, unused1, self.sector_count, self.load_rba,\n         self.selection_criteria_type,\n         self.selection_criteria) = struct.unpack_from(self.FMT, valstr, 0)\n\n        if self.boot_indicator not in (0x88, 0x00):\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid El Torito initial entry boot indicator')\n        if self.boot_media_type > 4:\n            raise pycdlibexception.PyCdlibInvalidISO('Invalid El Torito boot media type')\n\n        # FIXME: check that the system type matches the partition table\n\n        if unused1 != 0:\n            raise pycdlibexception.PyCdlibInvalidISO('El Torito unused field must be 0')\n\n        # According to the specification, the El Torito unused end field (bytes\n        # 0xc - 0x1f, unused2 field) should be all zero.  However, we have found\n        # ISOs in the wild where that is not the case, so skip that particular\n        # check here.\n\n        self._initialized = True"}
{"prompt": "def init_auth(username, password):\n    \"\"\"Initializes the auth settings for accessing MyAnimeList\n    through its official API from a given username and password.\n    :param username Your MyAnimeList account username.\n    :param password Your MyAnimeList account password.\n    :return A tuple containing your credentials.\n    \"\"\"", "output": "\n    return (username, password)\n\n\ndef get_my_anime_list_api(username, password):\n    \"\"\"Retrieves the API object for MyAnimeList\n    from the given username and password.\n    :param username Your MyAnimeList account username.\n    :param password Your MyAnimeList password.\n    :return A tuple containing your API object.\n    \"\"\"\n    return (get_my_anime_list(username, password), get_my_anime_list_by_id(username,'my_anime_list'))\n\n\ndef get_my_anime_", "solution": "\n    username = username.strip()\n    password = password.strip()\n    credentials = (username, password)\n    if helpers.verif_auth(credentials, header):\n        return credentials\n    else:\n        raise ValueError(constants.INVALID_CREDENTIALS)"}
{"prompt": "def dedent_block_string_value(raw_string: str) -> str:\n    \"\"\"Produce the value of a block string from its parsed raw value.\n\n    Similar to CoffeeScript's block string, Python's docstring trim or Ruby's\n    strip_heredoc.\n\n    This implements the GraphQL spec's BlockStringValue() static algorithm.\n    \"\"\"", "output": "\n    return BlockStringValue(\n        raw_string=raw_string,\n        start_mark=raw_string.find('\\n') + 1,\n        end_mark=raw_string.find('\\n') + 1,\n        start_mark_line=raw_string.find('\\n', start_mark + 1),\n        end_mark_line=raw_string.find('\\n', end_mark + 1),\n        start_mark_column=raw_string.find('\\n', start_mark + 1),\n        end_mark_column=raw_string.find('\\n', end_mark", "solution": "\n    lines = raw_string.splitlines()\n\n    common_indent = None\n    for line in lines[1:]:\n        indent = leading_whitespace(line)\n        if indent < len(line) and (common_indent is None or indent < common_indent):\n            common_indent = indent\n        if common_indent == 0:\n            break\n\n    if common_indent:\n        lines[1:] = [line[common_indent:] for line in lines[1:]]\n\n    while lines and not lines[0].strip():\n        lines = lines[1:]\n\n    while lines and not lines[-1].strip():\n        lines = lines[:-1]\n\n    return \"\\n\".join(lines)"}
{"prompt": "def async_get_measurements(self, uid, fields='*'):\n        \"\"\"Get measurements of a device.\"\"\"", "output": "\n        return await self.hass.async_add_executor_job(self._get_measurements, uid, fields)\n\n    async def _get_measurements(self, uid, fields='*', timeout=None):\n        \"\"\"Get measurements of a device.\"\"\"\n        if timeout is None:\n            timeout = self.timeout\n\n        if timeout is not None:\n            fields += ['{}_{}'.format(k, v) for k, v in timeout.items()]\n            return self.hass.async_add_executor_job(self._measures_timeout, uid, fields)\n\n        return await self.hass.async_add", "solution": "\n        return (yield from self._get('/pods/{}/measurements'.format(uid),\n                                     fields=fields))[0]"}
{"prompt": "def resolved_args(self):\n        \"\"\"Parse args if they have not already been parsed and return the Namespace for args.\n\n        .. Note:: Accessing args should only be done directly in the App.\n\n        Returns:\n            (namespace): ArgParser parsed arguments with Playbook variables automatically resolved.\n        \"\"\"", "output": "\n        return self._parse_args(self._args, self._namespace)\n\n    def _parse_args(self, args, namespace):\n        \"\"\"Parse the arguments.\n\n        Args:\n            args (list): Arguments to parse.\n            namespace (str): Namespace to use for parsing.\n        \"\"\"\n        args = [arg.strip() for arg in args.split(',')] if args else []\n        if namespace:\n            args.append(namespace)\n        return args\n\n    def _resolve_namespace(self, namespace):\n        \"\"\"Resolve a namespace.\n\n        Args:\n            namespace (str): Namespace to use for resolving.\n        \"\"\"\n        if", "solution": "\n\n        if not self._parsed_resolved:  # only resolve once\n            self.args()\n\n            # create new args Namespace for resolved args\n            self._default_args_resolved = Namespace()\n\n            # iterate over args and resolve any playbook variables\n            for arg in vars(self._default_args):\n                arg_val = getattr(self._default_args, arg)\n                if arg not in self.tc_reserved_args:\n                    if isinstance(arg_val, (str)):\n                        arg_val = self.tcex.playbook.read(arg_val)\n                setattr(self._default_args_resolved, arg, arg_val)\n\n            # set parsed bool to ensure args are only parsed once\n            self._parsed_resolved = True\n\n        return self._default_args_resolved"}
{"prompt": "def scale_app(self, app_id, instances=None, delta=None, force=False):\n        \"\"\"Scale an app.\n\n        Scale an app to a target number of instances (with `instances`), or scale the number of\n        instances up or down by some delta (`delta`). If the resulting number of instances would be negative,\n        desired instances will be set to zero.\n\n        If both `instances` and `delta` are passed, use `instances`.\n\n        :param str app_id: application ID\n        :param int instances: [optional] the number of instances to scale to\n        :param int delta: [optional] the number of instances to scale up or down by\n        :param bool force: apply even if a deployment is in progress\n\n        :returns: a dict containing the deployment id and version\n        :rtype: dict\n        \"\"\"", "output": " [optional] the number of instances up/down by delta\n        :param bool force: [optional] if True, force the scaling.\n        :rtype: dict\n        \"\"\"\n        params = {}\n        if instances is not None:\n            params['Instances'] = instances\n        if delta is not None:\n            params['Delta'] = delta\n        if params:\n            return self._get_response_by_key('Scale', params)\n        else:\n            return self._get_response_by_key('Scale', params)\n\n    def list_app_instances(self, app_id=None):\n        \"\"\"List all instances of", "solution": "\n        if instances is None and delta is None:\n            marathon.log.error('instances or delta must be passed')\n            return\n\n        try:\n            app = self.get_app(app_id)\n        except NotFoundError:\n            marathon.log.error('App \"{app}\" not found'.format(app=app_id))\n            return\n\n        desired = instances if instances is not None else (\n            app.instances + delta)\n        return self.update_app(app.id, MarathonApp(instances=desired), force=force)"}
{"prompt": "def list_groups(refresh=False):\n    \"\"\"\n    Return a list of groups\n\n    Args:\n\n        refresh (bool):\n            Refresh the info for all groups in ``__context__``. If False only\n            the groups in ``__context__`` will be returned. If True, the\n            ``__context__`` will be refreshed with current data and returned.\n            Default is False\n\n    Returns:\n        list: A list of groups on the machine\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' group.list_groups\n    \"\"\"", "output": "\n    if refresh:\n        return __context__['refresh_groups']\n    else:\n        return __context__['groups']\n\n\ndef list_users(refresh=False):\n    \"\"\"\n    Return a list of users\n\n    Args:\n\n        refresh (bool):\n            Refresh the info for all users in ``__context__``. If False only\n            the users in ``__context__`` will be returned. If True, the\n            ``__context__`` will be refreshed with current data and returned.\n            Default is False\n\n    Returns:\n        list: A list of users on the machine\n\n    CLI Example:\n\n   .. code-block:: bash\n\n        salt '*' group.list", "solution": "\n    if 'group.list_groups' in __context__ and not refresh:\n        return __context__['group.list_groups']\n\n    results = _get_all_groups()\n\n    ret = []\n\n    for result in results:\n        ret.append(result.Name)\n\n    __context__['group.list_groups'] = ret\n\n    return ret"}
{"prompt": "def run(command, **kwargs):\n    \"\"\"Run a shell command and wait for the response. The result object will\n    resolve to True if result.code == 0 and output/error results can be\n    retrieved from result.stdout and result.stderr variables.\n\n      >>> result = run('echo ${content}', content='Some $%^$## \"\" + \\' content')\n      >>> result.code\n      0\n      >>> if result:\n      ...     print('Stdout: %s' % result.stdout)\n      ... else:\n      ...     print('Stderr: %s' % result.stderr)\n      Stdout: Some $%^$## \"\" + ' content\n      >>> print(result.command)\n      echo 'Some $%^$## \"\" + '\"'\"' content'\n    \"\"\"", "output": " content\n      \"\"\"\n    if'shell' not in kwargs:\n        kwargs['shell'] = True\n    return _run(command, **kwargs)\n\n\ndef _run(command, **kwargs):\n    \"\"\"Execute a shell command and return the output.\n\n    This function will execute the command in a shell and wait for the\n    response. If the command is a string, it will be sent to the\n    subprocess.Popen object. Otherwise it will be sent to the\n    subprocess.Popen object.\n\n    If'stdout' is a subprocess.PIPE, it will be used for stdout.\n    Otherwise, it will be used for stderr.", "solution": "\n    env = None\n    if 'env' in kwargs:\n        if kwargs.get('env_empty'):\n            env = {}\n        else:\n            env = copy.deepcopy(os.environ)\n        env.update(kwargs['env'])\n    stdin = kwargs.get('stdin')\n    stdout = kwargs.get('stdout', False)\n    stderr = kwargs.get('stderr', False)\n    combine = kwargs.get('combine', False)\n    if stdout is True:\n        stdout = sys.stdout.write\n    if stderr is True:\n        stderr = sys.stderr.write\n    if kwargs:\n        args = {}\n        q = pipes.quote\n        for name, value in kwargs.items():\n            if isinstance(value, basestring_type):\n                args[name] = q(value)\n            elif isinstance(value, (list, tuple)):\n                args[name] = u' '.join([q(unicode_type(v)) for v in value])\n            elif isinstance(value, dict):\n                args[name] = u' '.join([u'%s %s' % (q(n), q(v)) for n, v in value.items()])\n            else:\n                args[name] = pipes.quote(unicode_type(value))\n        command = string.Template(command).safe_substitute(args)\n    log.debug('run: %s' % command)\n    ref = subprocess.Popen(\n        command,\n        stdin=None if stdin is None else subprocess.PIPE,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT if combine is True else subprocess.PIPE,\n        shell=kwargs.get('shell', True),\n        close_fds=kwargs.get('close_fds', True),\n        env=env,\n        cwd=kwargs.get('cwd', tempfile.gettempdir()),\n    )\n    if stdin is not None:\n        if not isinstance(stdin, basestring_type):\n            stdin = unicode_type(stdin)\n        if not isinstance(stdin, bytes_type):\n            stdin = stdin.encode('utf-8')\n        ref.stdin.write(stdin)\n        ref.stdin.flush()\n        ref.stdin.close()\n    fds = [ref.stdout]\n    if combine is not True:\n        fds.append(ref.stderr)\n    stdout_result = b''\n    stderr_result = b''\n    while fds:\n        for fd in select.select(fds, tuple(), tuple())[0]:\n            line = fd.readline()\n            if line:\n                if fd == ref.stdout:\n                    if stdout:\n                        stdout(line)\n                    stdout_result += line\n                elif fd == ref.stderr:\n                    if stderr:\n                        stderr(line)\n                    stderr_result += line\n            else:\n                fds.remove(fd)\n    ref.wait()\n    return obj({\n        'code': ref.returncode,\n        'command': command,\n        'stdout': stdout_result,\n        'stderr': stderr_result,\n    }, bool=ref.returncode == 0, grow=False)"}
{"prompt": "def set_rainbow(self, duration):\n        \"\"\"Turn the bulb on and create a rainbow.\"\"\"", "output": "\n        self.rainbow = Rainbow(duration)\n        self.rainbow.start()\n\n    def get_rainbow(self):\n        \"\"\"Return the rainbow.\"\"\"\n        return self.rainbow\n\n    def stop(self):\n        \"\"\"Stop the thread.\"\"\"\n        self.running = False\n\n    def run(self):\n        \"\"\"Run the thread.\"\"\"\n        self.set_thread_stop()\n        self.join()\n        self.set_thread_stop()\n\n    def join(self):\n        \"\"\"Wait for the thread to exit.\"\"\"\n        self.running = True\n        self.join_thread.join()\n        self", "solution": "\n        for i in range(0, 359):\n            self.set_color_hsv(i, 100, 100)\n            time.sleep(duration/359)"}
{"prompt": "def hill_i(self,x,threshold=0.1,power=2):\n        \"\"\" Inhibiting hill function.\n\n            Is equivalent to 1-hill_a(self,x,power,threshold).\n        \"\"\"", "output": "\n        return 1.0 - np.log(x) - threshold*np.log(power)\n\n    def hill_j(self,x,threshold=0.1,power=2):\n        \"\"\" Inhibiting hill function.\n\n            Is equivalent to 1-hill_a(self,x,power,threshold).\n        \"\"\"\n        return 1.0 - np.log(x) - threshold*np.log(power)\n\n    def hill_k(self,x,threshold=0.1,power=2):\n        \"\"\" Inhibiting hill function.\n\n            Is equivalent", "solution": "\n        x_pow = np.power(x,power)\n        threshold_pow = np.power(threshold,power)\n        return threshold_pow / (x_pow + threshold_pow)"}
{"prompt": "def _run_genotype_gvcfs_genomicsdb(genomics_db, region, out_file, data):\n    \"\"\"GenotypeGVCFs from a merged GenomicsDB input: GATK4.\n            ropts += [str(x) for x in resources.get(\"options\", [])]\n\n    No core scaling -- not yet supported in GATK4.\n    \"\"\"", "output": "\n    if not resources.get(\"options\", []):\n        raise ValueError(\"No core scaling options specified\")\n    if not region.endswith(\".\"):\n        region = region + \".\"\n    ropts = ropts + [str(x) for x in resources.get(\"options\", [])]\n    if not out_file.endswith(\".gvcf\"):\n        out_file = out_file + \".gvcf\"\n    with open(data, \"w\") as out_handle:\n        for ropt in ropts:\n            if ropt.startswith(\"options:\"):\n                out_handle.write(genomics_db.options", "solution": "\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            broad_runner = broad.runner_from_config(data[\"config\"])\n            params = [\"-T\", \"GenotypeGVCFs\",\n                      \"--variant\", \"gendb://%s\" % genomics_db,\n                      \"-R\", dd.get_ref_file(data),\n                      \"--output\", tx_out_file,\n                      \"-L\", bamprep.region_to_gatk(region)]\n            params += [\"-ploidy\", str(ploidy.get_ploidy([data], region))]\n            # Avoid slow genotyping runtimes with improved quality score calculation in GATK4\n            # https://gatkforums.broadinstitute.org/gatk/discussion/11471/performance-troubleshooting-tips-for-genotypegvcfs/p1\n            params += [\"--use-new-qual-calculator\"]\n            resources = config_utils.get_resources(\"gatk\", data[\"config\"])\n            params += [str(x) for x in resources.get(\"options\", [])]\n            cores = dd.get_cores(data)\n            memscale = {\"magnitude\": 0.9 * cores, \"direction\": \"increase\"} if cores > 1 else None\n            broad_runner.run_gatk(params, memscale=memscale)\n    return vcfutils.bgzip_and_index(out_file, data[\"config\"])"}
{"prompt": "def emergency_cast(colx, value):\n  \"\"\"ugly: this is a huge hack. get serious about where this belongs in the architecture.\n  For now, most types rely on being fed in as SubbedLiteral.\n  \"\"\"", "output": "\n  if isinstance(value, Literal):\n    return value.to_string()\n  return value\n\n\ndef emergency_cast_to_string(colx, value):\n  \"\"\"ugly: this is a huge hack. get serious about where this belongs in the architecture.\n  For now, most types rely on being fed in as SubbedLiteral.\n  \"\"\"\n  if isinstance(value, SubbedLiteral):\n    return value.to_string()\n  return value\n\n\ndef emergency_cast_to_string_or_none(colx, value):\n  \"\"\"ugly: this is", "solution": "\n  if colx.coltp.type.lower()=='boolean':\n    if isinstance(value,sqparse2.NameX): value = value.name\n    if isinstance(value,bool): return value\n    return dict(true=True, false=False)[value.lower()] # keyerror if other\n  else:\n    return value"}
{"prompt": "def p_portfolio(I,sigma,r,alpha,beta):\n    \"\"\"p_portfolio -- modified markowitz model for portfolio optimization.\n    Parameters:\n        - I: set of items\n        - sigma[i]: standard deviation of item i\n        - r[i]: revenue of item i\n        - alpha: acceptance threshold\n        - beta: desired confidence level\n    Returns a model, ready to be solved.\n    \"\"\"", "output": "\n    model = {}\n    ready = {}\n    for i in I:\n        if i not in model:\n            model[i] = {}\n        if i not in ready:\n            ready[i] = set()\n        if sigma[i] == r[i]:\n            model[i][r] = alpha\n        if beta == 0:\n            ready[i].add(i)\n        if sigma[i] == 0:\n            ready[i].add(r)\n            model[i][r].add(i)\n            ready[i].add(r)\n            model[i][r].add(i)\n           ", "solution": "\n\n    model = Model(\"p_portfolio\")\n\n    x = {}\n    for i in I:\n        x[i] = model.addVar(vtype=\"C\", name=\"x(%s)\"%i)  # quantity of i to buy\n    rho = model.addVar(vtype=\"C\", name=\"rho\")\n    rhoaux = model.addVar(vtype=\"C\", name=\"rhoaux\")\n\n    model.addCons(rho == quicksum(r[i]*x[i] for i in I))\n    model.addCons(quicksum(x[i] for i in I) == 1)\n\n    model.addCons(rhoaux == (alpha - rho)*(1/phi_inv(beta))) #todo\n    model.addCons(quicksum(sigma[i]**2 * x[i] * x[i] for i in I) <=  rhoaux * rhoaux)\n\n    model.setObjective(rho, \"maximize\")\n\n    model.data = x\n    return model"}
{"prompt": "def get_memory_cache(self, key=None):\n        \"\"\"Return result in memory cache for key 'key' or None if not found.\"\"\"", "output": "\n        return self._memory_cache\n\n    def get_memory_usage(self, key=None):\n        \"\"\"Return result in bytes for key 'key' or None if not found.\"\"\"\n        return self._memory_usage\n\n    def get_memory_info(self, key=None):\n        \"\"\"Return result in bytes for key 'key' or None if not found.\"\"\"\n        return self._memory_cache\n\n    def get_memory_info_ex(self, key=None):\n        \"\"\"Return result in bytes for key 'key' or None if not found.\"\"\"\n        return self.get_memory_info(key).", "solution": "\n        key = self.model.hash if key is None else key\n        self._results = self.memory_cache.get(key)\n        return self._results"}
{"prompt": "def service_timeouts(self):\n        \"\"\"\n        run callbacks on all expired timers\n        Called from the event thread\n        :return: next end time, or None\n        \"\"\"", "output": "\n        if self.timer_list:\n            for timer in self.timer_list:\n                if timer.is_expired():\n                    self.timer_list.remove(timer)\n        return self.timer_list\n\n    def _get_timer_list(self):\n        \"\"\"\n        get all timers that have expired\n        :return: list of expired timers\n        \"\"\"\n        return self.timers.values()\n\n    def _get_timer_list_by_name(self, name):\n        \"\"\"\n        get all timers by name\n        :param name: name of the timer\n        :return: list of timers\n       ", "solution": "\n        queue = self._queue\n        if self._new_timers:\n            new_timers = self._new_timers\n            while new_timers:\n                heappush(queue, new_timers.pop())\n\n        if queue:\n            now = time.time()\n            while queue:\n                try:\n                    timer = queue[0][1]\n                    if timer.finish(now):\n                        heappop(queue)\n                    else:\n                        return timer.end\n                except Exception:\n                    log.exception(\"Exception while servicing timeout callback: \")"}
{"prompt": "def grid_list(data):\n    \"\"\"\n    #=================================================\n    /process the grid data\n    /convert to list data for poly fitting\n    #=================================================\n    \"\"\"", "output": "\n    #=================================================\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #\n    #", "solution": "\n    a = []\n    b = []\n    M = []\n    for i in data:\n        a.append(i[0])  # np.array([i[1] for i in data], dtype=np.float64)\n        b.append(i[1])  # np.array([i[0] for i in data], dtype=np.float64)\n        M.append(i[2])  # np.array([i[2] for i in data], dtype=np.float64)\n    a = np.array(a, dtype=np.float64).tolist()\n    b = np.array(b, dtype=np.float64).tolist()\n    M = np.array(M, dtype=np.float64).tolist()\n    a = list(set(a))\n    b = list(set(b))\n    return a, b, M"}
{"prompt": "def set_progress_message(self, message, line_break=False):\n        \"\"\"Set a progress message.\n\n        The string needs to contain `{progress}`.\n\n        \"\"\"", "output": "\n        if line_break:\n            self.line_break()\n        self.status_message = message\n        self.progress = message\n\n    def set_progress_percentage(self, percentage):\n        \"\"\"Set a progress percentage.\n\n        The string needs to contain `{progress}`.\n\n        \"\"\"\n        if percentage < 0 or percentage > 100:\n            raise ValueError(\"Invalid progress percentage: %r\" % (percentage,))\n        self.line_percentage = percentage\n        self.status_percentage = percentage\n\n    def set_progress_bar(self, bar):\n        \"\"\"Set a progress bar.\n\n        The string needs to contain `{progress", "solution": "\n\n        end = '\\r' if not line_break else None\n\n        @self.connect\n        def on_progress(value, value_max, **kwargs):\n            kwargs['end'] = None if value == value_max else end\n            _default_on_progress(message, value, value_max, **kwargs)"}
{"prompt": "def add_ip(self, family='IPv4'):\n        \"\"\"\n        Allocate a new (random) IP-address to the Server.\n        \"\"\"", "output": "\n        if family == 'IPv4':\n            self.server_ip = random.choice(self.ip_addresses)\n        else:\n            self.server_ip = self.ip_addresses[0]\n\n    def add_ip_range(self, ip_range, family='IPv4'):\n        \"\"\"\n        Allocate a new (random) IP-range to the Server.\n        \"\"\"\n        if family == 'IPv4':\n            self.server_ip = random.choice(ip_range)\n        else:\n            self.server_ip = ip_range[0]\n            self.server_ip_range", "solution": "\n        IP = self.cloud_manager.attach_ip(self.uuid, family)\n        self.ip_addresses.append(IP)\n        return IP"}
{"prompt": "def exception(self, timeout=None):\n        \"\"\"Wait for the async function to complete and return its exception.\n\n        If the function did not raise an exception this returns ``None``.\n        \"\"\"", "output": "\n        return self._wait_for_function(self.async_function, timeout)\n\n    def _wait_for_function(self, function, timeout=None):\n        \"\"\"Wait for a function to complete.\n\n        If the function did not raise an exception this returns ``None``.\n        \"\"\"\n        start_time = time.time()\n        while function.func_name in self._async_functions:\n            if function.func_name == function.func_name:\n                return\n            time.sleep(0.1)\n        self.fail('timed out waiting for %s' % function.func_name)\n\n   ", "solution": "\n        if not self._done.wait(timeout):\n            raise Timeout('timeout waiting for future')\n        if self._state == self.S_EXCEPTION:\n            return self._result"}
{"prompt": "def set_options(self, **options):\n        \"\"\"\n        Set instance variables based on an options dict\n        \"\"\"", "output": "\n        self.options.update(options)\n        self.options['format'] = 'json'\n        self.options['indent'] = 4\n        self.options['sort_keys'] = True\n        self.options['separators'] = (',', ':')\n        self.options['sort_keys_first'] = True\n        self.options['sort_keys_last'] = True\n        self.options['sort_keys_last_item'] = True\n        self.options['sort_keys_last_item_item'] = True\n        self.options['sort_keys_last_item_text'] = True\n       ", "solution": "\n        self.interactive = False\n        self.verbosity = options['verbosity']\n        self.symlink = \"\"\n        self.clear = False\n        ignore_patterns = []\n        self.ignore_patterns = list(set(ignore_patterns))\n        self.page_themes_updated = 0\n        self.skins_updated = 0"}
{"prompt": "def use_comparative_assessment_taken_view(self):\n        \"\"\"Pass through to provider AssessmentTakenLookupSession.use_comparative_assessment_taken_view\"\"\"", "output": "\n        # Implemented from kitosid template for -\n        # osid.resource.ResourceLookupSession.use_comparative_assessment_taken_view\n        self._provider_session.use_comparative_assessment_taken_view()\n\n    def use_plenary_assessment_taken_view(self):\n        \"\"\"Pass through to provider PlenaryTakenLookupSession.use_plenary_assessment_taken_view\"\"\"\n        # Implemented from kitosid template for -\n        # osid.resource.ResourceLookupSession.use_plenary_assessment_taken_view\n       ", "solution": "\n        self._object_views['assessment_taken'] = COMPARATIVE\n        # self._get_provider_session('assessment_taken_lookup_session') # To make sure the session is tracked\n        for session in self._get_provider_sessions():\n            try:\n                session.use_comparative_assessment_taken_view()\n            except AttributeError:\n                pass"}
{"prompt": "def is_html_link(cls, tag, attribute):\n        \"\"\"Return whether the link is likely to be external object.\"\"\"", "output": "\n        return (tag.name == 'a' and attribute.name == 'href') or \\\n               (tag.name == 'img' and attribute.name =='src')\n\n    @classmethod\n    def get_external_url(cls, url):\n        \"\"\"Return the external URL for the given URL.\"\"\"\n        if not url:\n            return None\n        if url.startswith('http'):\n            return url\n        return 'http://www.w3.org/1999/xhtml'\n\n    @classmethod\n    def get_link_url(cls, link):\n        \"\"\"Return the external URL for the given link.\"\"\"\n        if not link", "solution": "\n        if tag in cls.TAG_ATTRIBUTES \\\n           and attribute in cls.TAG_ATTRIBUTES[tag]:\n            attr_flags = cls.TAG_ATTRIBUTES[tag][attribute]\n            return attr_flags & cls.ATTR_HTML\n\n        return attribute == 'href'"}
{"prompt": "def from_file(cls, filename):\n        \"\"\"Initialize datasource from file (.tds ot .tdsx)\"\"\"", "output": "\n        with open(filename, 'rb') as f:\n            tds = pickle.load(f)\n        return cls(tds)\n\n    def to_file(self, filename):\n        \"\"\"Write the datasource to a file (.tds ot.tdsx)\"\"\"\n        with open(filename, 'wb') as f:\n            pickle.dump(self.tds, f, pickle.HIGHEST_PROTOCOL)\n\n    def from_file(self, filename):\n        \"\"\"Initialize datasource from file (.tds ot.tdsx)\"\"\"\n        with open(filename, 'rb') as f:\n            tds", "solution": "\n\n        dsxml = xml_open(filename, 'datasource').getroot()\n        return cls(dsxml, filename)"}
{"prompt": "def get_sqlalchemy_url(database=None, host=None, port=None, username=None, password=None, driver='postgres'):\n        # type: (Optional[str], Optional[str], Union[int, str, None], Optional[str], Optional[str], str) -> str\n        \"\"\"Gets SQLAlchemy url from database connection parameters\n\n        Args:\n            database (Optional[str]): Database name\n            host (Optional[str]): Host where database is located\n            port (Union[int, str, None]): Database port\n            username (Optional[str]): Username to log into database\n            password (Optional[str]): Password to log into database\n            driver (str): Database driver. Defaults to 'postgres'.\n\n        Returns:\n            db_url (str): SQLAlchemy url\n        \"\"\"", "output": "]): Password to log into database\n\n        Returns:\n            str: SQLAlchemy url\n        \"\"\"\n        if database is not None:\n            return database + '@' + str(host) + ':' + str(port)\n        elif username is not None:\n            return 'postgresql://' + str(host) + ':' + str(port)\n        else:\n            return None\n\n    def get_table_name(self, table_name, database=None, driver='postgres'):\n        # type: (str, Optional[str], Union[int, str, None], Optional[str], Optional[str]) -> str\n        \"\"\"Gets table name from connection", "solution": "\n        strings = ['%s://' % driver]\n        if username:\n            strings.append(username)\n            if password:\n                strings.append(':%s@' % password)\n            else:\n                strings.append('@')\n        if host:\n            strings.append(host)\n        if port is not None:\n            strings.append(':%d' % int(port))\n        if database:\n            strings.append('/%s' % database)\n        return ''.join(strings)"}
{"prompt": "def bed(args):\n    \"\"\"\n    %prog bed anchorsfile\n\n    Convert ANCHORS file to BED format.\n    \"\"\"", "output": "\n    from jcvi.formats.bed import Bed\n    from jcvi.formats.bed import BedLine\n    from jcvi.utils.cbook import is_string\n    from jcvi.utils.cbook import is_bool\n    from jcvi.utils.cbook import is_bool_seq\n    from jcvi.utils.cbook import is_valid_name\n    from jcvi.utils.cbook import is_valid_bool\n    from jcvi.utils.cbook import is_valid_bool_name\n    from jcvi.utils.c", "solution": "\n    from collections import defaultdict\n    from jcvi.compara.synteny import AnchorFile, check_beds\n    from jcvi.formats.bed import Bed\n    from jcvi.formats.base import get_number\n\n    p = OptionParser(bed.__doc__)\n    p.add_option(\"--switch\", default=False, action=\"store_true\",\n                 help=\"Switch reference and aligned map elements\")\n    p.add_option(\"--scale\", type=\"float\",\n                 help=\"Scale the aligned map distance by factor\")\n    p.set_beds()\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    anchorsfile, = args\n    switch = opts.switch\n    scale = opts.scale\n    ac = AnchorFile(anchorsfile)\n    pairs = defaultdict(list)\n    for a, b, block_id in ac.iter_pairs():\n        pairs[a].append(b)\n\n    qbed, sbed, qorder, sorder, is_self = check_beds(anchorsfile, p, opts)\n    bd = Bed()\n    for q in qbed:\n        qseqid, qstart, qend, qaccn = q.seqid, q.start, q.end, q.accn\n        if qaccn not in pairs:\n            continue\n        for s in pairs[qaccn]:\n            si, s = sorder[s]\n            sseqid, sstart, send, saccn = s.seqid, s.start, s.end, s.accn\n        if switch:\n            qseqid, sseqid = sseqid, qseqid\n            qstart, sstart = sstart, qstart\n            qend, send = send, qend\n            qaccn, saccn = saccn, qaccn\n        if scale:\n            sstart /= scale\n        try:\n            newsseqid = get_number(sseqid)\n        except ValueError:\n            raise ValueError(\"`{0}` is on `{1}` with no number to extract\".\\\n                                format(saccn, sseqid))\n        bedline = \"\\t\".join(str(x) for x in (qseqid, qstart - 1, qend,\n                            \"{0}:{1}\".format(newsseqid, sstart)))\n        bd.add(bedline)\n\n    bd.print_to_file(filename=opts.outfile, sorted=True)"}
{"prompt": "def verify_recipient(self, recipient):\n        \"\"\"\n        Verify that I'm the recipient of the assertion\n\n        :param recipient: A URI specifying the entity or location to which an\n            attesting entity can present the assertion.\n        :return: True/False\n        \"\"\"", "output": "\n        return self.get_assertion(recipient)\n\n    def verify_audience(self, audience):\n        \"\"\"\n        Verify that the audience is valid\n\n        :param audience: URI specifying the audience to which an\n            attesting entity can present the assertion.\n        :return: True/False\n        \"\"\"\n        return self.get_audience(audience)\n\n    def verify_assertion(self, uri):\n        \"\"\"\n        Verify that the URI is valid\n\n        :param uri: URI specifying the assertion\n        :return: True/False\n        \"\"\"\n        return self.get_uri_type(uri", "solution": "\n        if not self.conv_info:\n            return True\n\n        _info = self.conv_info\n\n        try:\n            if recipient == _info['entity_id']:\n                return True\n        except KeyError:\n            pass\n\n        try:\n            if recipient in self.return_addrs:\n                return True\n        except KeyError:\n            pass\n\n        return False"}
{"prompt": "def read_tsv(f, delimiter=\"\\t\"):\n    \"\"\"\n    Take a tsv filepath and return an array; e.g. [[],[],[]].\n\n    For example, a file containing:\n\n    1   2   3\n    4   5   6\n    7   8   9\n\n    would return the array: [['1','2','3'], ['4','5','6'], ['7','8','9']]\n\n    :param tsv_filepath:\n    :return: tsv_array\n    \"\"\"", "output": "\n    with open(f, \"r\") as f_in:\n        return [line.strip().split(delimiter) for line in f_in]\n\n\ndef read_json(f, delimiter=\"\\t\"):\n    \"\"\"\n    Take a json filepath and return an array; e.g. [[], [1,2,3]].\n\n    For example, a file containing:\n\n    {\"a\": {\"b\": {\"c\": \"d\"}}}\n\n    would return the array: [['a', 'b', 'c'], ['d', 'a', 'b', 'c']]\n\n    :param json_filepath:\n    :", "solution": "\n    tsv_array = []\n    with open(f, \"r\") as f:\n        data_file = csv.reader(f, delimiter=delimiter)\n        for line in data_file:\n            tsv_array.append(line)\n    return (tsv_array)"}
{"prompt": "def grid_destroy_from_ids(oargrid_jobids):\n    \"\"\"Destroy all the jobs with corresponding ids\n\n    Args:\n        oargrid_jobids (list): the ``(site, oar_job_id)`` list of tuple\n            identifying the jobs for each site. \"\"\"", "output": "\n    for jobid in oargrid_jobids:\n        oarjob = OarJob.objects.get(id=jobid[0], site=None)\n        if oarjob.status!= 'DELETED':\n            oargrid_jobids.remove(jobid)\n\n    oargrid_jobs = OargridJob.objects.filter(status='DELETED')\n    grid_ids = []\n    for oargrid_job in oargrid_jobs:\n        grid_ids.append(oargrid_job.id)\n\n    grid_ids = tuple(set(grid_ids))", "solution": "\n    jobs = grid_reload_from_ids(oargrid_jobids)\n    for job in jobs:\n        job.delete()\n        logger.info(\"Killing the jobs %s\" % oargrid_jobids)"}
{"prompt": "def files(context, yes, tag, bundle, before, notondisk):\n    \"\"\"Delete files based on tags.\"\"\"", "output": "\n    if not ondisk:\n        return\n    for f in notondisk:\n        if f.tag == tag:\n            f.delete()\n    return\n\n\ndef _get_files_by_type(context, type):\n    \"\"\"Get files by type.\"\"\"\n    if type == 'all':\n        return _get_all_files()\n    elif type == 'notondisk':\n        return _get_noton_all_files()\n    else:\n        raise ValueError('Invalid type: %s' % type)\n\n\ndef _get_all_files():\n    \"\"\"Get all files.\"\"\"\n    files = []\n   ", "solution": "\n    file_objs = []\n\n    if not tag and not bundle:\n        click.echo(\"I'm afraid I can't let you do that.\")\n        context.abort()\n\n    if bundle:\n        bundle_obj = context.obj['store'].bundle(bundle)\n        if bundle_obj is None:\n            click.echo(click.style('bundle not found', fg='red'))\n            context.abort()\n\n    query = context.obj['store'].files_before(bundle = bundle, tags = tag, before = before)\n\n    if notondisk:\n        file_objs = set(query) - context.obj['store'].files_ondisk(query)\n    else:\n        file_objs = query.all()\n\n    if len(file_objs) > 0 and len(yes) < 2:\n        if not click.confirm(f\"Are you sure you want to delete {len(file_objs)} files?\"):\n            context.abort()\n\n    for file_obj in file_objs:\n        if yes or click.confirm(f\"remove file from disk and database: {file_obj.full_path}\"):\n            file_obj_path = Path(file_obj.full_path)\n            if file_obj.is_included and (file_obj_path.exists() or file_obj_path.is_symlink()):\n                file_obj_path.unlink()\n            file_obj.delete()\n            context.obj['store'].commit()\n            click.echo(f'{file_obj.full_path} deleted')"}
{"prompt": "def get_ssh_gateway_config(vm_):\n    \"\"\"\n    Return the ssh_gateway configuration.\n    \"\"\"", "output": "\n    vm_nics = vm_['nic']\n    ssh_gateway = {}\n    ssh_gateway['name'] = vm_nics['nic'][0]['network']\n    ssh_gateway['properties'] = {}\n    ssh_gateway['properties']['networkProfile'] = network_profile\n    ssh_gateway['properties']['cpuCount'] = len(vm_nics['nic'])\n    ssh_gateway['properties']['memory'] = int(vm_nics['nic'][0]['memory'])\n    ssh_gateway['properties']['cpuSpeed'] = int(vm_nics['nic'][0]['cpuSpeed'])\n    ssh_gateway['properties']['", "solution": "\n    ssh_gateway = config.get_cloud_config_value(\n        'ssh_gateway', vm_, __opts__, default=None,\n        search_global=False\n    )\n\n    # Check to see if a SSH Gateway will be used.\n    if not isinstance(ssh_gateway, six.string_types):\n        return None\n\n    # Create dictionary of configuration items\n\n    # ssh_gateway\n    ssh_gateway_config = {'ssh_gateway': ssh_gateway}\n\n    # ssh_gateway_port\n    ssh_gateway_config['ssh_gateway_port'] = config.get_cloud_config_value(\n        'ssh_gateway_port', vm_, __opts__, default=None,\n        search_global=False\n    )\n\n    # ssh_gateway_username\n    ssh_gateway_config['ssh_gateway_user'] = config.get_cloud_config_value(\n        'ssh_gateway_username', vm_, __opts__, default=None,\n        search_global=False\n    )\n\n    # ssh_gateway_private_key\n    ssh_gateway_config['ssh_gateway_key'] = config.get_cloud_config_value(\n        'ssh_gateway_private_key', vm_, __opts__, default=None,\n        search_global=False\n    )\n\n    # ssh_gateway_password\n    ssh_gateway_config['ssh_gateway_password'] = config.get_cloud_config_value(\n        'ssh_gateway_password', vm_, __opts__, default=None,\n        search_global=False\n    )\n\n    # ssh_gateway_command\n    ssh_gateway_config['ssh_gateway_command'] = config.get_cloud_config_value(\n        'ssh_gateway_command', vm_, __opts__, default=None,\n        search_global=False\n    )\n\n    # Check if private key exists\n    key_filename = ssh_gateway_config['ssh_gateway_key']\n    if key_filename is not None and not os.path.isfile(key_filename):\n        raise SaltCloudConfigError(\n            'The defined ssh_gateway_private_key \\'{0}\\' does not exist'\n            .format(key_filename)\n        )\n    elif (\n        key_filename is None and\n        not ssh_gateway_config['ssh_gateway_password']\n    ):\n        raise SaltCloudConfigError(\n            'No authentication method. Please define: '\n            ' ssh_gateway_password or ssh_gateway_private_key'\n        )\n\n    return ssh_gateway_config"}
{"prompt": "def write(self, ontol, **args):\n        \"\"\"\n        Write a `ontology` object\n        \"\"\"", "output": "\n        self.write_ontology(ontol, *args)\n\n    def write_ontology(self, ontol, **args):\n        \"\"\"\n        Write a `ontology` object\n        \"\"\"\n        self.write_ontology_file(ontol, ontol, *args)\n\n    def write_ontology_file(self, ontol, **args):\n        \"\"\"\n        Write a `ontology` object\n        \"\"\"\n        self.write_ontology_file_as_json(ontol, ontol, *args)\n\n    def write_ontology_file_as_json(self, on", "solution": "\n        s = self.render(ontol, **args)\n        if self.outfile is None:\n            print(s)\n        else:\n            f = open(self.outfile, 'w')\n            f.write(s)\n            f.close()"}
{"prompt": "def pack(self, value=None):\n        \"\"\"Pack the value as a binary representation.\n\n        Returns:\n            bytes: The binary representation.\n\n        \"\"\"", "output": "\n        return self._write_length(len(value)) + self._write_length(len(self._value))\n\n    def unpack(self, data, offset=0):\n        \"\"\"Unpack the value into a binary representation.\n\n        Args:\n            data: The binary representation.\n            offset: The number of bytes to skip at the start of the value.\n\n        Returns:\n            tuple[bytes, int]: The unpacked binary representation.\n\n        \"\"\"\n        return self._read_length(data, offset)\n\n    def __len__(self):\n        \"\"\"Return the number of bytes in the buffer.\"\"\"\n        return self._read_length(", "solution": "\n        if isinstance(value, type(self)):\n            return value.pack()\n\n        if value is None:\n            value = self\n        else:\n            container = type(self)(items=None)\n            container.extend(value)\n            value = container\n\n        bin_message = b''\n        try:\n            for item in value:\n                bin_message += item.pack()\n            return bin_message\n        except exceptions.PackException as err:\n            msg = \"{} pack error: {}\".format(type(self).__name__, err)\n            raise exceptions.PackException(msg)"}
{"prompt": "def bench(image, thread_count):\n  \"\"\"Encode `image` to PNG on `thread_count` threads in parallel.\n\n  Returns:\n    A `float` representing number of seconds that it takes all threads\n    to finish encoding `image`.\n  \"\"\"", "output": "\n  return (time.time() - start) * thread_count\n\n\ndef main():\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      '--image',\n      help='Path to the input image file, e.g., /path/to/example.png')\n  parser.add_argument(\n      '--threads',\n      type=int,\n      default=1,\n      help='Number of threads to run, e.g., 1')\n  parser.add_argument(\n      '--output',\n      help='Path to the output image file, e.g., /path/to/output.", "solution": "\n  threads = [threading.Thread(target=lambda: encoder.encode_png(image))\n             for _ in xrange(thread_count)]\n  start_time = datetime.datetime.now()\n  for thread in threads:\n    thread.start()\n  for thread in threads:\n    thread.join()\n  end_time = datetime.datetime.now()\n  delta = (end_time - start_time).total_seconds()\n  return delta"}
{"prompt": "def schedule_to_array(schedule, events, slots):\n    \"\"\"Convert a schedule from schedule to array form\n\n    Parameters\n    ----------\n    schedule : list or tuple\n        of instances of :py:class:`resources.ScheduledItem`\n    events : list or tuple\n        of :py:class:`resources.Event` instances\n    slots : list or tuple\n        of :py:class:`resources.Slot` instances\n\n    Returns\n    -------\n    np.array\n        An E by S array (X) where E is the number of events and S the\n        number of slots. Xij is 1 if event i is scheduled in slot j and\n        zero otherwise\n    \"\"\"", "output": " else 0.\n    \"\"\"\n    if isinstance(schedule, (tuple, list)):\n        return np.array(events)\n    elif isinstance(schedule, (list, tuple)):\n        return np.array(slots)\n    else:\n        raise ValueError(\"Schedule must be a tuple or list of E instances\")\n\n\nclass Epochs(object):\n    \"\"\"Container for epochs\n\n    Parameters\n    ----------\n    epochs : list\n        The epochs to be iterated over\n    \"\"\"\n    def __init__(self, epochs):\n        self.epochs = epochs\n\n    def __iter__(self):\n        return self.epochs\n\n    def next(self):\n        raise NotImplementedError", "solution": "\n    array = np.zeros((len(events), len(slots)), dtype=np.int8)\n    for item in schedule:\n        array[events.index(item.event), slots.index(item.slot)] = 1\n    return array"}
{"prompt": "def activate(self, span, finish_on_close):\n        \"\"\"\n        Make a :class:`~opentracing.Span` instance active.\n\n        :param span: the :class:`~opentracing.Span` that should become active.\n        :param finish_on_close: whether *span* should automatically be\n            finished when :meth:`Scope.close()` is called.\n\n        :return: a :class:`~opentracing.Scope` instance to control the end\n            of the active period for the :class:`~opentracing.Span`.\n            It is a programming error to neglect to call :meth:`Scope.close()`\n            on the returned instance.\n        \"\"\"", "output": " the user.\n        \"\"\"\n        self._set_state(self.STATE_ACTIVE)\n        self._set_span(span)\n        self._set_finish_on_close(finish_on_close)\n        self._set_span_state(self.STATE_ACTIVE)\n        return self\n\n    def deactivate(self, span):\n        \"\"\"\n        Make a :class:`~opentracing.Span` instance inactive.\n\n        :param span: the :class:`~opentracing.Span` that should become inactive.\n        :return: a :class:`~opentracing.Scope instance to control the end", "solution": "\n\n        scope = _GeventScope(self, span, finish_on_close)\n        self._set_greenlet_scope(scope)\n\n        return scope"}
{"prompt": "def info(gandi, resource, id, altnames, csr, cert, all_status):\n    \"\"\" Display information about a certificate.\n\n    Resource can be a CN or an ID\n    \"\"\"", "output": "\n    if not id:\n        return\n    if not resource:\n        return\n    if not all_status:\n        return\n    if not isinstance(altnames, list):\n        altnames = [altnames]\n    if not isinstance(csr, str):\n        csr = str(csr)\n    if not isinstance(cert, str):\n        cert = str(cert)\n    if not isinstance(all_status, list):\n        all_status = [all_status]\n    if not all_status:\n        return\n    if isinstance(gandi, str):\n        gandi = [gandi]\n    if isinstance(resource, str", "solution": "\n    output_keys = ['cn', 'date_created', 'date_end', 'plan', 'status']\n\n    if id:\n        output_keys.append('id')\n\n    if altnames:\n        output_keys.append('altnames')\n\n    if csr:\n        output_keys.append('csr')\n\n    if cert:\n        output_keys.append('cert')\n\n    ids = []\n    for res in resource:\n        ids.extend(gandi.certificate.usable_ids(res))\n\n    result = []\n    for num, id_ in enumerate(set(ids)):\n        cert = gandi.certificate.info(id_)\n        if not all_status and cert['status'] not in ['valid', 'pending']:\n            continue\n        if num:\n            gandi.separator_line()\n        cert['plan'] = package_desc(gandi, cert['package'])\n        output_cert(gandi, cert, output_keys)\n        result.append(cert)\n\n    return result"}
{"prompt": "def draw(self):\n        \"\"\"\n        Clear the terminal screen and redraw all of the sub-windows\n        \"\"\"", "output": "\n        self.clear()\n        self.draw_windows()\n\n    def draw_windows(self):\n        \"\"\"\n        Draw the windows in the current window\n        \"\"\"\n        for win in self.windows:\n            win.draw()\n        self.windows = []\n\n    def draw_windows_with_label(self, win_label):\n        \"\"\"\n        Draw the windows in the current window with a label\n        \"\"\"\n        for win in self.windows:\n            win.draw_label(win_label)\n        self.windows_label.draw()\n\n    def draw_windows_with_label_and_window(self, win_", "solution": "\n        n_rows, n_cols = self.term.stdscr.getmaxyx()\n        if n_rows < self.term.MIN_HEIGHT or n_cols < self.term.MIN_WIDTH:\n            # TODO: Will crash when you try to navigate if the terminal is too\n            # small at startup because self._subwindows will never be populated\n            return\n\n        self._row = 0\n        self._draw_header()\n        self._draw_banner()\n        self._draw_content()\n        self._draw_footer()\n        self.term.clear_screen()\n        self.term.stdscr.refresh()"}
{"prompt": "def publish(build_entry: dict):\n    \"\"\"Publishes the specified build entry to docker hub\"\"\"", "output": "\n    if build_entry.get('docker_version') is not None:\n        return _publish_docker_hub(build_entry['docker_version'], build_entry['docker_image'])\n    else:\n        return _publish_docker_hub(None, None)\n\n\ndef _publish_docker_hub(version: str, image: str) -> None:\n    \"\"\"Publishes a docker hub to the DockerHub\"\"\"\n    if version is None:\n        version = 'latest'\n    if image is None:\n        image = 'alpine:latest'\n    try:\n        client = docker.from_", "solution": "\n    for tag in build_entry['tags']:\n        print('[PUSHING]:', tag)\n        os.system('docker push {}'.format(tag))"}
{"prompt": "def _read_import_root_map_file(path):\n    \"\"\"Reads a file mapping import paths to roots (e.g., example.org/pkg/foo -> example.org).\"\"\"", "output": "\n    with open(path, 'r') as f:\n        return {\n            os.path.basename(path): os.path.dirname(path)\n            for path in f.read().splitlines()\n        }\n\n\ndef _get_cache_dir(cache_dir):\n    \"\"\"Returns the cache directory for the given directory.\"\"\"\n    return os.path.join(cache_dir, '.cache')\n\n\ndef _get_cache_file(cache_file):\n    \"\"\"Returns the path to the given directory in the cache.\"\"\"\n    return os.path.join(_get_cache_dir(cache_dir", "solution": "\n    if os.path.exists(path):\n      with open(path, 'r') as fp:\n        return dict({import_path: root for import_path, root in\n                     (x.strip().split('\\t') for x in fp.readlines())})\n    else:\n      return {}"}
{"prompt": "def save_config(self, cmd=\"write mem\", confirm=False, confirm_response=\"\"):\n        \"\"\"Saves Config Using Copy Run Start\"\"\"", "output": "\n        return self.send_command(cmd, \"copy running-config startup-config\")\n\n    def get_config(self, cmd=\"read mem\", confirm=False, confirm_response=\"\"):\n        \"\"\"Gets Config using Copy Run Start\"\"\"\n        return self.send_command(cmd, \"running-config\")\n\n    def get_config_status(self, cmd=\"get running-config status\", confirm=False):\n        \"\"\"Gets the running-config status using Get Running Config\"\"\"\n        return self.send_command(cmd, \"running-config status\")\n\n    def get_config_info(self, cmd=\"get", "solution": "\n        return super(CiscoIosBase, self).save_config(\n            cmd=cmd, confirm=confirm, confirm_response=confirm_response\n        )"}
{"prompt": "def pick_cert_for_twisted(netloc, possible):\n    \"\"\"\n    Pick the right client key/certificate to use for the given server and\n    return it in the form Twisted wants.\n\n    :param NetLocation netloc: The location of the server to consider.\n    :param dict[TLSCredentials] possible: The available credentials from which\n        to choose.\n\n    :return: A two-tuple.  If no credentials were found, the elements are\n        ``None`` and ``[]``.  Otherwise, the first element is a\n        ``twisted.internet.ssl.PrivateCertificate`` instance representing the\n        client certificate to use and the second element is a ``tuple`` of\n        ``twisted.internet.ssl.Certificate`` instances representing the rest\n        of the chain necessary to validate the client certificate.\n    \"\"\"", "output": " use, and the second element is a\n        ``twisted.internet.ssl.Certificate`` instance representing the client\n        certificate to use.\n    \"\"\"\n    for cert in possible.values():\n        if cert is not None:\n            return (cert, None)\n    for entry in NetLocation.get_all():\n        if entry is not None:\n            return (entry.get_peer_certificate(), None)\n    raise ValueError(\"No suitable certificate found for %s\" % (netloc,))\n\ndef _get_client_certificate(netloc):\n    \"\"\"\n    Pick the right client certificate to use for the given server and\n    return it", "solution": "\n    try:\n        creds = possible[netloc]\n    except KeyError:\n        return (None, ())\n\n    key = ssl.KeyPair.load(creds.key.as_bytes(), FILETYPE_PEM)\n    return (\n        ssl.PrivateCertificate.load(\n            creds.chain.certificates[0].as_bytes(), key, FILETYPE_PEM,\n        ),\n        tuple(\n            ssl.Certificate.load(cert.as_bytes(), FILETYPE_PEM)\n            for cert\n            in creds.chain.certificates[1:]\n        ),\n    )"}
{"prompt": "def fillna(self, value, limit=None, inplace=False, downcast=None):\n        \"\"\" fillna on the block with the value. If we fail, then convert to\n        ObjectBlock and try again\n        \"\"\"", "output": "\n        if inplace:\n            raise ValueError(\"Cannot fillna with a block with a block inplace\")\n        values = np.asarray(value)\n        values = values.fillna(value, limit=limit)\n        return self.make_block_same_class(values)\n\n    def fillna_indexer(self, value, limit=None):\n        \"\"\" fillna on the block with the value.  If we fail, then convert to\n        ObjectBlock and try again\n        \"\"\"\n        if inplace:\n            raise ValueError(\"Cannot fillna with a block with a block inplace\")\n        values = np.asarray(value)\n        values", "solution": "\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n\n        if not self._can_hold_na:\n            if inplace:\n                return self\n            else:\n                return self.copy()\n\n        mask = isna(self.values)\n        if limit is not None:\n            if not is_integer(limit):\n                raise ValueError('Limit must be an integer')\n            if limit < 1:\n                raise ValueError('Limit must be greater than 0')\n            if self.ndim > 2:\n                raise NotImplementedError(\"number of dimensions for 'fillna' \"\n                                          \"is currently limited to 2\")\n            mask[mask.cumsum(self.ndim - 1) > limit] = False\n\n        # fillna, but if we cannot coerce, then try again as an ObjectBlock\n        try:\n            values, _ = self._try_coerce_args(self.values, value)\n            blocks = self.putmask(mask, value, inplace=inplace)\n            blocks = [b.make_block(values=self._try_coerce_result(b.values))\n                      for b in blocks]\n            return self._maybe_downcast(blocks, downcast)\n        except (TypeError, ValueError):\n\n            # we can't process the value, but nothing to do\n            if not mask.any():\n                return self if inplace else self.copy()\n\n            # operate column-by-column\n            def f(m, v, i):\n                block = self.coerce_to_target_dtype(value)\n\n                # slice out our block\n                if i is not None:\n                    block = block.getitem_block(slice(i, i + 1))\n                return block.fillna(value,\n                                    limit=limit,\n                                    inplace=inplace,\n                                    downcast=None)\n\n            return self.split_and_operate(mask, f, inplace)"}
{"prompt": "def CaffeLMDB(lmdb_path, shuffle=True, keys=None):\n    \"\"\"\n    Read a Caffe LMDB file where each value contains a ``caffe.Datum`` protobuf.\n    Produces datapoints of the format: [HWC image, label].\n\n    Note that Caffe LMDB format is not efficient: it stores serialized raw\n    arrays rather than JPEG images.\n\n    Args:\n        lmdb_path, shuffle, keys: same as :class:`LMDBData`.\n\n    Returns:\n        a :class:`LMDBDataDecoder` instance.\n\n    Example:\n        .. code-block:: python\n\n            ds = CaffeLMDB(\"/tmp/validation\", keys='{:0>8d}')\n    \"\"\"", "output": " python\n\n            # Decode the raw label file.\n            caffe_label = caffe.proto.caffe_pb2.DatumProto()\n            caffe_label.ParseFromString(open(label_file, 'rb').read())\n\n            # Decode the raw image file.\n            image = caffe.io.decode_caffe_pb(image_data)\n\n            # Decode the label.\n            labels = caffe.io.decode_labels(image, caffe_label)\n    \"\"\"\n    if keys is None:\n        keys = []\n    if shuffle:\n        random.shuffle(keys)\n    if keys:\n        datums", "solution": "\n\n    cpb = get_caffe_pb()\n    lmdb_data = LMDBData(lmdb_path, shuffle, keys)\n\n    def decoder(k, v):\n        try:\n            datum = cpb.Datum()\n            datum.ParseFromString(v)\n            img = np.fromstring(datum.data, dtype=np.uint8)\n            img = img.reshape(datum.channels, datum.height, datum.width)\n        except Exception:\n            log_once(\"Cannot read key {}\".format(k), 'warn')\n            return None\n        return [img.transpose(1, 2, 0), datum.label]\n    logger.warn(\"Caffe LMDB format doesn't store jpeg-compressed images, \\\n        it's not recommended due to its inferior performance.\")\n    return LMDBDataDecoder(lmdb_data, decoder)"}
{"prompt": "def _init_metadata(self):\n        \"\"\"Have to call these all separately because they are \"end\" classes,\n        with no super() in them. Non-cooperative.\"\"\"", "output": "\n        self.metadata = {}\n        self.metadata['id'] = self.id\n        self.metadata['name'] = self.name\n        self.metadata['description'] = self.description\n        self.metadata['author'] = self.author\n        self.metadata['version'] = self.version\n        self.metadata['url'] = self.url\n        self.metadata['license'] = self.license\n        self.metadata['copyright'] = self.copyright\n        self.metadata['version'] = self.version\n        self.metadata['author'] = self.author\n        self.metadata['email'] = self.email\n       ", "solution": "\n        ItemTextsFormRecord._init_metadata(self)\n        ItemFilesFormRecord._init_metadata(self)\n        edXBaseFormRecord._init_metadata(self)\n        IRTItemFormRecord._init_metadata(self)\n        TimeValueFormRecord._init_metadata(self)\n        ProvenanceFormRecord._init_metadata(self)\n        super(edXItemFormRecord, self)._init_metadata()"}
{"prompt": "def update_paths_and_config(self, config, pkg_dir_name,\n                                pkg_cache_dir=None):\n        \"\"\"Handle remote source defined sys.paths & configs.\n\n        Args:\n            config (dict): git config dictionary\n            pkg_dir_name (string): directory name of the stacker archive\n            pkg_cache_dir (string): fully qualified path to stacker cache\n                                    cache directory\n\n        \"\"\"", "output": "\n        if pkg_dir_name is None:\n            pkg_dir_name = os.path.join(config['git_dir'], 'packages')\n        if pkg_cache_dir is None:\n            pkg_cache_dir = os.path.join(config['git_dir'],'stacker-cache')\n        if pkg_dir_name is None:\n            pkg_dir_name = os.path.join(config['git_dir'], 'config')\n        if pkg_dir_name is None:\n            return\n        if pkg_cache_dir is None:\n            pkg_cache_dir =", "solution": "\n        if pkg_cache_dir is None:\n            pkg_cache_dir = self.package_cache_dir\n        cached_dir_path = os.path.join(pkg_cache_dir, pkg_dir_name)\n\n        # Add the appropriate directory (or directories) to sys.path\n        if config.get('paths'):\n            for path in config['paths']:\n                path_to_append = os.path.join(cached_dir_path,\n                                              path)\n                logger.debug(\"Appending \\\"%s\\\" to python sys.path\",\n                             path_to_append)\n                sys.path.append(path_to_append)\n        else:\n            sys.path.append(cached_dir_path)\n\n        # If the configuration defines a set of remote config yamls to\n        # include, add them to the list for merging\n        if config.get('configs'):\n            for config_filename in config['configs']:\n                self.configs_to_merge.append(os.path.join(cached_dir_path,\n                                                          config_filename))"}
{"prompt": "async def destroy_tournament(self, t: Tournament):\n        \"\"\" completely removes a tournament from Challonge\n\n        |methcoro|\n\n        Note:\n            |from_api| Deletes a tournament along with all its associated records. There is no undo, so use with care!\n\n        Raises:\n            APIException\n\n        \"\"\"", "output": "\n        await self.bot.say(\"You are not allowed to destroy a challonge.\")\n\n    @commands.command(pass_context=True, aliases=['add_tournament'])\n    async def add_tournament_record(self, t: TournamentRecord):\n        \"\"\" adds a tournament record to the current list of records\n\n        Args:\n            t: Tournament record\n\n        Raises:\n            APIException\n\n        \"\"\"\n        await self.to_api.add_tournament_record(t)\n\n    @commands.command(pass_context=True, aliases=['destroy_tournament'])\n    async def destroy_", "solution": "\n        await self.connection('DELETE', 'tournaments/{}'.format(t.id))\n        if t in self.tournaments:\n            self.tournaments.remove(t)"}
{"prompt": "def get_issuers(self):\n        \"\"\"\n        Gets the issuers (from message and from assertion)\n\n        :returns: The issuers\n        :rtype: list\n        \"\"\"", "output": "\n        return self._issuers\n\n    def get_issuers_with_message(self, message):\n        \"\"\"\n        Gets the issuers with a message\n\n        :param message: The message\n        :type message: str\n        :returns: The issuers with a message\n        :rtype: list\n        \"\"\"\n        return self._issuers\n\n    def get_issuers_with_assertion(self, assertions):\n        \"\"\"\n        Gets the issuers with a list of assertions\n\n        :param assertions: The assertions\n        :type assertions: list\n        :returns: The issuers with a list of assertions\n       ", "solution": "\n        issuers = []\n\n        message_issuer_nodes = OneLogin_Saml2_Utils.query(self.document, '/samlp:Response/saml:Issuer')\n        if len(message_issuer_nodes) > 0:\n            if len(message_issuer_nodes) == 1:\n                issuers.append(OneLogin_Saml2_Utils.element_text(message_issuer_nodes[0]))\n            else:\n                raise OneLogin_Saml2_ValidationError(\n                    'Issuer of the Response is multiple.',\n                    OneLogin_Saml2_ValidationError.ISSUER_MULTIPLE_IN_RESPONSE\n                )\n\n        assertion_issuer_nodes = self.__query_assertion('/saml:Issuer')\n        if len(assertion_issuer_nodes) == 1:\n            issuers.append(OneLogin_Saml2_Utils.element_text(assertion_issuer_nodes[0]))\n        else:\n            raise OneLogin_Saml2_ValidationError(\n                'Issuer of the Assertion not found or multiple.',\n                OneLogin_Saml2_ValidationError.ISSUER_NOT_FOUND_IN_ASSERTION\n            )\n\n        return list(set(issuers))"}
{"prompt": "def blob(self, sha):\n        \"\"\"Get the blob indicated by ``sha``.\n\n        :param str sha: (required), sha of the blob\n        :returns: :class:`Blob <github3.git.Blob>` if successful, otherwise\n            None\n        \"\"\"", "output": "\n        return self._get_single_ref(self._get_blob, sha)\n\n    def _get_single_ref(self, ref, sha):\n        \"\"\"Get a single reference.\n\n        :param str ref: (required), reference name\n        :param str sha: (required), SHA of the reference\n        :returns: :class:`Reference <github3.git.Reference>` if successful,\n            otherwise None\n        \"\"\"\n        return self._get_ref(self._get_blob, ref, sha)\n\n    def _get_ref(self, ref, ref_type, sha):\n        \"\"\"Get a reference", "solution": "\n        url = self._build_url('git', 'blobs', sha, base_url=self._api)\n        json = self._json(self._get(url), 200)\n        return Blob(json) if json else None"}
{"prompt": "def _get_data_from_bigquery(self, queries):\n    \"\"\"Get data from bigquery table or query.\"\"\"", "output": "\n    if self.table_schema:\n      return self.table_schema.get_table(self.table_name, queries)\n    else:\n      return self.bigquery_client.get_table(self.dataset_name, queries)\n\n  def _get_table_schema(self):\n    \"\"\"Get table schema.\"\"\"\n    if self.table_schema:\n      return self.table_schema\n    else:\n      return self.bigquery_client.get_schema(self.dataset_name)\n\n  def _get_table_name(self):\n    \"\"\"Get table name.\"\"\"\n    if self.dataset_name", "solution": "\n\n    all_df = []\n    for query in queries:\n      all_df.append(query.execute().result().to_dataframe())\n    df = pd.concat(all_df, ignore_index=True)\n    return df"}
{"prompt": "def _parse_canonical_double(doc):\n    \"\"\"Decode a JSON double to python float.\"\"\"", "output": "\n    if not isinstance(doc, dict):\n        return None\n    return float(doc[\"double\"])\n\n\ndef _parse_canonical_string(doc):\n    \"\"\"Decode a JSON string to python str.\"\"\"\n    if not isinstance(doc, dict):\n        return None\n    return doc[\"string\"]\n\n\ndef _parse_canonical_string_list(doc):\n    \"\"\"Decode a JSON string to python list.\"\"\"\n    if not isinstance(doc, list):\n        return None\n    return doc[\"list\"]\n\n\ndef _parse_canonical_time(doc):\n    \"\"\"Decode a JSON time to python datetime.\"\"\"\n    if not isinstance", "solution": "\n    d_str = doc['$numberDouble']\n    if len(doc) != 1:\n        raise TypeError('Bad $numberDouble, extra field(s): %s' % (doc,))\n    if not isinstance(d_str, string_type):\n        raise TypeError('$numberDouble must be string: %s' % (doc,))\n    return float(d_str)"}
{"prompt": "def version(self, value):\n        \"\"\"\n        Setter for **self.__version** attribute.\n\n        :param value: Attribute value.\n        :type value: unicode\n        \"\"\"", "output": "\n\n        self.__version = value\n\n    def __init__(self, value):\n        \"\"\"\n        Setter for **self.__version** attribute.\n\n        :param value: Attribute value.\n        :type value: unicode\n        \"\"\"\n\n        self.set_attr(value)\n\n    def set_attr(self, value):\n        \"\"\"\n        Setter for **self.__attributes** attribute.\n\n        :param value: Attribute value.\n        :type value: unicode\n        \"\"\"\n\n        self.attr = value\n\n    def version(self):\n        \"\"\"\n        Getter for **self.__version** attribute.\n\n        :return: Attribute value.\n        :rtype:", "solution": "\n\n        if value is not None:\n            assert type(value) is unicode, \"'{0}' attribute: '{1}' type is not 'unicode'!\".format(\n                \"version\", value)\n        self.__version = value"}
{"prompt": "def get_portchannel_info_by_intf_output_lacp_actor_priority(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"", "output": "\n        config = ET.Element(\"config\")\n        portchannel_info = ET.SubElement(config, \"portchannel-info\", xmlns=\"urn:brocade.com:mgmt:brocade-interface\")\n        portchannel_info_name = ET.SubElement(portchannel_info, \"portchannel-info-name\")\n        portchannel_info_name.text = kwargs.pop('name')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(portchannel_info, **kwargs)\n\n    def get_portchannel_info_output_lacp_actor_priority_info", "solution": "\n        config = ET.Element(\"config\")\n        get_portchannel_info_by_intf = ET.Element(\"get_portchannel_info_by_intf\")\n        config = get_portchannel_info_by_intf\n        output = ET.SubElement(get_portchannel_info_by_intf, \"output\")\n        lacp = ET.SubElement(output, \"lacp\")\n        actor_priority = ET.SubElement(lacp, \"actor-priority\")\n        actor_priority.text = kwargs.pop('actor_priority')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)"}
{"prompt": "def sparse_arrays(self, value):\n        \"\"\"Validate and enable spare arrays.\"\"\"", "output": "\n        if not isinstance(value, (np.ndarray, SparseArray)):\n            raise TypeError(\"SparseArray.sparse_arrays must be called with a \"\n                            \"sparse matrix as its input.\")\n        if value.dtype.kind!= \"b\":\n            raise TypeError(\"SparseArray.sparse_arrays must be called with a \"\n                            \"b matrix as its input.\")\n        if value.dtype.kind!= \"c\":\n            raise TypeError(\"SparseArray.sparse_arrays must be called with a \"\n                            \"c matrix as its input.\")\n\n    def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):", "solution": "\n        if not isinstance(value, bool):\n            raise TypeError('sparse_arrays attribute must be a logical type.')\n        self._sparse_arrays = value"}
{"prompt": "def police_priority_map_conform_map_pri7_conform(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"", "output": "\n        config = ET.Element(\"config\")\n        police_priority_map = ET.SubElement(config, \"police-priority-map\")\n        police_priority_map_name = ET.SubElement(police_priority_map, \"police-priority-map-name\")\n        police_priority_map_name.text = kwargs.pop('police_priority_map_name')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)\n        \n    def police_priority_map_conform_map_pri_map_name(self,", "solution": "\n        config = ET.Element(\"config\")\n        police_priority_map = ET.SubElement(config, \"police-priority-map\", xmlns=\"urn:brocade.com:mgmt:brocade-policer\")\n        name_key = ET.SubElement(police_priority_map, \"name\")\n        name_key.text = kwargs.pop('name')\n        conform = ET.SubElement(police_priority_map, \"conform\")\n        map_pri7_conform = ET.SubElement(conform, \"map-pri7-conform\")\n        map_pri7_conform.text = kwargs.pop('map_pri7_conform')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)"}
{"prompt": "def save(self, **kwargs):\n        \"\"\"\n        Save and return a list of object instances.\n        \"\"\"", "output": "\n        if self.pk is None:\n            raise Exception(\"Cannot save an unsaved object\")\n        return [self.get_object(pk=o.pk) for o in self.get_queryset(**kwargs)]\n\n    def get_queryset(self, **kwargs):\n        \"\"\"\n        Returns a queryset matching the given keyword arguments.\n        \"\"\"\n        return self.model._default_manager.filter(**kwargs)\n\n    def get_context_data(self, **kwargs):\n        \"\"\"\n        Insert the list of messages into the context for the response.\n        \"\"\"\n        context = super(MessageListView, self).get_context_", "solution": "\n        # Guard against incorrect use of `serializer.save(commit=False)`\n        assert 'commit' not in kwargs, (\n            \"'commit' is not a valid keyword argument to the 'save()' method. \"\n            \"If you need to access data before committing to the database then \"\n            \"inspect 'serializer.validated_data' instead. \"\n            \"You can also pass additional keyword arguments to 'save()' if you \"\n            \"need to set extra attributes on the saved model instance. \"\n            \"For example: 'serializer.save(owner=request.user)'.'\"\n        )\n\n        validated_data = [\n            dict(list(attrs.items()) + list(kwargs.items()))\n            for attrs in self.validated_data\n        ]\n\n        if self.instance is not None:\n            self.instance = self.update(self.instance, validated_data)\n            assert self.instance is not None, (\n                '`update()` did not return an object instance.'\n            )\n        else:\n            self.instance = self.create(validated_data)\n            assert self.instance is not None, (\n                '`create()` did not return an object instance.'\n            )\n\n        return self.instance"}
{"prompt": "def create_transaction_output(address, output_value):\n    \"\"\"\n    Create a single transaction output\n    :param address:\n    :param output_value:\n    :return:\n    \"\"\"", "output": "\n    return TransactionOutput(\n        address=address,\n        amount=output_value,\n        fee=output_value,\n        fee_currency=output_value,\n        fee_value=output_value,\n        fee_currency_id=output_value,\n        fee_value_currency=output_value,\n        fee_value_value=output_value,\n        fee_value_value_currency_id=output_value,\n        fee_value_value=output_value,\n        fee_value_id=output_value,\n        fee_value,\n        fee_currency,\n        fee_value_currency", "solution": "\n    bitcoin_address = CBitcoinAddress(address)\n    tx_out = CMutableTxOut(output_value, bitcoin_address.to_scriptPubKey())\n    return tx_out"}
{"prompt": "def set_years(self, startyear=1765, endyear=2100):\n        \"\"\"\n        Set the start and end dates of the simulations.\n\n        Parameters\n        ----------\n        startyear : int\n            Start year of the simulation\n\n        endyear : int\n            End year of the simulation\n\n        Returns\n        -------\n        dict\n            The contents of the namelist\n        \"\"\"", "output": "\n        self.namelist['years'] = startyear, endyear\n        return self\n\n    def set_time_units(self, units='seconds'):\n        \"\"\"\n        Set the time units of the simulation.\n\n        Parameters\n        ----------\n        units : str\n            The units of the time dimension.\n\n        Returns\n        -------\n        dict\n            The contents of the namelist\n        \"\"\"\n        self.namelist['time_units'] = units,\n        return self\n\n    def set_time_units_units(self, units='seconds'):\n        \"\"\"\n        Set the units of the time dimension.\n\n        Parameters\n        ----------\n        units : str\n            The units of the time", "solution": "\n        # TODO: test altering stepsperyear, I think 1, 2 and 24 should all work\n        return self.set_config(\n            \"MAGCFG_NMLYEARS.CFG\",\n            \"nml_years\",\n            endyear=endyear,\n            startyear=startyear,\n            stepsperyear=12,\n        )"}
{"prompt": "def target_sdp_state(self, state):\n        \"\"\"Update the target state of SDP.\"\"\"", "output": "\n        self._update_device_info(state)\n        self._state = state\n\n    def _update_device_info(self, state):\n        \"\"\"Update the device info.\"\"\"\n        self._device_info = {\n            \"identifiers\": {(DOMAIN, self._device_id)},\n            \"name\": state.name,\n            \"manufacturer\": state.manufacturer,\n            \"model\": state.model,\n            \"sw_version\": state.swversion,\n        }\n\n\nclass SsdpEntity(MediaPlayerDevice):\n    \"\"\"Representation of a SSDP entity.\"\"\"\n\n    def __init__(self, ssdp_", "solution": "\n        LOG.info('Setting SDP target state to %s', state)\n        if self._sdp_state.current_state == state:\n            LOG.info('Target state ignored, SDP is already \"%s\"!', state)\n        if state == 'on':\n            self.set_state(DevState.ON)\n        if state == 'off':\n            self.set_state(DevState.OFF)\n        if state == 'standby':\n            self.set_state(DevState.STANDBY)\n        if state == 'disable':\n            self.set_state(DevState.DISABLE)\n        self._sdp_state.update_target_state(state)"}
{"prompt": "def presenter(self, presenter_name):\n\t\t\"\"\" :meth:`.WWebPresenterCollectionProto.presenter` method implementation\n\t\t\"\"\"", "output": "\n\t\treturn WWebPresenter(self, presenter_name)\n\n\tdef get_presenter(self, presenter_name):\n\t\t\"\"\" :meth:`.WWebPresenterCollectionProto.get_presenter` implementation\n\t\t\"\"\"\n\t\treturn WWebPresenter(self, presenter_name)\n\n\tdef get_presenter_by_name(self, presenter_name):\n\t\t\"\"\" :meth:`.WWebPresenterCollectionProto.get_presenter_by_name` implementation\n\t\t\"\"\"\n\t\treturn WWebPresenterCollection(self, presenter_name", "solution": "\n\t\tif presenter_name in self.__presenters.keys():\n\t\t\treturn self.__presenters[presenter_name]"}
{"prompt": "def work_once(self, free_pool_slots=1, max_jobs=None):\n        \"\"\" Does one lookup for new jobs, inside the inner work loop \"\"\"", "output": "\n        if max_jobs is None:\n            max_jobs = self.max_jobs\n        if max_jobs < 1:\n            max_jobs = 1\n        if max_jobs > 1:\n            max_jobs = 1\n        if max_jobs == 0:\n            max_jobs = self.max_jobs\n        if max_jobs > 1:\n            max_jobs = 1\n        if max_jobs > 0:\n            self.logger.info(\"Using %d jobs\", max_jobs)\n        else:\n            self.logger.info(\"Using 1 jobs\")\n        self.logger.info(\"Using %d slots\",", "solution": "\n\n        dequeued_jobs = 0\n\n        available_queues = [\n            queue for queue in self.queues\n            if queue.root_id not in self.paused_queues and\n            queue.id not in self.paused_queues\n        ]\n\n        for queue_i in range(len(available_queues)):\n\n            queue = available_queues[(queue_i + self.queue_offset) % len(available_queues)]\n\n            max_jobs_per_queue = free_pool_slots - dequeued_jobs\n\n            if max_jobs_per_queue <= 0:\n                queue_i -= 1\n                break\n\n            if self.config[\"dequeue_strategy\"] == \"parallel\":\n                max_jobs_per_queue = max(1, int(max_jobs_per_queue / (len(available_queues) - queue_i)))\n\n            for job in queue.dequeue_jobs(\n                max_jobs=max_jobs_per_queue,\n                job_class=self.job_class,\n                worker=self\n            ):\n                dequeued_jobs += 1\n\n                self.gevent_pool.spawn(self.perform_job, job)\n\n        # At the next pass, start at the next queue to avoid always dequeuing the same one\n        if self.config[\"dequeue_strategy\"] == \"parallel\":\n            self.queue_offset = (self.queue_offset + queue_i + 1) % len(self.queues)\n\n        # TODO consider this when dequeuing jobs to have strict limits\n        if max_jobs and self.done_jobs >= max_jobs:\n            self.log.info(\"Reached max_jobs=%s\" % self.done_jobs)\n            return \"break\", dequeued_jobs\n\n        # We seem to have exhausted available jobs, we can sleep for a\n        # while.\n        if dequeued_jobs == 0:\n\n            if self.config[\"dequeue_strategy\"] == \"burst\":\n                self.log.info(\"Burst mode: stopping now because queues were empty\")\n                return \"break\", dequeued_jobs\n\n            return \"wait\", dequeued_jobs\n\n        return None, dequeued_jobs"}
{"prompt": "def serveMonth(self, request, year=None, month=None):\n        \"\"\"Monthly calendar view.\"\"\"", "output": "\n        if year is None:\n            year = self.currentYear\n        if month is None:\n            month = self.currentMonth\n        if year is None:\n            month = self.currentYear + 1\n        if month is None:\n            month = 1\n        if month is None:\n            month = 12\n        if year is None:\n            year = self.currentYear\n        if month is None:\n            month = self.currentMonth + 1\n        if year is None:\n            year = self.currentYear\n        if month is None:\n            month = 12\n        if year is None:\n            year = self.currentYear\n        if month", "solution": "\n        myurl = self.get_url(request)\n        def myUrl(urlYear, urlMonth):\n            if 1900 <= urlYear <= 2099:\n                return myurl + self.reverse_subpage('serveMonth',\n                                                    args=[urlYear, urlMonth])\n        today = timezone.localdate()\n        if year is None: year = today.year\n        if month is None: month = today.month\n        year = int(year)\n        month = int(month)\n\n        if year == today.year and month == today.month:\n            weekNum = gregorian_to_week_date(today)[1]\n        else:\n            weekNum = gregorian_to_week_date(dt.date(year, month, 7))[1]\n        weeklyUrl = myurl + self.reverse_subpage('serveWeek',\n                                                 args=[year, weekNum])\n        listUrl = myurl + self.reverse_subpage('serveUpcoming')\n\n        prevMonth = month - 1\n        prevMonthYear = year\n        if prevMonth == 0:\n            prevMonth = 12\n            prevMonthYear -= 1\n\n        nextMonth = month + 1\n        nextMonthYear = year\n        if nextMonth == 13:\n            nextMonth = 1\n            nextMonthYear += 1\n\n        # TODO Consider changing to a TemplateResponse\n        # https://stackoverflow.com/questions/38838601\n        return render(request, \"joyous/calendar_month.html\",\n                      {'self':         self,\n                       'page':         self,\n                       'version':      __version__,\n                       'year':         year,\n                       'month':        month,\n                       'today':        today,\n                       'yesterday':    today - dt.timedelta(1),\n                       'lastweek':     today - dt.timedelta(7),\n                       'prevMonthUrl': myUrl(prevMonthYear, prevMonth),\n                       'nextMonthUrl': myUrl(nextMonthYear, nextMonth),\n                       'prevYearUrl':  myUrl(year - 1, month),\n                       'nextYearUrl':  myUrl(year + 1, month),\n                       'weeklyUrl':    weeklyUrl,\n                       'listUrl':      listUrl,\n                       'thisMonthUrl': myUrl(today.year, today.month),\n                       'monthName':    MONTH_NAMES[month],\n                       'weekdayAbbr':  weekday_abbr,\n                       'events':       self._getEventsByWeek(request, year, month)})"}
{"prompt": "def remove_each(text, items, count=None, strip=False):\n    \"\"\"\n    Like ``remove``, where each occurrence in ``items`` is ``what`` to remove.\n    \"\"\"", "output": "\n    if count is not None:\n        return [i for i in items if i.text == text][0]\n    else:\n        return [i for i in items if i.text!= text][0]\n\n\ndef remove_all(text, items):\n    \"\"\"\n    Like ``remove_each``, but removes all occurrences of ``text`` in ``items``.\n    \"\"\"\n    if count is not None:\n        return [i for i in items if i.text == text]\n    else:\n        return [i for i in items if i.text!= text]\n\n\ndef remove_all_by_text", "solution": "\n    for item in items:\n        text = remove(text, item, count=count, strip=strip)\n    return text"}
{"prompt": "def triangulate(self, **kwargs):\n        \"\"\"\n        Create a region- aware triangulation of the 2D path.\n\n        Parameters\n        -------------\n        **kwargs : dict\n          Passed to trimesh.creation.triangulate_polygon\n\n        Returns\n        -------------\n        vertices : (n, 2) float\n          2D vertices of triangulation\n        faces : (n, 3) int\n          Indexes of vertices for triangles\n        \"\"\"", "output": "\n        return trimesh.creation.create_polygon(self, **kwargs)\n\n    def triangulate_points(self, **kwargs):\n        \"\"\"\n        Create a region-aware triangulation of the 2D path.\n\n        Parameters\n        -------------\n        **kwargs : dict\n          Passed to trimesh.creation.triangulate_polygon\n\n        Returns\n        -------------\n        vertices : (n, 3) float\n          2D points of triangulation\n        faces : (n, 3) int\n          Indexes of vertices for triangles\n        \"\"\"\n        return trimesh.creation.create_points(self, **kwargs)\n\n   ", "solution": "\n        from ..creation import triangulate_polygon\n\n        # append vertices and faces into sequence\n        v_seq = []\n        f_seq = []\n\n        # loop through polygons with interiors\n        for polygon in self.polygons_full:\n            v, f = triangulate_polygon(polygon, **kwargs)\n            v_seq.append(v)\n            f_seq.append(f)\n\n        return util.append_faces(v_seq, f_seq)"}
{"prompt": "def fix_indentation(code, new_indents):\n    \"\"\"Change the indentation of `code` to `new_indents`\"\"\"", "output": "\n    if not code:\n        return code\n    if not new_indents:\n        return '\\n'\n    return '\\n'.join(new_indents) + '\\n'\n\n\ndef get_indent(code):\n    \"\"\"Return the indentation of `code`\"\"\"\n    if not code:\n        return ''\n    return fix_indentation(code, get_indents(code))\n\n\ndef get_indents(code):\n    \"\"\"Return the number of spaces in `code`\"\"\"\n    if not code:\n        return ''\n    return fix_indentation(code, get_indents(code))\n\n\ndef get", "solution": "\n    min_indents = find_minimum_indents(code)\n    return indent_lines(code, new_indents - min_indents)"}
{"prompt": "def general_setting(key, default=None, expected_type=None, qsettings=None):\n    \"\"\"Helper function to get a value from settings.\n\n    :param key: Unique key for setting.\n    :type key: basestring\n\n    :param default: The default value in case of the key is not found or there\n        is an error.\n    :type default: basestring, None, boolean, int, float\n\n    :param expected_type: The type of object expected.\n    :type expected_type: type\n\n    :param qsettings: A custom QSettings to use. If it's not defined, it will\n        use the default one.\n    :type qsettings: qgis.PyQt.QtCore.QSettings\n\n    :returns: The value of the key in the setting.\n    :rtype: object\n\n    Note:\n    The API for QSettings to get a value is different for PyQt and Qt C++.\n    In PyQt we can specify the expected type.\n    See: http://pyqt.sourceforge.net/Docs/PyQt4/qsettings.html#value\n    \"\"\"", "output": "\n        use the default value.\n    :type qsettings: QSettings\n\n    :returns: The value of the setting.\n    :rtype: basestring, QPyNullVariant\n\n    \"\"\"\n    if qsettings is None:\n        qsettings = QSettings()\n    if expected_type is None:\n        expected_type = key\n    if expected_type == 'bool':\n        return expected_type.lower() == 'true'\n    elif expected_type == 'int':\n        return expected_type.lower() == 'int'\n    elif expected_type == 'float':\n        return expected_type.lower() == 'float'\n   ", "solution": "\n    if qsettings is None:\n        qsettings = QSettings()\n    try:\n        if isinstance(expected_type, type):\n            return qsettings.value(key, default, type=expected_type)\n        else:\n            return qsettings.value(key, default)\n\n    except TypeError as e:\n        LOGGER.debug('exception %s' % e)\n        LOGGER.debug('%s %s %s' % (key, default, expected_type))\n        return qsettings.value(key, default)"}
{"prompt": "def requires(*params):\n    \"\"\"\n    Raise ValueError if any ``params`` are omitted from the decorated kwargs.\n\n    None values are considered omissions.\n\n    Example usage on an AWS() method:\n\n        @requires('zone', 'security_groups')\n        def my_aws_method(self, custom_args, **kwargs):\n            # We'll only get here if 'kwargs' contained non-None values for\n            # both 'zone' and 'security_groups'.\n    \"\"\"", "output": "\n    def requires(*params):\n        \"\"\"\n        Return True if any ``params`` are omitted from the decorated kwargs.\n        \"\"\"\n        return True\n    return None not in params and not kwargs\n\n\ndef _get_security_group_ids(zone, security_groups):\n    \"\"\"\n    Return a list of security group IDs for a given zone.\n\n    :param zone: The name of the zone.\n    :param security_groups: The security groups to filter by.\n    :return: A list of security group IDs.\n    \"\"\"\n    if zone is not None:\n        security_groups = security_groups.filter(\n            zone", "solution": "\n    def requires(f, self, *args, **kwargs):\n        missing = filter(lambda x: kwargs.get(x) is None, params)\n        if missing:\n            msgs = \", \".join([PARAMETERS[x]['msg'] for x in missing])\n            raise ValueError(\"Missing the following parameters: %s\" % msgs)\n        return f(self, *args, **kwargs)\n    return decorator(requires)"}
{"prompt": "def normalize_parameters(params):\n        \"\"\" Normalize parameters \"\"\"", "output": "\n        if params is None:\n            return None\n        if isinstance(params, dict):\n            return params\n        return {k: float(v) for k, v in params.items() if v is not None}\n\n    def _get_default_values(self):\n        \"\"\" Returns default values for parameters \"\"\"\n        return {\n           'max_length': self.max_length,\n           'min_length': self.min_length,\n           'max_length_in_bytes': self.max_length_in_bytes,\n           'min_length_in_bytes': self.min_length_in_", "solution": "\n        params = params or {}\n        normalized_parameters = OrderedDict()\n\n        def get_value_like_as_php(val):\n            "}
{"prompt": "def get_replies(self, new=True):\n        \"\"\"\n        Return all reply notifications for this user.\n\n        :param new: False for all notifications, True for only non-viewed\n            notifications.\n        \"\"\"", "output": "\n        if new:\n            return self.replies.filter(user=self).order_by('-created')\n        else:\n            return self.replies.filter(user=self).order_by('-created')\n\n    def get_last_login(self):\n        \"\"\"\n        Return the last login of this user.\n        \"\"\"\n        return self.last_login\n\n    def get_last_activity(self):\n        \"\"\"\n        Return the last activity of this user.\n        \"\"\"\n        return self.last_activity\n\n    def get_last_activity_by_id(self, activity_id):\n        \"\"\"\n        Return the", "solution": "\n        url = (self._imgur._base_url + \"/3/account/{0}/\"\n               \"notifications/replies\".format(self.name))\n        return self._imgur._send_request(url, needs_auth=True)"}
{"prompt": "def run(self, fastafile, params=None, tmp=None):\n        \"\"\"\n        Run the tool and predict motifs from a FASTA file.\n\n        Parameters\n        ----------\n        fastafile : str\n            Name of the FASTA input file.\n\n        params : dict, optional\n            Optional parameters. For some of the tools required parameters\n            are passed using this dictionary.\n\n        tmp : str, optional\n            Directory to use for creation of temporary files.\n       \n        Returns\n        -------\n        motifs : list of Motif instances\n            The predicted motifs.\n\n        stdout : str\n            Standard out of the tool.\n        \n        stderr : str\n            Standard error of the tool.\n        \"\"\"", "output": " : str\n            Standard error of the tool.\n        \"\"\"\n        if params is None:\n            params = {}\n        if tmp is None:\n            tmp = tempfile.mkdtemp()\n        if not os.path.exists(tmp):\n            os.makedirs(tmp)\n        if not os.path.exists(self.outputdir):\n            os.makedirs(self.outputdir)\n        if not os.path.exists(self.output_dir):\n            raise ValueError(\"Output directory %s does not exist\" % self.outputdir)\n        if not os.path.exists(self.output_dir):\n            raise ValueError(\"", "solution": "\n        if not self.is_configured():\n            raise ValueError(\"%s is not configured\" % self.name)\n\n        if not self.is_installed():\n            raise ValueError(\"%s is not installed or not correctly configured\" % self.name)\n        \n        self.tmpdir = mkdtemp(prefix=\"{0}.\".format(self.name), dir=tmp)\n        fastafile = os.path.abspath(fastafile)\n \n        try:\n            return self._run_program(self.bin(), fastafile, params)\n        except KeyboardInterrupt:\n            return ([], \"Killed\", \"Killed\")"}
{"prompt": "def get_adapter_path(obj, to_cls):\n    \"\"\"\n    Returns the adapter path that would be used to adapt `obj` to `to_cls`.\n    \"\"\"", "output": "\n    return '%s.%s' % (obj.__class__.__module__, to_cls.__name__)\n\n\ndef get_concrete_function(model, attrs):\n    \"\"\"\n    Given a model class, returns the `ForProxyModel` that should be used\n    to retrieve the concrete model class.\n    \"\"\"\n    opts = model._meta\n    return (opts.app_label, opts.module_name) + get_concrete_fields(opts.fields, opts.exclude)\n\n\ndef get_concrete_fields(fields, exclude=None):\n    \"\"\"\n    Given any model field, returns a dict of all the", "solution": "\n    from_cls = type(obj)\n    key = (from_cls, to_cls)\n    if key not in __mro__:\n        __mro__[key] = list(itertools.product(inspect.getmro(from_cls), inspect.getmro(to_cls)))\n\n    return __mro__[key]"}
{"prompt": "def merge_featurecollection(*jsons):\n    \"\"\"\n    merge features into one featurecollection\n\n    Keyword arguments:\n    jsons   -- jsons object list \n\n    return geojson featurecollection\n    \"\"\"", "output": "\n    merged_jsons = []\n    for json in jsons:\n        merged_json = merge_json(json)\n        merged_jsons.append(merged_json)\n    return featurecollection.merge(merged_jsons)\n\ndef merge_featurecollection_by_name(*json_list):\n    \"\"\"\n    merge features into one featurecollection by name\n\n    Keyword arguments:\n    json_list -- list of geojson objects \n\n    return geojson featurecollection\n    \"\"\"\n    merged_json = []\n    for json in json_list:\n        merged_json.append(merge_json(json, name=None", "solution": "\n    features = []\n    for json in jsons:\n        if json['type'] == 'FeatureCollection':\n            for feature in json['features']:\n                features.append(feature)\n    return {\"type\":'FeatureCollection', \"features\":features}"}
{"prompt": "def get_block_adjustments(crypto, points=None, intervals=None, **modes):\n    \"\"\"\n    This utility is used to determine the actual block rate. The output can be\n    directly copied to the `blocktime_adjustments` setting.\n    \"\"\"", "output": "\n    if points is None:\n        points = crypto.get_points()\n    if intervals is None:\n        intervals = crypto.get_intervals()\n    if modes is None:\n        modes = crypto.get_modes()\n    if crypto.has_key('blocktime'):\n        blocktime_points = []\n        for point in points:\n            blocktime_points.append(crypto.get_points(point))\n        blocktime_points = sorted(blocktime_points, key=lambda x: x[1])\n        for point in blocktime_points:\n            if point[0] in modes:\n                point[", "solution": "\n    from moneywagon import get_block\n    all_points = []\n\n    if intervals:\n        latest_block_height = get_block(crypto, latest=True, **modes)['block_number']\n        interval = int(latest_block_height / float(intervals))\n        all_points = [x * interval for x in range(1, intervals - 1)]\n\n    if points:\n        all_points.extend(points)\n\n    all_points.sort()\n\n    adjustments = []\n    previous_point = 0\n    previous_time = (crypto_data[crypto.lower()].get('genesis_date').replace(tzinfo=pytz.UTC)\n        or get_block(crypto, block_number=0, **modes)['time']\n    )\n\n    for point in all_points:\n        if point == 0:\n            continue\n        point_time = get_block(crypto, block_number=point, **modes)['time']\n        length = point - previous_point\n        minutes = (point_time - previous_time).total_seconds() / 60\n        rate = minutes / length\n        adjustments.append([previous_point, rate])\n\n        previous_time = point_time\n        previous_point = point\n\n    return adjustments"}
{"prompt": "def pycode_to_body(co, context):\n    \"\"\"\n    Convert a Python code object to a list of AST body elements.\n    \"\"\"", "output": "\n    if isinstance(co, (ast.Tuple, ast.List)):\n        return [pycode_to_tuple(co, context) for co in co]\n    elif isinstance(co, ast.Dict):\n        return {key: pycode_to_body(value, context) for key, value in co.items()}\n    elif isinstance(co, (ast.ListComp, ast.Tuple)):\n        return pycode_to_body(co, context)\n    elif isinstance(co, ast.List):\n        return [pycode_to_list(co, context) for co in co]\n   ", "solution": "\n    code = Code.from_pycode(co)\n\n    # On each instruction, temporarily store all the jumps to the **next**\n    # instruction.  This is used in _make_expr to determine when an expression\n    # is part of a short-circuiting expression.\n    for a, b in sliding_window(2, code.instrs):\n        a._next_target_of = b._target_of\n    b._next_target_of = set()\n\n    try:\n        body = instrs_to_body(deque(code.instrs), context)\n        if context.in_function_block:\n            return make_global_and_nonlocal_decls(code.instrs) + body\n        return body\n    finally:\n        # Clean up jump target data.\n        for i in code.instrs:\n            del i._next_target_of"}
{"prompt": "def get_params_from_list(params_list):\n    \"\"\"Transform params list to dictionary.\n    \"\"\"", "output": "\n    params = {}\n    for param in params_list:\n        if param in ('page', 'page_size'):\n            params['page_' + param] = int(params_list.pop(0)[param])\n        elif param in ('sort','sort_dir'):\n            params['sort__' + param] = params_list.pop(0)[param]\n        elif param in ('sort_key','sort_key_dir'):\n            params['sort__' + param] = params_list.pop(0)[param]\n        elif param in ('sort_key_type','sort_key", "solution": "\n    params = {}\n    for i in range(0, len(params_list)):\n        if '=' not in params_list[i]:\n            try:\n                if not isinstance(params[key], list): params[key] = [params[key]]\n                params[key] += [params_list[i]]\n            except KeyError:\n                raise ValueError('Pass parameters like `key1=a key2=b c d key3=...`.')\n        else:\n            key_val = params_list[i].split('=')\n            key, val = key_val\n            params[key] = convert_string(val)\n    return params"}
{"prompt": "def gui():\n    \"\"\"remove libraries by GUI.\"\"\"", "output": "\n    for lib in list(libraries):\n        if lib.is_installed():\n            libraries.remove(lib)\n    return libraries\n\n\ndef main():\n    \"\"\"Run the script.\"\"\"\n    if len(sys.argv) < 2:\n        print(\"Usage: %s <path to library>\" % sys.argv[0])\n        sys.exit(1)\n\n    # get the path to the library\n    library_path = sys.argv[1]\n\n    # get the path to the script\n    script_path = sys.argv[2]\n\n    # get the path to the output file\n    output_path = sys.", "solution": "\n\n    sel = psidialogs.multi_choice(libraries(),\n                                  'select libraries to remove from %s!' % libraries_dir(),\n                                  title='remove boards')\n    print('%s selected' % sel)\n\n    if sel:\n        if psidialogs.ask_yes_no('Do you really want to remove selected libraries?\\n' + '\\n'.join(sel)):\n            for x in sel:\n                remove_lib(x)\n                print('%s was removed' % x)"}
{"prompt": "def rock(cls, data, eps, number_clusters, threshold=0.5, ccore=False):\n        \"\"\"\n        Constructor of the ROCK cluster analysis algorithm\n\n        :param eps: Connectivity radius (similarity threshold), points are neighbors if distance between them is less than connectivity radius\n        :param number_clusters: Defines number of clusters that should be allocated from the input data set\n        :param threshold: Value that defines degree of normalization that influences on choice of clusters for merging during processing\n        :param ccore: Defines should be CCORE (C++ pyclustering library) used instead of Python code or not.\n        :return: The resulting clustering.rst object\n        \"\"\"", "output": "\n        :return:\n        \"\"\"\n        super(ROCK, cls).__init__(data, number_clusters, threshold, ccore)\n        self.rocks = {}\n        self.rocks_count = {}\n        self.rocks_count_count = {}\n        self.rocks_count_count = {}\n        for i in range(number_clusters):\n            self.rocks[i] = data.rocks[i]\n            self.rocks_count[i] = data.rocks_count[i]\n            self.rocks_count_count[i] = data.rocks", "solution": "\n        data = cls.input_preprocess(data)\n        model = rock(data, eps, number_clusters, threshold, ccore)\n        return cls(model)"}
{"prompt": "def preview(self, stream=sys.stdout):\n        \"\"\"A quick preview of docpie. Print all the parsed object\"\"\"", "output": "\n        print(self.parse(stream))\n\n    def parse(self, stream):\n        \"\"\"Parse the given stream and return a list of (text, tag, attrs)\"\"\"\n        parser = DocpieParser(stream)\n        parser.parse()\n        return parser.get_text(), parser.get_tag(), parser.get_attrs()\n\n    def get_attrs(self):\n        \"\"\"Return a list of (tag, attrs) for the parsed object\"\"\"\n        return self.attrs\n\n    def get_text(self):\n        \"\"\"Return the text of the parsed object\"\"\"\n        return ''.join(self.attrs)\n\n    def get", "solution": "\n\n        write = stream.write\n\n        write(('[Quick preview of Docpie %s]' % self._version).center(80, '='))\n        write('\\n')\n\n        write(' sections '.center(80, '-'))\n\n        write('\\n')\n        write(self.usage_text)\n        write('\\n')\n        option_sections = self.option_sections\n        if option_sections:\n            write('\\n')\n            write('\\n'.join(option_sections.values()))\n        write('\\n')\n\n        write(' str '.center(80, '-'))\n        write('\\n[%s]\\n' % self.usage_name)\n        for each in self.usages:\n            write('    %s\\n' % each)\n        write('\\n[Options:]\\n\\n')\n        for title, sections in self.options.items():\n            if title:\n                full_title = '%s %s' % (title, self.option_name)\n            else:\n                full_title = self.option_name\n\n            write(full_title)\n            write('\\n')\n\n            for each in sections:\n                write('    %s\\n' % each)\n\n            write('\\n')\n\n        write(' repr '.center(80, '-'))\n        write('\\n[%s]\\n' % self.usage_name)\n        for each in self.usages:\n            write('    %r\\n' % each)\n        write('\\n[Options:]\\n\\n')\n        for title, sections in self.options.items():\n            if title:\n                full_title = '%s %s' % (title, self.option_name)\n            else:\n                full_title = self.option_name\n\n            write(full_title)\n            write('\\n')\n\n            for each in sections:\n                write('    %r\\n' % each)\n\n            write('\\n')\n\n        write(' auto handlers '.center(80, '-'))\n        write('\\n')\n        for key, value in self.extra.items():\n            write('%s %s\\n' % (key, value))"}
{"prompt": "def issuer(self):\n        \"\"\"The certificate issuer field as :py:class:`~django_ca.subject.Subject`.\"\"\"", "output": "\n        return self.cleaned_data.get('issuer', None)\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        :param args: The arguments to use when instantiating the\n            :py:class:`~django_ca.certificate.Certificate` object.\n        :param kwargs: The keyword arguments to use when instantiating the\n            :py:class:`~django_ca.certificate.Certificate` object.\n        \"\"\"\n        super(CertificateForm, self).__init__(*args, **kwargs)\n        self.fields['certificate'].choices = self.fields['certificate'].choices + (\n            (cert.pk", "solution": "\n        return Subject([(s.oid, s.value) for s in self.x509.issuer])"}
{"prompt": "def sample_following_dist(handle_iter, n, totalf):\n    \"\"\"Samples n passwords following the distribution from the handle\n    @handle_iter is an iterator that gives (pw,f) @n is the total\n    number of samle asked for @totalf is the total number of users,\n    which is euqal to sum(f for pw,f in handle_iter)\n    As, handle_iterator is an iterator and can only traverse once, @totalf\n    needs to be supplied to the funciton.\n\n    Returns, an array of @n tuples (id, pw) sampled from @handle_iter.\n    \"\"\"", "output": " from the handle_iter\n    \"\"\"\n    for pw, f in handle_iter:\n        if f == 0:\n            yield (id, pw)\n        else:\n            yield (id, f) * totalf + (1 - f) * (1 - 1/totalf)\n\n\ndef sample_user_dist(n, totalf):\n    \"\"\"Samples n passwords user's password distribution\n    @n is the total number of users, which is euqal to sum(f for f in handle_iter)\n    @totalf is the total number of users, which is euqal to sum(", "solution": "\n    multiplier = 1.0\n    if totalf == 1.0:\n        multiplier = 1e8\n        # print \"WARNING!! I don't except probabilities\"\n\n    totalf = totalf * multiplier\n    print(\"# Population Size\", totalf)\n    A = np.sort(np.unique(np.random.randint(0, totalf, size=n*2))[:n])\n    A = A[::-1]\n    # Uniqueness check, non necessarily required, but not very\n    # computationally intensive\n    assert len(A) == n, \"Not enough randomnumbers generated\"\\\n        \"Requried {}, generated only {}\".format(n, len(A))\n    j = 0\n    sampled = 0\n    val = A.pop()\n    # print handle_iter\n    for _,w,f in handle_iter:\n        j += f*multiplier\n        if not A: break\n        while val<j:\n            sampled += 1\n            if sampled %5000 == 0:\n                print (\"Sampled:\",sampled)\n            yield (val, w)\n            if A:\n                val = A.pop()\n            else:\n                break\n\n    print (\"# Stopped at:\", w, f, j, '\\n')\n    while A and val<j:\n        yield (val, w)\n        if A:\n            i, val = A.pop()\n        else:\n            break"}
{"prompt": "def is_probabilistic_classifier(clf):\n    # type: (Any) -> bool\n    \"\"\" Return True if a classifier can return probabilities \"\"\"", "output": "\n    return hasattr(clf, 'probabilistic_classifier') and clf.probabilistic_classifier is not None\n\n\ndef is_probabilistic_classifier_with_name(clf_name):\n    # type: (str) -> bool\n    \"\"\" Return True if a classifier can return a given name \"\"\"\n    return is_probabilistic_classifier(clf_name) and hasattr(clf_name, 'name') and clf_name.name == clf_name.name\n\n\ndef is_probabilistic_classifier_with_id(clf_id):\n    # type", "solution": "\n    if not hasattr(clf, 'predict_proba'):\n        return False\n    if isinstance(clf, OneVsRestClassifier):\n        # It currently has a predict_proba method, but does not check if\n        # wrapped estimator has a predict_proba method.\n        return hasattr(clf.estimator, 'predict_proba')\n    return True"}
{"prompt": "def read_next_into_buf(self):\n        \"\"\"Read data from the file in self.bufsize chunks until we're\n           certain we have a full line in the buffer.\n        \"\"\"", "output": "\n        # See if we have a newline before the next buffer\n        if self.newlines and self.newlines[-1]!= '\\n':\n            self.newlines.append('')\n\n        while True:\n            line = self.fp.readline(self.bufsize)\n            if not line:\n                break\n            if line[-2:] == '\\r\\n':\n                line = line[:-2]\n            self.newlines.append(line)\n            if self.newlines and line[-1]!= '\\n':\n                self.newlines.append('')\n        return self.buf\n\n    def readlines(self):\n        \"\"\"Return a list of all lines", "solution": "\n        file_pos = self.fileobject.tell()\n        if (file_pos == 0) and (self.buf == b''):\n            raise StopIteration\n        while file_pos and (self.get_start_of_line() == 0):\n            bytes_to_read = min(self.bufsize, file_pos)\n            file_pos = file_pos - bytes_to_read\n            self.fileobject.seek(file_pos)\n            new_stuff = self.fileobject.read(bytes_to_read)[:bytes_to_read]\n            self.fileobject.seek(file_pos)\n            self.buf = new_stuff + self.buf"}
{"prompt": "def export(self, title, data):\n        \"\"\"Generate graph from the data.\n\n        Example for the mem plugin:\n        {'percent': [\n            (datetime.datetime(2018, 3, 24, 16, 27, 47, 282070), 51.8),\n            (datetime.datetime(2018, 3, 24, 16, 27, 47, 540999), 51.9),\n            (datetime.datetime(2018, 3, 24, 16, 27, 50, 653390), 52.0),\n            (datetime.datetime(2018, 3, 24, 16, 27, 53, 749702), 52.0),\n            (datetime.datetime(2018, 3, 24, 16, 27, 56, 825660), 52.0),\n            ...\n            ]\n        }\n\n        Return:\n        * True if the graph have been generated\n        * False if the graph have not been generated\n        \"\"\"", "output": "), 53.0),\n            (datetime.datetime(2018, 3, 24, 16, 27, 50, 667790), 52.0),\n            (datetime.datetime(2018, 3, 24, 16, 27, 53, 749702), 53.0),\n            (datetime.datetime(2018, 3, 24, 16, 27, 53, 749702), 54.0),\n            (datetime.datetime(2018, 3, 24, 16, 27, 53, 749702), 55.0),\n            (datetime.datetime(2018, 3, 24, 16, 27, 53, 74", "solution": "\n        if data == {}:\n            return False\n\n        chart = DateTimeLine(title=title.capitalize(),\n                             width=self.width,\n                             height=self.height,\n                             style=self.style,\n                             show_dots=False,\n                             legend_at_bottom=True,\n                             x_label_rotation=20,\n                             x_value_formatter=lambda dt: dt.strftime('%Y/%m/%d %H:%M:%S'))\n        for k, v in iteritems(time_serie_subsample(data, self.width)):\n            chart.add(k, v)\n        chart.render_to_file(os.path.join(self.path,\n                                          title + '.svg'))\n        return True"}
{"prompt": "def format(self, result):\n        \"\"\"Generate HTML report.\n\n        :return: Report body\n        :rtype: str\n        \"\"\"", "output": "\n        return self.html_report(result,'report_result')\n\n    def html_report(self, result, type):\n        \"\"\"Generate HTML report.\n\n        :param result: Result object.\n        :type result: Result\n        :param type: Report type.\n        :type type: str\n        :return: HTML report body.\n        :rtype: str\n        \"\"\"\n        html_report = result.html_report()\n        html_report += '\\n<h3>%s</h3>' % type\n        html_report += '\\n<ul>'\n        for key in sorted(self.html_report_attrs", "solution": "\n        css = \"\\n\".join(self.css)\n        content = \"{}{}\".format(self._header(), self._body(result))\n        if self._email:\n            text = "}
{"prompt": "def get_client_unread_messages_count(self, client_name=None):\n        \"\"\"Gets count of unread messages from client\n        \"\"\"", "output": "\n        if client_name is None:\n            client_name = self.client_name\n\n        if not client_name:\n            return self.get_unread_messages(count=True)\n        else:\n            return self.get_unread_messages(count=True, client_name=client_name)\n\n    def get_client_unread_count_display(self, client_name=None):\n        \"\"\"Gets count of unread messages from client\n        \"\"\"\n        if client_name is None:\n            client_name = self.client_name\n\n        if not client_name:\n            return self.get_unread", "solution": "\n        client = self._clients.get_with_name(client_name)[0]\n        return client.get_messages_count_in_buffer()"}
{"prompt": "async def join(self, channel, password=None):\n        \"\"\" Join channel, optionally with password. \"\"\"", "output": "\n        await self.bot.say(\"Joining the bot\")\n        await self.bot.say(\"```{}```\".format(self.bot.escape_name(channel.mention)) +\n                           \"```\")\n        try:\n            await self.bot.change_presence(presence=discord.Game(name=self.bot.escape_name(self.bot.guild.name))['presence']\n        except discord.HTTPException:\n            await self.bot.say(\"Failed to change presence\")\n        else:\n            await self.bot.say(\"Joined the bot\")\n\n    @commands.command(pass", "solution": "\n        if self.in_channel(channel):\n            raise AlreadyInChannel(channel)\n\n        if password:\n            await self.rawmsg('JOIN', channel, password)\n        else:\n            await self.rawmsg('JOIN', channel)"}
{"prompt": "def get_parameters(rq, variables, endpoint, query_metadata, auth=None):\n    \"\"\"\n        ?_name The variable specifies the API mandatory parameter name. The value is incorporated in the query as plain literal.\n        ?__name The parameter name is optional.\n        ?_name_iri The variable is substituted with the parameter value as a IRI (also: number or literal).\n        ?_name_en The parameter value is considered as literal with the language 'en' (e.g., en,it,es, etc.).\n        ?_name_integer The parameter value is considered as literal and the XSD datatype 'integer' is added during substitution.\n        ?_name_prefix_datatype The parameter value is considered as literal and the datatype 'prefix:datatype' is added during substitution. The prefix must be specified according to the SPARQL syntax.\n    \"\"\"", "output": " integer. The value is incorporated in the query as plain integer.\n       ?_name_float The parameter value is considered as literal and float. The value is incorporated in the query as plain float.\n       ?_name_boolean The parameter value is considered as literal and boolean. The value is incorporated in the query as plain boolean.\n       ?_name_float The parameter value is considered as literal and float. The value is incorporated in the query as plain float.\n       ?_name_boolean The parameter value is considered as literal and boolean. The value is incorporated in the query as plain boolean.\n       ?", "solution": "\n\n    # variables = translateQuery(Query.parseString(rq, parseAll=True)).algebra['_vars']\n\n    ## Aggregates\n    internal_matcher = re.compile(\"__agg_\\d+__\")\n    ## Basil-style variables\n    variable_matcher = re.compile(\n        \"(?P<required>[_]{1,2})(?P<name>[^_]+)_?(?P<type>[a-zA-Z0-9]+)?_?(?P<userdefined>[a-zA-Z0-9]+)?.*$\")\n\n    parameters = {}\n    for v in variables:\n        if internal_matcher.match(v):\n            continue\n\n        match = variable_matcher.match(v)\n        # TODO: currently only one parameter per triple pattern is supported\n        if match:\n            vname = match.group('name')\n            vrequired = True if match.group('required') == '_' else False\n            vtype = 'string'\n            # All these can be None\n            vcodes = get_enumeration(rq, vname, endpoint, query_metadata, auth)\n            vdefault = get_defaults(rq, vname, query_metadata)\n            vlang = None\n            vdatatype = None\n            vformat = None\n\n            mtype = match.group('type')\n            muserdefined = match.group('userdefined')\n\n            if mtype in ['number', 'literal', 'string']:\n                vtype = mtype\n            elif mtype in ['iri']:  # TODO: proper form validation of input parameter uris\n                vtype = 'string'\n                vformat = 'iri'\n            elif mtype:\n                vtype = 'string'\n\n                if mtype in static.XSD_DATATYPES:\n                    vdatatype = 'xsd:{}'.format(mtype)\n                elif len(mtype) == 2:\n                    vlang = mtype\n                elif muserdefined:\n                    vdatatype = '{}:{}'.format(mtype, muserdefined)\n\n            parameters[vname] = {\n                'original': '?{}'.format(v),\n                'required': vrequired,\n                'name': vname,\n                'type': vtype\n            }\n\n            # Possibly None parameter attributes\n            if vcodes is not None:\n                parameters[vname]['enum'] = sorted(vcodes)\n            if vlang is not None:\n                parameters[vname]['lang'] = vlang\n            if vdatatype is not None:\n                parameters[vname]['datatype'] = vdatatype\n            if vformat is not None:\n                parameters[vname]['format'] = vformat\n            if vdefault is not None:\n                parameters[vname]['default'] = vdefault\n\n            glogger.info('Finished parsing the following parameters: {}'.format(parameters))\n\n    return parameters"}
{"prompt": "def _cast_value(self, value):\n        \"\"\"Internal method that makes sure every value in dictionary\n        is properly cast into the correct types, instead of\n        just treating everything like a string from the csv file.\n\n        Args:\n            value : The value to be casted\n\n        Returns:\n            A casted Value.\n        \"\"\"", "output": "\n        if value is None:\n            return None\n        if isinstance(value, str):\n            return self._cast_csv_value(value)\n        elif isinstance(value, list):\n            return self._cast_csv_value(value)\n        else:\n            raise TypeError(\"Can't cast %s to a string\" % type(value))\n\n    def _cast_csv_value(self, value):\n        \"\"\"Internal method that makes sure every value in csv\n        file is properly cast into the correct types, instead of\n        just treating everything like a string from the csv file.\n\n        Args:\n            value : The value to", "solution": "\n        # Try to convert to a datetime (if requested)\n        if (self.convert_datetimes):\n            try:\n                date_time = datetime.datetime.fromtimestamp(float(value))\n                if datetime.datetime(1970, 1, 1) > date_time:\n                    raise ValueError\n                else:\n                    return date_time\n\n            # Next try a set of primitive types\n            except ValueError:\n                pass\n\n        # Try conversion to basic types\n        tests = (int, float, str)\n        for test in tests:\n            try:\n                return test(value)\n            except ValueError:\n                continue\n        return value"}
{"prompt": "def update_build_properties(self, document, project, build_id):\n        \"\"\"UpdateBuildProperties.\n        [Preview API] Updates properties for a build.\n        :param :class:`<[JsonPatchOperation]> <azure.devops.v5_0.build.models.[JsonPatchOperation]>` document: A json-patch document describing the properties to update.\n        :param str project: Project ID or project name\n        :param int build_id: The ID of the build.\n        :rtype: :class:`<object> <azure.devops.v5_0.build.models.object>`\n        \"\"\"", "output": ".JsonPatchOperation>`\n        \"\"\"\n        if isinstance(document, str):\n            document = json.loads(document)\n\n        if not isinstance(project, str):\n            raise ValueError(\"Parameter 'project' must be of type'str'.\")\n\n        if not document:\n            raise ValueError(\"Parameter 'document' must not be empty.\")\n\n        if not build_id:\n            raise ValueError(\"Parameter 'build_id' must not be empty.\")\n\n        if 'provisioning_state' in document:\n            if document['provisioning_state'] == ProvisioningState.Succeeded:\n                return ProvisioningState.CreateOrUpdateSucceeded\n            elif document", "solution": "\n        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if build_id is not None:\n            route_values['buildId'] = self._serialize.url('build_id', build_id, 'int')\n        content = self._serialize.body(document, '[JsonPatchOperation]')\n        response = self._send(http_method='PATCH',\n                              location_id='0a6312e9-0627-49b7-8083-7d74a64849c9',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              content=content,\n                              media_type='application/json-patch+json')\n        return self._deserialize('object', response)"}
{"prompt": "def TransformedRandomVariable(random_variable,  # pylint: disable=invalid-name\n                              reversible_layer,\n                              name=None,\n                              sample_shape=(),\n                              value=None):\n  \"\"\"Random variable for f(x), where x ~ p(x) and f is reversible.\"\"\"", "output": "\n  if name is None:\n    name = \"trans_masked_random_variable\"\n  if name not in random_variable.name:\n    random_variable.name.append(name)\n  if sample_shape:\n    sample_shape = [sample_shape]\n  if value is not None:\n    value = np.array(value, dtype=np.float32)\n  if random_variable.shape:\n    sample_shape = random_variable.shape\n  if reversible_layer:\n    return ReversibleLayer(name, sample_shape, sample_shape, sample_shape)\n  else:", "solution": "\n  return ed.RandomVariable(\n      distribution=TransformedDistribution(random_variable.distribution,\n                                           reversible_layer,\n                                           name=name),\n      sample_shape=sample_shape,\n      value=value)"}
{"prompt": "def nodes_in(self, name) -> iter:\n        \"\"\"Yield all nodes contained in given (power) node\"\"\"", "output": "\n        for node in self.nodes:\n            if node.name == name:\n                yield node\n\n    def nodes_out(self, name) -> iter:\n        \"\"\"Yield all nodes contained in given (power) node\"\"\"\n        for node in self.nodes:\n            if node.name == name:\n                yield node\n\n    def get_nodes_in(self, name) -> iter:\n        \"\"\"Return all nodes contained in given (power) node\"\"\"\n        for node in self.nodes:\n            if node.name == name:\n                yield node\n\n    def get_nodes_out(self, name) -> iter:\n        \"\"\"", "solution": "\n        yield from (node for node in self.all_in(name) if self.is_node(node))"}
{"prompt": "def write_stats_to_json(cls, file_name, stats):\n    \"\"\"Write stats to a local json file.\"\"\"", "output": "\n    with open(file_name, 'w') as f:\n      json.dump(stats, f)\n\n  @classmethod\n  def load_stats_from_json(cls, file_name):\n    \"\"\"Load stats from a local json file.\"\"\"\n    with open(file_name, 'r') as f:\n      return json.load(f)\n\n  @classmethod\n  def get_stats(cls, stats_file):\n    \"\"\"Read stats from a local json file.\"\"\"\n    with open(stats_file, 'r') as f:\n      return cls.load_stats_from_json(f)\n\n ", "solution": "\n    params = cls._json_dump_options(stats)\n    mode = 'w' if PY3 else 'wb'\n    try:\n      safe_file_dump(file_name, params, mode=mode)\n    except Exception as e: # Broad catch - we don't want to fail in stats related failure.\n      print('WARNING: Failed to write stats to {} due to Error: {}'.format(file_name, e),\n            file=sys.stderr)"}
{"prompt": "def compare_files(path1, path2):\n    # type: (str, str) -> List[str]\n    \"\"\"Returns the delta between two files using -, ?, + format excluding\n    lines that are the same\n\n    Args:\n        path1 (str): Path to first file\n        path2 (str): Path to second file\n\n    Returns:\n        List[str]: Delta between the two files\n\n    \"\"\"", "output": "\n    with open(path1, 'r') as f:\n        file1 = f.readlines()\n    with open(path2, 'r') as f:\n        file2 = f.readlines()\n    delta = difflib.unified_diff(file1, file2,\n        fromfile=lambda x: x.replace(path2, path1).splitlines(1),\n        tofile=lambda x: x.replace(file2, path2).splitlines(1))\n    return list(delta)\n\n\ndef compare_files_ignore_case(path1, path2):\n    # type: (str", "solution": "\n    diff = difflib.ndiff(open(path1).readlines(), open(path2).readlines())\n    return [x for x in diff if x[0] in ['-', '+', '?']]"}
{"prompt": "def get_common_parameters(input_files, collection=None):\n    \"\"\"Gets a list of variable params that are common across all input files.\n\n    If no common parameters are found, a ``ValueError`` is raised.\n\n    Parameters\n    ----------\n    input_files : list of str\n        List of input files to load.\n    collection : str, optional\n        What group of parameters to load. Can be the name of a list of\n        parameters stored in the files' attrs (e.g., \"variable_params\"), or\n        \"all\". If \"all\", will load all of the parameters in the files'\n        samples group. Default is to load all.\n\n    Returns\n    -------\n    list :\n        List of the parameter names.\n    \"\"\"", "output": ".\n\n    Returns\n    -------\n    dict\n        Dictionary of all the parameters.\n    \"\"\"\n    if collection is None:\n        collection = \"all\"\n    return {\n        \"variable_params\": collection,\n        \"variable_params_mask\": \"mask\",\n        \"variable_params_name\": \"variable_params\",\n        \"variable_params_type\": \"list\",\n        \"group_name\": \"samples\",\n        \"group_type\": \"samples_group\",\n        \"group_label\": \"samples\",\n        \"group_label_mask\": \"mask\",\n        \"group_label_name\": \"samples\",\n        \"group", "solution": "\n    if collection is None:\n        collection = \"all\"\n    parameters = []\n    for fn in input_files:\n        fp = loadfile(fn, 'r')\n        if collection == 'all':\n            ps = fp[fp.samples_group].keys()\n        else:\n            ps = fp.attrs[collection]\n        parameters.append(set(ps))\n        fp.close()\n    parameters = list(set.intersection(*parameters))\n    if parameters == []:\n        raise ValueError(\"no common parameters found for collection {} in \"\n                         \"files {}\".format(collection, ', '.join(input_files)))\n    return parameters"}
{"prompt": "def allow(ctx, foreign_account, permission, weight, threshold, account):\n    \"\"\" Add a key/account to an account's permission\n    \"\"\"", "output": "\n    if not isinstance(ctx, dict):\n        raise errors.AnsibleFilterError(\"expected a dictionary\")\n\n    if not isinstance(weight, int):\n        raise errors.AnsibleFilterError(\"weight must be an integer\")\n    if not isinstance(threshold, int):\n        raise errors.AnsibleFilterError(\"threshold must be an integer\")\n    if agefilter(age, ctx, foreign_account, permission, ctx['age_days']):\n        return False\n\n    # Check that age is not older than threshold\n    if agefilter(time.time(), ctx, foreign_account, '1d'):\n        return False\n\n    # Check that there", "solution": "\n    if not foreign_account:\n        from peerplaysbase.account import PasswordKey\n\n        pwd = click.prompt(\n            \"Password for Key Derivation\", hide_input=True, confirmation_prompt=True\n        )\n        foreign_account = format(\n            PasswordKey(account, pwd, permission).get_public(), \"PPY\"\n        )\n    pprint(\n        ctx.peerplays.allow(\n            foreign_account,\n            weight=weight,\n            account=account,\n            permission=permission,\n            threshold=threshold,\n        )\n    )"}
{"prompt": "def classify(self, tweets):\n        \"\"\"\n        Classify tweet or tweets\n        :param tweets: String or array of strings to classify.\n        :return: String or array of strings depicting sentiment. Sentiment can be POSITIVE, NEGATIVE or NEUTRAL.\n        \"\"\"", "output": "\n        if isinstance(tweets, str):\n            tweets = [tweets]\n        if isinstance(tweets, ndarray):\n            tweets = np.array(tweets)\n        if isinstance(tweets, np.ndarray):\n            tweets = np.array(tweets)\n        if isinstance(tweets, np.ndarray):\n            tweets = np.array(tweets)\n        if isinstance(tweets, np.ndarray):\n            tweets = np.array(tweets)\n        if isinstance(tweets, np.ndarray):\n            tweets = np.array(tweets)\n        if isinstance", "solution": "\n        if type(tweets) == str:\n            return self.classifier.classify(tweets)\n\n        return list(map(lambda tweet: self.classifier.classify(tweet), tweets))"}
{"prompt": "def Contradiction(expr1: Expression, expr2: Expression) -> Expression:\n    \"\"\"Return expression which is the contradiction of `expr1` and `expr2`.\"\"\"", "output": "\n    return expr1.value\n\n\ndef Contradiction(expr: Expression) -> Expression:\n    \"\"\"Return expression which is the contradiction of `expr` and `expr`.\"\"\"\n    return expr.value\n\n\ndef Contradiction(expr: Expression) -> Expression:\n    \"\"\"Return expression which is the contradiction of `expr` and `expr`.\"\"\"\n    return expr.value\n\n\ndef Expression(expr: Expression) -> Expression:\n    \"\"\"Return expression which is the expression `expr`.\"\"\"\n    return expr.value\n\n\ndef test_contract_simple(contradiction: Contradiction, expression: Expression) ->", "solution": "\n    expr = Disjunction(Conjunction(expr1, Negation(expr2)),\n                       Conjunction(Negation(expr1), expr2))\n    return ast.fix_missing_locations(expr)"}
{"prompt": "def _type_translation(self, str_type):\n        \"\"\"\n        Internal method to translate the named CRITs TLO type to a URL\n        specific string.\n        \"\"\"", "output": "\n        if str_type == 'url':\n            return self.url_prefix\n        elif str_type == 'url-prefix':\n            return self.url_prefix\n        else:\n            raise ValueError(\"Invalid URL type: %s\" % str_type)\n\n    def _get_url_prefix(self, url):\n        \"\"\"\n        Internal method to get the URL prefix for a given URL.\n        \"\"\"\n        if url.startswith(self.url_prefix):\n            return url[len(self.url_prefix):]\n        else:\n            return self.url_prefix\n\n    def _get_url_type(self,", "solution": "\n        if str_type == 'Indicator':\n            return 'indicators'\n        if str_type == 'Domain':\n            return 'domains'\n        if str_type == 'IP':\n            return 'ips'\n        if str_type == 'Sample':\n            return 'samples'\n        if str_type == 'Event':\n            return 'events'\n        if str_type == 'Actor':\n            return 'actors'\n        if str_type == 'Email':\n            return 'emails'\n        if str_type == 'Backdoor':\n            return 'backdoors'\n\n        raise CRITsInvalidTypeError('Invalid object type specified: '\n                                    '{}'.format(str_type))"}
{"prompt": "def copy(self, repository=None, tag=None,\n             source_transport=None,\n             target_transport=SkopeoTransport.DOCKER,\n             source_path=None, target_path=None,\n             logs=True):\n        \"\"\" Copy this image\n\n        :param repository to be copied to\n        :param tag\n        :param source_transport Transport\n        :param target_transport Transport\n        :param source_path needed to specify for dir, docker-archive or oci transport\n        :param target_path needed to specify for dir, docker-archive or oci transport\n        :param logs enable/disable logs\n        :return: the new DockerImage\n        \"\"\"", "output": "\n        \"\"\"\n        if repository is None:\n            repository = self.repository\n        if tag is None:\n            tag = self.tag\n\n        if not self.exists:\n            raise ImageFactoryException(\"Image %s does not exist\" % self.name)\n\n        if not self.is_public:\n            raise ImageFactoryException(\"Image %s is not public\" % self.name)\n\n        if not self.is_protected:\n            raise ImageFactoryException(\"Image %s is protected\" % self.name)\n\n        if not self.is_public:\n            raise ImageFactoryException(\"Image %s is not public\" % self", "solution": "\n        if not repository:\n            repository = self.name\n        if not tag:\n            tag = self.tag if self.tag else \"latest\"\n        if target_transport == SkopeoTransport.OSTREE and tag and logs:\n            logging.warning(\"tag was ignored\")\n        target = (DockerImage(repository, tag, pull_policy=DockerImagePullPolicy.NEVER)\n                  .using_transport(target_transport, target_path))\n        self.using_transport(source_transport, source_path)\n\n        try:\n            run_cmd([\"skopeo\", \"copy\",\n                     transport_param(self),\n                     transport_param(target)])\n        except subprocess.CalledProcessError:\n            raise ConuException(\"There was an error while copying repository\", self.name)\n\n        return target"}
{"prompt": "def sample(name, reads_in_tuple):\n    \"\"\"\tCreate a new sample.\n\t\"\"\"", "output": "\n    return [reads_in_tuple[0]] + reads_in_tuple[1:]\n\ndef get_reads_in_file(name):\n    \"\"\"\tGet the reads in a file.\n\t\"\"\"\n    return os.path.join(os.path.dirname(os.path.abspath(__file__)), name + \".fasta\"\n\ndef get_reads_in_file_2(name):\n    \"\"\"\tGet the reads in a file.\n\t\"\"\"\n    return os.path.join(os.path.dirname(os.path.abspath(__file__)), name + \".fasta\"", "solution": "\n    if name in [sample_x.get_name() for sample_x in __SAMPLES__]:\n        rnftools.utils.error(\n            \"Multiple samples have the same name. Each sample must have a unique name.\",\n            program=\"RNFtools\",\n            subprogram=\"MIShmash\",\n            exception=ValueError,\n        )\n\n    Sample(\n        name=name,\n        reads_in_tuple=reads_in_tuple,\n    )\n    add_input(current_sample().fq_fns())"}
{"prompt": "def associate(self, from_email, new_parent_incident=None):\n        \"\"\"Associate an alert with an incident using a valid email address.\"\"\"", "output": "\n        if new_parent_incident is None:\n            new_parent_incident = self.incident\n        if new_parent_incident is None:\n            new_parent_incident = self.parent_incident\n        if from_parent_incident is None:\n            from_parent_incident = self.parent_incident\n        if from_parent_incident.email_address == new_email:\n            return\n        if from_parent_incident.parent_incident_id == self.id:\n            self.parent_incident = new_parent_incident\n            self.incident = new_parent_incident.parent_incident_id", "solution": "\n        if from_email is None or not isinstance(from_email, six.string_types):\n            raise MissingFromEmail(from_email)\n\n        if new_parent_incident is None:\n            raise InvalidArguments(new_parent_incident)\n\n        parent_incident_id = self['incident']['id']\n        endpoint_format = 'incidents/{0}/alerts/{1}'\n        endpoint = endpoint_format.format(parent_incident_id, self['id'])\n\n        if isinstance(new_parent_incident, Entity):\n            new_parent_incident_id = new_parent_incident['id']\n        else:\n            new_parent_incident_id = new_parent_incident\n\n        add_headers = {'from': from_email, }\n        data = {\n            'alert': {\n                'id': self['id'],\n                'type': 'alert',\n                'incident': {\n                    'type': 'incident',\n                    'id': new_parent_incident_id,\n                }\n            }\n        }\n\n        result = self.request('PUT',\n                              endpoint=endpoint,\n                              add_headers=add_headers,\n                              data=data,)\n        return result"}
{"prompt": "def get_member(self, name):\n        \"\"\"return a member by *name*\"\"\"", "output": "\n        return self.get_member_raw(name)\n\n    def get_member_raw(self, name):\n        \"\"\"return a member by *name*\"\"\"\n        return self.get_member_info_raw(name)\n\n    def get_member_info_raw(self, name):\n        \"\"\"return a member by *name*\"\"\"\n        return self.get_member_info_info_raw(name)\n\n    def get_member_info_info_list(self, name):\n        \"\"\"return a list of members by *name*\"\"\"\n        return self.get_member_info_info_", "solution": "\n        for member in self.members:\n            if member.name == name:\n                return member\n\n        return None"}
{"prompt": "def slugify(value):\n    \"\"\"\n    Converts to lowercase, removes non-word characters (alphanumerics and\n    underscores) and converts spaces to hyphens. Also strips leading and\n    trailing whitespace.\n    \"\"\"", "output": "\n    value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore')\n    value = re.sub(r'\\s+','', value)\n    value = re.sub(r'\\s+$', '', value)\n    value = re.sub(r'[-\\s]+','', value)\n    return re.sub(r'[-\\s]+', '-', value)\n\n\ndef get_random_string(length=12, allowed_chars='abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'):\n    \"\"\"\n    Returns a securely generated random string.\n    The default length of 12 with the a-z, A-Z", "solution": "\n    if six.PY3:\n        value = str(value)\n    else:\n        value = unicode(value)\n\n    value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n    value = to_remove.sub('', value).strip().lower()\n    return remove_dup.sub('-', value)"}
{"prompt": "def menu_pages(colors, page=1, print_keys=True, per_page=15):\n    \"\"\"return menu items by page from list: colors\"\"\"", "output": "\n    if per_page < 1:\n        per_page = 15\n    if page > 1:\n        page = 1\n    if print_keys:\n        print(colors[0])\n        print(colors[1])\n        print(colors[2])\n        print(colors[3])\n        print(colors[4])\n        print(colors[5])\n        print(colors[6])\n        print(colors[7])\n        print(colors[8])\n        print(colors[9])\n        print(colors[10])\n        print(colors[11])\n        print(colors[12])\n        print(colors[13])", "solution": "\n\n    c = os.system('clear')\n    length = len(colors)\n    last_page = length / per_page\n    if (last_page * per_page) < length:\n        last_page += 1\n\n    page_display = \"page (%d/%d)\" % (page, last_page)\n    start = per_page * (page - 1)\n    keys = "}
{"prompt": "def renameToNamespaceScopes(self):\n        \"\"\"\n        Helper method for :func:`~exhale.graph.ExhaleRoot.reparentAll`. Some compounds in\n        Breathe such as functions and variables do not have the namespace name they are\n        declared in before the name of the actual compound.  This method prepends the\n        appropriate (nested) namespace name before the name of any child that does not\n        already have it.\n\n        For example, the variable ``MAX_DEPTH`` declared in namespace ``external`` would\n        have its ExhaleNode's ``name`` attribute changed from ``MAX_DEPTH`` to\n        ``external::MAX_DEPTH``.\n        \"\"\"", "output": "`` to\n        ``external``.\n\n        :return: A list of the renamed namespace scopes.\n        \"\"\"\n        return self._renameToScopes(self.node.name)\n\n    def _renameToScopes(self, name):\n        \"\"\"\n        Helper method for :func:`~exhale.graph.ExhaleRoot.renameToNamespaceScopes`.\n        \"\"\"\n        if name =='maxDepth':\n            return self.node.maxDepth\n        elif name == 'name':\n            return self.node.name\n        else:\n            raise ValueError(\"Unknown name '%s'\" % name)\n\n    def _renameToScope(self", "solution": "\n        for n in self.namespaces:\n            namespace_name = \"{0}::\".format(n.name)\n            for child in n.children:\n                if namespace_name not in child.name:\n                    child.name = \"{0}{1}\".format(namespace_name, child.name)"}
{"prompt": "def node_link_graph(data, directed=False, attrs=_attrs):\n    \"\"\"Return graph from node-link data format.\n\n    Parameters\n    ----------\n    data : dict\n        node-link formatted graph data\n\n    directed : bool\n        If True, and direction not specified in data, return a directed graph.\n\n    attrs : dict\n        A dictionary that contains three keys 'id', 'source', 'target'.\n        The corresponding values provide the attribute names for storing\n        Dynetx-internal graph data. Default value:\n        :samp:`dict(id='id', source='source', target='target')`.\n\n    Returns\n    -------\n    G : DyNetx graph\n       A DyNetx graph object\n\n    Examples\n    --------\n    >>> from dynetx.readwrite import json_graph\n    >>> G = dn.DynGraph([(1,2)])\n    >>> data = json_graph.node_link_data(G)\n    >>> H = json_graph.node_link_graph(data)\n\n    See Also\n    --------\n    node_link_data\n    \"\"\"", "output": " -------\n    NetworkX graph\n\n    Examples\n    --------\n    >>> G = nx.DiGraph()\n    >>> G.add_path([0,1,2,3])\n    >>> G.add_node(0)\n    >>> G.add_node(1)\n    >>> G.add_edge(0, 1, weight=2)\n    >>> G.add_edge(1, 2, weight=3)\n    >>> G.add_edge(2, 3, weight=1)\n    >>> G.node[0]['weight'] = 4\n    >>> G.node[1]['weight'] = 3\n    >>> G", "solution": "\n\n    directed = data.get('directed', directed)\n    graph = dn.DynGraph()\n    if directed:\n        graph = graph.to_directed()\n\n    id_ = attrs['id']\n    mapping = []\n    graph.graph = data.get('graph', {})\n    c = count()\n    for d in data['nodes']:\n        node = d.get(id_, next(c))\n        mapping.append(node)\n        nodedata = dict((make_str(k), v) for k, v in d.items() if k != id_)\n        graph.add_node(node, **nodedata)\n    for d in data['links']:\n        graph.add_interaction(d['source'], d[\"target\"], d['time'])\n\n    return graph"}
{"prompt": "def learningCurve(expPath, suite):\n  \"\"\"\n  Print the test and overall noise errors from each iteration of this experiment\n  \"\"\"", "output": "\n  print \"Learning Curve\"\n  print \"=====================\"\n  print \"Test: \", suite, \"Path: \", expPath, \"Noise: \", np.sqrt(np.mean(expPath**2))\n\ndef runExperiment(suite, noise, expPath, expName, learningCurve, learningCurveName):\n  \"\"\"\n  Run the experiment\n  \"\"\"\n  # Create the experiment object\n  exp = Experiment(suite, noise, expPath, expName, learningCurve, learningCurveName)\n  # Run the experiment\n  runExperimentWithNoise(suite, exp, learningCurve, learningCurveName)\n\n", "solution": "\n  print(\"\\nLEARNING CURVE ================\",expPath,\"=====================\")\n  try:\n    headers=[\"testerror\",\"totalCorrect\",\"elapsedTime\",\"entropy\"]\n    result = suite.get_value(expPath, 0, headers, \"all\")\n    info = []\n    for i,v in enumerate(zip(result[\"testerror\"],result[\"totalCorrect\"],\n                             result[\"elapsedTime\"],result[\"entropy\"])):\n      info.append([i, v[0], v[1], int(v[2]), v[3]])\n    headers.insert(0,\"iteration\")\n    print(tabulate(info, headers=headers, tablefmt=\"grid\"))\n  except:\n    print(\"Couldn't load experiment\",expPath)"}
{"prompt": "def _clean_check(cmd, target):\n    \"\"\"\n    Run the command to download target. If the command fails, clean up before\n    re-raising the error.\n    \"\"\"", "output": "\n    try:\n        subprocess.check_call(cmd, shell=True)\n    except subprocess.CalledProcessError:\n        if os.path.exists(target):\n            os.remove(target)\n        raise\n\n\ndef _get_file_list(path):\n    \"\"\"\n    Return a list of files in a given path.\n    \"\"\"\n    with open(path, 'r') as f:\n        return f.read().splitlines()\n\n\ndef _get_file_list_from_url(url):\n    \"\"\"\n    Return a list of files from a given url.\n    \"\"\"\n    with urllib.request.urlopen(url", "solution": "\n    try:\n        subprocess.check_call(cmd)\n    except subprocess.CalledProcessError:\n        if os.access(target, os.F_OK):\n            os.unlink(target)\n        raise"}
{"prompt": "def new(self, kind, **extra):\n        \"\"\"\n        Return a host name for a new node of the given kind.\n\n        The new name is formed by interpolating ``{}``-format\n        specifiers in the string given as ``pattern`` argument to the\n        class constructor.  The following names can be used in the\n        ``{}``-format specifiers:\n\n        * ``kind`` -- the `kind` argument\n        * ``index`` -- a positive integer number, garanteed to be unique (per kind)\n        * any other keyword argument used in the call to :meth:`new`\n\n        Example::\n\n          >>> p = NodeNamingPolicy(pattern='node-{kind}-{index}{spec}')\n          >>> p.new('foo', spec='bar')\n          'node-foo-1bar'\n          >>> p.new('foo', spec='quux')\n          'node-foo-2quux'\n        \"\"\"", "output": " = Path('/usr/local/lib/python2.7/site-packages/nikola.py')\n          >>> p.new('python', kind='py')\n          'python2.7'\n\n          >>> p.new('python', index=2)\n          'python2.7'\n\n          >>> p.new('python', pattern='/usr/local/lib/python2.7/site-packages/nikola.py')\n          'python2.7-site-packages/nikola.py'\n\n        \"\"\"\n        return self.interpolate(pattern, kind, **extra)\n\n    def interpolate(self,", "solution": "\n        if self._free[kind]:\n            index = self._free[kind].pop()\n        else:\n            self._top[kind] += 1\n            index = self._top[kind]\n        return self._format(self.pattern, kind=kind, index=index, **extra)"}
{"prompt": "def get_current():\n    \"\"\"return current Xresources color theme\"\"\"", "output": "\n    return _Xresources.get_current()\n\ndef set_current(color):\n    \"\"\"set current Xresources color theme\"\"\"\n    _Xresources.set_current(color)\n\ndef get_current_color():\n    \"\"\"return current Xresources color theme\"\"\"\n    return _Xresources.get_current_color()\n\ndef set_current_color(color):\n    \"\"\"set current Xresources color theme\"\"\"\n    _Xresources.set_current_color(color)\n\ndef get_current_resource():\n    \"\"\"return Xresources resource color theme\"\"\"\n    return _Xresources.get_", "solution": "\n    global current\n    if exists( SETTINGSFILE ):\n        f = open( SETTINGSFILE ).read()\n        current = re.findall('config[^\\s]+.+', f)[1].split('/')[-1]\n        return current\n    else:\n        return \"** Not Set **\""}
{"prompt": "def _filter_schema(schema, schema_tables, exclude_table_columns):\n    \"\"\"Filters a schema to only include the specified tables in the\n       schema_tables parameter.  This will also filter out any colums for\n       included tables that reference tables that are not included\n       in the schema_tables parameter\n\n    :param schema: Schema dict to be filtered\n    :param schema_tables: List of table names to filter on.\n                          EX: ['Bridge', 'Controller', 'Interface']\n                          NOTE: This list is case sensitive.\n    :return: Schema dict:\n                filtered if the schema_table parameter contains table names,\n                else the original schema dict\n    \"\"\"", "output": "\n                if the schema_tables parameter contains tables that reference more than one\n                if the schema_table parameter contains tables that\n                are included more than one if the schema_tables parameter contains\n                tables that reference more than one\n\n    \"\"\"\n    if 'tables' in schema:\n        schema['tables'] = schema_tables\n    if 'exclude_columns' in schema:\n        schema['exclude_columns'] = schema_exclude_columns(schema['exclude_columns'], exclude_table_columns)\n    return schema\n\ndef _get_schema_from_table(table):\n    \"\"\"Returns a schema dict for the specified table\n\n    :", "solution": "\n\n    tables = {}\n    for tbl_name, tbl_data in schema['tables'].items():\n        if not schema_tables or tbl_name in schema_tables:\n            columns = {}\n\n            exclude_columns = exclude_table_columns.get(tbl_name, [])\n            for col_name, col_data in tbl_data['columns'].items():\n                if col_name in exclude_columns:\n                    continue\n\n                # NOTE(Alan Quillin) Needs to check and remove\n                # and columns that have references to tables that\n                # are not to be configured\n                type_ = col_data.get('type')\n                if type_:\n                    if type_ and isinstance(type_, dict):\n                        key = type_.get('key')\n                        if key and isinstance(key, dict):\n                            ref_tbl = key.get('refTable')\n                            if ref_tbl and isinstance(ref_tbl,\n                                                      six.string_types):\n                                if ref_tbl not in schema_tables:\n                                    continue\n                        value = type_.get('value')\n                        if value and isinstance(value, dict):\n                            ref_tbl = value.get('refTable')\n                            if ref_tbl and isinstance(ref_tbl,\n                                                      six.string_types):\n                                if ref_tbl not in schema_tables:\n                                    continue\n\n                columns[col_name] = col_data\n\n            tbl_data['columns'] = columns\n            tables[tbl_name] = tbl_data\n\n    schema['tables'] = tables\n\n    return schema"}
{"prompt": "def get_object_query_dict(self):\n        \"\"\"returns objects keys according to self.object_query_code\n        which can be json encoded queryset filter dict or key=value set\n         in the following format: ```\"key=val, key2 = val2 , key3= value with spaces\"```\n\n        Returns:\n             (dict): Queryset filtering dicqt\n        \"\"\"", "output": "\n        if self.object_query_code == 'json':\n            return self.get_keys_from_json()\n        elif self.object_query_code == 'filter':\n            return self.get_keys_from_filter()\n        elif self.object_query_code == 'key':\n            return self.get_keys_from_key()\n        elif self.object_query_code == 'value':\n            return self.get_keys_from_value()\n        else:\n            raise ValueError('Invalid object query code')\n\n    def get_keys_from_json(self):\n        \"\"\"returns keys", "solution": "\n        if isinstance(self.object_query_code, dict):\n            # _DATE_ _DATETIME_\n            return self.object_query_code\n        else:\n            # comma separated, key=value pairs. wrapping spaces will be ignored\n            # eg: \"key=val, key2 = val2 , key3= value with spaces\"\n            return dict(pair.split('=') for pair in self.object_query_code.split(','))"}
{"prompt": "def add(self, src):\n        \"\"\" store an audio file to storage dir\n\n        :param src: audio file path\n        :return: checksum value\n        \"\"\"", "output": "\n        self.logger.debug(\"add %s\", src)\n        self.logger.debug(\"add %s\", os.path.join(self.storage_dir, src))\n        self.logger.debug(\"add %s\", os.path.join(self.storage_dir, src.split('/')[-1]))\n        self.logger.debug(\"add %s\", os.path.join(self.storage_dir, src))\n        self.logger.debug(\"add %s\", os.path.join(self.storage_dir, src.split('/')[-1]))\n        self.logger.debug(\"", "solution": "\n        if not audio.get_type(src):\n            raise TypeError('The type of this file is not supported.')\n\n        return super().add(src)"}
{"prompt": "def _add_sub_parsers(self, top_level_parser, methods_to_parse, class_name):\n        \"\"\"Add all the sub-parsers to the top_level_parser.\n\n        Args:\n            top_level_parser: the top level parser\n            methods_to_parse: dict of method name pointing to their associated\n            argument parser\n            class_name: name of the decorated class\n\n        Returns:\n            a dict of registered name of the parser i.e. sub command name\n            pointing to the method real name\n        \"\"\"", "output": "\n        methods_to_parse[class_name] = top_level_parser\n        for name, method in methods_to_parse.items():\n            if isinstance(method, types.FunctionType):\n                self._add_function_parsers(name, method, types.FunctionType)\n            elif isinstance(method, types.MethodType):\n                self._add_sub_parsers(name, method, class_name)\n            elif isinstance(method, types.FunctionType):\n                self._add_function_parsers(name, method, types.FunctionType)\n            elif isinstance(method, types.MethodType):\n                self._add_sub", "solution": "\n        description = \"Accessible methods of {}\".format(class_name)\n        sub_parsers = top_level_parser.add_subparsers(description=description,\n                                                      dest=\"method\")\n        # Holds the mapping between the name registered for the parser\n        # and the method real name. It is useful in the 'inner_call'\n        # method retrieve the real method\n        parser_to_method = {}\n        for method_name, parser in methods_to_parse.items():\n            # We use the name provided in 'create_parser` or the name of the\n            # decorated method\n            parser_name = parser.get_name() or method_name\n            # Make the method name compatible for the argument parsing\n            if parser_name.startswith(\"_\"):\n                if not self._parse_private:\n                    # We skip private methods if the caller asked not to\n                    # parse them\n                    continue\n                # 'Private' methods are exposed without their leading or\n                # trailing '_'s. Also works for 'special' methods.\n                parser_name = parser_name.strip(\"_\")\n            parser_name = parser_name.replace(\"_\", \"-\")\n            parser_to_method[parser_name] = method_name\n            sub_parsers.add_parser(parser_name, parents=[parser],\n                                   add_help=False,\n                                   description=parser.description)\n        return parser_to_method"}
{"prompt": "def _get_csv_fieldnames(csv_reader):\n    \"\"\"Finds fieldnames in Polarion exported csv file.\"\"\"", "output": "\n    fieldnames = []\n    for row in csv_reader:\n        if row[0] == 'id':\n            fieldnames.append(row[1])\n        elif row[0] == 'name':\n            fieldnames.append(row[1])\n        elif row[0] == 'description':\n            fieldnames.append(row[1])\n        elif row[0] == 'type':\n            fieldnames.append(row[1])\n        else:\n            raise ValueError('Unknown field type: %s' % row[1])\n    return fieldnames\n\n\ndef _get_csv_data(csv_", "solution": "\n    fieldnames = []\n    for row in csv_reader:\n        for col in row:\n            field = (\n                col.strip()\n                .replace('\"', \"\")\n                .replace(\" \", \"\")\n                .replace(\"(\", \"\")\n                .replace(\")\", \"\")\n                .lower()\n            )\n            fieldnames.append(field)\n        if \"id\" in fieldnames:\n            break\n        else:\n            # this is not a row with fieldnames\n            del fieldnames[:]\n    if not fieldnames:\n        return None\n    # remove trailing unannotated fields\n    while True:\n        field = fieldnames.pop()\n        if field:\n            fieldnames.append(field)\n            break\n    # name unannotated fields\n    suffix = 1\n    for index, field in enumerate(fieldnames):\n        if not field:\n            fieldnames[index] = \"field{}\".format(suffix)\n            suffix += 1\n\n    return fieldnames"}
{"prompt": "def _set_choices(self, value):\n        \"\"\"\n        Also update the widget's choices.\n        \"\"\"", "output": "\n        self.widget.choices = value\n\n    choices = property(_get_choices, _set_choices)\n\n    def _get_choices(self):\n        \"\"\"\n        Getter method for the 'choices' property.\n        \"\"\"\n        return self.widget.choices\n\n    def _set_help_text(self, value):\n        \"\"\"\n        Also update the widget's help text.\n        \"\"\"\n        self.widget.help_text = value\n\n    help_text = property(_get_help_text, _set_help_text)\n\n    def _get_help_text(self):\n        \"\"\"\n        Getter method for the", "solution": "\n        super(LazyChoicesMixin, self)._set_choices(value)\n        self.widget.choices = value"}
{"prompt": "def context_register(self, func, *args):\n        \"\"\"\n        :term:`Context manager <context manager>` which temporarily registers a\n        filter function.\n\n        :param func: The filter function to register.\n        :param order: The sorting key for the filter function.\n        :rtype: :term:`context manager`\n        :return: Context manager which temporarily registers the filter\n                 function.\n\n        If :meth:`register` does not require `order` because it has been\n        overridden in a subclass, the `order` argument can be omitted here,\n        too.\n\n        .. versionadded:: 0.9\n        \"\"\"", "output": "\n        if not callable(func):\n            raise TypeError('filter_function must be callable')\n        if not isinstance(order, basestring):\n            raise TypeError('order must be a string')\n        if not order:\n            order = '*'\n        self.register(order, func, *args)\n        return self\n\n    def context_unregister(self, ctx):\n        \"\"\"\n        :term:`Context manager <context manager>` which unregisters a\n        filter function.\n\n        :param ctx: The filter function to unregister.\n        :rtype: :term:`context manager`\n        :return: Context manager which unregisters the filter\n                 function.\n\n        If", "solution": "\n        token = self.register(func, *args)\n        try:\n            yield\n        finally:\n            self.unregister(token)"}
{"prompt": "def GetArgs():\n   \"\"\"\n   Supports the command-line arguments listed below.\n   \"\"\"", "output": "\n   parser = argparse.ArgumentParser(description='Run the tests.')\n   parser.add_argument('-c', '--config', default='config.json', help='Path to the configuration file.')\n   parser.add_argument('-d', '--debug', action='store_true', help='Enable debug mode.')\n   parser.add_argument('-e', '--export', action='store_true', help='Export the tests.')\n   parser.add_argument('-f', '--force', action='store_true', help='Force the test run even if the test fails.')\n   parser.add_argument('-s', '--suite", "solution": "\n   parser = argparse.ArgumentParser(\n       description='Process args for retrieving all the Virtual Machines')\n   parser.add_argument('-s', '--host', required=True, action='store',\n                       help='Remote host to connect to')\n   parser.add_argument('-o', '--port', type=int, default=443, action='store',\n                       help='Port to connect on')\n   parser.add_argument('-u', '--user', required=True, action='store',\n                       help='User name to use when connecting to host')\n   parser.add_argument('-p', '--password', required=False, action='store',\n                       help='Password to use when connecting to host')\n   args = parser.parse_args()\n   return args"}
{"prompt": "def set_secure(self, section, option, value):\n        \"\"\"Set an option and mark it as secure.\n\n        Any subsequent uses of 'set' or 'get' will also\n        now know that this option is secure as well.\n        \"\"\"", "output": "\n        if not self.secure:\n            self.secure = value\n        else:\n            self.secure_flag = True\n\n    def get_secure(self):\n        \"\"\"Get the current value of the'secure' option.\"\"\"\n        return self.secure_flag\n\n    def set_httponly(self, section, option, value):\n        \"\"\"Set an option and mark it as httponly.\n\n        Any subsequent uses of'set' or 'get' will also\n        now know that this option is httponly as well.\n        \"\"\"\n        if not self.httponly:\n            self.httponly = value\n        else:\n            self.", "solution": "\n        if self.keyring_available:\n            s_option = \"%s%s\" % (section, option)\n            self._unsaved[s_option] = ('set', value)\n            value = self._secure_placeholder\n        ConfigParser.set(self, section, option, value)"}
{"prompt": "def load_alias_hash(self):\n        \"\"\"\n        Load (create, if not exist) the alias hash file.\n        \"\"\"", "output": "\n        if not os.path.isfile(self.alias_hash_file):\n            return False\n\n        self.alias_hash = open(self.alias_hash_file, 'r').read()\n        return True\n\n    def get_alias_hash(self):\n        \"\"\"\n        Return the hash of the alias.\n        \"\"\"\n        return self.alias_hash\n\n    def get_hash(self):\n        \"\"\"\n        Return the hash of the alias.\n        \"\"\"\n        return self.hash\n\n    def get_alias_list(self):\n        \"\"\"\n        Return the list of all aliases.\n        \"\"\"\n        return self.alias_list", "solution": "\n        # w+ creates the alias hash file if it does not exist\n        open_mode = 'r+' if os.path.exists(GLOBAL_ALIAS_HASH_PATH) else 'w+'\n        with open(GLOBAL_ALIAS_HASH_PATH, open_mode) as alias_config_hash_file:\n            self.alias_config_hash = alias_config_hash_file.read()"}
{"prompt": "def to_dict(self):\n        \"\"\"Return a dictionary representation of the Predicate.\"\"\"", "output": "\n        return {\n            'id': self.id,\n            'name': self.name,\n            'type': self.type,\n            'description': self.description,\n            'is_draft': self.is_draft,\n            'is_active': self.is_active,\n            'is_retired': self.is_retired,\n            'is_archived': self.is_archived,\n            'is_active_text': self.is_active_text,\n            'is_retired_text': self.is_retired_text,\n            'is", "solution": "\n        return {\n            'predicate': self.predicate,\n            'parents': list(self.supertypes),\n            'synopses': [[role.to_dict() for role in synopsis]\n                         for synopsis in self.synopses]\n        }"}
{"prompt": "def check_thickness(s):\n    \"\"\"\n    Check and parse thickness specs as either a single [s] or a list of [s,s,s,...]\n    \"\"\"", "output": "\n    if isinstance(s, list):\n        for s in s:\n            check_thickness(s)\n    elif isinstance(s, str):\n        s = s.split(',')\n        for s in s:\n            check_thickness(s)\n    else:\n        raise ValueError(\"thickness must be a string or a list of strings\")\n\n\ndef check_color(s):\n    \"\"\"\n    Check and parse color specs as either a single [s] or a list of [s,s,...]\n    \"\"\"\n    if isinstance(s, list):\n        for s in s:\n            check_color(s)\n    elif isinstance(", "solution": "\n\n    s = check_1d(s, \"thickness\")\n    if any(map(lambda d: d <= 0, s)):\n        raise Exception('Thickness cannot be 0 or negative')\n\n    return s"}
{"prompt": "def detect_sv(items, all_items=None, stage=\"standard\"):\n    \"\"\"Top level parallel target for examining structural variation.\n    \"\"\"", "output": "\n    if all_items is None:\n        all_items = items\n    if all_items is not None:\n        for item in all_items:\n            if item.name == item.structural_variation:\n                if item.stage == stage:\n                    return item\n                else:\n                    return None\n    else:\n        return None\n\n\ndef detect_sv_target(stage=\"standard\"):\n    \"\"\"\n    Detect structural variation target.\n    \"\"\"\n    if stage == \"standard\":\n        return \"std\"\n    elif stage == \"gpu\":\n        return \"gpu\"\n    else:\n        return None\n\n\ndef detect_nv", "solution": "\n    items = [utils.to_single_data(x) for x in items]\n    items = cwlutils.unpack_tarballs(items, items[0])\n    svcaller = items[0][\"config\"][\"algorithm\"].get(\"svcaller\")\n    caller_fn = _get_callers(items, stage, special_cases=True).get(svcaller)\n    out = []\n    if svcaller and caller_fn:\n        if (all_items and svcaller in _NEEDS_BACKGROUND and\n                not vcfutils.is_paired_analysis([x.get(\"align_bam\") for x in items], items)):\n            names = set([dd.get_sample_name(x) for x in items])\n            background = [x for x in all_items if dd.get_sample_name(x) not in names]\n            for svdata in caller_fn(items, background):\n                out.append([svdata])\n        else:\n            for svdata in caller_fn(items):\n                out.append([svdata])\n    else:\n        for data in items:\n            out.append([data])\n    # Avoid nesting of callers for CWL runs for easier extraction\n    if cwlutils.is_cwl_run(items[0]):\n        out_cwl = []\n        for data in [utils.to_single_data(x) for x in out]:\n            # Run validation directly from CWL runs since we're single stage\n            data = validate.evaluate(data)\n            data[\"svvalidate\"] = {\"summary\": tz.get_in([\"sv-validate\", \"csv\"], data)}\n            svs = data.get(\"sv\")\n            if svs:\n                assert len(svs) == 1, svs\n                data[\"sv\"] = svs[0]\n            else:\n                data[\"sv\"] = {}\n            data = _add_supplemental(data)\n            out_cwl.append([data])\n        return out_cwl\n    return out"}
{"prompt": "def apply_theme(self, themename, themeoptions):\n        \"\"\"Apply a new theme to the document.\n\n        This will store the existing theme configuration and apply a new one.\n\n        \"\"\"", "output": "\n        self.themename = themename\n        self.themeoptions = themeoptions\n        self.filename = themename + '.xml'\n        self.xml = etree.parse(self.filename)\n        self.styles.append(self.xml)\n        self.xml = self.xml.getroot()\n        self.styles.sort(key=lambda x: x.get('order', 'normal'))\n        self.styles.insert(0, self.xml)\n        self.xml.write(open(self.filename, 'w'))\n\n    def get_theme_xml(self):\n        \"\"\"Return the current configuration", "solution": "\n\n        # push the existing values onto the Stack\n        self._theme_stack.append(\n            (self.theme, self.theme_options)\n        )\n\n        theme_factory = HTMLThemeFactory(self.app)\n        theme_factory.load_additional_themes(self.get_builtin_theme_dirs() + self.config.slide_theme_path)\n\n        self.theme = theme_factory.create(themename)\n        self.theme_options = themeoptions.copy()\n        self.templates.init(self, self.theme)\n        self.templates.environment.filters['json'] = json.dumps\n\n        if self.theme not in self._additional_themes:\n            self._additional_themes.append(self.theme)"}
{"prompt": "def prefer_replica(nodes: List[DiscoveredNode]) -> Optional[DiscoveredNode]:\n    \"\"\"\n    Select a random replica if any are available or fall back to the master.\n    \"\"\"", "output": "\n    if not nodes:\n        return None\n    if len(nodes) == 1:\n        return nodes[0]\n    return random.choice(nodes)\n\n\ndef _get_master_ip_address(node: DiscoveredNode) -> Optional[str]:\n    \"\"\"\n    Return the master IP address if any are available.\n    \"\"\"\n    if not node.master_ip:\n        return None\n    if len(node.master_ip) == 1:\n        return node.master_ip[0]\n    return _get_master_ip_address(node.master_ip[0])\n\n\ndef _get", "solution": "\n    masters = [node for node in nodes if node.state == NodeState.Master]\n    replicas = [node for node in nodes if node.state != NodeState.Master]\n\n    if replicas:\n        return random.choice(replicas)\n    else:\n        # if you have more than one master then you're on your own, bud.\n\n        return masters[0]"}
{"prompt": "def uncancel_confirmation(self, confirmation_id):\n        \"\"\"\n        Uncancelles an confirmation\n\n        :param confirmation_id: the confirmation id\n        \"\"\"", "output": "\n        self.db.execute(\"UPDATE users SET status='canceled' WHERE id=%s\",\n                       (confirmation_id,))\n        self.db.commit()\n\n    def get_user_by_id(self, id):\n        \"\"\"\n        Get a user by id\n\n        :param id: the id of the user\n        \"\"\"\n        return self.db.execute(\"SELECT * FROM users WHERE id=%s\", (id,)).fetchone()\n\n    def get_user_by_email(self, email):\n        \"\"\"\n        Get a user by email\n\n        :param email: the email of the user\n        \"\"\"\n        return self.", "solution": "\n        return self._create_put_request(\n            resource=CONFIRMATIONS,\n            billomat_id=confirmation_id,\n            command=UNCANCEL,\n        )"}
{"prompt": "def patch_datasette():\n    \"\"\"\n    Monkey patching for original Datasette\n    \"\"\"", "output": "\n    # Create a new dataset\n    dataset = Dataset(\n        name='test_dataset',\n        description='test dataset',\n        files=[\n            File(name='test_file', path='/test/file', size=10, content='a'),\n            File(name='test_file2', path='/test/file2', size=10, content='b'),\n            File(name='test_file3', path='/test/file3', size=10, content='c'),\n            File(name='test_file4', path='/test/file4', size=10, content='d'),\n            File(", "solution": "\n\n    def inspect(self):\n        \" Inspect the database and return a dictionary of table metadata \"\n        if self._inspect:\n            return self._inspect\n\n        _inspect = {}\n        files = self.files\n\n        for filename in files:\n            self.files = (filename,)\n            path = Path(filename)\n            name = path.stem\n            if name in _inspect:\n                raise Exception(\"Multiple files with the same stem %s\" % name)\n            try:\n                _inspect[name] = self.original_inspect()[name]\n            except sqlite3.DatabaseError:\n                tables, views, dbtype = connectors.inspect(path)\n                _inspect[name] = {\n                    \"hash\": inspect_hash(path),\n                    \"file\": str(path),\n                    \"dbtype\": dbtype,\n                    \"tables\": tables,\n                    \"views\": views,\n                }\n\n        self.files = files\n        self._inspect = _inspect\n        return self._inspect\n\n    datasette.app.Datasette.original_inspect = datasette.app.Datasette.inspect\n    datasette.app.Datasette.inspect = inspect\n\n\n    async def execute(self, db_name, sql, params=None, truncate=False, custom_time_limit=None, page_size=None):\n        "}
{"prompt": "def sendOACK(self):\n        \"\"\"This method sends an OACK packet with the options from the current\n        context.\"\"\"", "output": "\n        self.send(self.getOACK())\n\n    def getOACK(self):\n        \"\"\"This method returns the OACK packet that is sent to the\n        other side of the connection.\"\"\"\n        return self.__getOACK()\n\n    def __getOACK(self):\n        \"\"\"This method returns the OACK packet that is sent to the\n        other side of the connection.\"\"\"\n        return self.send(self.getOACK())\n\n    def __setOACK(self, value):\n        \"\"\"This method sets the OACK packet that is sent to the\n        other side of the connection.\"\"\"\n        self.send(self", "solution": "\n        log.debug(\"In sendOACK with options %s\", self.context.options)\n        pkt = TftpPacketOACK()\n        pkt.options = self.context.options\n        self.context.sock.sendto(pkt.encode().buffer,\n                                 (self.context.host,\n                                  self.context.tidport))\n        self.context.last_pkt = pkt"}
{"prompt": "def get_cmdclass():\n    \"\"\"\n    Returns-->dict: to be assigned to setuptools.setup\n                    cmdclass param\n    \"\"\"", "output": "\n    return {\n        'install': InstallCommand,\n        'develop': DevelopCommand,\n       'sdist': SdistCommand,\n       'sdist_with_egg_info': SdistWithEggInfoCommand,\n       'sdist_with_data_files': SdistWithDataFilesCommand,\n       'sdist_with_egg_info_scripts': SdistWithEggInfoScriptsCommand,\n       'sdist_with_egg_info_data': SdistWithEggDataFilesCommand,\n       'sdist_with_scripts': SdistWithScriptsCommand,\n        'install_scripts': InstallScriptsCommand,\n       ", "solution": "\n    ret = {}\n    cfg = __get_config()\n    if cfg is None:\n        return\n    mapdict = {\n        'build': CustomBuild,\n        'build_clib': CustomBuildCLib,\n        'build_ext': CustomBuildExt,\n        'build_py': CustomBuildPy,\n        'build_scripts': CustomBuildScripts,\n        'install': CustomInstall,\n        'install_data': CustomInstallData,\n        'install_headers': CustomInstallHeaders,\n        'install_lib': CustomInstallLib,\n        'install_scripts': CustomInstallScripts,\n    }\n    for step in cfg.keys():\n        if step in mapdict:\n            ret[step] = mapdict[step]\n    return ret"}
{"prompt": "def generate_apiary_doc(task_router):\n    \"\"\"Generate apiary documentation.\n\n    Create a Apiary generator and add application packages to it.\n\n    :param task_router: task router, injected\n    :type task_router: TaskRouter\n    :return: apiary generator\n    :rtype: ApiaryDoc\n    \"\"\"", "output": "\n    if not task_router.is_task_router:\n        yield task_router.create_document(\n            'apiary/index.html',\n            task_name='index',\n            title='APIary',\n            content=html_generator.HTML_TEMPLATE.format(\n                name=task_name,\n                title_link=link_generator.generate_link(\n                    'apiary',\n                    'index',\n                    task_name='index'\n                ),\n                link_path=link_generator.generate_link(\n                    'apiary',\n                    'index',\n                    task_name='index'\n                ),\n               ", "solution": "\n    generator = ApiaryDoc()\n\n    for m in task_router.get_task_packages() + get_method_packages():\n        m = importlib.import_module(m)\n        generator.docmodule(m)\n\n    return generator"}
{"prompt": "def fetch_limb(self, diam_sun, base_shape=(1280, 1280)):\n        \"\"\"\n        reads the solar limb template file and adjust to the requested solar diameter\n        :param diam_sun: diameter of sun in suvi pixels\n        :param base_shape: the shape of a suvi image\n        :return: product name, None, scaled image\n        \"\"\"", "output": "\n        # read the template file\n        template_file = self.read_suvi_template(diam_sun)\n\n        # read the scaled image\n        scaled_name = self.read_suvi_scaled_template(template_file)\n\n        # if the scale is not a square, then it is a square\n        if not base_shape:\n            base_shape = (1280, 1280)\n\n        # if the shape is not a tuple, then it is a tuple\n        if not isinstance(base_shape, tuple):\n            base_shape = (1280, 1280)\n\n        # read the template", "solution": "\n        from scipy.ndimage.interpolation import zoom\n\n        fn = pkg_resources.resource_filename(\"suvitrainer\", \"path_length_280_noisy.fits\")\n        calculated_diam = int(os.path.basename(fn).split(\"_\")[2]) * 2\n        with fits.open(fn) as hdus:\n            limb_unscaled = hdus[0].data\n        scale_factor = diam_sun / calculated_diam\n        limb_scaled = zoom(limb_unscaled, scale_factor)\n        shape = limb_scaled.shape[0]\n        excess = int((shape - base_shape[0]) / 2.0)\n        limb_scaled = limb_scaled[excess:shape - excess, excess:shape - excess]\n        limb_scaled = limb_scaled[0:base_shape[0], 0:base_shape[1]]\n        return \"limb\", None, limb_scaled"}
{"prompt": "def csvdir_bundle(environ,\n                  asset_db_writer,\n                  minute_bar_writer,\n                  daily_bar_writer,\n                  adjustment_writer,\n                  calendar,\n                  start_session,\n                  end_session,\n                  cache,\n                  show_progress,\n                  output_dir,\n                  tframes=None,\n                  csvdir=None):\n    \"\"\"\n    Build a zipline data bundle from the directory with csv files.\n    \"\"\"", "output": "\n    if tframes is None:\n        tframes = []\n    if csvdir is None:\n        csvdir = environ['HOME'] + '/data/csv'\n    if not os.path.exists(csvdir):\n        raise Exception('csvdir does not exist: %s' % csvdir)\n    if not os.path.isdir(output_dir):\n        raise Exception('output_dir does not exist: %s' % output_dir)\n    if not os.path.exists(minute_bar_path):\n        raise Exception('minute_bar_path does not exist: %s' % minute_", "solution": "\n    if not csvdir:\n        csvdir = environ.get('CSVDIR')\n        if not csvdir:\n            raise ValueError(\"CSVDIR environment variable is not set\")\n\n    if not os.path.isdir(csvdir):\n        raise ValueError(\"%s is not a directory\" % csvdir)\n\n    if not tframes:\n        tframes = set([\"daily\", \"minute\"]).intersection(os.listdir(csvdir))\n\n        if not tframes:\n            raise ValueError(\"'daily' and 'minute' directories \"\n                             \"not found in '%s'\" % csvdir)\n\n    divs_splits = {'divs': DataFrame(columns=['sid', 'amount',\n                                              'ex_date', 'record_date',\n                                              'declared_date', 'pay_date']),\n                   'splits': DataFrame(columns=['sid', 'ratio',\n                                                'effective_date'])}\n    for tframe in tframes:\n        ddir = os.path.join(csvdir, tframe)\n\n        symbols = sorted(item.split('.csv')[0]\n                         for item in os.listdir(ddir)\n                         if '.csv' in item)\n        if not symbols:\n            raise ValueError(\"no <symbol>.csv* files found in %s\" % ddir)\n\n        dtype = [('start_date', 'datetime64[ns]'),\n                 ('end_date', 'datetime64[ns]'),\n                 ('auto_close_date', 'datetime64[ns]'),\n                 ('symbol', 'object')]\n        metadata = DataFrame(empty(len(symbols), dtype=dtype))\n\n        if tframe == 'minute':\n            writer = minute_bar_writer\n        else:\n            writer = daily_bar_writer\n\n        writer.write(_pricing_iter(ddir, symbols, metadata,\n                     divs_splits, show_progress),\n                     show_progress=show_progress)\n\n        # Hardcode the exchange to \"CSVDIR\" for all assets and (elsewhere)\n        # register \"CSVDIR\" to resolve to the NYSE calendar, because these\n        # are all equities and thus can use the NYSE calendar.\n        metadata['exchange'] = \"CSVDIR\"\n\n        asset_db_writer.write(equities=metadata)\n\n        divs_splits['divs']['sid'] = divs_splits['divs']['sid'].astype(int)\n        divs_splits['splits']['sid'] = divs_splits['splits']['sid'].astype(int)\n        adjustment_writer.write(splits=divs_splits['splits'],\n                                dividends=divs_splits['divs'])"}
{"prompt": "def _dedup_index(self, df_a):\n        \"\"\"Build an index for deduplicating a dataset.\n\n        Parameters\n        ----------\n        df_a : (tuple of) pandas.Series\n            The data of the DataFrame to build the index with.\n\n        Returns\n        -------\n        pandas.MultiIndex\n            A pandas.MultiIndex with record pairs. Each record pair\n            contains the index values of two records. The records are\n            sampled from the lower triangular part of the matrix.\n        \"\"\"", "output": "\n        if len(df_a) == 1:\n            return df_a[0]\n        else:\n            return df_a[1]\n\n    def _dedup_matrix(self, df_a):\n        \"\"\"Build an index for deduplicating a matrix.\n\n        Parameters\n        ----------\n        df_a : (tuple of) pandas.Series\n            The data of the DataFrame to build the index with.\n\n        Returns\n        -------\n        pandas.MultiIndex\n            A pandas.MultiIndex with record pairs. Each record pair\n            contains the index values of two records. The records are\n            sampled from the lower triangular part of", "solution": "\n        pairs = self._link_index(df_a, df_a)\n\n        # Remove all pairs not in the lower triangular part of the matrix.\n        # This part can be inproved by not comparing the level values, but the\n        # level itself.\n        pairs = pairs[pairs.labels[0] > pairs.labels[1]]\n\n        return pairs"}
{"prompt": "def smallest_prime_factor(Q):\r\n    \"\"\"Find the smallest number factorable by the small primes 2, 3, 4, and 7 \r\n    that is larger than the argument Q\"\"\"", "output": "\r\n    if Q.size < 3:\r\n        return 2\r\n    elif Q.size > 4:\r\n        return 3\r\n    elif Q.size > 7:\r\n        return 4\r\n    else:\r\n        return 7\r\n\r\ndef largest_prime_factor(Q):\r\n    \"\"\"Find the largest number factorable by the small primes 2, 3, 4, and 7 \r\n    that is larger than the argument Q\"\"\"\r\n    if Q.size < 3:\r\n        return 3\r\n    elif Q.size > 4:\r\n        return 4\r\n    else:\r\n        return 7\r\n\r\ndef is_prime(Q):\r\n    \"\"\"Check if the argument Q is a prime number", "solution": "\r\n\r\n    A = Q;\r\n    while(A != 1):\r\n        if(np.mod(A, 2) == 0):\r\n            A = A / 2\r\n        elif(np.mod(A, 3) == 0):\r\n            A = A / 3\r\n        elif(np.mod(A, 5) == 0):\r\n            A = A / 5\r\n        elif(np.mod(A, 7) == 0):\r\n            A = A / 7;\r\n        else:\r\n            A = Q + 1;\r\n            Q = A;\r\n\r\n    return Q"}
{"prompt": "def name(self):\n        \"\"\"Define table name based on its metadata (filename used on import)\n\n        If `filename` is not available, return `table1`.\n        \"\"\"", "output": "\n        return self.schema.name\n\n    def _get_table_key(self, table):\n        \"\"\"Define the key name based on its table's schema\n\n        :param table: The table to define the key name for\n        :type table: str\n        :rtype: str\n        \"\"\"\n        return self.schema.name, table\n\n    def _get_table_name(self, table):\n        \"\"\"Define the name of a key in the table\n\n        :param table: The table to define the key name for\n        :type table: str\n        :rtype: str\n        \"\"\"\n        return self.schema.name, table\n\n    def", "solution": "\n\n        from rows.plugins.utils import slug\n\n        name = self.meta.get(\"name\", None)\n        if name is not None:\n            return slug(name)\n\n        source = self.meta.get(\"source\", None)\n        if source and source.uri:\n            return slug(os.path.splitext(Path(source.uri).name)[0])\n\n        return \"table1\""}
{"prompt": "def record_participation(self, client, dt=None):\n        \"\"\"Record a user's participation in a test along with a given variation\"\"\"", "output": "\n        if dt is None:\n            dt = datetime.datetime.now()\n        if not dt:\n            dt = datetime.datetime.now()\n        if not client.is_authenticated():\n            return\n        participation = Participation(\n            user=client.user,\n            variation=dt\n        )\n        participation.save()\n        if client.has_perm('testplans.change_participation', self.get_object().slug):\n            client.delete()\n\n    def record_test_participation(self, client, participation):\n        \"\"\"Record a", "solution": "\n        if dt is None:\n            date = datetime.now()\n        else:\n            date = dt\n\n        experiment_key = self.experiment.name\n\n        pipe = self.redis.pipeline()\n\n        pipe.sadd(_key(\"p:{0}:years\".format(experiment_key)), date.strftime('%Y'))\n        pipe.sadd(_key(\"p:{0}:months\".format(experiment_key)), date.strftime('%Y-%m'))\n        pipe.sadd(_key(\"p:{0}:days\".format(experiment_key)), date.strftime('%Y-%m-%d'))\n\n        pipe.execute()\n\n        keys = [\n            _key(\"p:{0}:_all:all\".format(experiment_key)),\n            _key(\"p:{0}:_all:{1}\".format(experiment_key, date.strftime('%Y'))),\n            _key(\"p:{0}:_all:{1}\".format(experiment_key, date.strftime('%Y-%m'))),\n            _key(\"p:{0}:_all:{1}\".format(experiment_key, date.strftime('%Y-%m-%d'))),\n            _key(\"p:{0}:{1}:all\".format(experiment_key, self.name)),\n            _key(\"p:{0}:{1}:{2}\".format(experiment_key, self.name, date.strftime('%Y'))),\n            _key(\"p:{0}:{1}:{2}\".format(experiment_key, self.name, date.strftime('%Y-%m'))),\n            _key(\"p:{0}:{1}:{2}\".format(experiment_key, self.name, date.strftime('%Y-%m-%d'))),\n        ]\n        msetbit(keys=keys, args=([self.experiment.sequential_id(client), 1] * len(keys)))"}
{"prompt": "def check_version_info(redis_client):\n    \"\"\"Check if various version info of this process is correct.\n\n    This will be used to detect if workers or drivers are started using\n    different versions of Python, pyarrow, or Ray. If the version\n    information is not present in Redis, then no check is done.\n\n    Args:\n        redis_client: A client for the primary Redis shard.\n\n    Raises:\n        Exception: An exception is raised if there is a version mismatch.\n    \"\"\"", "output": "\n    if not redis_client:\n        raise Exception('Unable to initialize Redis shard.')\n\n    if redis_version_info is None:\n        raise Exception('Unable to check version info of primary Redis shard.')\n\n    if not redis_version_info.get('version'):\n        raise Exception('Unable to check version info of primary Redis shard.')\n\n    if not redis_version_info.get('version_info'):\n        raise Exception('Unable to check version info of primary Redis shard.')\n\n    if not redis_version_info.get('version_info'):\n        raise Exception('Unable to check version info of primary Redis shard.')\n\n    if", "solution": "\n    redis_reply = redis_client.get(\"VERSION_INFO\")\n\n    # Don't do the check if there is no version information in Redis. This\n    # is to make it easier to do things like start the processes by hand.\n    if redis_reply is None:\n        return\n\n    true_version_info = tuple(json.loads(ray.utils.decode(redis_reply)))\n    version_info = _compute_version_info()\n    if version_info != true_version_info:\n        node_ip_address = ray.services.get_node_ip_address()\n        error_message = (\"Version mismatch: The cluster was started with:\\n\"\n                         \"    Ray: \" + true_version_info[0] + \"\\n\"\n                         \"    Python: \" + true_version_info[1] + \"\\n\"\n                         \"    Pyarrow: \" + str(true_version_info[2]) + \"\\n\"\n                         \"This process on node \" + node_ip_address +\n                         \" was started with:\" + \"\\n\"\n                         \"    Ray: \" + version_info[0] + \"\\n\"\n                         \"    Python: \" + version_info[1] + \"\\n\"\n                         \"    Pyarrow: \" + str(version_info[2]))\n        if version_info[:2] != true_version_info[:2]:\n            raise Exception(error_message)\n        else:\n            logger.warning(error_message)"}
{"prompt": "def set_dict_item(dct, name_string, set_to):\n    \"\"\"Sets dictionary item identified by name_string to set_to.\n\n    name_string is the indentifier generated using flatten_dict.\n\n    Maintains the type of the orginal object in dct and tries to convert set_to\n    to that type.\n    \"\"\"", "output": "\n    if not isinstance(set_to, set):\n        raise TypeError(\"set_to must be a set\")\n    for name in set_to:\n        if name == name_string:\n            dct[name] = set_to[name_string]\n            break\n    else:\n        raise KeyError(\"dictionary item not identified by name_string\")\n\n\ndef flatten_dict(d):\n    \"\"\"Flattens a dictionary.\n\n    d is a dictionary.\n    \"\"\"\n    if isinstance(d, dict):\n        for key, value in d.items():\n            if isinstance(value, dict):\n                flatten_dict(value)", "solution": "\n    key_strings = str(name_string).split('-->')\n    d = dct\n    for ks in key_strings[:-1]:\n        d = d[ks]\n    item_type = type(d[key_strings[-1]])\n    d[key_strings[-1]] = item_type(set_to)"}
{"prompt": "def add_upsert(self, value, criteria):\n        \"\"\"Add a tag or populator to the batch by value and criteria\"\"\"", "output": "\n        if isinstance(value, dict):\n            value = add_upsert(self, value, criteria)\n        else:\n            raise TypeError(\"value must be a dict or a list\")\n\n        if self.tag_type == \"tag\":\n            self.tags.append(value)\n        elif self.tag_type == \"populator\":\n            self.populators.append(value)\n\n    def __len__(self):\n        \"\"\"Return the number of tags in the batch\"\"\"\n        return len(self.tags)\n\n    def __getitem__(self, key):\n        \"\"\"Get a batch by key\"\"\"\n        return self.tags", "solution": "\n\n        value = value.strip()\n        v = value.lower()\n        self.lower_val_to_val[v] = value\n        criteria_array = self.upserts.get(v)\n        if criteria_array is None:\n            criteria_array = []\n            # start with # '{\"value\": \"some_value\", \"criteria\": []}, '\n            self.upserts_size[v] = 31 + len(value)\n        criteria_array.append(criteria.to_dict())\n        self.upserts[v] = criteria_array\n        self.upserts_size[v] += criteria.json_size()"}
{"prompt": "def _make_title_node(self, node, increment=True):\n        \"\"\"Generate a new title node for ``node``.\n\n        ``node`` is a ``nextslide`` node. The title will use the node's\n        parent's title, or the title specified as an argument.\n\n        \"\"\"", "output": "\n        if increment:\n            self.title_counter += 1\n        else:\n            self.title_counter = 0\n        return node.title\n\n    def _make_slide_node(self, node, increment=True):\n        \"\"\"Generate a new slide node for ``node``.\n\n        ``node`` is a ``nextslide`` node. The slide will use the node's\n        parent's slide, or the slide specified as an argument.\n\n        \"\"\"\n        if increment:\n            self.slide_counter += 1\n        else:\n            self.slide_counter = 0\n        return node.slide\n\n    def _make_slide_node(self", "solution": "\n\n        parent_title_node = node.parent.next_node(nodes.title)\n        nextslide_info = getattr(\n            parent_title_node, 'nextslide_info',\n            (parent_title_node.deepcopy().children, 1),\n        )\n        nextslide_info = (\n            nextslide_info[0],\n            nextslide_info[1] + 1,\n        )\n\n        if node.args:\n            textnodes, messages = node.state.inline_text(\n                node.args[0],\n                1,\n            )\n            new_title = nodes.title(node.args[0], '', *textnodes)\n\n        else:\n\n            title_nodes = nextslide_info[0][:]\n\n            if 'increment' in node.attributes:\n                title_nodes.append(\n                    nodes.Text(' (%s)' % nextslide_info[1])\n                )\n\n            new_title = nodes.title(\n                '', '',\n                *title_nodes\n            )\n\n        new_title.nextslide_info = nextslide_info\n        return new_title"}
{"prompt": "def flush(self):\n        \"\"\"Flushes this instance's cache.\n\n        The driver of this instance should call this method every\n        `flush_interval`.\n\n        Returns:\n          list['ServicecontrolServicesAllocateQuotaRequest']: corresponding\n          to AllocateQuotaRequests that were pending\n\n        \"\"\"", "output": "\n        return self._flush_impl(self._pending_requests)\n\n    def _flush_impl(self, pending_requests):\n        \"\"\"Flushes this instance's cache.\n\n        Args:\n          pending_requests (list['ServicecontrolServicesAllocateQuotaRequest']): pending\n          list['ServicecontrolServicesList']: list of ServicecontrolServices\n\n        Returns:\n          list['ServicecontrolServicesFlushQuotaResponse']: corresponding\n          to FlushQuotaRequests that were pending\n\n        \"\"\"\n        if not pending_requests:\n            return []\n\n        if len(pending_requests) == 1:\n            return pending_requests[0]\n\n        for pending_request in pending", "solution": "\n        if self._cache is None:\n            return []\n        with self._cache as c, self._out as out:\n            c.expire()\n            now = self._timer()\n            for item in c.values():\n                if (not self._in_flush_all) and (not self._should_expire(item)):\n                    if (not item.is_in_flight) and item._op_aggregator is not None:\n                        item.is_in_flight = True\n                        item.last_refresh_timestamp = now\n                        out.append(item.extract_request())  # pylint: disable=no-member\n            flushed_items = list(out)\n            out.clear()  # pylint: disable=no-member\n            for req in flushed_items:\n                assert isinstance(req, sc_messages.ServicecontrolServicesAllocateQuotaRequest)\n            return flushed_items"}
{"prompt": "def sort_key_process(request, sort_key='sort'):\n    \"\"\"\n        process sort-parameter value (for example, \"-name\")\n        return:\n            current_param - field for sorting (\"name)\n            current_reversed - revers flag (True)\n    \"\"\"", "output": "\n    if request.method == 'POST':\n        if'sort' in request.POST:\n            sort_key = request.POST['sort']\n            sort = True if sort_key == 'name' else False\n            if sort:\n                sort_key = sort_key.lower()\n                sort_dir = 'desc'\n            else:\n                sort_key ='reversed'\n                sort_dir = 'desc'\n            if'sort' in request.POST:\n                sort_key = request.POST['sort']\n                sort_reversed = True if sort_key =='reversed' else False\n                if sort_key", "solution": "\n    current = request.GET.get(sort_key)\n    current_reversed = False\n    current_param = None\n    if current:\n        mo = re.match(r'^(-?)(\\w+)$', current)    # exclude first \"-\" (if exist)\n        if mo:\n            current_reversed = mo.group(1) == '-'\n            current_param = mo.group(2)\n\n    return current_param, current_reversed"}
{"prompt": "def emitCurrentRecordChanged(self, item):\r\n        \"\"\"\r\n        Emits the record changed signal for the given item, provided the\r\n        signals are not currently blocked.\r\n        \r\n        :param      item | <QTreeWidgetItem>\r\n        \"\"\"", "output": "\r\n        self.emit(SIGNAL(\"recordChanged(QTreeWidgetItem *, QTreeWidgetItem *)\"), item)\r\n    \r\n    def emitCurrentRecordChanged(self, item):\r\n        \"\"\"\r\n        Emits the record changed signal for the given item, provided the\r\n        signals are not currently blocked.\r\n        \r\n        :param      item | <QTreeWidgetItem>\r\n        \"\"\"\r\n        self.emit(SIGNAL(\"recordChanged(QTreeWidgetItem *, QTreeWidgetItem *)\"), item)\r\n    \r\n    def emitCurrentRecord(self, item):\r\n        \"\"\"\r\n        Emits the current record signal for the given item.\r\n        \r\n        :param      item | <QTreeWidgetItem>\r\n        \"\"\"", "solution": "\r\n        if self.signalsBlocked():\r\n            return\r\n        \r\n        # emit that the record has been clicked\r\n        if isinstance(item, XOrbRecordItem):\r\n            self.currentRecordChanged.emit(item.record())\r\n        else:\r\n            self.currentRecordChanged.emit(None)"}
{"prompt": "def add(self, model):\n        \"\"\"Add a index method.\n\n        This method is used to add index algorithms. If multiple algorithms\n        are added, the union of the record pairs from the algorithm is taken.\n\n        Parameters\n        ----------\n        model : list, class\n            A (list of) index algorithm(s) from\n            :mod:`recordlinkage.index`.\n        \"\"\"", "output": "\n        if len(model) == 1:\n            model = recordlinkage.index(model[0])\n        else:\n            model = recordlinkage.index(model[0])\n\n        if len(model) == 0:\n            raise ValueError('No index method found for model {0}'.format(model))\n\n        if len(model) == 1:\n            model = recordlinkage.index(model[0])\n        else:\n            model = recordlinkage.index(model[0])\n\n        if len(model) == 0:\n            raise ValueError('No index method found for model {0}'.format(model))\n\n        self.index", "solution": "\n        if isinstance(model, list):\n            self.algorithms = self.algorithms + model\n        else:\n            self.algorithms.append(model)"}
{"prompt": "def get_pwhash_bits(params):\n    \"\"\" Extract bits for password hash validation from params. \"\"\"", "output": "\n    if params['password'] is not None:\n        return params['password'], params['password_hash']\n\n    return None, None\n\n\ndef get_pwhash_context(params):\n    \"\"\" Extracts the pwhash context from params. \"\"\"\n    if params['password'] is not None:\n        return params['password_context']\n\n    return None, None\n\n\ndef get_pwhash_context_for_password(params):\n    \"\"\" Extracts the pwhash context from params. \"\"\"\n    if params['password'] is not None:\n        return params['password_context'], params['password_", "solution": "\n    if not \"pwhash\" in params or \\\n            not \"nonce\" in params or \\\n            not \"aead\" in params or \\\n            not \"kh\" in params:\n        raise Exception(\"Missing required parameter in request (pwhash, nonce, aead or kh)\")\n    pwhash = params[\"pwhash\"][0]\n    nonce = params[\"nonce\"][0]\n    aead = params[\"aead\"][0]\n    key_handle = pyhsm.util.key_handle_to_int(params[\"kh\"][0])\n    return pwhash, nonce, aead, key_handle"}
{"prompt": "def temporal_efficiency(tnet=None, paths=None, calc='global'):\n    r\"\"\"\n    Returns temporal efficiency estimate. BU networks only.\n\n    Parameters\n    ----------\n    Input should be *either* tnet or paths.\n\n    data : array or dict\n\n        Temporal network input (graphlet or contact). nettype: 'bu', 'bd'.\n\n    paths : pandas dataframe\n\n        Output of TenetoBIDS.networkmeasure.shortest_temporal_paths\n\n    calc : str\n        Options: 'global' (default) - measure averages over time and nodes;\n        'node' or 'node_from' average over nodes (i) and time. Giving average efficiency for i to j;\n        'node_to' measure average over nodes j and time;\n         Giving average efficiency using paths to j from  i;\n\n    Returns\n    -------\n\n    E : array\n        Global temporal efficiency\n\n    \"\"\"", "output": "_avg' - node averages over time and nodes;\n        'paths' - path averages over time and nodes;\n\n    Returns\n    -------\n    float\n        The average of the shortest paths.\n\n    Notes\n    -----\n    The averages are calculated over the time of the network.\n    \"\"\"\n    if tnet is None and paths is None and calc == 'node':\n        return node_avg(paths=paths, data=data)\n    elif tnet is None and paths is None and calc == 'node_avg':\n        return node_avg(data=data)\n    else:\n        raise ValueError(\"Invalid value for 't", "solution": "\n\n    if tnet is not None and paths is not None:\n        raise ValueError('Only network or path input allowed.')\n    if tnet is None and paths is None:\n        raise ValueError('No input.')\n    # if shortest paths are not calculated, calculate them\n    if tnet is not None:\n        paths = shortest_temporal_path(tnet)\n\n    pathmat = np.zeros([paths[['from', 'to']].max().max(\n    )+1, paths[['from', 'to']].max().max()+1, paths[['t_start']].max().max()+1]) * np.nan\n    pathmat[paths['from'].values, paths['to'].values,\n            paths['t_start'].values] = paths['temporal-distance']\n\n    # Calculate efficiency which is 1 over the mean path.\n    if calc == 'global':\n        eff = 1 / np.nanmean(pathmat)\n    elif calc == 'node' or calc == 'node_from':\n        eff = 1 / np.nanmean(np.nanmean(pathmat, axis=2), axis=1)\n    elif calc == 'node_to':\n        eff = 1 / np.nanmean(np.nanmean(pathmat, axis=2), axis=0)\n\n    return eff"}
{"prompt": "def generate_association_rules(patterns, confidence_threshold):\n    \"\"\"\n    Given a set of frequent itemsets, return a dict\n    of association rules in the form\n    {(left): ((right), confidence)}\n    \"\"\"", "output": "\n    result = {}\n    for p in patterns:\n        result[p] = []\n        for i in range(1, len(p)+1):\n            if i == 0:\n                result[p[0]].append((p[i], 0))\n            else:\n                result[p[0]].append((p[i], confidence_threshold*i))\n                result[p[i]].append((p[i-1], confidence_threshold*i))\n            if p[i] == 0:\n                result[p[i]].append((p[i], 0))\n            else:\n                result[p[i", "solution": "\n    rules = {}\n    for itemset in patterns.keys():\n        upper_support = patterns[itemset]\n\n        for i in range(1, len(itemset)):\n            for antecedent in itertools.combinations(itemset, i):\n                antecedent = tuple(sorted(antecedent))\n                consequent = tuple(sorted(set(itemset) - set(antecedent)))\n\n                if antecedent in patterns:\n                    lower_support = patterns[antecedent]\n                    confidence = float(upper_support) / lower_support\n\n                    if confidence >= confidence_threshold:\n                        rules[antecedent] = (consequent, confidence)\n\n    return rules"}
{"prompt": "def _read_generic_csv(file):\n        \"\"\" Read and parse generic CSV file.\n\n        Notes\n        -----\n        Assumes columns are 'rsid', 'chrom' / 'chromosome', 'pos' / 'position', and 'genotype';\n        values are comma separated; unreported genotypes are indicated by '--'; and one header row\n        precedes data. For example:\n\n            rsid,chromosome,position,genotype\n            rs1,1,1,AA\n            rs2,1,2,CC\n            rs3,1,3,--\n\n        Parameters\n        ----------\n        file : str\n            path to file\n\n        Returns\n        -------\n        pandas.DataFrame\n            individual's genetic data normalized for use with `lineage`\n        str\n            name of data source\n        \"\"\"", "output": "\n        \"\"\"\n        with open(file, 'r') as f:\n            rsid, chromosome, pos, header = next(f)\n            if header.startswith('#'):\n                header = header[1:]\n            header = header.split()\n            pos = int(pos)\n            if rsid == '':\n                raise ValueError('No RSID found in file %s' % file)\n            rsid = int(rsid)\n            if chromosome == '':\n                raise ValueError('No chromosome found in file %s' % file)\n            chromosome = int(chromosome)\n            if pos == 0:\n                raise ValueError", "solution": "\n        df = pd.read_csv(\n            file,\n            skiprows=1,\n            na_values=\"--\",\n            names=[\"rsid\", \"chrom\", \"pos\", \"genotype\"],\n            index_col=0,\n            dtype={\"chrom\": object, \"pos\": np.int64},\n        )\n\n        return sort_snps(df), \"generic\""}
{"prompt": "def weird_log_graph(self, stream):\n        \"\"\" Build up a graph (nodes and edges from a Bro weird.log) \"\"\"", "output": "\n        for i, node in enumerate(self.nodes):\n            for j, edge in enumerate(node.edges):\n                if edge.source == 'Bro':\n                    if i == 0:\n                        self.graph.add_edge(node, edge)\n                    else:\n                        self.graph.add_edge(node, edge, weight=1)\n        return self.graph\n\n    def get_node_attributes(self, node):\n        \"\"\" Build up the attributes for a node \"\"\"\n        for attr in node.attributes:\n            if attr.name =='source':\n                return attr.value\n            elif attr.name ==", "solution": "\n        weird_log = list(stream)\n        print 'Entering weird_log_graph...(%d rows)' % len(weird_log)\n\n        # Here we're just going to capture that something weird\n        # happened between two hosts\n        weird_pairs = set()\n        for row in weird_log:\n            weird_pairs.add((row['id.orig_h'], row['id.resp_h']))\n\n        # Okay now make the weird node for each pair\n        for pair in weird_pairs:\n\n            # Skip '-' hosts\n            if (pair[0] == '-'):\n                continue\n\n            # Add the originating host\n            self.add_node(pair[0], pair[0], ['host', 'origin'])\n\n            # Add the response host\n            self.add_node(pair[1], pair[1], ['host'])\n\n            # Add a weird node\n            weird_name = 'weird'+pair[0]+'_'+pair[1]\n            self.add_node(weird_name, 'weird', ['weird'])\n\n            # The relationships between the nodes\n            self.add_rel(pair[0], weird_name, 'weird')\n            self.add_rel(weird_name, pair[1], 'weird')"}
{"prompt": "def block_sep1(self, Y):\n        r\"\"\"Separate variable into component corresponding to\n        :math:`\\mathbf{y}_1` in :math:`\\mathbf{y}\\;\\;`.\n        \"\"\"", "output": "\n        return self.component(Y)\n\n    def block_sep2(self, Y):\n        r\"\"\"Separate variable into component corresponding to\n        :math:`\\mathbf{y}_2` in :math:`\\mathbf{y}\\;\\;`.\n        \"\"\"\n        return self.component(Y)\n\n    def block_sep3(self, Y):\n        r\"\"\"Separate variable into component corresponding to\n        :math:`\\mathbf{y}_3` in :math:`\\mathbf{y}\\;\\;`.\n        \"\"\"\n        return self.component(Y)\n\n    def block_sep4(self, Y):\n       ", "solution": "\n\n        # This method is overridden because we have to change the\n        # mechanism for combining the Y0 and Y1 blocks into a single\n        # array (see comment in the __init__ method).\n        shp = Y.shape[0:self.cri.axisC] + self.y1shp[self.cri.axisC:]\n        return Y[(slice(None),)*self.cri.axisC +\n                 (slice(self.y0I, None),)].reshape(shp)"}
{"prompt": "def fingerprint(blob):\n    \"\"\"\n    Compute SSH fingerprint for specified blob.\n\n    See https://en.wikipedia.org/wiki/Public_key_fingerprint for details.\n    \"\"\"", "output": "\n    return hashlib.sha1(blob.encode()).hexdigest()\n\n\ndef get_ssh_fingerprint(key_id):\n    \"\"\"\n    Compute SSH fingerprint for specified key_id.\n\n    See https://en.wikipedia.org/wiki/Public_key_fingerprint for details.\n    \"\"\"\n    return get_ssh_fingerprint(key_id).replace(':', '')\n\n\ndef get_ssh_public_key(key_id):\n    \"\"\"\n    Get SSH public key for specified key_id.\n\n    See https://en.wikipedia.org/wiki/RSA_public_key for details.\n    \"\"\"\n    return get", "solution": "\n    digest = hashlib.md5(blob).digest()\n    return ':'.join('{:02x}'.format(c) for c in bytearray(digest))"}
{"prompt": "def getskyimg(self,chip):\n        \"\"\"\n        Notes\n        =====\n        Return an array representing the sky image for the detector.  The value\n        of the sky is what would actually be subtracted from the exposure by\n        the skysub step.\n\n        :units: electrons\n\n        \"\"\"", "output": "\n        return self.sky_array[chip]\n\n    def get_sky_array(self):\n        \"\"\"\n        Returns the list of sky arrays for the detector.\n\n        :return: list of sky arrays\n\n        \"\"\"\n        return self.sky_array\n\n    def get_exposure_array(self):\n        \"\"\"\n        Notes\n        =====\n        Return an array representing the exposure image for the detector.\n        The value of the exposure is what would actually be subtracted from the exposure\n        by the skysub step.\n\n        :return: list of exposure arrays\n\n        \"\"\"\n        return self.exposure_array\n\n    def get_exposure_", "solution": "\n        sci_chip = self._image[self.scienceExt,chip]\n        skyimg = np.ones(sci_chip.image_shape,dtype=sci_chip.image_dtype)*sci_chip.subtractedSky\n        if sci_chip._conversionFactor != 1.0: # If units are not already ELECTRONS\n            skyimg *= self.getexptimeimg(chip)\n        return skyimg"}
{"prompt": "def update_view_data(self, properties=None):\n        \"\"\"Update the local object from the server-side object.\n        \n        >>> vm = VirtualMachine.find_one(client, filter={\"name\": \"genesis\"})\n        >>> # Update all properties\n        >>> vm.update_view_data()\n        >>> # Update the config and summary properties\n        >>> vm.update_view_data(properties=[\"config\", \"summary\"]\n\n        :param properties: A list of properties to update.\n        :type properties: list\n\n        \"\"\"", "output": "\n        if properties is None:\n            properties = self.default_view_properties\n        if not properties:\n            return\n        vm = self.get_vm()\n        for prop in properties:\n            if prop == \"config\":\n                vm.config = vm.config.to_dict()\n            elif prop == \"summary\":\n                vm.summary = vm.summary.to_dict()\n            else:\n                raise ValueError(\"Unknown property %s\" % prop)\n\n    def update_view_properties(self, properties=None):\n        \"\"\"Update the local object from the server-side object.\n        \n        >>> # Update all properties\n        >>>", "solution": "\n        if properties is None:\n            properties = []\n        logger.info(\"Updating view data for object of type %s\",\n                    self._mo_ref._type)\n        property_spec = self._client.create('PropertySpec')\n        property_spec.type = str(self._mo_ref._type)\n        # Determine which properties to retrieve from the server\n        if properties is None:\n            properties = []\n        else:\n            if properties == \"all\":\n                logger.debug(\"Retrieving all properties\")\n                property_spec.all = True\n            else:\n                logger.debug(\"Retrieving %s properties\", len(properties))\n                property_spec.all = False\n                property_spec.pathSet = properties\n\n        object_spec = self._client.create('ObjectSpec')\n        object_spec.obj = self._mo_ref\n\n        pfs = self._client.create('PropertyFilterSpec')\n        pfs.propSet = [property_spec]\n        pfs.objectSet = [object_spec]\n\n        # Create a copy of the property collector and call the method\n        pc = self._client.sc.propertyCollector\n        object_content = pc.RetrieveProperties(specSet=pfs)[0]\n        if not object_content:\n            # TODO: Improve error checking and reporting\n            logger.error(\"Nothing returned from RetrieveProperties!\")\n\n        self._set_view_data(object_content)"}
{"prompt": "def emap(self, func):\n        \"\"\"\n        :param func:\n        :type func: T, int -> U\n        :rtype: TList[U]\n\n        Usage:\n\n            >>> TList([10, 20, 30, 40, 50]).emap(lambda x, i: (x+1, i))\n            [(11, 0), (21, 1), (31, 2), (41, 3), (51, 4)]\n        \"\"\"", "output": "\n        if func == 0:\n            return [self.head]\n        elif func == 1:\n            return [self.tail]\n        elif func == 2:\n            return [self.tail]\n        elif func == 3:\n            return [self.tail]\n        elif func == 4:\n            return [self.tail]\n        elif func == 5:\n            return [self.tail]\n        elif func == 6:\n            return [self.tail]\n        elif func == 7:\n            return [self.tail]\n        elif func == 8:\n            return [self.tail]\n        elif func == 9:\n            return [self.", "solution": "\n        return TList([func(x, i) for i, x in enumerate(self)])"}
{"prompt": "def select_unrectified_slitlet(image2d, islitlet, csu_bar_slit_center,\n                               params, parmodel, maskonly):\n    \"\"\"Returns image with the indicated slitlet (zero anywhere else).\n\n    Parameters\n    ----------\n    image2d : numpy array\n        Initial image from which the slitlet data will be extracted.\n    islitlet : int\n        Slitlet number.\n    csu_bar_slit_center : float\n        CSU bar slit center.\n    params : :class:`~lmfit.parameter.Parameters`\n        Parameters to be employed in the prediction of the distorted\n        boundaries.\n    parmodel : str\n        Model to be assumed. Allowed values are 'longslit' and\n        'multislit'.\n    maskonly : bool\n        If True, returns simply a mask (1 in the slitlet region and\n        zero anywhere else.\n\n    Returns\n    -------\n    image2d_output : numpy array\n        2D image with the pixel information corresponding to the\n        selected slitlet and zero everywhere else.\n\n    \"\"\"", "output": "loyed.\n    parmodel : :class:`~lmfit.parameter.Parameters`\n        Model parameters.\n    maskonly : bool\n        Mask the image with the indicated island (True) or island (False).\n\n    Returns\n    -------\n    :class:`~numpy.ndarray`\n        Image with the indicated island (1) or island (0)\n    \"\"\"\n    if maskonly:\n        return image2d\n    else:\n        return image2d[maskonly, :]\n\n\ndef select_island(image, island):\n    \"\"\"Returns image with the indicated island (1 anywhere else).\n\n    Parameters\n    ----------", "solution": "\n\n    # protection\n    if image2d.shape != (EMIR_NAXIS2, EMIR_NAXIS1):\n        raise ValueError(\"NAXIS1, NAXIS2 unexpected for EMIR detector\")\n\n    # initialize image output\n    image2d_output = np.zeros_like(image2d)\n\n    # expected slitlet frontiers\n    list_expected_frontiers = expected_distorted_frontiers(\n        islitlet, csu_bar_slit_center,\n        params, parmodel, numpts=101, deg=5, debugplot=0\n    )\n    pol_lower_expected = list_expected_frontiers[0].poly_funct\n    pol_upper_expected = list_expected_frontiers[1].poly_funct\n\n    # main loop: compute for each channel the minimum and maximum scan\n    for j in range(EMIR_NAXIS1):\n        xchannel = j + 1\n        y0_lower = pol_lower_expected(xchannel)\n        y0_upper = pol_upper_expected(xchannel)\n        n1, n2 = nscan_minmax_frontiers(y0_frontier_lower=y0_lower,\n                                        y0_frontier_upper=y0_upper,\n                                        resize=True)\n        # note that n1 and n2 are scans (ranging from 1 to NAXIS2)\n        if maskonly:\n            image2d_output[(n1 - 1):n2, j] = np.repeat(\n                [1.0], (n2 - n1 + 1)\n            )\n        else:\n            image2d_output[(n1 - 1):n2, j] = image2d[(n1 - 1):n2, j]\n\n    return image2d_output"}
{"prompt": "def endswith(self, suffix, start=0, end=None):\n        \"\"\"Return True if ends with the specified suffix, False otherwise.\n\n        With optional start, test beginning at that position. With optional end, stop comparing at that position.\n        suffix can also be a tuple of strings to try.\n\n        :param str suffix: Suffix to search.\n        :param int start: Beginning position.\n        :param int end: Stop comparison at this position.\n        \"\"\"", "output": "\n        if end is not None:\n            if end[0] > start[0]:\n                return False\n            if end[1] > start[1]:\n                return False\n        if not suffix[0]:\n            return False\n        if not suffix[1]:\n            return True\n        if not start[0]:\n            return True\n        if not end[0]:\n            return False\n        if not start[1]:\n            return True\n        if not end[1]:\n            return False\n        if not start[0]:\n            return True\n        if end[0]:\n            return False\n        return self.find(suffix[0], start, end)", "solution": "\n        args = [suffix, start] + ([] if end is None else [end])\n        return self.value_no_colors.endswith(*args)"}
{"prompt": "def _base_placeholder(self):\n        \"\"\"\n        Return the layout placeholder this slide placeholder inherits from.\n        Not to be confused with an instance of |BasePlaceholder|\n        (necessarily).\n        \"\"\"", "output": "\n        return self._placeholder\n\n    def _get_slide_layout(self):\n        \"\"\"\n        Return a |SlideLayout| object that represents this slide.\n        \"\"\"\n        return self._slide_layout\n\n    def _set_slide_layout(self, value):\n        \"\"\"\n        Set the SlideLayout object this slide placeholder inherits from.\n        Not to be confused with an instance of |SlideLayout|\n        (necessarily).\n        \"\"\"\n        self._slide_layout = value\n\n    slide_layout = property(_get_slide_layout, _set_slide_layout)\n    \"\"\"\n    The |SlideLayout|", "solution": "\n        layout, idx = self.part.slide_layout, self._element.ph_idx\n        return layout.placeholders.get(idx=idx)"}
{"prompt": "def read_validating_webhook_configuration(self, name, **kwargs):\n        \"\"\"\n        read the specified ValidatingWebhookConfiguration\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_validating_webhook_configuration(name, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the ValidatingWebhookConfiguration (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param bool exact: Should the export be exact.  Exact export maintains cluster-specific fields like 'Namespace'. Deprecated. Planned for removal in 1.18.\n        :param bool export: Should this value be exported.  Export strips fields that a user can not specify. Deprecated. Planned for removal in 1.18.\n        :return: V1beta1ValidatingWebhookConfiguration\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"", "output": "param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.\n        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.\n        :return: ValidatingWebhookConfiguration\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n\n        all_params = ['name', 'pretty', 'label_selector', 'field_selector', 'async_req']\n        all_params.append('async_req')\n        all_params.append('_return_http_data_only')\n\n        params =", "solution": "\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_validating_webhook_configuration_with_http_info(name, **kwargs)\n        else:\n            (data) = self.read_validating_webhook_configuration_with_http_info(name, **kwargs)\n            return data"}
{"prompt": "def create_int_or_none(help_string=NO_HELP, default=NO_DEFAULT):\n        # type: (str, Union[int, None, NO_DEFAULT_TYPE]) -> Union[int, None]\n        \"\"\"\n        Create an int parameter\n        :param help_string:\n        :param default:\n        :return:\n        \"\"\"", "output": "\n        if not help_string:\n            return None\n        return int(help_string)\n\n    def create_bool_or_none(help_string=NO_HELP, default=NO_DEFAULT):\n        # type: (str, Union[bool, None, NO_DEFAULT_TYPE]) -> Union[bool, None]\n        \"\"\"\n        Create an bool parameter\n        :param help_string:\n        :param default:\n        :return:\n        \"\"\"\n        if not help_string:\n            return None\n        return bool(help_string)\n\n    def create_float_or_none(help_string=NO_HELP", "solution": "\n        # noinspection PyTypeChecker\n        return ParamFunctions(\n            help_string=help_string,\n            default=default,\n            type_name=\"Union[int, None]\",\n            function_s2t=convert_string_to_int_or_none,\n            function_t2s=convert_int_or_none_to_string,\n        )"}
{"prompt": "def angle_between(v1, v2):\n    \"\"\"Returns the angle in radians between vectors 'v1' and 'v2'.\n\n    >>> angle_between((1, 0, 0), (0, 1, 0))\n    1.5707963267948966\n    >>> angle_between((1, 0, 0), (1, 0, 0))\n    0.0\n    >>> angle_between((1, 0, 0), (-1, 0, 0))\n    3.141592653589793\n\n    \"\"\"", "output": "\n    return math.acos(v1 * v2)\n\ndef angle_between_vectors(v1, v2):\n    \"\"\"Returns the angle in radians between vectors 'v1' and 'v2'.\n\n    >>> angle_between_vectors((1, 0, 0), (0, 1, 0))\n    1.5707963267948966\n    >>> angle_between_vectors((1, 0, 0), (1, 0, 0))\n    0.0\n    >>> angle_between_vectors((1, 0, 0), (0, 1, 0))\n    1.57", "solution": "\n    v1_u = unit_vector(v1)\n    v2_u = unit_vector(v2)\n    # Don't use `np.dot`, does not work with all shapes\n    angle = np.arccos(np.inner(v1_u, v2_u))\n    return angle"}
{"prompt": "def recycle(self):\n        \"\"\"Clean up and return connections back to the pool.\n\n        Connections should be kept alive if supported.\n        \"\"\"", "output": "\n        if self.use_greenlets:\n            self.greenlets.remove(self)\n            return self.greenlets\n        else:\n            return self.pool.connections_count()\n\n    def _recycle_greenlets(self):\n        \"\"\"Clean up greenlets.\n\n        This method should be called when the greenlet pool is recycled.\n        \"\"\"\n        if self.use_greenlets:\n            self.greenlets.remove(self)\n            return self.greenlets\n        else:\n            return self.pool.greenlet_connections_count()\n\n    def _recycle_greenlets_pool(self", "solution": "\n\n        for connection in self._connections:\n            self._connection_pool.no_wait_release(connection)\n\n        self._connections.clear()"}
{"prompt": "def analog_units(self):\r\n        \"\"\"\r\n        Shortcut to retrieve all analog points units [Used by Bokeh trending feature]\r\n        \"\"\"", "output": "\r\n        return self._analog_units\r\n\r\n    @property\r\n    def analog_units_str(self):\r\n        \"\"\"\r\n        Shortcut to retrieve all analog points units [Used by Bokeh trending feature]\r\n        \"\"\"\r\n        return self._analog_units_str\r\n\r\n    @property\r\n    def analog_units_int(self):\r\n        \"\"\"\r\n        Shortcut to retrieve the number of analog points units [Used by Bokeh trending feature]\r\n        \"\"\"\r\n        return self._analog_units_int\r\n\r\n    @property\r\n    def analog_units_float(self):\r\n        \"\"\"\r\n        Shortcut to retrieve the analog points", "solution": "\r\n        au = []\r\n        us = []\r\n        for each in self.points:\r\n            if isinstance(each, NumericPoint):\r\n                au.append(each.properties.name)\r\n                us.append(each.properties.units_state)\r\n        return dict(zip(au, us))"}
{"prompt": "def GetEntries(self, parser_mediator, match=None, **unused_kwargs):\n    \"\"\"Extracts relevant Airport entries.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      match (Optional[dict[str: object]]): keys extracted from PLIST_KEYS.\n    \"\"\"", "output": "\n    for entry in self.entries:\n      if 'airport' in entry:\n        return entry['airport']\n    return None\n\n  def GetEntriesWithin(self, match=None, **unused_kwargs):\n    \"\"\"Extracts relevant Airport entries with interactions.\n\n    Args:\n      match (Optional[dict[str: object]]): keys extracted from PLIST_KEYS.\n      **unused_kwargs: unused arguments.\n\n    Returns:\n      list[dict]: list of Airport entries with interactions between\n              parsers and other components.\n    \"\"\"\n    for entry in self.entries:\n      if 'interactions'", "solution": "\n    if 'RememberedNetworks' not in match:\n      return\n\n    for wifi in match['RememberedNetworks']:\n      ssid = wifi.get('SSIDString', 'UNKNOWN_SSID')\n      security_type = wifi.get('SecurityType', 'UNKNOWN_SECURITY_TYPE')\n\n      event_data = plist_event.PlistTimeEventData()\n      event_data.desc = (\n          '[WiFi] Connected to network: <{0:s}> using security {1:s}').format(\n              ssid, security_type)\n      event_data.key = 'item'\n      event_data.root = '/RememberedNetworks'\n\n      datetime_value = wifi.get('LastConnected', None)\n      if datetime_value:\n        event = time_events.PythonDatetimeEvent(\n            datetime_value, definitions.TIME_DESCRIPTION_WRITTEN)\n\n      else:\n        date_time = dfdatetime_semantic_time.SemanticTime('Not set')\n        event = time_events.DateTimeValuesEvent(\n            date_time, definitions.TIME_DESCRIPTION_NOT_A_TIME)\n\n      parser_mediator.ProduceEventWithEventData(event, event_data)"}
{"prompt": "def gft(self, s):\n        r\"\"\"Compute the graph Fourier transform.\n\n        The graph Fourier transform of a signal :math:`s` is defined as\n\n        .. math:: \\hat{s} = U^* s,\n\n        where :math:`U` is the Fourier basis attr:`U` and :math:`U^*` denotes\n        the conjugate transpose or Hermitian transpose of :math:`U`.\n\n        Parameters\n        ----------\n        s : array_like\n            Graph signal in the vertex domain.\n\n        Returns\n        -------\n        s_hat : ndarray\n            Representation of s in the Fourier domain.\n\n        Examples\n        --------\n        >>> G = graphs.Logo()\n        >>> G.compute_fourier_basis()\n        >>> s = np.random.normal(size=(G.N, 5, 1))\n        >>> s_hat = G.gft(s)\n        >>> s_star = G.igft(s_hat)\n        >>> np.all((s - s_star) < 1e-10)\n        True\n\n        \"\"\"", "output": ".\n\n        Notes\n        -----\n        The Fourier transform of a tensor :math:`A` is defined as\n\n       .. math:: \\hat{A} = U^T A,\n\n        where :math:`U^T` is the conjugate transpose or Hermitian transpose of\n        :math:`U`.\n\n        The tensor :math:`A` is the tensor that is to be transformed.\n        The tensor :math:`A` is the tensor that is to be inverted.\n        The tensor :math:`U` is the tensor that is to be used to transform the\n        tensor.\n\n        The tensor :math:`A` is the tensor that", "solution": "\n        s = self._check_signal(s)\n        U = np.conjugate(self.U)  # True Hermitian. (Although U is often real.)\n        return np.tensordot(U, s, ([0], [0]))"}
{"prompt": "def get_datetext(year, month, day):\n    \"\"\"year=2005, month=11, day=16 => '2005-11-16 00:00:00'\"\"\"", "output": "\n    return datetext_from_string(datetext_from_date(year, month, day))\n\n\ndef get_datetext_from_string(datetext):\n    \"\"\"datetext=2005, day=16 => '2005-11-16 00:00:00'\"\"\"\n    return datetext_from_date(datetext)\n\n\ndef get_datetext_from_date(datetext):\n    \"\"\"datetext=2005, day=16 => '2005-11-16 16:16:16'\"\"\"\n    return datetext_from_date_string(datetext_from_string(", "solution": "\n    input_format = \"%Y-%m-%d\"\n    try:\n        datestruct = time.strptime(\"%i-%i-%i\" % (year, month, day),\n                                   input_format)\n        return strftime(datetext_format, datestruct)\n    except:\n        return datetext_default"}
{"prompt": "def load(self, fobj, index=None):\r\n        \"\"\"\r\n        Loads given DataFile object. **tolerant with None**\r\n\r\n        Args:\r\n            fobj: object of one of accepted classes\r\n            index: tab index to load fobj into. If not passed, loads into current tab\r\n        \"\"\"", "output": "\r\n        if index is None:\r\n            index = self.tab_index\r\n        if index is None:\r\n            index = self.tab_index + 1\r\n        if fobj.is_binary:\r\n            self.read_binary(fobj)\r\n        elif fobj.is_text:\r\n            self.read_text(fobj)\r\n        elif fobj.is_binary:\r\n            self.read_binary(fobj)\r\n        elif fobj.is_text:\r\n            self.read_text(fobj)\r\n        elif fobj.is_text:\r\n            self.read_text(fobj)\r\n        elif fobj.is_text:\r\n            self.read_", "solution": "\r\n        if index is None:\r\n            index = self._get_tab_index()\r\n        page = self.pages[index]\r\n\r\n        if fobj is None:\r\n            return\r\n\r\n\r\n        if not isinstance(fobj, tuple(page.clss_load)):\r\n            raise RuntimeError('Object to load must be in {0!s} (not a {1!s})'.format(\r\n             [x.__name__ for x in page.clss_load], fobj.__class__.__name__))\r\n\r\n        page.editor.load(fobj)\r\n        self._update_gui_text_tabs()"}
{"prompt": "def install_host(trg_queue, *hosts, **kwargs):\n    \"\"\" Atomically install host queues \"\"\"", "output": "\n    for host in hosts:\n        if host not in trg_queue:\n            trg_queue[host] = {}\n        if not host in trg_queue[host]:\n            trg_queue[host][host] = {}\n        if not host in trg_queue[host][host_name]:\n            trg_queue[host][host_name][host] = {}\n        if not host_name in trg_queue[host]:\n            trg_queue[host][host_name][host] = {}\n        if not host_name_short in trg_queue[host]:", "solution": "\n    user = kwargs.pop('user', None)\n    group = kwargs.pop('group', None)\n    mode = kwargs.pop('mode', None)\n    item_user = kwargs.pop('item_user', None)\n    item_group = kwargs.pop('item_group', None)\n    item_mode = kwargs.pop('item_mode', None)\n    is_down = kwargs.pop('is_down', False)\n\n    #set modes\n    mode, user, group, item_user, item_group, item_mode =\\\n        _def_mode(mode, user, group, item_user, item_group, item_mode)\n    uid, gid = uid_gid(user, group)\n    host_path = fsq_path.hosts(trg_queue)\n    try:\n        _instdir(host_path, mode, uid, gid)\n    except (OSError, IOError, ), e:\n        if e.errno not in ( errno.EEXIST, errno.ENOTEMPTY, ):\n            raise FSQInstallError(e.errno, wrap_io_os_err(e))\n    if hosts:\n        for host in hosts:\n            host = fsq_path.valid_name(host)\n            # uid_gid makes calls to the pw db and|or gr db, in addition to\n            # potentially stat'ing, as such, we want to avoid calling it\n            # unless we absoultely have to\n            uid, gid = uid_gid(user, group)\n            tmp_full, tmp_queue = _tmp_trg(host, host_path)\n            try:\n                # open once to cut down on stat/open for chown/chmod combo\n                fd = os.open(tmp_full, os.O_RDONLY)\n                try:\n                    # always fchmod here as mkdtemp is different than normal\n                    # mkdir\n                    os.fchmod(fd, mode)\n                    if -1 != uid or -1 != gid:\n                        os.fchown(fd, uid, gid)\n                finally:\n                    os.close(fd)\n\n                # bless our queue with its children\n                _instdir(fsq_path.tmp(trg_queue, tmp_queue), mode, uid, gid)\n                _instdir(fsq_path.queue(trg_queue, tmp_queue), mode, uid, gid)\n                _instdir(fsq_path.done(trg_queue, tmp_queue), mode, uid, gid)\n                _instdir(fsq_path.fail(trg_queue, tmp_queue), mode, uid, gid)\n\n                # down via configure.down if necessary\n                if is_down:\n                    down_host(tmp_queue, host, user=item_user,\n                              group=item_group, mode=item_mode)\n\n                # atomic commit -- by rename\n                os.rename(tmp_full, fsq_path.base(trg_queue, host))\n            except (OSError, IOError, ), e:\n                shutil.rmtree(tmp_full)\n                if e.errno == errno.ENOTEMPTY:\n                    raise FSQInstallError(e.errno, u'queue exists: {0}'.format(\n                                          trg_queue))\n                if isinstance(e, FSQError):\n                    raise e"}
{"prompt": "def register(cls, encryptor: Encryptor):\n        \"\"\"\n        Register this encryptable with an encryptor.\n\n        Instances of this encryptor will be encrypted on initialization and decrypted on load.\n\n        \"\"\"", "output": "\n        cls.encryptor = encryptor\n\n    @classmethod\n    def _get_key_handle(cls, key_handle: KeyHandle) -> KeyHandle:\n        \"\"\"\n        Get a key handle for this encryptable.\n\n        :param key_handle: The key handle to be used.\n        :return: The key handle.\n        \"\"\"\n        return key_handle\n\n    @classmethod\n    def _get_key_handle_and_iv(cls, key_handle: KeyHandle, iv: int) -> KeyHandle:\n        \"\"\"\n        Get a key handle and iv for this encryptable.\n\n        :param key_handle:", "solution": "\n        # save the current encryptor statically\n        cls.__encryptor__ = encryptor\n\n        # NB: we cannot use the before_insert listener in conjunction with a foreign key relationship\n        # for encrypted data; SQLAlchemy will warn about using 'related attribute set' operation so\n        # late in its insert/flush process.\n        listeners = dict(\n            init=on_init,\n            load=on_load,\n        )\n\n        for name, func in listeners.items():\n            # If we initialize the graph multiple times (as in many unit testing scenarios),\n            # we will accumulate listener functions -- with unpredictable results. As protection,\n            # we need to remove existing listeners before adding new ones; this solution only\n            # works if the id (e.g. memory address) of the listener does not change, which means\n            # they cannot be closures around the `encryptor` reference.\n            #\n            # Hence the `__encryptor__` hack above...\n            if contains(cls, name, func):\n                remove(cls, name, func)\n            listen(cls, name, func)"}
{"prompt": "def vec2ang(vectors, lonlat=False):\n    \"\"\"Drop-in replacement for healpy `~healpy.pixelfunc.vec2ang`.\"\"\"", "output": "\n    if lonlat:\n        return np.deg2rad(np.arctan2(*vectors[::-1]))\n    else:\n        return np.rad2deg(np.arctan2(*vectors[::-1]))\n\ndef vec2vec(vectors, lonlat=False):\n    \"\"\"Drop-in replacement for healpy `~healpy.pixelfunc.vec2vec`.\"\"\"\n    if lonlat:\n        return np.deg2rad(np.arctan2(*vectors[::-1]))\n    else:\n        return np.rad2deg(np.arctan2(*vectors[::-1]))\n\ndef vec2vec", "solution": "\n    x, y, z = vectors.transpose()\n    rep_car = CartesianRepresentation(x, y, z)\n    rep_sph = rep_car.represent_as(UnitSphericalRepresentation)\n    return _lonlat_to_healpy(rep_sph.lon.ravel(), rep_sph.lat.ravel(), lonlat=lonlat)"}
{"prompt": "def from_dict(cls, **kwargs):\n        \"\"\"\n        Construct an object an mass-assign its attributes using +kwargs+,\n        ignoring all protections of id and created_at. Intended for\n        constructing objects already present in the database (i.e for use by\n        methods such as find or within Query).\n        \"\"\"", "output": "\n        if cls._meta.abstract:\n            return cls.abstract\n        if cls._meta.ordering:\n            return cls.query.order_by(*cls._meta.ordering)\n        if cls._meta.ordering_aliases:\n            return cls.query.order_by(*cls.ordering_aliases)\n        if cls._meta.ordering_nulls:\n            return cls.query.order_by(*cls.ordering_nulls)\n        return cls.query.order_by(*cls.ordering)\n\n    @classmethod\n    def get_default_columns(cls):\n        return [field.column_name for field in cls", "solution": "\n        # Create the object\n        obj = cls()\n        # By default, objects are initialized with attribute values of None\n        # We need to clear those out so that we get AttributeError on access\n        for attr in cls.__all_attributes__:\n            delattr(obj, \"_\" + attr)\n        del obj._id\n        # Set the attributes that were passed\n        for attr, val in kwargs.items():\n            setattr(obj, \"_\" + attr, val)\n        return obj"}
{"prompt": "def months_between(date1, date2, roundOff=True):\n    \"\"\"\n    Returns number of months between dates date1 and date2.\n    If date1 is later than date2, then the result is positive.\n    If date1 and date2 are on the same day of month, or both are the last day of month,\n    returns an integer (time of day will be ignored).\n    The result is rounded off to 8 digits unless `roundOff` is set to `False`.\n\n    >>> df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['date1', 'date2'])\n    >>> df.select(months_between(df.date1, df.date2).alias('months')).collect()\n    [Row(months=3.94959677)]\n    >>> df.select(months_between(df.date1, df.date2, False).alias('months')).collect()\n    [Row(months=3.9495967741935485)]\n    \"\"\"", "output": "-30')], ['d'])\n    >>> df.select(months_between(df.d, 10), df.months_between(df.date1, df.date2))\n    df.select(months_between(date2, 10, False), df.months_between(date1, False, True))\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.months_between(_to_java_column(date1), _to_java_column(date2), roundOff))\n\n\n@ignore_unicode_prefix\n@since(1", "solution": "\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.months_between(\n        _to_java_column(date1), _to_java_column(date2), roundOff))"}
{"prompt": "def make_temp_path(path, new_ext=None):\n    \"\"\"\n    Arguments:\n\n      new_ext: the new file extension, including the leading dot.\n        Defaults to preserving the existing file extension.\n\n    \"\"\"", "output": "\n    if new_ext is None:\n      new_ext = tempfile.template\n    if new_ext.startswith(tempfile.template_prefix):\n      return new_ext[len(tempfile.template_prefix):]\n    else:\n      return new_ext\n\n  def _GetTempFile(self, path):\n    \"\"\"Returns a temporary file name for the given path.\n\n    Arguments:\n\n      path: the path of the file to create, relative to the test file.\n\n    \"\"\"\n    fd, temp_file = tempfile.mkstemp(dir=path)\n    os.close(fd)\n    if self.filesystem:\n      temp_", "solution": "\n    root, ext = os.path.splitext(path)\n    if new_ext is None:\n        new_ext = ext\n    temp_path = root + TEMP_EXTENSION + new_ext\n    return temp_path"}
{"prompt": "def _exec_template(callable_, context, args=None, kwargs=None):\n    \"\"\"execute a rendering callable given the callable, a\n    Context, and optional explicit arguments\n\n    the contextual Template will be located if it exists, and\n    the error handling options specified on that Template will\n    be interpreted here.\n    \"\"\"", "output": "\n    if args is None:\n        args = []\n    if kwargs is None:\n        kwargs = {}\n    if callable_ is not None:\n        callable_ = callable_.__name__\n    if context is not None:\n        context = context.__name__\n    if callable_ is not None:\n        callable_ = callable_.__name__\n    if context:\n        context = context.__name__\n    try:\n        callable_ = getattr(builtins, callable_)\n    except AttributeError:\n        raise ImportError(\"no builtins function %r\" % callable_)\n    if not callable_():\n        raise ImportError(\"no such type %r\" % type(", "solution": "\n    template = context._with_template\n    if template is not None and \\\n            (template.format_exceptions or template.error_handler):\n        try:\n            callable_(context, *args, **kwargs)\n        except Exception:\n            _render_error(template, context, compat.exception_as())\n        except:\n            e = sys.exc_info()[0]\n            _render_error(template, context, e)\n    else:\n        callable_(context, *args, **kwargs)"}
{"prompt": "def newpin(digits=4):\n    \"\"\"\n    Return a random numeric string with the specified number of digits,\n    default 4.\n\n    >>> len(newpin())\n    4\n    >>> len(newpin(5))\n    5\n    >>> newpin().isdigit()\n    True\n    \"\"\"", "output": "\n    return ''.join(random.choice(string.digits) for _ in range(randint(0, digits))\n\n\ndef newpinlist(digits=4):\n    \"\"\"\n    Return a list of random strings with the specified number of\n    digits, default 4.\n\n    >>> newpinlist(5)\n    [random.choice('a'), random.choice('b'), random.choice('c')]\n    \"\"\"\n    return [newpin(digits) for _ in range(randint(0, digits))]\n\n\ndef newpinlist2(digits=4):\n    \"\"\"\n    Return a list of random strings with the specified", "solution": "\n    randnum = randint(0, 10 ** digits)\n    while len(str(randnum)) > digits:\n        randnum = randint(0, 10 ** digits)\n    return (u'%%0%dd' % digits) % randnum"}
{"prompt": "def wateryear(self):\n        \"\"\"The actual hydrological year according to the selected\n        reference month.\n\n        The reference mont reference |Date.refmonth| defaults to November:\n\n        >>> october = Date('1996.10.01')\n        >>> november = Date('1996.11.01')\n        >>> october.wateryear\n        1996\n        >>> november.wateryear\n        1997\n\n        Note that changing |Date.refmonth| affects all |Date| objects:\n\n        >>> october.refmonth = 10\n        >>> october.wateryear\n        1997\n        >>> november.wateryear\n        1997\n        >>> october.refmonth = 'November'\n        >>> october.wateryear\n        1996\n        >>> november.wateryear\n        1997\n        \"\"\"", "output": "ryear()\n        Traceback (most recent call last):\n       ...\n        ValueError: Date.refmonth is not a leap year.\n\n        \"\"\"\n        return self._wateryear\n\n    @wateryear.setter\n    def wateryear(self, value):\n        self._wateryear = value\n\n    @property\n    def leapyear(self):\n        \"\"\"The actual year according to the selected reference\n        month.\n\n        The reference mont reference |Date.leapyear| defaults to November:\n\n        >>> november = Date('1996.10.01')\n        >>> november.leapyear", "solution": "\n        if self.month < self._firstmonth_wateryear:\n            return self.year\n        return self.year + 1"}
{"prompt": "def create_qcos_client(self, app_uri):\n        \"\"\"\u521b\u5efa\u8d44\u6e90\u7ba1\u7406\u5ba2\u6237\u7aef\n\n        \"\"\"", "output": "\n        self.app_uri = app_uri\n        self.app_secret = app_uri.split(' ')[1]\n        self.app_secret = app_secret\n        self.client = QcosClient(app_uri, self.app_secret)\n        self.client.set_app_id(self.app_id)\n        self.client.set_app_key(self.app_key)\n        self.client.set_app_secret(self.app_secret)\n        return self.client\n\n    def get_user_info(self):\n        \"\"\"\u83b7\u53d6\u7528\u6237", "solution": "\n\n        if (self.auth is None):\n            return QcosClient(None)\n\n        products = self.get_app_region_products(app_uri)\n        auth = self.get_valid_app_auth(app_uri)\n\n        if products is None or auth is None:\n            return None\n\n        return QcosClient(auth, products.get('api'))"}
{"prompt": "def __init_configrs(self, rs_cfg):\n        \"\"\"Create and start a config replica set.\"\"\"", "output": "\n        self.rs_cfg = rs_cfg\n        self.config_replica_set = rs_cfg.get('config_replica_set', {})\n        self.config_replica_set_name = rs_cfg.get('config_replica_set_name', 'default')\n        self.config_replica_set_ip = rs_cfg.get('config_replica_set_ip', '')\n        self.config_replica_set_port = rs_cfg.get('config_replica_set_port', '')\n        self.config_replica_set_timeout = rs_cfg.get", "solution": "\n        # Use 'rs_id' to set the id for consistency, but need to rename\n        # to 'id' to use with ReplicaSets.create()\n        rs_cfg['id'] = rs_cfg.pop('rs_id', None)\n        for member in rs_cfg.setdefault('members', [{}]):\n            member['procParams'] = self._strip_auth(\n                member.get('procParams', {}))\n            member['procParams']['configsvr'] = True\n            if self.enable_ipv6:\n                common.enable_ipv6_single(member['procParams'])\n        rs_cfg['sslParams'] = self.sslParams\n        self._configsvrs.append(ReplicaSets().create(rs_cfg))"}
{"prompt": "def get_stack_info(\n    frames,\n    with_locals=True,\n    library_frame_context_lines=None,\n    in_app_frame_context_lines=None,\n    include_paths_re=None,\n    exclude_paths_re=None,\n    locals_processor_func=None,\n):\n    \"\"\"\n    Given a list of frames, returns a list of stack information\n    dictionary objects that are JSON-ready.\n\n    We have to be careful here as certain implementations of the\n    _Frame class do not contain the necessary data to lookup all\n    of the information we want.\n\n    :param frames: a list of (Frame, lineno) tuples\n    :param with_locals: boolean to indicate if local variables should be collected\n    :param include_paths_re: a regex to determine if a frame is not a library frame\n    :param exclude_paths_re: a regex to exclude frames from not being library frames\n    :param locals_processor_func: a function to call on all local variables\n    :return:\n    \"\"\"", "output": "param frames: A list of frames.\n    :param with_locals: Whether to include the paths in the output.\n    :param library_frame_context_lines: A list of lines of context for the library.\n    :param in_app_frame_context_lines: A list of lines of context for the app.\n    :param include_paths_re: Whether to include the paths in the output.\n    :param exclude_paths_re: Whether to exclude the paths from the output.\n    :param locals_processor_func: A function that is called to get the\n        locals for a given frame.", "solution": "\n    results = []\n    for frame, lineno in frames:\n        result = get_frame_info(\n            frame,\n            lineno,\n            library_frame_context_lines=library_frame_context_lines,\n            in_app_frame_context_lines=in_app_frame_context_lines,\n            with_locals=with_locals,\n            include_paths_re=include_paths_re,\n            exclude_paths_re=exclude_paths_re,\n            locals_processor_func=locals_processor_func,\n        )\n        if result:\n            results.append(result)\n    return results"}
{"prompt": "def iter_surrounding(self, center_key):\n        \"\"\"Iterate over all bins surrounding the given bin\"\"\"", "output": "\n        for i in range(self.bins):\n            yield self.bins[i], self.bins[i+1]\n\n    def get_bin_edges(self, center_key):\n        \"\"\"Get the edges of the given bin\"\"\"\n        for i in range(self.bins):\n            yield self.bins[i], self.bins[i+1]\n\n    def get_bin_centers(self, center_key):\n        \"\"\"Get the centers of the given bin\"\"\"\n        for i in range(self.bins):\n            yield self.bins[i], self.bins[i+1]\n\n    def", "solution": "\n        for shift in self.neighbor_indexes:\n            key = tuple(np.add(center_key, shift).astype(int))\n            if self.integer_cell is not None:\n                key = self.wrap_key(key)\n            bin = self._bins.get(key)\n            if bin is not None:\n                yield key, bin"}
{"prompt": "def add_model(self, ic, N=1, index=0):\n        \"\"\"\n        Should only be able to do this to a leaf node.\n\n        Either N and index both integers OR index is\n        list of length=N\n        \"\"\"", "output": "\n        if isinstance(ic, int):\n            ic = self.get_int(ic)\n        elif isinstance(ic, list):\n            ic = self.get_list(ic)\n        elif isinstance(ic, int):\n            ic = self.get_int(ic, index)\n        elif isinstance(ic, float):\n            ic = self.get_float(ic, index)\n        elif isinstance(ic, str):\n            ic = self.get_string(ic, index)\n        elif isinstance(ic, int):\n            ic = self.get_int(ic, index)", "solution": "\n        if type(index) in [list,tuple]:\n            if len(index) != N:\n                raise ValueError('If a list, index must be of length N.')\n        else:\n            index = [index]*N\n\n        for idx in index:\n            existing = self.get_system(idx)\n            tag = len(existing)\n            self.add_child(ModelNode(ic, index=idx, tag=tag))"}
{"prompt": "def generateRevision(self):\n        \"\"\"\n        Generates the revision file for this builder.\n        \"\"\"", "output": "\n        return self.revision\n\n    def getBuild(self, buildid):\n        \"\"\"\n        Returns the Build object for this builder.\n        \"\"\"\n        return self.builder_status.getBuild(buildid)\n\n    def getBuildByNumber(self, number):\n        \"\"\"\n        Returns the Build object for this builder.\n        \"\"\"\n        return self.builder_status.getBuildByNumber(number)\n\n    def getBuildByFlavor(self, flavor):\n        \"\"\"\n        Returns the Build object for this builder.\n        \"\"\"\n        return self.builder_status.getBuildByFlavor(flavor)\n\n    def getBuildByVersion", "solution": "\n        revpath = self.sourcePath()\n        if not os.path.exists(revpath):\n            return\n\n        # determine the revision location\n        revfile = os.path.join(revpath, self.revisionFilename())\n        mode = ''\n        # test for svn revision\n        try:\n            args = ['svn', 'info', revpath]\n            proc = subprocess.Popen(args, stdout=subprocess.PIPE)\n            mode = 'svn'\n        except WindowsError:\n            try:\n                args = ['git', 'rev-parse', 'HEAD', revpath]\n                proc = subprocess.Popen(args, stdout=subprocess.PIPE)\n                mode = 'git'\n            except WindowsError:\n                return\n\n        # process SVN revision\n        rev = None\n\n        if mode == 'svn':\n            for line in proc.stdout:\n                data = re.match('^Revision: (\\d+)', line)\n                if data:\n                    rev = int(data.group(1))\n                    break\n\n        if rev is not None:\n            try:\n                f = open(revfile, 'w')\n                f.write('__revision__ = {0}\\n'.format(rev))\n                f.close()\n            except IOError:\n                pass"}
{"prompt": "def _unify_sources_and_hashes(source=None, source_hash=None,\n                              sources=None, source_hashes=None):\n    \"\"\"\n    Silly little function to give us a standard tuple list for sources and\n    source_hashes\n    \"\"\"", "output": "\n    if source_hash and source:\n        raise ValueError(\"Can't combine sources and hashes with the same \"\n                         \"source\")\n    if source_hash:\n        return tuple(source), tuple(source_hash)\n    elif source_hashes:\n        return tuple(source_hashes), tuple(source_hashes)\n    else:\n        return None, None\n\n\ndef _get_source_and_hash(source, source_hash):\n    \"\"\"\n    Get the hash of the source and the hash of the hash of the\n    binary data.\n    \"\"\"\n    if source_hash:\n        return source, source_hash\n    elif", "solution": "\n    if sources is None:\n        sources = []\n\n    if source_hashes is None:\n        source_hashes = []\n\n    if source and sources:\n        return (False,\n                \"source and sources are mutually exclusive\", [])\n\n    if source_hash and source_hashes:\n        return (False,\n                \"source_hash and source_hashes are mutually exclusive\", [])\n\n    if source:\n        return (True, '', [(source, source_hash)])\n\n    # Make a nice neat list of tuples exactly len(sources) long..\n    return True, '', list(zip_longest(sources, source_hashes[:len(sources)]))"}
{"prompt": "def _find_loopback(self, use_loopback=True, var_name='loopback'):\n        \"\"\"Finds a free loopback device that can be used. The loopback is stored in :attr:`loopback`. If *use_loopback*\n        is True, the loopback will also be used directly.\n\n        :returns: the loopback address\n        :raises NoLoopbackAvailableError: if no loopback could be found\n        \"\"\"", "output": "\n        for device in self.loopback:\n            if device.is_connected:\n                if use_loopback:\n                    return device.address\n                else:\n                    return device.get_address()\n        raise NoLoopbackAvailableError('No loopback device found.')\n\n    def _find_loopback_device(self, use_loopback=True):\n        \"\"\"Finds a free loopback device that can be used.\n\n        :param use_loopback: if *use_loopback* is True, the device will be found in\n        :returns: the free loopback address\n        :raises NoLoopbackAvailableError: if no free loopback could be found", "solution": "\n\n        # noinspection PyBroadException\n        try:\n            loopback = _util.check_output_(['losetup', '-f']).strip()\n            setattr(self, var_name, loopback)\n        except Exception:\n            logger.warning(\"No free loopback device found.\", exc_info=True)\n            raise NoLoopbackAvailableError()\n\n        # noinspection PyBroadException\n        if use_loopback:\n            try:\n                cmd = ['losetup', '-o', str(self.offset), '--sizelimit', str(self.size),\n                       loopback, self.get_raw_path()]\n                if not self.disk.read_write:\n                    cmd.insert(1, '-r')\n                _util.check_call_(cmd, stdout=subprocess.PIPE)\n            except Exception:\n                logger.exception(\"Loopback device could not be mounted.\")\n                raise NoLoopbackAvailableError()\n        return loopback"}
{"prompt": "def check( state_engine, token_op, block_id, checked_ops ):\n    \"\"\"\n    Verify that a token transfer operation is permitted.\n    * the token feature must exist\n    * the sender must be unlocked---i.e. able to send at this point\n    * the sender must have enough balance of the given token to send the amount requested\n    * the token value must be positive\n    * the consensus hash must be valid\n\n    Return True if accepted\n    Return False if not\n    \"\"\"", "output": "\n    return state_engine.verify_op( token_op, \"check\", [block_id] )\n\n\ndef unlock( state_engine, token_hash, block_id, checked_hash, consensus_hash, sender_id, sender_hash ):\n    \"\"\"\n    Verify that a token transfer operation is not permitted.\n    * the token feature must exist\n    * the sender must be unlocked---i.e. not able to send at this point\n    * the hash must be valid\n\n    Return True if not accepted\n    Return False if not\n    \"\"\"\n    return state_engine.verify_op( token", "solution": "\n\n    epoch_features = get_epoch_features(block_id)\n    if EPOCH_FEATURE_TOKEN_TRANSFER not in epoch_features:\n        log.warning(\"Token transfers are not enabled in this epoch\")\n        return False\n\n    consensus_hash = token_op['consensus_hash']\n    address = token_op['address']\n    recipient_address = token_op['recipient_address']\n    token_type = token_op['token_units']\n    token_value = token_op['token_fee']\n\n    # token value must be positive\n    if token_value <= 0:\n        log.warning(\"Zero-value token transfer from {}\".format(address))\n        return False\n\n    # can't send to ourselves \n    if address == recipient_address:\n        log.warning('Cannot transfer token from the account to itself ({})'.format(address))\n        return False\n\n    # consensus hash must be valid\n    if not state_engine.is_consensus_hash_valid(block_id, consensus_hash):\n        log.warning('Invalid consensus hash {}'.format(consensus_hash))\n        return False\n\n    # sender account must exist\n    account_info = state_engine.get_account(address, token_type)\n    if account_info is None:\n        log.warning(\"No account for {} ({})\".format(address, token_type))\n        return False\n\n    # sender must not be transfer-locked\n    if block_id < account_info['lock_transfer_block_id']:\n        log.warning('Account {} is blocked from transferring tokens until block height {}'.format(address, account_info['lock_transfer_block_id']))\n        return False\n\n    # sender must have enough balance of the token  \n    account_balance = state_engine.get_account_balance(account_info)\n    if account_balance < token_value:\n        log.warning('Account {} has {} {}; tried to send {}'.format(address, account_balance, token_type, token_value))\n        return False\n    \n    receiver_account = state_engine.get_account(recipient_address, token_type)\n    if receiver_account is not None:\n        if not receiver_account['receive_whitelisted']:\n            log.warning('Receiver account {} is not whitelisted'.format(recipient_address))\n            return False\n\n    log.debug(\"Account {} will pay {} {} to {}\".format(address, token_value, token_type, recipient_address))\n\n    # will execute a debit against the sender address\n    token_operation_put_account_payment_info(token_op, address, token_type, token_value)\n\n    # will execute a credit against the receiver address \n    token_operation_put_account_credit_info(token_op, recipient_address, token_type, token_value)\n\n    # preserve token_fee as a string to prevent overflow\n    token_op['token_fee'] = '{}'.format(token_op['token_fee'])\n    return True"}
{"prompt": "def to_cfn_resource_name(name):\n    # type: (str) -> str\n    \"\"\"Transform a name to a valid cfn name.\n\n    This will convert the provided name to a CamelCase name.\n    It's possible that the conversion to a CFN resource name\n    can result in name collisions.  It's up to the caller\n    to handle name collisions appropriately.\n\n    \"\"\"", "output": "\n    return re.sub(r'[^a-zA-Z0-9_]', '_', name)\n\n\ndef to_camel_case(name):\n    # type: (str) -> str\n    \"\"\"Transform a name to a valid camelCase name.\n\n    This will convert the provided name to a CamelCase name.\n    It's possible that the conversion to a camelCase\n    name can result in name collisions.  It's up to the caller\n    to handle name collisions appropriately.\n\n    \"\"\"\n    return re.sub(r'[^a-zA-Z0-9_]', '_', name)\n\n\ndef to_", "solution": "\n    if not name:\n        raise ValueError(\"Invalid name: %r\" % name)\n    word_separators = ['-', '_']\n    for word_separator in word_separators:\n        word_parts = [p for p in name.split(word_separator) if p]\n        name = ''.join([w[0].upper() + w[1:] for w in word_parts])\n    return re.sub(r'[^A-Za-z0-9]+', '', name)"}
{"prompt": "def action(self, relationship):\n        \"\"\"Add a nested File Action.\"\"\"", "output": "\n        self.add_file_action(relationship, 'add')\n\n    def add_file_action(self, relationship, action_type):\n        \"\"\"Add a nested File Action.\"\"\"\n        self.add_file_action(relationship, 'add_file')\n\n    def add_file_action_file(self, relationship, action_type):\n        \"\"\"Add a file to the action.\"\"\"\n        self.add_file_action(relationship, 'add_file_file')\n\n    def add_file_action_file_system(self, relationship, action_type):\n        \"\"\"Add a file to the action.\"\"\"", "solution": "\n        action_obj = FileAction(self.xid, relationship)\n        self._children.append(action_obj)"}
{"prompt": "def find(self, path):\n        \"\"\"Return the node for a path, or None.\"\"\"", "output": "\n        return self._nodes.get(path, None)\n\n    def find_all(self):\n        \"\"\"Return all nodes.\"\"\"\n        return self._nodes.values()\n\n    def find_path(self, path):\n        \"\"\"Return the node for a path, or None.\"\"\"\n        return self._nodes.get(path, None)\n\n    def find_path_to(self, path):\n        \"\"\"Return the node for a path, or None.\"\"\"\n        return self._nodes.get(path, None)\n\n    def find_path_to_regex(self, path):\n        \"\"\"Return the node for a path, or", "solution": "\n        path = path.split('.')\n        node = self\n        while node._parent:\n            node = node._parent\n        for name in path:\n            node = node._tree.get(name, None)\n            if node is None or type(node) is float:\n                return None\n        return node"}
{"prompt": "def generate_object_graphs_by_class(classlist):\n    \"\"\"\n    Generate reference and backreference graphs\n    for objects of type class for each class given in classlist.\n    Useful for debugging reference leaks in framework etc.\n\n    Usage example to generate graphs for class \"someclass\":\n    >>> import someclass\n    >>> someclassobject = someclass()\n    >>> generate_object_graphs_by_class(someclass)\n\n    Needs \"objgraph\" module installed.\n    \"\"\"", "output": "\n    if not classlist:\n        return None\n    if not hasattr(classlist, '__iter__'):\n        return None\n    if not hasattr(classlist, '__next__'):\n        return None\n    if len(classlist) == 1:\n        return generate_object_graphs_by_class(classlist[0])\n    return None\n\n\ndef generate_object_graphs_by_module(module):\n    \"\"\"\n    Generate reference and backreference modules\n    for object of type module for each module given in module.\n\n    Needs \"module\" module installed.\n    \"\"\"\n    if not module:\n        return None\n    if hasattr(module,", "solution": "\n    try:\n        import objgraph\n        import gc\n    except ImportError:\n        return\n    graphcount = 0\n    if not isinstance(classlist, list):\n        classlist = [classlist]\n    for class_item in classlist:\n        for obj in gc.get_objects():\n            if isinstance(obj, class_item):\n                graphcount += 1\n                objgraph.show_refs([obj], filename='%d_%s_%d_refs.png' % (\n                    ogcounter, obj.__class__.__name__, graphcount))\n                objgraph.show_backrefs([obj], filename='%d_%s_%d_backrefs.png' % (\n                    ogcounter, obj.__class__.__name__, graphcount))"}
{"prompt": "def ParseGshadowEntry(self, line):\n    \"\"\"Extract the members of each group from /etc/gshadow.\n\n    Identifies the groups in /etc/gshadow and several attributes of the group,\n    including how the password is crypted (if set).\n\n    gshadow files have the format group_name:passwd:admins:members\n    admins are both group members and can manage passwords and memberships.\n\n    Args:\n      line: An entry in gshadow.\n    \"\"\"", "output": "\n    if not line:\n      return\n\n    if self.password_set_name:\n      self.password_set_name = line.strip()\n\n    if self.group_set_name:\n      self.group_set_name = line.strip()\n\n    if self.group_set_name:\n      self.group_set_name = line.strip()\n\n    if self.group_set_name:\n      self.group_set_name = line.strip()\n\n    if self.group_set_name:\n      self.group_set_name = line.strip()\n\n    if self.group_", "solution": "\n    fields = (\"name\", \"passwd\", \"administrators\", \"members\")\n    if line:\n      rslt = dict(zip(fields, line.split(\":\")))\n      # Add the shadow state to the internal store.\n      name = rslt[\"name\"]\n      pw_entry = self.shadow.setdefault(name, rdf_client.PwEntry())\n      pw_entry.store = self.shadow_store\n      pw_entry.hash_type = self.GetHashType(rslt[\"passwd\"])\n      # Add the members to the internal store.\n      members = self.gshadow_members.setdefault(name, set())\n      for accts in rslt[\"administrators\"], rslt[\"members\"]:\n        if accts:\n          members.update(accts.split(\",\"))"}
{"prompt": "def as_es2_command(command):\n    \"\"\" Modify a desktop command so it works on es2.\n    \"\"\"", "output": "\n    if command.startswith('es2:'):\n        command = command.replace('es2:', 'es2_desktop')\n    return command\n\n\ndef get_es_version():\n    \"\"\" Get the version of the ES installation.\n    \"\"\"\n    return __salt__['cmd.run']('equery -v', python_shell=True)\n\n\ndef get_es_version_info():\n    \"\"\" Get the version of the ES installation.\n    \"\"\"\n    return __salt__['cmd.run']('equery -v', python_shell=True, output_loglevel='debug')\n\n\ndef get_es_version", "solution": "\n\n    if command[0] == 'FUNC':\n        return (command[0], re.sub(r'^gl([A-Z])',\n                lambda m: m.group(1).lower(), command[1])) + command[2:]\n    if command[0] == 'SHADERS':\n        return command[:2] + convert_shaders('es2', command[2:])\n    if command[0] == 'UNIFORM':\n        return command[:-1] + (command[-1].tolist(),)\n    return command"}
{"prompt": "def p_example(self, p):\n        \"\"\"example : KEYWORD ID NL INDENT docsection example_fields DEDENT\n                   | KEYWORD ID NL\"\"\"", "output": "\n        p[0] = ast.Constant(p[1], p[3])\n\n    def p_error(self, p):\n        \"\"\"error : ERROR ID NL INDENT docsection_errors DEDENT\"\"\"\n        p[0] = ast.Constant(p[1], p[3])\n\n    def p_field(self, p):\n        \"\"\"field : ID COLON field_fields RBRACE field_fields RBRACE\"\"\"\n        p[0] = ast.ID(p[3], p[5])\n\n    def p_field_list(self, p):\n        \"\"\"field_list : ID COLON", "solution": "\n        if len(p) > 4:\n            seen_fields = set()\n            for example_field in p[6]:\n                if example_field.name in seen_fields:\n                    self.errors.append(\n                        (\"Example with label '%s' defines field '%s' more \"\n                        \"than once.\" % (p[2], example_field.name),\n                        p.lineno(1), self.path))\n                seen_fields.add(example_field.name)\n            p[0] = AstExample(\n                self.path, p.lineno(1), p.lexpos(1), p[2], p[5],\n                OrderedDict((f.name, f) for f in p[6]))\n        else:\n            p[0] = AstExample(\n                self.path, p.lineno(1), p.lexpos(1), p[2], None, OrderedDict())"}
{"prompt": "def load_from_file(cls, file_path):\n        \"\"\"Load the meta data given a file_path or empty meta data\"\"\"", "output": "\n        if not os.path.isfile(file_path):\n            return None\n\n        with open(file_path, 'r') as f:\n            return cls.from_json(json.load(f))\n\n    @classmethod\n    def from_json(cls, file_obj):\n        \"\"\"\n        Create a new instance of this class from a json file\n        \"\"\"\n        if isinstance(file_obj, dict):\n            return cls.from_json(file_obj)\n        elif file_obj is None:\n            return None\n\n    @classmethod\n    def from_file_json(cls, file_path):\n        \"\"\"\n       ", "solution": "\n        data = None\n        if os.path.exists(file_path):\n            metadata_file = open(file_path)\n            data = json.loads(metadata_file.read())\n        return cls(initial=data)"}
{"prompt": "def _forward_gradients(ys, xs, d_xs):\n        \"\"\"\n        Forward-mode pushforward analogous to the pullback defined by tf.gradients.\n        With tf.gradients, grad_ys is the vector being pulled back, and here d_xs is\n        the vector being pushed forward, i.e. this computes (\u2202ys / \u2202xs)^T \u2202xs.\n\n        This is adapted from https://github.com/HIPS/autograd/pull/175#issuecomment-306984338\n\n        :param ys: list of variables being differentiated (tensor)\n        :param xs: list of variables to differentiate wrt (tensor)\n        :param d_xs: list of gradients to push forward (same shapes as ys)\n        :return: the specified moment of the variational distribution\n        \"\"\"", "output": ")\n        :param xs: list of variables being pushed forward (tensor)\n        :param d_xs: list of gradients (tensor)\n        :return: list of variables being pushed forward (tensor)\n        \"\"\"\n        return [y - d_ys[i] for i, y in enumerate(ys)]\n\n    def _backward_gradients(self, ys, xs, d_ys):\n        \"\"\"\n        Backward-mode popback analogous to the autograd defined by\n        tf.gradients.\n\n        :param ys: list of variables being differentiated (tensor)\n        :param xs: list of variables being pushed backward (", "solution": "\n        # this should be v = [tf.placeholder(y.dtype) for y in ys], but tensorflow\n        # wants a value for the placeholder, even though it never gets used\n        v = [tf.placeholder_with_default(tf.zeros(y.get_shape(), dtype=y.dtype),\n                                         shape=y.get_shape()) for y in ys]\n\n        g = tf.gradients(ys, xs, grad_ys=v)\n        return tf.gradients(g, v, grad_ys=d_xs)"}
{"prompt": "def process(self, versions):\n        \"\"\"Logging version sorted ascending by tool name.\"\"\"", "output": "\n        if self.args.verbose:\n            self.log.setLevel(logging.DEBUG)\n        else:\n            self.log.setLevel(logging.INFO)\n\n        # Set up logging\n        self.log = logging.getLogger(__name__)\n        self.log.debug(\"Starting logging for %s\" % self.args.tool)\n        self.log.debug(\"Using %s\" % self.args.tool)\n        self.log.debug(\"Using %s\" % self.args.tool)\n        self.log.debug(\"Using %s\" % self.args.tool)\n\n        # Parse the version", "solution": "\n        for tool_name in sorted(versions.keys()):\n            version = versions[tool_name]\n            self._log(\"Using tool '%s', %s\" % (tool_name, version))"}
{"prompt": "def cancel(self):\n        \"\"\"Cancel the future.\n\n        Returns True if any of the elements of the iterables is cancelled.\n        False otherwise.\n        \"\"\"", "output": "\n        return False\n\n    def __iter__(self):\n        \"\"\"Return an iterator over the iterables in the stream.\"\"\"\n        return self\n\n    def __next__(self):\n        \"\"\"Return the next item in the stream.\"\"\"\n        return self.next()\n\n    def __enter__(self):\n        \"\"\"Enter the stream context and return a context manager that will\n        enter the future.\n\n        Returns:\n            ContextManager: The stream context manager.\n        \"\"\"\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        \"\"\"Exit the stream context and clean up.\n\n        Args:\n            exc_type: The exception", "solution": "\n        super(ProcessMapFuture, self).cancel()\n\n        return any(tuple(f.cancel() for f in self._futures))"}
{"prompt": "def north_arrow_path(feature, parent):\n    \"\"\"Retrieve the full path of default north arrow logo.\"\"\"", "output": "\n    return os.path.join(parent.image_path, 'images', 'default_north_arrow.png')\n\n\ndef get_default_icon_path(icon_name):\n    \"\"\"Retrieve the full path of the default icon.\"\"\"\n    return os.path.join(\n        get_icon_path(icon_name),\n        'icons',\n        'default_icon.png'\n    )\n\n\ndef get_icon_path(icon_name):\n    \"\"\"Retrieve the full path of the icon.\"\"\"\n    return os.path.join(\n        get_icon_path(icon_name),", "solution": "\n    _ = feature, parent  # NOQA\n\n    north_arrow_file = setting(inasafe_north_arrow_path['setting_key'])\n    if os.path.exists(north_arrow_file):\n        return north_arrow_file\n    else:\n        LOGGER.info(\n            'The custom north arrow is not found in {north_arrow_file}. '\n            'Default north arrow will be used.').format(\n            north_arrow_file=north_arrow_file)\n        return inasafe_default_settings['north_arrow_path']"}
{"prompt": "def filter_localization_probability(df, threshold=0.75):\n    \"\"\"\n    Remove rows with a localization probability below 0.75\n\n    Return a ``DataFrame`` where the rows with a value < `threshold` (default 0.75) in column 'Localization prob' are removed.\n    Filters data to remove poorly localized peptides (non Class-I by default).\n\n    :param df: Pandas ``DataFrame``\n    :param threshold: Cut-off below which rows are discarded (default 0.75)\n    :return: Pandas ``DataFrame``\n    \"\"\"", "output": "\n    return df.filter(lambda row: row[localization_probability] > threshold, axis='columns')\n\n\ndef filter_class_probability(df, threshold=0.75):\n    \"\"\"\n    Remove rows with a class-specific probability below 0.75\n\n    Return a ``DataFrame`` where the rows with a value < `threshold` (default 0.75) in column 'Class prob' are removed.\n    Filters data to remove poorly localized classes (Class I by default).\n\n    :param df: Pandas ``DataFrame``\n    :param threshold: Cut-off below which rows are removed (default", "solution": "\n    df = df.copy()\n    localization_probability_mask = df['Localization prob'].values >= threshold\n    return df.iloc[localization_probability_mask, :]"}
{"prompt": "def request_upload_token(self, file):\n        \"\"\"\n        Request an upload token.\n\n        :param file: A file handler pointing to the file to upload.\n        :returns: True if the file uploaded successfully, False otherwise, \\\n            and the JSON response from the API.\n        :rtype: tuple\n        \"\"\"", "output": "\n        return self._post(\n            'upload_token',\n            data=json.dumps({'file': file}),\n            headers={'Content-Type': 'application/json'},\n            files={'file': ('file', file, file)},\n            params={'token': self.token}\n        )\n\n    def request_upload_token_v2(self, file):\n        \"\"\"\n        Request an upload token.\n\n        :param file: A file handler pointing to the file to upload.\n        :returns: True if the file uploaded successfully, False otherwise \\\n            and the JSON response from the API.\n        :rtype: tuple\n        \"\"\"\n        return", "solution": "\n        self.kwargs['name'] = os.path.basename(file.name)\n        self.kwargs['size'] = os.fstat(file.fileno()).st_size\n\n        response = self._requester.request(\n            'POST',\n            self.url,\n            _kwargs=combine_kwargs(**self.kwargs)\n        )\n\n        return self.upload(response, file)"}
{"prompt": "def _single_resource_html_response(resource):\n    \"\"\"Return the HTML representation of *resource*.\n\n    :param resource: :class:`sandman.model.Model` to render\n    :type resource: :class:`sandman.model.Model`\n    :rtype: :class:`flask.Response`\n\n    \"\"\"", "output": "\n    return render_template('single_resource.html',\n                           resource=resource,\n                           title=resource.title,\n                           description=resource.description,\n                           url=resource.url,\n                           resource_type=resource.type,\n                           resource_id=resource.id,\n                           resource_title=resource.title,\n                           resource_description=resource.description,\n                           resource_type_id=resource.type.id,\n                           resource_type_title=resource.type.title,\n                           resource_type_description=resource.type.description,\n                           resource_id_list=resource.id_list,\n                           resource", "solution": "\n    tablename = resource.__tablename__\n    resource.pk = getattr(resource, resource.primary_key())\n    resource.attributes = resource.as_dict()\n    return make_response(render_template(\n        'resource.html',\n        resource=resource,\n        tablename=tablename))"}
{"prompt": "def load_configuration():\n    \"\"\"Load the configuration\"\"\"", "output": "\n    global configuration\n    configuration = ConfigParser.ConfigParser()\n    configuration.read(configuration_file)\n    return configuration\n\ndef get_configuration():\n    \"\"\"Return the configuration\"\"\"\n    return load_configuration()\n\ndef get_config_file():\n    \"\"\"Return the configuration file\"\"\"\n    return get_config_file()\n\ndef get_config_section(section):\n    \"\"\"Return the configuration section\"\"\"\n    return get_config_section_name(section) + \".ini\"\n\ndef get_config_section_name(section):\n    \"\"\"Return the configuration section name\"\"\"\n    return get_config_section_", "solution": "\n\n    (belbio_conf_fp, belbio_secrets_fp) = get_belbio_conf_files()\n    log.info(f\"Using conf: {belbio_conf_fp} and secrets files: {belbio_secrets_fp} \")\n\n    config = {}\n    if belbio_conf_fp:\n        with open(belbio_conf_fp, \"r\") as f:\n            config = yaml.load(f, Loader=yaml.SafeLoader)\n            config[\"source_files\"] = {}\n            config[\"source_files\"][\"conf\"] = belbio_conf_fp\n\n    if belbio_secrets_fp:\n        with open(belbio_secrets_fp, \"r\") as f:\n            secrets = yaml.load(f, Loader=yaml.SafeLoader)\n            config[\"secrets\"] = copy.deepcopy(secrets)\n            if \"source_files\" in config:\n                config[\"source_files\"][\"secrets\"] = belbio_secrets_fp\n\n    get_versions(config)\n\n    # TODO - needs to be completed\n    # add_environment_vars(config)\n\n    return config"}
{"prompt": "def get_paths(folder, ignore_endswith=ignore_endswith):\n    \"\"\"Return hologram file paths\n\n    Parameters\n    ----------\n    folder: str or pathlib.Path\n        Path to search folder\n    ignore_endswith: list\n        List of filename ending strings indicating which\n        files should be ignored.\n    \"\"\"", "output": "\n    if isinstance(folder, str):\n        return [folder]\n    elif isinstance(folder, pathlib.Path):\n        return [folder.resolve()]\n    else:\n        raise ValueError(\"Invalid path type: %s\" % type(folder))\n\n\ndef get_paths_from_file(filepath):\n    \"\"\"Return hologram file paths from a file\n\n    Parameters\n    ----------\n    filepath: str\n        Path to search folder\n    \"\"\"\n    if isinstance(filepath, str):\n        return [filepath]\n    elif isinstance(filepath, pathlib.Path):\n        return [filepath.resolve()]\n    else:\n        raise ValueError(\"Invalid path type", "solution": "\n    folder = pathlib.Path(folder).resolve()\n    files = folder.rglob(\"*\")\n    for ie in ignore_endswith:\n        files = [ff for ff in files if not ff.name.endswith(ie)]\n    return sorted(files)"}
{"prompt": "def _read_json_binary_field(deserialized, field):\n    \"\"\" Read the value of a JSON field that may be string or base64-encoded.\n    \"\"\"", "output": "\n    if isinstance(field, six.text_type):\n        field = field.encode('utf-8')\n    return json.loads(deserialized.decode('utf-8')) if field else None\n\n\ndef _read_json_object(deserialized, klass):\n    \"\"\" Read the value of a JSON object.\n    \"\"\"\n    if isinstance(klass, six.text_type):\n        klass = klass.encode('utf-8')\n    return klass(deserialized.decode('utf-8')) if deserialized else None\n\n\ndef _read_json_array(deserialized, klass):\n    \"\"\" Read", "solution": "\n    val = deserialized.get(field)\n    if val is not None:\n        return utils.convert_to_bytes(val)\n    val = deserialized.get(field + '64')\n    if val is None:\n        return None\n    return utils.raw_urlsafe_b64decode(val)"}
{"prompt": "def read_series(source, name, match=None):\n    \"\"\"Read a `Series` from LIGO_LW-XML\n\n    Parameters\n    ----------\n    source : `file`, `str`, :class:`~ligo.lw.ligolw.Document`\n        file path or open LIGO_LW-format XML file\n\n    name : `str`\n        name of the relevant `LIGO_LW` element to read\n\n    match : `dict`, optional\n        dict of (key, value) `Param` pairs to match correct LIGO_LW element,\n        this is useful if a single file contains multiple `LIGO_LW` elements\n        with the same name\n    \"\"\"", "output": "IGO_LW` elements.\n        If `None`, this is a valid match.\n\n    Returns\n    -------\n    series : `LIGO_LW.Series`\n        series data\n    \"\"\"\n    if match is None:\n        match = {}\n    return read_ligolw_xml(source, name, **match)\n\n\ndef read_series_xml(source, **kwargs):\n    \"\"\"Read a `LIGO_LW-format` XML file\n\n    Parameters\n    ----------\n    source : `str`\n        path to the XML file\n\n    Returns\n    -------\n    ligolw.ligolw.Document\n        series", "solution": "\n    from ligo.lw.ligolw import (LIGO_LW, Time, Array, Dim)\n    from ligo.lw.param import get_param\n\n    # read document\n    xmldoc = read_ligolw(source, contenthandler=series_contenthandler())\n\n    # parse match dict\n    if match is None:\n        match = dict()\n\n    def _is_match(elem):\n        try:\n            if elem.Name != name:\n                return False\n        except AttributeError:  # Name is not set\n            return False\n        for key, value in match.items():\n            try:\n                if get_param(elem, key).pcdata != value:\n                    return False\n            except ValueError:  # no Param with this Name\n                return False\n        return True\n\n    # parse out correct element\n    matches = filter(_is_match, xmldoc.getElementsByTagName(LIGO_LW.tagName))\n    try:\n        elem, = matches\n    except ValueError as exc:\n        if not matches:\n            exc.args = (\"no LIGO_LW elements found matching request\",)\n        else:\n            exc.args = ('multiple LIGO_LW elements found matching request, '\n                        'please consider using `match=` to select the '\n                        'correct element',)\n        raise\n\n    # get data\n    array, = elem.getElementsByTagName(Array.tagName)\n\n    # parse dimensions\n    dims = array.getElementsByTagName(Dim.tagName)\n    xdim = dims[0]\n    x0 = xdim.Start\n    dx = xdim.Scale\n    xunit = xdim.Unit\n    try:\n        ndim = dims[1].n\n    except IndexError:\n        pass\n    else:\n        if ndim > 2:\n            raise ValueError(\"Cannot parse LIGO_LW Array with {} \"\n                             \"dimensions\".format(ndim))\n\n    # parse metadata\n    array_kw = {\n        'name': array.Name,\n        'unit': array.Unit,\n        'xunit': xunit,\n    }\n    try:\n        array_kw['epoch'] = to_gps(\n            elem.getElementsByTagName(Time.tagName)[0].pcdata)\n    except IndexError:\n        pass\n    for key in ('channel',):\n        try:\n            array_kw[key] = get_param(elem, key)\n        except ValueError:\n            pass\n\n    # build Series\n    try:\n        xindex, value = array.array\n    except ValueError:  # not two dimensions stored\n        return Series(array.array[0], x0=x0, dx=dx, **array_kw)\n    return Series(value, xindex=xindex, **array_kw)"}
{"prompt": "def fetch_reference_restriction(self, ):\n        \"\"\"Fetch whether referencing is restricted\n\n        :returns: True, if referencing is restricted\n        :rtype: :class:`bool`\n        :raises: None\n        \"\"\"", "output": "\n        return self._get_element(self.REFS_REFERENCE_RESTRICTIONS,\n                                 xsi_type=\"boolean\",\n                                 name=\"xsi:reference_restriction\",\n                                 value=True)\n\n    def _get_element(self, element, xsi_type=\"string\", name=None):\n        \"\"\"Create a new :class:`Element` object\n\n        :param element: element to be created\n        :type element: :class:`lxml.etree._Element`\n        :param xsi_type: type of the element\n        :type xsi_type: :class:`str`\n        :param name", "solution": "\n        inter = self.get_refobjinter()\n        restricted = self.status() is not None\n        return restricted or inter.fetch_action_restriction(self, 'reference')"}
{"prompt": "def _extend(self, newsub):\n        \"\"\"\n        Append a subclass (extension) after the base class. For parser internal use.\n        \"\"\"", "output": "\n        self.submodules.extend(newsub.submodules)\n        self.submodules.extend(newsub.submodules)\n        self.submodules.extend(newsub.submodules)\n        self.submodules.extend(newsub.submodules)\n        self.submodules.extend(newsub.submodules)\n        self.submodules.extend(newsub.submodules)\n        self.submodules.extend(newsub.submodules)\n        self.submodules.extend(newsub.submodules)\n        self.submodules.extend(_get_extension_modules(newsub", "solution": "\n        current = self\n        while hasattr(current, '_sub'):\n            current = current._sub\n        _set(current, '_sub', newsub)\n        try:\n            object.__delattr__(self, '_extra')\n        except:\n            pass"}
{"prompt": "def sct_report_string(report):\n    \"\"\"Return a human-readable string representation of the error report\n    returned by lxml's schematron validator.\n    \"\"\"", "output": "\n    return etree.tostring(report, pretty_print=True)\n\n\ndef sct_report_error_handler(report, error):\n    \"\"\"Return a function to be used for reporting\n    errors.\n    \"\"\"\n    if error is None:\n        return lambda x: None\n    if error == 'error':\n        return lambda x: None\n    if error == 'warning':\n        return lambda x: None\n    if error == 'info':\n        return lambda x: None\n    if error == 'warning':\n        return lambda x: None\n    if error == 'error':\n        return lambda x: None\n    if error == 'error", "solution": "\n    ret = []\n    namespaces = {\"svrl\": \"http://purl.oclc.org/dsdl/svrl\"}\n    for index, failed_assert_el in enumerate(\n        report.findall(\"svrl:failed-assert\", namespaces=namespaces)\n    ):\n        ret.append(\n            \"{}. {}\".format(\n                index + 1,\n                failed_assert_el.find(\"svrl:text\", namespaces=namespaces).text,\n            )\n        )\n        ret.append(\"   test: {}\".format(failed_assert_el.attrib[\"test\"]))\n        ret.append(\"   location: {}\".format(failed_assert_el.attrib[\"location\"]))\n        ret.append(\"\\n\")\n    return \"\\n\".join(ret)"}
{"prompt": "def extend(klass, name=None):\r\n    \"\"\"A function decorator for extending an existing class.\r\n\r\n    Use as a decorator for functions to add to an existing class.\r\n\r\n    Args:\r\n        klass: The class to be decorated.\r\n\r\n        name: The name the new method is to be given in the klass class.\r\n\r\n    Returns:\r\n        A decorator function which accepts a single function as its only\r\n        argument.  The decorated function will be added to class klass.\r\n\r\n    Raises:\r\n        ValueError: If klass already has an attribute with the same name as the\r\n            extension method.\r\n    \"\"\"", "output": "\r\n    def wrap(f):\r\n        def wrapped_f(*args, **kwargs):\r\n            return f(*args, **kwargs)\r\n        return wrapped_f\r\n    return klass(wrap)\r\n\r\ndef make_cookie(name, value, secret=None, max_age=None, path='/', domain=None, secure=False, httponly=False):\r\n    \"\"\"Create a new cookie.\r\n\r\n    Args:\r\n        name: The name the cookie should be created under.\r\n\r\n        value: The value the cookie should be created under.\r\n\r\n        secret: The value the cookie should be created under.\r\n\r\n        max_age: The maximum age in", "solution": "\r\n    def decorator(f):\r\n        return add_method(f, klass, name)\r\n\r\n    return decorator"}
{"prompt": "def handle_get_version_command(self):\n        \"\"\" Handles <get_version> command.\n\n        @return: Response string for <get_version> command.\n        \"\"\"", "output": "\n        if self.version:\n            return self.version\n\n        if self.module.params['state'] in ('present', 'enabled', 'disabled'):\n            return self.execute_show_command()\n\n        data = self.send_request(self.hostname, data='version')\n        match = re.search(r'Version (\\S+)', data)\n        if match:\n            ver = match.group(1)\n        else:\n            ver = None\n\n        if ver:\n            self.version = ver\n\n        data = self.send_request(self.hostname, data='show version')\n        match = re.search(", "solution": "\n        protocol = Element('protocol')\n        for name, value in [('name', 'OSP'), ('version', self.get_protocol_version())]:\n            elem = SubElement(protocol, name)\n            elem.text = value\n\n        daemon = Element('daemon')\n        for name, value in [('name', self.get_daemon_name()), ('version', self.get_daemon_version())]:\n            elem = SubElement(daemon, name)\n            elem.text = value\n\n        scanner = Element('scanner')\n        for name, value in [('name', self.get_scanner_name()), ('version', self.get_scanner_version())]:\n            elem = SubElement(scanner, name)\n            elem.text = value\n\n        content = [protocol, daemon, scanner]\n\n        if self.get_vts_version():\n            vts = Element('vts')\n            elem = SubElement(vts, 'version')\n            elem.text = self.get_vts_version()\n            content.append(vts)\n\n        return simple_response_str('get_version', 200, 'OK', content)"}
{"prompt": "def _redirect_complete(self, text: str, line: str, begidx: int, endidx: int, compfunc: Callable) -> List[str]:\n        \"\"\"Called by complete() as the first tab completion function for all commands\n        It determines if it should tab complete for redirection (|, <, >, >>) or use the\n        completer function for the current command\n\n        :param text: the string prefix we are attempting to match (all returned matches must begin with it)\n        :param line: the current input line with leading whitespace removed\n        :param begidx: the beginning index of the prefix text\n        :param endidx: the ending index of the prefix text\n        :param compfunc: the completer function for the current command\n                         this will be called if we aren't completing for redirection\n        :return: a list of possible tab completions\n        \"\"\"", "output": "\n        :param endidx: the end index of the prefix text\n        :param compfunc: the completer function to use (if any)\n        :return: the list of possible matches\n        \"\"\"\n        if len(line) > 1:\n            return compfunc(text, line[1:begidx], begidx, endidx)\n        else:\n            return compfunc(text, line, begidx, endidx)\n\n    def _complete_path(self, path: str) -> str:\n        \"\"\"Called by path() as the first path completion function for all commands\n        It determines if it should return", "solution": "\n        if self.allow_redirection:\n\n            # Get all tokens through the one being completed. We want the raw tokens\n            # so we can tell if redirection strings are quoted and ignore them.\n            _, raw_tokens = self.tokens_for_completion(line, begidx, endidx)\n            if not raw_tokens:\n                return []\n\n            if len(raw_tokens) > 1:\n\n                # Check if there are redirection strings prior to the token being completed\n                seen_pipe = False\n                has_redirection = False\n\n                for cur_token in raw_tokens[:-1]:\n                    if cur_token in constants.REDIRECTION_TOKENS:\n                        has_redirection = True\n\n                        if cur_token == constants.REDIRECTION_PIPE:\n                            seen_pipe = True\n\n                # Get token prior to the one being completed\n                prior_token = raw_tokens[-2]\n\n                # If a pipe is right before the token being completed, complete a shell command as the piped process\n                if prior_token == constants.REDIRECTION_PIPE:\n                    return self.shell_cmd_complete(text, line, begidx, endidx)\n\n                # Otherwise do path completion either as files to redirectors or arguments to the piped process\n                elif prior_token in constants.REDIRECTION_TOKENS or seen_pipe:\n                    return self.path_complete(text, line, begidx, endidx)\n\n                # If there were redirection strings anywhere on the command line, then we\n                # are no longer tab completing for the current command\n                elif has_redirection:\n                    return []\n\n        # Call the command's completer function\n        return compfunc(text, line, begidx, endidx)"}
{"prompt": "def get_website_configuration(self, headers=None):\n        \"\"\"\n        Returns the current status of website configuration on the bucket.\n\n        :rtype: dict\n        :returns: A dictionary containing a Python representation\n                  of the XML response from S3. The overall structure is:\n\n        * WebsiteConfiguration\n\n          * IndexDocument\n\n            * Suffix : suffix that is appended to request that\n              is for a \"directory\" on the website endpoint\n            * ErrorDocument\n\n              * Key : name of object to serve when an error occurs\n        \"\"\"", "output": "\n        if not headers:\n            headers = {}\n        return self._get_bucket_website_configuration(headers)\n\n    def _get_bucket_website_configuration(self, headers):\n        \"\"\"\n        Returns the current status of website configuration on the bucket.\n\n        :rtype: dict\n        :returns: A dictionary containing a Python representation\n                  of the XML response from S3. The overall structure is:\n\n        * WebsiteConfiguration\n\n          * IndexDocument\n\n            * Suffix : suffix that is appended to request that\n              is for a \"directory\" on the website endpoint\n            * ErrorDocument\n\n              * Key : name of object to serve when an", "solution": "\n        response = self.connection.make_request('GET', self.name,\n                query_args='website', headers=headers)\n        body = response.read()\n        boto.log.debug(body)\n        if response.status == 200:\n            e = boto.jsonresponse.Element()\n            h = boto.jsonresponse.XmlHandler(e, None)\n            h.parse(body)\n            return e\n        else:\n            raise self.connection.provider.storage_response_error(\n                response.status, response.reason, body)"}
{"prompt": "def download_parallel_gui(root, urls, directory, min_file_size, max_file_size, no_redirects):\n\t\"\"\"\n\tcalled when paralled downloading is true\n\t\"\"\"", "output": "\n\tglobal _progress_bar\n\tglobal _progress_bar_lock\n\tglobal _progress_bar_lock_timeout\n\n\tif not no_redirects:\n\t\turls = [url for url in urls if url.startswith('http://') and url.endswith('index.html')]\n\t\tif urls:\n\t\t\tprint('Downloading %d files from %s' % (len(urls), directory))\n\t\t\tfor url in urls:\n\t\t\t\tif not _download_file(url, directory, min_file_size, max_file_size):\n\t\t\t\t\tprint('Failed to download %s", "solution": "\n\tglobal parallel\n\n\t# create directory to save files\n\tif not os.path.exists(directory):\n\t\tos.makedirs(directory)\n\tparallel = True\n\tapp = progress_class(root, urls, directory, min_file_size, max_file_size, no_redirects)"}
{"prompt": "def add_hashed_value(self, hash_value, store_key):\n        \"\"\"Add hashed value to the index.\n\n        :param hash_value: The hashed value to be added to the index\n        :type hash_value: str\n        :param store_key: The key for the document in the store\n        :type store_key: object\n\n        \"\"\"", "output": "\n        self.hashed_values[store_key] = hash_value\n\n    def get_hashed_value(self, hash_value):\n        \"\"\"Get hashed value.\n\n        :param hash_value: The hashed value\n        :type hash_value: str\n        :return: The hashed value\n        :rtype: str\n\n        \"\"\"\n        return self.hashed_values[hash_value]\n\n    def get_document_id(self, document_id):\n        \"\"\"Get the document id.\n\n        :param document_id: The document id\n        :type document_id: str\n\n        \"\"\"\n        return document_id\n\n    def get", "solution": "\n        if self._unique and hash_value in self._index:\n            raise NonUnique('Hash value {} already in index'.format(hash_value))\n        if store_key not in self._index[hash_value]:\n            self._index[hash_value].append(store_key)\n        if hash_value not in self._reverse_index[store_key]:\n            self._reverse_index[store_key].append(hash_value)"}
{"prompt": "def iuptri(items, diago=True, with_inds=False):\n    \"\"\"\n    A generator that yields the upper triangle of the matrix (items x items)\n\n    Args:\n        items: Iterable object with elements [e0, e1, ...]\n        diago: False if diagonal matrix elements should be excluded\n        with_inds: If True, (i,j) (e_i, e_j) is returned else (e_i, e_j)\n\n    >>> for (ij, mate) in iuptri([0,1], with_inds=True):\n    ...     print(\"ij:\", ij, \"mate:\", mate)\n    ij: (0, 0) mate: (0, 0)\n    ij: (0, 1) mate: (0, 1)\n    ij: (1, 1) mate: (1, 1)\n    \"\"\"", "output": "(ij, mate)\n    0\n    1\n    2\n    3\n    4\n    5\n\n    >>> for (ij, mate) in iuptri([0,1], diago=True):\n   ...     print(ij, mate)\n    0 1\n    2 3\n    4 5\n    0\n    1\n    2 5\n    3\n\n    >>> for (ij, mate) in iuptri([0,1], diago=True, with_inds=True):\n   ...     print(ij, mate)\n    1\n    2\n    3\n    4 5\n    0\n    1\n    2 3\n    4 5\n   ", "solution": "\n    for (ii, item1) in enumerate(items):\n        for (jj, item2) in enumerate(items):\n            do_yield = (jj >= ii) if diago else (jj > ii)\n            if do_yield:\n                if with_inds:\n                    yield (ii, jj), (item1, item2)\n                else:\n                    yield item1, item2"}
{"prompt": "def add(self, effect):\n        \"\"\"\n        Add an LV2 plugin encapsulated as a jack client\n\n        :param Lv2Effect effect: Effect that will be loaded as LV2 plugin encapsulated\n        \"\"\"", "output": "\n        self.add_hook(effect)\n        self.add_hook(self.hook)\n\n    def remove(self, effect):\n        \"\"\"\n        Remove an LV2 plugin encapsulated as a jack client\n\n        :param Lv2Effect effect: Effect that will be loaded as LV2 plugin encapsulated\n        \"\"\"\n        self.remove_hook(effect)\n        self.remove_hook(self.hook)\n\n    def add_hook(self, effect):\n        \"\"\"\n        Add a LV2 hook\n\n        :param Lv2Effect effect: Effect that will be added as a LV2 hook", "solution": "\n        effect.instance = self.instance_index\n        self.instance_index += 1\n\n        self.connection.send(ProtocolParser.add(effect))"}
{"prompt": "def cftime_to_nptime(times):\n    \"\"\"Given an array of cftime.datetime objects, return an array of\n    numpy.datetime64 objects of the same size\"\"\"", "output": "\n    return np.array([t.year, t.month, t.day, t.hour, t.minute, t.second, t.microsecond,\n                    t.tzinfo.tzname(t.tzinfo.get_current_timezone()).astype(np.int64),\n                    t.tzinfo.dst(t.tzinfo.get_current_timezone()).astype(np.int64)])\n\n\ndef npdatetime_to_date(times):\n    \"\"\"Given an array of npdatetime.datetime objects, return an array of\n    datetime.date objects of the same size\"\"\"\n    return np.array([t.", "solution": "\n    times = np.asarray(times)\n    new = np.empty(times.shape, dtype='M8[ns]')\n    for i, t in np.ndenumerate(times):\n        try:\n            # Use pandas.Timestamp in place of datetime.datetime, because\n            # NumPy casts it safely it np.datetime64[ns] for dates outside\n            # 1678 to 2262 (this is not currently the case for\n            # datetime.datetime).\n            dt = pd.Timestamp(t.year, t.month, t.day, t.hour, t.minute,\n                              t.second, t.microsecond)\n        except ValueError as e:\n            raise ValueError('Cannot convert date {} to a date in the '\n                             'standard calendar.  Reason: {}.'.format(t, e))\n        new[i] = np.datetime64(dt)\n    return new"}
{"prompt": "def random(cls, span=1, seed=None):\n        \"\"\"\n        Creates a randomized coordinate system.\n\n        Useful for confirming that an *assembly* does not rely on its\n        origin coordinate system to remain intact.\n\n        For example, the :class:`CoordSysIndicator` *assembly* aligns 3 boxes\n        along each of the :math:`XYZ` axes.\n        Positioning it randomly by setting its ``world_coords`` shows that each\n        box is always positioned orthogonally to the other two.\n\n        .. doctest::\n\n            from cqparts_misc.basic.indicators import CoordSysIndicator\n            from cqparts.display import display\n            from cqparts.utils import CoordSystem\n\n            cs = CoordSysIndicator()\n            cs.world_coords = CoordSystem.random()\n\n            display(cs)  # doctest: +SKIP\n\n\n        :param span: origin of return will be :math:`\\pm span` per axis\n        :param seed: if supplied, return is psudorandom (repeatable)\n        :type seed: hashable object\n        :return: randomized coordinate system\n        :rtype: :class:`CoordSystem`\n        \"\"\"", "output": "SysIndicator\n            from cqparts.utils.random import random\n            from cqparts.utils.math import Math\n            from cqparts.utils.geometry import Geometry\n            from cqparts.utils.math import Vector\n            from cqparts.utils.shapes import Rectangle, Ellipse, Square, SquareShape\n\n            # Test randomization\n            random(1, seed=1)\n            # Test the calculation\n            # Positioning it randomly by setting its ``world_coords`` shows that\n            # each box is always positioned to the origin\n            for i in range(span):\n                for j in range(", "solution": "\n        if seed is not None:\n            random.seed(seed)\n\n        def rand_vect(min, max):\n            return (\n                random.uniform(min, max),\n                random.uniform(min, max),\n                random.uniform(min, max),\n            )\n\n        while True:\n            try:\n                return cls(\n                    origin=rand_vect(-span, span),\n                    xDir=rand_vect(-1, 1),\n                    normal=rand_vect(-1, 1),\n                )\n            except RuntimeError:  # Base.FreeCADError inherits from RuntimeError\n                # Raised if xDir & normal vectors are parallel.\n                # (the chance is very low, but it could happen)\n                continue"}
{"prompt": "def is_removable(self, device):\n        \"\"\"Check if device can be removed with ``auto_remove``.\"\"\"", "output": "\n        return device in self.auto_remove_devices\n\n    def turn_on(self, **kwargs):\n        \"\"\"Turn the device on.\"\"\"\n        if self._turn_on_action:\n            self._turn_on_action()\n\n    def turn_off(self, **kwargs):\n        \"\"\"Turn the device off.\"\"\"\n        if self._turn_off_action:\n            self._turn_off_action()\n\n    @property\n    def is_on(self):\n        \"\"\"Return true if switch is on.\"\"\"\n        return self._turn_on_state\n\n    @property\n    def device_state_attributes(self):\n       ", "solution": "\n        if not self.is_handleable(device):\n            return False\n        if device.is_filesystem:\n            return device.is_mounted\n        if device.is_crypto:\n            return device.is_unlocked\n        if device.is_partition_table or device.is_drive:\n            return any(self.is_removable(dev)\n                       for dev in self.get_all_handleable()\n                       if _is_parent_of(device, dev))\n        return False"}
{"prompt": "def integer(self, x):\n        \"\"\"\n        returns a plain integer\n        \"\"\"", "output": "\n        return int(x)\n\n    def float(self, x):\n        \"\"\"\n        returns a plain float\n        \"\"\"\n        return float(x)\n\n    def int(self, x):\n        \"\"\"\n        returns a plain integer\n        \"\"\"\n        return int(x)\n\n    def bool(self, x):\n        \"\"\"\n        returns a plain boolean\n        \"\"\"\n        return x\n\n    def string(self, x):\n        \"\"\"\n        returns a plain string\n        \"\"\"\n        return str(x)\n\n    def bool_string(self, x):\n        \"\"\"\n        returns a plain boolean\n        \"\"\"\n        return x\n\n    def string_list(self, x):\n       ", "solution": "\n        if type(x) is str:\n            hex = binascii.unhexlify(x)\n            return int.from_bytes(hex, 'big')\n\n        return x.value if isinstance(x, FiniteField.Value) else x"}
{"prompt": "def _O_dot(self, Mt):\n        \"\"\"\n        Mt is dim_r x dim_c x d tensor\n        \"\"\"", "output": "\n        return tensor.dot(Mt, self.W)\n\n    def _O_transpose(self, M):\n        \"\"\"\n        M is the transpose of the matrix A\n        \"\"\"\n        return tensor.transpose(M, axes=(0, 2, 1))\n\n    def _O_solve(self, rhs, x, y):\n        \"\"\"\n        rhs is the solution to the linear system\n        x is the solution to the linear system\n        y is the solution to the linear system\n        \"\"\"\n        raise NotImplementedError\n\n    def _O_trace(self, M):\n        \"\"\"\n        M is the trace of the matrix A\n        \"\"\"\n        raise", "solution": "\n        DMt = self.D()[:, :, sp.newaxis] * Mt\n        WrDMtWc = vei_CoR_veX(DMt, R=self.Wr().T, C=self.Wc().T)\n        ve_WrDMtWc = sp.reshape(WrDMtWc, (WrDMtWc.shape[0] * WrDMtWc.shape[1], Mt.shape[2]), order='F')\n        Hi_ve_WrDMtWc = la.cho_solve((self.H_chol(), True), ve_WrDMtWc)\n        vei_HiveWrDMtWc = Hi_ve_WrDMtWc.reshape(WrDMtWc.shape, order = 'F')\n        Wr_HiveWrDMtWc_Wc = vei_CoR_veX(vei_HiveWrDMtWc, R=self.Wr(), C=self.Wc())\n        DWrHiveWrDMtWcWc = self.D()[:,:,sp.newaxis] * Wr_HiveWrDMtWc_Wc\n        RV = DMt - DWrHiveWrDMtWcWc\n        return RV"}
{"prompt": "def _get_primary_index_in_altered_table(self, diff):\n        \"\"\"\n        :param diff: The table diff\n        :type diff: orator.dbal.table_diff.TableDiff\n\n        :rtype: dict\n        \"\"\"", "output": "\n        return diff.primary_index\n\n    def _get_primary_index_in_table_and_column(self, diff, column):\n        \"\"\"\n        :param diff: The table diff\n        :type diff: orator.dbal.table_diff.TableDiff\n\n        :rtype: dict\n        \"\"\"\n        return orator.dbal.table_diff.get_table_index(diff.table, column)\n\n    def _get_primary_index_in_altered_table_and_row(self, diff, row):\n        \"\"\"\n        :param diff: The table diff\n        :type diff", "solution": "\n        primary_index = {}\n\n        for index in self._get_indexes_in_altered_table(diff).values():\n            if index.is_primary():\n                primary_index = {index.get_name(): index}\n\n        return primary_index"}
{"prompt": "def make_series_url(key):\n    \"\"\"For internal use. Given a series key, generate a valid URL to the series\n    endpoint for that key.\n\n    :param string key: the series key\n    :rtype: string\"\"\"", "output": "\n    return \"%s/api/v1/series/%s/series_json\" % (BASE_URL, key)\n\n\ndef make_series_url_from_id(id):\n    \"\"\"For internal use. Given a series id, generate a valid URL to the series\n    endpoint for that id.\n\n    :param string id: the series id\n    :rtype: string\"\"\"\n    return \"%s/api/v1/series/%s/series_json_id\" % (make_series_url(id), id)\n\n\ndef make_series_url_from_name(name):\n   ", "solution": "\n\n    url = urlparse.urljoin(endpoint.SERIES_ENDPOINT, 'key/')\n    url = urlparse.urljoin(url, urllib.quote(key))\n    return url"}
{"prompt": "def BackAssign(cls,\n                   other_entity_klass,\n                   this_entity_backpopulate_field,\n                   other_entity_backpopulate_field,\n                   is_many_to_one=False):\n        \"\"\"\n        Assign defined one side mapping relationship to other side.\n\n        For example, each employee belongs to one department, then one department\n        includes many employees. If you defined each employee's department,\n        this method will assign employees to ``Department.employees`` field.\n        This is an one to many (department to employee) example.\n\n        Another example would be, each employee has multiple tags. If you defined\n        tags for each employee, this method will assign employees to\n        ``Tag.employees`` field. This is and many to many (employee to tag) example.\n\n        Support:\n\n        - many to many mapping\n        - one to many mapping\n\n        :param other_entity_klass: a :class:`Constant` class.\n        :param this_entity_backpopulate_field: str\n        :param other_entity_backpopulate_field: str\n        :param is_many_to_one: bool\n        :return:\n        \"\"\"", "output": " you\n        defined each employee's tag, this method will assign tags to ``EmployeeTag.tags``\n        field.\n        \"\"\"\n        return cls._back_assign(\n            other_entity_klass,\n            this_entity_backpopulate_field,\n            other_entity_backpopulate_field,\n            is_many_to_one=is_many_to_one\n        )\n\n    @classmethod\n    def _back_assign(cls,\n                   other_entity_klass,\n                   this_entity_backpopulate_field,\n                   this_entity_backpopulate_field,\n                   is_many_to_one=False):", "solution": "\n        data = dict()\n        for _, other_klass in other_entity_klass.Subclasses():\n            other_field_value = getattr(\n                other_klass, this_entity_backpopulate_field)\n            if isinstance(other_field_value, (tuple, list)):\n                for self_klass in other_field_value:\n                    self_key = self_klass.__name__\n                    try:\n                        data[self_key].append(other_klass)\n                    except KeyError:\n                        data[self_key] = [other_klass, ]\n            else:\n                if other_field_value is not None:\n                    self_klass = other_field_value\n                    self_key = self_klass.__name__\n                    try:\n                        data[self_key].append(other_klass)\n                    except KeyError:\n                        data[self_key] = [other_klass, ]\n\n        if is_many_to_one:\n            new_data = dict()\n            for key, value in data.items():\n                try:\n                    new_data[key] = value[0]\n                except:  # pragma: no cover\n                    pass\n            data = new_data\n\n        for self_key, other_klass_list in data.items():\n            setattr(getattr(cls, self_key),\n                    other_entity_backpopulate_field, other_klass_list)"}
{"prompt": "def scale_ps(lat):\n    \"\"\"\n    This function calculates the scaling factor for a polar stereographic\n    projection (ie. SSM/I grid) to correct area calculations. The scaling\n    factor is defined (from Snyder, 1982, Map Projections used by the U.S.\n    Geological Survey) as:\n\n    k = (mc/m)*(t/tc), where:\n\n    m = cos(lat)/sqrt(1 - e2*sin(lat)^2)\n    t = tan(Pi/4 - lat/2)/((1 - e*sin(lat))/(1 + e*sin(lat)))^(e/2)\n    e2 = 0.006693883 is the earth eccentricity (Hughes ellipsoid)\n    e = sqrt(e2)\n    mc = m at the reference latitude (70 degrees)\n    tc = t at the reference latitude (70 degrees)\n\n    The ratio mc/tc is precalculated and stored in the variable m70_t70.\n\n    From Ben Smith PS scale m file (7/12/12)\n    \"\"\"", "output": "**2 + 1)\n\n    where:\n\n    - mc/m is the distance between the x-axis and the stereographic\n      projection and the x-axis of the projection (in degrees)\n    - t is the angle between the stereographic projection and the stereographic\n      projection (in degrees)\n    \"\"\"\n    return math.cos(lat) * math.tan(math.radians(90 - lat))\n\n\ndef scale_map(lat):\n    \"\"\"\n    This function calculates the scaling factor for a map stereographic\n    projection (ie. SSM/I grid) to correct area calculations. The", "solution": "\n    lat = np.array(lat)\n    if np.any(lat > 0):\n        m70_t70 = 1.9332279 \n        #Hack to deal with pole\n        lat[lat>=90.0] = 89.999999999\n    else:\n        # for 71 deg, southern PS  -- checked BS 5/2012\n        m70_t70 = 1.93903005  \n        lat[lat<=-90.0] = -89.999999999\n\n    #for WGS84, a=6378137, 1/f = 298.257223563 -> 1-sqrt(1-e^2) = f\n    #-> 1-(1-f)^2 = e2 =    0.006694379990141\n    #e2 = 0.006693883\n    e2 = 0.006694379990141  # BS calculated from WGS84 parameters 5/2012\n    e = np.sqrt(e2)\n\n    lat = np.abs(np.deg2rad(lat))\n    slat = np.sin(lat)\n    clat = np.cos(lat)\n\n    m = clat/np.sqrt(1. - e2*slat**2)\n    t = np.tan(np.pi/4 - lat/2)/((1. - e*slat)/(1. + e*slat))**(e/2)\n    k = m70_t70*t/m\n\n    scale=(1./k)\n    return scale"}
{"prompt": "def mean_centroid(candidates):\n    \"\"\"Take the mean of all centroids in set of reference triangles.\n\n    .. note::\n\n       This is used **only** as a helper for :func:`locate_point`.\n\n    Args:\n        candidates (List[Tuple[float, float, float, numpy.ndarray]): List of\n            4-tuples, each of which has been produced by :func:`locate_point`.\n            Each 4-tuple contains\n\n            * Three times centroid ``x``-value\n            * Three times centroid ``y``-value\n            * \"Width\" of a parameter space for a surface\n            * Control points for a surface\n\n            We only use the first two values, which are triple the desired\n            value so that we can put off division by three until summing in\n            our average. We don't use the other two values, they are just an\n            artifact of the way ``candidates`` is constructed by the caller.\n\n    Returns:\n        Tuple[float, float]: The mean of all centroids.\n    \"\"\"", "output": " ``x``-value and ``y``-values\n            * Control points ``x``-value and ``y``-values\n            * \"Height\" of a parameter space for a surface\n            * \"Depth\" of a parameter space for a surface\n            * \"Height\" of a surface\n            * \"Depth\" of a parameter space for a surface\n\n    Returns:\n        numpy.ndarray: Mean of all points in the surface.\n\n   .. note::\n\n       This is used **only** as a helper for :func:`locate_point`.\n\n    \"\"\"\n    return np.mean(candidates, axis=0)\n\n\ndef get_surface", "solution": "\n    sum_x = 0.0\n    sum_y = 0.0\n    for centroid_x, centroid_y, _, _ in candidates:\n        sum_x += centroid_x\n        sum_y += centroid_y\n    denom = 3.0 * len(candidates)\n    return sum_x / denom, sum_y / denom"}
{"prompt": "def _check_bounds(x, bounds):\n    \"\"\"\n    Checks whether `x` is within `bounds`. JIT-compiled in `nopython` mode\n    using Numba.\n\n    Parameters\n    ----------\n    x : ndarray(float, ndim=1)\n        1-D array with shape (n,) of independent variables.\n\n    bounds: ndarray(float, ndim=2)\n        Sequence of (min, max) pairs for each element in x.\n\n    Returns\n    ----------\n    bool\n        `True` if `x` is within `bounds`, `False` otherwise.\n\n    \"\"\"", "output": "\n    if len(x)!= len(bounds):\n        return False\n    for i, (min, max) in enumerate(zip(bounds, x)):\n        if not np.all(min <= i) and not np.all(max >= i):\n            return False\n    return True\n\n\ndef _check_bounds_shape(x, bounds):\n    \"\"\"\n    Checks whether `x` has the right shape.\n\n    Parameters\n    ----------\n    x : ndarray(float, ndim=1)\n        1-D array with shape (n,) of independent variables.\n\n    bounds: ndarray(float, ndim=2)\n        Sequence", "solution": "\n    if bounds.shape == (0, 2):\n        return True\n    else:\n        return ((np.atleast_2d(bounds)[:, 0] <= x).all() and\n                (x <= np.atleast_2d(bounds)[:, 1]).all())"}
{"prompt": "def set_tare(self, tare):\n        \"\"\"\n        :param tare: weight of tank as printed on can\n        :return: nothing\n        tare is not set in desired state, but on the main device.\n        \"\"\"", "output": "\n        if tare == None:\n            return\n        if tare == None:\n            self.write_error(\"Invalid weight\")\n            return\n        self.write_line(\"TANK: %s\" % tare)\n        self.write_line(\"WEIGHT: %s\" % self.get_tare(tare))\n\n    def get_tare(self, tare):\n        \"\"\"\n        :param tare: weight of tank as printed on can\n        :return: weight of tank as printed on the main device.\n        \"\"\"\n        if tare == None:\n            return\n        if tare == None", "solution": "\n        response = self.api_interface.set_device_state(self, {\"tare\": tare})\n        self._update_state_from_response(response)"}
{"prompt": "def _get_label(placeholder, label_mapping, label_i):\n    \"\"\"Helper function to either get the appropriate label for a given placeholder or generate\n    a new label and update the mapping.\n\n    See :py:func:`instantiate_labels` for usage.\n    \"\"\"", "output": "\n    if placeholder is None:\n        return None\n    if isinstance(placeholder, dict):\n        return generate_label(placeholder, label_mapping, label_i)\n    elif isinstance(placeholder, str):\n        return generate_label(placeholder, label_mapping, label_i)\n    else:\n        raise TypeError(\"Unknown type of label: {}\".format(type(placeholder)))\n\n\ndef generate_label(placeholder, label_mapping, label_i):\n    \"\"\"Generate a label for a given placeholder.\n\n    :param placeholder: The name of the placeholder.\n    :param label_mapping: A mapping from placeholder names to the", "solution": "\n    if placeholder in label_mapping:\n        return label_mapping[placeholder], label_mapping, label_i\n\n    new_target = Label(\"{}{}\".format(placeholder.prefix, label_i))\n    label_i += 1\n    label_mapping[placeholder] = new_target\n    return new_target, label_mapping, label_i"}
{"prompt": "def set_alias(cls, domain, login, aliases):\n        \"\"\"Update aliases on a mailbox.\"\"\"", "output": "\n        return cls._simple_command('SET', domain, login, aliases)\n\n    @classmethod\n    def setacl(cls, domain, acl):\n        \"\"\"Set the ACL on a mailbox.\"\"\"\n        return cls._simple_command('SETACL', domain, acl)\n\n    @classmethod\n    def setquota(cls, domain, mailbox, value):\n        \"\"\"Set the quota on a mailbox.\"\"\"\n        return cls._simple_command('SETQUOTA', domain, mailbox, value)\n\n    @classmethod\n    def setretr(cls, domain, mailbox, retr):\n        \"\"\"Set the retr on a mailbox.\"\"\"\n        return cls", "solution": "\n        return cls.call('domain.mailbox.alias.set', domain, login, aliases)"}
{"prompt": "def validate():\n    \"\"\"Validate the Swagger/OpenAPI specification with your config\"\"\"", "output": "\n    if not request.json:\n        abort(400, \"Missing request body\")\n    if not request.data:\n        abort(400, \"Missing request body\")\n    if not request.headers.get('Content-Type', None):\n        abort(400, \"Missing Content-Type header\")\n    if not request.headers.get('Accept', None):\n        abort(400, \"Missing Accept header\")\n    if not request.headers.get('Authorization', None):\n        abort(400, \"Missing Authorization header\")\n\n\ndef get_user_from_token(token):\n    \"\"\"Get the user object from the", "solution": "\n    with current_app.test_request_context():\n        schema = json.loads(json.dumps(api.__schema__))\n    try:\n        schemas.validate(schema)\n        success('API specifications are valid')\n    except schemas.SchemaValidationError as e:\n        exit_with_error('API specifications are not valid', e)"}
{"prompt": "def get_string(self):\n        \"\"\" Get file format as string\n\n        :return: String describing the file format\n        :rtype: str\n        \"\"\"", "output": "\n        return self.file_format\n\n    @property\n    def name(self):\n        \"\"\" Get file name\n\n        :return: Name of the file\n        :rtype: str\n        \"\"\"\n        return self.file_name\n\n    @property\n    def path(self):\n        \"\"\" Get file path\n\n        :return: Path of the file\n        :rtype: str\n        \"\"\"\n        return self.file_path\n\n    @property\n    def size(self):\n        \"\"\" Get file size\n\n        :return: Size of the file\n        :rtype: int\n        \"\"\"\n        return self.file_size\n\n    @property\n    def md5(self):\n        \"\"\" Get", "solution": "\n        if self in [MimeType.TIFF_d8, MimeType.TIFF_d16, MimeType.TIFF_d32f]:\n            return 'image/{}'.format(self.value)\n        if self is MimeType.JP2:\n            return 'image/jpeg2000'\n        if self in [MimeType.RAW, MimeType.REQUESTS_RESPONSE]:\n            return self.value\n        return mimetypes.types_map['.' + self.value]"}
{"prompt": "def closing_plugin(self, cancelable=False):\r\n        \"\"\"Perform actions before parent main window is closed.\"\"\"", "output": "\r\n        if cancelable:\r\n            self.close_action.setEnabled(False)\r\n        else:\r\n            self.close_action.setEnabled(True)\r\n\r\n    def set_plugin_state(self, state):\r\n        \"\"\"Set the plugin state.\"\"\"\r\n        self.plugin_state = state\r\n        self.plugin_name = state.plugin_name\r\n\r\n    def set_plugin_name(self, name):\r\n        \"\"\"Set the plugin name.\"\"\"\r\n        self.plugin_name = name\r\n        self.plugin_state = state.plugin_state\r\n\r\n    def set_plugin_state_from_text(self, text):\r\n        \"\"\"Set the", "solution": "\r\n        for cl in self.clients:\r\n            cl.close()\r\n        self.set_option('recent_notebooks', self.recent_notebooks)\r\n        return True"}
{"prompt": "def lookup(source, keys, fallback = None):\n  \"\"\"Traverses the source, looking up each key.  Returns None if can't find anything instead of raising an exception.\"\"\"", "output": "\n  for key in keys:\n    if key in source:\n      return key\n    elif fallback is not None:\n      return fallback\n    else:\n      if source[key] == None:\n        return None\n      elif source[key] == '':\n        return None\n      else:\n        return lookup(source[key], keys, fallback)\n  return None\n\ndef get_all_keys(source):\n  \"\"\"Returns a list of all keys in the source.\"\"\"\n  return [lookup(x, keys, x) for x in lookup(source, ['key', 'key', 'key', 'key', 'key', 'key',", "solution": "\n  try:\n    for key in keys:\n      source = source[key]\n    return source\n  except (KeyError, AttributeError, TypeError):\n    return fallback"}
{"prompt": "def _indicator_multitraj(ss, i, j):\n    \"\"\" Returns conditional sequence for transition i -> j given all conditional sequences \"\"\"", "output": "\n    if i == j:\n        return ss[j]\n    else:\n        return [ss[i], ss[j]]\n\ndef _get_transition_matrix(ss, i, j):\n    \"\"\" Returns conditional matrix for transition i -> j given all conditional sequences \"\"\"\n    if i == j:\n        return ss[j]\n    else:\n        return [ss[i], ss[j]]\n\ndef _get_transition_matrix_from_file(filename):\n    \"\"\" Returns the transition matrix from a file \"\"\"\n    with open(filename) as f:\n        ss = f.read()\n    return ss", "solution": "\n    iseqs = ss[i]\n    res = []\n    for iseq in iseqs:\n        x = np.zeros(len(iseq))\n        I = np.where(iseq == j)\n        x[I] = 1.0\n        res.append(x)\n    return res"}
{"prompt": "def valid_value(value, quote=default_cookie_quote, unquote=default_unquote):\n    \"\"\"Validate a cookie value string.\n\n    This is generic across quote/unquote functions because it directly verifies\n    the encoding round-trip using the specified quote/unquote functions.\n    So if you use different quote/unquote functions, use something like this\n    as a replacement for valid_value::\n\n        my_valid_value = lambda value: valid_value(value, quote=my_quote,\n                                                          unquote=my_unquote)\n    \"\"\"", "output": "\n    if value is None:\n        return default_unquote(quote, unquote)\n    if not value:\n        return default_cookie_quote(u'', u'')\n    # This is the old version which used to be supported\n    # I've seen this happening because I don't know what it means for now.\n    # I'm not sure if I can use the new version.\n    #    old_version = None\n    #    try:\n    #        old_version = parse_cookie(value)\n    #        if old_version is None:\n    #            return default_unquote(my_valid_value('", "solution": "\n    if value is None:\n        return False\n\n    # Put the value through a round trip with the given quote and unquote\n    # functions, so we will know whether data will get lost or not in the event\n    # that we don't complain.\n    encoded = encode_cookie_value(value, quote=quote)\n    decoded = parse_string(encoded, unquote=unquote)\n\n    # If the original string made the round trip, this is a valid value for the\n    # given quote and unquote functions. Since the round trip can generate\n    # different unicode forms, normalize before comparing, so we can ignore\n    # trivial inequalities.\n    decoded_normalized = (normalize(\"NFKD\", decoded)\n                          if not isinstance(decoded, bytes) else decoded)\n    value_normalized = (normalize(\"NFKD\", value)\n                        if not isinstance(value, bytes) else value)\n    if decoded_normalized == value_normalized:\n        return True\n    return False"}
{"prompt": "def atlas_peer_download_zonefile_inventory( my_hostport, peer_hostport, maxlen, bit_offset=0, timeout=None, peer_table={} ):\n    \"\"\"\n    Get the zonefile inventory from the remote peer\n    Start from the given bit_offset\n\n    NOTE: this doesn't update the peer table health by default;\n    you'll have to explicitly pass in a peer table (i.e. setting\n    to {} ensures that nothing happens).\n    \"\"\"", "output": "\n    if timeout is None:\n        timeout = 0\n    if timeout > 0:\n        timeout = int(timeout)\n    if bit_offset > 0:\n        bit_offset = int(bit_offset)\n    if timeout > maxlen:\n        maxlen = int(maxlen)\n    if timeout > 0:\n        timeout = int(timeout)\n    if not timeout:\n        timeout = 0\n    if timeout > 0:\n        timeout = int(timeout)\n    if timeout > 0:\n        my_hostport = int(my_hostport)\n        peer_hostport = int(peer_hostport)\n        if not atlas_", "solution": "\n\n    if timeout is None:\n        timeout = atlas_inv_timeout()\n\n    interval = 524288       # number of bits in 64KB\n    peer_inv = \"\"\n\n    log.debug(\"Download zonefile inventory %s-%s from %s\" % (bit_offset, maxlen, peer_hostport))\n\n    if bit_offset > maxlen:\n        # synced already\n        return peer_inv\n\n    for offset in xrange( bit_offset, maxlen, interval):\n        next_inv = atlas_peer_get_zonefile_inventory_range( my_hostport, peer_hostport, offset, interval, timeout=timeout, peer_table=peer_table )\n        if next_inv is None:\n            # partial failure\n            log.debug(\"Failed to sync inventory for %s from %s to %s\" % (peer_hostport, offset, offset+interval))\n            break\n\n        peer_inv += next_inv\n        if len(next_inv) < interval:\n            # end-of-interval\n            break\n\n    return peer_inv"}
{"prompt": "def get(self, pid, record, **kwargs):\n        \"\"\"Get a record.\n\n        Permissions: ``read_permission_factory``\n\n        Procedure description:\n\n        #. The record is resolved reading the pid value from the url.\n\n        #. The ETag and If-Modifed-Since is checked.\n\n        #. The HTTP response is built with the help of the link factory.\n\n        :param pid: Persistent identifier for record.\n        :param record: Record object.\n        :returns: The requested record.\n        \"\"\"", "output": "\n        return self._get_link(pid, record, **kwargs)\n\n    def _get_link(self, pid, record, **kwargs):\n        \"\"\"Get a record.\n\n        Permissions: ``read_permission_factory``\n\n        Procedure description:\n\n        #. The url is resolved using the record.\n\n        :param pid: Persistent identifier for record.\n        :param record: Record object.\n        :returns: The requested record.\n        \"\"\"\n        return record.links.filter(\n            id=pid,\n            record_id=record.id,\n            record_type=record.type,\n            status=record.", "solution": "\n        etag = str(record.revision_id)\n        self.check_etag(str(record.revision_id))\n        self.check_if_modified_since(record.updated, etag=etag)\n\n        return self.make_response(\n            pid, record, links_factory=self.links_factory\n        )"}
{"prompt": "def _dictload(l: Loader, value, type_) -> Dict:\n    \"\"\"\n    This loads into something like Dict[str,str]\n\n    Recursively loads both keys and values.\n    \"\"\"", "output": "\n    if isinstance(value, dict):\n        return _dictload(value, l, type_)\n    elif isinstance(value, list):\n        return _listload(value, l)\n    elif isinstance(value, str):\n        return _strload(l, value)\n    elif isinstance(value, str):\n        return _strload(value, value)\n    elif isinstance(value, str):\n        return _strload(value, value)\n    elif isinstance(value, str):\n        return _strload(value, value)\n    elif isinstance(value, str):\n        return _strload(value, value)", "solution": "\n    key_type, value_type = type_.__args__\n    try:\n        return {\n            l.load(k, key_type, annotation=Annotation(AnnotationType.KEY, k)): l.load(v, value_type, annotation=Annotation(AnnotationType.VALUE, v))\n            for k, v in value.items()}\n    except AttributeError as e:\n        raise TypedloadAttributeError(str(e), type_=type_, value=value)"}
{"prompt": "def parse_json(json_file):\n    \"\"\"Parse a whole json record from the given file.\n\n    Return None if the json file does not exists or exception occurs.\n\n    Args:\n        json_file (str): File path to be parsed.\n\n    Returns:\n        A dict of json info.\n    \"\"\"", "output": "\n    with open(json_file, 'r') as f:\n        return json.load(f)\n\n\ndef main():\n    \"\"\"Main function.\n\n    Args:\n        None.\n    \"\"\"\n    parser = argparse.ArgumentParser(description='Generate a JSON file.')\n    parser.add_argument('--output', '-o', type=str, default='output.json',\n                        help='Output file path. Defaults to output.json.')\n    parser.add_argument('--output-format', '-f', type=str, default='json',\n                        help='Output file format. Defaults to json.')\n    parser.add_argument", "solution": "\n    if not os.path.exists(json_file):\n        return None\n\n    try:\n        with open(json_file, \"r\") as f:\n            info_str = f.readlines()\n            info_str = \"\".join(info_str)\n            json_info = json.loads(info_str)\n            return unicode2str(json_info)\n    except BaseException as e:\n        logging.error(e.message)\n        return None"}
{"prompt": "def get_gradebook_lookup_session(self, proxy):\n        \"\"\"Gets the OsidSession associated with the gradebook lookup service.\n\n        arg:    proxy (osid.proxy.Proxy): a proxy\n        return: (osid.grading.GradebookLookupSession) - a\n                ``GradebookLookupSession``\n        raise:  NullArgument - ``proxy`` is ``null``\n        raise:  OperationFailed - unable to complete request\n        raise:  Unimplemented - ``supports_gradebook_lookup() is false``\n        *compliance: optional -- This method must be implemented if\n        ``supports_gradebook_lookup()`` is true.*\n\n        \"\"\"", "output": "_gradebook_lookup()`` is true.*\n\n        \"\"\"\n        return # osid.grading.GradebookLookupSession\n\n    gradebook_lookup_session = property(fget=get_gradebook_lookup_session)\n\n    def get_gradebook_form_for_create(self, gradebook_form_id):\n        \"\"\"Gets the gradebook form for creating new gradebook.\n\n        arg:    gradebook_form_id (osid.id.Id): the ``Id`` of the\n                ``Gradebook``\n        return: (osid.grading.GradebookForm) - the gradebook", "solution": "\n        if not self.supports_gradebook_lookup():\n            raise errors.Unimplemented()\n        # pylint: disable=no-member\n        return sessions.GradebookLookupSession(proxy=proxy, runtime=self._runtime)"}
{"prompt": "def from_jd(jd):\n    \"\"\"Calculate Mayan long count from Julian day\"\"\"", "output": "\n    return int(jd - jd.utcoffset()) * 86400 + int(jd.dst() / 86400)\n\ndef from_jd_date(jd):\n    \"\"\"Calculate Mayan date from Julian day\"\"\"\n    return datetime.date(jd - jd.utcoffset() * 86400, jd.dst() // 86400, jd.utcoffset() % 86400)\n\ndef from_jd_time(jd):\n    \"\"\"Calculate Mayan time from Julian day\"\"\"\n    return jd - jd.utcoffset(), jd - jd.utcoffset()\n\ndef from_jd_datetime(jd):", "solution": "\n    d = jd - EPOCH\n    baktun = trunc(d / 144000)\n    d = (d % 144000)\n    katun = trunc(d / 7200)\n    d = (d % 7200)\n    tun = trunc(d / 360)\n    d = (d % 360)\n    uinal = trunc(d / 20)\n    kin = int((d % 20))\n\n    return (baktun, katun, tun, uinal, kin)"}
{"prompt": "def _write(self, session, openFile, replaceParamFile):\n        \"\"\"\n        Raster Map File Write to File Method\n        \"\"\"", "output": "\n        if self.is_local:\n            self.file.write(openFile.read())\n        else:\n            self.file.write(openFile.read().replace(replaceParamFile, paramFile))\n        self.file.write(openFile.read())\n\n    def _write_header(self, session, header, data):\n        \"\"\"\n        Write the header to the file\n        \"\"\"\n        if self.is_local:\n            self.file.write(header)\n        else:\n            self.file.write(data)\n\n    def _write_data(self, session, openFile, replaceParamFile", "solution": "\n        # If the raster field is not empty, write from this field\n        if self.raster is not None:\n            # Configure RasterConverter\n            converter = RasterConverter(session)\n\n            # Use MapKit RasterConverter to retrieve the raster as a GRASS ASCII Grid\n            grassAsciiGrid = converter.getAsGrassAsciiRaster(rasterFieldName='raster',\n                                                             tableName=self.__tablename__,\n                                                             rasterIdFieldName='id',\n                                                             rasterId=self.id)\n            # Write to file\n            openFile.write(grassAsciiGrid)\n\n        elif self.rasterText is not None:\n            # Write file\n            openFile.write(self.rasterText)"}
{"prompt": "def object_from_string(name):\n    \"\"\"Creates a Python class or function from its fully qualified name.\n\n    :param name: A fully qualified name of a class or a function. In Python 3 this\n        is only allowed to be of text type (unicode). In Python 2, both bytes and unicode\n        are allowed.\n    :return: A function or class object.\n\n    This method is used by serialization code to create a function or class\n    from a fully qualified name.\n\n    \"\"\"", "output": "\n    if isinstance(name, bytes):\n        name = name.decode('utf-8')\n    if isinstance(name, str):\n        name = name.split('.')[-1]\n    return getattr(sys.modules[__name__], name)\n\n\ndef _get_callable_name(callable_):\n    \"\"\"Returns the fully qualified callable name of a callable.\"\"\"\n    if isinstance(callable_, types.FunctionType):\n        return callable_.__name__\n    return callable_.__class__.__name__\n\n\ndef _get_function_code(callable_):\n    \"\"\"Returns the code object for a callable.\"\"\"\n    if isinstance(callable_, types", "solution": "\n    if six.PY3:\n        if not isinstance(name, str):\n            raise TypeError(\"name must be str, not %r\" % type(name))\n    else:\n        if isinstance(name, unicode):\n            name = name.encode(\"ascii\")\n        if not isinstance(name, (str, unicode)):\n            raise TypeError(\"name must be bytes or unicode, got %r\" % type(name))\n\n    pos = name.rfind(\".\")\n    if pos < 0:\n        raise ValueError(\"Invalid function or class name %s\" % name)\n    module_name = name[:pos]\n    func_name = name[pos + 1 :]\n    try:\n        mod = __import__(module_name, fromlist=[func_name], level=0)\n    except ImportError:\n        # Hail mary. if the from import doesn't work, then just import the top level module\n        # and do getattr on it, one level at a time. This will handle cases where imports are\n        # done like `from . import submodule as another_name`\n        parts = name.split(\".\")\n        mod = __import__(parts[0], level=0)\n        for i in range(1, len(parts)):\n            mod = getattr(mod, parts[i])\n        return mod\n\n    else:\n        return getattr(mod, func_name)"}
{"prompt": "def adjust_for_length(self, key, r, kwargs):\n        \"\"\"\n        Converts the response to a string and compares its length to a max\n        length specified in settings. If the response is too long, an error is\n        logged, and an abbreviated response is returned instead.\n        \"\"\"", "output": "\n        if len(r) > 1000:\n            raise ValueError(\"The length of the response is too long to fit in a single query\")\n        if len(r) < 1:\n            raise ValueError(\"The length of the response is too short to fit in a single query\")\n        return self.connection.ops.convert_length(key, r)\n\n    def get_for_update(self, key, value):\n        \"\"\"\n        Returns the max length for the given key.\n        \"\"\"\n        if len(value) > 1000:\n            raise ValueError(\"The length of the response is too long to fit in a single update\")", "solution": "\n        length = len(str(kwargs))\n        if length > settings.defaults[\"max_detail_length\"]:\n            self._log_length_error(key, length)\n            r[\"max_detail_length_error\"] = length\n            return r\n        return kwargs"}
{"prompt": "def heapreplace(heap, item):\n    \"\"\"Pop and return the current smallest value, and add the new item.\n\n    This is more efficient than heappop() followed by heappush(), and can be\n    more appropriate when using a fixed-size heap.  Note that the value\n    returned may be larger than item!  That constrains reasonable uses of\n    this routine unless written as part of a conditional replacement:\n\n        if item > heap[0]:\n            item = heapreplace(heap, item)\n    \"\"\"", "output": "\n\n    def merge(heap, item):\n        if heap[0] < item:\n            return heap[0], heap[0] + 1\n        elif heap[0] > item:\n            return heap[0] - 1, heap[0]\n\n    def heappush(heap, item):\n        heappush(heap, item, item)\n        heappushpop(heap, item)\n\n    def heappop(heap):\n        \"\"\"\n        Merge the elements of a heap object into a single heappop.\n\n        This is a heappushed version of a heap.  It is similar to the 2-", "solution": "\n    returnitem = heap[0]    # raises appropriate IndexError if heap is empty\n    heap[0] = item\n    _siftup(heap, 0)\n    return returnitem"}
{"prompt": "def Close(self, abort=False):\n    \"\"\"Closes the queue.\n\n    Args:\n      abort (Optional[bool]): whether the Close is the result of an abort\n          condition. If True, queue contents may be lost.\n\n    Raises:\n      QueueAlreadyClosed: if the queue is not started, or has already been\n          closed.\n      RuntimeError: if closed or terminate event is missing.\n    \"\"\"", "output": "\n    if abort:\n      self.queue.abort()\n    self.closed = True\n    self.queue.close()\n\n  def Add(self, item):\n    \"\"\"Adds an item to the queue.\n\n    Args:\n      item (object): an item to add to the queue.\n\n    Raises:\n      ValueError: if the item is not a string.\n      TypeError: if the item is not a class.\n      ValueError: if the item is not a string.\n      TypeError: if the item is not a class.\n      ValueError: if the item is not a string.\n    \"\"\"\n    if not isinstance(item, basestring):\n      raise", "solution": "\n    if not self._closed_event or not self._terminate_event:\n      raise RuntimeError('Missing closed or terminate event.')\n\n    if not abort and self._closed_event.is_set():\n      raise errors.QueueAlreadyClosed()\n\n    self._closed_event.set()\n\n    if abort:\n      if not self._closed_event.is_set():\n        logger.warning(\n            '{0:s} queue aborting. Contents may be lost.'.format(self.name))\n\n      self._linger_seconds = 0\n\n      # We can't determine whether a there might be an operation being performed\n      # on the socket in a separate method or thread, so we'll signal that any\n      # such operation should cease.\n      self._terminate_event.set()\n\n    else:\n      logger.debug(\n          '{0:s} queue closing, will linger for up to {1:d} seconds'.format(\n              self.name, self._linger_seconds))"}
{"prompt": "def get_classname(class_, local=False):\n    r\"\"\"\n    Args:\n        class_ (type):\n        local (bool): (default = False)\n\n    Returns:\n        str: classname\n\n    CommandLine:\n        python -m utool.util_class --exec-get_classname --show\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_class import *  # NOQA\n        >>> import utool as ut\n        >>> class_ = ReloadingMetaclass\n        >>> local = False\n        >>> assert get_classname(class_, local) == 'utool.util_class.ReloadingMetaclass'\n        >>> assert get_classname(class_, local=True) == 'ReloadingMetaclass'\n    \"\"\"", "output": "class.ReloadingMetaclass'\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_class import *  # NOQA\n        >>> import utool.util.testing as tm\n        >>> # build test data\n        >>> data = TestData()\n        >>> data.add_reloading_class(ReloadingClass)\n        >>> data.add_reloading_data(data)\n        >>> # execute\n        >>> result = data.execute()\n        >>> assert result.shape == data.shape\n        >>> assert np.all(result == data)\n        >>> # verify\n        >>> assert np.all(result ==", "solution": "\n    if not local:\n        classname = class_.__module__ + '.' + class_.__name__\n    else:\n        classname = class_.__name__\n    return classname"}
{"prompt": "def _replace_global_vars(xs, global_vars):\n    \"\"\"Replace globally shared names from input header with value.\n\n    The value of the `algorithm` item may be a pointer to a real\n    file specified in the `global` section. If found, replace with\n    the full value.\n    \"\"\"", "output": "\n    if isinstance(xs, str):\n        return xs.replace('\\0', '')\n    else:\n        return os.path.abspath(os.path.join(os.path.dirname(xs), algorithm))\n\n\ndef _get_c_sources(sources, c_sources):\n    \"\"\"Return a list of C source files, given a list of C source files\n    list.\"\"\"\n    if not sources:\n        return c_sources\n    elif isinstance(sources[0], str):\n        return c_sources[sources].replace('\\0', '')\n    else:\n        return [sources] + c_sources\n\n\nclass", "solution": "\n    if isinstance(xs, (list, tuple)):\n        return [_replace_global_vars(x) for x in xs]\n    elif isinstance(xs, dict):\n        final = {}\n        for k, v in xs.items():\n            if isinstance(v, six.string_types) and v in global_vars:\n                v = global_vars[v]\n            final[k] = v\n        return final\n    else:\n        return xs"}
{"prompt": "def import_vmdk(self):\n        \"\"\"\n        All actions necessary to import vmdk (calls s3 upload, and import to aws ec2)\n        :param vmdk_location: location of vmdk to import. Can be provided as a string, or the result output of fabric\n        execution\n        :return:\n        \"\"\"", "output": "\n        if self.is_action_import_vmdk_or_s3_upload():\n            self.logger.info(\"Importing vmdk or s3 upload\")\n            self.import_vmdk()\n            self.logger.info(\"Importing s3 upload\")\n            self.import_s3()\n        else:\n            self.logger.info(\"Skipping import vmdk or s3 upload\")\n\n    def is_action_import_vmdk_or_s3_upload(self):\n        \"\"\"\n        Check if the action is import to AWS EC2 or S3.\n        :return:\n        \"\"\"\n       ", "solution": "\n        # Set the inital upload to be the first region in the list\n        first_upload_region = self.aws_regions[0]\n\n        print \"Initial AMI will be created in: {}\".format(first_upload_region)\n        self.upload_to_s3(region=first_upload_region)\n        # If the upload was successful, the name to reference for import is now the basename\n        description = \"AMI upload of: {}\".format(os.path.basename(self.upload_file))\n        temp_fd, file_location = self.create_config_file(os.path.basename(self.upload_file), description)\n        import_id = self.run_ec2_import(file_location, description, first_upload_region)\n        self.wait_for_import_to_complete(import_id)\n        self.rename_image(import_id, self.ami_name, source_region=first_upload_region)\n        return import_id"}
{"prompt": "def _input_as_lines(self,data):\n        \"\"\"Writes data to tempfile and sets -infile parameter\n\n        data -- list of lines, ready to be written to file\n        \"\"\"", "output": "\n        if self.infile:\n            self.infile.write(data)\n        self.infile.write('\\n'.join(data))\n        self.infile.flush()\n\n    def _output_as_lines(self,data):\n        \"\"\"Writes data to tempfile and sets -outfile parameter\n\n        data -- list of lines, ready to be written to file\n        \"\"\"\n        if self.outfile:\n            self.outfile.write(data)\n        self.outfile.flush()\n\n    def _get_file_path(self,filename):\n        \"\"\"Returns the full path to the file\n\n        filename -- full path to the file\n        \"\"\"", "solution": "\n        if data:\n            self.Parameters['-infile']\\\n                .on(super(Clustalw,self)._input_as_lines(data))\n        return ''"}
{"prompt": "def makeSoftwareVersion(store, version, systemVersion):\n    \"\"\"\n    Return the SoftwareVersion object from store corresponding to the\n    version object, creating it if it doesn't already exist.\n    \"\"\"", "output": "\n    if not store.hasFile(version.filename):\n        return None\n    return store.getFile(version.filename)\n\n\ndef makePythonVersion(pythonVersion):\n    \"\"\"\n    Return the PythonVersion object from the given PythonVersion,\n    creating it if it doesn't already exist.\n    \"\"\"\n    if not pythonVersion.isPython:\n        return None\n    return pythonVersion.getVersion()\n\n\ndef makePythonVersion(store, version, pythonVersion):\n    \"\"\"\n    Return the PythonVersion object from the given PythonVersion,\n    creating it if it doesn't already exist.\n    \"\"\"\n    if not store.has", "solution": "\n    return store.findOrCreate(SoftwareVersion,\n                              systemVersion=systemVersion,\n                              package=unicode(version.package),\n                              version=unicode(version.short()),\n                              major=version.major,\n                              minor=version.minor,\n                              micro=version.micro)"}
{"prompt": "def add_vcenter(self, **kwargs):\n        \"\"\"\n        Add vCenter on the switch\n\n        Args:\n            id(str) : Name of an established vCenter\n            url (bool) : vCenter URL\n            username (str): Username of the vCenter\n            password (str): Password of the vCenter\n            callback (function): A function executed upon completion of the\n                 method.\n\n        Returns:\n           Return value of `callback`.\n\n        Raises:\n            None\n        \"\"\"", "output": "\n        return self.rest.add_vcenter(id, **kwargs)\n\n    def delete_vcenter(self, **kwargs):\n        \"\"\"\n        Delete vCenter on the switch\n\n        Args:\n            id(str) : Name of an established vCenter\n            url (bool) : vCenter URL\n            username (str): Username of the vCenter\n            password (str): Password of the vCenter\n            callback (function): A function executed upon completion of the\n                 method.\n\n        Returns:\n           Return value of `callback`.\n\n        Raises:\n            None\n        \"\"\"\n        return self.rest.delete_vcenter(id, **kwargs", "solution": "\n        config = ET.Element(\"config\")\n        vcenter = ET.SubElement(config, \"vcenter\",\n                                xmlns=\"urn:brocade.com:mgmt:brocade-vswitch\")\n        id = ET.SubElement(vcenter, \"id\")\n        id.text = kwargs.pop('id')\n        credentials = ET.SubElement(vcenter, \"credentials\")\n        url = ET.SubElement(credentials, \"url\")\n        url.text = kwargs.pop('url')\n        username = ET.SubElement(credentials, \"username\")\n        username.text = kwargs.pop('username')\n        password = ET.SubElement(credentials, \"password\")\n        password.text = kwargs.pop('password')\n\n        try:\n            self._callback(config)\n            return True\n\n        except Exception as error:\n            logging.error(error)\n            return False"}
{"prompt": "def is_correct(self):\n        \"\"\"Check if this object configuration is correct ::\n\n        * Check our own specific properties\n        * Call our parent class is_correct checker\n\n        :return: True if the configuration is correct, otherwise False\n        :rtype: bool\n        \"\"\"", "output": "\n        return self._is_correct\n\n    @property\n    def is_correct_type(self):\n        \"\"\"Check if this object configuration is correct ::\n\n        * Check our own specific properties\n        * Call our parent class is_correct checker\n\n        :return: True if the configuration is correct, otherwise False\n        :rtype: bool\n        \"\"\"\n        return self._is_correct_type\n\n    @property\n    def is_correct_value(self):\n        \"\"\"Check if this object configuration is correct ::\n\n        * Check our own specific properties\n        * Call our parent class is_correct checker\n\n        :return: True if the configuration is correct, otherwise", "solution": "\n        state = True\n\n        # Internal checks before executing inherited function...\n        if not hasattr(self, 'check_command'):\n            msg = \"[checkmodulation::%s] do not have any check_command defined\" % (\n                self.get_name()\n            )\n            self.add_error(msg)\n            state = False\n        else:\n            if self.check_command is None:\n                msg = \"[checkmodulation::%s] a check_command is missing\" % (self.get_name())\n                self.add_error(msg)\n                state = False\n            if self.check_command and not self.check_command.is_valid():\n                msg = \"[checkmodulation::%s] a check_command is invalid\" % (self.get_name())\n                self.add_error(msg)\n                state = False\n\n        # Ok just put None as check_period, means 24x7\n        if not hasattr(self, 'check_period'):\n            self.check_period = None\n\n        return super(CheckModulation, self).is_correct() and state"}
{"prompt": "def get_attribute_classes() -> Dict[str, Attribute]:\n    \"\"\"\n    Lookup all builtin Attribute subclasses, load them, and return a dict\n    \"\"\"", "output": "\n    return {\n        attr.name: attr.value\n        for attr in Attribute.attributes\n        if isinstance(attr, Attribute)\n    }\n\n\ndef get_attribute_class(name: str) -> Attribute:\n    \"\"\"\n    Lookup a builtin attribute by name, load it, and return a Value\n    \"\"\"\n    return get_attribute_classes()[name]\n\n\ndef get_attribute_value(name: str, default: Any = None) -> Any:\n    \"\"\"\n    Lookup a value by name, load it, and return the value of the attribute\n    \"\"\"\n    return get_attribute_value(name, default,", "solution": "\n    attribute_children = pkgutil.iter_modules(\n        importlib.import_module('jawa.attributes').__path__,\n        prefix='jawa.attributes.'\n    )\n\n    result = {}\n    for _, name, _ in attribute_children:\n        classes = inspect.getmembers(\n            importlib.import_module(name),\n            lambda c: (\n                inspect.isclass(c) and issubclass(c, Attribute) and\n                c is not Attribute\n            )\n        )\n\n        for class_name, class_ in classes:\n            attribute_name = getattr(class_, 'ATTRIBUTE_NAME', class_name[:-9])\n            result[attribute_name] = class_\n\n    return result"}
{"prompt": "def set_as_environment_variables(self, override=False):\n        \"\"\"\n        Load the current dotenv as system environemt variable.\n        \"\"\"", "output": "\n        if override:\n            self.dotenv.update(override_env(self.env))\n        else:\n            self.dotenv.update(self.env)\n\n    def get_as_dict(self):\n        \"\"\"\n        Return the current environment as a dict.\n        \"\"\"\n        return self.dotenv\n\n\nclass Environment(object):\n    \"\"\"\n    A class representing a single environment.\n    \"\"\"\n\n    def __init__(self, name, data, description=None,\n                 version=None,\n                 environment_variables=None,\n                 override_environment_variables=False):\n        \"\"\"\n        Initialize a new Environment.\n\n       ", "solution": "\n        for k, v in self.dict().items():\n            if k in os.environ and not override:\n                continue\n            # With Python2 on Windows, force environment variables to str to avoid\n            # \"TypeError: environment can only contain strings\" in Python's subprocess.py.\n            if PY2 and WIN:\n                if isinstance(k, text_type) or isinstance(v, text_type):\n                    k = k.encode('ascii')\n                    v = v.encode('ascii')\n            os.environ[k] = v\n\n        return True"}
{"prompt": "def mock_cmd(self, release, *cmd, **kwargs):\n        \"\"\"Run a mock command in the chroot for a given release\"\"\"", "output": "\n        if release == '1':\n            return ['chroot', 'add', 'path', '-p', cmd]\n        else:\n            return ['chroot', 'add', 'path', '-t', cmd]\n\n    def test_add_with_no_args(self):\n        \"\"\"Test adding a path with no arguments\"\"\"\n        self.mock_cmd(self.release, *self.cmd)\n        self.assertEqual(self.mock_cmd(self.release, *self.cmd),\n                         ['add', 'path', '--release', self.release, '--args', ''])\n\n    def test_add_with", "solution": "\n        fmt = '{mock_cmd}'\n        if kwargs.get('new_chroot') is True:\n            fmt +=' --new-chroot'\n        fmt += ' --configdir={mock_dir}'\n        return self.call(fmt.format(**release).split()\n                         + list(cmd))"}
{"prompt": "def cached_instance(model, timeout=None, **filters):\n    \"\"\" Auto cached model instance.\n    \"\"\"", "output": "\n    if timeout is None:\n        timeout = config.get('CACHE_TIMEOUT', 300)\n    if timeout is None:\n        timeout = config.get('CACHE_TIMEOUT', 60)\n    return model.objects.get(timeout=timeout)\n\n\ndef get_cache_key(model, timeout=None):\n    \"\"\" Auto cached model instance.\n    \"\"\"\n    if timeout is None:\n        timeout = config.get('CACHE_TIMEOUT', 60)\n    if timeout is None:\n        timeout = config.get('CACHE_TIMEOUT', 60)\n    return model.objects.get(cache_timeout=timeout)\n\n\n", "solution": "\n    if isinstance(model, basestring):\n        model = _str_to_model(model)\n\n    cache_key = generate_cache_key(model, **filters)\n    return get_cached(cache_key, model.objects.select_related().get, kwargs=filters)"}
{"prompt": "def zoom_delta(self, d, p=(0., 0.), c=1.):\n        \"\"\"Zoom the view by a given amount.\"\"\"", "output": "\n        self.view_limits = (c, p, p)\n        self.update()\n\n    def zoom_to_data(self, d, zoom_factor=1.):\n        \"\"\"Zoom the view to a given factor.\"\"\"\n        self.view_limits = (zoom_factor, d, d)\n        self.update()\n\n    def update(self):\n        \"\"\"Update the view limits.\"\"\"\n        self.view_limits = self.zoom_limits\n        self.zoom_factor = self.view_limits[0]\n        self.p = self.view_limits[1]\n        self.zoom_factor", "solution": "\n        dx, dy = d\n        x0, y0 = p\n\n        pan_x, pan_y = self._pan\n        zoom_x, zoom_y = self._zoom\n        zoom_x_new, zoom_y_new = (zoom_x * math.exp(c * self._zoom_coeff * dx),\n                                  zoom_y * math.exp(c * self._zoom_coeff * dy))\n\n        zoom_x_new = max(min(zoom_x_new, self._zmax), self._zmin)\n        zoom_y_new = max(min(zoom_y_new, self._zmax), self._zmin)\n\n        self.zoom = zoom_x_new, zoom_y_new\n\n        if self._zoom_to_pointer:\n            zoom_x, zoom_y = self._zoom_aspect((zoom_x,\n                                                zoom_y))\n            zoom_x_new, zoom_y_new = self._zoom_aspect((zoom_x_new,\n                                                        zoom_y_new))\n\n            self.pan = (pan_x - x0 * (1. / zoom_x - 1. / zoom_x_new),\n                        pan_y - y0 * (1. / zoom_y - 1. / zoom_y_new))\n\n        self.update()"}
{"prompt": "def _known_populations(row, pops):\n    \"\"\"Find variants present in substantial frequency in population databases.\n    \"\"\"", "output": "\n    if row['populations'] is not None:\n        return [row['populations'][x] for x in pops if x in row['populations']]\n    else:\n        return None\n\ndef _known_populations_with_db(row, pops):\n    \"\"\"Find variants present in several pops with a database.\n    \"\"\"\n    if row['populations_with_db'] is not None:\n        return [x for x in pops if x in row['populations_with_db']]\n    else:\n        return None\n\ndef _known_populations_with_pops", "solution": "\n    cutoff = 0.01\n    out = set([])\n    for pop, base in [(\"esp\", \"af_esp_all\"), (\"1000g\", \"af_1kg_all\"),\n                      (\"exac\", \"af_exac_all\"), (\"anypop\", \"max_aaf_all\")]:\n        for key in [x for x in pops if x.startswith(base)]:\n            val = row[key]\n            if val and val > cutoff:\n                out.add(pop)\n    return sorted(list(out))"}
{"prompt": "def tokenize_sent(mtokens, raw='', auto_strip=True):\n    \"\"\" Tokenize a text to multiple sentences \"\"\"", "output": "\n    if not raw:\n        return mtokens\n    if not auto_strip:\n        return [token.strip() for token in raw.split('\\n') if token.strip()]\n    return [token.strip() for token in mtokens.split(raw) if token.strip()]\n\n\ndef tokenize_words(stokens, raw='', auto_strip=True):\n    \"\"\" Tokenize a text to multiple words \"\"\"\n    if not raw:\n        return stokens\n    if not auto_strip:\n        return [token.strip() for token in tokenize_sent(stokens, raw, auto_strip)]\n    return", "solution": "\n    sents = []\n    bucket = []\n    cfrom = 0\n    cto = 0\n    token_cfrom = 0\n    logger = getLogger()\n    logger.debug(\"raw text: {}\".format(raw))\n    logger.debug(\"tokens: {}\".format(mtokens))\n    for t in mtokens:\n        if t.is_eos:\n            continue\n        token_cfrom = raw.find(t.surface, cto)\n        cto = token_cfrom + len(t.surface)  # also token_cto\n        logger.debug(\"processing token {} <{}:{}>\".format(t, token_cfrom, cto))\n        bucket.append(t)\n        # sentence ending\n        if t.pos == '\u8a18\u53f7' and t.sc1 == '\u53e5\u70b9':\n            sent_text = raw[cfrom:cto]\n            getLogger().debug(\"sent_text = {} | <{}:{}>\".format(sent_text, cfrom, cto))\n            if auto_strip:\n                sent_text = sent_text.strip()\n            sents.append(MeCabSent(sent_text, bucket))\n            logger.debug(\"Found a sentence: {}\".format(sent_text))\n            cfrom = cto\n            bucket = []\n    if bucket:\n        logger.debug(\"Bucket is not empty: {}\".format(bucket))\n        sent_text = raw[cfrom:cto]\n        logger.debug(\"remaining text: {} [{}:{}]\".format(sent_text, cfrom, cto))\n        if auto_strip:\n            sent_text = sent_text.strip()\n        sents.append(MeCabSent(sent_text, bucket))\n    return sents"}
{"prompt": "def update_aliases(self):\n        \"\"\"Get aliases information from room state.\n\n        Returns:\n            boolean: True if the aliases changed, False if not\n        \"\"\"", "output": "\n        if self.aliases_changed:\n            self.aliases = self.get_aliases()\n            return True\n        return False\n\n    def get_aliases(self):\n        \"\"\"Get the room aliases information.\n\n        Returns:\n            list: List of room aliases\n        \"\"\"\n        if self.room_aliases:\n            return self.room_aliases\n\n        self.aliases_changed = True\n        self.room_aliases = []\n\n        for alias in self.aliases:\n            if alias in self.aliases_dict:\n                self.room_aliases.append(self.aliases_dict[alias])\n            else:\n                self.aliases_changed", "solution": "\n        try:\n            response = self.client.api.get_room_state(self.room_id)\n            for chunk in response:\n                if \"content\" in chunk and \"aliases\" in chunk[\"content\"]:\n                    if chunk[\"content\"][\"aliases\"] != self.aliases:\n                        self.aliases = chunk[\"content\"][\"aliases\"]\n                        return True\n                    else:\n                        return False\n        except MatrixRequestError:\n            return False"}
{"prompt": "def parse_args():\n    \"\"\"\n    Argument parser and validator\n    \"\"\"", "output": "\n    parser = argparse.ArgumentParser(description='A test script to test the SAML 2.0 SAML provider')\n    parser.add_argument('-s', '--saml-configuration', type=str, dest='saml_configuration',\n                        help='The path to the SAML configuration file')\n    parser.add_argument('-c', '--saml-configuration-class', type=str, dest='saml_configuration_class',\n                        help='The class of the SAML configuration to use')\n    parser.add_argument('-a', '--attribute-mapping', type=str, dest='attribute_mapping',\n                        help='", "solution": "\n    parser = argparse.ArgumentParser(description=\"Uploads specified VMDK file to AWS s3 bucket, and converts to AMI\")\n    parser.add_argument('-r', '--aws_regions', type=str, nargs='+', required=True,\n                        help='list of AWS regions where uploaded ami should be copied. Available'\n                             ' regions: {}.'.format(AWSUtilities.aws_regions))\n    parser.add_argument('-a', '--aws_profile', type=str, required=True, help='AWS profile name to use for aws cli commands')\n    parser.add_argument('-b', '--s3_bucket', type=str, required=True,\n                        help='The aws_bucket of the profile to upload and save vmdk to')\n    parser.add_argument('-f', '--vmdk_upload_file', type=str, required=True,\n                        help=\"The file to upload if executing \")\n    parser.add_argument('-n', '--ami_name', type=str, required=False, help='The name to give to the uploaded ami. '\n                                                                           'Defaults to the name of the file')\n    parser.add_argument('-d', '--directory', type=str, default=tempfile.mkdtemp(),\n                        help='Directory to save temp aws config upload files')\n    args = parser.parse_args()\n\n    if not args.ami_name:\n        args.ami_name = os.path.basename(args.vmdk_upload_file)\n\n    validate_args(args)\n\n    return args"}
{"prompt": "def _currentLineExtraSelections(self):\n        \"\"\"QTextEdit.ExtraSelection, which highlightes current line\n        \"\"\"", "output": "\n        return self._extraSelections\n\n    def _currentLine(self):\n        \"\"\"QTextEdit.CurrentLine, which returns the current line\n        \"\"\"\n        return self._currentEdit.textCursor().blockNumber()\n\n    def _setExtraSelections(self, extraSelections):\n        \"\"\"Set extra selections\n        \"\"\"\n        self._extraSelections = extraSelections\n        self.update()\n\n    def _setCurrentLine(self, currentLine):\n        \"\"\"Set current line\n        \"\"\"\n        self.set_edit_text(currentLine, self.extra_selections)\n        self.update()\n\n    def _set", "solution": "\n        if self._currentLineColor is None:\n            return []\n\n        def makeSelection(cursor):\n            selection = QTextEdit.ExtraSelection()\n            selection.format.setBackground(self._currentLineColor)\n            selection.format.setProperty(QTextFormat.FullWidthSelection, True)\n            cursor.clearSelection()\n            selection.cursor = cursor\n            return selection\n\n        rectangularSelectionCursors = self._rectangularSelection.cursors()\n        if rectangularSelectionCursors:\n            return [makeSelection(cursor) \\\n                        for cursor in rectangularSelectionCursors]\n        else:\n            return [makeSelection(self.textCursor())]"}
{"prompt": "def _unlock(self):\n        \"\"\"\n        Unlock this keyring by getting the password for the keyring from the\n        user.\n        \"\"\"", "output": "\n        return self._keyring.unlock()\n\n    def _get_password(self, username):\n        \"\"\"\n        Get the password for the username.\n        \"\"\"\n        return self._keyring.get_password(username)\n\n    def _get_keyring(self):\n        \"\"\"\n        Get the keyring.\n        \"\"\"\n        return self._keyring\n\n    def _get_keyring_handle(self):\n        \"\"\"\n        Get the handle to the keyring.\n        \"\"\"\n        return self._keyring_handle\n\n    def _get_keyring_handle_for_user(self, username):\n        \"\"\"\n        Get the handle to the keyring.\n        \"\"\"\n        return self", "solution": "\n        self.keyring_key = getpass.getpass(\n            'Please enter password for encrypted keyring: ')\n        try:\n            ref_pw = self.get_password('keyring-setting', 'password reference')\n            assert ref_pw == 'password reference value'\n        except AssertionError:\n            self._lock()\n            raise ValueError(\"Incorrect Password\")"}
{"prompt": "def build_hazard_stats(pgetter, N, hstats, individual_curves, monitor):\n    \"\"\"\n    :param pgetter: an :class:`openquake.commonlib.getters.PmapGetter`\n    :param N: the total number of sites\n    :param hstats: a list of pairs (statname, statfunc)\n    :param individual_curves: if True, also build the individual curves\n    :param monitor: instance of Monitor\n    :returns: a dictionary kind -> ProbabilityMap\n\n    The \"kind\" is a string of the form 'rlz-XXX' or 'mean' of 'quantile-XXX'\n    used to specify the kind of output.\n    \"\"\"", "output": " 'rlz-XXX-XXX'.\n    \"\"\"\n    if monitor.data_source.data_type == 'file':\n        return {'rlz-XXX': HazardStatsFile(hstats, N, hgetter, individual_curves)}\n    elif monitor.data_source.data_type == 'directory':\n        return {'rlz-XXX': HazardStatsDirectory(hgetter, N, monitor)}\n    elif monitor.data_source.data_type == 'hazard':\n        return build_hazard_stats(pgetter, N, hstats, hgetter.hazard_curves, monitor)", "solution": "\n    with monitor('combine pmaps'):\n        pgetter.init()  # if not already initialized\n        try:\n            pmaps = pgetter.get_pmaps()\n        except IndexError:  # no data\n            return {}\n        if sum(len(pmap) for pmap in pmaps) == 0:  # no data\n            return {}\n    R = len(pmaps)\n    imtls, poes, weights = pgetter.imtls, pgetter.poes, pgetter.weights\n    pmap_by_kind = {}\n    hmaps_stats = []\n    hcurves_stats = []\n    with monitor('compute stats'):\n        for statname, stat in hstats.items():\n            pmap = compute_pmap_stats(pmaps, [stat], weights, imtls)\n            hcurves_stats.append(pmap)\n            if pgetter.poes:\n                hmaps_stats.append(\n                    calc.make_hmap(pmap, pgetter.imtls, pgetter.poes))\n            if statname == 'mean' and R > 1 and N <= FEWSITES:\n                pmap_by_kind['rlz_by_sid'] = rlz = {}\n                for sid, pcurve in pmap.items():\n                    rlz[sid] = util.closest_to_ref(\n                        [pm.setdefault(sid, 0).array for pm in pmaps],\n                        pcurve.array)['rlz']\n    if hcurves_stats:\n        pmap_by_kind['hcurves-stats'] = hcurves_stats\n    if hmaps_stats:\n        pmap_by_kind['hmaps-stats'] = hmaps_stats\n    if R > 1 and individual_curves or not hstats:\n        pmap_by_kind['hcurves-rlzs'] = pmaps\n        if pgetter.poes:\n            with monitor('build individual hmaps'):\n                pmap_by_kind['hmaps-rlzs'] = [\n                    calc.make_hmap(pmap, imtls, poes) for pmap in pmaps]\n    return pmap_by_kind"}
{"prompt": "def _AskUser(self):\n    \"\"\"Prompt the user for the next action.\n\n    Returns:\n      A string, the character entered by the user.\n    \"\"\"", "output": "\n    return self._GetResultFromJSONRequest({\n      'command': 'GetNextAction',\n      'id': self._next_action_id,\n      'properties': {\n        'nextAction': self._next_action_properties\n      }\n    })\n\n  def _GetResultFromJSONRequest(self, request_dict):\n    \"\"\"Gets the result of a previously-submitted action.\n\n    Args:\n      request_dict: The JSON dict returned from the server.\n\n    Returns:\n      The result of the previously-submitted action.\n    \"\"\"\n    return request_dict['nextAction']\n\n  def _SetResult(self, action", "solution": "\n    if self._show_percent:\n      progress = int(self._displayed*100 / (len(self._text.splitlines())))\n      progress_text = ' (%d%%)' % progress\n    else:\n      progress_text = ''\n    question = AnsiText(\n        'Enter: next line, Space: next page, '\n        'b: prev page, q: quit.%s' %\n        progress_text, ['green'])\n    sys.stdout.write(question)\n    sys.stdout.flush()\n    ch = self._GetCh()\n    sys.stdout.write('\\r%s\\r' % (' '*len(question)))\n    sys.stdout.flush()\n    return ch"}
{"prompt": "def blk_nd_short(blk, shape):\n    \"\"\"Iterate trough the blocks that strictly cover an array.\n\n    Iterate trough the blocks that recover the part of the array\n    given by max_blk_coverage.\n\n    :param blk: the N-dimensional shape of the block\n    :param shape: the N-dimensional shape of the array\n    :return: a generator that yields the blocks\n\n    Example:\n\n        >>> result = list(blk_nd_short(blk=(5,3), shape=(11, 11)))\n        >>> result[0]\n        (slice(0, 5, None), slice(0, 3, None))\n        >>> result[1]\n        (slice(0, 5, None), slice(3, 6, None))\n        >>> result[-1]\n        (slice(5, 10, None), slice(6, 9, None))\n\n        In this case, the output of max_blk_coverage\n        is (10, 9), so only this part of the array is covered\n\n\n    .. seealso::\n\n        :py:func:`blk_nd`\n          Yields blocks of blk size until the remaining part is\n          smaller than `blk` and the yields smaller blocks.\n\n    \"\"\"", "output": "0, 3, None), slice(0, 11, None), slice(0, 11, 11))\n        >>> for block in result:\n       ...     print(block)\n\n    \"\"\"\n    for i in range(0, len(shape)):\n        if shape[i] > max_blk_coverage:\n            max_coverage = shape[i]\n            yield (i, slice(max_coverage, None))\n\n\ndef _get_slice_nd(shape, start, end):\n    \"\"\"Get a slice of the N-dimensional array.\n\n    :param shape: the N-dimensional shape of the array\n    :", "solution": "\n    internals = (blk_1d_short(b, s) for b, s in zip(blk, shape))\n    return product(*internals)"}
{"prompt": "def set_duration(self, duration):\n        \"\"\"Sets the assessment duration.\n\n        arg:    duration (osid.calendaring.Duration): assessment\n                duration\n        raise:  InvalidArgument - ``duration`` is invalid\n        raise:  NoAccess - ``Metadata.isReadOnly()`` is ``true``\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"", "output": "\n        raise Unimplemented()\n\n    def get_duration_metadata(self):\n        \"\"\"Gets the metadata for an assessment.\n\n        return: (osid.Metadata) - metadata for the assessment\n        raise:  NullArgument - ``meta`` is ``null``\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n        return osid_objects.MetadataList(self._get_request_terms())\n\n    duration_metadata = property(fget=get_duration_metadata)\n\n    def get_assessment_offered_form_record(self):\n        \"\"\"Gets the assessment offered form record.\n\n        return:", "solution": "\n        # Implemented from template for osid.assessment.AssessmentOfferedForm.set_duration_template\n        if self.get_duration_metadata().is_read_only():\n            raise errors.NoAccess()\n        if not self._is_valid_duration(\n                duration,\n                self.get_duration_metadata()):\n            raise errors.InvalidArgument()\n        map = dict()\n        map['days'] = duration.days\n        map['seconds'] = duration.seconds\n        map['microseconds'] = duration.microseconds\n        self._my_map['duration'] = map"}
{"prompt": "def get_field(self, field_name):\n        \"\"\"Get field associated with given attribute.\"\"\"", "output": "\n        return self.fields.get(field_name, None)\n\n    def get_field_value(self, field_name):\n        \"\"\"Get value associated with given attribute.\"\"\"\n        return self.fields.get(field_name, None)\n\n    def set_field(self, field_name, value):\n        \"\"\"Set field to given value.\"\"\"\n        self.fields[field_name] = value\n\n    def get_field_type(self, field_name):\n        \"\"\"Get field type associated with given attribute.\"\"\"\n        return self.fields.get(field_name, None)\n\n    def get_field_", "solution": "\n        for attr_name, field in self:\n            if field_name == attr_name:\n                return field\n\n        raise errors.FieldNotFound('Field not found', field_name)"}
{"prompt": "def israw(self, **kwargs):\n        \"\"\"\n        Returns True if the PTY should operate in raw mode.\n\n        If the container was not started with tty=True, this will return False.\n        \"\"\"", "output": "\n        return self.is_running\n\n    def is_running(self):\n        \"\"\"\n        Returns True if the PTY should be running.\n        \"\"\"\n        return self.is_running\n\n    def start(self, **kwargs):\n        \"\"\"\n        Starts the PTY.\n\n        This will start the PTY, but will not wait for it to return.\n        \"\"\"\n        self.is_running = True\n        self.start_time = time.time()\n        self.wait_time = 0.1\n        self.wait_for_started = False\n        self.wait_for_stopped = False\n        self.wait_", "solution": "\n\n        if self.raw is None:\n            info = self._container_info()\n            self.raw = self.stdout.isatty() and info['Config']['Tty']\n\n        return self.raw"}
{"prompt": "def generate_zip_state_data(means, weights, z):\n    \"\"\"\n    Generates data according to the Zero-inflated Poisson Convex Mixture Model.\n\n    Args:\n        means (array): Cell types- genes x clusters\n        weights (array): Cell cluster assignments- clusters x cells\n        z (float): zero-inflation parameter\n\n    Returns:\n        data matrix - genes x cells\n    \"\"\"", "output": "\n    return np.array([np.random.poisson(x, weights) for x in means])\n\n\ndef generate_zero_inflation_data(z):\n    \"\"\"\n    Generates data according to the Zero-inflated Poisson Convex Mixture Model.\n\n    Args:\n        z (float): cell size in m\n        z (float): zero-inflation parameter\n\n    Returns:\n        data matrix - genes x cells\n    \"\"\"\n    return np.array([np.random.poisson(x) for x in z])\n\n\ndef generate_zero_inflation_data_with_weights(z,", "solution": "\n    x_true = np.dot(means, weights)\n    sample = np.random.poisson(x_true)\n    random = np.random.random(x_true.shape)\n    x_true[random < z] = 0\n    return sample.astype(float)"}
{"prompt": "def findunique(lst, key):\n    \"\"\"\n    Find all unique key values for items in lst.\n\n    Parameters\n    ----------\n\n    lst: list\n         A list of composite dictionaries e.g. ``layers``, ``classes``\n    key: string\n        The key name to search each dictionary in the list\n\n    Returns\n    -------\n\n    list\n        A sorted Python list of unique keys in the list\n\n    Example\n    -------\n\n    To find all ``GROUP`` values for ``CLASS`` in a ``LAYER``::\n\n        s = \"\"\"", "output": "\n        group = {\n            'name':'mygroup',\n            'type': 'group',\n            'children': [\n                {'name':'mylayer', 'type': 'layer'},\n                {'name':'mygroup', 'type': 'group'}\n            ]\n        }\n        }\n        \"\"\"\n        s += \"\\n\".join(map(lambda x: x.get(key), lst))\n\n    return s\n\n\ndef findunique_values(lst, key):\n    \"\"\"\n    Find all unique key values for items in lst.\n\n    Parameters\n    ----------\n\n    lst: list\n         A list of composite dictionaries e.g. ``", "solution": "\n        LAYER\n            CLASS\n                GROUP \"group1\"\n                NAME \"Class1\"\n                COLOR 0 0 0\n            END\n            CLASS\n                GROUP \"group2\"\n                NAME \"Class2\"\n                COLOR 0 0 0\n            END\n            CLASS\n                GROUP \"group1\"\n                NAME \"Class3\"\n                COLOR 0 0 0\n            END\n        END\n        "}
{"prompt": "def _download_without_backoff(url, as_file=True, method='GET', **kwargs):\n    \"\"\"\n    Get the content of a URL and return a file-like object.\n    \"\"\"", "output": "\n    try:\n        return requests.get(url, **kwargs)\n    except requests.exceptions.ConnectionError as e:\n        raise ConnectionError(e)\n\n\ndef _download_with_backoff(url, **kwargs):\n    \"\"\"\n    Get the content of a URL and return a file-like object.\n    \"\"\"\n    try:\n        return requests.get(url, **kwargs)\n    except requests.exceptions.ConnectionError as e:\n        raise ConnectionError(e)\n\n\ndef _get_content_type(url):\n    \"\"\"\n    Get the content type of a URL.\n    \"\"\"\n    try:\n        return requests.head(", "solution": "\n    # Make requests consistently hashable for caching.\n    # 'headers' is handled by requests itself.\n    # 'cookies' and 'proxies' contributes to headers.\n    # 'files' and 'json' contribute to data.\n    for k in ['data', 'params']:\n        if k in kwargs and isinstance(kwargs[k], dict):\n            kwargs[k] = OrderedDict(sorted(kwargs[k].items()))\n\n    kwargs_copy = dict(kwargs)\n    if not _is_url_in_cache(method, url, **kwargs):\n        now = datetime.datetime.now()\n        _rate_limit_for_url(url, now)\n        _rate_limit_touch_url(url, now)\n\n    L.info(\"Download {}\".format(url))\n    if 'timeout' not in kwargs_copy:\n        kwargs_copy['timeout'] = _TIMEOUT\n    if 'headers' in kwargs_copy:\n        head_dict = CaseInsensitiveDict(kwargs_copy['headers'])\n        if 'user-agent' not in head_dict:\n            head_dict['user-agent'] = _USER_AGENT\n        kwargs_copy['headers'] = head_dict\n    else:\n        kwargs_copy['headers'] = CaseInsensitiveDict({'user-agent': _USER_AGENT})\n\n    response = requests.request(method, url, **kwargs_copy)\n\n    if logging.getLogger().isEnabledFor(logging.DEBUG):\n        # This can be slow on large responses, due to chardet.\n        L.debug('\"{}\"'.format(response.text))\n\n    response.raise_for_status()\n\n    if as_file:\n        return BytesIO(response.content)\n    else:\n        return response"}
{"prompt": "def dump_emails(part):\n    \"\"\"Show the sent emails' tested parts, to aid in debugging.\"\"\"", "output": "\n    print(part)\n    print(email_to_name(part['email']))\n\n\ndef main():\n    \"\"\"Run the main program.\"\"\"\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument('email', help='email address to test')\n    parser.add_argument('password', help='password to test')\n    parser.add_argument('email_file', help='email file to test')\n    parser.add_argument('test_email', help='test email address')\n    parser.add_argument('part', help='test part number')\n    args = parser.parse", "solution": "\n\n    print(\"Sent emails:\")\n    for email in mail.outbox:\n        print(getattr(email, part))"}
{"prompt": "def set_aesthetic(palette=\"yellowbrick\", font=\"sans-serif\", font_scale=1,\n                  color_codes=True, rc=None):\n    \"\"\"\n    Set aesthetic parameters in one step.\n\n    Each set of parameters can be set directly or temporarily, see the\n    referenced functions below for more information.\n\n    Parameters\n    ----------\n    palette : string or sequence\n        Color palette, see :func:`color_palette`\n    font : string\n        Font family, see matplotlib font manager.\n    font_scale : float, optional\n        Separate scaling factor to independently scale the size of the\n        font elements.\n    color_codes : bool\n        If ``True`` and ``palette`` is a yellowbrick palette, remap the shorthand\n        color codes (e.g. \"b\", \"g\", \"r\", etc.) to the colors from this palette.\n    rc : dict or None\n        Dictionary of rc parameter mappings to override the above.\n    \"\"\"", "output": " elements.\n    color_codes : bool, optional\n        Whether to use color codes instead of the default 'greenbrick'\n    rc : dict, optional\n        Dictionary of parameters to pass to the backend.\n\n    Returns\n    -------\n    None\n\n    See Also\n    --------\n    set_color_codes : Set the default color codes to the given colormap.\n    set_palette : Set the color palette to the given palette.\n    set_font : Set the font to the given colormap.\n    set_font_properties : Set the matplotlib properties to the given colormap.\n\n    Notes\n    -----\n    This function is useful for adding a colorbar to a plot", "solution": "\n    _set_context(font_scale)\n    set_style(rc={\"font.family\": font})\n    set_palette(palette, color_codes=color_codes)\n    if rc is not None:\n        mpl.rcParams.update(rc)"}
{"prompt": "def add_personalization(self, personalization, index=0):\n        \"\"\"Add a Personaliztion object\n\n        :param personalizations: Add a Personalization object\n        :type personalizations: Personalization\n        :param index: The index where to add the Personalization\n        :type index: int\n        \"\"\"", "output": "\n        self.personalizations.append(personalization)\n        if personalization.id is not None:\n            self.personalizations[index].id = personalization.id\n        if personalization.name is not None:\n            self.personalizations[index].name = personalization.name\n        if personalization.description is not None:\n            self.personalizations[index].description = personalization.description\n        if personalization.image_url is not None:\n            self.personalizations[index].image_url = personalization.image_url\n        if personalization.description is not None:\n            self.personalizations", "solution": "\n        self._personalizations = self._ensure_append(\n            personalization, self._personalizations, index)"}
{"prompt": "def parse_exception(line):\n    \"\"\"Parse the first line of a Cartouche exception description.\n\n    Args:\n        line (str): A single line Cartouche exception description.\n\n    Returns:\n        A 2-tuple containing the exception type and the first line of the description.\n    \"\"\"", "output": "\n    # The first line of the description is the error description.\n    line = line.strip()\n    # Skip empty lines.\n    if not line:\n        return None, None\n\n    # Skip comments.\n    if line.startswith('#'):\n        return None, None\n\n    # Skip empty lines.\n    if line.startswith(' '):\n        return None, None\n\n    # Skip empty lines.\n    if not line:\n        return None, None\n\n    # Skip empty lines.\n    if line.startswith(' '):\n        return None, None\n\n    # Skip empty lines.\n    if not line:\n        return None, None\n\n    # Skip comments.\n    if", "solution": "\n    m = RAISES_REGEX.match(line)\n    if m is None:\n        raise CartoucheSyntaxError('Cartouche: Invalid argument syntax \"{line}\" for Raises block'.format(line=line))\n    return m.group(2), m.group(1)"}
{"prompt": "def _rgb_triangle(ax, r_label, g_label, b_label, loc):\n        \"\"\"\n        Draw an RGB triangle legend on the desired axis\n        \"\"\"", "output": "\n        ax.plot(loc, b_label, color=color, label=r_label)\n\n    def _rgb_line(self, ax, loc):\n        \"\"\"\n        Draw an RGB line legend on the desired axis\n        \"\"\"\n        ax.plot(loc, self.b_color, label=b_label)\n\n    def _get_legend_handles(self, ax):\n        \"\"\"\n        Return the handles of the legend\n        \"\"\"\n        handles = []\n        if self.handles is not None:\n            for handle in self.handles:\n                if handle is None:\n                    handles.append(handle)\n                else:", "solution": "\n        if not loc in range(1, 11):\n            loc = 2\n\n        from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n        inset_ax = inset_axes(ax, width=1, height=1, loc=loc)\n        mesh = 35\n        x = []\n        y = []\n        color = []\n        for r in range(0, mesh):\n            for g in range(0, mesh):\n                for b in range(0, mesh):\n                    if not (r == 0 and b == 0 and g == 0):\n                        r1 = r / (r + g + b)\n                        g1 = g / (r + g + b)\n                        b1 = b / (r + g + b)\n                        x.append(0.33 * (2. * g1 + r1) / (r1 + b1 + g1))\n                        y.append(0.33 * np.sqrt(3) * r1 / (r1 + b1 + g1))\n                        rc = math.sqrt(r ** 2 / (r ** 2 + g ** 2 + b ** 2))\n                        gc = math.sqrt(g ** 2 / (r ** 2 + g ** 2 + b ** 2))\n                        bc = math.sqrt(b ** 2 / (r ** 2 + g ** 2 + b ** 2))\n                        color.append([rc, gc, bc])\n\n        # x = [n + 0.25 for n in x]  # nudge x coordinates\n        # y = [n + (max_y - 1) for n in y]  # shift y coordinates to top\n        # plot the triangle\n        inset_ax.scatter(x, y, s=7, marker='.', edgecolor=color)\n        inset_ax.set_xlim([-0.35, 1.00])\n        inset_ax.set_ylim([-0.35, 1.00])\n\n        # add the labels\n        inset_ax.text(0.70, -0.2, g_label, fontsize=13,\n                      family='Times New Roman', color=(0, 0, 0),\n                      horizontalalignment='left')\n        inset_ax.text(0.325, 0.70, r_label, fontsize=13,\n                      family='Times New Roman', color=(0, 0, 0),\n                      horizontalalignment='center')\n        inset_ax.text(-0.05, -0.2, b_label, fontsize=13,\n                      family='Times New Roman', color=(0, 0, 0),\n                      horizontalalignment='right')\n\n        inset_ax.get_xaxis().set_visible(False)\n        inset_ax.get_yaxis().set_visible(False)"}
{"prompt": "def log(self, msg, lvl = 0):\n        \"\"\"Log messages according to the logging level (0 is highest priority).\"\"\"", "output": "\n        if lvl == 0:\n            self.log_message(msg)\n        else:\n            self.log_message(msg, lvl)\n\n    def log_message(self, msg, lvl = 0):\n        \"\"\"Log messages according to the logging level (0 is lowest priority).\"\"\"\n        if lvl == 0:\n            self.log_message_verbose(msg)\n        else:\n            self.log_message(msg, lvl)\n\n    def log_message_verbose(self, msg):\n        \"\"\"Log a message with a verbose level.\"\"\"\n        if self.verbose:\n            self.log_message", "solution": "\n        if self.log_level >= lvl:\n            self.log_fn(msg)"}
{"prompt": "def cee_map_precedence(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"", "output": "\n        config = ET.Element(\"config\")\n        ceemap = ET.SubElement(config, \"ceemap\", xmlns=\"urn:brocade.com:mgmt:brocade-aaa\")\n        ceemap_name = ET.SubElement(ceemap, \"ceemap-name\")\n        ceemap_name.text = kwargs.pop('ceemap_name')\n\n        callback = kwargs.pop('callback', self._callback)\n        if callback is not None:\n            return callback(config)\n        else:\n            return ET.Element(\"config-map-priority\")\n\n    def cee_", "solution": "\n        config = ET.Element(\"config\")\n        cee_map = ET.SubElement(config, \"cee-map\", xmlns=\"urn:brocade.com:mgmt:brocade-cee-map\")\n        name_key = ET.SubElement(cee_map, \"name\")\n        name_key.text = kwargs.pop('name')\n        precedence = ET.SubElement(cee_map, \"precedence\")\n        precedence.text = kwargs.pop('precedence')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)"}
{"prompt": "def on_revert(request, page_name):\n    \"\"\"Revert an old revision.\"\"\"", "output": "\n    page = get_object_or_404(Page, slug=page_name)\n    if request.method == 'POST':\n        revision = get_object_or_404(PageRevision, page=page, creator=request.user)\n        if not revision.can_edit(request.user):\n            return HttpResponseForbidden()\n        if request.POST.get('revert'):\n            revision.revert(request.user)\n            return HttpResponse('ok')\n        return HttpResponse('error')\n    else:\n        return HttpResponseNotAllowed(['POST'])\n\n\n@login_required\n@permission_required('wiki.change_document", "solution": "\n    rev_id = request.args.get(\"rev\", type=int)\n\n    old_revision = page = None\n    error = \"No such revision\"\n\n    if request.method == \"POST\" and request.form.get(\"cancel\"):\n        return redirect(href(page_name))\n\n    if rev_id:\n        old_revision = Revision.query.filter(\n            (Revision.revision_id == rev_id)\n            & (Revision.page_id == Page.page_id)\n            & (Page.name == page_name)\n        ).first()\n        if old_revision:\n            new_revision = (\n                Revision.query.filter(\n                    (Revision.page_id == Page.page_id) & (Page.name == page_name)\n                )\n                .order_by(Revision.revision_id.desc())\n                .first()\n            )\n            if old_revision == new_revision:\n                error = \"You tried to revert the current active revision.\"\n            elif old_revision.text == new_revision.text:\n                error = (\n                    \"There are no changes between the current \"\n                    \"revision and the revision you want to \"\n                    \"restore.\"\n                )\n            else:\n                error = \"\"\n                page = old_revision.page\n                if request.method == \"POST\":\n                    change_note = request.form.get(\"change_note\", \"\")\n                    change_note = \"revert\" + (change_note and \": \" + change_note or \"\")\n                    session.add(Revision(page, old_revision.text, change_note))\n                    session.commit()\n                    return redirect(href(page_name))\n\n    return Response(\n        generate_template(\n            \"action_revert.html\", error=error, old_revision=old_revision, page=page\n        )\n    )"}
{"prompt": "def play(self):\n\t\t\"\"\" Start the playback of the video. \n\t\tThe playback loop is run in a separate thread, so this function returns \n\t\timmediately. This allows one to implement things such as event handling \n\t\tloops (e.g. check for key presses) elsewhere.\n\t\t\"\"\"", "output": "\n\t\tself.video_thread = VideoThread(self.video_queue, self.video_thread_callback)\n\t\tself.video_thread.start()\n\n\tdef stop(self):\n\t\t\"\"\" Stop the playback of the video. \n\t\tThe thread is run in a separate thread, so this function returns \n\t\timmediately. This allows one to implement things such as event handling \n\t\tloops (e.g. check for key presses) elsewhere.\n\t\t\"\"\"\n\t\tself.video_queue.put(None)\n\t\tself.video_thread.join()\n\n\tdef video", "solution": "\n\t\t### First do some status checks\n\n\t\t# Make sure a file is loaded\n\t\tif self.status == UNINITIALIZED or self.clip is None:\n\t\t\traise RuntimeError(\"Player uninitialized or no file loaded\")\n\n\t\t# Check if playback has already finished (rewind needs to be called first)\n\t\tif self.status == EOS:\n\t\t\tlogger.debug(\"End of stream has already been reached\")\n\t\t\treturn\n\n\t\t# Check if playback hasn't already been started (and thus if play()\n\t\t# has not been called before from another thread for instance)\n\t\tif self.status in [PLAYING,PAUSED]:\n\t\t\tlogger.warning(\"Video already started\")\n\t\t\treturn\n\n\t\t### If all is in order start the general playing loop\n\t\tif self.status == READY:\n\t\t\tself.status = PLAYING\n\n\t\tself.last_frame_no = 0\n\n\t\tif not hasattr(self,\"renderloop\") or not self.renderloop.isAlive():\n\t\t\tif self.audioformat:\n\t\t\t\t# Chop the total stream into separate audio chunks that are the\n\t\t\t\t# lenght of a video frame (this way the index of each chunk \n\t\t\t\t# corresponds to the video frame it belongs to.)\n\t\t\t\tself.__calculate_audio_frames()\n\t\t\t\t# Start audio handling thread. This thread places audioframes\n\t\t\t\t# into a sound buffer, untill this buffer is full.\n\t\t\t\tself.audioframe_handler = threading.Thread(\n\t\t\t\t\ttarget=self.__audiorender_thread)\n\t\t\t\tself.audioframe_handler.start()\n\n\t\t\t# Start main rendering loop.\n\t\t\tself.renderloop = threading.Thread(target=self.__render)\n\t\t\tself.renderloop.start()\n\t\telse:\n\t\t\tlogger.warning(\"Rendering thread already running!\")"}
{"prompt": "def on_trial_complete(self,\n                          trial_id,\n                          result=None,\n                          error=False,\n                          early_terminated=False):\n        \"\"\"Passes the result to BayesOpt unless early terminated or errored\"\"\"", "output": "\n        if early_terminated:\n            self.on_terminated(trial_id)\n        if error:\n            self.on_error(trial_id)\n\n    def on_terminated(self, trial_id):\n        \"\"\"Called when the trial is terminated\"\"\"\n        pass\n\n    def on_error(self, trial_id):\n        \"\"\"Called when an error occurs\"\"\"\n        self.logger.error(\n            'Error in on_trial_complete: trial_id=%s result=%s error=%s early_terminated=%s' % (\n                trial_id,\n                result,\n                error,\n                early_terminated\n            )", "solution": "\n        if result:\n            self.optimizer.register(\n                params=self._live_trial_mapping[trial_id],\n                target=result[self._reward_attr])\n\n        del self._live_trial_mapping[trial_id]"}
{"prompt": "def _register_and_parse_flags_with_usage(\n    argv=None,\n    flags_parser=parse_flags_with_usage,\n):\n  \"\"\"Registers help flags, parses arguments and shows usage if appropriate.\n\n  This also calls sys.exit(0) if flag --only_check_args is True.\n\n  Args:\n    argv: [str], a non-empty list of the command line arguments including\n        program name, sys.argv is used if None.\n    flags_parser: Callable[[List[Text]], Any], the function used to parse flags.\n        The return value of this function is passed to `main` untouched.\n        It must guarantee FLAGS is parsed after this function is called.\n\n  Returns:\n    The return value of `flags_parser`. When using the default `flags_parser`,\n    it returns the following:\n    [str], a non-empty list of remaining command line arguments after parsing\n    flags, including program name.\n\n  Raises:\n    Error: Raised when flags_parser is called, but FLAGS is not parsed.\n    SystemError: Raised when it's called more than once.\n  \"\"\"", "output": " is passed to the corresponding\n        `flags_parser` argument.\n    only_check_args: bool, whether to only check flags that are present.\n        If None, the function will print an error message if the flags are not present.\n        If True, the function will print a warning if the flags are present.\n    flags_parser: Callable[[List[Text]], Any], the function used to parse flags.\n        The return value of this function is passed to the corresponding\n        `flags_parser` argument.\n  \"\"\"\n  if argv is None:\n    argv = sys.argv\n  flags_parser(argv, flags.", "solution": "\n  if _register_and_parse_flags_with_usage.done:\n    raise SystemError('Flag registration can be done only once.')\n\n  define_help_flags()\n\n  original_argv = sys.argv if argv is None else argv\n  args_to_main = flags_parser(original_argv)\n  if not FLAGS.is_parsed():\n    raise Error('FLAGS must be parsed after flags_parser is called.')\n\n  # Exit when told so.\n  if FLAGS.only_check_args:\n    sys.exit(0)\n  # Immediately after flags are parsed, bump verbosity to INFO if the flag has\n  # not been set.\n  if FLAGS['verbosity'].using_default_value:\n    FLAGS.verbosity = 0\n  _register_and_parse_flags_with_usage.done = True\n\n  return args_to_main"}
{"prompt": "def align_yaxis_np(axes):\n    \"\"\"Align zeros of the two axes, zooming them out by same ratio\"\"\"", "output": "\n    axes[0].set_ylim(0, 1)\n    axes[1].set_ylim(0, 1)\n    axes[0].set_ylabel('y')\n    axes[1].set_ylabel('y')\n    axes[0].set_xlabel('x')\n    axes[1].set_xlabel('y')\n    axes[0].set_title('Zeroes')\n\n\ndef test_align_yaxis_np():\n    \"\"\"Align zeros of the two axes, zooming them out by same ratio\"\"\"\n    fig = plt.figure()\n    ax = fig.add_subplot(111)", "solution": "\n    axes = np.array(axes)\n    extrema = np.array([ax.get_ylim() for ax in axes])\n\n    # reset for divide by zero issues\n    for i in range(len(extrema)):\n        if np.isclose(extrema[i, 0], 0.0):\n            extrema[i, 0] = -1\n        if np.isclose(extrema[i, 1], 0.0):\n            extrema[i, 1] = 1\n\n    # upper and lower limits\n    lowers = extrema[:, 0]\n    uppers = extrema[:, 1]\n\n    # if all pos or all neg, don't scale\n    all_positive = False\n    all_negative = False\n    if lowers.min() > 0.0:\n        all_positive = True\n\n    if uppers.max() < 0.0:\n        all_negative = True\n\n    if all_negative or all_positive:\n        # don't scale\n        return\n\n    # pick \"most centered\" axis\n    res = abs(uppers+lowers)\n    min_index = np.argmin(res)\n\n    # scale positive or negative part\n    multiplier1 = abs(uppers[min_index]/lowers[min_index])\n    multiplier2 = abs(lowers[min_index]/uppers[min_index])\n\n    for i in range(len(extrema)):\n        # scale positive or negative part based on which induces valid\n        if i != min_index:\n            lower_change = extrema[i, 1] * -1*multiplier2\n            upper_change = extrema[i, 0] * -1*multiplier1\n            if upper_change < extrema[i, 1]:\n                extrema[i, 0] = lower_change\n            else:\n                extrema[i, 1] = upper_change\n\n        # bump by 10% for a margin\n        extrema[i, 0] *= 1.1\n        extrema[i, 1] *= 1.1\n\n    # set axes limits\n    [axes[i].set_ylim(*extrema[i]) for i in range(len(extrema))]"}
{"prompt": "def x10_all_lights_off(self, housecode):\n        \"\"\"Send the X10 All Lights Off command.\"\"\"", "output": "\n        self._send_command(self._x10_all_lights_off, housecode)\n\n    def x10_all_lights_on(self, light_id):\n        \"\"\"Send the X10 All Lights On command.\"\"\"\n        self._send_command(self._x10_all_lights_on, light_id)\n\n    def _send_command(self, command, housecode=None):\n        \"\"\"Send a command to a X10 device.\"\"\"\n        import pexpect\n        try:\n            child = pexpect.spawn(command)\n            child.expect('.*: Entering.*$')\n            child.", "solution": "\n        msg = X10Send.command_msg(housecode, X10_COMMAND_ALL_LIGHTS_OFF)\n        self.send_msg(msg)\n        self._x10_command_to_device(housecode, X10_COMMAND_ALL_LIGHTS_OFF, msg)"}
{"prompt": "def read_squad_examples(input_file, is_training, version_2_with_negative):\n    \"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"", "output": "\n    with tf.io.gfile.GFile(input_file, \"r\") as f:\n        examples = json.load(f)\n\n    if is_training:\n        return [\n            SquadExample(\n                example[\"inputs\"], example[\"outputs\"], version_2_with_negative,\n                example[\"inputs_spec\"], example[\"outputs_spec\"],\n                example[\"inputs_data\"], example[\"outputs_data\"],\n                example[\"inputs_shape\"], example[\"outputs_shape\"],\n                example[\"inputs_dtype\"], example[\"outputs_dtype\"],\n                example[\"outputs_shape\"], example[\"inputs_dtype\"],\n                example", "solution": "\n    with open(input_file, \"r\", encoding='utf-8') as reader:\n        input_data = json.load(reader)[\"data\"]\n\n    def is_whitespace(c):\n        if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n            return True\n        return False\n\n    examples = []\n    for entry in input_data:\n        for paragraph in entry[\"paragraphs\"]:\n            paragraph_text = paragraph[\"context\"]\n            doc_tokens = []\n            char_to_word_offset = []\n            prev_is_whitespace = True\n            for c in paragraph_text:\n                if is_whitespace(c):\n                    prev_is_whitespace = True\n                else:\n                    if prev_is_whitespace:\n                        doc_tokens.append(c)\n                    else:\n                        doc_tokens[-1] += c\n                    prev_is_whitespace = False\n                char_to_word_offset.append(len(doc_tokens) - 1)\n\n            for qa in paragraph[\"qas\"]:\n                qas_id = qa[\"id\"]\n                question_text = qa[\"question\"]\n                start_position = None\n                end_position = None\n                orig_answer_text = None\n                is_impossible = False\n                if is_training:\n                    if version_2_with_negative:\n                        is_impossible = qa[\"is_impossible\"]\n                    if (len(qa[\"answers\"]) != 1) and (not is_impossible):\n                        raise ValueError(\n                            \"For training, each question should have exactly 1 answer.\")\n                    if not is_impossible:\n                        answer = qa[\"answers\"][0]\n                        orig_answer_text = answer[\"text\"]\n                        answer_offset = answer[\"answer_start\"]\n                        answer_length = len(orig_answer_text)\n                        start_position = char_to_word_offset[answer_offset]\n                        end_position = char_to_word_offset[answer_offset + answer_length - 1]\n                        # Only add answers where the text can be exactly recovered from the\n                        # document. If this CAN'T happen it's likely due to weird Unicode\n                        # stuff so we will just skip the example.\n                        #\n                        # Note that this means for training mode, every example is NOT\n                        # guaranteed to be preserved.\n                        actual_text = \" \".join(doc_tokens[start_position:(end_position + 1)])\n                        cleaned_answer_text = \" \".join(\n                            whitespace_tokenize(orig_answer_text))\n                        if actual_text.find(cleaned_answer_text) == -1:\n                            logger.warning(\"Could not find answer: '%s' vs. '%s'\",\n                                           actual_text, cleaned_answer_text)\n                            continue\n                    else:\n                        start_position = -1\n                        end_position = -1\n                        orig_answer_text = \"\"\n\n                example = SquadExample(\n                    qas_id=qas_id,\n                    question_text=question_text,\n                    doc_tokens=doc_tokens,\n                    orig_answer_text=orig_answer_text,\n                    start_position=start_position,\n                    end_position=end_position,\n                    is_impossible=is_impossible)\n                examples.append(example)\n    return examples"}
{"prompt": "def on_moved(self, event):\n        \"\"\"On moved method\"\"\"", "output": "\n        if self.selected_item:\n            self.selected_item.on_moved(event)\n            return True\n        return False\n\n    def on_double_clicked(self, event):\n        \"\"\"On double click method\"\"\"\n        if self.selected_item:\n            self.selected_item.on_double_clicked(event)\n            return True\n        return False\n\n    def on_double_released(self, event):\n        \"\"\"On double release method\"\"\"\n        if self.selected_item:\n            self.selected_item.on_double_released(event)\n            return True\n        return False\n\n    def on_double", "solution": "\n        for delegate in self.delegates:\n            if hasattr(delegate, \"on_moved\"):\n                delegate.on_moved(event)"}
{"prompt": "def _prepare_request(self, command, query):\n        \"\"\"\n        :param command: The Url command parameter\n        :type  command: str\n\n        :param query: will get json encoded.\n        :type  query: dict\n\n        :return:\n        \"\"\"", "output": "\n        if not self.session:\n            self.session = requests.Session()\n        if not self.session.headers.get('content-type'):\n            self.session.headers['content-type'] = 'application/json'\n        if not self.session.headers.get('content-length'):\n            self.session.headers['content-length'] = '0'\n        if not self.session.headers.get('content-md5'):\n            self.session.headers['content-md5'] = '0'\n        if not self.session.headers.get('content-md5-digest'):", "solution": "\n        from luckydonaldUtils.encoding import to_native as n\n        from pytgbot.api_types.sendable import Sendable\n        from pytgbot.api_types import as_array\n        from DictObject import DictObject\n        import json\n\n        params = {}\n        for key in query.keys():\n            element = query[key]\n            if element is not None:\n                if isinstance(element, Sendable):\n                    params[key] = json.dumps(as_array(element))\n                else:\n                    params[key] = element\n        url = self._base_url.format(api_key=n(self.api_key), command=n(command))\n        return DictObject(url=url, params=params)"}
{"prompt": "def parse_band_set_name(self, message):\n        \"\"\"Process incoming message indicating service name change.\"\"\"", "output": "\n        self.update_lock.acquire()\n        self.update_lock.release()\n        self.update_lock.acquire()\n        self.update_lock.release()\n        self.update_lock.release()\n        self.update_lock.release()\n        self.update_lock.release()\n        self.update_lock.release()\n        self.update_lock.release()\n        self.update_lock.release()\n        self.update_lock.release()\n        self.update_lock.release()\n        self.update_lock.release()\n        self.update_lock.release()\n        self", "solution": "\n        if message.get(\"name\"):\n            self._service_name = message[\"name\"]\n        else:\n            self.log.warning(\n                \"Received broken record on set_name band\\nMessage: %s\", str(message)\n            )"}
{"prompt": "def validate_sum(parameter_container, validation_message, **kwargs):\n    \"\"\"Validate the sum of parameter value's.\n\n    :param parameter_container: The container that use this validator.\n    :type parameter_container: ParameterContainer\n\n    :param validation_message: The message if there is validation error.\n    :type validation_message: str\n\n    :param kwargs: Keywords Argument.\n    :type kwargs: dict\n\n    :returns: Dictionary of valid and message.\n    :rtype: dict\n\n    Note: The code is not the best I wrote, since there are two alternatives.\n    1. If there is no None, the sum must be equal to 1\n    2. If there is no None, the sum must be less than 1\n    \"\"\"", "output": " a parameter with a default value, it is not allowed to be empty.\n    2. If there is a parameter with a default value, it is not allowed to be empty.\n    \"\"\"\n    if not kwargs:\n        return validation_message\n\n    if parameter_container.default_value is None:\n        return validation_message\n\n    if parameter_container.is_valid(kwargs):\n        return validation_message\n\n    if parameter_container.is_message(kwargs):\n        return validation_message\n\n    raise ValueError(\"Invalid sum of parameters: %s\" % kwargs)\n\n\ndef validate_min(parameter_container, validation_message", "solution": "\n    parameters = parameter_container.get_parameters(False)\n    values = []\n    for parameter in parameters:\n        if parameter.selected_option_type() in [SINGLE_DYNAMIC, STATIC]:\n            values.append(parameter.value)\n\n    sum_threshold = kwargs.get('max', 1)\n\n    if None in values:\n        # If there is None, just check to not exceeding validation_threshold\n        clean_value = [x for x in values if x is not None]\n        values.remove(None)\n        if sum(clean_value) > sum_threshold:\n            return {\n                'valid': False,\n                'message': validation_message\n            }\n    else:\n        # Just make sure to not have more than validation_threshold.\n        if sum(values) > sum_threshold:\n            return {\n                'valid': False,\n                'message': validation_message\n            }\n    return {\n        'valid': True,\n        'message': ''\n    }"}
{"prompt": "def draw_char_screen(self):\n\t\t\"\"\"\n\t\tDraws the output buffered in the char_buffer.\n\t\t\"\"\"", "output": "\n\t\tif self.is_running:\n\t\t\tself.is_running = False\n\t\t\tself.draw_char_running()\n\t\telse:\n\t\t\tself.is_running = True\n\t\t\tself.draw_char_not_running()\n\t\tif self.is_running:\n\t\t\tself.is_running = False\n\t\t\tself.draw_char_not_running()\n\t\telse:\n\t\t\tself.is_running = False\n\t\tif self.is_running:\n\t\t\tself.is_running = False\n\t\t\tself.draw_char_not_running()\n\t\t", "solution": "\n\t\tself.screen = Image.new(\"RGB\", (self.height, self.width))\n\t\tself.drawer = ImageDraw.Draw(self.screen)\n\n\t\tfor sy, line in enumerate(self.char_buffer):\n\t\t\tfor sx, tinfo in enumerate(line):\n\t\t\t\tself.drawer.text((sx * 6, sy * 9), tinfo[0], fill=tinfo[1:])\n\t\tself.output_device.interrupt()"}
{"prompt": "def has_sequential_ids(data_wrapper):\n    \"\"\"Check that IDs are increasing and consecutive\n\n    returns tuple (bool, list of IDs that are not consecutive\n    with their predecessor)\n    \"\"\"", "output": "\n    for i in range(len(data_wrapper.graph)):\n        if data_wrapper.graph[i] not in sequential_ids:\n            return False, []\n    return True, sequential_ids\n\n\ndef has_cycle(data_wrapper):\n    \"\"\"Check that cycles are increasing\n\n    returns tuple (bool, list of IDs that are not consecutive\n    with their predecessor)\n    \"\"\"\n    for i in range(len(data_wrapper.graph)):\n        if data_wrapper.graph[i] not in cycle:\n            return False, []\n    return True, cycle\n\n\ndef has_cycle_with", "solution": "\n    db = data_wrapper.data_block\n    ids = db[:, COLS.ID]\n    steps = ids[np.where(np.diff(ids) != 1)[0] + 1].astype(int)\n    return CheckResult(len(steps) == 0, steps)"}
{"prompt": "def zpipe(ctx):\n    \"\"\"build inproc pipe for talking to threads\n\n    mimic pipe used in czmq zthread_fork.\n\n    Returns a pair of PAIRs connected via inproc\n    \"\"\"", "output": "\n    ctx.inproc = zpipe_inproc(ctx.inproc)\n    return ctx.inproc\n\n\ndef zthread_fork(ctx):\n    \"\"\"build inthread fork for talking to threads\n\n    mimic fork used in czmq zthread_fork.\n\n    Returns a pair of PAIRs connected via inthread\n    \"\"\"\n    ctx.inthread = zpipe_inthread(ctx.inthread)\n    return ctx.inthread\n\n\ndef zmq_version():\n    \"\"\"return the version of zmq\n\n    returns a tuple of (major, minor)\n    \"\"\"\n    return tuple(map(int", "solution": "\n    a = ctx.socket(zmq.PAIR)\n    a.linger = 0\n    b = ctx.socket(zmq.PAIR)\n    b.linger = 0\n    socket_set_hwm(a, 1)\n    socket_set_hwm(b, 1)\n    iface = \"inproc://%s\" % binascii.hexlify(os.urandom(8))\n    a.bind(iface)\n    b.connect(iface)\n    return a, b"}
{"prompt": "def tasks(self):\n        \"\"\"Generates a list of all Tasks.\"\"\"", "output": "\n        tasks = []\n        for task in self.tasks_by_id.values():\n            tasks.append(task)\n        return tasks\n\n    def get_task(self, task_id):\n        \"\"\"Returns a Task object for the given task_id.\"\"\"\n        return self.tasks_by_id[task_id]\n\n    def get_task_by_id(self, task_id):\n        \"\"\"Returns a Task object for the given task_id.\"\"\"\n        return self.tasks_by_id\n\n    def get_tasks(self):\n        \"\"\"Generates a list of all Tasks.\"\"\"\n        return self.tasks", "solution": "\n        tasks_response = self.get_request('tasks/')\n        return [Task(self, tjson['task']) for tjson in tasks_response]"}
{"prompt": "def genmatrix(self, num_processes=1):\n        \"\"\"\n        Actually generate the matrix\n\n        :param num_processes: If you want to use multiprocessing to split up the\n            work and run ``combinfunc()`` in parallel, specify\n            ``num_processes > 1`` and this number of workers will be spun up,\n            the work is split up amongst them evenly.\n        \"\"\"", "output": "\n        if num_processes is None:\n            num_processes = self.num_workers\n        if num_processes > 1:\n            self.log.info(\"Using %d processes\" % num_processes)\n        else:\n            self.log.info(\"Using %d\" % num_processes)\n        self.log.info(\"Using %d\" % num_processes)\n        self.log.info(\"Using %d\" % num_processes)\n        self.log.info(\"Using %d\" % num_processes)\n        self.log.info(\"Using %d\" % num_processes)\n        self", "solution": "\n        use_multiprocessing = num_processes > 1\n        if use_multiprocessing:\n            self.task_queue = Queue()\n            self.done_queue = Queue()\n\n        self.matrix = []\n        logger.info(\"Generating matrix for %s items - O(n^2)\", len(self.data))\n        if use_multiprocessing:\n            logger.info(\"Using multiprocessing on %s processes!\", num_processes)\n\n        if use_multiprocessing:\n            logger.info(\"Spinning up %s workers\", num_processes)\n            processes = [Process(target=self.worker) for i in range(num_processes)]\n            [process.start() for process in processes]\n\n        for row_index, item in enumerate(self.data):\n            logger.debug(\"Generating row %s/%s (%0.2f%%)\",\n                         row_index,\n                         len(self.data),\n                         100.0 * row_index / len(self.data))\n            row = {}\n            if use_multiprocessing:\n                num_tasks_queued = num_tasks_completed = 0\n            for col_index, item2 in enumerate(self.data):\n                if self.diagonal is not None and col_index == row_index:\n                    # This is a cell on the diagonal\n                    row[col_index] = self.diagonal\n                elif self.symmetric and col_index < row_index:\n                    # The matrix is symmetric and we are \"in the lower left\n                    # triangle\" - fill this in after (in case of multiprocessing)\n                    pass\n                # Otherwise, this cell is not on the diagonal and we do indeed\n                # need to call combinfunc()\n                elif use_multiprocessing:\n                    # Add that thing to the task queue!\n                    self.task_queue.put((col_index, item, item2))\n                    num_tasks_queued += 1\n                    # Start grabbing the results as we go, so as not to stuff all of\n                    # the worker args into memory at once (as Queue.get() is a\n                    # blocking operation)\n                    if num_tasks_queued > num_processes:\n                        col_index, result = self.done_queue.get()\n                        row[col_index] = result\n                        num_tasks_completed += 1\n                else:\n                    # Otherwise do it here, in line\n                    "}
{"prompt": "def columns_used(self):\n        \"\"\"\n        Returns all the columns used across all models in the group\n        for filtering and in the model expression.\n\n        \"\"\"", "output": "\n        return list(self.get_group_by_filters())\n\n    def get_group_by_filters(self):\n        \"\"\"\n        Returns a list of all models that match the given filters.\n\n        \"\"\"\n        if self.query.where:\n            group_by = []\n            for bit in self.query.where.split():\n                group_by.append(bit.split()[0])\n            return group_by\n        else:\n            return self.model.objects.all()\n\n    def get_group_by_fields(self):\n        \"\"\"\n        Returns a list of all fields that match the given filters.\n\n       ", "solution": "\n        return list(tz.unique(tz.concatv(\n            util.columns_in_filters(self.fit_filters),\n            util.columns_in_filters(self.predict_filters),\n            util.columns_in_formula(self.default_model_expr),\n            self._group.columns_used(),\n            [self.segmentation_col])))"}
{"prompt": "def run(self, *args):\n        \"\"\"Add an identity to the registry.\"\"\"", "output": "\n        if not args:\n            print('No identity to remove.')\n        else:\n            for arg in args:\n                if not arg.startswith('-'):\n                    arg = '-%s' % arg\n                if arg not in self.args:\n                    self.args[arg] = None\n\n    def get_args(self):\n        \"\"\"Return the arguments to be passed to the command.\"\"\"\n        return self.args\n\n    def get_help(self):\n        \"\"\"Return the help text for the command.\"\"\"\n        return self.help\n\n    def get_help_args(self):\n        \"\"\"Return the arguments to be passed to the command.\"\"\"\n       ", "solution": "\n\n        params = self.parser.parse_args(args)\n\n        code = self.add(params.source, params.email, params.name, params.username,\n                        params.uuid, params.matching, params.interactive)\n\n        return code"}
{"prompt": "def pop_prefix(string: str):\n        \"\"\"Erases the prefix and returns it.\n        :throws IndexError: There is no prefix.\n        :return A set with two elements: 1- the prefix, 2- the type without it.\n        \"\"\"", "output": "\n        return set(string[1:].split(' '))\n\n    def get_prefix_from_string(self, string: str) -> str:\n        \"\"\"Returns the prefix of the given string.\n        :throws ValueError: There is no type.\n        :return The prefix of the given string.\n        \"\"\"\n        return self.get_prefix_from_string(string)\n\n    def get_prefix_from_type(self, type: str) -> str:\n        \"\"\"Returns the prefix of the given type.\n        :throws ValueError: There is no type.\n        :return The prefix of the given type.\n        \"\"\"\n       ", "solution": "\n        result = string.split(Naming.TYPE_PREFIX)\n        if len(result) == 1:\n            result = string.split(Naming.RESOURCE_PREFIX)\n            if len(result) == 1:\n                raise IndexError()\n        return result"}
{"prompt": "def export_public_key(self):\n\t\t\"\"\" Export a public key in PEM-format\n\n\t\t:return: bytes\n\t\t\"\"\"", "output": "\n\t\treturn self.key.export_to_pem()\n\n\tdef export_private_key(self):\n\t\t\"\"\" Export a private key in PEM-format\n\n\t\t:return: bytes\n\t\t\"\"\"\n\t\treturn self.key.export_to_pem(password=self.password)\n\n\tdef import_public_key(self, public_key_file, password=None):\n\t\t\"\"\" Import a public key from PEM-format\n\n\t\t:param str public_key_file: path to public key file\n\t\t:param str password: optional password for the public key file\n\t\t\"\"\"", "solution": "\n\t\tif self.__public_key is None:\n\t\t\traise ValueError('Unable to call this method. Public key must be set')\n\n\t\treturn self.__public_key.public_bytes(\n\t\t\tencoding=serialization.Encoding.PEM,\n\t\t\tformat=serialization.PublicFormat.SubjectPublicKeyInfo\n\t\t)"}
{"prompt": "def abort(err):\n    \"\"\"Abort everything, everywhere.\"\"\"", "output": "\n    sys.exit(err)\n\n\ndef main():\n    \"\"\"Main function.\"\"\"\n    # Parse command line arguments.\n    parser = argparse.ArgumentParser(description='Run the script.')\n    parser.add_argument('-v', '--verbose', action='store_true')\n    parser.add_argument('-q', '--quiet', action='store_false')\n    parser.add_argument('-o', '--output', type=str, default='-',\n                        help='Output file name (default: %(default)s).')\n    parser.add_argument('-p', '--parallel', type=int, default=1,\n                        help", "solution": "\n    if _debug: abort._debug(\"abort %r\", err)\n\n    # start with the server\n    if IOServer._highlander:\n        IOServer._highlander.abort(err)\n\n    # now do everything local\n    for controller in _local_controllers.values():\n        controller.abort(err)"}
{"prompt": "def login_details(self):\n        \"\"\"\n        Gets the login details\n\n        Returns:\n        List of login details\n        \"\"\"", "output": "\n        return self.get_list('GetLoginDetails', [\n            ('accountId', self.account_id),\n            ('login', self.login),\n            ('password', self.password),\n            ('accountName', self.account_name),\n            ('accountId', self.account_id),\n            ('loginName', self.login_name),\n            ('loginPassword', self.password),\n            ('loginPassword', self.password),\n            ('loginPassword', self.password),\n            ('loginPassword', self.password),\n            ('loginPassword', self.password),\n            ('loginPassword', self.password),\n           ", "solution": "\n        if not self.__login_details:\n            self.__login_details = LoginDetails(self.__connection)\n        return self.__login_details"}
{"prompt": "def role_get(name, user=None, host=None, port=None, maintenance_db=None,\n             password=None, runas=None, return_password=False):\n    \"\"\"\n    Return a dict with information about users of a Postgres server.\n\n    Set return_password to True to get password hash in the result.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' postgres.role_get postgres\n    \"\"\"", "output": "\n    return dict(\n        name=name,\n        user=user,\n        host=host,\n        port=port,\n        maintenance_db=maintenance_db,\n        password=hash_password(password, runas),\n        role_status=True,\n    )\n\n\ndef role_set(name, user=None, host=None, port=None, maintenance_db=None,\n             password=None, runas=None, return_password=False):\n    \"\"\"\n    Return a dict with information about users of a Postgres server.\n\n    Set return_password to True to get password hash in the result.", "solution": "\n    all_users = user_list(user=user,\n                          host=host,\n                          port=port,\n                          maintenance_db=maintenance_db,\n                          password=password,\n                          runas=runas,\n                          return_password=return_password)\n    try:\n        return all_users.get(name, None)\n    except AttributeError:\n        log.error('Could not retrieve Postgres role. Is Postgres running?')\n        return None"}
{"prompt": "def process_paths(options, candidates=None, error=True):\n    \"\"\"Process files and log errors.\"\"\"", "output": "\n    if candidates is None:\n        candidates = []\n    if error:\n        print('ERROR: %s' % error, file=sys.stderr)\n    for path in options.paths:\n        if path.startswith('-'):\n            path = path[1:]\n        if os.path.isdir(path):\n            if options.verbose:\n                print('Processing directory %s' % path, file=sys.stderr)\n            for root, dirs, files in os.walk(path):\n                for f in files:\n                    if options.verbose:\n                        print('  %s' % f, file=sys.stderr)", "solution": "\n    errors = check_path(options, rootdir=CURDIR, candidates=candidates)\n\n    if options.format in ['pycodestyle', 'pep8']:\n        pattern = \"%(filename)s:%(lnum)s:%(col)s: %(text)s\"\n    elif options.format == 'pylint':\n        pattern = \"%(filename)s:%(lnum)s: [%(type)s] %(text)s\"\n    else:  # 'parsable'\n        pattern = \"%(filename)s:%(lnum)s:%(col)s: [%(type)s] %(text)s\"\n\n    for er in errors:\n        if options.abspath:\n            er._info['filename'] = op.abspath(er.filename)\n        LOGGER.warning(pattern, er._info)\n\n    if error:\n        sys.exit(int(bool(errors)))\n\n    return errors"}
{"prompt": "def create_bundle(self, name=None, media_url=None,\n                      audio_channel=None, metadata=None, notify_url=None,\n                      external_id=None):\n        \"\"\"Create a new bundle.\n\n        'metadata' may be None, or an object that can be converted to a JSON\n        string.  See API documentation for restrictions.  The conversion\n        will take place before the API call.\n\n        All other parameters are also optional. For information about these\n        see https://api.clarify.io/docs#!/audio/v1audio_post_1.\n\n        Returns a data structure equivalent to the JSON returned by the API.\n\n        If the response status is not 2xx, throws an APIException.\n        If the JSON to python data struct conversion fails, throws an\n        APIDataException.\"\"\"", "output": ".\n        \"\"\"\n        if name is None:\n            name = self.name\n        if metadata is None:\n            metadata = {}\n        if media_channel is None:\n            media_channel = self.audio_channel\n        if notify_url is None:\n            notify_url = self.notify_url\n        if external_id is None:\n            external_id = self.external_id\n        if self.bundle_id is None:\n            self.bundle_id = self.audio_id + '-bundle'\n        if self.bundle_url is None:\n            self.bundle_url = self.audio_url +", "solution": "\n\n        # Prepare the data we're going to include in our bundle creation.\n        path = '/' + __api_version__ + '/' + BUNDLES_PATH\n\n        data = None\n\n        fields = {}\n        if name is not None:\n            fields['name'] = name\n        if media_url is not None:\n            fields['media_url'] = media_url\n        if audio_channel is not None:\n            fields['audio_channel'] = audio_channel\n        if metadata is not None:\n            fields['metadata'] = json.dumps(metadata)\n        if notify_url is not None:\n            fields['notify_url'] = notify_url\n        if external_id is not None:\n            fields['external_id'] = external_id\n\n        if len(fields) > 0:\n            data = fields\n\n        raw_result = self.post(path, data)\n\n        if raw_result.status < 200 or raw_result.status > 202:\n            raise APIException(raw_result.status, raw_result.json)\n\n        # Convert the JSON to a python data struct.\n\n        return self._parse_json(raw_result.json)"}
{"prompt": "def resp_set_label(self, resp, label=None):\n        \"\"\"Default callback for get_label/set_label\n        \"\"\"", "output": "\n        return self.set_label(resp, label)\n\n    def resp_set_title(self, resp, title=None):\n        \"\"\"Default callback for set_title\n        \"\"\"\n        return self.set_title(resp, title)\n\n    def resp_set_body(self, resp, body=None):\n        \"\"\"Default callback for set_body\n        \"\"\"\n        return self.set_body(resp, body)\n\n    def resp_set_body_from_file(self, resp, filename, body, filename_params=None):\n        \"\"\"Default callback for set_body_from_file\n        \"\"\"", "solution": "\n        if label:\n            self.label=label\n        elif resp:\n            self.label=resp.label.decode().replace(\"\\x00\", \"\")"}
{"prompt": "def directory_exists(value,\n                     allow_empty = False,\n                     **kwargs):\n    \"\"\"Validate that ``value`` is a valid directory that exists on the local\n    filesystem.\n\n    :param value: The value to validate.\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is empty. If ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`\n      if ``value`` is empty. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :returns: The file name represented by ``value``.\n    :rtype: Path-like object / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``allow_empty`` is ``False`` and ``value``\n      is empty\n    :raises NotPathlikeError: if ``value`` is not a path-like object\n    :raises PathExistsError: if ``value`` does not exist on the local filesystem\n    :raises NotADirectoryError: if ``value`` is not a valid directory\n\n    \"\"\"", "output": " :param kwargs: Keyword arguments to be passed to the\n      :class:`DirectoryExists <directory_exists.directories.DirectoryExists>` constructor.\n\n    :raises: :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`\n      if ``value`` is empty. Defaults to ``False``.\n    \"\"\"\n    if not allow_empty and not value:\n      raise EmptyValueError('Empty directory cannot be None')\n    elif not allow_empty and value:\n      raise EmptyValueError('Empty directory cannot be None')\n    elif not allow_empty and not value:\n      return None\n    else:\n      raise ValueError('DirectoryExists must have", "solution": "\n    if not value and not allow_empty:\n        raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif not value:\n        return None\n\n    value = path_exists(value, force_run = True)                                # pylint: disable=E1123\n\n    if not os.path.isdir(value):\n        raise errors.NotADirectoryError('value (%s) is not a directory' % value)\n\n    return value"}
{"prompt": "def write(self, data):\n        \"\"\"Write samples to an output stream.\n\n        As much as one blocksize of audio data will be played\n        without blocking. If more than one blocksize was provided,\n        the function will only return when all but one blocksize\n        has been played.\n\n        Data will be converted to a numpy matrix. Multichannel data\n        should be provided as a (frames, channels) matrix. If the\n        data is provided as a 1-dim array, it will be treated as mono\n        data and will be played on all channels simultaneously. If the\n        data is provided as a 2-dim matrix and fewer tracks are\n        provided than channels, silence will be played on the missing\n        channels. Similarly, if more tracks are provided than there\n        are channels, the extraneous channels will not be played.\n\n        \"\"\"", "output": " array, it will be treated as stereo data.\n        \"\"\"\n        if len(data.shape)!= 2:\n            raise ValueError(\"data must be 2-dim\")\n        if data.ndim!= 1:\n            raise ValueError(\"data must be 1-dim\")\n        if data.shape[1]!= self.channels:\n            raise ValueError(\"data must be of equal length\")\n        if data.shape[0]!= 1:\n            raise ValueError(\"data must be of equal length\")\n        if data.shape[0]!= 2:\n            raise ValueError(\"data must be 2-dim array\")\n        if data.shape", "solution": "\n        frames = len(data)\n        _, channels = _split(self.channels)\n        _, dtype = _split(self.dtype)\n\n        if (not isinstance(data, np.ndarray) or data.dtype != dtype):\n            data = np.array(data, dtype=dtype)\n        if len(data.shape) == 1:\n            # play mono signals on all channels\n            data = np.tile(data, (channels, 1)).T\n        if data.shape[1] > channels:\n            data = data[:, :channels]\n        if data.shape < (frames, channels):\n            # if less data is available than requested, pad with zeros.\n            tmp = data\n            data = np.zeros((frames, channels), dtype=dtype)\n            data[:tmp.shape[0], :tmp.shape[1]] = tmp\n\n        data = data.ravel().tostring()\n        err = _pa.Pa_WriteStream(self._stream, data, frames)\n        self._handle_error(err)"}
{"prompt": "def to_xml(self):\n        \"\"\"\n        Serialize all properties as XML\n        \"\"\"", "output": "\n        return etree.tostring(self._xml, encoding='unicode')\n\n    def to_str(self):\n        \"\"\"\n        Serialize all properties as string\n        \"\"\"\n        return etree.tostring(self._str, encoding='unicode')\n\n    def to_xml_string(self):\n        \"\"\"\n        Serialize all properties as XML string\n        \"\"\"\n        return etree.tostring(self._xml_string, encoding='unicode')\n\n    def to_str_list(self):\n        \"\"\"\n        Serialize all properties as list of strings\n        \"\"\"\n        return [to_xml(p) for p in self.to_str_list_", "solution": "\n        ret = '<exif>'\n        for k in self.__dict__:\n            ret += '<%s>%s</%s>' % (k, self.__dict__[k], k)\n        ret += '</exif>'\n        return ret"}
{"prompt": "def parse_irreg(self, l):\n        \"\"\" Constructeur de la classe Irreg.\n\n        :param l: Ligne de chargement des irr\u00e9guliers\n        :type l: str\n        \"\"\"", "output": "\n        self.lignes.append(l)\n        self.parse_irreg_amount(l)\n        self.parse_irreg_date(l)\n        self.parse_irreg_amount(l)\n        self.parse_irreg_amount(l)\n        self.parse_irreg_date(l)\n        self.parse_irreg_date(l)\n        self.parse_irreg_date(l)\n        self.parse_irreg_amount(l)\n        self.parse_irreg_date(l)\n        self.parse_irreg_date", "solution": "\n        ecl = l.split(':')\n        grq = ecl[0]\n        exclusif = False\n        if grq.endswith(\"*\"):\n            grq = grq[:-1]\n            exclusif = True\n        return Irreg(\n            graphie_accentuee=grq, graphie=atone(grq),\n            exclusif=exclusif,\n            morphos=listeI(ecl[2]),\n            lemme=self.lemmatiseur.lemme(ecl[1]),\n            parent=self.lemmatiseur\n        )"}
{"prompt": "def sigmoid(f=Ellipsis, mu=0, sigma=1, scale=1, invert=False, normalize=False):\n    \"\"\"\n    sigmoid() yields a potential function that is equivalent to the integral of gaussian(), i.e.,\n      the error function, but scaled to match gaussian().\n    sigmoid(f) is equivalent to compose(sigmoid(), f).\n\n    All options that are accepted by the gaussian() function are accepted by sigmoid() with the same\n    default values and are handled in an equivalent manner with the exception of the invert option;\n    when a sigmoid is inverted, the function approaches its maximum value at -inf and approaches 0\n    at inf.\n\n    Note that because sigmoid() explicitly matches gaussian(), the base formula used is as follows:\n      f(x) = scale * sigma * sqrt(pi/2) * erf((x - mu) / (sqrt(2) * sigma))\n      k*sig*Sqrt[Pi/2] Erf[(x - mu)/sig/Sqrt[2]]\n    \"\"\"", "output": "1.0.\n\n    :param f: The function to be approximated.\n    :param mu: The minimum value of the Gaussian.\n    :param sigma: The maximum value of the Gaussian.\n    :param scale: The scale of the Gaussian.\n    :param invert: If True, the function returns the inverse of the error function.\n    :param normalize: If True, the function returns the normalized value.\n    :return: The function approximation.\n    \"\"\"\n    if normalize:\n        f = f / (1.0 + np.sqrt(scale))\n\n    if invert:\n        f = 1.0 / (1.", "solution": "\n    f = to_potential(f)\n    F = erf((f - mu) / (sigma * np.sqrt(2.0)))\n    if invert: F = 1 - F\n    F = np.sqrt(np.pi / 2) * scale * F\n    if normalize: F = F / (np.sqrt(2.0*np.pi) * sigma)\n    return F"}
{"prompt": "def accumulate(self, axis: AxisIdentifier) -> HistogramBase:\n        \"\"\"Calculate cumulative frequencies along a certain axis.\n\n        Returns\n        -------\n        new_hist: Histogram of the same type & size\n        \"\"\"", "output": "\n        return HistogramBase(self.data.sum(axis=axis), self.data.size)\n\n    def __repr__(self) -> str:\n        return f\"Histogram({self.data.shape}, {self.name})\"\n\n    def __str__(self) -> str:\n        return self.name\n\n    def __eq__(self, other: object) -> bool:\n        if isinstance(other, Histogram):\n            return self.data == other.data\n        return NotImplemented\n\n    def __ne__(self, other: object) -> bool:\n        if isinstance(other, HistogramBase):\n            return not self", "solution": "\n        # TODO: Merge with Histogram1D.cumulative_frequencies\n        # TODO: Deal with errors and totals etc.\n        # TODO: inplace\n        new_one = self.copy()\n        axis_id = self._get_axis(axis)\n        new_one._frequencies = np.cumsum(new_one.frequencies, axis_id[0])\n        return new_one"}
{"prompt": "def decrypt(self, key_password):\n        \"\"\"\n        Decrypts the entry using the given password. Has no effect if the entry has already been decrypted.\n\n        :param str key_password: The password to decrypt the entry with. If the entry was loaded from a JCEKS keystore,\n                                 the password must not contain any characters outside of the ASCII character set.\n        :raises DecryptionFailureException: If the entry could not be decrypted using the given password.\n        :raises UnexpectedAlgorithmException: If the entry was encrypted with an unknown or unexpected algorithm\n        :raise ValueError: If the entry was loaded from a JCEKS keystore and the password contains non-ASCII characters.\n        \"\"\"", "output": " a JCEKS keystore, the given password is invalid.\n        \"\"\"\n        if not key_password:\n            raise ValueError(\"The password to decrypt must not be empty.\")\n        if not self.is_valid_key(key_password):\n            raise ValueError(\"The given password is invalid.\")\n        if self.is_valid_key(key_password):\n            raise ValueError(\"The given password is invalid.\")\n        self.key_password = key_password\n        self.decrypted = True\n\n    def is_valid_key(self, password):\n        \"\"\"\n        Checks if the given password is valid.\n\n        :param str password", "solution": "\n        if self.is_decrypted():\n            return\n\n        encrypted_info = decoder.decode(self._encrypted, asn1Spec=rfc5208.EncryptedPrivateKeyInfo())[0]\n        algo_id = encrypted_info['encryptionAlgorithm']['algorithm'].asTuple()\n        algo_params = encrypted_info['encryptionAlgorithm']['parameters'].asOctets()\n        encrypted_private_key = encrypted_info['encryptedData'].asOctets()\n\n        plaintext = None\n        try:\n            if algo_id == sun_crypto.SUN_JKS_ALGO_ID:\n                plaintext = sun_crypto.jks_pkey_decrypt(encrypted_private_key, key_password)\n\n            elif algo_id == sun_crypto.SUN_JCE_ALGO_ID:\n                if self.store_type != \"jceks\":\n                    raise UnexpectedAlgorithmException(\"Encountered JCEKS private key protection algorithm in JKS keystore\")\n                # see RFC 2898, section A.3: PBES1 and definitions of AlgorithmIdentifier and PBEParameter\n                params = decoder.decode(algo_params, asn1Spec=rfc2898.PBEParameter())[0]\n                salt = params['salt'].asOctets()\n                iteration_count = int(params['iterationCount'])\n                plaintext = sun_crypto.jce_pbe_decrypt(encrypted_private_key, key_password, salt, iteration_count)\n            else:\n                raise UnexpectedAlgorithmException(\"Unknown %s private key protection algorithm: %s\" % (self.store_type.upper(), algo_id))\n\n        except (BadHashCheckException, BadPaddingException):\n            raise DecryptionFailureException(\"Failed to decrypt data for private key '%s'; wrong password?\" % self.alias)\n\n        # at this point, 'plaintext' is a PKCS#8 PrivateKeyInfo (see RFC 5208)\n        private_key_info = decoder.decode(plaintext, asn1Spec=rfc5208.PrivateKeyInfo())[0]\n        key = private_key_info['privateKey'].asOctets()\n        algorithm_oid = private_key_info['privateKeyAlgorithm']['algorithm'].asTuple()\n\n        self._encrypted = None\n        self._pkey = key\n        self._pkey_pkcs8 = plaintext\n        self._algorithm_oid = algorithm_oid"}
{"prompt": "def _get_relative_reference(self, cursor, ref_key):\n        \"\"\"Returns absolute reference code for key.\n\n        Parameters\n        ----------\n\n        cursor: 3-tuple of Integer\n        \\tCurrent cursor position\n        ref_key: 3-tuple of Integer\n        \\tAbsolute reference key\n\n        \"\"\"", "output": "\n        return cursor[0] + ref_key[0] + ref_key[1]\n\n    def _get_key_code(self, key):\n        \"\"\"Returns the key code for key.\n\n        Parameters\n        ----------\n\n        key: Integer\n        \\tKey position\n\n        \"\"\"\n        return key[0] + key[1]\n\n    def _get_key_name(self, cursor, ref_key):\n        \"\"\"Returns the key name for key.\n\n        Parameters\n        ----------\n\n        cursor: 3-tuple of Integer\n        \\tCurrent cursor position\n        ref_key: 3-tuple of Integer\n        \\tKey name\n\n       ", "solution": "\n\n        magics = [\"X\", \"Y\", \"Z\"]\n\n        # mapper takes magic, key, ref_key to build string\n        def get_rel_key_ele(cursor_ele, ref_key_ele):\n            "}
{"prompt": "def get_data_point(self, n):\n        \"\"\"\n        Returns the n'th data point (starting at 0) from all columns.\n\n        Parameters\n        ----------\n        n       \n            Index of data point to return.\n        \"\"\"", "output": "\n        return self.data[n]\n\n    def get_column_index(self, column):\n        \"\"\"\n        Returns the index of the column in the matrix.\n\n        Parameters\n        ----------\n        column  \n            Column number.\n        \"\"\"\n        return self.data.index(column)\n\n    def get_row_index(self, row):\n        \"\"\"\n        Returns the index of the row in the matrix.\n\n        Parameters\n        ----------\n        row  \n            Row number.\n\n        Returns\n        -------\n        \"\"\"\n        return self.data.index(row)\n\n    def get_column_value(self, column, value):\n        \"\"\"\n        Returns the value", "solution": "\n        # loop over the columns and pop the data\n        point = []\n        for k in self.ckeys: point.append(self[k][n])\n        return point"}
{"prompt": "def energy(self):\n        \"\"\"Compute the energy of the system\"\"\"", "output": "\n        return self.get_energy()\n\n    def get_energy(self):\n        \"\"\"Return the energy of the system\"\"\"\n        return self.get_energy_per_atom()\n\n    def get_forces(self):\n        \"\"\"Return the forces\"\"\"\n        return self.get_forces_per_atom()\n\n    def get_forces_per_atom(self):\n        \"\"\"Return the forces per atom\"\"\"\n        return self.get_forces_per_atom(self.get_forces())\n\n    def get_forces_atom_type(self, atom_type):\n        \"\"\"Return the forces", "solution": "\n        result = 0.0\n        for index1 in range(self.numc):\n            for index2 in range(index1):\n                if self.scaling[index1, index2] > 0:\n                    for se, ve in self.yield_pair_energies(index1, index2):\n                        result += se*ve*self.scaling[index1, index2]\n        return result"}
{"prompt": "def phi( n ):\n  \"\"\"Return the Euler totient function of n.\"\"\"", "output": "\n  if n == 0:\n    return 0\n  if n == 1:\n    return 1\n  if n % 2 == 0:\n    return 2\n  return phi(n/2) + phi(n/3) + phi(n/4)\n\ndef main():\n  \"\"\"Main function.\"\"\"\n  # Create the solver.\n  solver = pywrapcp.Solver('Euler')\n\n  #\n  # data\n  #\n  #\n  #\n  #\n  #\n  #\n  #\n  # solver.add(\n  #    'solver',\n  #     pywrapcp.Solver,\n  #    'solvers/solver.", "solution": "\n\n  assert isinstance( n, integer_types )\n\n  if n < 3: return 1\n\n  result = 1\n  ff = factorization( n )\n  for f in ff:\n    e = f[1]\n    if e > 1:\n      result = result * f[0] ** (e-1) * ( f[0] - 1 )\n    else:\n      result = result * ( f[0] - 1 )\n  return result"}
{"prompt": "def get_stp_brief_info_output_has_more(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"", "output": "\n        config = ET.Element(\"config\")\n        get_stp_brief_info = ET.Element(\"get_stp_brief_info\")\n        config = get_stp_brief_info\n        output = ET.SubElement(get_stp_brief_info, \"output\")\n        for x in kwargs:\n            if x in output.attrib:\n                return True\n        return False\n\n    def get_stp_brief_info_output_table(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n        config = ET.Element(\"config\")\n        get_stp_brief_info = ET.Element(\"get_", "solution": "\n        config = ET.Element(\"config\")\n        get_stp_brief_info = ET.Element(\"get_stp_brief_info\")\n        config = get_stp_brief_info\n        output = ET.SubElement(get_stp_brief_info, \"output\")\n        has_more = ET.SubElement(output, \"has-more\")\n        has_more.text = kwargs.pop('has_more')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)"}
{"prompt": "def load_json_file(i):\n    \"\"\"\n    Input:  {\n              json_file - name of file with json\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         = 16, if file not found (may be warning)\n                                         >  0, if error\n              (error)  - error text if return > 0\n\n              dict     - dict from json file\n            }\n    \"\"\"", "output": "\n    try:\n        with open(i, 'r') as f:\n            return json.load(f)\n    except IOError:\n        return 0\n\n\ndef save_json_file(json_file, data):\n    \"\"\"\n    Input:  {\n              json_file - name of file with json\n            }\n\n    Output: {\n              return       - return code =  0, if successful\n                                         =  16, if file not found (may be warning)\n                                         >  0, if error\n              (error)  - error text if return > 0\n\n              dict     - dict from json file\n            }\n    \"\"\"\n    try:", "solution": "\n\n    fn=i['json_file']\n\n    try:\n      if sys.version_info[0]>2:\n         f=open(fn, 'r', encoding='utf8')\n      else:\n         f=open(fn, 'r')\n    except Exception as e:\n       return {'return':16, 'error':'problem opening json file='+fn+' ('+format(e)+')'}\n\n    try:\n      s=f.read()\n    except Exception as e:\n       f.close()\n       return {'return':1, 'error':'problem reading json file='+fn+' ('+format(e)+')'}\n\n    f.close()\n\n    try:\n      if sys.version_info[0]>2:\n         d=json.loads(s)\n      else:\n         d=json.loads(s, encoding='utf8')\n    except Exception as e:\n       return {'return':1, 'error':'problem parsing json from file='+fn+' ('+format(e)+')'}\n\n    return {'return':0, 'dict': d}"}
{"prompt": "def include(self):\n        \"\"\"Return fields to include\n\n        :return list: a list of include information\n        \"\"\"", "output": "\n        return self._get_fields(self._include)\n\n    def exclude(self):\n        \"\"\"Return fields to exclude\n\n        :return list: a list of exclude information\n        \"\"\"\n        return self._get_fields(self._exclude)\n\n    def include_all(self):\n        \"\"\"Return fields to include\n\n        :return list: a list of include information\n        \"\"\"\n        return self._get_fields(self._include_all)\n\n    def exclude_all(self):\n        \"\"\"Return fields to exclude\n\n        :return list: a list of exclude information\n        \"\"\"\n        return self._get_fields(self._exclude_all)", "solution": "\n        include_param = self.qs.get('include', [])\n\n        if current_app.config.get('MAX_INCLUDE_DEPTH') is not None:\n            for include_path in include_param:\n                if len(include_path.split('.')) > current_app.config['MAX_INCLUDE_DEPTH']:\n                    raise InvalidInclude(\"You can't use include through more than {} relationships\"\n                                         .format(current_app.config['MAX_INCLUDE_DEPTH']))\n\n        return include_param.split(',') if include_param else []"}
{"prompt": "def check_input(prolog_file):\n    \"\"\"\n    Check for illegal predicates (like reading/writing, opening sockets, etc).\n    \"\"\"", "output": "\n    if not os.path.exists(prolog_file):\n        raise Exception('Prolog file not found: %s' % prolog_file)\n    with open(prolog_file, 'r') as f:\n        for line in f:\n            if line.startswith('Error: Invalid input'):\n                return False\n    return True\n\n\ndef check_output(prolog_file):\n    \"\"\"\n    Check for illegal predicates (like reading/writing, opening sockets, etc).\n    \"\"\"\n    if not os.path.exists(prolog_file):\n        raise Exception('Prolog file not found:", "solution": "\n    if prolog_file == None:\n        return\n    for pred in illegal_predicates:\n        if type(pred) == tuple:\n            print_name = pred[1]\n            pred = pred[0]\n        else:\n            print_name = pred\n        if re.search(r'[^\\w]' + pred + r'\\s*[\\(\\)\\:\\.\\,\\;]+', prolog_file):\n            raise Exception('Illegal predicate \"%s\" used in your input, aborting. If your own predicate clashes with a predefined YAP predicate, you must rename it.' % print_name)"}
{"prompt": "def convert_simple_rnn(builder, layer, input_names, output_names, keras_layer):\n    \"\"\"Convert an SimpleRNN layer from keras to coreml.\n\n    Parameters\n    ----------\n    keras_layer: layer\n        A keras layer object.\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.\n    \"\"\"", "output": "\n    if keras_layer.type == \"rnn\":\n        return convert_rnn(builder, keras_layer, input_names, output_names)\n    elif keras_layer.type == \"simplernn\":\n        return convert_simple_rnn(builder, layer, input_names, output_names)\n    else:\n        raise ValueError(\"Unknown RNN layer type: %s\" % keras_layer.type)\n\n\ndef convert_rnn(builder, keras_layer, input_names, output_names):\n    \"\"\"Convert a keras layer from coreml to coreml.\n\n    Parameters\n    ----------\n    builder: Neural", "solution": "\n    # Get input and output names\n    hidden_size = keras_layer.output_dim\n    input_size = keras_layer.input_shape[-1]\n\n    output_all = keras_layer.return_sequences\n    reverse_input = keras_layer.go_backwards\n\n    if keras_layer.consume_less not in ['cpu', 'gpu']:\n        raise ValueError('Cannot convert Keras layer with consume_less = %s' % keras_layer.consume_less)\n\n    W_h = np.zeros((hidden_size, hidden_size))\n    W_x = np.zeros((hidden_size, input_size))\n    b = np.zeros((hidden_size,))\n\n    if keras_layer.consume_less == 'cpu':\n        W_h = keras_layer.get_weights()[1].T\n        W_x = keras_layer.get_weights()[0].T\n        b = keras_layer.get_weights()[2]\n    else:\n        W_h = keras_layer.get_weights()[1].T\n        W_x = keras_layer.get_weights()[0].T\n        b = keras_layer.get_weights()[2]\n\n    # Set actication type\n    activation_str = _get_recurrent_activation_name_from_keras(keras_layer.activation)\n\n    # Add to the network\n    builder.add_simple_rnn(\n        name = layer,\n        W_h = W_h, W_x = W_x, b = b,\n        hidden_size = hidden_size,\n        input_size = input_size,\n        activation = activation_str,\n        input_names = input_names,\n        output_names = output_names,\n        output_all=output_all,\n        reverse_input=reverse_input)"}
{"prompt": "def GetFileEntryByPathSpec(self, path_spec):\n    \"\"\"Retrieves a file entry for a path specification.\n\n    Args:\n      path_spec (PathSpec): a path specification.\n\n    Returns:\n      OSFileEntry: a file entry or None if not available.\n    \"\"\"", "output": "\n    if not path_spec.HasParent():\n      return None\n\n    if path_spec.last.path_spec == path_spec:\n      return path_spec.last\n\n    path_spec = path_spec.Copy()\n    path_spec.last.path_spec = path_spec.last.path_spec.Copy()\n    path_spec.last.path_spec.last = path_spec\n\n    return OSFileEntry(\n        path_spec=path_spec,\n        path_name=path_spec.name,\n        path_name_lower=path_spec.name.lower(),\n       ", "solution": "\n    if not self.FileEntryExistsByPathSpec(path_spec):\n      return None\n    return os_file_entry.OSFileEntry(self._resolver_context, self, path_spec)"}
{"prompt": "def get_field_to_observations_map(generator, query_for_tag=''):\n  \"\"\"Return a field to `Observations` dict for the event generator.\n\n  Args:\n    generator: A generator over event protos.\n    query_for_tag: A string that if specified, only create observations for\n      events with this tag name.\n\n  Returns:\n    A dict mapping keys in `TRACKED_FIELDS` to an `Observation` list.\n  \"\"\"", "output": "\n  if query_for_tag:\n    return [\n        track.observations.add()\n        for track in generator.track_list.tracks\n        if query_for_tag in track.tags and query_for_tag in track.tags[query_for_tag]\n    ]\n  if not query_for_tag:\n    return [\n        observation.add()\n        for observation in generator.observation_list.observations\n        if query_for_tag in observation.tags and query_for_tag in generator.tags[query_for_tag]\n    ]\n  if query_for_tag not in", "solution": "\n\n  def increment(stat, event, tag=''):\n    assert stat in TRACKED_FIELDS\n    field_to_obs[stat].append(Observation(step=event.step,\n                                          wall_time=event.wall_time,\n                                          tag=tag)._asdict())\n\n  field_to_obs = dict([(t, []) for t in TRACKED_FIELDS])\n\n  for event in generator:\n    ## Process the event\n    if event.HasField('graph_def') and (not query_for_tag):\n      increment('graph', event)\n    if event.HasField('session_log') and (not query_for_tag):\n      status = event.session_log.status\n      if status == event_pb2.SessionLog.START:\n        increment('sessionlog:start', event)\n      elif status == event_pb2.SessionLog.STOP:\n        increment('sessionlog:stop', event)\n      elif status == event_pb2.SessionLog.CHECKPOINT:\n        increment('sessionlog:checkpoint', event)\n    elif event.HasField('summary'):\n      for value in event.summary.value:\n        if query_for_tag and value.tag != query_for_tag:\n          continue\n\n        for proto_name, display_name in SUMMARY_TYPE_TO_FIELD.items():\n          if value.HasField(proto_name):\n            increment(display_name, event, value.tag)\n  return field_to_obs"}
{"prompt": "def debug_process(pid):\n    \"\"\"Interrupt a running process and debug it.\"\"\"", "output": "\n    if pid in _debug_processes:\n        _debug_processes.remove(pid)\n    _debug_processes.append(pid)\n\n\ndef get_test_instance_id():\n    \"\"\"Return a unique, globally unique string identifier for this test run.\"\"\"\n    return \"%s.%s.%s\" % (socket.gethostname(), os.getpid(), threading.currentThread().ident)\n\n\nclass TestRunner(object):\n    \"\"\"A test runner class that displays results in a table.\"\"\"\n    def __init__(self, verbosity, failfast, test_source_cls, test_source_cls_kwargs,", "solution": "\n    os.kill(pid, signal.SIGUSR1)  # Signal process.\n    pipe = NamedPipe(pipename(pid), 1)\n    try:\n        while pipe.is_open():\n            txt=raw_input(pipe.get()) + '\\n'\n            pipe.put(txt)\n    except EOFError:\n        pass # Exit.\n    pipe.close()"}
{"prompt": "def _start(self):\n        \"\"\"Start the server.\"\"\"", "output": "\n        self.server.start()\n        self.server.wait()\n\n    def _stop(self):\n        \"\"\"Stop the server.\"\"\"\n        self.server.stop()\n\n    def _get_server_url(self):\n        \"\"\"Return the server URL.\"\"\"\n        return self.url\n\n    def _get_server_id(self):\n        \"\"\"Return the server ID.\"\"\"\n        return self.id\n\n    def _get_server_name(self):\n        \"\"\"Return the server name.\"\"\"\n        return self.name\n\n    def _get_server_type(self):\n        \"\"\"Return the server type.\"\"\"\n        return self.type\n\n    def", "solution": "\n        assert self._tempdir is not None\n        assert self._process is None\n        self._process = subprocess.Popen(\n            [\n                self.paths.slapd,\n                '-f', self._slapd_conf,\n                '-h', self.uri,\n                '-d', str(self.slapd_debug),\n            ],\n            stdout=sys.stdout,\n            stderr=sys.stderr,\n        )\n        self._poll_slapd(timeout=self.max_server_startup_delay)"}
{"prompt": "def Kerr_factor(final_mass, distance):\n    \"\"\"Return the factor final_mass/distance (in dimensionless units) for Kerr\n    ringdowns\n    \"\"\"", "output": "\n    return final_mass * distance\n\ndef Kerr_factor_2(final_mass, distance):\n    \"\"\"Return the factor final_mass/distance (in dimensionless units) for Kerr\n    ringdowns\n    \"\"\"\n    return final_mass * distance\n\ndef Kerr_factor_3(final_mass, distance):\n    \"\"\"Return the factor final_mass/distance (in dimensionless units) for Kerr\n    ringdowns\n    \"\"\"\n    return Kerr_factor(final_mass, 2) * distance\n\ndef Kerr_factor_4(final_mass, distance):\n   ", "solution": "\n\n    # Convert solar masses to meters\n    mass = final_mass * lal.MSUN_SI * lal.G_SI / lal.C_SI**2\n    # Convert Mpc to meters\n    dist = distance * 1e6 * lal.PC_SI\n\n    return mass / dist"}
{"prompt": "def init_run(shell, no_daemon, daemon_options, daemon_outfile):\n    \"\"\"\n    Configure your shell.\n\n    Add the following line in your shell RC file and then you are\n    ready to go::\n\n      eval $(%(prog)s)\n\n    To check if your shell is supported, simply run::\n\n      %(prog)s --no-daemon\n\n    If you want to specify shell other than $SHELL, you can give\n    --shell option::\n\n      eval $(%(prog)s --shell zsh)\n\n    By default, this command also starts daemon in background to\n    automatically index shell history records.  To not start daemon,\n    use --no-daemon option like this::\n\n      eval $(%(prog)s --no-daemon)\n\n    To see the other methods to launch the daemon process, see\n    ``rash daemon --help``.\n\n    \"\"\"", "output": " use daemons, use this::\n\n      %(prog)s daemonize\n\n  -p, --pidfile=pidfile  Specify the path to the pid file to write to.\n    \"\"\"\n    if not no_daemon:\n        # first fork\n        try:\n            pid = os.fork()\n        except OSError, e:\n            sys.stderr.write(\"fork #1 failed: %s (%s)\\n\" % (e.strerror, e.errno))\n            sys.exit(1)\n\n        # second fork\n        if pid > 0:\n            sys.stderr.write(\"fork #2 failed: pid %d already running as pid", "solution": "\n    import sys\n    from .__init__ import __version__\n    init_file = find_init(shell)\n    if os.path.exists(init_file):\n        sys.stdout.write(INIT_TEMPLATE.format(\n            file=init_file, version=__version__))\n    else:\n        raise RuntimeError(\n            \"Shell '{0}' is not supported.\".format(shell_name(shell)))\n\n    if not no_daemon:\n        from .daemon import start_daemon_in_subprocess\n        start_daemon_in_subprocess(daemon_options, daemon_outfile)"}
{"prompt": "def ndim(self) -> int:\n        \"\"\"Get number of dimensions.\"\"\"", "output": "\n        return len(self._data)\n\n    @property\n    def ndims(self) -> int:\n        \"\"\"Get number of dimensions.\"\"\"\n        return self._n_dims\n\n    @property\n    def n_points(self) -> int:\n        \"\"\"Get number of points.\"\"\"\n        return self._n_points\n\n    @property\n    def n_dims_points(self) -> int:\n        \"\"\"Get number of points.\"\"\"\n        return self._n_points\n\n    @property\n    def n_dims_dims(self) -> int:\n        \"\"\"Get number of dimensions.\"\"\"\n        return self._n_dims\n\n    @property\n    def", "solution": "\n        try:\n            assert self._ndim is not None\n        except (AssertionError, AttributeError):\n            if len(self.variables) == 0:\n                self._ndim = 0\n            else:\n                self._ndim = self.variables[0].ndim\n        finally:\n            return self._ndim"}
{"prompt": "def render_activity(activity, grouped_activity=None, *args, **kwargs):\n    \"\"\"\n    Given an activity, will attempt to render the matching template snippet\n    for that activity's content object\n    or will return a simple representation of the activity.\n\n    Also takes an optional 'grouped_activity' argument that would match up with\n    what is produced by utils.group_activity\n    \"\"\"", "output": "\n    if grouped_activity is None:\n        grouped_activity = activity\n\n    if grouped_activity.is_published:\n        return render_published(grouped_activity, *args, **kwargs)\n    if grouped_activity.is_draft:\n        return render_draft(grouped_activity, *args, **kwargs)\n    if grouped_activity.is_published:\n        return render_published(grouped_activity, *args, **kwargs)\n    if grouped_activity.is_draft:\n        return render_draft(grouped_activity, *args, **kwargs)\n    if grouped_activity.is_published:", "solution": "\n    template_name = 'activity_monitor/includes/models/{0.app_label}_{0.model}.html'.format(activity.content_type)\n    try:\n        tmpl = loader.get_template(template_name)\n    except template.TemplateDoesNotExist:\n        return None\n    # we know we have a template, so render it\n    content_object = activity.content_object\n    return tmpl.render(Context({\n        'activity': activity,\n        'obj': content_object,\n        'grouped_activity': grouped_activity\n    }))"}
{"prompt": "def clean_blobstore_cache(self):\n        \"\"\"Deletes all of the existing buildpack caches in the blobstore\"\"\"", "output": "\n        self.blobstore.delete_all_caches()\n\n    def get_cache_key(self, name):\n        \"\"\"Returns a unique key for the given buildpack name\"\"\"\n        return self.blobstore.get_cache_key(name)\n\n    def get_cache_value(self, name):\n        \"\"\"Returns a value for the given buildpack name\"\"\"\n        return self.blobstore.get_cache_value(name)\n\n    def get_cache_type(self, name):\n        \"\"\"Returns a unique type for the given buildpack name\"\"\"\n        return self.blobstore.get_cache_type", "solution": "\n        url = self.api_url + self.blobstores_builpack_cache_url\n        resp, rcode = self.request('DELETE', url)\n        if rcode != 202:\n            raise CFException(resp, rcode)\n        return resp"}
{"prompt": "def _compute_ogg_page_crc(page):\n  \"\"\" Compute CRC of an Ogg page. \"\"\"", "output": "\n  return zlib.crc32(page.data[0x0:0x10] + [0x0] * 10) & 0xffffffff\n\ndef _compute_ogg_page_id(page):\n  \"\"\" Compute the Ogg page ID of an Ogg page. \"\"\"\n  return zlib.crc32(page.data[0x10:0x10 + page.data[0x10 + page.data_len(OggPage.OggPage))]\n\ndef _compute_ogg_page_size(page):\n  \"\"\" Compute the size of the page. \"\"\"\n  return len", "solution": "\n  page_zero_crc = page[:OGG_FIRST_PAGE_HEADER_CRC_OFFSET] + \\\n      b\"\\00\" * OGG_FIRST_PAGE_HEADER_CRC.size + \\\n      page[OGG_FIRST_PAGE_HEADER_CRC_OFFSET + OGG_FIRST_PAGE_HEADER_CRC.size:]\n  return ogg_page_crc(page_zero_crc)"}
{"prompt": "def domain_sites():\n    \"\"\"\n    Get a list of domains\n    \n    Each domain is an attribute dict with name, site_id and settings\n    \"\"\"", "output": "\n    return [site for site in Site.objects.all() if site.domain_id not in settings.DOMAINS]\n\ndef get_domain_by_name(name):\n    \"\"\"\n    Get a domain by name\n    \n    name - name of the domain\n    \"\"\"\n    return Site.objects.get(pk=name)\n\ndef get_domain_by_site_id(site_id):\n    \"\"\"\n    Get a domain by site_id\n    \n    site_id - id of the site\n    \"\"\"\n    return get_domain_by_name(Site.objects.get(pk=site_id))", "solution": "\n\n    if not hasattr(env,'domains'):\n        sites = _get_django_sites()\n        site_ids = sites.keys()\n        site_ids.sort()\n        domains = []\n        \n        for id in site_ids:\n\n            for file in _sitesettings_files():\n                domain = _AttributeDict({})\n\n                if file == 'settings.py':\n                    domain.name = sites[id]\n                else: #prefix indicates subdomain\n                    subdomain = file[:-12].replace('_','.')\n                    domain.name = ''.join([subdomain,sites[id]])\n\n                domain.settings = file\n                domain.site_id = id\n                domains.append(domain)\n                \n        env.domains = domains\n        if env.domains: env.root_domain = env.domains[0].name\n        else:\n            domain.name = _root_domain(); domain.site_id = 1; domain.settings='settings.py'\n            env.domains = [domain]\n            \n    return env.domains"}
{"prompt": "def parse_instance(self, global_params, region, reservation):\n        \"\"\"\n        Parse a single EC2 instance\n\n        :param global_params:           Parameters shared for all regions\n        :param region:                  Name of the AWS region\n        :param instance:                 Cluster\n        \"\"\"", "output": "\n        self.debug(\"Parsing EC2 instance: %s\" % instance)\n        self.debug(\"region: %s, reservation: %s\" % (region, reservation))\n\n        # Parse the cluster name\n        cluster_name = None\n        if 'cluster' in global_params:\n            cluster_name = global_params['cluster']\n\n        # Parse the instance name\n        instance_name = None\n        if 'instance' in global_params:\n            instance_name = global_params['instance']\n\n        # Parse the cluster size\n        instance_size = None\n        if 'instance_type' in global_params:\n           ", "solution": "\n        for i in reservation['Instances']:\n            instance = {}\n            vpc_id = i['VpcId'] if 'VpcId' in i and i['VpcId'] else ec2_classic\n            manage_dictionary(self.vpcs, vpc_id, VPCConfig(self.vpc_resource_types))\n            instance['reservation_id'] = reservation['ReservationId']\n            instance['id'] = i['InstanceId']\n            get_name(i, instance, 'InstanceId')\n            get_keys(i, instance, ['KeyName', 'LaunchTime', 'InstanceType', 'State', 'IamInstanceProfile', 'SubnetId'])\n            # Network interfaces & security groups\n            manage_dictionary(instance, 'network_interfaces', {})\n            for eni in i['NetworkInterfaces']:\n                nic = {}\n                get_keys(eni, nic, ['Association', 'Groups', 'PrivateIpAddresses', 'SubnetId', 'Ipv6Addresses'])\n                instance['network_interfaces'][eni['NetworkInterfaceId']] = nic\n            self.vpcs[vpc_id].instances[i['InstanceId']] = instance"}
{"prompt": "def get_scheduler_info():\n    \"\"\"Gets the scheduler info.\"\"\"", "output": "\n    return {\n        'name': 'Home Assistant',\n       'version': __version__,\n       'model': 'Home Assistant',\n       'source': 'home-assistant',\n    }\n\n\ndef setup(hass, config):\n    \"\"\"Set up the Home Assistant component.\"\"\"\n    hass.data[DOMAIN] = {}\n    hass.data[DOMAIN]['config'] = config\n\n    # Register services\n    hass.services.register(DOMAIN, SERVICE_HOMEASSISTANT_STOP,\n                           lambda service: service.async_stop(),\n                           schema=get_service_schema(config))\n\n    # Register our components\n    hass", "solution": "\n\n    scheduler = current_app.apscheduler\n\n    d = OrderedDict([\n        ('current_host', scheduler.host_name),\n        ('allowed_hosts', scheduler.allowed_hosts),\n        ('running', scheduler.running)\n    ])\n\n    return jsonify(d)"}
{"prompt": "def add(self, extension):\n        \"\"\"\n        Add a extension to the editor.\n\n        :param extension: The extension instance to add.\n\n        \"\"\"", "output": "\n        self.extension = extension\n        self.editor.add_extension(self.extension)\n        self.editor.set_current_name(self.extension.name)\n        self.editor.set_current_id(self.extension.id)\n        self.editor.set_current_extension(self.extension)\n\n    def remove(self):\n        \"\"\"\n        Remove the extension.\n\n        \"\"\"\n        self.extension.delete()\n\n    def save(self):\n        \"\"\"\n        Save the current extension.\n\n        \"\"\"\n        self.editor.save_current_name()\n        self.editor.set_current_id(", "solution": "\n        logger.debug('adding extension {}'.format(extension.name))\n        self._extensions[extension.name] = extension\n        extension.on_install(self.editor)\n        return extension"}
{"prompt": "def _image_loop(self):\n        \"\"\"Retrieve an iterable of images either with, or without a progress bar.\"\"\"", "output": "\n        if self.progress_bar is None:\n            return\n        else:\n            return self.progress_bar.iter_images()\n\n    def _get_image_from_path(self, path):\n        \"\"\"Return the image data from a given path.\"\"\"\n        image_file = open(path, 'rb')\n        image = image_file.read()\n        image_file.close()\n        return image\n\n    def _get_image_from_path_no_progress(self, path):\n        \"\"\"Return the image data from a given path, but no progress bar.\"\"\"\n        image_file = open(path,", "solution": "\n        if self.progress_bar and 'tqdm' in self.progress_bar.lower():\n            return tqdm(self.imgs, desc='Saving PNGs as flat PDFs', total=len(self.imgs), unit='PDFs')\n        else:\n            return self.imgs"}
{"prompt": "def get_sleepstage(self, stage_idx=None):\n        \"\"\"Score the sleep stage, using shortcuts or combobox.\"\"\"", "output": "\n        if stage_idx is None:\n            stage_idx = self.stage_idx\n        if self.combobox_idx is None:\n            self.combobox_idx = stage_idx\n        if self.combobox_idx == self.stage_idx:\n            return\n        self.combobox_idx = stage_idx\n        if self.combobox_idx == self.stage_idx:\n            self.combobox_idx = stage_idx\n        self.combobox.set_value(stage_idx)\n        self.combobox.set_active(self.combobox_idx)\n        self.", "solution": "\n        if self.annot is None:  # remove if buttons are disabled\n            error_dialog = QErrorMessage()\n            error_dialog.setWindowTitle('Error getting sleep stage')\n            error_dialog.showMessage('No score file loaded')\n            error_dialog.exec()\n            return\n\n        window_start = self.parent.value('window_start')\n        window_length = self.parent.value('window_length')\n\n        if window_length != self.epoch_length:\n            msg = ('Zoom to ' + str(self.epoch_length) + ' (epoch length) ' +\n                   'for sleep scoring.')\n            error_dialog = QErrorMessage()\n            error_dialog.setWindowTitle('Error getting sleep stage')\n            error_dialog.showMessage(msg)\n            error_dialog.exec()\n            lg.debug(msg)\n            return\n\n        try:\n            self.annot.set_stage_for_epoch(window_start,\n                                           STAGE_NAME[stage_idx])\n\n        except KeyError:\n            msg = ('The start of the window does not correspond to any epoch ' +\n                   'in sleep scoring file.\\n\\n'\n                   'Switch to the appropriate window length in View, then use '\n                   'Navigation --> Line Up with Epoch to line up the window.')\n            error_dialog = QErrorMessage()\n            error_dialog.setWindowTitle('Error getting sleep stage')\n            error_dialog.showMessage(msg)\n            error_dialog.exec()\n            lg.debug(msg)\n\n        else:\n            lg.debug('User staged ' + str(window_start) + ' as ' +\n                     STAGE_NAME[stage_idx])\n\n            self.set_stage_index()\n            self.parent.overview.mark_stages(window_start, window_length,\n                                             STAGE_NAME[stage_idx])\n            self.display_stats()\n            self.parent.traces.page_next()"}
{"prompt": "def do_rename(argdict):\n    \"\"\"Rename a page.\"\"\"", "output": "\n    if not argdict['name']:\n        raise ArgumentError('rename requires a name')\n    if argdict['name'] == 'new':\n        if len(argdict['aliases']) > 0:\n            raise ArgumentError('cannot rename aliases')\n        return argdict['name']\n    if len(argdict['aliases']) == 1:\n        if argdict['aliases'][0] == 'new':\n            return do_rename(argdict)\n        return argdict['aliases'][0]\n    if len(argdict['aliases']) > 1:\n        raise ArgumentError('cannot rename aliases')\n    return ''\n\ndef do", "solution": "\n    site = make_site_obj(argdict)\n    slug = argdict['slug']\n    newtitle = argdict['newtitle']\n    try:\n        site.rename_page(slug, newtitle)\n        print \"Renamed page.\"\n    except ValueError: # pragma: no cover\n        print \"Cannot rename. A page with the given slug does not exist.\"\n        sys.exit()"}
{"prompt": "def sync(self):\r\n        \"\"\"\r\n        Syncs the information for this settings out to the file system.\r\n        \"\"\"", "output": "\r\n        self.write_settings_file(self.config_file)\r\n        self.write_settings_file(self.settings_file)\r\n\r\n    def set_config_file(self, config_file):\r\n        \"\"\"\r\n        Sets the settings file.\r\n        \"\"\"\r\n        self.config_file = config_file\r\n\r\n    def set_settings_file(self, settings_file):\r\n        \"\"\"\r\n        Sets the settings file.\r\n        \"\"\"\r\n        self.settings_file = settings_file\r\n\r\n    def get_config_file(self):\r\n        \"\"\"\r\n        Returns the settings file.\r\n        \"\"\"\r\n        return self.config_file\r\n\r\n    def get", "solution": "\r\n        if self._customFormat:\r\n            self._customFormat.save(self.fileName())\r\n        else:\r\n            super(XSettings, self).sync()"}
{"prompt": "def categories(self, *category_slugs):\n        \"\"\"\n        Return the entries with the given category slugs.\n        When multiple tags are provided, they operate as \"OR\" query.\n        \"\"\"", "output": "\n        if not category_slugs:\n            return []\n        if not isinstance(category_slugs, (list, tuple)):\n            category_slugs = [category_slugs]\n        return [self.get_entry(tag) for tag in category_slugs]\n\n    def get_category_slugs(self, category):\n        \"\"\"\n        Return the entries with the given category slugs.\n        When multiple entries are provided, they operate as \"OR\" query.\n        \"\"\"\n        if not isinstance(category, (list, tuple)):\n            category = [category]\n        return category", "solution": "\n        categories_field = getattr(self.model, 'categories', None)\n        if categories_field is None:\n            raise AttributeError(\"The {0} does not include CategoriesEntryMixin\".format(self.model.__name__))\n\n        if issubclass(categories_field.rel.model, TranslatableModel):\n            # Needs a different field, assume slug is translated (e.g django-categories-i18n)\n            filters = {\n                'categories__translations__slug__in': category_slugs,\n            }\n\n            # TODO: should the current language also be used as filter somehow?\n            languages = self._get_active_rel_languages()\n            if languages:\n                if len(languages) == 1:\n                    filters['categories__translations__language_code'] = languages[0]\n                else:\n                    filters['categories__translations__language_code__in'] = languages\n\n            return self.filter(**filters).distinct()\n        else:\n            return self.filter(categories__slug=category_slugs)"}
{"prompt": "def add_download(self, info, future):\n        \"\"\"\n        Hand off a download to the Downloads plugin, if it is present.\n\n        Parameters\n        ----------\n        info : `~ginga.misc.Bunch.Bunch`\n            A bunch of information about the URI as returned by\n            `ginga.util.iohelper.get_fileinfo()`\n\n        future : `~ginga.misc.Future.Future`\n            A future that represents the future computation to be performed\n            after downloading the file.  Resolving the future will trigger\n            the computation.\n        \"\"\"", "output": "\n        if self.uri is None:\n            raise ValueError(\"No URI to add a download to.\")\n        if info.get('uri_type', None) == 'file':\n            return self.add_fileinfo(info, future)\n        elif info.get('uri_type', None) == 'http':\n            return self.add_httpinfo(info, future)\n        elif info.get('uri_type', None) == 'fileinfo':\n            return self.add_fileinfo(info, future)\n        elif info.get('uri_type', None) == 'httpinfo':\n            return self", "solution": "\n        if self.gpmon.has_plugin('Downloads'):\n            obj = self.gpmon.get_plugin('Downloads')\n            self.gui_do(obj.add_download, info, future)\n        else:\n            self.show_error(\"Please activate the 'Downloads' plugin to\"\n                            \" enable download functionality\")"}
{"prompt": "def current_task(self, args):\n        \"\"\"Name of current action for progress-bar output.\n\n        The specific task string is depends on the configuration via `args`.\n\n        Returns\n        -------\n        ctask : str\n            String representation of this task.\n        \"\"\"", "output": "\n        return self.name\n\n    @property\n    def parent_id(self):\n        \"\"\"ID of the parent action for progress-bar output.\n\n        Returns\n        -------\n        parent_id : int\n            ID of the parent action.\n        \"\"\"\n        return self.name\n\n    @property\n    def parent_name(self):\n        \"\"\"Name of the parent action for progress-bar output.\n\n        Returns\n        -------\n        parent_name : str\n            Name of the parent action.\n        \"\"\"\n        return self.name\n\n    @property\n    def ctask(self):\n        \"\"\"Current action for progress-bar output.\n\n        The specific task string is depends", "solution": "\n        ctask = self.nice_name if self.nice_name is not None else self.name\n        if args is not None:\n            if args.update:\n                ctask = ctask.replace('%pre', 'Updating')\n            else:\n                ctask = ctask.replace('%pre', 'Loading')\n        return ctask"}
{"prompt": "def convert_time(time):\n    \"\"\"Convert a time string into 24-hour time.\"\"\"", "output": "\n    if time[-1] in ('s','seconds'):\n        time = time[:-1]\n    return time\n\n\ndef convert_datetime(date_str):\n    \"\"\"Convert a datetime string into a 24-hour time.\"\"\"\n    if date_str is None:\n        return None\n    return datetime.strptime(date_str, '%Y-%m-%dT%H:%M:%S')\n\n\ndef convert_date(date_str):\n    \"\"\"Convert a date string into a 24-hour time.\"\"\"\n    return convert_time(date_str)\n\n\ndef convert_timedelta(date_str):\n    \"\"\"", "solution": "\n    split_time = time.split()\n    try:\n        # Get rid of period in a.m./p.m.\n        am_pm = split_time[1].replace('.', '')\n        time_str = '{0} {1}'.format(split_time[0], am_pm)\n    except IndexError:\n        return time\n    try:\n        time_obj = datetime.strptime(time_str, '%I:%M %p')\n    except ValueError:\n        time_obj = datetime.strptime(time_str, '%I %p')\n\n    return time_obj.strftime('%H:%M %p')"}
{"prompt": "def slurpChompedLines(file, expand=False):\n    r\"\"\"Return ``file`` a list of chomped lines. See `slurpLines`.\"\"\"", "output": "\n    with open(file, 'r') as f:\n        lines = f.readlines()\n    if expand:\n        lines = [line.replace(b'\\r\\n', b'\\n') for line in lines]\n    return ''.join(lines)\n\n\ndef slurpLines(lines, expand=False):\n    r\"\"\"Return ``lines`` a list of lines.\n\n    The first line is a list of ``text``, the second line is a list of ``text``\n    and the third line is a list of ``text`` and so on.\n\n    >>> slurpLines([b'foo\\n", "solution": "\n    f=_normalizeToFile(file, \"r\", expand)\n    try: return list(chompLines(f))\n    finally: f.close()"}
{"prompt": "def _summarize_losses(self, losses_dict):\n    \"\"\"Adds `tf.summary`s to all terms in the losses dictionary.\"\"\"", "output": "\n    for loss in losses_dict.values():\n      self._add_loss(loss)\n\n  def _add_loss(self, loss):\n    \"\"\"Adds a scalar loss to the loss dictionary.\"\"\"\n    if isinstance(loss, tf.Tensor):\n      self._losses[loss.name] = tf.summary.scalar(name=loss.name, data=loss)\n    else:\n      self._losses[loss.name] = tf.summary.scalar(name=loss.name, data=loss)\n\n  def _add_accuracy(self, accuracy):\n    \"\"\"Adds a scalar accuracy to the accuracy dictionary.\"\"\"", "solution": "\n    if common_layers.should_generate_summaries():\n      with tf.name_scope(\"losses\"):\n        for loss_name, loss_val in sorted(losses_dict.items()):\n          tf.summary.scalar(loss_name, loss_val)"}
{"prompt": "def expand_role(self, role):\n        \"\"\"Expand an IAM role name into an ARN.\n\n        If the role is already in the form of an ARN, then the role is simply returned. Otherwise we retrieve the full\n        ARN and return it.\n\n        Args:\n            role (str): An AWS IAM role (either name or full ARN).\n\n        Returns:\n            str: The corresponding AWS IAM role ARN.\n        \"\"\"", "output": "\n        if isinstance(role, str):\n            return role\n        elif isinstance(role, dict):\n            return expand_role(role)\n        else:\n            raise ValueError(\"Invalid IAM role name: %s\" % role)\n\n    def get_role_policy(self, role):\n        \"\"\"Get the policy for an IAM role.\n\n        Args:\n            role (str): An AWS IAM role (either name or full ARN).\n\n        Returns:\n            dict: The policy for an IAM role.\n        \"\"\"\n        if isinstance(role, str):\n            return self.get_iam_role_policy_from_name(role)", "solution": "\n        if '/' in role:\n            return role\n        else:\n            return self.boto_session.resource('iam').Role(role).arn"}
{"prompt": "def add_multi_sign_transaction(self, m: int, pub_keys: List[bytes] or List[str], signer: Account):\n        \"\"\"\n        This interface is used to generate an Transaction object which has multi signature.\n\n        :param tx: a Transaction object which will be signed.\n        :param m: the amount of signer.\n        :param pub_keys: a list of public keys.\n        :param signer: an Account object which will sign the transaction.\n        :return: a Transaction object which has been signed.\n        \"\"\"", "output": "\n        if not isinstance(m, int):\n            raise TypeError(\"m must be an integer\")\n        if not isinstance(pub_keys, list):\n            raise TypeError(\"pub_keys must be a list of bytes\")\n        if len(m)!= m_max_len:\n            raise ValueError(\"len(m) must be m_max_len\")\n        if len(pub_keys)!= pub_keys_len(self.get_public_key()):\n            raise ValueError(\"len(pub_keys) must be pub_keys_len(self.get_public_key())\")\n        if len(pub", "solution": "\n        for index, pk in enumerate(pub_keys):\n            if isinstance(pk, str):\n                pub_keys[index] = pk.encode('ascii')\n        pub_keys = ProgramBuilder.sort_public_keys(pub_keys)\n        tx_hash = self.hash256()\n        sig_data = signer.generate_signature(tx_hash)\n        if self.sig_list is None or len(self.sig_list) == 0:\n            self.sig_list = []\n        elif len(self.sig_list) >= TX_MAX_SIG_SIZE:\n            raise SDKException(ErrorCode.param_err('the number of transaction signatures should not be over 16'))\n        else:\n            for i in range(len(self.sig_list)):\n                if self.sig_list[i].public_keys == pub_keys:\n                    if len(self.sig_list[i].sig_data) + 1 > len(pub_keys):\n                        raise SDKException(ErrorCode.param_err('too more sigData'))\n                    if self.sig_list[i].m != m:\n                        raise SDKException(ErrorCode.param_err('M error'))\n                    self.sig_list[i].sig_data.append(sig_data)\n                    return\n        sig = Sig(pub_keys, m, [sig_data])\n        self.sig_list.append(sig)"}
{"prompt": "def create(self, data):\n        \"\"\" Create object from the given data.\n\n        The given data may or may not have been validated prior to calling\n        this function. This function will try its best in creating the object.\n        If the resulting object cannot be produced, raises ``ValidationError``.\n\n        The spec can affect how individual fields will be created by\n        implementing ``clean()`` for the fields needing customization.\n\n        :param data: the data as a dictionary.\n        :return: instance of ``klass`` or dictionary.\n        :raises: ``ValidationError`` if factory is unable to create object.\n        \"\"\"", "output": "\n        if data is None:\n            return None\n        return klass(**data)\n\n    def clean(self):\n        \"\"\" Clean up ``self.cleaned_data`` and ``self.cleaned_data['password']``.\"\"\"\n        if self.cleaned_data['password']!= self.cleaned_data.get('password'):\n            raise ValidationError('Passwords do not match')\n        return self.cleaned_data\n\n    def save(self, commit=True):\n        \"\"\" Save object to the database and return ``True`` if the object\n        was saved successfully.\n\n        :param commit: if True, the transaction is committed.\n        :return", "solution": "\n\n        # todo: copy-paste code from representation.validate -> refactor\n\n        if data is None:\n            return None\n\n        prototype = {}\n        errors = {}\n\n        # create and populate the prototype\n        for field_name, field_spec in self.spec.fields.items():\n            try:\n                value = self._create_value(data, field_name, self.spec)\n            except ValidationError, e:\n                if field_name not in self.default_create_values:\n                    if hasattr(e, 'message_dict'):\n                        # prefix error keys with top level field name\n                        errors.update(dict(zip(\n                            [field_name + '.' + key for key in e.message_dict.keys()],\n                            e.message_dict.values())))\n                    else:\n                        errors[field_name] = e.messages\n            else:\n                key_name = self.property_name_map[field_name]\n                prototype[key_name] = value\n\n        # check extra fields\n        if self.prevent_extra_fields:\n            extras = set(data.keys()) - set(self.property_name_map.keys())\n            if extras:\n                errors[', '.join(extras)] = ['field(s) not allowed']\n\n        # if errors, raise ValidationError\n        if errors:\n            raise ValidationError(errors)\n\n        # return dict or object based on the prototype\n        _data = deepcopy(self.default_create_values)\n        _data.update(prototype)\n        if self.klass:\n            instance = self.klass()\n            instance.__dict__.update(prototype)\n            return instance\n        else:\n            return prototype"}
{"prompt": "def _initURL(self):\n        \"\"\" sets proper URLs for AGOL \"\"\"", "output": "\n        self.url = 'http://www.arcgis.com/sharing/rest/services/World_Imagery/MapServer/MapServer?f=json&token=%s' % self.token\n        self.url = 'http://www.arcgis.com/sharing/rest/services/World_Imagery/MapServer/MapServer?f=json&token=%s' % self.token\n        self.url = 'http://www.arcgis.com/sharing/rest/services/World_Imagery/MapServer/MapServer?f=json&token", "solution": "\n\n        token = self._getTokenArcMap()\n        if 'error' in token:\n            self._valid = False\n            self._message = token['error']\n        else:\n            self._valid = True\n            self._message = \"Token Generated\"\n        self._org_url = arcpy.GetActivePortalURL()\n        if self._org_url.lower().find('/sharing/rest') > -1:\n            self._url = self._org_url\n        else:\n            self._url = self._org_url + \"/sharing/rest\"\n\n        if self._url.startswith('http://'):\n            self._surl = self._url.replace('http://', 'https://')\n        else:\n            self._surl  =  self._url\n\n        url = '{}/portals/self'.format( self._url)\n\n        parameters = {\n            'f': 'json'\n        }\n        portal_info = self._post(url=url,\n                                 param_dict=parameters,\n                                 securityHandler=self,\n                                 proxy_port=self._proxy_port,\n                                 proxy_url=self._proxy_url)\n\n        if 'user' in portal_info:\n            if 'username' in portal_info['user']:\n                self._username = portal_info['user']['username']\n\n\n        results = self._get(url= self._surl + '/portals/info',\n                               param_dict={'f':'json'},\n                               proxy_port=self._proxy_port,\n                               proxy_url=self._proxy_url)\n        if 'authInfo' in results and 'tokenServicesUrl' in results['authInfo']:\n\n            self._token_url = results['authInfo']['tokenServicesUrl']\n        else:\n            self._token_url = self._surl  + '/generateToken'"}
{"prompt": "def repl_proc(self, inputstring, log=True, **kwargs):\n        \"\"\"Process using replprocs.\"\"\"", "output": "\n        if log:\n            self.log.debug(\"Replacing %s with %s\", inputstring, kwargs)\n        self.inputstring = inputstring\n        self.log.debug(\"Input string: %s\", inputstring)\n        self.kwargs = kwargs\n        self.process_line(inputstring)\n        self.process_line(kwargs)\n\n    def process_line(self, line):\n        \"\"\"Process a single line of the log file.\"\"\"\n        if self.kwargs.get('log_all', False):\n            self.log.debug(\"Processing line: %s\", line)\n        if self.process", "solution": "\n        return self.apply_procs(self.replprocs, kwargs, inputstring, log=log)"}
{"prompt": "def get_job_class(klass_str):\n    \"\"\"\n    Return the job class\n    \"\"\"", "output": "\n    return get_job_class_by_name(klass_str, \"Job\")\n\n\ndef get_job_class_by_name(job_class_str, name):\n    \"\"\"\n    Return the job class\n    \"\"\"\n    return get_job_class(name).get_job_class()\n\n\ndef get_job_template_name(job_template_str):\n    \"\"\"\n    Return the template name for a job template\n    \"\"\"\n    return get_job_template(job_template_str).get_job_template_name()\n\n\ndef get_job_template(job_", "solution": "\n    mod_name, klass_name = klass_str.rsplit('.', 1)\n    try:\n        mod = importlib.import_module(mod_name)\n    except ImportError as e:\n        logger.error(\"Error importing job module %s: '%s'\", mod_name, e)\n        return\n    try:\n        klass = getattr(mod, klass_name)\n    except AttributeError:\n        logger.error(\"Module '%s' does not define a '%s' class\", mod_name, klass_name)\n        return\n    return klass"}
{"prompt": "def picard_fastq_to_bam(picard, fastq_one, fastq_two, out_dir, names, order=\"queryname\"):\n    \"\"\"Convert fastq file(s) to BAM, adding sample, run group and platform information.\n    \"\"\"", "output": "\n    out_file = os.path.join(out_dir, names[0] + \".bam\")\n    if not os.path.exists(picard):\n        with open(out_file, \"w\") as out_handle:\n            out_handle.write(fastq_one + \"\\n\")\n            out_handle.write(fastq_two + \"\\n\")\n    else:\n        cmd = \"samtools view -bS {0} | samtools merge -@ {1} | bamtobam -@ {2}\".format(\n            fastq_one, fastq_two, out", "solution": "\n    out_bam = os.path.join(out_dir, \"%s-fastq.bam\" %\n                           os.path.splitext(os.path.basename(fastq_one))[0])\n    if not file_exists(out_bam):\n        with tx_tmpdir(picard._config) as tmp_dir:\n            with file_transaction(picard._config, out_bam) as tx_out_bam:\n                opts = [(\"FASTQ\", fastq_one),\n                        (\"READ_GROUP_NAME\", names[\"rg\"]),\n                        (\"SAMPLE_NAME\", names[\"sample\"]),\n                        (\"PLATFORM_UNIT\", names[\"pu\"]),\n                        (\"PLATFORM\", names[\"pl\"]),\n                        (\"TMP_DIR\", tmp_dir),\n                        (\"OUTPUT\", tx_out_bam),\n                        (\"SORT_ORDER\", order)]\n                if fastq_two:\n                    opts.append((\"FASTQ2\", fastq_two))\n                picard.run(\"FastqToSam\", opts)\n    return out_bam"}
{"prompt": "def _anime_add(self, data):\n        \"\"\"\n        Adds an anime to a user's list.\n\n        :param data: A :class:`Pymoe.Mal.Objects.Anime` object with the anime data\n        :raises: SyntaxError on invalid data type\n        :raises: ServerError on failure to add\n        :rtype: Bool\n        :return: True on success\n        \"\"\"", "output": "\n        if not self.is_valid_anime(data):\n            raise SyntaxError(data)\n\n        if data.get_type() == 'file':\n            self.anime_files.append(data)\n            return True\n\n        if data.get_type() == 'file-list':\n            self.anime_files_list.append(data)\n            return True\n\n        if data.get_type() == 'file-list-single':\n            self.anime_files_list.append(data)\n            return True\n\n        if data.get_type() == 'file-list-single-single", "solution": "\n        if isinstance(data, Anime):\n            xmlstr = data.to_xml()\n            r = requests.get(self.apiurl + \"animelist/add/{}.xml\".format(data.id),\n                             params={'data': xmlstr},\n                             auth=HTTPBasicAuth(self._username, self._password),\n                             headers=self.header)\n            if r.status_code != 201:\n                raise ServerError(r.text, r.status_code)\n            return True\n        else:\n            raise SyntaxError(\n                \"Invalid type: data should be a Pymoe.Mal.Objects.Anime object. Got a {}\".format(type(data)))"}
{"prompt": "def get_task_df(self):\n\t\t\"\"\"\n\t\tReturns\n\t\t-------\n\n\t\t\"\"\"", "output": "\n\t\treturn self.get_task_df_from_file(self.file_name, self.file_path)\n\n\tdef get_task_df_from_file(self, file_name, path):\n\t\t\"\"\"\n\t\tReturns\n\t\t-------\n\n\t\t\"\"\"\n\t\tdf = pd.read_csv(path, sep=\"\\t\", header=None, index_col=False)\n\t\tdf.columns = [col.strip() for col in path.split(\"\\t\")]\n\t\treturn df\n\n\tdef get_task_df_from_list(self, list_of", "solution": "\n\t\tterm_time_df = self._get_term_time_df()\n\t\tterms_to_include = (\n\t\t\tterm_time_df\n\t\t\t\t.groupby('term')['top']\n\t\t\t\t.sum()\n\t\t\t\t.sort_values(ascending=False)\n\t\t\t\t.iloc[:self.num_terms_to_include].index\n\t\t)\n\t\ttask_df = (\n\t\t\tterm_time_df[term_time_df.term.isin(terms_to_include)][['time', 'term']]\n\t\t\t\t.groupby('term')\n\t\t\t\t.apply(lambda x: pd.Series(self._find_sequences(x['time'])))\n\t\t\t\t.reset_index()\n\t\t\t\t.rename({0: 'sequence'}, axis=1)\n\t\t\t\t.reset_index()\n\t\t\t\t.assign(start=lambda x: x['sequence'].apply(lambda x: x[0]))\n\t\t\t\t.assign(end=lambda x: x['sequence'].apply(lambda x: x[1]))\n\t\t\t[['term', 'start', 'end']]\n\t\t)\n\t\treturn task_df"}
{"prompt": "def delete_and_upload_images(client, image_type, language, base_dir):\n    \"\"\"\n    Delete and upload images with given image_type and language.\n\n    Function will stage delete and stage upload all\n    found images in matching folders.\n    \"\"\"", "output": "\n    for folder in image_type:\n        client.delete_image(folder)\n        print('Deleted image %s' % folder)\n        client.delete_folder(folder)\n\n\ndef main():\n    \"\"\"\n    Main function\n    \"\"\"\n    parser = argparse.ArgumentParser(description='Upload images to S3')\n    parser.add_argument('-u', '--upload-bucket', type=str, required=True,\n                        help='The name of the bucket to upload images to')\n    parser.add_argument('-p', '--prefix', type=str, required=True,\n                        help='The prefix of the bucket names", "solution": "\n    print('{0} {1}'.format(image_type, language))\n    files_in_dir = os.listdir(os.path.join(base_dir, language))\n    delete_result = client.deleteall(\n        'images', imageType=image_type, language=language)\n\n    deleted = delete_result.get('deleted', list())\n    for deleted_files in deleted:\n        print('  delete image: {0}'.format(deleted_files['id']))\n\n    for image_file in files_in_dir[:8]:\n        image_file_path = os.path.join(base_dir, language, image_file)\n        image_response = client.upload(\n            'images',\n            imageType=image_type,\n            language=language,\n            media_body=image_file_path)\n        print(\"  upload image {0} new id {1}\".format(image_file, image_response['image']['id']))"}
{"prompt": "def provision_machine(self):\n        \"\"\"Perform the initial provisioning of the target machine.\n\n        :return: bool: The client.AddMachineParams\n        :raises: :class:`paramiko.ssh_exception.AuthenticationException`\n            if the upload fails\n        \"\"\"", "output": "\n        try:\n            self.machine.connection.put_file(self.path, self.key, self.get_policy())\n            return True\n        except paramiko.ssh_exception.AuthenticationException:\n            return False\n\n    def close(self):\n        \"\"\"Close the connection to the device.\n\n        :return: bool: The client.Close\n        :raises: :class:`paramiko.ssh_exception.AuthenticationException`\n            if the upload fails\n        \"\"\"\n        self.machine.connection.close()\n        return False\n\n    def get_policy(self):\n        \"\"\"Get the policy for the target machine.\n\n        :return:", "solution": "\n        params = client.AddMachineParams()\n\n        if self._init_ubuntu_user():\n            try:\n\n                ssh = self._get_ssh_client(\n                    self.host,\n                    self.user,\n                    self.private_key_path\n                )\n\n                hw = self._detect_hardware_and_os(ssh)\n                params.series = hw['series']\n                params.instance_id = \"manual:{}\".format(self.host)\n                params.nonce = \"manual:{}:{}\".format(\n                    self.host,\n                    str(uuid.uuid4()),  # a nop for Juju w/manual machines\n                )\n                params.hardware_characteristics = {\n                    'arch': hw['arch'],\n                    'mem': int(hw['mem']),\n                    'cpu-cores': int(hw['cpu-cores']),\n                }\n                params.addresses = [{\n                    'value': self.host,\n                    'type': 'ipv4',\n                    'scope': 'public',\n                }]\n\n            except paramiko.ssh_exception.AuthenticationException as e:\n                raise e\n            finally:\n                ssh.close()\n\n        return params"}
{"prompt": "def _submit(self):\n        \"\"\"submit the question to the board. When we get here we should have \n           (under self.data)\n           \n                {'record_environment': [('DISPLAY', ':0')],\n                 'user_prompt_board': 'http://127.0.0.1',\n                 'user_prompt_issue': 'I want to know why dinosaurs are so great!',\n                 'user_prompt_title': 'Why are dinosaurs so great?'}\n\n           self.token should be propogated with the personal access token\n        \"\"\"", "output": "\n        self.data['token'] = self.token\n        self.data['user_prompt_board'] = self.data['token']\n        self.data['user_prompt_title'] = self.token\n        self.data['user_prompt_issue'] = self.token\n        self.data['user_prompt_title'] = self.token\n        \"\"\"submit the question to the issue\"\"\"\n        self.data['user_prompt_issue'] = self.token\n        \"\"\"submit the issue to the board\"\"\"\n        self.data['user_prompt_title'] = self.token\n        \"\"\"submit the board to", "solution": " \n        body = self.data['user_prompt_issue']\n        title = self.data['user_prompt_title']\n        board = self.data['user_prompt_board']\n        username = self.data['user_prompt_username']\n        category = self.data['user_prompt_category']\n\n        # Step 1: Token\n        if self.token == None:\n            self.token = self.request_token(board)\n            self._get_and_update_setting('HELPME_DISCOURSE_TOKEN', self.token)\n\n        # Step 1: Environment\n\n        envars = self.data.get('record_environment')        \n        body = body + envars_to_markdown(envars)\n\n        # Step 2: Asciinema\n\n        asciinema = self.data.get('record_asciinema')\n        if asciinema not in [None, '']:\n            url = upload_asciinema(asciinema)\n\n            # If the upload is successful, add a link to it.\n\n            if url is not None:\n                body += \"\\n[View Asciinema Recording](%s)\" % url\n \n        # Add other metadata about client\n\n        body += \"\\n\\ngenerated by [HelpMe](https://vsoch.github.io/helpme/)\"\n        body += \"\\nHelpMe Discourse Id: %s\" %(self.run_id)\n\n        # Submit the issue\n\n        post = self.create_post(title, body, board, category, username)\n        return post"}
{"prompt": "def sample(self, nsims=1000):\n        \"\"\" Samples from the posterior predictive distribution\n\n        Parameters\n        ----------\n        nsims : int (default : 1000)\n            How many draws from the posterior predictive distribution\n\n        Returns\n        ----------\n        - np.ndarray of draws from the data\n        \"\"\"", "output": "\n        return self.model.sample(nsims)\n\n    def __str__(self):\n        \"\"\" Returns a string representation of the posterior predictive distribution\n\n        Returns\n        ----------\n        - str: string representation of the posterior predictive distribution\n        \"\"\"\n        return \"Posterior predictive distribution of %s\" % self.model.__class__.__name__\n\n    def __repr__(self):\n        \"\"\" Returns a string representation of the posterior predictive distribution\n\n        Returns\n        ----------\n        - repr: string representation of the posterior predictive distribution\n        \"\"\"\n        return \"Posterior predictive distribution of %s\" % self.model.__", "solution": "     \n        if self.latent_variables.estimation_method not in ['BBVI', 'M-H']:\n            raise Exception(\"No latent variables estimated!\")\n        else:\n            lv_draws = self.draw_latent_variables(nsims=nsims)\n            mus = [self._model(lv_draws[:,i])[0] for i in range(nsims)]\n            model_scale, model_shape, model_skewness = self._get_scale_and_shape_sim(lv_draws)\n            data_draws = np.array([self.family.draw_variable(self.link(mus[i]), \n                np.repeat(model_scale[i], mus[i].shape[0]), np.repeat(model_shape[i], mus[i].shape[0]), \n                np.repeat(model_skewness[i], mus[i].shape[0]), mus[i].shape[0]) for i in range(nsims)])\n            return data_draws"}
{"prompt": "def get(self, element):\n        \"\"\"Gets ith element of a collection in an object of the corresponding \\\n           class.\n        Args:\n            output(string): can accept 'jsonstat' or 'dataframe_list'\n\n        Returns:\n            Serialized JSONstat or a list of Pandas Dataframes,depending on \\\n            the 'output' parameter.\n\n        \"\"\"", "output": "\n        return self.get_jsonstat(element)\n\n    def get_jsonstat(self, element):\n        \"\"\"Gets ith element of a collection in an object of the corresponding \\\n           class.\n        Args:\n            output(string): can accept 'jsonstat' or 'dataframe_list'\n\n        Returns:\n            JSONStat or a list of Pandas Dataframes,depending on \\\n            the 'output' parameter.\n\n        \"\"\"\n        return element\n\n    def get_dataframe_list(self, element):\n        \"\"\"Gets the list of DataFrames in an object of the corresponding \\\n           class.\n\n        Args:\n            output(", "solution": "\n\n        if self['link']['item'][element]['class'] == 'dataset':\n            return Dataset.read(self['link']['item'][element]['href'])\n        elif self['link']['item'][element]['class'] == 'collection':\n            return Collection.read(self['link']['item'][element]['href'])\n        elif self['link']['item'][element]['class'] == 'dimension':\n            return Dimension.read(self['link']['item'][element]['href'])\n        else:\n            raise ValueError(\n                \"Class not allowed. Please use dataset, collection or \"\n                \"dimension'\")"}
{"prompt": "def _GetClientLib(service_class_names, language, output_path, build_system,\n                  hostname=None, application_path=None):\n  \"\"\"Fetch client libraries from a cloud service.\n\n  Args:\n    service_class_names: A list of fully qualified ProtoRPC service names.\n    language: The client library language to generate. (java)\n    output_path: The directory to output the discovery docs to.\n    build_system: The target build system for the client library language.\n    hostname: A string hostname which will be used as the default version\n      hostname. If no hostname is specificied in the @endpoints.api decorator,\n      this value is the fallback. Defaults to None.\n    application_path: A string containing the path to the AppEngine app.\n\n  Returns:\n    A list of paths to client libraries.\n  \"\"\"", "output": "ficed, the default version will be used.\n    application_path: The path to the application for the client library.\n\n  Returns:\n    A list of client libraries.\n  \"\"\"\n  if hostname is None:\n    hostname = build_system.GetDefaultHostname()\n  client_lib_dir = os.path.join(output_path, 'client')\n  if not os.path.isdir(client_lib_dir):\n    return []\n\n  client_lib_files = []\n  for service_class_name in service_class_names:\n    client_lib_files.extend(\n        _GetClientLib", "solution": "\n  client_libs = []\n  service_configs = GenApiConfig(\n      service_class_names, hostname=hostname,\n      config_string_generator=discovery_generator.DiscoveryGenerator(),\n      application_path=application_path)\n  for api_name_version, config in service_configs.iteritems():\n    client_name = api_name_version + '.zip'\n    client_libs.append(\n        _GenClientLibFromContents(config, language, output_path,\n                                  build_system, client_name))\n  return client_libs"}
{"prompt": "def idngram2lm(idngram_file, vocab_file, output_file, context_file=None, vocab_type=1, oov_fraction=0.5, four_byte_counts=False, min_unicount=0, zeroton_fraction=False, n=3, verbosity=2, arpa_output=True, ascii_input=False):\n    \"\"\"\n        Takes an idngram-file (in either binary (by default) or ASCII (if specified) format), a vocabulary file, and (optionally) a context cues file. Additional command line parameters will specify the cutoffs, the discounting strategy and parameters, etc. It outputs a language model, in either binary format (to be read by evallm), or in ARPA format.\n    \"\"\"", "output": " parameters are passed to the LM.\n        Returns a dictionary of {word: idngram}\n    \"\"\"\n    if context_file is not None:\n        if not isinstance(context_file, str):\n            raise TypeError(\"context file must be a binary or ASCII string\")\n        if not isinstance(vocab_file, str):\n            raise TypeError(\"vocab file must be a binary or ASCII string\")\n        if not isinstance(output_file, str):\n            raise TypeError(\"output file must be a binary or ASCII string\")\n        if not isinstance(idngram_file, str):\n            raise TypeError(\"idngram file must", "solution": "\n     # TODO: Args still missing\n     # [ -calc_mem | -buffer 100 | -spec_num y ... z ]\n     # [ -two_byte_bo_weights   \n     #     [ -min_bo_weight nnnnn] [ -max_bo_weight nnnnn] [ -out_of_range_bo_weights] ]\n     # [ -linear | -absolute | -good_turing | -witten_bell ]\n     # [ -disc_ranges 1 7 7 ]\n     # [ -cutoffs 0 ... 0 ]\n\n    cmd = ['idngram2lm', '-idngram', os.path.abspath(idngram_file),\n                         '-vocab', os.path.abspath(vocab_file),\n                         '-vocab_type', vocab_type,\n                         '-oov_fraction', oov_fraction,\n                         '-min_unicount',min_unicount,\n                         '-verbosity',verbosity,\n                         '-n',n]\n    if arpa_output:\n        cmd.extend(['-arpa',output_file])\n    else:\n        cmd.extend(['-binary',output_file])\n\n    if four_byte_counts:\n        cmd.append('-four_byte_counts')\n\n    if zeroton_fraction:\n        cmd.append('-zeroton_fraction')\n\n    if ascii_input:\n        cmd.append('-ascii_input')\n    else:\n        cmd.append('-bin_input')\n\n    # Ensure that every parameter is of type 'str'\n    cmd = [str(x) for x in cmd]\n\n    with tempfile.SpooledTemporaryFile() as output_f:\n        with  output_to_debuglogger() as err_f:\n            exitcode = subprocess.call(cmd, stdout=output_f, stderr=err_f)\n        output = output_f.read()\n    \n    logger = logging.getLogger(__name__)\n    logger.debug(\"Command '%s' returned with exit code '%d'.\" % (' '.join(cmd), exitcode))\n\n    if exitcode != 0:\n        raise ConversionError(\"'%s' returned with non-zero exit status '%s'\" % (cmd[0], exitcode))\n\n    if sys.version_info >= (3,) and type(output) is bytes:\n        output = output.decode('utf-8')\n\n    return output.strip()"}
{"prompt": "def get_DID_subdomain(self, did, cur=None):\n        \"\"\"\n        Get a subdomain, given its DID\n        Raise ValueError if the DID is invalid\n        Raise SubdomainNotFound if the DID does not correspond to a subdomain\n        \"\"\"", "output": "\n        if cur is None:\n            cur = self.cur\n            self.cur = None\n        if not cur.is_subdomain(did):\n            raise ValueError(\"DID %s does not belong to a subdomain %s\" % (did, cur.get_name()))\n        return cur\n\n    def get_DID_type(self, did):\n        \"\"\"\n        Get the type of a subdomain\n        Raise ValueError if the DID is invalid\n        \"\"\"\n        if not cur:\n            cur = self.cur\n        if not did in cur.get_types():\n            raise ValueError(\"DID %s is not a valid DID", "solution": "\n        did = str(did)\n\n        try:\n            did_info = parse_DID(did)\n            assert did_info['name_type'] == 'subdomain', 'Not a subdomain DID'\n        except:\n            raise ValueError(\"Invalid DID: {}\".format(did))\n        \n        original_address = did_info['address']\n        name_index = did_info['index']\n\n        # find the initial subdomain (the nth subdomain created by this address)\n        cmd = 'SELECT fully_qualified_subdomain FROM {} WHERE owner = ? AND sequence = ? ORDER BY parent_zonefile_index, zonefile_offset LIMIT 1 OFFSET ?;'.format(self.subdomain_table)\n        args = (original_address, 0, name_index)\n\n        cursor = None\n        if cur is None:\n            cursor = self.conn.cursor()\n        else:\n            cursor = cur\n\n        subdomain_name = None\n\n        rows = db_query_execute(cursor, cmd, args)\n        for r in rows:\n            subdomain_name = r['fully_qualified_subdomain']\n            break\n\n        if not subdomain_name:\n            raise SubdomainNotFound('Does not correspond to a subdomain: {}'.format(did))\n\n        # get the current form\n        subrec = self.get_subdomain_entry(subdomain_name, cur=cur)\n        subrec.did_info = did_info\n        return subrec"}
{"prompt": "def seek(self, offset, whence=os.SEEK_SET):\n    \"\"\"Seeks to an offset within the file-like object.\n\n    Args:\n      offset (int): offset to seek to.\n      whence (Optional(int)): value that indicates whether offset is an absolute\n          or relative position within the file.\n\n    Raises:\n      IOError: if the seek failed.\n      OSError: if the seek failed.\n    \"\"\"", "output": "\n    self._check_open()\n    self._stream.seek(offset, whence)\n    self._position += offset\n\n  def tell(self):\n    \"\"\"Returns the current position of the stream.\n\n    Returns:\n      int: current position of the stream.\n    \"\"\"\n    return self._position\n\n  def __enter__(self):\n    \"\"\"Enters the stream context.\"\"\"\n    self._check_open()\n    return self\n\n  def __exit__(self, unused_type, unused_value, unused_traceback):\n    \"\"\"Exits the stream context.\"\"\"\n    self._check_open()\n    self._stream.close()\n\n  def __", "solution": "\n    if not self._is_open:\n      raise IOError('Not opened.')\n\n    if whence == os.SEEK_CUR:\n      offset += self._current_offset\n    elif whence == os.SEEK_END:\n      offset += self._cpio_archive_file_entry.data_size\n    elif whence != os.SEEK_SET:\n      raise IOError('Unsupported whence.')\n\n    if offset < 0:\n      raise IOError('Invalid offset value less than zero.')\n\n    self._current_offset = offset"}
{"prompt": "def registRequest(self, CorpNum, taxinvoice, memo=None, UserID=None):\n        \"\"\" \uc989\uc2dc \uc694\uccad\n            args\n                CorpNum : \ud31d\ube4c\ud68c\uc6d0 \uc0ac\uc5c5\uc790\ubc88\ud638\n                taxinvoice : \uc138\uae08\uacc4\uc0b0\uc11c \uac1d\uccb4\n                memo : \uba54\ubaa8\n                UsreID : \ud31d\ube4c\ud68c\uc6d0 \uc544\uc774\ub514\n            return\n                \uac80\uc0c9\uacb0\uacfc \uc815\ubcf4\n            raise\n                PopbillException\n        \"\"\"", "output": "\uc0ac \uc694\uccad\n            \"\"\"\n        if memo == None:\n            memo = {}\n        if UserID == None:\n            UserID = self.currentUser\n        if TaxID == None:\n            TaxID = self.currentTaxID\n        if memo.has_key(UserID):\n            return memo[UserID]\n        else:\n            return taxinvoice.registRequest(self.currentUser, CorpNum, taxinvoice.id, TaxID)\n\n    def registTax(self, TaxNum, UserID=None):\n        \"\"\" \uc989\uc2dc \uc694\uccad\n            args\n               ", "solution": "\n\n        if memo != None and memo != '':\n            taxinvoice.memo = memo\n\n        postData = self._stringtify(taxinvoice)\n\n        return self._httppost('/Taxinvoice', postData, CorpNum, UserID, \"REQUEST\")"}
{"prompt": "def print_(*objects, **kwargs):\n    \"\"\"print_(*objects, sep=None, end=None, file=None, flush=False)\n\n    Args:\n        objects (object): zero or more objects to print\n        sep (str): Object separator to use, defaults to ``\" \"``\n        end (str): Trailing string to use, defaults to ``\"\\\\n\"``.\n            If end is ``\"\\\\n\"`` then `os.linesep` is used.\n        file (object): A file-like object, defaults to `sys.stdout`\n        flush (bool): If the file stream should be flushed\n    Raises:\n        EnvironmentError\n\n    Like print(), but:\n\n    * Supports printing filenames under Unix + Python 3 and Windows + Python 2\n    * Emulates ANSI escape sequence support under Windows\n    * Never fails due to encoding/decoding errors. Tries hard to get everything\n      on screen as is, but will fall back to \"?\" if all fails.\n\n    This does not conflict with ``colorama``, but will not use it on Windows.\n    \"\"\"", "output": " \"\"\"\n    for obj in objects:\n        print(obj, file=file, **kwargs)\n\n\ndef print_file(*objects, **kwargs):\n    \"\"\"print_file(*objects, sep=None, end=None, file=None, flush=False)\n\n    Args:\n        objects (object): zero or more objects to print\n        kwargs: Keyword arguments to print()\n    \"\"\"\n    for obj in objects:\n        print(obj, **kwargs)\n\n\ndef print_file_lines(file_lines, **kwargs):\n    \"\"\"print_file_lines(*file_lines, sep=None, end=None", "solution": "\n\n    sep = kwargs.get(\"sep\")\n    sep = sep if sep is not None else \" \"\n    end = kwargs.get(\"end\")\n    end = end if end is not None else \"\\n\"\n    file = kwargs.get(\"file\")\n    file = file if file is not None else sys.stdout\n    flush = bool(kwargs.get(\"flush\", False))\n\n    if is_win:\n        _print_windows(objects, sep, end, file, flush)\n    else:\n        _print_unix(objects, sep, end, file, flush)"}
{"prompt": "def fit_radius_from_potentials(z, SampleFreq, Damping, HistBins=100, show_fig=False):\n    \"\"\"\n    Fits the dynamical potential to the Steady \n    State Potential by varying the Radius.\n    \n    z : ndarray\n        Position data\n    SampleFreq : float\n        frequency at which the position data was \n        sampled\n    Damping : float\n        value of damping (in radians/second)\n    HistBins : int\n        number of values at which to evaluate \n        the steady state potential / perform\n        the fitting to the dynamical potential\n\n    Returns\n    -------\n    Radius : float\n        Radius of the nanoparticle\n    RadiusError : float\n        One Standard Deviation Error in the Radius from the Fit\n        (doesn't take into account possible error in damping)\n    fig : matplotlib.figure.Figure object\n        figure showing fitted dynamical potential and stationary potential\n    ax : matplotlib.axes.Axes object\n        axes for above figure\n\n    \"\"\"", "output": " potential.\n    show_fig : bool\n        whether to show the figure or not\n    \"\"\"\n    if show_fig:\n        plt.figure()\n        plt.plot(z, Damping, 'r')\n        plt.title('Steady State Potential')\n        plt.ylabel('Damping')\n        plt.xlabel('Hist')\n        plt.ylabel('HistBins')\n        plt.show()\n    return z, Damping, HistBins, z.shape[0]\n\ndef fit_position_from_potentials(z, HistBins=100, show", "solution": "\n    dt = 1/SampleFreq\n    boltzmann=Boltzmann\n    temp=300 # why halved??\n    density=1800\n    SteadyStatePotnl = list(steady_state_potential(z, HistBins=HistBins))\n    yoffset=min(SteadyStatePotnl[1])\n    SteadyStatePotnl[1] -= yoffset\n\n    SpringPotnlFunc = dynamical_potential(z, dt)\n    SpringPotnl = SpringPotnlFunc(z)\n    kBT_Gamma = temp*boltzmann*1/Damping\n    \n    DynamicPotentialFunc = make_dynamical_potential_func(kBT_Gamma, density, SpringPotnlFunc)\n    FitSoln = _curve_fit(DynamicPotentialFunc, SteadyStatePotnl[0], SteadyStatePotnl[1], p0 = 50)\n    print(FitSoln)\n    popt, pcov = FitSoln\n    perr = _np.sqrt(_np.diag(pcov))\n    Radius, RadiusError = popt[0], perr[0]\n\n    mass=((4/3)*pi*((Radius*10**-9)**3))*density\n    yfit=(kBT_Gamma/mass)\n    Y = yfit*SpringPotnl\n    \n    fig, ax = _plt.subplots()\n    ax.plot(SteadyStatePotnl[0], SteadyStatePotnl[1], 'bo', label=\"Steady State Potential\")\n    _plt.plot(z,Y, 'r-', label=\"Dynamical Potential\")\n    ax.legend(loc='best')\n    ax.set_ylabel('U ($k_{B} T $ Joules)')\n    ax.set_xlabel('Distance (mV)')\n    _plt.tight_layout()\n    if show_fig == True:\n        _plt.show()\n    return Radius*1e-9, RadiusError*1e-9, fig, ax"}
{"prompt": "def apply_trans_rot(ampal, translation, angle, axis, point, radians=False):\n    \"\"\"Applies a translation and rotation to an AMPAL object.\"\"\"", "output": "\n    if radians:\n        return ampal.apply_trans_rot(point, axis)\n    else:\n        return ampal.apply_rot(axis, point)\n\n\ndef apply_trans_scale(ampal, scale, axis, point):\n    \"\"\"Applies a scale to an AMPAL object.\"\"\"\n    if scale is None:\n        return ampal.apply_trans_scale(point)\n    else:\n        return scale.apply_trans_scale(axis)\n\n\ndef apply_trans_rot_scale(ampal, scale, angle, radians=False):\n    \"\"\"Applies a scale to", "solution": "\n    if not numpy.isclose(angle, 0.0):\n        ampal.rotate(angle=angle, axis=axis, point=point, radians=radians)\n    ampal.translate(vector=translation)\n    return"}
{"prompt": "def __initialize_node(self, attributes_flags=int(Qt.ItemIsSelectable | Qt.ItemIsEnabled)):\n        \"\"\"\n        Initializes the node.\n\n        :param attributes_flags: Attributes flags.\n        :type attributes_flags: int\n        \"\"\"", "output": "\n        self.attributes_flags = attributes_flags\n        self.set_selected(True)\n\n    def __set_selected(self, selected):\n        \"\"\"\n        Sets the node as selected or unselected.\n\n        :param selected: The node is selected or unselected.\n        :type selected: bool\n        \"\"\"\n        self.set_selected(selected)\n\n    def __select_node(self, node):\n        \"\"\"\n        Selects the node.\n\n        :param node: The node is selected or unselected.\n        :type node: QTreeWidgetItem\n        \"\"\"\n        self.select_node(node, True)\n        self.", "solution": "\n\n        attributes = dir(self.__component)\n        for attribute in attributes:\n            if attribute == \"name\":\n                continue\n\n            if not \"_Profile__{0}\".format(attribute) in attributes:\n                continue\n\n            value = getattr(self.__component, attribute)\n            value = \", \".join(value) if type(value) in (tuple, list) else value\n            roles = {Qt.DisplayRole: value,\n                     Qt.EditRole: value}\n            self[attribute] = umbra.ui.nodes.GraphModelAttribute(attribute, value, roles, attributes_flags)\n\n        self.update_tool_tip()"}
{"prompt": "def _get_next_occurrence(haystack, offset, needles):\n        \"\"\"\n        Find next occurence of one of the needles in the haystack\n\n        :return: tuple of (index, needle found)\n             or: None if no needle was found\"\"\"", "output": "\n        for needle in needles:\n            if offset >= needle.end and offset < needle.start:\n                return needle, needle.end, offset\n        return None, None, None\n\n    def _get_next_occurrence_in_range(haystack, start, end):\n        \"\"\"\n        Find next occurence of one of the needles in the haystack\n\n        :return: tuple of (index, occurence found)\n             or: None if no occurence was found\"\"\"\n        for needle in haystack.get_needles():\n            if start >= need", "solution": "\n        # make map of first char to full needle (only works if all needles\n        # have different first characters)\n        firstcharmap = dict([(n[0], n) for n in needles])\n        firstchars = firstcharmap.keys()\n        while offset < len(haystack):\n            if haystack[offset] in firstchars:\n                possible_needle = firstcharmap[haystack[offset]]\n                if haystack[offset:offset + len(possible_needle)] == possible_needle:\n                    return offset, possible_needle\n            offset += 1\n        return None"}
{"prompt": "def sequence_length(fasta):\n    \"\"\"\n    return a dict of the lengths of sequences in a fasta file\n    \"\"\"", "output": "\n    seqs = {}\n    for seq_record in SeqIO.parse(fasta, \"fasta\"):\n        seqs[seq_record.id] = len(seq_record.seq)\n    return seqs\n\n\ndef get_fasta_records(fasta_file):\n    \"\"\"\n    return a list of SeqRecord objects\n    \"\"\"\n    records = []\n    for record in SeqIO.parse(open(fasta_file, \"r\"), \"fasta\"):\n        if len(record.seq) > 0:\n            records.append(SeqRecord(record.id, record.seq))\n    return records\n\n\ndef main", "solution": "\n    sequences = SeqIO.parse(fasta, \"fasta\")\n    records = {record.id: len(record) for record in sequences}\n    return records"}
{"prompt": "def decode_response(client_message, to_object=None):\n    \"\"\" Decode response from client message\"\"\"", "output": "\n    if to_object is None:\n        to_object = {}\n    if type(to_object) is dict:\n        to_object = decode_dict(client_message)\n    return to_object\n\n\ndef decode_dict(client_message):\n    \"\"\" Decode dict from client message\"\"\"\n    return json.loads(client_message)\n\n\ndef decode_dict_response(to_object):\n    \"\"\" Decode dict response from client message\"\"\"\n    return json.loads(to_object)\n\n\ndef get_message_from_json(json_message):\n    \"\"\" Get message from json message\"\"\"\n   ", "solution": "\n    parameters = dict(base=None, increment=None, batch_size=None)\n    parameters['base'] = client_message.read_long()\n    parameters['increment'] = client_message.read_long()\n    parameters['batch_size'] = client_message.read_int()\n    return parameters"}
{"prompt": "def sentence(random=random, *args, **kwargs):\n    \"\"\"\n    Return a whole sentence\n\n    >>> mock_random.seed(0)\n    >>> sentence(random=mock_random)\n    \"Agatha Incrediblebritches can't wait to smell two chimps in Boatbencheston.\"\n\n    >>> mock_random.seed(2)\n    >>> sentence(random=mock_random, slugify=True)\n    'blistersecret-studios-is-the-best-company-in-liveronion'\n\n    \"\"\"", "output": "\n    return random.choice(args, **kwargs)\n\ndef get_random_string(length=6, chars=string.ascii_lowercase + string.digits):\n    \"\"\"\n    Return a random string\n\n    >>> get_random_string(length=6)\n    'Agatha Incrediblebritches can't wait to smell two chimps in Boatbencheston.'\n\n    \"\"\"\n    return ''.join(choice(chars) for _ in range(length))\n\ndef get_random_slug(length=6, chars=slugify_string(string.ascii_lowercase", "solution": "\n    if 'name' in kwargs and kwargs['name']:\n        nm = kwargs(name)\n    elif random.choice([True, False, False]):\n        nm = name(capitalize=True, random=random)\n    else:\n        nm = random.choice(people)\n\n    def type_one():\n        return \"{name} will {verb} {thing}.\".format(name=nm,\n                                                    verb=verb(random=random),\n                                                    thing=random.choice([a_thing(random=random),\n                                                                         things(random=random)]))\n\n    def type_two():\n        return \"{city} is in {country}.\".format(city=city(capitalize=True, random=random),\n                                                country=country(capitalize=True, random=random))\n\n    def type_three():\n        return \"{name} can't wait to {verb} {thing} in {city}.\".format(name=nm,\n                                                                      verb=verb(random=random),\n                                                                      thing=a_thing(random=random),\n                                                                      city=city(capitalize=True, random=random))\n\n    def type_four():\n        return \"{name} will head to {company} to buy {thing}.\".format(name=nm,\n                                                                     company=company(capitalize=True, random=random),\n                                                                     thing=a_thing(random=random))\n\n\n    def type_five():\n        return \"{company} is the best company in {city}.\".format(city=city(capitalize=True, random=random),\n                                                                 company=company(capitalize=True, random=random))\n\n    def type_six():\n        return \"To get to {country}, you need to go to {city}, then drive {direction}.\".format(\n            country=country(capitalize=True, random=random),\n            city=city(capitalize=True, random=random),\n            direction=direction(random=random))\n\n    def type_seven():\n        return \"{name} needs {thing}, badly.\".format(name=nm, thing=a_thing(random=random))\n\n    def type_eight():\n        return \"{verb} {noun}!\".format(verb=verb(capitalize=True, random=random), noun=noun(random=random))\n\n    return random.choice([type_one,\n                          type_two,\n                          type_three,\n                          type_four,\n                          type_five,\n                          type_six,\n                          type_seven,\n                          type_eight])()"}
{"prompt": "def segment(self, source, language=None):\n    \"\"\"Returns a chunk list from the given sentence.\n\n    Args:\n      source (str): Source string to segment.\n      language (:obj:`str`, optional): A language code.\n\n    Returns:\n      A chunk list. (:obj:`budou.chunk.ChunkList`)\n\n    Raises:\n      ValueError: If :obj:`language` is given and it is not included in\n                  :obj:`supported_languages`.\n    \"\"\"", "output": "\n    if language is not None:\n      if not self.supported_languages.has_key(language):\n        raise ValueError('Unsupported language: %s' % language)\n      return self.supported_languages[language].get_chunk_list()\n    else:\n      return []\n\n  def __iter__(self):\n    \"\"\"Iterates over the sentence.\n\n    Yields:\n      :obj:`budou.chunk.Chunk` instances.\n\n    Raises:\n      :obj:`StopIteration`: If :obj:`language` is not given and\n                  it is not included in :obj:`supported_languages`.\n    \"\"\"\n    if self.", "solution": "\n    if language and not language in self.supported_languages:\n      raise ValueError(\n          'Language {} is not supported by NLAPI segmenter'.format(language))\n\n    chunks = ChunkList()\n    results = tinysegmenter.tokenize(source)\n    seek = 0\n    for word in results:\n      word = word.strip()\n      if not word:\n        continue\n      if source[seek: seek + len(word)] != word:\n        assert source[seek] == ' '\n        assert source[seek + 1: seek + len(word) + 1] == word\n        chunks.append(Chunk.space())\n        seek += 1\n\n      dependency = None\n      if word in _PARTICLES or word in _AUX_VERBS or is_hiragana(word):\n        dependency = False\n\n      chunk = Chunk(word, dependency=dependency)\n      if chunk.is_punct():\n        chunk.dependency = chunk.is_open_punct()\n      chunks.append(chunk)\n      seek += len(word)\n    chunks.resolve_dependencies()\n    return chunks"}
{"prompt": "def check_announcements():\n    \"\"\"Check for any announcements from cs50.me, raise Error if so.\"\"\"", "output": "\n    try:\n        raw = urllib2.urlopen(announce_url).read()\n    except urllib2.HTTPError, e:\n        raise Error(\"Couldn't read announcement: %s\" % e)\n    if raw.startswith(\"Error:\"):\n        raise Error(\"Couldn't read error: %s\" % raw)\n    if raw.startswith(\"Error:\"):\n        raise Error(\"Couldn't read error: %s\" % raw)\n    raise Error(\"Couldn't read error: %s\" % raw)\n\ndef check_for_updates():\n    \"\"\"Check for any updates from cs50.me, raise Error", "solution": "\n    res = requests.get(\"https://cs50.me/status/submit50\")  # TODO change this to submit50.io!\n    if res.status_code == 200 and res.text.strip():\n        raise Error(res.text.strip())"}
{"prompt": "def highlightBlock(self, text):\n        \"\"\"Takes a block, applies format to the document.\n        according to what's in it.\n        \"\"\"", "output": "\n        # TODO: this is a hack to get the line number from the first line of the block\n        #       to be able to detect the line number of a block\n        #       and then use that to highlight the block\n        #       (and the line number of the block)\n        #       and use that to highlight the line number of a block\n        #       and then use that to highlight the line number of a block\n        #       to be able to detect the line number of a block\n        #       and then use that to highlight the line number of a block\n        #       to be able to detect the line number of a", "solution": "\n\n        # I need to know where in the document we are,\n        # because our formatting info is global to\n        # the document\n        cb = self.currentBlock()\n        p = cb.position()\n\n        # The \\n is not really needed, but sometimes\n        # you are in an empty last block, so your position is\n        # **after** the end of the document.\n        text = str(self.document().toPlainText()) + '\\n'\n\n        # Yes, re-highlight the whole document.\n        # There **must** be some optimizacion possibilities\n        # but it seems fast enough.\n        highlight(text, self.lexer, self.formatter)\n\n        # Just apply the formatting to this block.\n        # For titles, it may be necessary to backtrack\n        # and format a couple of blocks **earlier**.\n        for i in range(len(str(text))):\n            try:\n                self.setFormat(i, 1, self.formatter.data[p + i])\n            except IndexError:\n                pass\n\n        # I may need to do something about this being called\n        # too quickly.\n        self.tstamp = time.time()"}
{"prompt": "def paired_paths(main_path, fmt, formats):\n    \"\"\"Return the list of paired notebooks, given main path, and the list of formats\"\"\"", "output": "\n    paths = []\n    for fmt in formats:\n        if fmt.lower() == main_path.lower():\n            paths.append(fmt)\n    return paths\n\n\ndef get_all_notebooks(path):\n    \"\"\"Return a list of all notebooks in the given directory\"\"\"\n    return [os.path.join(path, f) for f in os.listdir(path) if f.lower().endswith(fmt)]\n\n\ndef get_notebooks_in_directory(path, formats):\n    \"\"\"Return a list of all notebooks in the given directory\"\"\"\n    return [os.path.join", "solution": "\n    if not formats:\n        return [(main_path, {'extension': os.path.splitext(main_path)[1]})]\n\n    formats = long_form_multiple_formats(formats)\n\n    # Is there a format that matches the main path?\n    base = base_path(main_path, fmt)\n    paths = [full_path(base, fmt) for fmt in formats]\n\n    if main_path not in paths:\n        raise InconsistentPath(u\"Paired paths '{}' do not include the current notebook path '{}'. \"\n                               u\"Current format is '{}', and paired formats are '{}'.\"\n                               .format(\"','\".join(paths), main_path, short_form_one_format(fmt),\n                                       short_form_multiple_formats(formats)))\n\n    if len(paths) > len(set(paths)):\n        raise InconsistentPath(u'Duplicate paired paths for this notebook. Please fix jupytext.formats.')\n\n    return list(zip(paths, formats))"}
{"prompt": "def ratio(value, decimal_places=0, failure_string='N/A'):\n    \"\"\"\n    Converts a floating point value a X:1 ratio.\n    \n    Number of decimal places set by the `precision` kwarg. Default is one.\n    \"\"\"", "output": "\n    if value is None:\n        return None\n    return (value / float(decimal_places)) * 100.0\n\ndef get_format(format):\n    \"\"\"\n    Returns the format string with the given name, using the given format string.\n    \n    If the given format string is not valid, returns `None`.\n    \"\"\"\n    if not format:\n        return None\n    return format.lower()\n\ndef get_format_lazy(format):\n    \"\"\"\n    Returns the name of the given format string, but with a `lazy`\n    attribute. Default is the name of the `lazy` attribute.\n    \"\"\"\n    if", "solution": "\n    try:\n        f = float(value)\n    except ValueError:\n        return failure_string\n    return _saferound(f, decimal_places) + ':1'"}
{"prompt": "def get_cli_returns(\n            self,\n            jid,\n            minions,\n            timeout=None,\n            tgt='*',\n            tgt_type='glob',\n            verbose=False,\n            show_jid=False,\n            **kwargs):\n        \"\"\"\n        Starts a watcher looking at the return data for a specified JID\n\n        :returns: all of the information for the JID\n        \"\"\"", "output": "\n        return self.core.start_watch(jid, minions, timeout, **kwargs)\n\n    def get_cli_returns_with_timeout(self, timeout):\n        \"\"\"\n        Starts a watcher looking at the return data for a specified JID\n\n        :param timeout: timeout in seconds\n        :type timeout: int\n        :returns: all of the information for the JID\n        \"\"\"\n        return self.core.start_watch(\n            self.core.start_return_path,\n            minions=self.minions,\n            timeout=timeout,\n            **self.core.core_args\n        )", "solution": "\n        if verbose:\n            msg = 'Executing job with jid {0}'.format(jid)\n            print(msg)\n            print('-' * len(msg) + '\\n')\n        elif show_jid:\n            print('jid: {0}'.format(jid))\n        if timeout is None:\n            timeout = self.opts['timeout']\n        fret = {}\n        # make sure the minions is a set (since we do set operations on it)\n        minions = set(minions)\n\n        found = set()\n        # start this before the cache lookup-- in case new stuff comes in\n        event_iter = self.get_event_iter_returns(jid, minions, timeout=timeout)\n\n        # get the info from the cache\n        ret = self.get_cache_returns(jid)\n        if ret != {}:\n            found.update(set(ret))\n            yield ret\n\n        # if you have all the returns, stop\n        if len(found.intersection(minions)) >= len(minions):\n            raise StopIteration()\n\n        # otherwise, get them from the event system\n        for event in event_iter:\n            if event != {}:\n                found.update(set(event))\n                yield event\n            if len(found.intersection(minions)) >= len(minions):\n                self._clean_up_subscriptions(jid)\n                raise StopIteration()"}
{"prompt": "def get_plural(amount, variants, absence=None):\n    \"\"\"\n    Get proper case with value\n\n    @param amount: amount of objects\n    @type amount: C{integer types}\n\n    @param variants: variants (forms) of object in such form:\n        (1 object, 2 objects, 5 objects).\n    @type variants: 3-element C{sequence} of C{unicode}\n        or C{unicode} (three variants with delimeter ',')\n\n    @param absence: if amount is zero will return it\n    @type absence: C{unicode}\n\n    @return: amount with proper variant\n    @rtype: C{unicode}\n    \"\"\"", "output": "\n    @rtype: C{integer types}\n    \"\"\"\n    if absence is None:\n        absence = \"zero\"\n    if absence == \"zero\":\n        return amount\n    if amount == 0:\n        return 0\n    if amount == 1:\n        return 1\n    if amount == 2:\n        return 2\n    if amount == 5:\n        return 5\n    if amount < 0:\n        raise ValueError(\"amount must be non-negative\")\n    if amount > 1:\n        raise ValueError(\"amount must be greater than 1\")\n    if amount > 2:\n        raise ValueError(\"amount must be greater than 2\")\n    if amount > 5", "solution": "\n    if amount or absence is None:\n        return u\"%d %s\" % (amount, choose_plural(amount, variants))\n    else:\n        return absence"}
{"prompt": "def flash(self, partition, timeout_ms=None, info_cb=DEFAULT_MESSAGE_CALLBACK):\n    \"\"\"Flashes the last downloaded file to the given partition.\n\n    Args:\n      partition: Partition to flash.\n      timeout_ms: Optional timeout in milliseconds to wait for it to finish.\n      info_cb: See Download. Usually no messages.\n\n    Returns:\n      Response to a download request, normally nothing.\n    \"\"\"", "output": "\n    if timeout_ms is None:\n      timeout_ms = self.default_message_callback_ms\n    if info_cb is None:\n      info_cb = self.default_message_callback_cb\n    if info_cb is None:\n      info_cb = self.default_message_callback_ms\n    if partition.size > 0:\n      self.size = partition.size\n    else:\n      self.size = 0\n    self.download_start_time = time.time()\n    self.download_end_time = time.time()\n    self.download_time = time.time()", "solution": "\n    return self._simple_command('flash', arg=partition, info_cb=info_cb,\n                                timeout_ms=timeout_ms)"}
{"prompt": "def map(requests, stream=False, size=None, exception_handler=None, gtimeout=None):\n    \"\"\"Concurrently converts a list of Requests to Responses.\n\n    :param requests: a collection of Request objects.\n    :param stream: If True, the content will not be downloaded immediately.\n    :param size: Specifies the number of requests to make at a time. If None, no throttling occurs.\n    :param exception_handler: Callback function, called when exception occured. Params: Request, Exception\n    :param gtimeout: Gevent joinall timeout in seconds. (Note: unrelated to requests timeout)\n    \"\"\"", "output": " to the Gevent API)\n    :return: a collection of Response objects.\n    \"\"\"\n    if size is None:\n        size = requests.size()\n    if exception_handler is None:\n        exception_handler = lambda req: None\n    if not stream:\n        return collection_response(requests, exception_handler)\n    else:\n        return collection_response(requests, exception_handler, gtimeout)\n\n\ndef map_async(requests, stream=False, size=None, exception_handler=None):\n    \"\"\"Concurrently converts a list of Requests to Responses.\n\n    :param requests: a collection of Request", "solution": "\n\n    requests = list(requests)\n\n    pool = Pool(size) if size else None\n    jobs = [send(r, pool, stream=stream) for r in requests]\n    gevent.joinall(jobs, timeout=gtimeout)\n\n    ret = []\n\n    for request in requests:\n        if request.response is not None:\n            ret.append(request.response)\n        elif exception_handler and hasattr(request, 'exception'):\n            ret.append(exception_handler(request, request.exception))\n        else:\n            ret.append(None)\n\n    return ret"}
{"prompt": "def select_projects(self, *args):\n        \"\"\"Copy the query and add filtering by monitored projects.\n\n        This is only useful if the target project represents a Stackdriver\n        account containing the specified monitored projects.\n\n        Examples::\n\n            query = query.select_projects('project-1')\n            query = query.select_projects('project-1', 'project-2')\n\n        :type args: tuple\n        :param args: Project IDs limiting the resources to be included\n            in the query.\n\n        :rtype: :class:`Query`\n        :returns: The new query object.\n        \"\"\"", "output": "\n        return self._client.select_projects(*args)\n\n    def get_project(self, project_id):\n        \"\"\"Get a specific target project.\n\n        :type project_id: str\n        :param project_id: The ID of the target project.\n\n        :rtype: :class:`boto.cloudsearch2.search.Resource`\n        :returns: The ID of the target project.\n        \"\"\"\n        return self._client.get_project(project_id)\n\n    def get_project_policy(self, project_id):\n        \"\"\"Get the current policy for a specific target project.\n\n        :type project", "solution": "\n        new_query = copy.deepcopy(self)\n        new_query._filter.projects = args\n        return new_query"}
{"prompt": "def encode(self,\n               data: mx.sym.Symbol,\n               data_length: mx.sym.Symbol,\n               seq_len: int) -> Tuple[mx.sym.Symbol, mx.sym.Symbol, int]:\n        \"\"\"\n        Encodes data given sequence lengths of individual examples and maximum sequence length.\n\n        :param data: Input data.\n        :param data_length: Vector with sequence lengths.\n        :param seq_len: Maximum sequence length.\n        :return: Encoded versions of input data (data, data_length, seq_len).\n        \"\"\"", "output": "\n        if self.is_training:\n            return self._encode_training(data, data_length, seq_len)\n        elif self.is_inference:\n            return self._encode_inference(data, seq_len)\n        elif self.is_inference_with_length:\n            return self._encode_inference_with_length(data, data_length, seq_len)\n        else:\n            raise NotImplementedError()\n\n    def _encode_training(self, data: mx.sym.Symbol, seq_len: int) -> Tuple[mx.sym.Symbol, int]:\n        \"\"\"\n        Encodes", "solution": "\n        if self.layout[0] == 'N':\n            data = mx.sym.swapaxes(data=data, dim1=0, dim2=1)\n        data = self._encode(data, data_length, seq_len)\n        if self.layout[0] == 'N':\n            data = mx.sym.swapaxes(data=data, dim1=0, dim2=1)\n        return data, data_length, seq_len"}
{"prompt": "def destroy_unit(self, unit):\n        \"\"\"Delete a unit from the cluster\n\n        Args:\n            unit (str, Unit): The Unit, or name of the unit to delete\n\n        Returns:\n            True: The unit was deleted\n\n        Raises:\n            fleet.v1.errors.APIError: Fleet returned a response code >= 400\n\n        \"\"\"", "output": "\n        url = \"/api/v1/clusters/{0}/units/{1}\".format(self.cluster_id, unit)\n        return self.client.delete(url)\n\n    def get_cluster_units(self, cluster):\n        \"\"\"Get a list of all units for a cluster\n\n        Args:\n            cluster (str): The name of the cluster\n\n        Returns:\n            (list of Unit): The list of all units for the cluster\n\n        Raises:\n            fleet.v1.errors.APIError: Fleet returned a response code >= 400\n\n        \"\"\"\n        return self.client.get(\"/api/v", "solution": "\n\n        # if we are given an object, grab it's name property\n        # otherwise, convert to unicode\n        if isinstance(unit, Unit):\n            unit = unit.name\n        else:\n            unit = str(unit)\n\n        self._single_request('Units.Delete', unitName=unit)\n        return True"}
{"prompt": "def make_or_augment_meta(self, role):\n        \"\"\"\n        Create or augment a meta file.\n        \"\"\"", "output": "\n        if role == 'all':\n            self.log.info(\"Generating all files for role: %s\", role)\n            self.gen_all_files(self.config_dir)\n        elif role =='meta':\n            self.log.info(\"Generating meta files for role: %s\", role)\n            self.gen_meta_files(self.config_dir)\n        else:\n            self.log.info(\"Unknown meta role: %s\", role)\n\n    def gen_all_files(self, dir):\n        \"\"\"\n        Generate all files in a directory.\n        \"\"\"\n        for filename in os", "solution": "\n        if not os.path.exists(self.paths[\"meta\"]):\n            utils.create_meta_main(self.paths[\"meta\"], self.config, role, \"\")\n            self.report[\"state\"][\"ok_role\"] += 1\n            self.report[\"roles\"][role][\"state\"] = \"ok\"\n\n        # swap values in place to use the config values\n        swaps = [\n            (\"author\", self.config[\"author_name\"]),\n            (\"company\", self.config[\"author_company\"]),\n            (\"license\", self.config[\"license_type\"]),\n        ]\n\n        (new_meta, _) = utils.swap_yaml_string(self.paths[\"meta\"], swaps)\n\n        # normalize the --- at the top of the file by removing it first\n        new_meta = new_meta.replace(\"---\", \"\")\n        new_meta = new_meta.lstrip()\n\n        # augment missing main keys\n        augments = [\n            (\"ansigenome_info\", \"{}\"),\n            (\"galaxy_info\", \"{}\"),\n            (\"dependencies\", \"[]\"),\n        ]\n\n        new_meta = self.augment_main_keys(augments, new_meta)\n\n        # re-attach the ---\n        new_meta = \"---\\n\\n\" + new_meta\n\n        travis_path = os.path.join(self.paths[\"role\"], \".travis.yml\")\n        if os.path.exists(travis_path):\n            new_meta = new_meta.replace(\"travis: False\", \"travis: True\")\n\n        utils.string_to_file(self.paths[\"meta\"], new_meta)"}
{"prompt": "def parse_args():\n    \"\"\"\n    Argument parser and validator\n    :return: args\n    \"\"\"", "output": "\n    parser = argparse.ArgumentParser(description='A simple script to generate a new project')\n    parser.add_argument('-p', '--project', help='The name of the project', required=True)\n    parser.add_argument('-f', '--file', help='The name of the file', required=True)\n    parser.add_argument('-s', '--source', help='The name of the source', required=True)\n    parser.add_argument('-t', '--type', help='The type of project', required=True)\n    parser.add_argument('-p', '--path', help='The", "solution": "\n    parser = argparse.ArgumentParser(description=\"Converts, and downloads a vm by name from vCenter to OVF in specified\"\n                                                 \" directory, then uploads the image as an AMI. AMI will be uploaded \"\n                                                 \"using specified AWS profile, to specified regions. \")\n    parser.add_argument('-i', '--vcenter_host', type=str, required=True, help='Hostname or Ip of vCenter API of VM')\n    parser.add_argument('-u', '--vcenter_user', type=str, required=True, help='Username for vCenter authentication')\n    parser.add_argument('-p', '--vcenter_pass', type=str, required=True, help='Password for authentication to vCenter API')\n    parser.add_argument('-n', '--vm_name', type=str, required=True, help='Name of the VM in vCenter')\n    parser.add_argument('-d', '--directory', type=str, default=tempfile.mkdtemp(),\n                        help='Directory to save the vmdk temp file (defaults to temp location')\n    parser.add_argument('-w', '--vcenter_port', type=str, default='443',\n                        help='Port to use for communication to vcenter api. Default is 443')\n    args = parser.parse_args()\n    validate_args(args)\n    return args"}
{"prompt": "def _readPPN(self, fname, sldir):\n        \"\"\"\n        Private method that reads in and organizes the .ppn file\n        Loads the data of the .ppn file into the variable cols.\n\n        \"\"\"", "output": "\n        # read the.ppn file\n        try:\n            f = open(fname, 'r')\n        except IOError:\n            return\n        # read the.ppn file header\n        try:\n            header = f.readline()\n        except IOError:\n            return\n        # read the.ppn file data\n        f.seek(0, 2)\n        # read the.ppn file data\n        for line in f:\n            # skip comments\n            if line.startswith('#'):\n                continue\n            # skip blank lines\n            if not line.strip():\n                continue\n            # split the line\n            lineSplit = line.split()\n            # get", "solution": "\n        if sldir.endswith(os.sep):\n                    #Making sure fname will be formatted correctly\n            fname = str(sldir)+str(fname)\n        else:\n            fname = str(sldir)+os.sep+str(fname)\n            self.sldir+=os.sep\n        f=open(fname,'r')\n        lines=f.readlines()\n        for i in range(len(lines)):\n            lines[i]=lines[i].strip()\n\n        cols = ['ISOTP', 'ABUNDANCE_MF'] #These are constant, .ppn files have no header to read from\n        for i in range(len(lines)):\n            if not lines[i].startswith('H'):\n                index = i-1\n                break\n\n        return cols, index"}
{"prompt": "def fix_e711(self, result):\n        \"\"\"Fix comparison with None.\"\"\"", "output": "\n        if result is None:\n            return\n        if result is False:\n            return\n        if result is None:\n            return\n        if result is True:\n            return\n        if result is False:\n            assert False, \"Unknown comparison type\"\n        for key, value in result.items():\n            if value is None:\n                del self.data[key]\n            else:\n                if key in self.data and self.data[key] == value:\n                    del self.data[key]\n\n    def test_e712(self):\n        \"\"\"Test that the 'None' value is replaced with 'None' when setting to None.\"\"\"", "solution": "\n        (line_index, offset, target) = get_index_offset_contents(result,\n                                                                 self.source)\n\n        right_offset = offset + 2\n        if right_offset >= len(target):\n            return []\n\n        left = target[:offset].rstrip()\n        center = target[offset:right_offset]\n        right = target[right_offset:].lstrip()\n\n        if not right.startswith('None'):\n            return []\n\n        if center.strip() == '==':\n            new_center = 'is'\n        elif center.strip() == '!=':\n            new_center = 'is not'\n        else:\n            return []\n\n        self.source[line_index] = ' '.join([left, new_center, right])"}
{"prompt": "def register_offchain_secret(\n        channel_state: NettingChannelState,\n        secret: Secret,\n        secrethash: SecretHash,\n) -> None:\n    \"\"\"This will register the secret and set the lock to the unlocked stated.\n\n    Even though the lock is unlock it is *not* claimed. The capacity will\n    increase once the next balance proof is received.\n    \"\"\"", "output": "\n    if not isinstance(secret, SecretHash):\n        raise TypeError(\"offchain_secret must be a SecretHash\")\n    if not isinstance(secrethash, SecretHash):\n        raise TypeError(\"offchain_secrethash must be a SecretHash\")\n\n    if secrethash.lock is not None:\n        raise ValueError(\"The lock must be a NettingSecrethash\")\n    if secrethash.unlock is not None:\n        raise ValueError(\"The unlock must be a NettingSecrethash\")\n\n    if not secrethash.is_locked:\n        raise ValueError(\"The lock", "solution": "\n    our_state = channel_state.our_state\n    partner_state = channel_state.partner_state\n\n    register_secret_endstate(our_state, secret, secrethash)\n    register_secret_endstate(partner_state, secret, secrethash)"}
{"prompt": "def next_id(self, subfolder):\n        \"\"\"\n        ProjectFileEvent Write to File Method\n        \"\"\"", "output": "\n        self.project_file_events.append(subfolder)\n        return subfolder\n\n    def write_file(self, subfolder, file_name):\n        \"\"\"\n        ProjectFileEvent Write to File\n        \"\"\"\n        self.project_file_events.append(subfolder)\n        self.project_file_methods.append(file_name)\n        return subfolder\n\n    def get_folder(self):\n        \"\"\"\n        ProjectFileEvent Get Folder\n        \"\"\"\n        return self.project_file_methods\n\n    def get_id(self):\n        \"\"\"\n        ProjectFileEvent Get ID\n        \"\"\"\n        return self.write_", "solution": "\n        evt_sim_folders = self.events.filter(\n                        ProjectFileEvent.subfolder\n                            .like(\"{0}_%\".format(subfolder))\n                    )\n        max_id = 0\n        num_search = re.compile(r'{0}_(\\d+)'.format(subfolder), re.IGNORECASE)\n        for prj_event in evt_sim_folders:\n            found_num = num_search.findall(prj_event.subfolder)\n            if found_num is not None:\n                max_id = max(max_id, int(found_num[0]))\n        return max_id + 1"}
{"prompt": "def do_directives(self, line):\n        \"\"\"List all directives supported by the bot\"\"\"", "output": "\n        if line.startswith('#'):\n            return\n        self.do_list(line)\n\n    def do_list(self, line):\n        \"\"\"List all commands supported by the bot\"\"\"\n        if line.startswith('PING'):\n            self.do_ping(line)\n        elif line.startswith('PONG'):\n            self.do_pong(line)\n        elif line.startswith('JOIN'):\n            self.do_join(line)\n        elif line.startswith('PART'):\n            self.do_part(line)\n        elif line.startswith('PRIVMSG'):\n            self.do_privmsg(line)\n        elif", "solution": "\n        for name, cmd in self.adapter.directives.items():\n            with colorize('blue'):\n                print('bot %s:' % name)\n                if cmd.__doc__:\n                    for line in cmd.__doc__.split('\\n'):\n                        print('  %s' % line)\n                else:\n                    print()"}
{"prompt": "def insert(self, schema, fields, **kwargs):\n        \"\"\"\n        Persist d into the db\n\n        schema -- Schema()\n        fields -- dict -- the values to persist\n\n        return -- int -- the primary key of the row just inserted\n        \"\"\"", "output": "\n        if not self.db.has_key(schema):\n            raise KeyError(\"No such schema: %s\" % schema)\n\n        if not self.db[schema].has_key(fields):\n            self.db[schema][fields] = {}\n\n        self.db[schema][fields][d_key] = fields_to_int(fields)\n        self.db[schema][fields][d_key][\"index\"] = fields_to_int(fields)\n        self.db[schema][fields][d_key][\"type\"] = \"int\"\n\n        return self.db[schema][fields][", "solution": "\n        r = 0\n\n        with self.connection(**kwargs) as connection:\n            kwargs['connection'] = connection\n            try:\n                with self.transaction(**kwargs):\n                    r = self._insert(schema, fields, **kwargs)\n\n            except Exception as e:\n                exc_info = sys.exc_info()\n                if self.handle_error(schema, e, **kwargs):\n                    r = self._insert(schema, fields, **kwargs)\n                else:\n                    self.raise_error(e, exc_info)\n\n        return r"}
{"prompt": "def i3extract(infile, outfile=None):\n    \"\"\"Main event loop\"\"\"", "output": "\n    if outfile is None:\n        outfile = sys.stdout\n    else:\n        outfile = open(outfile, \"w\")\n    if not outfile:\n        outfile.write(sys.stdout)\n    for i in range(1, len(infile)):\n        line = infile[i]\n        if line.startswith(\"BEGIN:VCALENDAR\"):\n            if line.startswith(\"END:VCALENDAR\"):\n                break\n            outfile.write(line)\n        elif line.startswith(\"BEGIN:VECTOR\"):\n            if line.startswith(\"BEGIN:VCALENDAR\"):\n                outfile.write(\"BEGIN:VECTOR", "solution": "\n    if outfile is None:\n        outfile = infile + '.h5'\n    tray = I3Tray()\n    tray.AddModule('I3Reader', 'i3_reader', filename=infile)\n    # tray.AddModule(KeepReconstructed, \"event_selector\")\n    tray.AddModule(ReadEventMeta, 'read_meta')    # grab the event ID\n    tray.AddModule(\n        Distance,\n        \"compare_space\",\n        particle_1=\"best_FirstDusjOrcaVertexFit_FitResult\",\n        particle_2=\"best_SecondDusjOrcaVertexFit_FitResult\"\n    )\n    tray.AddModule(\n        TimeDistance,\n        \"compare_time\",\n        particle_1=\"best_FirstDusjOrcaVertexFit_FitResult\",\n        particle_2=\"best_SecondDusjOrcaVertexFit_FitResult\",\n    )\n    tray.AddModule(\n        ReadRecoParticle,\n        'read_particle_first',\n        ParticleName='best_FirstDusjOrcaVertexFit_FitResult'\n    )\n    tray.AddModule(\n        ReadRecoParticle,\n        'read_particle_second',\n        ParticleName='best_SecondDusjOrcaVertexFit_FitResult'\n    )\n    tray.AddModule(\n        ReadRecoParticle,\n        'read_particle_proba',\n        ParticleName='best_DusjOrcaUsingProbabilitiesFinalFit_FitResult'\n    )    # noqa\n    tray.AddModule(\n        ReadLLHValues,\n        'read_llh_chere',\n        LLHParamContainer=\n        'best_DusjOrcaUsingProbabilitiesFinalFit_FitResult_FinalLLHValues'\n    )    # noqa\n    tray.AddModule(WriteScalars, 'write_scalars', filename=outfile)\n    tray.AddModule('TrashCan', 'dustbin')\n    tray.Execute()\n    tray.Finish()"}
{"prompt": "def Fold(seglist1, seglist2):\n\t\"\"\"\n\tAn iterator that generates the results of taking the intersection\n\tof seglist1 with each segment in seglist2 in turn.  In each result,\n\tthe segment start and stop values are adjusted to be with respect\n\tto the start of the corresponding segment in seglist2.  See also\n\tthe segmentlist_range() function.\n\n\tThis has use in applications that wish to convert ranges of values\n\tto ranges relative to epoch boundaries.  Below, a list of time\n\tintervals in hours is converted to a sequence of daily interval\n\tlists with times relative to midnight.\n\n\tExample:\n\n\t>>> from pycbc_glue.segments import *\n\t>>> x = segmentlist([segment(0, 13), segment(14, 20), segment(22, 36)])\n\t>>> for y in Fold(x, segmentlist_range(0, 48, 24)): print y\n\t...\n\t[segment(0, 13), segment(14, 20), segment(22, 24)]\n\t[segment(0, 12)]\n\t\"\"\"", "output": " seconds.\n\t\"\"\"\n\tfor i in range(0, len(seglist1)):\n\t\tif seglist1[i] <= seglist2[i]:\n\t\t\tyield (i, seglist1[i], seglist2[i], seglist2[i] - seglist1[i])\n\t#\n\t#\n\t# The following code is a bit of a hack.  It is not\n\t# very efficient to use the list comprehension to generate\n\t# the results of the function:\n\t#\n\t#   for i in range(0, len(", "solution": "\n\tfor seg in seglist2:\n\t\tyield (seglist1 & segments.segmentlist([seg])).shift(-seg[0])"}
{"prompt": "def initialize(self, io_loop):\n        \"\"\" Start a Bokeh Server Tornado Application on a given Tornado IOLoop.\n\n        \"\"\"", "output": "\n        self.io_loop = io_loop\n\n    def open(self, url, **kwargs):\n        \"\"\" Open a Bokeh server.\n\n        \"\"\"\n        self.url = url\n        self.io_loop.add_callback(self._open_callback, **kwargs)\n\n    def _open_callback(self, callback, **kwargs):\n        \"\"\" Callback for when a Bokeh server sends a request.\n\n        \"\"\"\n        if callback is not None:\n            self.io_loop.add_callback(callback, **kwargs)\n\n    def _open_error(self, error, url):\n        \"\"\" Callback for when a", "solution": "\n        self._loop = io_loop\n\n        for app_context in self._applications.values():\n            app_context._loop = self._loop\n\n        self._clients = set()\n\n        self._stats_job = PeriodicCallback(self._log_stats,\n                                           self._stats_log_frequency_milliseconds)\n\n        if self._mem_log_frequency_milliseconds > 0:\n            self._mem_job = PeriodicCallback(self._log_mem,\n                                             self._mem_log_frequency_milliseconds)\n        else:\n            self._mem_job = None\n\n\n        self._cleanup_job = PeriodicCallback(self._cleanup_sessions,\n                                             self._check_unused_sessions_milliseconds)\n\n        if self._keep_alive_milliseconds > 0:\n            self._ping_job = PeriodicCallback(self._keep_alive, self._keep_alive_milliseconds)\n        else:\n            self._ping_job = None"}
{"prompt": "def serialize(self, resources):\n        \"\"\"Serialize resource(s) according to json-api spec.\"\"\"", "output": "\n        return json.dumps(resources, cls=JSONEncoder)\n\n    def deserialize(self, data):\n        \"\"\"Deserialize resource(s) according to json-api spec.\"\"\"\n        return json.loads(data)\n\n    def get_resource(self, resource):\n        \"\"\"Get a resource from the resource specification.\"\"\"\n        return self._get_resource(resource, 'data')\n\n    def get_resource_type(self, resource_type):\n        \"\"\"Get a resource type from the resource specification.\"\"\"\n        return self._get_resource_type(resource_type, 'data')\n\n    def _get_resource(self, resource,", "solution": "\n        serialized = {\n            'meta': {\n                'sqlalchemy_jsonapi_version': '4.0.9'\n            },\n            'jsonapi': {\n                'version': '1.0'\n            }\n        }\n        # Determine multiple resources by checking for SQLAlchemy query count.\n        if hasattr(resources, 'count'):\n            serialized['data'] = []\n            for resource in resources:\n                serialized['data'].append(\n                    self._render_resource(resource))\n        else:\n            serialized['data'] = self._render_resource(resources)\n\n        return serialized"}
{"prompt": "def changelog(**kwargs):\n    \"\"\"\n    Generates the changelog since the last release.\n    :raises ImproperConfigurationError: if there is no current version\n    \"\"\"", "output": "\n    if 'changelog' not in kwargs:\n        raise ImproperConfigurationError('changelog is missing \"changelog\" section')\n    if'version' not in kwargs:\n        raise ImproperConfigurationError('version is missing \"version\" section')\n    if 'changelog' not in kwargs:\n        raise ImproperConfigurationError('changelog is missing \"changelog\" section')\n    if 'log_file' not in kwargs:\n        raise ImproperConfigurationError('log_file is missing \"log_file\" section')\n    if 'log_file_size' not in kwargs:\n        raise ImproperConfigurationError('", "solution": "\n    current_version = get_current_version()\n    debug('changelog got current_version', current_version)\n\n    if current_version is None:\n        raise ImproperConfigurationError(\n            \"Unable to get the current version. \"\n            \"Make sure semantic_release.version_variable \"\n            \"is setup correctly\"\n        )\n    previous_version = get_previous_version(current_version)\n    debug('changelog got previous_version', previous_version)\n\n    log = generate_changelog(previous_version, current_version)\n    click.echo(markdown_changelog(current_version, log, header=False))\n\n    debug('noop={}, post={}'.format(kwargs.get('noop'), kwargs.get('post')))\n    if not kwargs.get('noop') and kwargs.get('post'):\n        if check_token():\n            owner, name = get_repository_owner_and_name()\n            click.echo('Updating changelog')\n            post_changelog(\n                owner,\n                name,\n                current_version,\n                markdown_changelog(current_version, log, header=False)\n            )\n        else:\n            click.echo(\n                click.style('Missing token: cannot post changelog', 'red'), err=True)"}
{"prompt": "def mason_morrow(target, throat_perimeter='throat.perimeter',\n                 throat_area='throat.area'):\n    r\"\"\"\n    Mason and Morrow relate the capillary pressure to the shaped factor in a\n    similar way to Mortensen but for triangles.\n\n    References\n    ----------\n    Mason, G. and Morrow, N.R.. Capillary behavior of a perfectly wetting\n    liquid in irregular triangular tubes. Journal of Colloid and Interface\n    Science, 141(1), pp.262-274 (1991).\n    \"\"\"", "output": " pp. 1-13.\n\n    Examples\n    --------\n    >>> from scipy.spatial import Delaunay\n    >>> w = Delaunay(90, [0,0,1], [-1,1,1])\n    >>> w.as_matrix()\n    array([[  0.,   98.,   98.,   0.,   0.],\n           [  0.,   0.,   0.,   0.,   0.],\n           [  0.,   0.,   0.,   0.,   0.],\n           [  0.,   0.,   0.,   0.,   0.],\n           [  0.,", "solution": "\n    # Only apply to throats with an area\n    ts = target.throats()[target[throat_area] <= 0]\n    P = target[throat_perimeter]\n    A = target[throat_area]\n    value = A/(P**2)\n    value[ts] = 1/(4*_sp.pi)\n    return value"}
{"prompt": "def Add(self, key, help=\"\", default=None, validator=None, converter=None, **kw):\n        \"\"\"\n        Add an option.\n\n\n        @param key: the name of the variable, or a list or tuple of arguments\n        @param help: optional help text for the options\n        @param default: optional default value\n        @param validator: optional function that is called to validate the option's value\n        @type validator: Called with (key, value, environment)\n        @param converter: optional function that is called to convert the option's value before putting it in the environment.\n        \"\"\"", "output": "\n        if isinstance(key, types.ListType):\n            for i in key:\n                if not i in self.options:\n                    self.options[i] = environment.ErrorVariable(\"Option '%s' not found\" % (i))\n                if not help:\n                    help = self.options[i]\n                if not validator:\n                    validator = self.options[i][0]\n                self.options[i] = environment.EnvironmentVariable(i, converter=converter)\n        elif isinstance(key, types.TupleType):\n            self.options[key[0]] = key[1]\n        else:\n            raise", "solution": "\n\n        if SCons.Util.is_List(key) or isinstance(key, tuple):\n            self._do_add(*key)\n            return\n\n        if not SCons.Util.is_String(key) or \\\n            not SCons.Environment.is_valid_construction_var(key):\n                raise SCons.Errors.UserError(\"Illegal Variables.Add() key `%s'\" % str(key))\n\n        self._do_add(key, help, default, validator, converter)"}
{"prompt": "def query_gfy(self, gfyname):\n        \"\"\"\n        Query a gfy name for URLs and more information.\n        \"\"\"", "output": "\n        return self.gfy_url(gfyname)\n\n    def gfy_url(self, gfyname):\n        \"\"\"\n        Return the URL for a gfy.\n        \"\"\"\n        return gfyname.replace('http://', 'https://')\n\n    def gfy_url_for_file(self, gfyname, filename):\n        \"\"\"\n        Return the URL for a gfy.\n        \"\"\"\n        return self.gfy_url(filename)\n\n    def gfy_url_for_stream(self, gfyname, filename):\n        \"\"\"\n        Return the URL for a gfy.\n       ", "solution": "\n        self.check_token()\n        \n        r = requests.get(QUERY_ENDPOINT + gfyname, headers=self.headers)\n        \n        response = r.json()\n        \n        if r.status_code != 200 and not ERROR_KEY in response:\n            raise GfycatClientError('Bad response from Gfycat',\n                                    r.status_code)\n        elif ERROR_KEY in response:\n            raise GfycatClientError(response[ERROR_KEY], r.status_code)\n        \n        return response"}
{"prompt": "def is_sub_dict(sub_dict, dictionary):\n    \"\"\"Legacy filter for determining if a given dict is present.\"\"\"", "output": "\n    return (\n        isinstance(sub_dict, dict)\n        and all(is_sub_dict(sub_dict, dictionary) for sub_dict in dictionary.values())\n    )\n\n\ndef _filter_state(state, dictionary):\n    \"\"\"Filter out the state.\"\"\"\n    if state.state == STATE_ON:\n        return _filter_state(dictionary, state.attributes)\n    if state.state == STATE_OFF:\n        return _filter_state(dictionary, state.attributes)\n\n\ndef _filter_attributes(state, attributes):\n    \"\"\"Filter out the attributes.\"\"\"\n    if state.attributes", "solution": "\n    for key in sub_dict.keys():\n        if key not in dictionary:\n            return False\n        if (type(sub_dict[key]) is not dict) and (sub_dict[key] != dictionary[key]):\n            return False\n        if (type(sub_dict[key]) is dict) and (not is_sub_dict(sub_dict[key], dictionary[key])):\n            return False\n    return True"}
{"prompt": "def _ext_pillar(minion_id,\n                root_dir,\n                follow_dir_links,\n                debug,\n                keep_newline,\n                render_default,\n                renderer_blacklist,\n                renderer_whitelist,\n                template):\n    \"\"\"\n    Compile pillar data for a single root_dir for the specified minion ID\n    \"\"\"", "output": "\n    # Get the path to the minion\n    path = os.path.join(root_dir,'minion.txt')\n\n    # Compile the data\n    command = [\n        'java',\n        '-jar',\n        path,\n        '-m',\n        '-cp',\n        str(minion_id),\n        str(path),\n    ]\n\n    # Add the command line arguments\n    if debug:\n        command.append('-v')\n    if minion_id_file:\n        command.append(str(minion_id_file))\n    if minion_id_file_path:\n        command.append(", "solution": "\n    log.debug('file_tree: reading %s', root_dir)\n\n    if not os.path.isdir(root_dir):\n        log.error(\n            'file_tree: root_dir %s does not exist or is not a directory',\n            root_dir\n        )\n        return {}\n\n    if not isinstance(keep_newline, (bool, list)):\n        log.error(\n            'file_tree: keep_newline must be either True/False or a list '\n            'of file globs. Skipping this ext_pillar for root_dir %s',\n            root_dir\n        )\n        return {}\n\n    ngroup_pillar = {}\n    nodegroups_dir = os.path.join(root_dir, 'nodegroups')\n    if os.path.exists(nodegroups_dir) and __opts__.get('nodegroups'):\n        master_ngroups = __opts__['nodegroups']\n        ext_pillar_dirs = os.listdir(nodegroups_dir)\n        if ext_pillar_dirs:\n            for nodegroup in ext_pillar_dirs:\n                if (os.path.isdir(nodegroups_dir) and\n                        nodegroup in master_ngroups):\n                    ckminions = salt.utils.minions.CkMinions(__opts__)\n                    _res = ckminions.check_minions(\n                        master_ngroups[nodegroup],\n                        'compound')\n                    match = _res['minions']\n                    if minion_id in match:\n                        ngroup_dir = os.path.join(\n                            nodegroups_dir, six.text_type(nodegroup))\n                        ngroup_pillar = salt.utils.dictupdate.merge(ngroup_pillar,\n                            _construct_pillar(ngroup_dir,\n                                              follow_dir_links,\n                                              keep_newline,\n                                              render_default,\n                                              renderer_blacklist,\n                                              renderer_whitelist,\n                                              template),\n                            strategy='recurse'\n                        )\n        else:\n            if debug is True:\n                log.debug(\n                    'file_tree: no nodegroups found in file tree directory %s, skipping...',\n                    ext_pillar_dirs\n                )\n    else:\n        if debug is True:\n            log.debug('file_tree: no nodegroups found in master configuration')\n\n    host_dir = os.path.join(root_dir, 'hosts', minion_id)\n    if not os.path.exists(host_dir):\n        if debug is True:\n            log.debug(\n                'file_tree: no pillar data for minion %s found in file tree directory %s',\n                minion_id,\n                host_dir\n            )\n        return ngroup_pillar\n\n    if not os.path.isdir(host_dir):\n        log.error('file_tree: %s exists, but is not a directory', host_dir)\n        return ngroup_pillar\n\n    host_pillar = _construct_pillar(host_dir,\n                                    follow_dir_links,\n                                    keep_newline,\n                                    render_default,\n                                    renderer_blacklist,\n                                    renderer_whitelist,\n                                    template)\n    return salt.utils.dictupdate.merge(ngroup_pillar,\n                                       host_pillar,\n                                       strategy='recurse')"}
{"prompt": "def pkg_tracking(self):\n        \"\"\"Tracking package dependencies\n        \"\"\"", "output": "\n        if self.package_dependencies is None:\n            self.package_dependencies = []\n        self.package_dependencies.append(self.package_name)\n        self.package_dependencies.append(self.package_version)\n        self.package_dependencies.append(self.package_dependencies_id)\n        self.package_dependencies.append(self.package_dependencies_name)\n        self.package_dependencies.append(self.package_dependencies_version)\n        self.package_name = None\n        self.package_version = None\n\n    def __str__(self):\n        \"\"\"String representation of the", "solution": "\n        flag = []\n        options = [\n            \"-t\",\n            \"--tracking\"\n        ]\n        additional_options = [\n            \"--check-deps\",\n            \"--graph=\",\n            \"--case-ins\"\n        ]\n        for arg in self.args[2:]:\n            if arg.startswith(additional_options[1]):\n                flag.append(arg)\n                self.args.remove(arg)\n            if arg in additional_options:\n                flag.append(arg)\n        # clean additional options from args\n        for f in flag:\n            if f in self.args:\n                self.args.remove(f)\n        # print usage message if wrong additional option\n        for arg in self.args:\n            if arg.startswith(\"--\"):\n                if arg not in additional_options:\n                    usage(\"\")\n                    raise SystemExit()\n        if (len(self.args) >= 3 and len(self.args) <= 3 and\n                self.args[0] in options and\n                self.args[1] in self.meta.repositories):\n            TrackingDeps(self.args[2], self.args[1], flag).run()\n        elif (len(self.args) >= 2 and\n                self.args[1] not in self.meta.repositories):\n            usage(self.args[1])\n        else:\n            usage(\"\")"}
{"prompt": "def create_deeper(self,\n                      new_t1,\n                      new_t2,\n                      child_relationship_class,\n                      child_relationship_param=None,\n                      report_type=None):\n        \"\"\"\n        Start a new comparison level and correctly link it to this one.\n        :rtype: DiffLevel\n        :return: New level\n        \"\"\"", "output": "\n        if not child_relationship_param:\n            child_relationship_param = {}\n        if not child_relationship_class:\n            child_relationship_class = {}\n        if not child_relationship_param.get('parent_relationship'):\n            child_relationship_param['parent_relationship'] = child_relationship_param.get('parent_relationship_param', {})\n        if not report_type:\n            report_type = 'default'\n        if report_type not in self.report_types:\n            raise ValueError('Unknown report type: %s' % report_type)\n        if report_type == 'default", "solution": "\n        level = self.all_down\n        result = DiffLevel(\n            new_t1, new_t2, down=None, up=level, report_type=report_type)\n        level.down = result\n        level.auto_generate_child_rel(\n            klass=child_relationship_class, param=child_relationship_param)\n        return result"}
{"prompt": "def global_id_field(type_name, id_fetcher=None):\n    \"\"\"\n    Creates the configuration for an id field on a node, using `to_global_id` to\n    construct the ID from the provided typename. The type-specific ID is fetcher\n    by calling id_fetcher on the object, or if not provided, by accessing the `id`\n    property on the object.\n    \"\"\"", "output": "\n    if id_fetcher is None:\n        id_fetcher = id_gen.IdGen()\n    if not id_fetcher.is_node():\n        id_fetcher.set_id(global_id_field_name(type_name, id_fetcher))\n    return id_fetcher.to_global_id(type_name)\n\ndef global_id_field_name(type_name, id_fetcher=None):\n    \"\"\"\n    Creates the configuration for an id field on a node, using `to_global_id` to\n    construct the ID from the provided typename. The type-", "solution": "\n    return GraphQLField(\n        GraphQLNonNull(GraphQLID),\n        description='The ID of an object',\n        resolver=lambda obj, args, context, info: to_global_id(\n            type_name or info.parent_type.name,\n            id_fetcher(obj, context, info) if id_fetcher else obj.id\n        )\n    )"}
{"prompt": "def dir_on_bezier_curve(P=[(0.0, 0.0)], t=0.5):\n    \"\"\"Return direction at t on bezier curve defined by control points P.\nList of vectors per pair of dimensions are returned in radians.\nE.g. Where X is \"right\", Y is \"up\", Z is \"in\" on a computer screen, and\n  returned value is [pi/4, -pi/4], then the vector will be coming out the\n  screen over the viewer's right shoulder.\n    \"\"\"", "output": "\n    return P[0] * math.pi / 180.0 * math.asin(t), P[1] * math.pi / 180.0 * math.acos(t), P[2] * math.pi / 180.0\n\ndef dir_on_bezier_curve_old(P=[(0.0, 0.0)], t=0.5):\n    \"\"\"Return direction at t on bezier curve defined by control points P.\nList of radians per pair of control points is returned in radians.\nE.g. Where X is \"left\", Y is", "solution": "\n    assert isinstance(P, list)\n    assert len(P) > 0\n    if not len(P) > 1:\n        return None # Points have no gradient.\n    for p in P:\n        assert isinstance(p, tuple)\n        for i in p:\n            assert len(p) > 1\n            assert isinstance(i, float)\n    assert isinstance(t, float)\n    assert 0 <= t <= 1\n    \n    O = len(P) - 1 # Order of curve\n    \n    # Recurse down the orders calculating the next set of control points until\n    #   there are only two left, which is the points on the gradient we want.\n    Q = P\n    while O > 1:\n        Q = [pt_between_pts(Q[l], Q[l+1], t) for l in range(O)]\n        O -= 1\n    \n    assert len(Q) == 2\n    # Now that we have the two points in N dimensions, we can reduce to the\n    #   gradients on N-1 planes.\n    q0 = Q[0]\n    q1 = Q[1]\n    \n    return dir_between_pts(q0, q1)"}
{"prompt": "def datetime_from_ldap(value):\n    \"\"\"Convert a LDAP-style datetime to a Python aware object.\n\n    See https://tools.ietf.org/html/rfc4517#section-3.3.13 for details.\n\n    Args:\n        value (str): the datetime to parse\n    \"\"\"", "output": "\n    return datetime.datetime.strptime(value, '%Y-%m-%d %H:%M:%S')\n\n\ndef datetime_to_dn(value):\n    \"\"\"Convert a Python aware datetime to a DN.\n\n    See https://tools.ietf.org/html/rfc4517#section-3.3.13 for details.\n\n    Args:\n        value (str): the datetime to parse\n    \"\"\"\n    return value.strftime('%Y-%m-%d %H:%M:%S')\n\n\ndef dn_to_datetime(dn):\n    \"\"\"Convert a DN to a Python aware object.\n\n    Args:", "solution": "\n    if not value:\n        return None\n    match = LDAP_DATETIME_RE.match(value)\n    if not match:\n        return None\n    groups = match.groupdict()\n    if groups['microsecond']:\n        groups['microsecond'] = groups['microsecond'].ljust(6, '0')[:6]\n    tzinfo = groups.pop('tzinfo')\n    if tzinfo == 'Z':\n        tzinfo = timezone.utc\n    else:\n        offset_mins = int(tzinfo[-2:]) if len(tzinfo) == 5 else 0\n        offset = 60 * int(tzinfo[1:3]) + offset_mins\n        if tzinfo[0] == '-':\n            offset = - offset\n        tzinfo = timezone.get_fixed_timezone(offset)\n    kwargs = {k: int(v) for k, v in groups.items() if v is not None}\n    kwargs['tzinfo'] = tzinfo\n    return datetime.datetime(**kwargs)"}
{"prompt": "def Setup():\n    \"\"\"\n    Called when the code is installed. Sets up directories and downloads\n    the K2 catalog.\n\n    \"\"\"", "output": "\n    global _k2_dir\n    global _k2_dir_download\n    global _k2_dir_download_dir\n    global _k2_dir_download_dir_name\n    global _k2_dir_download_dir = os.path.join(os.path.dirname(__file__), 'data')\n    if not os.path.exists(_k2_dir_dir):\n        os.makedirs(_k2_dir_dir)\n    if not os.path.exists(_k2_dir_download):\n        os.makedirs(_k2_dir_download_dir)\n   ", "solution": "\n\n    if not os.path.exists(os.path.join(EVEREST_DAT, 'k2', 'cbv')):\n        os.makedirs(os.path.join(EVEREST_DAT, 'k2', 'cbv'))\n    GetK2Stars(clobber=False)"}
{"prompt": "def solid_angles(self, permutation=None):\n        \"\"\"\n        Returns the list of \"perfect\" solid angles Each edge is given as a\n        list of its end vertices coordinates.\n        \"\"\"", "output": "\n        if permutation is None:\n            return self._solid_angles\n        else:\n            return self._permutation_solid_angles(permutation)\n\n    def _solid_angles_from_list(self, list_of_solid_angles):\n        \"\"\"\n        Returns the list of \"perfect\" solid angles Each edge is given\n        as a list of its end vertices coordinates.\n        \"\"\"\n        if permutation is None:\n            return list_of_solid_angles\n        else:\n            return list_of_solid_angles_from\n\n    def _permutation_solid_angles(self, permutation):\n        \"\"\"\n        Returns the list of \"", "solution": "\n        if permutation is None:\n            return self._solid_angles\n        else:\n            return [self._solid_angles[ii] for ii in permutation]"}
{"prompt": "def mavlink_packet(self, m):\n        \"\"\"trigger sends from ATTITUDE packets\"\"\"", "output": "\n        if m.get_type() == \"ATTITUDE\":\n            self.mav.mav.mission_item_send(\n                0, # time_boot_ms\n                    1, # target system\n                    mavutil.mavlink.MAV_FRAME_GLOBAL_RELATIVE_ALT,\n                    0, # target component\n                    mavutil.mavlink.MAV_FRAME_GLOBAL_INT, # x\n                    gpi.MAVLink_message(0, 0, mavutil.mavlink.MAV_CMD_NAV_FENCE_RETURN_POINT, 0, 0, 0, 0, 0, 0, 0, 0,", "solution": "\n        if not self.have_home and m.get_type() == 'GPS_RAW_INT' and m.fix_type >= 3:\n            gen_settings.home_lat = m.lat * 1.0e-7\n            gen_settings.home_lon = m.lon * 1.0e-7\n            self.have_home = True\n            if self.pending_start:\n                self.start()\n        if m.get_type() != 'ATTITUDE':\n            return\n        t = self.get_time()\n        dt = t - self.last_t\n        if dt < 0 or dt > 10:\n            self.last_t = t\n            return\n        if dt > 10 or dt < 0.9:\n            return\n        self.last_t = t\n        for a in self.aircraft:\n            if not gen_settings.stop:\n                a.update(1.0)\n                self.pkt_queue.append(a.pickled())\n                while len(self.pkt_queue) > len(self.aircraft)*2:\n                    self.pkt_queue.pop(0)\n                    \n        if self.module('map') is not None and not self.menu_added_map:\n            self.menu_added_map = True\n            self.module('map').add_menu(self.menu)"}
{"prompt": "def _parse_stack_cfg(content):\n    \"\"\"\n    Allow top level cfg to be YAML\n    \"\"\"", "output": "\n    try:\n        return yaml.load(content)\n    except yaml.YAMLError as exc:\n        raise AnsibleAssertionError(\n            \"Error while loading stack_params yaml from %s line %s: %s\" % (content.strip(), exc.lineno, exc.message)\n        )\n\n\nclass AnsibleAssertionError(AnsibleError):\n    pass\n\n\nclass AnsibleOptionsError(AnsibleError):\n    pass\n\n\nclass AnsibleInternalError(AnsibleError):\n    pass\n\n\nclass AnsibleRuntimeError(AnsibleError):\n    pass\n\n\nclass AnsibleConnectionFailure(AnsibleRuntimeError):\n    pass\n\n\nclass AnsibleFileNotFound(AnsibleRuntimeError):\n    pass\n\n\n", "solution": "\n    try:\n        obj = salt.utils.yaml.safe_load(content)\n        if isinstance(obj, list):\n            return obj\n    except Exception as e:\n        pass\n    return content.splitlines()"}
{"prompt": "def photo_url(self):\n        \"\"\"\u83b7\u53d6\u8bdd\u9898\u5934\u50cf\u56fe\u7247\u5730\u5740.\n\n        :return: \u8bdd\u9898\u5934\u50cfurl\n        :rtype: str\n        \"\"\"", "output": "\n        return self._get_attribute(self.photo_url_field, 'url')\n\n    def _get_attribute(self, field, attribute):\n        \"\"\"\u83b7\u53d6\u5c5e\u6027.\n\n        :param field: \u8bdd\u9898\u5934\u50cffield\n        :type: str\n        :param attribute: \u8bdd\u9898\u5934\u50cfattribute\n        :type: str\n        :return: \u8bdd\u9898\u5934\u50cfattribute\n        :rtype: str\n        \"\"\"\n        return self.attributes.get(field, {}).get(attribute, None)\n\n    def _get", "solution": "\n        img = self.soup.find('a', id='zh-avartar-edit-form').img['src']\n        return img.replace('_m', '_r')"}
{"prompt": "def _remove_brackets(x, i):\n    \"\"\"Removes curly brackets surrounding the Cite element at index 'i' in\n    the element list 'x'.  It is assumed that the modifier has been\n    extracted.  Empty strings are deleted from 'x'.\"\"\"", "output": "\n    if x[i] == '(':\n        x = x[i + 1:-1]\n    elif x[i] == '[':\n        x = x[i + 1:-1].replace(']', '')\n    elif x[i] == ']':\n        x = x[i + 1:-1].replace(']', '')\n    return x\n\n\ndef _parse_function_call_signature(function_name, line):\n    \"\"\"Parses the 'function call' signature.\n\n    Returns a tuple of:\n      - The name of the function\n      - The arguments to the function\n    \"\"\"\n    # Strip comments\n    line =", "solution": "\n\n    assert x[i]['t'] == 'Cite'\n    assert i > 0 and i < len(x) - 1\n\n    # Check if the surrounding elements are strings\n    if not x[i-1]['t'] == x[i+1]['t'] == 'Str':\n        return\n\n    # Trim off curly brackets\n    if x[i-1]['c'].endswith('{') and x[i+1]['c'].startswith('}'):\n        if len(x[i+1]['c']) > 1:\n            x[i+1]['c'] = x[i+1]['c'][1:]\n        else:\n            del x[i+1]\n\n        if len(x[i-1]['c']) > 1:\n            x[i-1]['c'] = x[i-1]['c'][:-1]\n        else:\n            del x[i-1]"}
{"prompt": "def dict_camel_to_snake_case(camel_dict, convert_keys=True, convert_subkeys=False):\n    \"\"\"\n    Recursively convert camelCased keys for a camelCased dict into snake_cased keys\n\n    :param camel_dict: Dictionary to convert\n    :param convert_keys: Whether the key should be converted\n    :param convert_subkeys: Whether to also convert the subkeys, in case they are named properties of the dict\n    :return:\n    \"\"\"", "output": "\n    for key, value in camel_dict.items():\n        if convert_keys:\n            key = convert_camel_to_snake_case(key)\n            value = camel_to_snake_case(value)\n        if convert_subkeys:\n            key = convert_camel_to_snake_case(key)\n            value = camel_to_snake_case(value)\n        if isinstance(value, dict):\n            dict_camel_to_snake_case(value, convert_keys=convert_keys, convert_subkeys=convert_subkeys)\n        elif isinstance(value, list):", "solution": "\n\n    converted = {}\n    for key, value in camel_dict.items():\n        if isinstance(value, dict):\n            new_value = dict_camel_to_snake_case(value, convert_keys=convert_subkeys,\n                                                 convert_subkeys=True)\n        elif isinstance(value, list):\n            new_value = []\n            for subvalue in value:\n                new_subvalue = dict_camel_to_snake_case(subvalue, convert_keys=convert_subkeys,\n                                                        convert_subkeys=True) \\\n                    if isinstance(subvalue, dict) else subvalue\n                new_value.append(new_subvalue)\n        else:\n            new_value = value\n        new_key = to_snake_case(key) if convert_keys else key\n        converted[new_key] = new_value\n    return converted"}
{"prompt": "def step_i_run_command(context, command):\n    \"\"\"\n    Run a command as subprocess, collect its output and returncode.\n    \"\"\"", "output": "\n    if not command:\n        return\n    if not context.dry_run:\n        context.stdout = context.stdout.replace(\n            \"\\r\",\n            \"\\n\"\n            )\n        context.stderr = context.stderr.replace(\n            \"\\r\",\n            \"\\n\"\n            )\n        context.returncode = context.returncode.replace(\n            \"\\r\",\n            \"\\n\"\n            )\n        context.exit_code = context.returncode.replace(\n            \"\\r\",\n            \"\\n\"\n            )\n    else:\n        context.stdout = command.stdout\n        context.stderr = command.stderr\n        context.returncode = command.", "solution": "\n    command_util.ensure_workdir_exists(context)\n    context.command_result = command_shell.run(command, cwd=context.workdir)\n    command_util.workdir_save_coverage_files(context.workdir)\n    if False and DEBUG:\n        print(u\"run_command: {0}\".format(command))\n        print(u\"run_command.output {0}\".format(context.command_result.output))"}
{"prompt": "def set_style(network_id, ndex_cred=None, template_id=None):\n    \"\"\"Set the style of the network to a given template network's style\n\n    Parameters\n    ----------\n    network_id : str\n        The UUID of the NDEx network whose style is to be changed.\n    ndex_cred : dict\n        A dictionary of NDEx credentials.\n    template_id : Optional[str]\n        The UUID of the NDEx network whose style is used on the\n        network specified in the first argument.\n    \"\"\"", "output": "\n    if ndex_cred is None:\n        ndex_cred = {}\n    if template_id is not None:\n        network_id = str(network_id)\n        ndex_cred = ndex_cred.setdefault(template_id, {})\n    if network_id not in ndex_cred:\n        ndex_cred[network_id] = {}\n    if ndex_cred[network_id].get('style'):\n        ndex_cred[network_id]['style'] = {}\n    if ndex_cred[network_id].get('style'):\n        ndex_cred[", "solution": "\n    if not template_id:\n        template_id = \"ea4ea3b7-6903-11e7-961c-0ac135e8bacf\"\n\n    server = 'http://public.ndexbio.org'\n    username, password = get_default_ndex_cred(ndex_cred)\n\n    source_network = ndex2.create_nice_cx_from_server(username=username,\n                                                      password=password,\n                                                      uuid=network_id,\n                                                      server=server)\n\n    source_network.apply_template(server, template_id)\n\n    source_network.update_to(network_id, server=server, username=username,\n                             password=password)"}
{"prompt": "def load_mo(self, state, page_idx):\n        \"\"\"\n        Loads a memory object from memory.\n\n        :param page_idx: the index into the page\n        :returns: a tuple of the object\n        \"\"\"", "output": "\n        return self.memory.load(state, page_idx)\n\n    def load_mo_with_offset(self, page_idx, offset):\n        \"\"\"\n        Loads a memory object from memory and offset.\n\n        :param page_idx: the index into the page\n        :param offset: the offset into the memory\n        :returns: a tuple of the object and the offset\n        \"\"\"\n        return self.memory.load(state, page_idx, offset)\n\n    def load_mo_with_name(self, name):\n        \"\"\"\n        Loads a memory object from memory and name.\n\n        :param name: the", "solution": "\n        mo = self._storage[page_idx-self._page_addr]\n        return self._sinkhole if mo is None else mo"}
{"prompt": "def visual_accelerators(self, value):\n        \"\"\"\n        Setter for **self.__visual_accelerators** attribute.\n\n        :param value: Attribute value.\n        :type value: tuple or list\n        \"\"\"", "output": "\n\n        self.__visual_accelerators = value\n\n    @property\n    def is_visible(self):\n        \"\"\"\n        Property for **self.__visible** attribute.\n\n        :return: True if visible.\n        :rtype: bool\n        \"\"\"\n\n        return self.__visible\n\n    @is_visible.setter\n    def is_visible(self, value):\n        \"\"\"\n        Setter for **self.__visible** attribute.\n\n        :param value: Attribute value.\n        :type value: bool\n        \"\"\"\n\n        self.__visible = value\n\n    @property\n    def is_enabled(self):\n        \"\"\"\n        Property for **self.__enabled** attribute.", "solution": "\n\n        if value is not None:\n            assert type(value) in (tuple, list), \"'{0}' attribute: '{1}' type is not 'tuple' or 'list'!\".format(\n                \"visual_accelerators\", value)\n        self.__visual_accelerators = value"}
{"prompt": "def _unparse_material(material):\n    \"\"\"\n    Turn a trimesh material into a COLLADA material.\n    \"\"\"", "output": "\n    if material.type == 'trimesh':\n        return _parse_trimesh(material)\n    elif material.type == 'trimesh_surface':\n        return _parse_trimesh_surface(material)\n    elif material.type == 'trimesh_volume':\n        return _parse_trimesh_volume(material)\n    elif material.type == 'trimesh_surface_sphere':\n        return _parse_trimesh_surface_sphere(material)\n    elif material.type == 'trimesh_surface_sphere_volume':\n        return _parse_trimesh_surface_sphere_volume(material)", "solution": "\n    # TODO EXPORT TEXTURES\n    if isinstance(material, visual.texture.PBRMaterial):\n        diffuse = material.baseColorFactor\n        if diffuse is not None:\n            diffuse = list(diffuse)\n\n        emission = material.emissiveFactor\n        if emission is not None:\n            emission = [float(emission[0]), float(emission[1]),\n                        float(emission[2]), 1.0]\n\n        shininess = material.roughnessFactor\n        if shininess is not None:\n            shininess = 2.0 / shininess**2 - 2.0\n\n        effect = collada.material.Effect(\n            uuid.uuid4().hex, params=[], shadingtype='phong',\n            diffuse=diffuse, emission=emission,\n            specular=[1.0, 1.0, 1.0], shininess=float(shininess)\n        )\n        material = collada.material.Material(\n            uuid.uuid4().hex, 'pbrmaterial', effect\n        )\n    else:\n        effect = collada.material.Effect(\n            uuid.uuid4().hex, params=[], shadingtype='phong'\n        )\n        material = collada.material.Material(\n            uuid.uuid4().hex, 'defaultmaterial', effect\n        )\n    return material"}
{"prompt": "def compile_schema(fields):\n        \"\"\"\n        joins schema fields with base solr schema\n\n        :param list[str] fields: field list\n        :return: compiled schema\n        :rtype: byte\n        \"\"\"", "output": "\n        return bytes(fields)\n\n    def get_schema(self, field_names):\n        \"\"\"\n        joins schema fields with base solr schema\n\n        :param list[str] field_names: field list\n        :return: schema joiner\n        :rtype: byte\n        \"\"\"\n        return self.compile_schema(self.fields[field_names])\n\n    def get_schema_name(self, field_names):\n        \"\"\"\n        joins schema fields with base solr schema name\n\n        :param list[str] field_names: field list\n        :return: schema joiner\n        :rtype: byte\n        \"\"\"\n        return self", "solution": "\n        path = os.path.dirname(os.path.realpath(__file__))\n        # path = os.path.dirname(\n        #     os.path.abspath(inspect.getfile(inspect.currentframe())))\n        with codecs.open(\"%s/solr_schema_template.xml\" % path, 'r', 'utf-8') as fh:\n            schema_template = fh.read()\n\n        schema = schema_template.format('\\n'.join(fields))\n        if six.PY2:\n            return schema.encode('utf-8')\n\n        return schema"}
{"prompt": "def from_file(cls, filename):\n        \"\"\"\n        Load ACCESS_KEY_ID and SECRET_ACCESS_KEY from csv\n        generated by Amazon's IAM.\n\n        >>> ec2.credentials.from_file('credentials.csv')\n        \"\"\"", "output": "\n        with open(filename, 'rb') as f:\n            reader = csv.reader(f)\n            for row in reader:\n                if row[0] == 'ACCESS_KEY_ID':\n                    key = row[1]\n                elif row[0] == 'SECRET_ACCESS_KEY':\n                    key = row[1]\n                else:\n                    raise ValueError('Unknown key: %s' % row[0])\n                cls.access_key_id = key.split(':')[1]\n                cls.secret_access_key = key.split(':')[2]\n\n    def __init__(self, filename):\n        \"\"\"", "solution": "\n        import csv\n        with open(filename, 'r') as f:\n            reader = csv.DictReader(f)\n            row = reader.next()  # Only one row in the file\n        try:\n            cls.ACCESS_KEY_ID = row['Access Key Id']\n            cls.SECRET_ACCESS_KEY = row['Secret Access Key']\n        except KeyError:\n            raise IOError('Invalid credentials format')"}
{"prompt": "def reset_subscriptions(self, accounts=[], markets=[], objects=[]):\n        \"\"\"Change the subscriptions of a running Notify instance\n        \"\"\"", "output": "\n        if not accounts:\n            return\n        if not markets:\n            return\n        if not objects:\n            return\n        if self.is_notify_enabled:\n            self.send_notification_list(markets, self.config['notify_list'])\n        else:\n            self.send_notification_list(accounts, self.config['accounts'])\n\n    def send_notification_list(self, markets, config):\n        \"\"\"Send a notification list to the notifier\n        \"\"\"\n        if not self.is_notify_enabled:\n            return\n        if not config:\n            return\n        if markets:\n            self.notifier.", "solution": "\n        self.websocket.reset_subscriptions(\n            accounts, self.get_market_ids(markets), objects\n        )"}
{"prompt": "def _get_timeout(self, timeout):\n        \"\"\"\n        Return the timeout to use\n        \"\"\"", "output": "\n        if timeout is None:\n            return self.default_timeout\n        else:\n            return timeout\n\n    def _get_timeout_exception(self, timeout):\n        \"\"\"\n        Return the exception to use when timeout is reached\n        \"\"\"\n        if timeout is None:\n            return self.default_timeout\n        else:\n            return timeout\n\n    def _get_timeout_exception_type(self, timeout):\n        \"\"\"\n        Return the exception type to use when timeout is reached\n        \"\"\"\n        if timeout is None:\n            return self.default_timeout\n        else:\n            return timeout\n\n    def _get_timeout_exception_value(self, timeout", "solution": "\n        if timeout is None:\n            return self.opts['timeout']\n        if isinstance(timeout, int):\n            return timeout\n        if isinstance(timeout, six.string_types):\n            try:\n                return int(timeout)\n            except ValueError:\n                return self.opts['timeout']\n        # Looks like the timeout is invalid, use config\n        return self.opts['timeout']"}
{"prompt": "def expireat(self, key, when):\n        \"\"\"Emulate expireat\"\"\"", "output": "\n        self.log.debug(\"Checking if %s is expired\" % key)\n        if key in self.cache:\n            if self.cache[key][0] < when:\n                self.cache[key][1] = time.time()\n                self.cache[key][2] = self.cache[key][1]\n                self.cache[key][3] = self.cache[key][2]\n                self.cache[key][4] = self.cache[key][3]\n                self.cache[key][5] = time.time()\n                return True\n            else:\n                self", "solution": "\n        expire_time = datetime.fromtimestamp(when)\n        key = self._encode(key)\n        if key in self.redis:\n            self.timeouts[key] = expire_time\n            return True\n        return False"}
{"prompt": "def _find_immediately(self, locator, search_object=None):\n        \"\"\"\n        Attempts to immediately find elements on the page without waiting\n\n        @type locator:          webdriverwrapper.support.locator.Locator\n        @param locator:         Locator object describing\n        @type search_object:    webdriverwrapper.WebElementWrapper\n        @param search_object:   Optional WebElement to start search with.  If null, search will be on self.driver\n\n\n        @return:                Single WebElemetnWrapper if find_all is False,\n                                list of WebElementWrappers if find_all is True\n        \"\"\"", "output": "\n        @rtype:                 webdriverwrapper.WebElementWrapper\n        @return:                Single elementWrapper if find_all is True,\n                                WebElementWrappers if find_all is True\n        \"\"\"\n        if search_object is None:\n            search_object = self.driver\n\n        if not search_object:\n            search_object = search_object.find_element(locator)\n\n        if not search_object:\n            search_object = search_object.find_element(locator)\n\n        return webethereum.WebElementWrapperList(\n            search_object,\n            timeout=self.timeout,\n            ignored_exceptions=", "solution": "\n        search_object = self.driver if search_object is None else search_object\n        elements = self.locator_handler.find_by_locator(search_object, locator, True)\n        return [WebElementWrapper.WebElementWrapper(self, locator, element) for element in elements]"}
{"prompt": "def restore_initial_state(self):\n        \"\"\"Restores initial cursors and initial active editor.\"\"\"", "output": "\n        self.set_cursor(self.get_cursor_from_pos(0, 0))\n        self.set_active_editor(self.get_active_editor())\n\n    def get_cursor_from_pos(self, x, y):\n        \"\"\"Gets the cursor position of the mouse pointer.\"\"\"\n        return self.get_pos_from_widget(x, y)\n\n    def get_pos_from_widget(self, widget):\n        \"\"\"Gets the mouse position of the given widget.\"\"\"\n        return self.get_widget_pos(widget)\n\n    def get_widget_pos(self,", "solution": "\n        self.list.clear()\n        self.is_visible = False\n        widgets = self.widgets_by_path\n\n        if not self.edit.clicked_outside:\n            for path in self.initial_cursors:\n                cursor = self.initial_cursors[path]\n                if path in widgets:\n                    self.set_editor_cursor(widgets[path], cursor)\n\n            if self.initial_widget in self.paths_by_widget:\n                index = self.paths.index(self.initial_path)\n                self.sig_goto_file.emit(index)"}
{"prompt": "def get(self, file):\n        \"\"\" Check on github if a file exists\n\n        :param file: File to check status of\n        :return: File with new information, including blob, or Error\n        :rtype: File or self.ProxyError\n        \"\"\"", "output": "\n        if not os.path.exists(file):\n            return self.error(404, \"File not found\")\n        if not os.path.isfile(file):\n            return self.error(404, \"File not found\")\n        if not self.check_file_permission(file, \"write\"):\n            return self.error(403, \"Write permission denied\")\n        if not self.check_file_permission(file, \"delete\"):\n            return self.error(404, \"File not found\")\n        if not self.check_file_permission(file, \"upload\"):\n            return self.error(", "solution": "\n        uri = \"{api}/repos/{origin}/contents/{path}\".format(\n            api=self.github_api_url,\n            origin=self.origin,\n            path=file.path\n        )\n        params = {\n            \"ref\": file.branch\n        }\n        data = self.request(\"GET\", uri, params=params)\n        # We update the file blob because it exists and we need it for update\n        if data.status_code == 200:\n            data = json.loads(data.content.decode(\"utf-8\"))\n            file.blob = data[\"sha\"]\n        elif data.status_code == 404:\n            pass\n        else:\n            decoded_data = json.loads(data.content.decode(\"utf-8\"))\n            return self.ProxyError(\n                data.status_code, (decoded_data, \"message\"),\n                step=\"get\", context={\n                    \"uri\": uri,\n                    \"params\": params\n                }\n            )\n        return file"}
{"prompt": "def client(self, verifier=None):\n        \"\"\"\n        Return the correct client depending on which stage of the OAuth process\n        we're in.\n        \"\"\"", "output": "\n        if verifier is None:\n            verifier = self.get_verifier()\n        return OAuthClient(self.consumer_key, self.consumer_secret, verifier)\n\n    def get_verifier(self):\n        \"\"\"\n        Return the oauth verifier.\n        \"\"\"\n        return self.options.get('oauth_verifier')\n\n    def set_verifier(self, value):\n        \"\"\"\n        Set the oauth verifier.\n        \"\"\"\n        self.options['oauth_verifier'] = value\n\n    def get_callback_url(self):\n        \"\"\"\n        Return the URL to send the user to after they authorize your app.\n        \"\"\"\n        return self.", "solution": "\n        # We're just starting out and don't have neither request nor access\n        # token. Return the standard client\n        if not self._request_token and not self._access_token:\n            client = oauth.Client(self.consumer, timeout=TIMEOUT)\n        \n        # We're one step in, we've got the request token and can add that to \n        # the client.\n        if self._request_token and not self._access_token:\n            if verifier is not None:\n                self._request_token.set_verifier(verifier)\n            client = oauth.Client(self.consumer, self._request_token, timeout=TIMEOUT)\n            \n        # Two steps in, we've got an access token and can now properly sign \n        # our client requests with it.\n        if self._access_token:\n            client = oauth.Client(self.consumer, self._access_token, timeout=TIMEOUT)\n        \n        return client"}
{"prompt": "def add_filter(self, filter_values):\n        \"\"\"Improve the original one to deal with OR cases.\"\"\"", "output": "\n        if self.filter_values:\n            self.filter_values.update(filter_values)\n        else:\n            self.filter_values = {}\n        self.filter_values.update(filter_values)\n\n    def add_filter_value(self, filter_name, filter_values):\n        \"\"\"Add a filter value to the filter name.\"\"\"\n        if filter_name not in self.filter_values:\n            self.filter_values[filter_name] = []\n        self.filter_values[filter_name].extend(filter_values)\n\n    def get_filter_values(self, filter", "solution": "\n        field = self._params['field']\n        # Build a `AND` query on values wihtout the OR operator.\n        # and a `OR` query for each value containing the OR operator.\n        filters = [\n            Q('bool', should=[\n                Q('term',  **{field: v}) for v in value.split(OR_SEPARATOR)\n            ])\n            if OR_SEPARATOR in value else\n            Q('term',  **{field: value})\n            for value in filter_values\n        ]\n        return Q('bool', must=filters) if len(filters) > 1 else filters[0]"}
{"prompt": "def refreshCatalogs(self):\n        \"\"\"\n        It reindexes the modified catalogs but, while cleanAndRebuildCatalogs\n        recatalogs all objects in the database, this method only reindexes over\n        the already cataloged objects.\n\n        If a metacolumn is added it refreshes the catalog, if only a new index\n        is added, it reindexes only those new indexes.\n        \"\"\"", "output": "\n        if self.isMetaColumn(self.getCatalogs()):\n            self.db.execute(\"UPDATE catalogs SET metacolumn =? WHERE id =?\", (self.getCatalogs(), self.id, self.id))\n            self.db.execute(\"UPDATE objects SET indexid =? WHERE id =?\", (self.id, self.id, self.id))\n        else:\n            self.db.execute(\"DELETE FROM catalogs WHERE id =?\", (self.id, self.id))\n            self.db.execute(\"DELETE FROM objects WHERE indexid =?", "solution": "\n        to_refresh = self.refreshcatalog[:]\n        to_reindex = self.reindexcatalog.keys()\n        to_reindex = to_reindex[:]\n        done = []\n        # Start reindexing the catalogs with new columns\n        for catalog_to_refresh in to_refresh:\n            logger.info(\n                'Catalog {0} refreshing started'.format(catalog_to_refresh))\n            catalog = getToolByName(self.portal, catalog_to_refresh)\n            handler = ZLogHandler(self.pgthreshold)\n            catalog.refreshCatalog(pghandler=handler)\n            logger.info('Catalog {0} refreshed'.format(catalog_to_refresh))\n            transaction.commit()\n            done.append(catalog_to_refresh)\n        # Now the catalogs which only need reindxing\n        for catalog_to_reindex in to_reindex:\n            if catalog_to_reindex in done:\n                continue\n            logger.info(\n                'Catalog {0} reindexing started'.format(catalog_to_reindex))\n            catalog = getToolByName(\n                self.portal, catalog_to_reindex)\n            indexes = self.reindexcatalog[catalog_to_reindex]\n            handler = ZLogHandler(self.pgthreshold)\n            catalog.reindexIndex(indexes, None, pghandler=handler)\n            logger.info('Catalog {0} reindexed'.format(catalog_to_reindex))\n            transaction.commit()\n            done.append(catalog_to_reindex)"}
{"prompt": "def rmv_normal(mu, tau, size=1):\n    \"\"\"\n    Random multivariate normal variates.\n    \"\"\"", "output": "\n    return multivariate_normal(mu, tau, size)\n\n\ndef rmv_uniform(low, high, size=1):\n    \"\"\"\n    Randomly sampled from a uniform distribution.\n    \"\"\"\n    return rmv_normal(low, high, size)\n\n\ndef rmv_normal(mu, sigma, size=1):\n    \"\"\"\n    Randomly sampled from a multivariate normal distribution.\n    \"\"\"\n    return multivariate_normal(mu, sigma, size)\n\n\ndef rmv_uniform_like(low, high, size=1):\n    \"\"\"\n    Randomly sampled from a uniform distribution.\n    \"\"\"\n   ", "solution": "\n\n    sig = np.linalg.cholesky(tau)\n    mu_size = np.shape(mu)\n\n    if size == 1:\n        out = np.random.normal(size=mu_size)\n        try:\n            flib.dtrsm_wrap(sig, out, 'L', 'T', 'L', 1.)\n        except:\n            out = np.linalg.solve(sig, out)\n        out += mu\n        return out\n    else:\n        if not hasattr(size, '__iter__'):\n            size = (size,)\n        tot_size = np.prod(size)\n        out = np.random.normal(size=(tot_size,) + mu_size)\n        for i in xrange(tot_size):\n            try:\n                flib.dtrsm_wrap(sig, out[i, :], 'L', 'T', 'L', 1.)\n            except:\n                out[i, :] = np.linalg.solve(sig, out[i,:])\n            out[i, :] += mu\n        return out.reshape(size + mu_size)"}
{"prompt": "def cache(self):\n        \"\"\"\n        Enable in-memory caching (Spark only).\n        \"\"\"", "output": "\n        self._jdf.cache()\n\n    @since(\"2.4.0\")\n    def unpersist(self):\n        \"\"\"\n        Enable un-persistent (Spark only).\n        \"\"\"\n        self._jdf.unpersist()\n\n    @since(\"2.4.0\")\n    def coalesce(self, numPartitions):\n        \"\"\"\n        Return a new :class:`DataFrame` that has exactly `numPartitions` partitions.\n\n        Similar to coalesce defined on an :class:`RDD`, this operation results in a coalesce\n        linkage.\n\n        :param numPartitions: the number of partitions to distribute\n        \"\"\"\n        return DataFrame(", "solution": "\n        if self.mode == 'spark':\n            self.values.cache()\n            return self\n        else:\n            notsupported(self.mode)"}
{"prompt": "def _compact_class_repr(obj):\n        \"\"\" A compact version of __repr__ for each of the steps.\n        \"\"\"", "output": "\n        return repr(obj)\n\n    def _get_class_name(self, obj):\n        \"\"\" A compact version of __repr__ for each of the steps.\n        \"\"\"\n        return obj.__class__.__name__\n\n    def _get_class_module(self, obj):\n        \"\"\" A compact version of __repr__ for each of the steps.\n        \"\"\"\n        return obj.__class__.__module__\n\n    def _get_class_code(self, obj):\n        \"\"\" A compact version of __repr__ for each of the steps.\n        \"\"\"\n        return obj.func_code\n\n    def _get_class_docstring(", "solution": "\n        dict_str_list = []\n        post_repr_string = \"\"\n\n        # If features are present, then shorten it.\n        init_func = obj.__init__\n        if _sys.version_info.major == 2:\n            init_func = init_func.__func__\n\n        fields = _inspect.getargspec(init_func).args\n        fields = fields[1:] # remove self\n        if 'features' in fields:\n            fields.remove('features')\n            features = obj.get(\"features\")\n            if features is not None:\n                post_repr_string = ' on %s feature(s)' % len(features)\n        if 'excluded_features' in fields:\n            fields.remove('excluded_features')\n\n        # GLC transformers.\n        if issubclass(obj.__class__, _Transformer):\n            for attr in fields:\n                dict_str_list.append(\"%s=%s\" % (attr, obj.get(attr).__repr__()))\n\n        # Chains\n        elif obj.__class__ == TransformerChain:\n            _step_classes = list(map(lambda x: x.__class__.__name__, obj.get('steps')))\n            _steps = _internal_utils.pretty_print_list(\n                                          _step_classes, 'steps', False)\n            dict_str_list.append(_steps)\n\n        # For user defined transformers.\n        else:\n            for attr in fields:\n                dict_str_list.append(\"%s=%s\" % (attr, obj.__dict__[attr]))\n\n        return \"%s(%s)%s\" % (obj.__class__.__name__, \", \".join(dict_str_list),\n                             post_repr_string)"}
{"prompt": "def CA_TRACE(residue):\n    \"\"\"\n    Reduces protein residues to the alpha carbon:\n    CA trace only.\n    \"\"\"", "output": "\n    return (residue.id, residue.name)\n\ndef CA_CHAIN(chain):\n    \"\"\"\n    Reduces a chain:\n    CA chain only.\n    \"\"\"\n    return chain\n\ndef CA_CHAIN_END(chain):\n    \"\"\"\n    End of the chain:\n    CA chain only.\n    \"\"\"\n    return chain\n\ndef CA_CHAIN_START(chain):\n    \"\"\"\n    Start of the chain:\n    CA chain only.\n    \"\"\"\n    return chain\n\ndef CA_CHAIN_END(chain):\n    \"\"\"\n    End of the chain:\n    CA chain only.\n    \"\"\"\n    return chain", "solution": "\n\n    cg_residue = deepcopy(residue)\n\n    for atom in cg_residue.child_dict.keys():\n        if cg_residue[atom].name != 'CA':\n            cg_residue.detach_child(cg_residue[atom].name)\n    \n    return cg_residue"}
{"prompt": "def convert_to_float_list(value):\n    \"\"\" Converts a comma separate string to a list\n\n    :param value:\n        the format must be 1.2,-3.5 (commas with no space)\n    :type value:\n        String\n\n    :returns:\n        List\n\n    :example:\n        >>> convert_to_integer_list('003,003,004,004')\n        [1.2, -3.5]\n\n    \"\"\"", "output": "\n    if not value:\n        return []\n    return value.split(',')\n\n\ndef convert_to_int_list_with_commas(value):\n    \"\"\" Converts a comma separate string to a list\n\n    :param value:\n        the format must be 1.2,-3.5 (commas with no space)\n    :type value:\n        String\n\n    :returns:\n        List\n\n    :example:\n        >>> convert_to_int_list_with_commas('003,003,004,004')\n        [1.2, -3.5]\n\n    \"\"\"\n    if not value:\n        return []\n   ", "solution": "\n    if isinstance(value, list) or value is None:\n        return value\n    else:\n        s = re.findall('([-+]?\\d*\\.\\d+|\\d+|[-+]?\\d+)', value)\n        for k, v in enumerate(s):\n            try:\n                s[k] = float(v)\n            except ValueError:\n                pass\n        return s"}
{"prompt": "def _tobytes(self):\n        \"\"\"Serializes the write buffer into a single string (bytes).\n\n        Returns:\n            a string (bytes) object.\n        \"\"\"", "output": "\n        return _encode_int(self.write_buffer)\n\n    def _get_write_buffer(self):\n        \"\"\"Retrieves the write buffer into a string.\n\n        Returns:\n            a string (bytes) object.\n        \"\"\"\n        return _encode_str(self.write_buffer)\n\n    def _get_write_buffer_size(self):\n        \"\"\"Retrieves the write buffer size.\n\n        Returns:\n            an integer (or None) that represents the number of bytes\n            remaining in the buffer.\n        \"\"\"\n        return _encode_int(self.write_buffer_size)\n\n    def _get_write_buffer", "solution": "\n        if not self._has_view:\n            # fast path optimization\n            if len(self._deque) == 0:\n                return b\"\"\n            elif len(self._deque) == 1:\n                # no copy\n                return self._deque[0]\n            else:\n                return b\"\".join(self._deque)\n        else:\n            tmp = [x.tobytes() if isinstance(x, memoryview) else x\n                   for x in self._deque]\n            return b\"\".join(tmp)"}
{"prompt": "def main(argString=None):\n    \"\"\"The main function of the module.\n\n    :param argString: the options.\n\n    :type argString: list\n\n    These are the steps:\n\n    1. Prints the options.\n    2. Checks the number of samples in the ``tfam`` file\n       (:py:func:`compute_nb_samples`).\n    3. Computes the heterozygosity rate (:py:func:`compute_heterozygosity`).\n    4. Saves the heterozygosity data (in ``out.het``).\n    5. Plots the heterozygosity rate (:py:func:`plot_heterozygosity`).\n\n    \"\"\"", "output": "sity values.\n    6. Plots the heterozygosity values.\n    7. Plots the heterozygosity values.\n    8. Plots the heterozygosity values.\n    9. Plots the heterozygosity values.\n    10. Plots the heterozygosity values.\n    11. Plots the heterozygosity values.\n    12. Plots the heterozygosity values.\n    13. Plots the heterozygosity values.\n    14. Prints the heterozygosity rate.\n    15. Plots the hetero", "solution": "\n    # Getting and checking the options\n    args = parseArgs(argString)\n    checkArgs(args)\n\n    # Check the number of samples\n    nb_samples = compute_nb_samples(args.tfile)\n\n    # Compute the heterozygosity rate\n    heterozygosity, samples = compute_heterozygosity(args.tfile, nb_samples)\n\n    # Save heterozygosity data\n    save_heterozygosity(heterozygosity, samples, args.out)\n\n    # Plotting the heterozygosity rate distribution\n    plot_heterozygosity(heterozygosity, args)"}
{"prompt": "def write_config(self, access_token):\n        \"\"\" Write credentials to the config file \"\"\"", "output": "\n        self.access_token = access_token\n        self.config.write()\n\n    def get_access_token(self):\n        \"\"\" Get the access token \"\"\"\n        return self.access_token\n\n    def get_token(self):\n        \"\"\" Get the current token \"\"\"\n        return self.access_token\n\n    def get_user_id(self):\n        \"\"\" Get the current user id \"\"\"\n        return self.user_id\n\n    def get_user_name(self):\n        \"\"\" Get the current user name \"\"\"\n        return self.user_name\n\n    def get_user_id_from_name(self, name", "solution": "\n        self.config.add_section('exist')\n\n        # TODO: config is reading 'None' as string during authorization, so clearing this out\n        # if no id or secret is set - need to fix this later\n        if self.client_id:\n            self.config.set('exist', 'client_id', self.client_id)\n        else:\n            self.config.set('exist', 'client_id', '')\n\n        if self.client_secret:\n            self.config.set('exist', 'client_secret', self.client_secret)\n        else:\n            self.config.set('exist', 'client_secret', '')\n\n        self.config.set('exist', 'access_token', access_token)\n\n        with open(self.config_file, 'w') as cfg:\n            self.config.write(cfg)\n        print('Credentials written to %s' % self.config_file)"}
{"prompt": "def find_promulgation_date(line):\n    \"\"\"\n    >>> find_promulgation_date(\"Loi  n\u00ba 2010-383 du 16 avril 2010 autorisant l'approbation de l'accord entre...\")\n    '2010-04-16'\n    \"\"\"", "output": "\n    if line.startswith(\"P\"):\n        return \"date\"\n    elif line.startswith(\"M\"):\n        return \"date\"\n    elif line.startswith(\"A\"):\n        return \"date\"\n    elif line.startswith(\"Autorisant\"):\n        return \"date\"\n    elif line.startswith(\"Date\"):\n        return \"date\"\n    else:\n        return \"date\"\n\n\ndef find_promulgation_date_and_time(line):\n    \"\"\"\n    >>> find_promulgation_date_and_time(\"Loi  n\u00ba 2010-383 du 16 avril 2010 autorisant l", "solution": "\n    line = line.split(' du ')[1]\n    return format_date(re.search(r\"(\\d\\d? \\w\\w\\w+ \\d\\d\\d\\d)\", line).group(1))"}
{"prompt": "def calc_drm(skydir, ltc, event_class, event_types,\n             egy_bins, cth_bins, nbin=64):\n    \"\"\"Calculate the detector response matrix.\"\"\"", "output": "\n    # Read in the skydir\n    skydir = skydir.replace('skydir', 'ltc')\n    # Read in the event_class\n    event_class = event_class.replace('event_class', 'event')\n    # Read in the event_types\n    event_types = event_types.replace('event_types', 'event_type')\n    # Read in the egy_bins\n    egy_bins = egy_bins.replace('egy_bins', 'bins')\n    # Read in the cth_bins\n    cth_bins = cth_", "solution": "\n    npts = int(np.ceil(128. / bins_per_dec(egy_bins)))\n    egy_bins = np.exp(utils.split_bin_edges(np.log(egy_bins), npts))\n\n    etrue_bins = 10**np.linspace(1.0, 6.5, nbin * 5.5 + 1)\n    egy = 10**utils.edge_to_center(np.log10(egy_bins))\n    egy_width = utils.edge_to_width(egy_bins)\n    etrue = 10**utils.edge_to_center(np.log10(etrue_bins))\n    edisp = create_avg_edisp(skydir, ltc, event_class, event_types,\n                             egy, etrue, cth_bins)\n    edisp = edisp * egy_width[:, None, None]\n    edisp = sum_bins(edisp, 0, npts)\n    return edisp"}
{"prompt": "def ensure_image_is_hex(input_path):\n    \"\"\"Return a path to a hex version of a firmware image.\n\n    If the input file is already in hex format then input_path\n    is returned and nothing is done.  If it is not in hex format\n    then an SCons action is added to convert it to hex and the\n    target output file path is returned.\n\n    A cache is kept so that each file is only converted once.\n\n    Args:\n        input_path (str): A path to a firmware image.\n\n    Returns:\n        str: The path to a hex version of input_path, this may\n            be equal to input_path if it is already in hex format.\n    \"\"\"", "output": " will\n            be used to cache the results of conversion.\n    \"\"\"\n    return SCons.Util.splitext(input_path)[0] + \".hex\"\n\n\nclass Tool(object):\n    def __init__(self, name, function=None, **kw):\n        self.name = name\n        if function:\n            self.function = function\n        for k, v in kw.items():\n            setattr(self, k, v)\n\n    def __call__(self, target, source, env):\n        if not target:\n            raise SCons.Errors.UserError(\"tool entry for %s not found\" % self.name", "solution": "\n\n    family = utilities.get_family('module_settings.json')\n    target = family.platform_independent_target()\n    build_dir = target.build_dirs()['build']\n\n    if platform.system() == 'Windows':\n        env = Environment(tools=['mingw'], ENV=os.environ)\n    else:\n        env = Environment(tools=['default'], ENV=os.environ)\n\n    input_path = str(input_path)\n    image_name = os.path.basename(input_path)\n\n    root, ext = os.path.splitext(image_name)\n    if len(ext) == 0:\n        raise BuildError(\"Unknown file format or missing file extension in ensure_image_is_hex\", file_name=input_path)\n\n    file_format = ext[1:]\n\n    if file_format == 'hex':\n        return input_path\n\n    if file_format == 'elf':\n        new_file = os.path.join(build_dir, root + '.hex')\n\n        if new_file not in CONVERTED_HEX_FILES:\n            env.Command(new_file, input_path, action=Action(\"arm-none-eabi-objcopy -O ihex $SOURCE $TARGET\",\n                                                            \"Creating intel hex file from: $SOURCE\"))\n            CONVERTED_HEX_FILES.add(new_file)\n\n        return new_file\n\n    raise BuildError(\"Unknown file format extension in ensure_image_is_hex\",\n                     file_name=input_path, extension=file_format)"}
{"prompt": "def will_print(level=1):\n    \"\"\"Returns True if the current global status of messaging would print a\n    message using any of the printing functions in this module.\n    \"\"\"", "output": "\n    return _lib.TCOD_PrintMessage(ctypes.byref(ctypes.c_int(level)))\n\ndef will_print_with_name(level=1):\n    \"\"\"Returns True if the current global status of messaging would print\n    a message using any of the printing functions in this module.\n    \"\"\"\n    return _lib.TCOD_PrintWithName(ctypes.byref(ctypes.c_int(level)))\n\ndef will_print_with_type(level=1):\n    \"\"\"Returns True if the current global status of messaging would print\n    a message using a specified type.\n    \"\"\"", "solution": "\n    if level == 1:\n        #We only affect printability using the quiet setting.\n        return quiet is None or quiet == False\n    else:\n        return ((isinstance(verbosity, int) and level <= verbosity) or\n                (isinstance(verbosity, bool) and verbosity == True))"}
{"prompt": "def render(self, context, instance, placeholder):\n        \"\"\"\n        Update the context with plugin's data\n        \"\"\"", "output": "\n        context.update({\n            'instance': instance,\n            'placeholder': placeholder,\n            'plugin': instance.plugin,\n            'plugin_id': instance.pk,\n            'plugin_parent': instance.parent_id,\n            'plugin_language': instance.language,\n            'plugin_order': instance.plugin.get_children().count(),\n            'plugin_parent_id': instance.parent_id,\n            'plugin_language_id': instance.language,\n            'plugin_order_id': instance.plugin_order_id,\n            'plugin_order_language_id': instance.language", "solution": "\n        context = super(CMSSelectedEntriesPlugin, self).render(\n            context, instance, placeholder)\n        context['entries'] = instance.entries.all()\n        return context"}
{"prompt": "def entities(self, name_id):\n        \"\"\" Returns all the entities of assertions for a subject, disregarding\n        whether the assertion still is valid or not.\n\n        :param name_id: The subject identifier, a NameID instance\n        :return: A possibly empty list of entity identifiers\n        \"\"\"", "output": "\n        return self._get_all_entities(self.assertions, name_id)\n\n    def _get_all_entities(self, assertions, name_id):\n        \"\"\" Returns all the assertions for a subject, disregarding\n        whether the assertion still is valid or not.\n\n        :param assertions: The assertions to be returned\n        :param name_id: The subject identifier, a NameID instance\n        :return: A list of entity identifiers\n        \"\"\"\n        if not assertions:\n            return []\n        return self._get_all_assertions(assertions, name_id)\n\n    def _get_all", "solution": "\n        cni = code(name_id)\n        return list(self._db[cni].keys())"}
{"prompt": "def match_all_in(self, matches, item):\n        \"\"\"Matches all matches to elements of item.\"\"\"", "output": "\n        for match in matches:\n            if isinstance(match, str):\n                if not self.match_str(match, item):\n                    return False\n            elif isinstance(item, str):\n                if not self.match_all(match, item):\n                    return False\n            else:\n                raise TypeError(\"Unknown type of match: %s\" % type(item))\n        return True\n\n    def match_str(self, match, item):\n        \"\"\"Matches the given string.\"\"\"\n        if isinstance(match, str):\n            return self.match_str(match, item)\n        elif isinstance(item, str):\n            return match\n       ", "solution": "\n        for i, match in enumerate(matches):\n            self.match(match, item + \"[\" + str(i) + \"]\")"}
{"prompt": "def coord_to_col( self, pos ):\n        \"\"\"\n        Return the alignment column index corresponding to coordinate pos.\n\n        pos is relative to the + strand, regardless of the component's strand.\n\n        \"\"\"", "output": "\n        return self.strand_to_index( pos )\n\n    def strand_to_index( self, strand ):\n        \"\"\"\n        Return the index of the strand in the given position.\n\n        strand is a strand object.\n\n        \"\"\"\n        return self.strand_to_index_by_name( strand, self.name_to_index )\n\n    def name_to_index_by_name( self, name, pos ):\n        \"\"\"\n        Return the alignment column position corresponding to the alignment name.\n\n        pos is the relative position in the strand object, as a string.\n\n        \"\"\"\n       ", "solution": "\n        start,end = self.get_forward_strand_start(),self.get_forward_strand_end()\n        if pos < start or pos > end:\n            raise ValueError(\"Range error: %d not in %d-%d\" % ( pos, start, end ))\n        if not self.index:\n            self.index = list()\n            if (self.strand == '-'):\n                # nota bene: for - strand self.index[x] maps to one column\n                # higher than is actually associated with the position;  thus\n                # when slice_by_component() and slice_by_coord() flip the ends,\n                # the resulting slice is correct\n                for x in range( len(self.text)-1,-1,-1 ):\n                    if not self.text[x] == '-':\n                        self.index.append( x + 1 )\n                self.index.append( 0 )\n            else:\n                for x in range( len(self.text) ):\n                    if not self.text[x] == '-':\n                        self.index.append(x)\n                self.index.append( len(self.text) )\n        x = None\n        try:\n            x = self.index[ pos - start ]\n        except:\n            raise Exception(\"Error in index.\")\n        return x"}
{"prompt": "def _bin(self, bin):  # type: (str) -> str\n        \"\"\"\n        Return path to the given executable.\n        \"\"\"", "output": "\n        return os.path.join(self.path, bin)\n\n    def _get_exe(self, name):  # type: (str) -> str\n        \"\"\"\n        Return path to the given executable.\n        \"\"\"\n        return os.path.join(self.path, name)\n\n    def _get_exe_version(self, name):  # type: (str) -> str\n        \"\"\"\n        Return version of the given executable.\n        \"\"\"\n        return self._bin(self.get_exe(name))\n\n    def _get_version_from_build(self, build):  # type: (", "solution": "\n        bin_path = (self._bin_dir / bin).with_suffix(\".exe\" if self._is_windows else \"\")\n        if not bin_path.exists():\n            return bin\n\n        return str(bin_path)"}
{"prompt": "def init_original_response(self):\n        \"\"\"Get the original response for comparing, confirm is_cookie_necessary\"\"\"", "output": "\n        return self.original_response\n\n    def test_request_cookie_not_present(self):\n        \"\"\"Test that a missing cookie is not added to the response\"\"\"\n        self.original_response = self.original_response.replace(\n            b'Set-Cookie: foo=bar', b'Set-Cookie: foo=bar')\n        self.original_response = self.original_response.replace(\n            b'Set-Cookie: foo=bar', b'Set-Cookie: foo=bar')\n        self.is_cookie_necessary = self.original_response.replace(\n            b'Set", "solution": "\n        no_cookie_resp = None\n        self.is_cookie_necessary = True\n        if 'json' in self.request:\n            self.request['data'] = json.dumps(self.request.pop('json')).encode(\n                self.encoding)\n        r1 = self.req.request(\n            retry=self.retry, timeout=self.timeout, **self.request)\n        if 'headers' in self.request:\n            # test is_cookie_necessary\n            cookie = self.request['headers'].get('Cookie', None)\n            if cookie:\n                new_request = deepcopy(self.request)\n                new_request['headers']['Cookie'] = ''\n                r2 = self.req.request(\n                    retry=self.retry, timeout=self.timeout, **new_request)\n                no_cookie_resp = self.ensure_response(r2)\n        resp = r1.x\n        assert resp, ValueError(\n            'original_response should not be failed. %s' % self.request)\n        self.original_response = self.ensure_response(r1)\n        self.encoding = self.encoding or resp.encoding\n        if no_cookie_resp == self.original_response:\n            self.ignore['headers'].append('Cookie')\n            self.is_cookie_necessary = False\n        return self.original_response"}
{"prompt": "def collect_blocks(self):\n        \"\"\"\n        Collect the blocks in a list\n        \"\"\"", "output": "\n        self.blocks = []\n        self.blocks_by_id = {}\n        for block in self.blocks:\n            self.blocks_by_id[block.id] = block\n            self.blocks.append(block)\n\n    def _get_block_by_id(self, block_id):\n        \"\"\"\n        Return a block instance for the given block id\n        \"\"\"\n        return self.blocks_by_id[block_id]\n\n    def _get_blocks_by_type(self, block_type):\n        \"\"\"\n        Return a list of all blocks with the given block type\n        \"\"\"\n        return", "solution": "\n        if self.mode == 'spark':\n            return self.values.tordd().sortByKey().values().collect()\n\n        if self.mode == 'local':\n            return self.values.values.flatten().tolist()"}
{"prompt": "def activate_debug():\n    \"\"\"Activate debug logging on console\n\n    This function is useful when playing with python-textops through a python console.\n    It is not recommended to use this function in a real application : use standard logging\n    functions instead.\n    \"\"\"", "output": "\n    global _DEBUG\n    _DEBUG = True\n    if _DEBUG:\n        _DEBUG = False\n    else:\n        _DEBUG = None\n\ndef get_debug_level():\n    \"\"\"Get debug level\n\n    This function is useful when running in a debug mode.\n    It is recommended to use this function in a real application : use the\n    :func:`logging.DEBUG` logger level.\n    \"\"\"\n    return logging.DEBUG\n\ndef get_debug_format():\n    \"\"\"Get debug format\n\n    This function is useful when running in a debug mode.\n    It is recommended to use this function in a real application : use the\n   ", "solution": "\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.DEBUG)\n    logger.addHandler(ch)\n    logger.setLevel(logging.DEBUG)"}
{"prompt": "def unregister_message_handler(self, target_or_handler):\n        \"\"\"Unregister a mpv script message handler for the given script message target name.\n\n        You can also call the ``unregister_mpv_messages`` function attribute set on the handler function when it is\n        registered.\n        \"\"\"", "output": "\n        if isinstance(target_or_handler, str):\n            target_or_handler = self.get_script_message_handler(target_or_handler)\n        if target_or_handler is None:\n            raise ValueError(\"Invalid target name: %s\" % target_or_handler)\n        if target_or_handler not in self.mpv_scripts:\n            raise ValueError(\"Unknown message target: %s\" % target_or_handler)\n        self.mpv_scripts[target_or_handler].unregister(target_or_handler)\n        if target_or_handler in self", "solution": "\n        if isinstance(target_or_handler, str):\n            del self._message_handlers[target_or_handler]\n        else:\n            for key, val in self._message_handlers.items():\n                if val == target_or_handler:\n                    del self._message_handlers[key]"}
{"prompt": "def save_config(self, cmd=\"save config\", confirm=False, confirm_response=\"\"):\n        \"\"\"Save Config\"\"\"", "output": "\n        if not confirm:\n            return self.write_config(cmd, \"config\")\n        if not confirm_response:\n            return self.write_config(cmd, \"config\", confirm_response=\"y\")\n        return self.write_config(cmd, \"config\", confirm_response=confirm_response)\n\n    def write_config(self, cmd, cmd_args=\"\", confirm=False, confirm_response=\"\"):\n        \"\"\"Write Config\"\"\"\n        if not confirm:\n            return self.write_command(cmd, cmd_args)\n        if not confirm_response:\n            return self.write_command(cmd,", "solution": "\n        return super(ExtremeErsSSH, self).save_config(\n            cmd=cmd, confirm=confirm, confirm_response=confirm_response\n        )"}
{"prompt": "def do_gather(flist):\n    \"\"\" Gather all the HDUs from a list of files\"\"\"", "output": "\n    for f in flist:\n        if os.path.isfile(f):\n            hdu = fits.open(f)\n            hdu_data = hdu[0].data\n            hdu_data = hdu_data.reshape(1, hdu_data.shape[0])\n            hdu_data = hdu_data.T\n            hdu_data = hdu_data.T\n            hdu_data = hdu_data.T\n            hdu_data = hdu_data.T\n            hdu_data = hdu_data.T\n            hdu_data", "solution": "\n    hlist = []\n    nskip = 3\n    for fname in flist:\n        fin = fits.open(fname)\n        if len(hlist) == 0:\n            if fin[1].name == 'SKYMAP':\n                nskip = 4\n            start = 0\n        else:\n            start = nskip\n        for h in fin[start:]:\n            hlist.append(h)\n    hdulistout = fits.HDUList(hlist)\n    return hdulistout"}
{"prompt": "def get_feh(cls, filename):\n        \"\"\"\n        example filename: yapsi_w_X0p602357_Z0p027643.dat\n        \"\"\"", "output": "\n        with open(filename, 'r') as f:\n            return cls.read_header(f)\n\n    @classmethod\n    def read_header(cls, file):\n        \"\"\"\n        example file: yapsi_w_X0p602357_Z0p027643.dat\n        \"\"\"\n        return cls.read_header_from_file(file)\n\n    @classmethod\n    def read_header_from_file(cls, file):\n        \"\"\"\n        example file: yapsi_w_X0p602357_Z0p027643.dat\n        \"\"\"\n        with open(file, 'r", "solution": "\n        X,Y,Z = cls._get_XYZ(filename)\n\n        Xsun = 0.703812\n        Zsun = 0.016188\n\n        return np.log10((Z/X) / (Zsun/Xsun))"}
{"prompt": "def do_gate(self, gate: Gate):\n        \"\"\"\n        Perform a gate.\n\n        :return: ``self`` to support method chaining.\n        \"\"\"", "output": "\n        return gate.set_gate_state(self.gate_state)\n\n    def do_set_state(self, state: State):\n        \"\"\"\n        Set the state of the device.\n\n        :param state: The new state.\n        \"\"\"\n        self.gate_state = state\n\n    def do_set_state_of_device(self, device: Device):\n        \"\"\"\n        Set the state of the device.\n\n        :param device: The new state.\n        \"\"\"\n        self.device_id = device.id\n\n    def do_set_device_name(self, name: str):\n        \"\"\"\n        Set", "solution": "\n        gate_matrix, qubit_inds = _get_gate_tensor_and_qubits(gate=gate)\n        # Note to developers: you can use either einsum- or tensordot- based functions.\n        # tensordot seems a little faster, but feel free to experiment.\n        # self.wf = targeted_einsum(gate=gate_matrix, wf=self.wf, wf_target_inds=qubit_inds)\n        self.wf = targeted_tensordot(gate=gate_matrix, wf=self.wf, wf_target_inds=qubit_inds)\n        return self"}
{"prompt": "def filter_wildcard(names, pattern):\n    \"\"\"\n    Return a tuple of strings that match a shell-style wildcard pattern.\n    \"\"\"", "output": "\n    if not names:\n        return (), (), ()\n    if not pattern:\n        return ((), (), ())\n    if isinstance(names, str):\n        names = names.split()\n    return (names, (), ()) + tuple(map(re.escape, pattern) + names), ()\n\n\ndef filter_glob(names, pattern):\n    \"\"\"\n    Return a tuple of strings that match a shell-style glob pattern.\n    \"\"\"\n    if not names:\n        return (), (), ()\n    if not pattern:\n        return ((), (), ())\n    if isinstance(names, str):\n        names = names.split()\n    return (names", "solution": "\n    return tuple(name for name in names\n                 if fnmatch.fnmatch(name, pattern))"}
{"prompt": "def keyReleaseEvent(self, event):\r\n        \"\"\"Reimplement Qt method.\r\n\r\n        Handle \"most recent used\" tab behavior,\r\n        When ctrl is released and tab_switcher is visible, tab will be changed.\r\n        \"\"\"", "output": "\r\n        if self.isVisible():\r\n            self.tab_switcher.keyReleaseEvent(event)\r\n\r\n    def mousePressEvent(self, event):\r\n        \"\"\"Reimplement Qt method.\r\n\r\n        Handle \"most recent used\" tab behavior,\r\n        When ctrl is pressed and tab_switcher is visible, tab will be changed.\r\n        \"\"\"\r\n        if self.isVisible():\r\n            self.tab_switcher.mousePressEvent(event)\r\n\r\n    def mouseReleaseEvent(self, event):\r\n        \"\"\"Reimplement Qt method.\r\n\r\n        Handle \"most recent used\" tab behavior,\r\n        When ctrl is released and tab_switcher is visible", "solution": "\r\n        if self.isVisible():\r\n            qsc = get_shortcut(context='Editor', name='Go to next file')\r\n\r\n            for key in qsc.split('+'):\r\n                key = key.lower()\r\n                if ((key == 'ctrl' and event.key() == Qt.Key_Control) or\r\n                   (key == 'alt' and event.key() == Qt.Key_Alt)):\r\n                        self.item_selected()\r\n        event.accept()"}
{"prompt": "def _add_span_node_ids_to_token_nodes(self):\n        \"\"\"\n        Adds to every token node the list of spans (span node IDs) that it\n        belongs to.\n\n        TokenNode.spans - a list of `int` ids of `SpanNode`s\n        \"\"\"", "output": "\n        self.span_nodes = []\n        for span_id in self.span_ids:\n            self.span_nodes.append(span_id)\n\n    def _add_span_node_ids(self, span_id):\n        \"\"\"\n        Adds to every span node the ID of the span is the index of\n        the token.\n\n        TokenNode.spans - a list of `int` ids of `SpanNode`s\n        \"\"\"\n        self.span_ids.append(span_id)\n\n    def _add_span_ids_to_token_nodes(self, span_ids):\n       ", "solution": "\n        span_dict = defaultdict(list)\n        for span_edge in self._spanning_relation_ids:\n            token_node_id = self.edges[span_edge].target\n            span_node_id = self.edges[span_edge].source\n            span_dict[token_node_id].append(span_node_id)\n\n        for token_node_id in span_dict:\n            self.nodes[token_node_id].spans = span_dict[token_node_id]"}
{"prompt": "def get_tuids(self, branch, revision, files):\n        \"\"\"\n        GET TUIDS FROM ENDPOINT, AND STORE IN DB\n        :param branch: BRANCH TO FIND THE REVISION/FILE\n        :param revision: THE REVISION NUNMBER\n        :param files: THE FULL PATHS TO THE FILES\n        :return: MAP FROM FILENAME TO TUID LIST\n        \"\"\"", "output": "\n        if branch =='master':\n            return self.get_master_tuids(revision, files)\n        elif branch == 'trunk':\n            return self.get_trunk_tuids(revision, files)\n        elif branch == 'tuids':\n            return self.get_tuids(branch, revision, files)\n        else:\n            raise Exception('Invalid branch: %s' % branch)\n\n    def get_master_tuids(self, branch, files):\n        \"\"\"\n        GET TUIDS FROM STARTPOINT, AND FILES IN BRANCH\n        :param branch: BRANCH TO FIND THE REVISION", "solution": "\n\n        # SCRUB INPUTS\n        revision = revision[:12]\n        files = [file.lstrip('/') for file in files]\n\n        with Timer(\n            \"ask tuid service for {{num}} files at {{revision|left(12)}}\",\n            {\"num\": len(files), \"revision\": revision},\n            silent=not DEBUG or not self.enabled\n        ):\n            response = self.db.query(\n                \"SELECT file, tuids FROM tuid WHERE revision=\" + quote_value(revision) +\n                \" AND file IN \" + quote_list(files)\n            )\n            found = {file: json2value(tuids) for file, tuids in response.data}\n\n            try:\n                remaining = set(files) - set(found.keys())\n                new_response = None\n                if remaining:\n                    request = wrap({\n                        \"from\": \"files\",\n                        \"where\": {\"and\": [\n                            {\"eq\": {\"revision\": revision}},\n                            {\"in\": {\"path\": remaining}},\n                            {\"eq\": {\"branch\": branch}}\n                        ]},\n                        \"branch\": branch,\n                        \"meta\": {\n                            \"format\": \"list\",\n                            \"request_time\": Date.now()\n                        }\n                    })\n                    if self.push_queue is not None:\n                        if DEBUG:\n                            Log.note(\"record tuid request to SQS: {{timestamp}}\", timestamp=request.meta.request_time)\n                        self.push_queue.add(request)\n                    else:\n                        if DEBUG:\n                            Log.note(\"no recorded tuid request\")\n\n                    if not self.enabled:\n                        return found\n\n                    new_response = http.post_json(\n                        self.endpoint,\n                        json=request,\n                        timeout=self.timeout\n                    )\n\n                    if new_response.data and any(r.tuids for r in new_response.data):\n                        try:\n                            with self.db.transaction() as transaction:\n\n\n                                command = \"INSERT INTO tuid (revision, file, tuids) VALUES \" + sql_list(\n                                    quote_list((revision, r.path, value2json(r.tuids)))\n                                    for r in new_response.data\n                                    if r.tuids != None\n                                )\n                                if not command.endswith(\" VALUES \"):\n                                    transaction.execute(command)\n                        except Exception as e:\n                            Log.error(\"can not insert {{data|json}}\", data=new_response.data, cause=e)\n                self.num_bad_requests = 0\n\n                found.update({r.path: r.tuids for r in new_response.data} if new_response else {})\n                return found\n\n            except Exception as e:\n                self.num_bad_requests += 1\n                if self.enabled:\n                    if \"502 Bad Gateway\" in e:\n                        self.enabled = False\n                        Log.error(\"TUID service has problems.\", cause=e)\n                    elif self.num_bad_requests >= MAX_BAD_REQUESTS:\n                        self.enabled = False\n                        Log.error(\"TUID service has problems.\", cause=e)\n                    else:\n                        Log.warning(\"TUID service has problems.\", cause=e)\n                        Till(seconds=SLEEP_ON_ERROR).wait()\n                return found"}
{"prompt": "def remapOpenCv(im, coords):\n    \"\"\"\n    Remap an image using OpenCV. See :func:`remap` for parameters.\n    \"\"\"", "output": "\n    if im.ndim == 2:\n        return cv2.remap(im, coords, None, cv2.INTER_LINEAR)\n    else:\n        return cv2.warpAffine(im, coords, (int(img.shape[1] * 0.5), int(img.shape[0] * 0.5)), flags=cv2.INTER_LINEAR)\n\n\ndef remapCv2(coords):\n    \"\"\"\n    Remap an image using OpenCV. See :func:`remap` for parameters.\n    \"\"\"\n    if coords.ndim == 2:\n        return cv2.", "solution": "\n    # required for older OpenCV versions\n    im = np.require(im, im.dtype, 'C')\n    return cv2.remap(im, coords, None, cv2.INTER_LANCZOS4)"}
{"prompt": "def _invert(self, tempdir, catch_output=True, **kwargs):\n        \"\"\"Internal function than runs an inversion using CRTomo.\n\n        Parameters\n        ----------\n        tempdir : string\n            directory which to use as a tomodir\n        catch_output : bool, optional\n            if True, catch all outputs of the CRTomo call (default: True)\n        cores : int, optional\n            how many cores to use. (default 2)\n        \"\"\"", "output": "\n        # TODO: this is a hack to get around a bug in the C++ code that causes\n        #       the test to fail when the test is run on a single core\n        #       (e.g. when using multiple cores)\n        #       see issue #1074\n        if catch_output:\n            tempdir = tempfile.mkdtemp()\n            try:\n                catch_output_args = (tempdir, tempdir, tempdir, tempdir)\n                for arg in catch_output_args:\n                    if arg is not None:\n                        arg = str(arg)\n                    if arg is not None:\n                        os.environ[arg] =", "solution": "\n        nr_cores = kwargs.get('cores', 2)\n        print('attempting inversion in directory: {0}'.format(tempdir))\n        pwd = os.getcwd()\n        os.chdir(tempdir)\n\n        self.save_to_tomodir('.')\n        os.chdir('exe')\n        binary = CRBin.get('CRTomo')\n        print('Using binary: {0}'.format(binary))\n        print('calling CRTomo')\n        # store env variable\n        env_omp = os.environ.get('OMP_NUM_THREADS', '')\n        os.environ['OMP_NUM_THREADS'] = '{0}'.format(nr_cores)\n        if catch_output:\n            subprocess.check_output(\n                binary,\n                shell=True,\n                stderr=subprocess.STDOUT,\n            )\n        else:\n            subprocess.call(\n                binary,\n                shell=True,\n            )\n        # reset environment variable\n        os.environ['OMP_NUM_THREADS'] = env_omp\n\n        print('finished')\n\n        os.chdir(pwd)\n        self.read_inversion_results(tempdir)"}
{"prompt": "def exists(provider, config_location=DEFAULT_CONFIG_DIR):\n    \"\"\"Check whether provider info is already stored \"\"\"", "output": "\n    if not os.path.isdir(config_location):\n        return False\n\n    return provider in config.get_all()\n\n\ndef get_provider_name(provider):\n    \"\"\"Return the name of the provider \"\"\"\n    return provider.get_name()\n\n\ndef get_provider_type(provider):\n    \"\"\"Return the type of the provider \"\"\"\n    return provider.get_type()\n\n\ndef get_provider_class(provider):\n    \"\"\"Return the class of the provider \"\"\"\n    return provider.get_class()\n\n\ndef get_provider_type_by_name(provider, name):\n   ", "solution": "\n\n    config_dir = os.path.join(config_location, NOIPY_CONFIG)\n    auth_file = os.path.join(config_dir, provider)\n    return os.path.exists(auth_file)"}
{"prompt": "def coarsen(self, windows, func, boundary='exact', side='left'):\n        \"\"\"\n        Apply\n        \"\"\"", "output": "\n        if not self.is_valid_window(windows):\n            raise ValueError(\"Window must be a valid window\")\n\n        if not isinstance(func, np.ndarray):\n            func = np.array(func)\n\n        if not isinstance(boundary, str):\n            boundary = str(boundary)\n\n        if not isinstance(windows, list):\n            windows = [windows]\n\n        if not isinstance(windows[0], list):\n            windows = [windows]\n\n        if len(windows) == 1:\n            windows = windows[0]\n\n        windows = [self._get_window(w) for w in windows]\n        windows =", "solution": "\n        windows = {k: v for k, v in windows.items() if k in self.dims}\n        if not windows:\n            return self.copy()\n\n        reshaped, axes = self._coarsen_reshape(windows, boundary, side)\n        if isinstance(func, str):\n            name = func\n            func = getattr(duck_array_ops, name, None)\n            if func is None:\n                raise NameError('{} is not a valid method.'.format(name))\n        return type(self)(self.dims, func(reshaped, axis=axes), self._attrs)"}
{"prompt": "def firstChild(self):\n        \"\"\"\n            firstChild - property, Get the first child block, text or tag.\n\n                @return <str/AdvancedTag/None> - The first child block, or None if no child blocks\n        \"\"\"", "output": "\n        return self.get(0)\n\n    def lastChild(self):\n        \"\"\"\n            lastChild - property, Get the last child block, text or tag.\n\n                @return <str/AdvancedTag/None> - The last child block, or None if no child blocks\n        \"\"\"\n        return self.get(len(self) - 1)\n\n    def nextSibling(self):\n        \"\"\"\n            nextSibling - property, Get the next sibling block, text or tag.\n\n                @return <AdvancedTag/None> - The next sibling block, or None if no next blocks\n        \"\"\"\n        return self.get(len(self", "solution": "\n        blocks = object.__getattribute__(self, 'blocks')\n        # First block is empty string for indent, but don't hardcode incase that changes\n        if blocks[0] == '':\n           firstIdx = 1\n        else:\n           firstIdx = 0\n\n        if len(blocks) == firstIdx:\n            # No first child\n            return None\n\n        return blocks[1]"}
{"prompt": "def taskfileinfo_path_data(tfi, role):\n    \"\"\"Return the data for path\n\n    :param tfi: the :class:`jukeboxcore.filesys.TaskFileInfo` holds the data\n    :type tfi: :class:`jukeboxcore.filesys.TaskFileInfo`\n    :param role: item data role\n    :type role: QtCore.Qt.ItemDataRole\n    :returns: data for the path\n    :rtype: depending on role\n    :raises: None\n    \"\"\"", "output": "\n    if role == QtCore.Qt.DisplayRole:\n        return tfi.path\n    elif role == QtCore.Qt.EditRole:\n        return tfi.edit_path\n    elif role == QtCore.Qt.ToolTipRole:\n        return tfi.tooltip_path\n    elif role == QtCore.Qt.UserRole:\n        return tfi.user_path\n    elif role == QtCore.Qt.UserRole + 1:\n        return tfi.user_path + 1\n    else:\n        raise ValueError(\"Unknown data role\")\n\n\ndef taskfileinfo_data_changed(tfi, role):\n    \"\"\"Return the", "solution": "\n    if role == QtCore.Qt.DisplayRole or role == QtCore.Qt.EditRole:\n        return JB_File(tfi).get_fullpath()"}
{"prompt": "def preserve_cwd(func: Callable) -> Callable:\n    \"\"\"\n    Decorator to preserve the current working directory in calls to the\n    decorated function.\n\n    Example:\n\n    .. code-block:: python\n\n        @preserve_cwd\n        def myfunc():\n            os.chdir(\"/faraway\")\n\n        os.chdir(\"/home\")\n        myfunc()\n        assert os.getcwd() == \"/home\"\n    \"\"\"", "output": "\n\n    @functools.wraps(func)\n    def wrapper(*args: Any, **kwargs: Any) -> Callable:\n        with chdir(os.path.abspath(os.path.expanduser(args[0])):\n            return func(*args, **kwargs)\n    return wrapper\n\n\ndef get_test_config() -> ConfigType:\n    \"\"\"\n    Return a ConfigType object with the default settings.\n    \"\"\"\n\n    return Config(\n        data_dir=os.path.join(os.path.expanduser(\"~\"), \".local\", \"share\", \"test-data\"),\n        project_dir=os.path.join(", "solution": "\n    # http://stackoverflow.com/questions/169070/python-how-do-i-write-a-decorator-that-restores-the-cwd  # noqa\n    def decorator(*args_, **kwargs) -> Any:\n        cwd = os.getcwd()\n        result = func(*args_, **kwargs)\n        os.chdir(cwd)\n        return result\n    return decorator"}
{"prompt": "def add_dos(self, label, dos):\n        \"\"\"\n        Adds a dos for plotting.\n\n        Args:\n            label:\n                label for the DOS. Must be unique.\n            dos:\n                Dos object\n        \"\"\"", "output": "\n        self.dos.append(dos)\n\n    def add_dos_line(self, label, line):\n        \"\"\"\n        Adds a single DOS line.\n\n        Args:\n            label:\n                label for the line. Must be unique.\n            line:\n                Dos line object\n        \"\"\"\n        self.dos_lines.append(self.Line(label, line))\n\n    def add_dos_label(self, label, label_type):\n        \"\"\"\n        Adds a DOS label.\n\n        Args:\n            label:\n                label for the DOS. Must be unique.\n            label_type:\n                label type for the", "solution": "\n        energies = dos.energies - dos.efermi if self.zero_at_efermi \\\n            else dos.energies\n        densities = dos.get_smeared_densities(self.sigma) if self.sigma \\\n            else dos.densities\n        efermi = dos.efermi\n        self._doses[label] = {'energies': energies, 'densities': densities,\n                              'efermi': efermi}"}
{"prompt": "def shutdown(self):\n        \"\"\"Revert stdin and stdout, close the socket.\"\"\"", "output": "\n        self.close()\n        self.sock.close()\n        self.sock = None\n        self.rfile.close()\n\n    def close(self):\n        \"\"\"Close the socket.\"\"\"\n        self.rfile = None\n        self.close_on_exec = True\n        self.close_on_exit = True\n        self.rfile = None\n        self.close_on_exec = False\n        self.rfile = None\n        self.wfile = None\n        self.wfile = None\n        self.shutdown_lock.release()\n\n    def get_request(self):\n        \"\"\"Get the request and client address from the socket.\"\"\"\n        self", "solution": "\n        sys.stdout = self.old_stdout\n        sys.stdin = self.old_stdin\n        self.skt.close()\n        self.set_continue()"}
{"prompt": "def update(self, item, **kwargs):\n        \"\"\"\n        Update the information in this instance.\n        \n        :param item: a dictionary or a Message instance \n        \"\"\"", "output": "\n        if item is None:\n            return\n        if isinstance(item, dict):\n            self.update_dict(item)\n        elif isinstance(item, Message):\n            self.update_message(item)\n        elif isinstance(item, MessageList):\n            self.update_message_list(item)\n        elif isinstance(item, MessageListList):\n            self.update_message_list_list(item)\n        elif isinstance(item, MessageListListList):\n            self.update_message_list_list(item)\n        else:\n            raise TypeError(\"Invalid type for update: %s\" % item)\n\n   ", "solution": "\n        if isinstance(item, dict):\n            self._dict.update(item)\n        elif isinstance(item, Message):\n            for key, val in item.items():\n                self._dict[key] = val\n        else:\n            raise ValueError(\"Can't update message using: '%s'\" % (item,))"}
{"prompt": "def matches(self, properties):\n        \"\"\"\n        Tests if the given criterion matches this LDAP criterion\n\n        :param properties: A dictionary of properties\n        :return: True if the properties matches this criterion, else False\n        \"\"\"", "output": "\n        return self.get_dn(properties) == self.dn\n\n    def get_dn(self, properties):\n        \"\"\"\n        Returns the DN of the LDAP entry\n\n        :param properties: A dictionary of properties\n        :return: The DN of the LDAP entry\n        \"\"\"\n        return self.ldap_dn_from_properties(properties)\n\n    def get_object_sid(self, properties):\n        \"\"\"\n        Returns the SID of the object\n\n        :param properties: A dictionary of properties\n        :return: The SID of the object\n        \"\"\"\n        return self.get_sid(properties) == self.sid\n\n    def", "solution": "\n        try:\n            # Use the comparator\n            return self.comparator(self.value, properties[self.name])\n        except KeyError:\n            # Criterion key is not in the properties\n            return False"}
{"prompt": "def keep_only_positive_boxes(boxes):\n    \"\"\"\n    Given a set of BoxList containing the `labels` field,\n    return a set of BoxList for which `labels > 0`.\n\n    Arguments:\n        boxes (list of BoxList)\n    \"\"\"", "output": "\n    return {box for box in boxes if box.num_boxes > 0 and box.num_boxes <= 1}\n\n\ndef top_k_boxes(boxes):\n    \"\"\"\n    Given a set of BoxList containing the `labels` field,\n    return a set of BoxList for which `labels < k` - 1.\n\n    Arguments:\n        boxes (list of BoxList)\n    \"\"\"\n    return {box for box in boxes if box.num_boxes > 0 and box.num_boxes > k}\n\n\ndef bottom_k_boxes(boxes):\n    \"\"\"\n    Given a set of BoxList containing", "solution": "\n    assert isinstance(boxes, (list, tuple))\n    assert isinstance(boxes[0], BoxList)\n    assert boxes[0].has_field(\"labels\")\n    positive_boxes = []\n    positive_inds = []\n    num_boxes = 0\n    for boxes_per_image in boxes:\n        labels = boxes_per_image.get_field(\"labels\")\n        inds_mask = labels > 0\n        inds = inds_mask.nonzero().squeeze(1)\n        positive_boxes.append(boxes_per_image[inds])\n        positive_inds.append(inds_mask)\n    return positive_boxes, positive_inds"}
{"prompt": "def render_reply(self):\n        \"\"\"\n        Create a :class:`~.Data` object equal to the object from which the from\n        was created through :meth:`from_xso`, except that the values of the\n        fields are exchanged with the values set on the form.\n\n        Fields which have no corresponding form descriptor are left untouched.\n        Fields which are accessible through form descriptors, but are not in\n        the original :class:`~.Data` are not included in the output.\n\n        This method only works on forms created through :meth:`from_xso`.\n\n        The resulting :class:`~.Data` instance has the :attr:`~.Data.type_` set\n        to :attr:`~.DataType.SUBMIT`.\n        \"\"\"", "output": ":`reply_to` and\n        :attr:`from_reply_to` attributes set.\n\n        :return: a new :class:`~.Data` object with the values set on the form\n        :rtype: :class:`~.Form`\n        \"\"\"\n        return Form(self.data, self.reply_to, self.from_xso, self.data_key, self.data_value)\n\n    def __str__(self):\n        \"\"\"\n        Return a string representation of the :class:`~.Data` object.\n\n        :return: a string representation of the :class:`~.Data` object\n        :rtype: :class:`~.String", "solution": "\n\n        data = copy.copy(self._recv_xso)\n        data.type_ = forms_xso.DataType.SUBMIT\n        data.fields = list(self._recv_xso.fields)\n\n        for i, field_xso in enumerate(data.fields):\n            if field_xso.var is None:\n                continue\n            if field_xso.var == \"FORM_TYPE\":\n                continue\n            key = fields.descriptor_ns, field_xso.var\n            try:\n                descriptor = self.DESCRIPTOR_MAP[key]\n            except KeyError:\n                continue\n\n            bound_field = descriptor.__get__(self, type(self))\n            data.fields[i] = bound_field.render(\n                use_local_metadata=False\n            )\n\n        return data"}
{"prompt": "def tokenize_middle_high_german_words(text):\n    \"\"\"Tokenizes MHG text\"\"\"", "output": "\n    return [word for word in re.findall(r'\\w+', text) if word not in ['M', 'M', 'M', 'M']]\n\n\ndef tokenize_middle_high_german_sents(text):\n    \"\"\"Tokenizes MHG text\"\"\"\n    return [word for word in re.findall(r'\\w+', text) if word not in ['M', 'M', 'M']]\n\n\ndef tokenize_middle_high_german_words(text):\n    \"\"\"Tokenizes MHG text\"\"\"\n    return [word for word in re.findall(r'\\w+', text", "solution": "\n\n    assert isinstance(text, str)\n    # As far as I know, hyphens were never used for compounds, so the tokenizer treats all hyphens as line-breaks\n    text = re.sub(r'-\\n',r'-', text)\n    text = re.sub(r'\\n', r' ', text)\n    text = re.sub(r'(?<=.)(?=[\\.\\\";\\,\\:\\[\\]\\(\\)!&?])',r' ', text)\n    text = re.sub(r'(?<=[\\.\\\";\\,\\:\\[\\]\\(\\)!&?])(?=.)',r' ', text)\n    text = re.sub(r'\\s+',r' ', text)\n    text = str.split(text)\n\n    return text"}
{"prompt": "def parse_cli_args():\n    \"\"\"parse args from the CLI and return a dict\"\"\"", "output": "\n    parser = argparse.ArgumentParser(description='Run a test suite.')\n    parser.add_argument('--verbose', '-v', action='store_true',\n                        help='Verbose output')\n    parser.add_argument('--debug', '-d', action='store_true',\n                        help='Debug output')\n    parser.add_argument('--no-build', '-n', action='store_true',\n                        help='Do not build the test suite')\n    parser.add_argument('--profile', '-p', default=None,\n                        help='Profile to use')\n    parser.add_argument('--with-coverage', action='", "solution": "\n    parser = argparse.ArgumentParser(description='2048 in your terminal')\n    parser.add_argument('--mode', dest='mode', type=str,\n                        default=None, help='colors mode (dark or light)')\n    parser.add_argument('--az', dest='azmode', action='store_true',\n                        help='Use the letters a-z instead of numbers')\n    parser.add_argument('--resume', dest='resume', action='store_true',\n                        help='restart the game from where you left')\n    parser.add_argument('-v', '--version', action='store_true')\n    parser.add_argument('-r', '--rules', action='store_true')\n    return vars(parser.parse_args())"}
{"prompt": "def url_join(url, path):\n    \"\"\" url version of os.path.join\n    \"\"\"", "output": "\n    return urlparse.urljoin(url, path)\n\n\ndef url_join_path(url, path):\n    \"\"\" url version of os.path.join_path\n    \"\"\"\n    return os.path.join(url, path)\n\n\ndef url_join_path_native(url, path):\n    \"\"\" url version of os.path.join_path_native\n    \"\"\"\n    return os.path.join_path_native(url, path)\n\n\ndef url_join_path_linux(url, *args):\n    \"\"\" url version of os.path.join_path_linux\n    \"\"\"", "solution": "\n    p = six.moves.urllib.parse.urlparse(url)\n\n    t = None\n    if p.path and p.path[-1] == '/':\n        if path and path[0] == '/':\n            path = path[1:]\n        t = ''.join([p.path, path])\n    else:\n        t = ('' if path and path[0] == '/' else '/').join([p.path, path])\n\n    return six.moves.urllib.parse.urlunparse(\n        p[:2]+\n        (t,)+ # os.sep is different on windows, don't use it here.\n        p[3:]\n    )"}
{"prompt": "def defaults(self):\n        \"\"\"Return the defaults, with their values interpolated (with the\n        defaults dict itself)\n\n        Mainly to support defaults using values such as %(here)s\n        \"\"\"", "output": "\n        return self.defaults\n\n    def __call__(self, value):\n        \"\"\"Return the value, with the appropriate substitutions\"\"\"\n        if value is None:\n            return ''\n        return self.defaults.get(value, value)\n\n    def __getitem__(self, key):\n        \"\"\"Return the value, with the appropriate substitutions\"\"\"\n        if key in self.defaults:\n            return self.defaults[key]\n        return None\n\n    def __setitem__(self, key, value):\n        \"\"\"Set the value, with the appropriate substitutions\"\"\"\n        if key in self.defaults:\n            self.defaults[key] = value\n        else:\n           ", "solution": "\n        defaults = ConfigParser.defaults(self).copy()\n        for key, val in iteritems(defaults):\n            defaults[key] = self.get('DEFAULT', key) or val\n        return defaults"}
{"prompt": "def speak(self, speech, play_behavior=None):\n        # type: (str, PlayBehavior) -> 'ResponseFactory'\n        \"\"\"Say the provided speech to the user.\n\n        :param speech: the output speech sent back to the user.\n        :type speech: str\n        :param play_behavior: attribute to control alexa's speech\n            interruption\n        :type play_behavior: ask_sdk_model.ui.play_behavior.PlayBehavior\n        :return: response factory with partial response being built and\n            access from self.response.\n        :rtype: ResponseFactory\n        \"\"\"", "output": "\n        return ResponseFactory(self.response, speech, play_behavior)\n\n    def speak_async(self, play_behavior=None):\n        # type: (str, PlayBehavior) -> 'ResponseFactory'\n        \"\"\"Say the provided playback to the user.\n\n        :param playback: the output speech access from the user.\n        :type playback: str\n        :return: response factory with asynchronous response being built\n            and access from self.response.\n        :rtype: ResponseFactory\n        \"\"\"\n        return ResponseFactory(self.response, playback, None)\n\n    def speak_async_async(", "solution": "\n        ssml = \"<speak>{}</speak>\".format(self.__trim_outputspeech(\n            speech_output=speech))\n        self.response.output_speech = SsmlOutputSpeech(\n            ssml=ssml, play_behavior=play_behavior)\n        return self"}
{"prompt": "def polygon_scale(polygon):\n    \"\"\"\n    For a Polygon object, return the diagonal length of the AABB.\n\n    Parameters\n    ------------\n    polygon: shapely.geometry.Polygon object\n\n    Returns\n    ------------\n    scale: float, length of AABB diagonal\n    \"\"\"", "output": "\n    return polygon.area / 2.0\n\n\ndef polygon_length(polygon):\n    \"\"\"\n    For a Polygon object, return the length of the AABB.\n\n    Parameters\n    ------------\n    polygon: shapely.geometry.Polygon object\n\n    Returns\n    ------------\n    length: float, diagonal length of the polygon\n    \"\"\"\n    return np.linalg.norm(np.cross(polygon.exterior.coords, polygon.interior.coords))\n\n\ndef get_polygon_center(polygon):\n    \"\"\"\n    For a Polygon object, return the center of the AABB.\n\n    Parameters\n    ------------\n    polygon", "solution": "\n    extents = np.reshape(polygon.bounds, (2, 2)).ptp(axis=0)\n    scale = (extents ** 2).sum() ** .5\n\n    return scale"}
{"prompt": "def check_login(func):\n    \"\"\"\u68c0\u67e5\u7528\u6237\u767b\u5f55\u72b6\u6001\n    :param func: \u9700\u8981\u88ab\u68c0\u67e5\u7684\u51fd\u6570\n    \"\"\"", "output": "\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        if not current_user.is_authenticated:\n            return redirect(url_for('login'))\n        return func(*args, **kwargs)\n    return wrapper\n\n\n@app.route('/logout')\ndef logout():\n    \"\"\"\u68c0\u67e5\u7528\u6237\u767b\u51fa\u786e\u8ba4\u72b6\u6001\n    :param current_user: \u7528\u6237\u5bf9\u8c61\n    \"\"\"\n    logout_user()\n    return redirect(url_for('index'))\n\n\n@app.route('/login', methods=['GET', 'POST'])\ndef login():", "solution": "\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        ret = func(*args, **kwargs)\n        if type(ret) == requests.Response:\n            # \u68c0\u6d4b\u7ed3\u679c\u662f\u5426\u4e3aJSON\n            if ret.content[0]!=b'{' and ret.content[0]!=b'[':\n                return ret\n            try:\n                foo = json.loads(ret.content.decode('utf-8'))\n                if 'errno' in foo and foo['errno'] == -6:\n                    logging.debug(\n                            'Offline, deleting cookies file then relogin.')\n                    path = '.{0}.cookies'.format(args[0].username)\n                    if os.path.exists(path):\n                        os.remove(path)\n                    args[0]._initiate()\n            except:\n                raise LoginFailed('User unsigned in.')\n        return ret\n\n    return wrapper"}
{"prompt": "def _get_kwarg(self, name, kwargs):\n        \"\"\"\n        Helper to get value of a named attribute irrespective of whether it is passed\n        with or without \"@\" prefix.\n        \"\"\"", "output": "\n        if name.startswith('@') and name.endswith('@'):\n            name = name[1:-1]\n        return kwargs.get(name, None)\n\n    def _get_attr_type(self, obj, name):\n        \"\"\"\n        Helper to get the type of an attribute.\n        \"\"\"\n        if obj is None:\n            return None\n        return obj.get_type_string()\n\n    def _get_attr_name(self, obj, name):\n        \"\"\"\n        Helper to get the name of an attribute.\n        \"\"\"\n        if obj is None:\n            return None\n        return obj.get_name()\n\n    def", "solution": "\n        at_name = '@{}'.format(name)\n\n        if name in kwargs:\n            if at_name in kwargs:\n                raise ValueError('Both {!r} and {!r} specified in kwargs'.format(name, at_name))\n            return kwargs[name]\n\n        if at_name in kwargs:\n            return kwargs[at_name]\n\n        return not_set"}
{"prompt": "def iter_variants(self):\n        \"\"\"Iterate over the variants within this package, in index order.\n\n        Returns:\n            `Variant` iterator.\n        \"\"\"", "output": "\n        for variant in self.variants:\n            yield variant\n\n    def iter_variants_by_id(self, id):\n        \"\"\"Iterate over the variants within this package, in order.\n\n        Args:\n            id (str): Package id.\n\n        Returns:\n            `Variant` iterator.\n        \"\"\"\n        for variant in self.iter_variants():\n            if id == variant.id:\n                yield variant\n\n    def iter_variants_by_name(self, name):\n        \"\"\"Iterate over the variants within this package, in name order.\n\n        Args:\n            name (str): Package name.\n\n        Returns:\n            `Variant", "solution": "\n        for variant in self.repository.iter_variants(self.resource):\n            yield Variant(variant, context=self.context, parent=self)"}
{"prompt": "def url_view(url_pattern, name=None, priority=None):\n    \"\"\"\n    Decorator for registering functional views.\n    Meta decorator syntax has to be used in order to accept arguments.\n\n    This decorator does not really do anything that magical:\n\n    This:\n    >>> from urljects import U, url_view\n    >>> @url_view(U / 'my_view')\n    ... def my_view(request)\n    ...     pass\n\n    is equivalent to this:\n    >>> def my_view(request)\n    ...     pass\n    >>> my_view.urljects_view = True\n    >>> my_view.url = U / 'my_view'\n    >>> my_view.url_name = 'my_view'\n\n    Those view are then supposed to be used with ``view_include`` which will\n    register all views that have ``urljects_view`` set to ``True``.\n\n    :param url_pattern: regex or URLPattern or anything passable to url()\n    :param name: name of the view, __name__ will be used otherwise.\n    :param priority: priority of the view, the lower the better\n    \"\"\"", "output": " >>> my_view.url_pattern ='my_view'\n    >>> my_view.priority ='my_view'\n    >>> my_view.url_view(url_pattern='http://example.com/my_view')\n   ... def my_view(url, request):\n   ...     return request.response.body\n\n    \"\"\"\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            if url_pattern is not None:\n                return url_pattern.format(*args, **kwargs)\n            if priority is not None:\n                return priority", "solution": "\n\n    def meta_wrapper(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            return func(*args, **kwargs)\n\n        wrapper.urljects_view = True\n        wrapper.url = url_pattern\n        wrapper.url_name = name or func.__name__\n        wrapper.url_priority = priority\n\n        return wrapper\n    return meta_wrapper"}
{"prompt": "def _get_tasks_by_queue(self):\n        \"\"\"Return the tasks for this Context, grouped by queue.\"\"\"", "output": "\n        return self.tasks.values()\n\n    def get_task(self, task_id):\n        \"\"\"Return the task with the given task_id.\"\"\"\n        if task_id not in self._get_tasks_by_id:\n            raise ValueError(\"No task with id %s\" % task_id)\n        return self._get_tasks_by_id[task_id][0]\n\n    def get_task_by_id(self, task_id):\n        \"\"\"Return the task with the given task_id.\"\"\"\n        if task_id not in self._get_tasks_by_id:", "solution": "\n        task_map = {}\n        _checker = None\n\n        # Ask the persistence engine for an Async to use for checking if the\n        # context is complete.\n        if self._persistence_engine:\n            _checker = self._persistence_engine.context_completion_checker\n\n        for async in self._tasks:\n            queue = async.get_queue()\n            if _checker:\n                async.update_options(_context_checker=_checker)\n\n            task = async.to_task()\n            task_map.setdefault(queue, []).append(task)\n\n        return task_map"}
{"prompt": "async def fromURL(\n            cls, url, *, credentials=None, insecure=False):\n        \"\"\"Return a `SessionAPI` for a given MAAS instance.\"\"\"", "output": "\n        return cls(\n            self._loop,\n            loop=self._loop,\n            timeout=self._timeout,\n            ssl=self._ssl,\n            auth=self._auth,\n            verify=self._verify,\n            caCerts=self._ca_certs,\n            loop=self._loop,\n            sslContext=self._ssl_context,\n            url=url,\n            credentials=credentials,\n            insecure=insecure)\n\n    @asyncio.coroutine\n    def asyncGet(self, request, timeout=None):\n        \"\"\"Return a `SessionAPI` for a given MAAS instance.\"\"\"\n        if timeout is None:\n            request", "solution": "\n        try:\n            description = await helpers.fetch_api_description(\n                url, insecure=insecure)\n        except helpers.RemoteError as error:\n            # For now just re-raise as SessionError.\n            raise SessionError(str(error))\n        else:\n            session = cls(description, credentials)\n            session.insecure = insecure\n            return session"}
{"prompt": "def _ParseContinue(s):\n  \"\"\"Parses a Unicode continuation field.\n\n  These are of the form '<Name, First>' or '<Name, Last>'.\n  Instead of giving an explicit range in a single table entry,\n  some Unicode tables use two entries, one for the first\n  code value in the range and one for the last.\n  The first entry's description is '<Name, First>' instead of 'Name'\n  and the second is '<Name, Last>'.\n\n    '<Name, First>' => ('Name', 'First')\n    '<Name, Last>' => ('Name', 'Last')\n    'Anything else' => ('Anything else', None)\n\n  Args:\n    s: continuation field string\n\n  Returns:\n    pair: name and ('First', 'Last', or None)\n  \"\"\"", "output": " => ('Name', 'Anything else')\n\n  Args:\n    s: The Unicode string to parse.\n\n  Returns:\n    The parsed Unicode string.\n  \"\"\"\n  if s.startswith(u'\\u0000'):\n    return s[2:]\n  else:\n    return s\n\n\ndef _ParseControlChar(s):\n  \"\"\"Parses a control character field.\n\n  Args:\n    s: The control character to parse.\n\n  Returns:\n    The parsed control character.\n  \"\"\"\n  if s.startswith(u'\\u0000'):\n    return s[2:]\n  else:\n    return s\n\n\ndef _ParseControlWord(s", "solution": "\n\n  match = re.match(\"<(.*), (First|Last)>\", s)\n  if match is not None:\n    return match.groups()\n  return (s, None)"}
{"prompt": "def make_record(level, xref_id, tag, value, sub_records, offset, dialect,\n                parser=None):\n    \"\"\"Create Record instance based on parameters.\n\n    :param int level: Record level number.\n    :param str xref_id: Record reference ID, possibly empty.\n    :param str tag: Tag name.\n    :param value: Record value, possibly empty. Value can be None, bytes, or\n        string object, if it is bytes then it should be decoded into strings\n        before calling freeze(), this is normally done by the parser which\n        knows about encodings.\n    :param list sub_records: Initial list of subordinate records,\n        possibly empty. List can be updated later.\n    :param int offset: Record location in a file.\n    :param dialect: One of DIALECT_* constants.\n    :param parser: Instance of `GedcomReader` class, only needed for\n        records whose walue is a pointer.\n    :return: Instance of :py:class:`Record` (or one of its subclasses).\n    \"\"\"", "output": " str sub_records: Sub-records, possibly empty.\n        Sub-records should be a sequence of bytes.\n    :param int offset: Offset of record in bytes.\n    :param dialect: The dialect to use to interpret record values.\n    :param str parser: The parser to use to interpret record values.\n    :return: The Record instance.\n    :rtype: :class:`Record`\n    \"\"\"\n    if parser is None:\n        parser = RecordParser()\n    return parser.make_record(xref_id, tag, value, sub_records, offset)\n\n\ndef decode_record(xref_id,", "solution": "\n    # value can be bytes or string so we check for both, 64 is code for '@'\n    if value and len(value) > 2 and \\\n        ((value[0] == '@' and value[-1] == '@') or\n         (value[0] == 64 and value[-1] == 64)):\n        # this looks like a <pointer>, make a Pointer record\n        klass = Pointer\n        rec = klass(parser)\n    else:\n        klass = _tag_class.get(tag, Record)\n        rec = klass()\n\n    rec.level = level\n    rec.xref_id = xref_id\n    rec.tag = tag\n    rec.value = value\n    rec.sub_records = sub_records\n    rec.offset = offset\n    rec.dialect = dialect\n    return rec"}
{"prompt": "def _decdeg_distance(pt1, pt2):\n    \"\"\"\n    Earth surface distance (in km) between decimal latlong points using\n    Haversine approximation.\n\n    http://stackoverflow.com/questions/15736995/\n    how-can-i-quickly-estimate-the-distance-between-two-latitude-longitude-\n    points\n    \"\"\"", "output": "\n    lat1 = pt1[0]\n    lat2 = pt2[0]\n    lat3 = pt1[1]\n    lat4 = pt2[1]\n    lat5 = pt1[2]\n    lat6 = pt1[3]\n    lat7 = pt2[3]\n    lat8 = pt2[4]\n    lat9 = pt3[0]\n    lat10 = pt3[1]\n    lat11 = pt3[2]\n    lat12 = pt3[3]\n    lat13 = pt3[4]\n    lat14 = pt4[0]\n    lat", "solution": "\n\n    lat1, lon1 = pt1\n    lat2, lon2 = pt2\n\n    # Convert decimal degrees to radians\n    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n\n    # haversine formula\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n    c = 2 * np.arcsin(np.sqrt(a))\n    km = 6367 * c\n\n    return km"}
{"prompt": "def setup_top(self):\n        \"\"\"Create top-level elements of the hybrid schema.\"\"\"", "output": "\n        self.top = self.create_element('top')\n        self.top.set('name', 'Top')\n        self.top.set('type', 'object')\n        self.top.append(self.create_element('object', {'name': 'foo'}))\n        self.top.append(self.create_element('object', {'name': 'bar'}))\n        self.top.append(self.create_element('object', {'name': 'baz'})\n        self.top.append(self.create_element('object', {'name': 'quux'})\n        self.top.append(", "solution": "\n        self.top_grammar = SchemaNode(\"grammar\")\n        self.top_grammar.attr = {\n            \"xmlns\": \"http://relaxng.org/ns/structure/1.0\",\n            \"datatypeLibrary\": \"http://www.w3.org/2001/XMLSchema-datatypes\"}\n        self.tree = SchemaNode(\"start\")"}
{"prompt": "def partitionBy(self, numPartitions, partitionFunc=portable_hash):\n        \"\"\"\n        Return a copy of the DStream in which each RDD are partitioned\n        using the specified partitioner.\n        \"\"\"", "output": "\n        return self.map(lambda x: x.partitioner).filter(lambda x: x > numPartitions).collect()\n\n    def foreachPartition(self, func):\n        \"\"\"\n        Apply a function to each RDD in this group and return a result that\n        contains all of the elements in each RDD resulting from applying the function\n        to each RDD.\n\n        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n        >>> def f(iterator):\n       ...     for x in iterator:\n       ...         yield x\n        >>> x.foreachPartition(f)\n        Traceback (most recent call last):\n           ...", "solution": "\n        return self.transform(lambda rdd: rdd.partitionBy(numPartitions, partitionFunc))"}
{"prompt": "def _ModifiedDecoder(wire_type, decode_value, modify_value):\n  \"\"\"Like SimpleDecoder but additionally invokes modify_value on every value\n  before storing it.  Usually modify_value is ZigZagDecode.\n  \"\"\"", "output": "\n\n  def InnerDecode(buffer, pos):\n    \"\"\"Inner decode function that adds the new line to the buffer, and\n    returns the new line number.\n    \"\"\"\n\n    return wire_type * len(modify_value) + pos\n\n  return InnerDecode\n\n\ndef _ModifiedEncoder(wire_type, modify_value, modify_state):\n  \"\"\"Like _ModifiedDecoder but additionally invokes modify_state on every\n  encoder line that matches the modify_value.  Usually modify_value is ZigZagEncode.\n  \"\"\"\n\n  if wire_type == wire_format.WIRETYPE_VARINT:\n    modify_value", "solution": "\n\n  # Reusing _SimpleDecoder is slightly slower than copying a bunch of code, but\n  # not enough to make a significant difference.\n\n  def InnerDecode(buffer, pos):\n    (result, new_pos) = decode_value(buffer, pos)\n    return (modify_value(result), new_pos)\n  return _SimpleDecoder(wire_type, InnerDecode)"}
{"prompt": "def _create_server(host, port):\n    \"\"\"\n    Helper function.  Creates a listening socket on the designated\n    host and port.  Modeled on the socket.create_connection()\n    function.\n    \"\"\"", "output": "\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    s.setblocking(0)\n    s.connect((host, port))\n    return s\n\n\ndef _create_client(host, port):\n    \"\"\"\n    Helper function.  Creates a client socket on the designated\n    host and port.  Modeled on the socket.create_connection()\n    function.\n    \"\"\"\n    c = _create_connection(host, port)\n    return c\n\n\ndef _create_connection(host, port):\n    \"\"\"\n    Helper function.  Creates a new socket connection to the", "solution": "\n\n    exc = socket.error(\"getaddrinfo returns an empty list\")\n    for res in socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM):\n        af, socktype, proto, canonname, sa = res\n        sock = None\n        try:\n            # Create the listening socket\n            sock = socket.socket(af, socktype, proto)\n            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n            sock.bind(sa)\n            sock.listen(1024)\n            return sock\n\n        except socket.error as exc:\n            # Clean up after ourselves\n            if sock is not None:\n                sock.close()\n\n    # Couldn't create a listening socket\n    raise exc"}
{"prompt": "def injections_from_cli(opts):\n    \"\"\"Gets injection parameters from the inference file(s).\n\n    Parameters\n    ----------\n    opts : argparser\n        Argparser object that has the command-line objects to parse.\n\n    Returns\n    -------\n    FieldArray\n        Array of the injection parameters from all of the input files given\n        by ``opts.input_file``.\n    \"\"\"", "output": "\n    parser = ArgumentParser(prog='mx classify',\n                            description='Evaluate a trained model.')\n    parser.add_argument('--model', '-m', default='imagenet',\n                        help='Model to use. Options are:'+\n                        'imagenet, imagenet-mlp, imagenet-cnn, imagenet-nn, imagenet-mlp, imagenet-cnn, imagenet-nn, imagenet-mlp, imagenet-cnn, imagenet-nn, imagenet-mlp, imagenet-cnn,", "solution": "\n    input_files = opts.input_file\n    if isinstance(input_files, str):\n        input_files = [input_files]\n    injections = None\n    # loop over all input files getting the injection files\n    for input_file in input_files:\n        fp = loadfile(input_file, 'r')\n        these_injs = fp.read_injections()\n        if injections is None:\n            injections = these_injs\n        else:\n            injections = injections.append(these_injs)\n    return injections"}
{"prompt": "def _bigger_than_zero(value: str) -> int:\n    \"\"\"Type evaluator for argparse.\"\"\"", "output": "\n    return int(value) >= 0\n\n\ndef _parse_args() -> argparse.Namespace:\n    \"\"\"Parse command line arguments.\"\"\"\n    parser = argparse.ArgumentParser(\n        description='A tool to generate a random password.')\n    parser.add_argument(\n        '--output', '-o',\n        help='The output file path.',\n        required=True,\n        type=_bigger_than_zero,\n        default=None)\n    return parser.parse_args()\n\n\ndef _parse_args_and_config() -> argparse.Namespace:\n    \"\"\"Parse command line and config file options.\"\"\"\n    args = _parse", "solution": "\n    ivalue = int(value)\n    if ivalue < 0:\n        raise ArgumentTypeError(\n            '{} should be bigger than 0'.format(ivalue)\n        )\n    return ivalue"}
{"prompt": "def _render_item(self, dstack, key, value = None, **settings):\n        \"\"\"\n        Format single tree line.\n        \"\"\"", "output": "\n        if value is None:\n            value = self.get_default(key, self.tree_default)\n        if value is None:\n            value = self.get_default(key, self.tree_default)\n        if value is None:\n            value = self.get_default(key, self.tree_default)\n        if value is None:\n            value = self.get_default(key, self.tree_default)\n        if value is None:\n            value = self.get_default(key, self.tree_default)\n        if value is None:\n            value = self.get_", "solution": "\n        cur_depth = len(dstack) - 1\n\n        treeptrn = ''\n        s = self._es_text(settings, settings[self.SETTING_TREE_FORMATING])\n        for ds in dstack:\n            treeptrn += ' ' + self.fmt_text(self.tchar(settings[self.SETTING_TREE_STYLE], cur_depth, *ds), **s) + ''\n\n        strptrn = \"{}\"\n        if value is not None:\n            strptrn += \": {}\"\n        s = self._es_text(settings, settings[self.SETTING_TEXT_FORMATING])\n        strptrn = self.fmt_text(strptrn.format(key, value), **s)\n\n        return '{} {}'.format(treeptrn, strptrn)"}
{"prompt": "def run(self):\n        \"\"\"Run parent install, and then save the man file.\"\"\"", "output": "\n        self.save_man_file()\n        self.install_man_file()\n\n    def save_man_file(self):\n        \"\"\"Save the man file.\"\"\"\n        man_path = os.path.join(self.install_dir,'share','man','man1')\n        man_file = os.path.join(man_path,'man1', '1.0','man1')\n        with open(man_file, 'w') as man_file:\n            man_file.write(self.man_file_content)\n\n    def install_man_file", "solution": "\n        install.run(self)\n\n        # man directory\n        if self._custom_man_dir is not None:\n            if not os.path.exists(self._custom_man_dir):\n                os.makedirs(self._custom_man_dir)\n            shutil.copy(\"man/fades.1\", self._custom_man_dir)"}
{"prompt": "def trainGP(self,fast=False,scales0=None,fixed0=None,lambd=None):\n        \"\"\"\n        Train the gp\n       \n        Args:\n            fast:       if true and the gp has not been initialized, initializes a kronSum gp\n            scales0:\tinitial variance components params\n            fixed0:     initial fixed effect params\n        \"\"\"", "output": "\n        if scales0 is None:\n            if fast:\n                self.gp = kronSumGP(self.data)\n            else:\n                self.gp = kronSumGP(self.data)\n        else:\n            self.gp.setScales(scales0)\n            self.gp.setFixed(fixed0)\n            self.gp.setLambdas(lambd)\n            self.data = self.data + self.data.mean()\n\n    def testGP(self,fast=False,scales0=None,fixed0=None,test=False):\n        \"\"\"\n        Test the", "solution": "\n        assert self.n_terms>0, 'CVarianceDecomposition:: No variance component terms'\n\n        if not self.init:\t\tself.initGP(fast=fast)\n\n        # set lambda\n        if lambd!=None:\t\tself.gp.setLambda(lambd)\n\n        # set scales0\n        if scales0!=None:\n            self.setScales(scales0)\n        # init gp params\n        self.vd.initGPparams()\n        # set fixed0\n        if fixed0!=None:\n            params = self.gp.getParams()\n            params['dataTerm'] = fixed0\n            self.gp.setParams(params)\n\n        # LIMIX CVARIANCEDECOMPOSITION TRAINING\n        conv =self.vd.trainGP()\n        \n        self.cache['Sigma']   = None\n        self.cache['Hessian'] = None\n            \n        return conv"}
{"prompt": "def get_relationships_for_destination(self, destination_id=None):\n        \"\"\"Gets a ``RelationshipList`` corresponding to the given peer ``Id``.\n\n        arg:    destination_id (osid.id.Id): a peer ``Id``\n        return: (osid.relationship.RelationshipList) - the relationships\n        raise:  NullArgument - ``destination_id`` is ``null``\n        raise:  OperationFailed - unable to complete request\n        raise:  PermissionDenied - authorization failure\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"", "output": "\n        return  # osid.relationship.RelationshipList\n\n    @utilities.arguments_not_none\n    def get_relationships_for_destination_on_date(self, source_id=None, destination_id=None, date=None):\n        \"\"\"Gets a ``RelationshipList`` corresponding to the given peer ``Id``\n        and effective during the entire given date range inclusive but not\n        limited to, or in the future.\n\n        arg:    source_id (osid.id.Id): a peer ``Id``\n        arg:    destination_id (osid.id.Id): a peer ``Id", "solution": "\n        if destination_id is None:\n            raise NullArgument()\n        url_path = ('/handcar/services/relationship/families/' +\n                    self._catalog_idstr + '/relationships?sourceid=' +\n                    str(destination_id))\n        return objects.RelationshipList(self._get_request(url_path))"}
{"prompt": "def cli(obj, role, scopes, delete):\n    \"\"\"Add or delete role-to-permission lookup entry.\"\"\"", "output": "\n    if not obj.has_permission(role,'read', 'write'):\n        raise PermissionDenied()\n    if not obj.has_permission(role, 'write','manage'):\n        raise PermissionDenied()\n    if delete:\n        obj.delete()\n    else:\n        for scope in scopes:\n            if scope =='read':\n                continue\n            obj.permission(role, 'write', scope)\n        for scope in scopes:\n            if scope == 'write':\n                continue\n            obj.permission(role,'manage', scope)\n\n\ndef _get_user_permissions(user):\n    \"\"\"Return a list of permissions for", "solution": "\n    client = obj['client']\n    if delete:\n        client.delete_perm(delete)\n    else:\n        if not role:\n            raise click.UsageError('Missing option \"--role\".')\n        if not scopes:\n            raise click.UsageError('Missing option \"--scope\".')\n        try:\n            perm = client.create_perm(role, scopes)\n        except Exception as e:\n            click.echo('ERROR: {}'.format(e))\n            sys.exit(1)\n        click.echo(perm.id)"}
{"prompt": "def import_(module, objects=None, via=None):\n    \"\"\"\n    :param module: py3 compatiable module path\n    :param objects: objects want to imported, it should be a list\n    :param via: for some py2 module, you should give the import path according the\n        objects which you want to imported\n    :return: object or module\n    \"\"\"", "output": "\n    if objects is None:\n        objects = []\n    if via is None:\n        via = {}\n    if isinstance(module, str):\n        module = import_(module)\n    if isinstance(module, types.ModuleType):\n        module = import_(module.__name__)\n    if isinstance(module, str):\n        module = import_(module)\n    if isinstance(module, types.ModuleType):\n        module = import_(module.__name__)\n    if isinstance(module, types.ModuleType):\n        module = import_(module)\n    if isinstance(module, types.ModuleType):\n        module = import_(module)\n    if isinstance(module,", "solution": "\n    if PY3:\n        mod = __import__(module, fromlist=['*'])\n    else:\n        path = modules_mapping.get(module)\n        if not path:\n            raise Exception(\"Can't find the module %s in mappings.\" % module)\n        if isinstance(path, list):\n            if not via:\n                raise Exception(\"You should give a via parameter to enable import from py2.\")\n            path = via\n        mod = __import__(path, fromlist=['*'])\n\n    if objects:\n        if not isinstance(objects, (list, tuple)):\n            raise Exception(\"objects parameter should be a list or tuple.\")\n        r = []\n        for x in objects:\n            r.append(getattr(mod, x))\n        if len(r) > 1:\n            return tuple(r)\n        else:\n            return r[0]\n    else:\n        return mod"}
{"prompt": "def conditional_http_tween_factory(handler, registry):\n    \"\"\"\n    Tween that adds ETag headers and tells Pyramid to enable \n    conditional responses where appropriate.\n    \"\"\"", "output": "\n    def conditional_http_tween(request):\n        if request.method == 'POST':\n            return handler(request, **request.form)\n        else:\n            return handler(request, **request.args)\n\n    return registry.add_route(\n        'tween', '/tween', handler=request_tween_factory,\n        methods=['GET', 'HEAD', 'POST'])\n\ndef conditional_http_tween_factory_with_headers(handler, registry):\n    \"\"\"\n    Tween that adds ETag headers and tells Pyramid to enable \n    conditional responses where appropriate.\n    \"\"\"\n    def", "solution": "\n    settings = registry.settings if hasattr(registry, 'settings') else {}\n    not_cacheble_list = []\n    if 'not.cachable.list' in settings:\n        not_cacheble_list = settings.get('not.cachable.list').split()\n\n    def conditional_http_tween(request):\n        response = handler(request)\n\n        if request.path not in not_cacheble_list:\n\n            # If the Last-Modified header has been set, we want to enable the\n            # conditional response processing.\n            if response.last_modified is not None:\n                response.conditional_response = True\n\n            # We want to only enable the conditional machinery if either we\n            # were given an explicit ETag header by the view or we have a\n            # buffered response and can generate the ETag header ourself.\n            if response.etag is not None:\n                response.conditional_response = True\n            elif (isinstance(response.app_iter, Sequence) and\n                          len(response.app_iter) == 1) and response.body is not None:\n                response.conditional_response = True\n                response.md5_etag()\n\n        return response\n\n    return conditional_http_tween"}
{"prompt": "def submit_the_only_form(self):\n    \"\"\"\n    Look for a form on the page and submit it.\n\n    Asserts if more than one form exists.\n    \"\"\"", "output": "\n    for form in self.doc.xpath('//form'):\n      if len(form.xpath('input', {'name': 'action', 'type':'submit'})) > 1:\n        raise AssertionError('More than one form exists.')\n      if len(form.xpath('input', {'name':'submit'})) == 1:\n        return form.submit()\n    raise AssertionError('No form found.')\n\n  def get_form_by_id(self, id):\n    \"\"\"\n    Look for a form on the page and return it.\n\n    Args:\n      id: The id of the form to look for.\n\n    Returns:", "solution": "\n    form = ElementSelector(world.browser, str('//form'))\n    assert form, \"Cannot find a form on the page.\"\n    form.submit()"}
{"prompt": "def restart(name, timeout=10):\n    \"\"\"\n    Restarts a container\n\n    name\n        Container name or ID\n\n    timeout : 10\n        Timeout in seconds after which the container will be killed (if it has\n        not yet gracefully shut down)\n\n\n    **RETURN DATA**\n\n    A dictionary will be returned, containing the following keys:\n\n    - ``status`` - A dictionary showing the prior state of the container as\n      well as the new state\n    - ``result`` - A boolean noting whether or not the action was successful\n    - ``restarted`` - If restart was successful, this key will be present and\n      will be set to ``True``.\n\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt myminion docker.restart mycontainer\n        salt myminion docker.restart mycontainer timeout=20\n    \"\"\"", "output": " to False.\n    \"\"\"\n    return _restart(name, timeout)\n\n\ndef stop(name, timeout=10):\n    \"\"\"\n    Stops a container\n\n    name\n        Container name or ID\n\n    timeout : 10\n        Timeout in seconds after which the container will be killed (if it has\n        not yet gracefully shut down)\n\n\n    **RETURN DATA**\n\n    A dictionary will be returned, containing the following keys:\n\n    - ``status`` - A dictionary showing the prior state of the container as\n      well as the new state\n    - ``result`` - A boolean noting whether or not the action was successful\n    - ``restarted``", "solution": "\n    ret = _change_state(name, 'restart', 'running', timeout=timeout)\n    if ret['result']:\n        ret['restarted'] = True\n    return ret"}
{"prompt": "def find(self, obj):\n        \"\"\"Returns the index of the given object in the queue, it might be string\n        which will be searched inside each task.\n\n        :arg obj: object we are looking\n\n        :return: -1 if the object is not found or else the location of the task\n        \"\"\"", "output": "\n        if isinstance(obj, str):\n            return self.index(obj)\n        elif isinstance(obj, int):\n            return self.location(obj)\n        else:\n            raise TypeError(\"Can't find object of type %s\" % type(obj))\n\n    def __iter__(self):\n        \"\"\"Returns an iterator over the tasks in the queue.\n\n        :return: An iterator over the tasks in the queue\n        \"\"\"\n        return self.tasks.__iter__()\n\n    def __len__(self):\n        \"\"\"Returns the number of tasks in the queue.\n\n        :return: -1 if the queue is empty\n        \"\"\"\n        return self", "solution": "\n        if not self.connected:\n            raise ConnectionError('Queue is not connected')\n\n        data = self.rdb.lrange(self._name, 0, -1)\n        for i, datum in enumerate(data):\n            if datum.find(str(obj)) != -1:\n                return i\n        return -1"}
{"prompt": "def marginBuy(self, currencyPair, rate, amount, lendingRate=None):\n        \"\"\"Places a margin buy order in a given market. Required POST\n        parameters are \"currencyPair\", \"rate\", and \"amount\". You may optionally\n         specify a maximum lending rate using the \"lendingRate\" parameter.\n         If successful, the method will return the order number and any trades\n         immediately resulting from your order.\"\"\"", "output": "\n        if lendingRate is not None:\n            if lendingRate < 0:\n                raise ArgumentsRequired(self, \"lendingRate must be non-negative\")\n            if lendingRate > len(currencyPair):\n                raise ArgumentsRequired(self, \"lendingRate must be less than lendingPair\")\n            if lendingRate < len(rate):\n                raise ArgumentsRequired(self, \"lendingRate must be less than lendingPair\")\n            if lendingRate > len(currencyPair) + 1:\n                raise ArgumentsRequired(self, \"lendingPair must be less than lendingPair\")\n            if", "solution": "\n        return self._private('marginBuy', currencyPair=currencyPair, rate=rate,\n                             amount=amount, lendingRate=lendingRate)"}
{"prompt": "def _build_filename_from_browserstack_json(j):\n    \"\"\" Build a useful filename for an image from the screenshot json metadata \"\"\"", "output": "\n    return \"%s-%s.png\" % (j[\"name\"], j[\"id\"])\n\n\ndef _build_filename_from_json(j):\n    \"\"\" Build a useful filename for an image from the screenshot json metadata \"\"\"\n    return \"%s-%s.json\" % (j[\"name\"], j[\"id\"])\n\n\ndef _build_filename_from_local_json(j):\n    \"\"\" Build a useful filename for an image from the screenshot local json metadata \"\"\"\n    return \"%s-%s.png\" % (j[\"name\"], j[\"id\"])\n\n\ndef _build_filename_from_", "solution": "\n    filename = ''\n    device = j['device'] if j['device'] else 'Desktop'\n    if j['state'] == 'done' and j['image_url']:\n        detail = [device, j['os'], j['os_version'],\n                  j['browser'], j['browser_version'], '.jpg']\n        filename = '_'.join(item.replace(\" \", \"_\") for item in detail if item)\n    else:\n        print 'screenshot timed out, ignoring this result'\n    return filename"}
{"prompt": "def transformer_parsing_base():\n  \"\"\"HParams for parsing on WSJ only.\"\"\"", "output": "\n  hparams = common_hparams.HParams(\n      data_dir=os.path.join(hparams.data_dir, 'transformer_data'),\n      train_data_pattern=r'train_data/(\\d+).txt',\n      test_data_pattern=r'test_data/(\\d+).txt',\n      num_examples=1,\n      num_classes=2,\n      num_features=3,\n      num_classes_per_layer=2,\n      num_filters=32,\n      filter_size=3,\n      num_channels=4,\n      num", "solution": "\n  hparams = transformer_base()\n  hparams.attention_dropout = 0.2\n  hparams.layer_prepostprocess_dropout = 0.2\n  hparams.max_length = 512\n  hparams.learning_rate_warmup_steps = 16000\n  hparams.hidden_size = 1024\n  hparams.learning_rate = 0.05\n  hparams.shared_embedding_and_softmax_weights = False\n  return hparams"}
{"prompt": "def parse(self, filename):\n        \"\"\"\n        Parses a file into an AstromData structure.\n\n        Args:\n          filename: str\n            The name of the file whose contents will be parsed.\n\n        Returns:\n          data: AstromData\n            The file contents extracted into a data structure for programmatic\n            access.\n        \"\"\"", "output": "\n        with open(filename, 'r') as f:\n            contents = f.read()\n\n        return self._parse_data(contents, self._program_name_to_data_type)\n\n    def _parse_data(self, contents, program_name_to_data_type):\n      \"\"\"\n      Parses a string into a list of programmatic data structures.\n\n      Args:\n        contents: str\n            The contents of the file.\n        program_name_to_data_type: dict\n            A dictionary mapping programmatic data types to the data structure\n            that will be parsed.\n\n      Returns:\n        data: list", "solution": "\n        filehandle = storage.open_vos_or_local(filename, \"rb\")\n        assert filehandle is not None, \"Failed to open file {} \".format(filename)\n        filestr = filehandle.read()\n        filehandle.close()\n\n        assert filestr is not None, \"File contents are None\"\n\n        observations = self._parse_observation_list(filestr)\n\n        self._parse_observation_headers(filestr, observations)\n\n        sys_header = self._parse_system_header(filestr)\n\n        sources = self._parse_source_data(filestr, observations)\n\n        return AstromData(observations, sys_header, sources, discovery_only=self.discovery_only)"}
{"prompt": "def remove_tag(self, key, value=None):\n        \"\"\"\n        Remove a tag from this object.  Removing a tag involves a round-trip\n        to the EC2 service.\n\n        :type key: str\n        :param key: The key or name of the tag being stored.\n\n        :type value: str\n        :param value: An optional value that can be stored with the tag.\n                      If a value is provided, it must match the value\n                      currently stored in EC2.  If not, the tag will not\n                      be removed.  If a value of None is provided, all\n                      tags with the specified name will be deleted.\n                      NOTE: There is an important distinction between\n                      a value of '' and a value of None.\n        \"\"\"", "output": " key will be removed.\n        \"\"\"\n        params = {'Action': 'RemoveTags',\n                  'Key': key,\n                  'Value': value}\n        return self.get_status('RemoveTags', params)\n\n    # Tag methods\n\n    def get_all_tags(self, filters=None):\n        \"\"\"\n        Retrieve all the tags associated with the current instance.\n\n        :type filters: dict\n        :param filters: Optional filters that can be used to limit\n                        the results returned.  Filters are provided\n                        in the form of a dictionary consisting of\n                        filter names as the key and filter values\n                        as the value.  The set of", "solution": "\n        if value:\n            tags = {key : value}\n        else:\n            tags = [key]\n        status = self.connection.delete_tags([self.id], tags)\n        if key in self.tags:\n            del self.tags[key]"}
{"prompt": "def as_tuple(self, value):\n        \"\"\"Utility function which converts lists to tuples.\"\"\"", "output": "\n        if isinstance(value, list):\n            return tuple(as_tuple(self, v) for v in value)\n        else:\n            return value\n\n    def __init__(self, value):\n        \"\"\"Constructor.\"\"\"\n        self.value = value\n\n    def __repr__(self):\n        \"\"\"Return the representation.\"\"\"\n        return repr(self.value)\n\n    def __str__(self):\n        \"\"\"Return the string representation.\"\"\"\n        return str(self.value)\n\n    def __eq__(self, other):\n        \"\"\"Return whether other is equal to self.\"\"\"\n        return self.value == other.value\n\n    def __ne__(self,", "solution": "\n        if isinstance(value, list):\n            value = tuple(value)\n        return value"}
{"prompt": "def movable_items(self):\n        \"\"\"Filter selection\n\n        Filter items of selection that cannot be moved (i.e. are not instances of `Item`) and return the rest.\n        \"\"\"", "output": "\n        return self.filter_items(self.selected_items)\n\n    def filter_items(self, items):\n        \"\"\"Filter items\n\n        Filter items of selection that cannot be moved (i.e. are not instances of `Item`) and return the rest.\n        \"\"\"\n        if not items:\n            return items\n        if len(items) == 1:\n            return items[0]\n        if len(items) > 2:\n            return items\n        if len(items) == 0:\n            return items[0]\n        if len(items) == 2:\n            return items[1]\n        if len(items) >", "solution": "\n        view = self.view\n\n        if self._move_name_v:\n            yield InMotion(self._item, view)\n        else:\n            selected_items = set(view.selected_items)\n            for item in selected_items:\n                if not isinstance(item, Item):\n                    continue\n                yield InMotion(item, view)"}
{"prompt": "def devices(self, timeout=None):\n        \"\"\"Executes adb devices -l and returns a list of objects describing attached devices.\n\n        :param timeout: optional integer specifying the maximum time in\n            seconds for any spawned adb process to complete before\n            throwing an ADBTimeoutError.  This timeout is per adb call. The\n            total time spent may exceed this value. If it is not\n            specified, the value set in the ADBHost constructor is used.\n        :returns: an object contain\n        :raises: * ADBTimeoutError\n                 * ADBError\n\n        The output of adb devices -l ::\n\n            $ adb devices -l\n            List of devices attached\n            b313b945               device usb:1-7 product:d2vzw model:SCH_I535 device:d2vzw\n\n        is parsed and placed into an object as in\n\n        [{'device_serial': 'b313b945', 'state': 'device', 'product': 'd2vzw',\n          'usb': '1-7', 'device': 'd2vzw', 'model': 'SCH_I535' }]\n\n        \"\"\"", "output": " devices -l\n            0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0", "solution": "\n        # b313b945               device usb:1-7 product:d2vzw model:SCH_I535 device:d2vzw\n        # from Android system/core/adb/transport.c statename()\n        re_device_info = re.compile(r'([^\\s]+)\\s+(offline|bootloader|device|host|recovery|sideload|no permissions|unauthorized|unknown)')\n        devices = []\n        lines = self.command_output([\"devices\", \"-l\"], timeout=timeout).split('\\n')\n        for line in lines:\n            if line == 'List of devices attached ':\n                continue\n            match = re_device_info.match(line)\n            if match:\n                device = {\n                    'device_serial': match.group(1),\n                    'state': match.group(2)\n                }\n                remainder = line[match.end(2):].strip()\n                if remainder:\n                    try:\n                        device.update(dict([j.split(':')\n                                            for j in remainder.split(' ')]))\n                    except ValueError:\n                        self._logger.warning('devices: Unable to parse '\n                                             'remainder for device %s' % line)\n                devices.append(device)\n        return devices"}
{"prompt": "def wait_for_ajax_calls_to_complete(self, timeout=5):\n        \"\"\"\n        Waits until there are no active or pending ajax requests.\n\n        Raises TimeoutException should silence not be had.\n\n        :param timeout: time to wait for silence (default: 5 seconds)\n        :return: None\n        \"\"\"", "output": "\n        time.sleep(timeout)\n        while True:\n            try:\n                self.driver.find_element_by_xpath(\n                        '//*[@id=\"ajax-error-message\"]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/", "solution": "\n        from selenium.webdriver.support.ui import WebDriverWait\n\n        WebDriverWait(self.driver, timeout).until(lambda s: s.execute_script(\"return jQuery.active === 0\"))"}
{"prompt": "def is_char_in_pairs(pos_char, pairs):\r\n        \"\"\"Return True if the charactor is in pairs of brackets or quotes.\"\"\"", "output": "\r\n        return (pos_char == \"(\" and pairs[0] == pos_char) or \\\r\n                (pos_char == \"{\" and pairs[0] == pos_char) or \\\r\n                (pos_char == \"}\" and pairs[0] == \"]\")\r\n\r\n    def is_char_in_string(pos_string, string):\r\n        \"\"\"Return True if the charactor is in string.\"\"\"\r\n        return (string[0] == pos_string) and \\\r\n                (string[1] == pos_string) or \\\r\n                (string[1] == \"{\")\r\n\r\n    def is_char_in_", "solution": "\r\n        for pos_left, pos_right in pairs.items():\r\n            if pos_left < pos_char < pos_right:\r\n                return True\r\n\r\n        return False"}
{"prompt": "def insert_code(filename, code, save=True, marker='# ATX CODE END'):\n    \"\"\" Auto append code \"\"\"", "output": "\n    with open(filename, 'w') as f:\n        f.write(code)\n        if save:\n            f.write(f'Saving {filename} code to {code}...')\n        if save:\n            f.write(f'Saving {filename} code to {code}...')\n        if save:\n            f.write(f'Saving {filename} code to {code}...')\n        if save:\n            f.write(f'Saving {filename} code to {code}...')\n        if save:\n            f.write(f'Saving {filename} code to {code}...", "solution": "\n    content = ''\n    found = False\n    for line in open(filename, 'rb'):\n        if not found and line.strip() == marker:\n            found = True\n            cnt = line.find(marker)\n            content += line[:cnt] + code\n        content += line\n    if not found:\n        if not content.endswith('\\n'):\n            content += '\\n'\n        content += code + marker + '\\n'\n    if save:\n        with open(filename, 'wb') as f:\n            f.write(content)\n    return content"}
{"prompt": "def __properties_update(self, properties):\n        \"\"\"\n        Internal update of configuration properties. Does not notifies the\n        ConfigurationAdmin of this modification.\n\n        :param properties: the new set of properties for this configuration\n        :return: True if the properties have been updated, else False\n        \"\"\"", "output": "\n        if not self.is_valid_property(properties, \"name\"):\n            raise ValueError(\"Configuration name is invalid\")\n        if not self.is_valid_property(properties, \"value\"):\n            raise ValueError(\"Configuration value is invalid\")\n        if not self.is_valid_property(properties, \"value_type\"):\n            raise ValueError(\"Configuration value_type is invalid\")\n        if not self.is_valid_property(properties, \"value_type_name\"):\n            raise ValueError(\"Configuration value_type_name is invalid\")\n        if not self.is_valid_property(properties, \"value_", "solution": "\n        if not properties:\n            # Nothing to do\n            return False\n\n        with self.__lock:\n            # Make a copy of the properties\n            properties = properties.copy()\n\n            # Override properties\n            properties[services.CONFIG_PROP_PID] = self.__pid\n\n            if self.__location:\n                properties[\n                    services.CONFIG_PROP_BUNDLE_LOCATION\n                ] = self.__location\n\n            if self.__factory_pid:\n                properties[\n                    services.CONFIG_PROP_FACTORY_PID\n                ] = self.__factory_pid\n\n            # See if new properties are different\n            if properties == self.__properties:\n                return False\n\n            # Store the copy (before storing data)\n            self.__properties = properties\n            self.__updated = True\n\n            # Store the data\n            # it will cause FileInstall to update this configuration again, but\n            # this will ignored because self.__properties has already been\n            # saved\n            self.__persistence.store(self.__pid, properties)\n            return True"}
{"prompt": "def after_invoke(self, coro):\n        \"\"\"A decorator that registers a coroutine as a post-invoke hook.\n\n        A post-invoke hook is called directly after the command is\n        called. This makes it a useful function to clean-up database\n        connections or any type of clean up required.\n\n        This post-invoke hook takes a sole parameter, a :class:`.Context`.\n\n        See :meth:`.Bot.after_invoke` for more info.\n\n        Parameters\n        -----------\n        coro: :ref:`coroutine <coroutine>`\n            The coroutine to register as the post-invoke hook.\n\n        Raises\n        -------\n        TypeError\n            The coroutine passed is not actually a coroutine.\n        \"\"\"", "output": "\n        HTTPException\n            The command failed to register.\n        \"\"\"\n        self.register_hook(coroutine)\n\n    def after_invoke_error(self, error):\n        \"\"\"A decorator that registers an error as a post-invoke hook.\n\n        A post-invoke hook is called directly after the error is\n        called. This makes it a useful function to clean-up database\n        connections or any type of clean up required.\n\n        This post-invoke hook takes a sole parameter, a :class:`.Context`.\n\n        See :meth:`.Bot.after_invoke` for more info.\n\n        Parameters\n        -----------\n        error: :ref:`coroutine", "solution": "\n        if not asyncio.iscoroutinefunction(coro):\n            raise TypeError('The post-invoke hook must be a coroutine.')\n\n        self._after_invoke = coro\n        return coro"}
{"prompt": "def main():\n    \"\"\"\n    Example application that shows how to handle attach/detach events generated\n    by the USB devices.\n\n    In this case we open the device and listen for messages when it is attached.\n    And when it is detached we remove it from our list of monitored devices.\n    \"\"\"", "output": "\n    import sys\n    import time\n\n    # Create a new instance of the USBDevice class\n    usb = USBDevice(0)\n\n    # Open the device\n    while True:\n        # Wait for a message from the device to be generated\n        time.sleep(0.1)\n        # Remove the event from the device from the list of monitored devices\n        usb.remove_event(usb.get_event_count())\n\n        # Check if we have a message\n        if usb.get_message_count() > 0:\n            # If we have a message, we can open the device\n            if usb.get_message(0):", "solution": "\n    try:\n        # Start up the detection thread such that handle_attached and handle_detached will\n        # be called when devices are attached and detached, respectively.\n        USBDevice.start_detection(on_attached=handle_attached, on_detached=handle_detached)\n\n        # Wait for events.\n        while True:\n            time.sleep(1)\n\n    except Exception as ex:\n        print('Exception:', ex)\n\n    finally:\n        # Close all devices and stop detection.\n        for sn, device in __devices.items():\n            device.close()\n\n        USBDevice.stop_detection()"}
{"prompt": "def download_icon_font(icon_font, directory):\n    \"\"\"Download given (implemented) icon font into passed directory\"\"\"", "output": "\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n    filename = os.path.join(directory, icon_font)\n    if not os.path.exists(filename):\n        raise ValueError(\"Invalid icon font: %s\" % icon_font)\n    with open(filename, 'wb') as f:\n        f.write(urllib2.urlopen(url).read())\n\n\ndef get_icon_font_path(icon_font):\n    \"\"\"Return full path to the icon font\"\"\"\n    return os.path.join(directory, 'images', icon_font)", "solution": "\n    try:\n        downloader = AVAILABLE_ICON_FONTS[icon_font]['downloader'](directory)\n        downloader.download_files()\n        return downloader\n    except KeyError:  # pragma: no cover\n        raise Exception(\"We don't support downloading font '{name}'\".format(\n            name=icon_font)\n        )"}
{"prompt": "def start(self, message):\n        \"\"\"\n        Manually starts timer with the message.\n\n        :param message:  The display message.\n        \"\"\"", "output": "\n        self.message = message\n        self.timer = Timer(message.time, message.time)\n        self.timer.start()\n\n    def stop(self):\n        \"\"\"\n        Stops timer.\n        \"\"\"\n        self.timer.stop()\n\n    def __str__(self):\n        \"\"\"\n        Returns a string representation of the timer.\n        \"\"\"\n        return self.message.format(self.name)\n\n    def __repr__(self):\n        \"\"\"\n        Returns a string representation of the timer.\n        \"\"\"\n        return self.message.format(self.name)\n\n    def __eq__(self, other):\n        \"\"\"\n        Returns true", "solution": "\n        self._start = time.clock()\n        VSGLogger.info(\"{0:<20} - Started\".format(message))"}
{"prompt": "def serve_websocket(request, port):\n    \"\"\"Start UWSGI websocket loop and proxy.\"\"\"", "output": "\n    # TODO: this is a hack to get around the fact that we can't use the same\n    #       socket for the server, but it works.\n    #       It's not clear what we want, but it's the best we can do.\n    #       We should probably use a timeout here, but that's not a problem.\n    #       We should probably use a socket timeout here, but that's not a problem.\n    #       We should probably use a socket timeout here, but that's not a problem.\n    #       We should probably use a socket timeout here, but that's not a problem.\n   ", "solution": "\n    env = request.environ\n\n    # Send HTTP response 101 Switch Protocol downstream\n    uwsgi.websocket_handshake(env['HTTP_SEC_WEBSOCKET_KEY'], env.get('HTTP_ORIGIN', ''))\n\n    # Map the websocket URL to the upstream localhost:4000x Notebook instance\n    parts = urlparse(request.url)\n    parts = parts._replace(scheme=\"ws\", netloc=\"localhost:{}\".format(port))\n    url = urlunparse(parts)\n\n    # Proxy initial connection headers\n    headers = [(header, value) for header, value in request.headers.items() if header.lower() in CAPTURE_CONNECT_HEADERS]\n\n    logger.info(\"Connecting to upstream websockets: %s, headers: %s\", url, headers)\n\n    ws = ProxyClient(url, headers=headers)\n    ws.connect()\n\n    # TODO: Will complain loudly about already send headers - how to abort?\n    return httpexceptions.HTTPOk()"}
{"prompt": "def pack(self):\n    \"\"\"Pack to framed binary message.\n\n    \"\"\"", "output": "\n    packed = []\n    packed.append(struct.pack(\"!B\", self.type))\n    packed.append(struct.pack(\"!H\", 0)) # placeholder for length at index 2\n    packed.append(struct.pack(\"!L\", self.xid)\n    packed.append(struct.pack(\"!H\", self.err_type)\n    packed.append(struct.pack(\"!H\", self.code)\n    packed.append(struct.pack(\"!H\", self.data)\n    length = sum([len(x) for x in packed])\n    if length > 255:\n      self.max_len =", "solution": "\n    buf = np.zeros(512, dtype=np.uint8)\n    packed_len = self._get_framed(buf, 0, self._copy_payload)\n    d = buf[:packed_len]\n    return d.tobytes()"}
{"prompt": "def get_task_class(tasks, task_args):\n    \"\"\"Reads in a list of tasks provided by the user,\n    loads the appropiate task, and returns two lists,\n    pre_stop_tasks and post_stop_tasks\n    :param tasks: list of strings locating tasks to load\n    :type tasks: list\n    :param task_args: list of strings to be used as args\n    :type task_args: list\n    \"\"\"", "output": "\n    if len(tasks)!= len(task_args):\n        raise ValueError(\"Length of list of tasks must be equal to len(task_args)\")\n    tasks = [task_args[i] for i in range(0, len(task_args), 2)]\n    task_args = [task_args[i] for i in range(1, len(task_args), 2)]\n    task_class = []\n    for task in tasks:\n        if task.startswith(\"task_\"):\n            task_class.append(get_task_class(task[5:], task_args))\n        elif", "solution": "\n    pre_stop_tasks = []\n    post_stop_tasks = []\n    task_to_task_args = dict(list(zip(tasks, task_args)))\n    tasks_classes = [PreStopTask, PostStopTask]\n\n    for func, task_args in task_to_task_args.items():\n        for task_class in tasks_classes:\n            imported_class = dynamic_import(func, task_class)\n            if imported_class:\n                if task_class is PreStopTask:\n                    pre_stop_tasks.append(imported_class(task_args))\n                elif task_class is PostStopTask:\n                    post_stop_tasks.append(imported_class(task_args))\n                else:\n                    print(\"ERROR: Class is not a type of Pre/Post StopTask:\" + func)\n                    sys.exit(1)\n    return pre_stop_tasks, post_stop_tasks"}
