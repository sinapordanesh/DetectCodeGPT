--------------------
Prompt: 
def tgread_vector(self):
        """Reads a vector (a list) of Telegram objects."""
----------
Output: 

        return self._client.get_vector(self.token, self.user_id)

    def get_all_tweets(self):
        """Returns a list of all the tweets in the chat."""
        return self._client.get_all_tweets(self.token, self.user_id)

    def get_tweets_by_id(self, tweet_ids):
        """Returns a list of tweets of the given id."""
        return self._client.get_tweets(self.token, tweet_ids)

    def get_tweets_by_user(
----------
Solution: 

        if 0x1cb5c415 != self.read_int(signed=False):
            raise RuntimeError('Invalid constructor code, vector was expected')

        count = self.read_int()
        return [self.tgread_object() for _ in range(count)]
--------------------
Prompt: 
def get(self):
        """
        Get a JSON-ready representation of this Personalization.

        :returns: This Personalization, ready for use in a request body.
        :rtype: dict
        """
----------
Output: 

        return {
            'id': self.id,
            'name': self.name,
            'description': self.description,
            'url': self.url,
            'image': self.image,
            'created_at': self.created_at,
            'updated_at': self.updated_at,
            'created_by': self.created_by.to_dict(),
            'created_at': self.created_at.to_dict(),
            'updated_by': self.updated_by.to_dict(),
            'updated_at': self.updated_at.to
----------
Solution: 

        personalization = {}

        for key in ['tos', 'ccs', 'bccs']:
            value = getattr(self, key)
            if value:
                personalization[key[:-1]] = value

        for key in ['subject', 'send_at', 'dynamic_template_data']:
            value = getattr(self, key)
            if value:
                personalization[key] = value

        for prop_name in ['headers', 'substitutions', 'custom_args']:
            prop = getattr(self, prop_name)
            if prop:
                obj = {}
                for key in prop:
                    obj.update(key)
                    personalization[prop_name] = obj

        return personalization
--------------------
Prompt: 
def build_lines_data(self, code_obj):
        """
        Generate various line-related helper data.
        """
----------
Output: 

        lines = []
        if self.is_comment:
            lines.append(self.comment)
        if self.is_line_comment:
            lines.append(self.line_comment)
        if self.is_block_comment:
            lines.append(self.block_comment)
        if self.is_block_comment:
            lines.append(self.block_comment)
        if self.is_block_comment:
            lines.append(self.block_comment)
        if self.is_line_comment:
            lines.append(self.line_comment)
        if
----------
Solution: 


        # Offset: lineno pairs, only for offsets which start line.
        # Locally we use list for more convenient iteration using indices
        if self.version > 1.4:
            linestarts = list(self.opc.findlinestarts(code_obj))
        else:
            linestarts = [[0, 1]]
        self.linestarts = dict(linestarts)

        # 'List-map' which shows line number of current op and offset of
        # first op on following line, given offset of op as index
        lines = []
        LineTuple = namedtuple('LineTuple', ['l_no', 'next'])

        # Iterate through available linestarts, and fill
        # the data for all code offsets encountered until
        # last linestart offset
        _, prev_line_no = linestarts[0]
        offset = 0
        for start_offset, line_no in linestarts[1:]:
            while offset < start_offset:
                lines.append(LineTuple(prev_line_no, start_offset))
                offset += 1
            prev_line_no = line_no

        # Fill remaining offsets with reference to last line number
        # and code length as start offset of following non-existing line
        codelen = len(self.code)
        while offset < codelen:
            lines.append(LineTuple(prev_line_no, codelen))
            offset += 1
        return lines
--------------------
Prompt: 
def get_brokendate_fx_forward_rate(self, asset_manager_id,  asset_id, price_date, value_date):
        """
        This method takes calculates broken date forward FX rate based on the passed in parameters
        """
----------
Output: 

        if self.is_asset_used(asset_id):
            return 0
        if self.is_value_used(value_date):
            return 0
        if self.is_fuel_used(price_date):
            return 0
        if self.is_fuel_used(value_date):
            return 0
        if self.is_fuel_used(value_date):
            return 0
        return value_date - price_date

    def is_asset_used(self,  asset_id):
        """
        This method takes a single asset id and returns True if the passed
----------
Solution: 

        self.logger.info('Calculate broken date FX Forward - Asset Manager: %s - Asset (currency): %s - Price Date: %s - Value Date: %s', asset_manager_id, asset_id, price_date, value_date)
        url = '%s/brokendateforward/%s' % (self.endpoint, asset_manager_id)
        params = {'value_date': value_date, 'asset_id':asset_id, 'price_date': price_date}
        response = self.session.get(url=url, params = params)
        if response.ok:
            forward_rate = response.json()
            self.logger.info('Retrieved broken date FX forward rate %s - %s: %s', asset_id, price_date, value_date)
            return forward_rate
        else:
            self.logger.error(response.text)
            response.raise_for_status()
--------------------
Prompt: 
def save(self, *args, **kwargs):
        """
        Create the new user. If no username is supplied (may be hidden
        via ``ACCOUNTS_PROFILE_FORM_EXCLUDE_FIELDS`` or
        ``ACCOUNTS_NO_USERNAME``), we generate a unique username, so
        that if profile pages are enabled, we still have something to
        use as the profile's slug.
        """
----------
Output: 

        if not self.is_active:
            self.slug = self.get_slug()
            self.save_m2m()

    def delete(self, *args, **kwargs):
        """
        Delete the specified user (if it exists).
        """
        if not self.is_active:
            self.slug = self.get_slug()
            self.delete_m()

    def generate_username(self, *args, **kwargs):
        """
        Generate a unique username for the current site.

        The username is composed of the username and the current site's username.
        """
        # Generate
----------
Solution: 


        kwargs["commit"] = False
        user = super(ProfileForm, self).save(*args, **kwargs)
        try:
            self.cleaned_data["username"]
        except KeyError:
            if not self.instance.username:
                try:
                    username = ("%(first_name)s %(last_name)s" %
                                self.cleaned_data).strip()
                except KeyError:
                    username = ""
                if not username:
                    username = self.cleaned_data["email"].split("@")[0]
                qs = User.objects.exclude(id=self.instance.id)
                user.username = unique_slug(qs, "username", slugify(username))
        password = self.cleaned_data.get("password1")
        if password:
            user.set_password(password)
        elif self._signup:
            try:
                user.set_unusable_password()
            except AttributeError:
                # This could happen if using a custom user model that
                # doesn't inherit from Django's AbstractBaseUser.
                pass
        user.save()

        try:
            profile = get_profile_for_user(user)
            profile_form = self.get_profile_fields_form()
            profile_form(self.data, self.files, instance=profile).save()
        except ProfileNotConfigured:
            pass

        if self._signup:
            if (settings.ACCOUNTS_VERIFICATION_REQUIRED or
                    settings.ACCOUNTS_APPROVAL_REQUIRED):
                user.is_active = False
                user.save()
            else:
                token = default_token_generator.make_token(user)
                user = authenticate(uidb36=int_to_base36(user.id),
                                    token=token,
                                    is_active=True)
        return user
--------------------
Prompt: 
def beautify_file(self, path):
        """Beautify bash script file."""
----------
Output: 

        with open(path, 'r') as f:
            return f.read()

    def _get_script_path(self, path):
        """Get the path to the bash script."""
        return os.path.join(self.path, path)

    def _get_script_args(self, script_name, args):
        """Get the arguments to the bash script."""
        return [self._get_script_path(arg) for arg in args]

    def _get_script_env(self, script_name, env):
        """Get the environment variables to set."""
        return
----------
Solution: 

        error = False
        if(path == '-'):
            data = sys.stdin.read()
            result, error = self.beautify_string(data, '(stdin)')
            sys.stdout.write(result)
        else:  # named file
            data = self.read_file(path)
            result, error = self.beautify_string(data, path)
            if(data != result):
                if(self.check_only):
                    if not error:
                        # we want to return 0 (success) only if the given file is already
                        # well formatted:
                        error = (result != data)
                else:
                    if(self.backup):
                        self.write_file(path+'.bak', data)
                    self.write_file(path, result)
        return error
--------------------
Prompt: 
def _get_var_array(self, k, use_raw=False, layer='X'):
        """Get an array from the layer (default layer='X') along the variables dimension by first looking up
        ``var.keys`` and then ``obs.index``."""
----------
Output: 

        return self._var_array[k]

    def _get_var_names(self, var):
        """Get an array of the names of variables (default layer='X')."""
        return self._var_names[var]

    def _get_var_units(self, var):
        """Get an array of the units of variables (default layer='m')."""
        return self._var_units[var]

    def _get_var_shape(self, var):
        """Get an array of the shape of variables (default layer='m')."""
        return self._var_shape[
----------
Solution: 

        in_raw_obs_names = k in self.raw.obs_names if self.raw is not None else False

        if use_raw and self.raw is None:
            raise ValueError('.raw doesn\'t exist')

        if k in self.var.keys():
            x = self._var[k]
        elif in_raw_obs_names and use_raw and layer == 'X':
            x = self.raw[k].X
        elif k in self.obs_names and not use_raw and (layer == 'X' or layer in self.layers.keys()):
            x = self[k].X if layer=='X' else self[k].layers[layer]
        elif use_raw and layer != 'X':
            raise ValueError('No layers in .raw')
        elif layer != 'X' and layer not in self.layers.keys():
            raise ValueError('Did not find {} in layers.keys.'
                             .format(layer))
        else:
            raise ValueError('Did not find {} in var.keys or obs_names.'
                             .format(k))
        return x
--------------------
Prompt: 
def worker_logwarning(self, message, code, nodeid, fslocation):
        """Emitted when a node calls the pytest_logwarning hook."""
----------
Output: 

        self.log.warning(message, nodeid, fslocation)

    def pytest_warning_captured(self, node, fslocation):
        """Emitted when a warning is captured by the pytest_logwarning hook."""
        self.log.warning(
            "Warning captured, node: {}, fslocation: {}".format(
                node, fslocation))

    def pytest_warning_no_nodeid(self, nodeid, fslocation):
        """Emitted when a node has no id."""
        self.log.warning("Node id: {}".format(nodeid))

    def pytest_logwarning_captured_no
----------
Solution: 

        kwargs = dict(message=message, code=code, nodeid=nodeid, fslocation=fslocation)
        self.config.hook.pytest_logwarning.call_historic(kwargs=kwargs)
--------------------
Prompt: 
def ListVariable(key, help, default, names, map={}):
    """
    The input parameters describe a 'package list' option, thus they
    are returned with the correct converter and validator appended. The
    result is usable for input to opts.Add() .

    A 'package list' option may either be 'all', 'none' or a list of
    package names (separated by space).
    """
----------
Output: 

    if not key in map:
        map[key] = default
    else:
        map[key] = map[key].split()
    return list_converter(map)

def ListPackage(key, help, default, names, map={}):
    """
    The input parameters describe a 'package list' option, thus they
    are returned with the correct converter and validator appended. The
    result is usable for input to opts.Add().

    A 'package list' option may either be 'all', 'none' or a list of
    package names (separated by space).
    """
    if not key in
----------
Solution: 

    names_str = 'allowed names: %s' % ' '.join(names)
    if SCons.Util.is_List(default):
        default = ','.join(default)
    help = '\n    '.join(
        (help, '(all|none|comma-separated list of names)', names_str))
    return (key, help, default,
            None, #_validator,
            lambda val: _converter(val, names, map))
--------------------
Prompt: 
def read(filename):
    """ reads a Horn SAT formula from a text file

    :file format:
        # comment
        A     # clause with unique positive literal
        :- A  # clause with unique negative literal
        A :- B, C, D # clause where A is positive and B,C,D negative
        # variables are strings without spaces
    """
----------
Output: 

    with open(filename, 'r') as f:
        formula = f.read()
    return formula


def write(filename, formula):
    """ writes a Horn SAT formula to a text file

    :file format:
        # comment
        A     # variable
        :- B  # variable
        # variable
        # variable
        # variable
        # variable
        # variable
        # variable
        # variable
        # variable
        # variable
        # variable
        # variable
        # variable
        # variable
        # variable
        # variable
        # variable
        # variable
        # variable
        # variable
        # variable
        # variable
        #
----------
Solution: 

    formula = []
    for line in open(filename, 'r'):
        line = line.strip()
        if line[0] == "#":
            continue
        lit = line.split(":-")
        if len(lit) == 1:
            posvar = lit[0]
            negvars = []
        else:
            assert len(lit) == 2
            posvar = lit[0].strip()
            if posvar == '':
                posvar = None
            negvars = lit[1].split(',')
            for i in range(len(negvars)):
                negvars[i] = negvars[i].strip()
        formula.append((posvar, negvars))
    return formula
--------------------
Prompt: 
def ticks_difference(price_1, price_2):
    """Returns the absolute difference in terms of "ticks" (i.e. individual
    price increments) between two Betfair prices.

    :param float price_1: An exact, valid Betfair price
    :param float price_2: An exact, valid Betfair price
    :returns: The absolute value of the difference between the prices in "ticks"
    :rtype: int
    """
----------
Output: 

    return abs(price_1 - price_2)


def get_price_difference(price_1, price_2):
    """Returns the absolute difference in terms of "price" (i.e. individual
    price increments) between two Betfair prices.

    :param float price_1: An exact, valid Betfair price
    :param float price_2: An exact, valid Betfair price
    :returns: The absolute value of the difference between the prices in "price"
    :rtype: int
    """
    return ticks_difference(price_1, price
----------
Solution: 

    price_1_index = PRICES.index(as_dec(price_1))
    price_2_index = PRICES.index(as_dec(price_2))
    return abs(price_1_index - price_2_index)
--------------------
Prompt: 
async def onConnect(self):
        """
        Configure the component
        """
----------
Output: 

        self.log.info("Configuring component")
        await self.register(self.COMPONENT_NAME, self.COMPONENT_CONFIG)
        self.log.info("Connected to component")

    async def onDisconnect(self):
        """
        Disconnect the component
        """
        self.log.info("Disconnected from component")
        await self.unregister(self.COMPONENT_NAME)
        self.log.info("Disconnected from component")

    async def onDisconnectAll(self):
        """
        Disconnect all the components
        """
        self.log.info("Disconnecting all the components")
        await self
----------
Solution: 

        # Add extra attribute
        # This allows for following crossbar/autobahn spec
        # without changing legacy configuration
        if not hasattr(self.config, 'extra'):
            original_config = {'config': self.config}
            self.config = objdict(self.config)
            setattr(self.config, 'extra', original_config)
            self.config.extra['handlers'] = self.handlers

        # setup transport host
        self.transport_host = self.config.extra['config']['transport_host']

        # subscription setup
        self.subscribe_options = SubscribeOptions(**self.config.extra['config']['sub_options'])
        self.replay_events = self.config.extra['config']['replay_events']

        # publishing setup
        self.publish_topic = self.config.extra['config']['publish_topic']['topic']
        self.publish_options = PublishOptions(**self.config.extra['config']['pub_options'])

        # setup callback
        self.handlers = self.config.extra['handlers']

        # optional subscribed topics from config.json
        self.subscribed_topics = self.config.extra['config']['subscribed_topics']

        # put name on session
        self.name = self.config.extra['config']['name']

        # setup db pool - optionally
        if self.config.extra['config']['pub_options']['retain'] is True:
            self.pool = await asyncpg.create_pool(
                user=EVENT_DB_USER,
                password=EVENT_DB_PASS,
                host=EVENT_DB_HOST,
                database=EVENT_DB_NAME
            )

        # Handle non crossbar drivers
        try:
            self.join(self.config.realm)
        except AttributeError:
            pass
--------------------
Prompt: 
def main():
    """The main entry point for command line invocation. It's output
    is adjusted by command line arguments. By default it outputs 10
    passwords.

    For help on accepted arguments, run::

        $ passgen -h

    Or::

        $ python -m passgen -h
    """
----------
Output: 

    from getpass import getpass
    from string import ascii_letters, digits
    from random import choice
    from string import digits
    from random import randint
    from string import ascii_letters, ascii_lowercase

    # Create the password manager
    password_manager = PasswordManager()

    # Create the password manager
    password_manager.add_password("password", "password")
    password_manager.add_password("password2", "password2")
    password_manager.add_password("password3", "password3")

    # Create the passwords
    password_manager.add_password("password", "password3")
    password
----------
Solution: 

    parser = argparse.ArgumentParser(
        description="Generate random password."
    )
    parser.add_argument("-l", "--length",
                        help="the length of the generated "
                             "password (default: 12)",
                        type=int, default=12)
    parser.add_argument("-n", "--number",
                        help="how many passwords to generate (default: 10)",
                        type=int, default=10)
    parser.add_argument("-p", "--punctuation",
                        help="use punctuation characters",
                        action='store_true')
    parser.add_argument("--limit-punctuation",
                        help="specify allowed punctuation characters",
                        action='store', default='')
    alnum_group = parser.add_mutually_exclusive_group()
    alnum_group.add_argument("--no-digits",
                             help="don't use digits",
                             action='store_false', dest='digits')
    alnum_group.add_argument("--no-letters",
                             help="don't use letters",
                             action='store_false', dest='letters')
    case_group = parser.add_mutually_exclusive_group()
    case_group.add_argument("--upper",
                            help="use only upper case letters",
                            action='store_true')
    case_group.add_argument("--lower",
                            help="use only lower case letters",
                            action='store_true')
    args = parser.parse_args()

    if args.length < 1:
        _error("argument -l/--length must be greater than zero")
    if args.number < 1:
        _error("argument -n/--number must be greater than zero")

    if args.lower:
        case = "lower"
    elif args.upper:
        case = "upper"
    else:
        case = "both"

    for _ in range(args.number):
        print(passgen(args.length, punctuation=args.punctuation,
                      limit_punctuation=args.limit_punctuation,
                      digits=args.digits,
                      letters=args.letters, case=case))
--------------------
Prompt: 
def last_modified():
    """ information about the most recently modified file """
----------
Output: 

    return datetime.fromtimestamp(os.path.getmtime(os.path.join(app.config['UPLOAD_FOLDER'], 'last_modified'))

def last_modified_iso():
    """ information about the most recently modified ISO file """
    return datetime.fromtimestamp(os.path.getmtime(os.path.join(app.config['UPLOAD_FOLDER'], 'last_modified_iso'))

def get_file_size(filename):
    """ returns the size of a file """
    return os.path.getsize(os.path.join(app.config['UPLOAD_FOLDER'], filename))
----------
Solution: 

    files = model.FileFingerprint.select().order_by(
        orm.desc(model.FileFingerprint.file_mtime))
    for file in files:
        return file.file_mtime, file.file_path
    return None, None
--------------------
Prompt: 
def dateint_to_datetime(dateint):
    """Converts the given dateint to a datetime object, in local timezone.

    Arguments
    ---------
    dateint : int
        An integer object decipting a specific calendaric day; e.g. 20161225.

    Returns
    -------
    datetime.datetime
        A timezone-unaware datetime object representing the start of the given
        day (so at 0 hours, 0 minutes, etc...) in the local timezone.
    """
----------
Output: 

    return dateint - timedelta(hours=0, minutes=0, seconds=0)


def datetime_to_datestr(date):
    """Converts the given date to a string, in local timezone.

    Arguments
    ---------
    date : datetime.datetime
        An instance of `datetime.datetime`; e.g. `date.today()` or
        `datetime.datetime.now()`.

    Returns
    -------
    str
        A string representing the date.
    """
    return date.strftime("%Y-%m-%d")


def datestr_to_datetime(datestr):
    """Converts the
----------
Solution: 

    if len(str(dateint)) != 8:
        raise ValueError(
            'Dateints must have exactly 8 digits; the first four representing '
            'the year, the next two the months, and the last two the days.')
    year, month, day = decompose_dateint(dateint)
    return datetime(year=year, month=month, day=day)
--------------------
Prompt: 
def ipynb_file_name(params):
    """
    Returns OS path to notebook based on route parameters.
    """
----------
Output: 

    if params.get('notebook_name'):
        return params['notebook_name']
    else:
        return os.path.join(IPYTHON_DIR, 'nb_files', 'notebook', 'default')

def ipynb_file_url(params):
    """
    Returns OS path to notebook based on route parameters.
    """
    if params.get('notebook_url'):
        return params['notebook_url']
    else:
        return os.path.join(IPYTHON_DIR, 'nb_files', 'notebook', 'default', 'file')

def ipy
----------
Solution: 

    global notebook_dir
    p = notebook_dir + [param.replace('/', '') for param in params if param is not None]
    return path.join(*p)
--------------------
Prompt: 
def check(text):
    """Check the text."""
----------
Output: 

    if not text:
        return False
    if text[0] == '#':
        return False
    if text[0] == '$':
        return False
    if text[0] == '(':
        return False
    if text[0] == ')':
        return False
    if text[0] == '[':
        return False
    if text[0] == ']':
        return False
    if text[0] == '{':
        return False
    if text[0] == '}':
        return False
    if text[0] == '(':
        return False
    if text[0] == ')':
        return False
   
----------
Solution: 

    err = "lexical_illusions.misc"
    msg = u"There's a lexical illusion here: a word is repeated."

    list = [
        "the\sthe",
        "am\sam",
        "has\shas"
    ]

    return existence_check(text, list, err, msg)
--------------------
Prompt: 
def visibility(self, strip_pos, particle_pos):
        """
        Compute particle visibility based on distance between current
        strip position being rendered and particle position. A value
        of 0.0 is returned if they are >= one aperture away, values
        between 0.0 and 1.0 are returned if they are less than one
        aperature apart.
        """
----------
Output: 

        if strip_pos == 0:
            return 0.0
        if strip_pos == 1:
            return 1.0
        return 0.0

    def get_particle_position(self, particle_id):
        """
        Get particle position based on id.
        """
        if particle_id == 0:
            return self.particle_a
        elif particle_id == 1:
            return self.particle_b
        else:
            return self.particle_a

    def get_particle_position_in_aperture(self, aperture):
        """
        Compute particle position based on aperture.
----------
Solution: 

        dist = abs(particle_pos - strip_pos)
        if dist > self.half_size:
            dist = self._size - dist
        if dist < self.aperture:
            return (self.aperture - dist) / self.aperture
        else:
            return 0
--------------------
Prompt: 
def setattr(self, name, value):
        """Set an attribute to a new value for all Parameters.

        For example, set grad_req to null if you don't need gradient w.r.t a
        model's Parameters::

            model.collect_params().setattr('grad_req', 'null')

        or change the learning rate multiplier::

            model.collect_params().setattr('lr_mult', 0.5)

        Parameters
        ----------
        name : str
            Name of the attribute.
        value : valid type for attribute name
            The new value for the attribute.
        """
----------
Output: 

        if isinstance(value, str):
            value = self.module.params[name]
        elif isinstance(value, valid_types):
            value = self.module.params[name] = value
        else:
            raise ValueError(
                "Cannot set attribute %s to %s. "
                "Check the documentation for more information." % (name, value))
        self.update()
        return value

    def update(self):
        """Update the parameters of this model.

        This method updates the model parameters, and returns the updated
        parameters for this model.

        Returns
        -------
        list of str
            List of
----------
Solution: 

        for i in self.values():
            setattr(i, name, value)
--------------------
Prompt: 
def _get_block_transactions(
            self,
            transaction_root: Hash32,
            transaction_class: Type['BaseTransaction']) -> Iterable['BaseTransaction']:
        """
        Memoizable version of `get_block_transactions`
        """
----------
Output: 

        if self.block_transactions is None:
            self.block_transactions = []
            for tx in self.transactions:
                self.block_transactions.append(tx)
        return self.block_transactions

    def get_block_transactions(self, block_hash: Hash32) -> Iterable['BlockTransaction']:
        """
        Returns a list of the transactions in the block.
        """
        return self._get_block_transactions(
            self.block_root,
            self.transaction_class_to_hash(block_hash))

    def get_block_root(self) -> Hash32:
----------
Solution: 

        for encoded_transaction in self._get_block_transaction_data(self.db, transaction_root):
            yield rlp.decode(encoded_transaction, sedes=transaction_class)
--------------------
Prompt: 
def AddAttribute(self, attribute, value=None, age=None):
    """Add an additional attribute to this object.

    If value is None, attribute is expected to be already initialized with a
    value. For example:

    fd.AddAttribute(fd.Schema.CONTAINS("some data"))

    Args:
       attribute: The attribute name or an RDFValue derived from the attribute.
       value: The value the attribute will be set to.
       age: Age (timestamp) of the attribute. If None, current time is used.

    Raises:
       IOError: If this object is read only.
    """
----------
Output: 
 """
    if not self._IsOpen():
      raise IOError("Can't add attribute %s to %s" % (attribute, self._path))

    if not value and not age:
      value = self._GetDefaultValue()

    if isinstance(value, RDFValue):
      value = value.Copy()
      value.CopyFrom(attribute)
      if age is not None:
        value.age = age

    self._dirty = True
    self._last_modified = time.time()

  def _GetDefaultValue(self):
    """Get the default value for this object.

    Returns:
       A string
----------
Solution: 

    if "w" not in self.mode:
      raise IOError("Writing attribute %s to read only object." % attribute)

    if value is None:
      value = attribute
      attribute = value.attribute_instance

    # Check if this object should be locked in order to add the attribute.
    # NOTE: We don't care about locking when doing blind writes.
    if self.mode != "w" and attribute.lock_protected and not self.transaction:
      raise IOError("Object must be locked to write attribute %s." % attribute)

    self._CheckAttribute(attribute, value)
    # Does this represent a new version?
    if attribute.versioned:
      if attribute.creates_new_object_version:
        self._new_version = True

      # Update the time of this new attribute.
      if age:
        value.age = age
      else:
        value.age = rdfvalue.RDFDatetime.Now()

    # Non-versioned attributes always replace previous versions and get written
    # at the earliest timestamp (so they appear in all objects).
    else:
      self._to_delete.add(attribute)
      self.synced_attributes.pop(attribute, None)
      self.new_attributes.pop(attribute, None)
      value.age = 0

    self._AddAttributeToCache(attribute, value, self.new_attributes)
    self._dirty = True
--------------------
Prompt: 
def list_subnets(self, identifier=None, datacenter=None, version=0,
                     subnet_type=None, network_space=None, **kwargs):
        """Display a list of all subnets on the account.

        This provides a quick overview of all subnets including information
        about data center residence and the number of devices attached.

        :param string identifier: If specified, the list will only contain the
                                    subnet matching this network identifier.
        :param string datacenter: If specified, the list will only contain
                                    subnets in the specified data center.
        :param int version: Only returns subnets of this version (4 or 6).
        :param string subnet_type: If specified, it will only returns subnets
                                     of this type.
        :param string network_space: If specified, it will only returns subnets
                                       with the given address space label.
        :param dict \\*\\*kwargs: response-level options (mask, limit, etc.)
        """
----------
Output: 
0=all, 1=v1, 2=v2)
        :param list subnet_type: A list of strings that can be used to
                                    filter the network (e.g. ['public', 'private']).
                                    If specified, only subnets matching this
                                    network type will be returned.
        :param list network_space: A list of dictionaries that describe the
                                network. Valid keys are 'private', 'public',
                                'private_v4', 'public_v6' and 'private_v6'.
        :return: A list of dictionaries that describe the network.
        :rtype: list[dict
----------
Solution: 

        if 'mask' not in kwargs:
            kwargs['mask'] = DEFAULT_SUBNET_MASK

        _filter = utils.NestedDict(kwargs.get('filter') or {})

        if identifier:
            _filter['subnets']['networkIdentifier'] = (
                utils.query_filter(identifier))
        if datacenter:
            _filter['subnets']['datacenter']['name'] = (
                utils.query_filter(datacenter))
        if version:
            _filter['subnets']['version'] = utils.query_filter(version)
        if subnet_type:
            _filter['subnets']['subnetType'] = utils.query_filter(subnet_type)
        else:
            # This filters out global IPs from the subnet listing.
            _filter['subnets']['subnetType'] = {'operation': '!= GLOBAL_IP'}
        if network_space:
            _filter['subnets']['networkVlan']['networkSpace'] = (
                utils.query_filter(network_space))

        kwargs['filter'] = _filter.to_dict()
        kwargs['iter'] = True
        return self.client.call('Account', 'getSubnets', **kwargs)
--------------------
Prompt: 
def _check_jp2h_child_boxes(self, boxes, parent_box_name):
        """Certain boxes can only reside in the JP2 header."""
----------
Output: 

        for box in boxes:
            if box.header.get('bitpix', None) == '1':
                return True
        return False

    def _get_exif_data(self, header, box_name):
        """Return the data for an EXIF header."""
        if header.get('exif', None) is not None:
            return header.get('exif', None)
        return None

    def _get_exif_image(self, header, box_name):
        """Return the image for an EXIF header."""
        if header.get('bitpix', None) ==
----------
Solution: 

        JP2H_CHILDREN = set(['bpcc', 'cdef', 'cmap', 'ihdr', 'pclr'])

        box_ids = set([box.box_id for box in boxes])
        intersection = box_ids.intersection(JP2H_CHILDREN)
        if len(intersection) > 0 and parent_box_name not in ['jp2h', 'jpch']:
            msg = "A {0} box can only be nested in a JP2 header box."
            raise IOError(msg.format(list(intersection)[0]))

        # Recursively check any contained superboxes.
        for box in boxes:
            if hasattr(box, 'box'):
                self._check_jp2h_child_boxes(box.box, box.box_id)
--------------------
Prompt: 
def get_key_for_value(dict_obj, value, default=None):
    """
    >>> get_key_for_value({0: 'what', 'k': 'ever', 'you': 'want', 'to find': None}, 'you')
    >>> get_key_for_value({0: 'what', 'k': 'ever', 'you': 'want', 'to find': None}, 'you', default='Not Found')
    'Not Found'
    >>> get_key_for_value({0: 'what', 'k': 'ever', 'you': 'want', 'to find': None}, 'other', default='Not Found')
    'Not Found'
    >>> get_key_for_value({0: 'what', 'k': 'ever', 'you': 'want', 'to find': None}, 'want')
    'you'
    >>> get_key_for_value({0: 'what', '': 'ever', 'you': 'want', 'to find': None, 'you': 'too'}, 'what')
    0
    >>> get_key_for_value({0: 'what', '': 'ever', 'you': 'want', 'to find': None, 'you': 'too', ' ': 'want'}, 'want')
    ' '
    """
----------
Output: 
want', 'to find': None}, 'what')
    'ever'
    >>> get_key_for_value({0: 'what', 'k': 'ever', 'you': 'want', 'to find': None}, 'you')
    'want'
    >>> get_key_for_value({0: 'what', 'k': 'ever', 'you': 'want', 'to find': None}, 'k')
    'ever'
    """
    if value is None:
        return None
    return dict_obj.get(key(dict_obj, default), None)


def get
----------
Solution: 

    for k, v in viewitems(dict_obj):
        if v == value:
            return k
    return default
--------------------
Prompt: 
def scroll(self, lines):
        """
        Scroll the window up or down.

        :param lines: Number of lines to scroll.  Negative numbers move the buffer up.
        """
----------
Output: 

        if self.scroll_offset == 0:
            self.scroll_offset = len(lines)
        elif self.scroll_offset == 1:
            self.scroll_offset = max(lines)
        else:
            raise Exception("Invalid scroll offset: %d" % self.scroll_offset)
        self.scroll_offset = min(self.scroll_offset, self.rows - 1)
        self.update()

    def update(self):
        """
        Update the window.
        """
        self.rows = self.rows + 1
        self.scroll_offset = self.scroll_offset + 1
----------
Solution: 

        line = [(ord(u" "), Screen.COLOUR_WHITE, 0, 0, 1) for _ in range(self._width)]
        if lines > 0:
            # Limit to buffer size - this will just invalidate all the data
            lines = min(lines, self._height)
            for y in range(0, self._height - lines):
                self._double_buffer[y] = self._double_buffer[y + lines]
                self._screen_buffer[y] = self._screen_buffer[y + lines]
            for y in range(self._height - lines, self._height):
                self._double_buffer[y] = line[:]
                self._screen_buffer[y] = line[:]
        else:
            # Limit to buffer size - this will just invalidate all the data
            lines = max(lines, -self._height)
            for y in range(0, -lines):
                self._double_buffer[self._height + lines + y] = line[:]
                self._screen_buffer[y] = line[:]
            for y in range(self._height - 1, -lines, -1):
                self._double_buffer[y] = self._double_buffer[y + lines]
                self._screen_buffer[y] = self._screen_buffer[y + lines]
--------------------
Prompt: 
def parse_lrvalue_string(search_string,
                         delimiter=":"):
    """
    The function takes a multi-line output/string with the format
    "name/descr   : value", and converts it to a dictionary object
    with key value pairs, where key is built from the name/desc
    part and value as the value.

    eg:  "Serial Number: FCH1724V1GT" will be translated to
    dict['serial_number'] = "FCH1724V1GT"
    """
----------
Output: 

    # The regex below is a little tricky. It will match all the
    # string that is not a valid regex, but will not match the whole
    # string.
    #
    # The regex below is a little tricky. It will match all the string
    # that is not a valid regex, but will not match the whole string.
    #
    # The regex below is a little tricky. It will match all the string
    # that is not a valid regex, but will not match the whole string.
    #
    # The regex below is a little tricky. It will match all the string
    # that is not
----------
Solution: 

    mac_search_pattern = r"(.*) *%s ([\w|\d]+.*)" % delimiter
    search_pattern = r"(.*) *%s *(.*)" % delimiter

    rexdict = {}
    for line in search_string.splitlines():
        line = line.strip()
        mobj = re.match(mac_search_pattern, line)
        if mobj:
            key = mobj.group(1).lower()
            key = "_".join(key.split()[0:3])
            key = key.strip()
            rexdict[key] = mobj.group(2)
            continue

        mobj = re.match(search_pattern, line)
        if mobj:
            key = mobj.group(1).lower()
            key = "_".join(key.split()[0:3])
            key = key.strip()
            rexdict[key] = mobj.group(2)

    return rexdict
--------------------
Prompt: 
def datetime_to_djd(time):
    """
    Converts a datetime to the Dublin Julian Day

    Parameters
    ----------
    time : datetime.datetime
        time to convert

    Returns
    -------
    float
        fractional days since 12/31/1899+0000
    """
----------
Output: 

    return (time - datetime.datetime(1970, 1, 1)).total_seconds()


def djd_to_datetime(jd):
    """
    Converts a Julian Day to a datetime

    Parameters
    ----------
    jd : float
        Julian Day

    Returns
    -------
    datetime.datetime
        time to convert
    """
    return jd + datetime.timedelta(days=31)


def datetime_to_string(time):
    """
    Converts a datetime to a string

    Parameters
    ----------
    time : datetime.datetime
        time to convert

    Returns
    -------
    str
        date and time
    """
----------
Solution: 


    if time.tzinfo is None:
        time_utc = pytz.utc.localize(time)
    else:
        time_utc = time.astimezone(pytz.utc)

    djd_start = pytz.utc.localize(dt.datetime(1899, 12, 31, 12))
    djd = (time_utc - djd_start).total_seconds() * 1.0/(60 * 60 * 24)

    return djd
--------------------
Prompt: 
def save_blocks(self, id_env, blocks):
        """
        Save blocks from environment

        :param id_env: Environment id
        :param blocks: Lists of blocks in order. Ex: ['content one', 'content two', ...]

        :return: None

        :raise AmbienteNaoExisteError: Ambiente não cadastrado.
        :raise InvalidValueError: Invalid parameter.
        :raise UserNotAuthorizedError: Permissão negada.
        :raise DataBaseError: Falha na networkapi ao acessar o banco de dados.
        :raise XMLError: Falha na networkapi ao ler o XML de requisição ou gerar o XML de resposta.
        """
----------
Output: 
.
        """
        return self._save_blocks(id_env, blocks)

    def _save_blocks(self, id_env, blocks):
        """
        Save blocks from environment

        :param id_env: Environment id
        :param blocks: List of blocks in order. Ex: ['content one', 'content two',...]

        :param id_env: Environment id
        :return: None

        :raise AmbienteNaoExisteError: Ambiente não cadastrado.
        :raise InvalidValueError: Invalid parameter.
        :raise UserNotAuthorizedError: Permiss
----------
Solution: 


        url = 'environment/save_blocks/'

        map_dict = dict()

        map_dict['id_env'] = id_env
        map_dict['blocks'] = blocks

        code, xml = self.submit({'map': map_dict}, 'POST', url)

        return self.response(code, xml)
--------------------
Prompt: 
def command(cmd):
    """Execute command and raise an exception upon an error.

      >>> 'README' in command('ls')
      True
      >>> command('nonexistingcommand')  #doctest: +ELLIPSIS
      Traceback (most recent call last):
      ...
      SdistCreationError

    """
----------
Output: 

    try:
        return _command_cache[cmd]
    except KeyError:
        pass

    # Check that the module name ends with "main"
    if cmd[-3:] == ".py":
        if cmd[-3:] == ".pyw":
            # runpy() is a function in Pythonw mode, not a class
            return _run_python_module(cmd)
        else:
            # runpy() is a class in w mode, not a function
            return _run_class_mode(cmd)
    else:
        if cmd[-3:] == ".s":
            # s() is a string in
----------
Solution: 

    status, out = commands.getstatusoutput(cmd)
    if status is not 0:
        logger.error("Something went wrong:")
        logger.error(out)
        raise SdistCreationError()
    return out
--------------------
Prompt: 
def ranker(self, X, meta):
        """
        Sort the place features list by the score of its relevance.
        """
----------
Output: 

        if self.relevance_type == "score":
            X = self.score_features(X)
        return X

    def score_features(self, X):
        """
        Calculate the score of the features in the dataset.
        """
        return self.model.score(X)

    def get_feature_names(self):
        """
        Return the names of the features.
        """
        return self.model.get_feature_names()

    def get_feature_types(self):
        """
        Return the types of the features.
        """
        return self.model.get_feature_types
----------
Solution: 

        # total score is just a sum of each row
        total_score = X.sum(axis=1).transpose()
        total_score = np.squeeze(np.asarray(total_score))  # matrix to array
        ranks = total_score.argsort()
        ranks = ranks[::-1]
        # sort the list of dicts according to ranks
        sorted_meta = [meta[r] for r in ranks]
        sorted_X = X[ranks]
        return (sorted_X, sorted_meta)
--------------------
Prompt: 
def notebook_type(self, notebook_type):
        """ Notebook type, acceptable values are 'jupyter' as well as any names
        defined by external notebook hooks that have been installed.

        """
----------
Output: 

        return self._notebook_type

    def _get_notebook_dir(self, notebook_dir):
        """ Get the path to the notebook directory.

        """
        if notebook_dir is None:
            return None
        else:
            return os.path.join(notebook_dir, "notebooks")

    def _get_notebook_file(self, notebook_file):
        """ Get the path to the notebook file.

        """
        return self._get_notebook_dir(notebook_file)

    def _get_notebook_name(self, notebook_file):
        """ Get the name of the notebook
----------
Solution: 

        if notebook_type is None or not isinstance(notebook_type, string_types):
            raise ValueError("Notebook type must be a string")
        self._notebook_type = notebook_type.lower()
--------------------
Prompt: 
def download_api(branch=None) -> str:
    """download API documentation from _branch_ of Habitica\'s repo on Github"""
----------
Output: 

    if branch is None:
        branch = _branch_

    if not os.path.exists(os.path.join(repo_dir, "docs", "api.rst")):
        print("Error: Could not find Habitica\'s API documentation")
        return None

    with open(os.path.join(repo_dir, "docs", "api.rst")) as f:
        return f.read()


def get_version() -> str:
    """Get version from the current source"""
    with open(os.path.join(repo_dir, "version.txt"))
----------
Solution: 

    habitica_github_api = 'https://api.github.com/repos/HabitRPG/habitica'
    if not branch:
        branch = requests.get(habitica_github_api + '/releases/latest').json()['tag_name']
    curl = local['curl']['-sL', habitica_github_api + '/tarball/{}'.format(branch)]
    tar = local['tar'][
        'axzf', '-', '--wildcards', '*/website/server/controllers/api-v3/*', '--to-stdout']
    grep = local['grep']['@api']
    sed = local['sed']['-e', 's/^[ */]*//g', '-e', 's/  / /g', '-']
    return (curl | tar | grep | sed)()
--------------------
Prompt: 
def _err(self, errclass: str="error", *args) -> "Err":
        """
        Creates an error
        """
----------
Output: 

        return self.error(errclass, *args)

    def _get_error_message(self, errclass: str) -> str:
        """
        Returns the error message
        """
        return self._err(errclass, *self.error_args)

    def _get_error_args(self, errclass: str) -> List[str]:
        """
        Returns the error arguments
        """
        return self.error_args

    def _get_error_msg(self, errclass: str) -> str:
        """
        Returns the error message
        """
        return self._err(err
----------
Solution: 

        error = self._new_err(errclass, *args)
        if self.log_errs is True:
            sep = " "
            if self.log_format == "csv":
                sep = ","
            msg = str(datetime.now()) + sep + \
                self._errmsg(error, msgformat=self.log_format)
            self.logger.error(msg)
        print(self._errmsg(error))
        self._add(error)
        return error
--------------------
Prompt: 
def inline_css(self, html):
        """Inlines CSS defined in external style sheets.
        """
----------
Output: 

        for element in html.xpath('//*[@style="text-align:center"]'):
            self.css.append(CSSRule(element, parent=self))

    def _get_css_rules(self):
        """Returns a list of CSSRule objects.
        """
        return self.css

    def _get_css_selectors(self):
        """Returns a list of CSSSelector objects.
        """
        return self.css

    def _get_css_rules_by_type(self, selector_type):
        """Returns a list of CSSRule objects.
        """
        return self._get
----------
Solution: 

        premailer = Premailer(html)
        inlined_html = premailer.transform(pretty_print=True)
        return inlined_html
--------------------
Prompt: 
def halt_running():
    """Halts all running servers"""
----------
Output: 

    for server in servers:
        server.halt()

def halt_stopped():
    """Halts all stopped servers"""
    for server in servers:
        server.halt()

def halt_running_servers():
    """Halts all running servers"""
    for server in servers:
        server.halt()

def halt_stopped_servers():
    """Halts all stopped servers"""
    for server in servers:
        server.halt()

def wait_for_server_to_stop(server):
    """Waits for a server to stop"""
----------
Solution: 

    vultr = Vultr(API_KEY)

    try:
            serverList = vultr.server.list()
            #logging.info('Listing servers:\n%s', dumps(
            #serverList, indent=2
        #))
    except VultrError as ex:
        logging.error('VultrError: %s', ex)

    for serverID in serverList:
        if serverList[serverID]['power_status'] == 'running':
            logging.info(serverList[serverID]['label'] + " will be gracefully shutdown.")
            vultr.server.halt(serverID)
--------------------
Prompt: 
def register_cleanup(cleanup):
    """Register a cleanup function to be called at the end of every validation
    task. Takes either a callable (including a class with a __call_ method),
    or a class with a `cleanup` class method."""
----------
Output: 

    if callable(cleanup):
        return cleanup(cleanup)
    else:
        return cleanup


def _get_default_config_files(env):
    """Return default configuration files for the given environment."""
    config_files = [
        os.path.join(env.topobjdir, 'etc', 'profile.d', 'config.py'),
        os.path.join(env.topobjdir, 'etc', 'profile.d', 'config.py'),
        os.path.join(env.topobjdir, 'etc', 'profile.d', 'config.py'),
       
----------
Solution: 


    if not callable(cleanup):
        # Allow decorating a class with a `cleanup` classm ethod.
        cleanup = cleanup.cleanup

    CLEANUP_FUNCTIONS.append(cleanup.cleanup)
    return cleanup
--------------------
Prompt: 
def dump(mer_file, output_file='counts.fasta', options='', returncmd=False):
    """
    Dumps output from jellyfish count into a human-readable format.
    :param mer_file: Output from jellyfish count.
    :param output_file: Where to store output. Default counts.fasta
    :param options: Other options to pass to jellyfish. Input should be a string, with options typed as they would be
    on the command line.
    :param returncmd: If set to true, function will return the cmd string passed to subprocess as a third value.
    :return: Stdout and stderr from calling jellyfish.
    """
----------
Output: 

    :return:
    """
    if returncmd:
        return subprocess.check_output(returncmd, shell=True)
    else:
        return subprocess.check_output(
            'wc -l {0} {1} | wc -l {2} {3} | wc -l {4} {5} | '
            'wc -l {6} | wc -l {7} | wc -l {8} | wc -l {9} | '
            'wc -l {10} | wc -l {11} | wc -l {12} | '
            'wc
----------
Solution: 

    cmd = 'jellyfish dump {} -o {} {}'.format(mer_file, output_file, options)
    out, err = accessoryfunctions.run_subprocess(cmd)
    if returncmd:
        return out, err, cmd
    else:
        return out, err
--------------------
Prompt: 
def list_deelgemeenten(self, gewest=2):
        """
        List all `deelgemeenten` in a `gewest`.

        :param gewest: The :class:`Gewest` for which the \
            `deelgemeenten` are wanted. Currently only Flanders is supported.
        :rtype: A :class:`list` of :class:`Deelgemeente`.
        """
----------
Output: 

        return self.deelgemeenten(self.deelgemeenten_id, gewest)

    def deelgemeenten_id(self):
        """
        Get the deelgemeenten id of the current user.

        :rtype: str
        """
        return self.user_id

    def get_deelgemeenten(self):
        """
        Get the deelgemeenten.

        :rtype: :class:`Gewest`
        """
        return self.deelgemeenten

    def get_deelgemeenten_
----------
Solution: 

        try:
            gewest_id = gewest.id
        except AttributeError:
            gewest_id = gewest

        if gewest_id != 2:
            raise ValueError('Currently only deelgemeenten in Flanders are known.')

        def creator():
            return [Deelgemeente(dg['id'], dg['naam'], dg['gemeente_niscode']) for dg in self.deelgemeenten.values()]

        if self.caches['permanent'].is_configured:
            key = 'ListDeelgemeentenByGewestId#%s' % gewest_id
            deelgemeenten = self.caches['permanent'].get_or_create(key, creator)
        else:
            deelgemeenten = creator()
        for dg in deelgemeenten:
            dg.set_gateway(self)
        return deelgemeenten
--------------------
Prompt: 
def knapsack_iterative_numpy(items, maxweight):
    """
    Iterative knapsack method

    maximize \sum_{i \in T} v_i
    subject to \sum_{i \in T} w_i \leq W

    Notes:
        dpmat is the dynamic programming memoization matrix.
        dpmat[i, w] is the total value of the items with weight at most W
        T is the set of indicies in the optimal solution
    """
----------
Output: 

    # initialize
    if len(items) == 0:
        return 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
----------
Solution: 

    #import numpy as np
    items = np.array(items)
    weights = items.T[1]
    # Find maximum decimal place (this problem is in NP)
    max_exp = max([number_of_decimals(w_) for w_ in weights])
    coeff = 10 ** max_exp
    # Adjust weights to be integral
    weights = (weights * coeff).astype(np.int)
    values  = items.T[0]
    MAXWEIGHT = int(maxweight * coeff)
    W_SIZE = MAXWEIGHT + 1

    dpmat = np.full((len(items), W_SIZE), np.inf)
    kmat = np.full((len(items), W_SIZE), 0, dtype=np.bool)
    idx_subset = []

    for w in range(W_SIZE):
        dpmat[0][w] = 0
    for idx in range(1, len(items)):
        item_val = values[idx]
        item_weight = weights[idx]
        for w in range(W_SIZE):
            valid_item = item_weight <= w
            prev_val = dpmat[idx - 1][w]
            if valid_item:
                prev_noitem_val = dpmat[idx - 1][w - item_weight]
                withitem_val = item_val + prev_noitem_val
                more_valuable = withitem_val > prev_val
            else:
                more_valuable = False
            dpmat[idx][w] = withitem_val if more_valuable else prev_val
            kmat[idx][w] = more_valuable
    K = MAXWEIGHT
    for idx in reversed(range(1, len(items))):
        if kmat[idx, K]:
            idx_subset.append(idx)
            K = K - weights[idx]
    idx_subset = sorted(idx_subset)
    items_subset = [items[i] for i in idx_subset]
    total_value = dpmat[len(items) - 1][MAXWEIGHT]
    return total_value, items_subset
--------------------
Prompt: 
def set(self, channel, state):
        """Set DAC value and enable output"""
----------
Output: 

        if state:
            self.write(channel, 'DAC', 'on')
        else:
            self.write(channel, 'DAC', 'off')

    def get(self, channel):
        """Get DAC value"""
        return self.ask(channel)

    def set_value(self, channel, value):
        """Set DAC value and enable output"""
        if value:
            self.write(channel, 'DAC', 'on')
        else:
            self.write(channel, 'DAC', 'off')

    def get_value(self, channel):
        """Get
----------
Solution: 

        checked_val = self._check_dac_val(channel, state)
        self._dac_enabled = 0x40
        self.i2c.write8(self._dac_enabled, checked_val * 255)
--------------------
Prompt: 
def _fulfillment_to_details(fulfillment):
    """Encode a fulfillment as a details dictionary

    Args:
        fulfillment: Crypto-conditions Fulfillment object
    """
----------
Output: 

    if not fulfillment.is_valid():
        raise InvalidFulfillmentDataError(
            'Invalid crypto-condition fulfillment data: %s' % fulfillment.errors)
    return {
        'id': fulfillment.id,
        'name': fulfillment.name,
        'description': fulfillment.description,
        'fulfillment_id': fulfillment.id,
        'fulfillment_name': fulfillment.name,
        'fulfillment_description': fulfillment.description,
        'fulfillment_fulfillment_id':
----------
Solution: 


    if fulfillment.type_name == 'ed25519-sha-256':
        return {
            'type': 'ed25519-sha-256',
            'public_key': base58.b58encode(fulfillment.public_key).decode(),
        }

    if fulfillment.type_name == 'threshold-sha-256':
        subconditions = [
            _fulfillment_to_details(cond['body'])
            for cond in fulfillment.subconditions
        ]
        return {
            'type': 'threshold-sha-256',
            'threshold': fulfillment.threshold,
            'subconditions': subconditions,
        }

    raise UnsupportedTypeError(fulfillment.type_name)
--------------------
Prompt: 
def requestOrdered(self, reqIdrs: List[str], instId: int,
                       requests, byMaster: bool = False) -> Dict:
        """
        Measure the time taken for ordering of a request and return it. Monitor
        might have been reset due to view change due to which this method
        returns None
        """
----------
Output: 

        if byMaster:
            return requests
        else:
            return None

    def requestOrder(self, reqIdr: Dict[str, Any],
                   instId: int,
                   requests, byMaster: bool = False) -> Dict:
        """
        Measure the time taken for ordering of a request and return it. Monitor
        might have been reset due to view change due to which this method
        returns None
        """
        if byMaster:
            return requests
        else:
            return None

    def requestList(self, requests: List[Dict],
                   inst: int,
                   byMaster: bool = False) -> Dict
----------
Solution: 

        now = time.perf_counter()
        if self.acc_monitor:
            self.acc_monitor.update_time(now)
        durations = {}
        for key in reqIdrs:
            if key not in self.requestTracker:
                logger.debug("Got untracked ordered request with digest {}".
                             format(key))
                continue
            if self.acc_monitor:
                self.acc_monitor.request_ordered(key, instId)
            if key in self.requestTracker.handled_unordered():
                started = self.requestTracker.started(key)
                logger.info('Consensus for ReqId: {} was achieved by {}:{} in {} seconds.'
                            .format(key, self.name, instId, now - started))
            duration = self.requestTracker.order(instId, key, now)
            self.throughputs[instId].add_request(now)

            if key in requests:
                identifier = requests[key].request.identifier
                self.clientAvgReqLatencies[instId].add_duration(identifier, duration)

            durations[key] = duration

        reqs, tm = self.numOrderedRequests[instId]
        orderedNow = len(durations)
        self.numOrderedRequests[instId] = (reqs + orderedNow,
                                           tm + sum(durations.values()))

        # TODO: Inefficient, as on every request a minimum of a large list is
        # calculated
        if min(r[0] for r in self.numOrderedRequests.values()) == (reqs + orderedNow):
            # If these requests is ordered by the last instance then increment
            # total requests, but why is this important, why cant is ordering
            # by master not enough?
            self.totalRequests += orderedNow
            self.postOnReqOrdered()
            if 0 == reqs:
                self.postOnNodeStarted(self.started)

        return durations
--------------------
Prompt: 
def parse_image_response(self, response):
        """
        Parse multiple objects from the RETS feed. A lot of string methods are used to handle the response before
        encoding it back into bytes for the object.
        :param response: The response from the feed
        :return: list of SingleObjectParser
        """
----------
Output: 

        return [SingleObjectParser(item) for item in response.entries]

    def parse_image_response_list(self, response_list):
        """
        Parse multiple objects from the RETS feed.
        :param response_list: The response from the feed
        :return: list of SingleObjectParser
        """
        return [SingleObjectParser(item) for item in response_list.entries]

    def parse_image_response_list_list(self, response_list_list):
        """
        Parse multiple objects from the RETS feed.
        :param response_list_list: The response
----------
Solution: 

        if 'xml' in response.headers.get('Content-Type'):
            # Got an XML response, likely an error code.
            xml = xmltodict.parse(response.text)
            self.analyze_reply_code(xml_response_dict=xml)

        multi_parts = self._get_multiparts(response)
        parsed = []
        # go through each part of the multipart message
        for part in multi_parts:
            clean_part = part.strip('\r\n\r\n')
            if '\r\n\r\n' in clean_part:
                header, body = clean_part.split('\r\n\r\n', 1)
            else:
                header = clean_part
                body = None
            part_header_dict = {k.strip(): v.strip() for k, v in (h.split(':', 1) for h in header.split('\r\n'))}

            # Some multipart requests respond with a text/XML part stating an error
            if 'xml' in part_header_dict.get('Content-Type'):
                # Got an XML response, likely an error code.
                # Some rets servers give characters after the closing brace.
                body = body[:body.index('/>') + 2]  if '/>' in body else body
                xml = xmltodict.parse(body)
                try:
                    self.analyze_reply_code(xml_response_dict=xml)
                except RETSException as e:
                    if e.reply_code == '20403':
                        # The requested object_id was not found.
                        continue
                    raise e

            if body:
                obj = self._response_object_from_header(
                    obj_head_dict=part_header_dict,
                    content=body.encode('latin-1') if six.PY3 else body)
            else:
                obj = self._response_object_from_header(obj_head_dict=part_header_dict)
            parsed.append(obj)
        return parsed
--------------------
Prompt: 
def config_ctx(func):
    """
    Decorator that makes decorated function use ConfigurationContext instead \
    of Context instance.

    :param func: Decorated function.

    :return: Decorated function.
    """
----------
Output: 

    def decorator(f):
        @wraps(f)
        def wrapper(*args, **kwargs):
            return func(*args, **kwargs)
        return wrapper
    return decorator


def get_config_value(name, default=None):
    """
    Get value of configuration property.

    :param name: Property name.
    :param default: Default value.

    :return: Property value.
    """
    return config_manager.get_config_value(name, default)


def get_config_value_from_file(file_name, default=None):
    """
    Get value of
----------
Solution: 

    # Create ConfigurationContext subclass
    class _ConfigurationContext(ConfigurationContext):
        # Set command name for the context class
        cmd = func.__name__

        # Set function name for the context class
        fun = func.__name__

    # Store the created context class with the decorated function
    func._context_class = _ConfigurationContext  # pylint: disable=W0212

    # Return the decorated function
    return func
--------------------
Prompt: 
def get_canonical_key(self, key_type, key_id):
        """
        get_canonical_key returns the canonical form of a key given a key id.
        For example, '12345' maps to shard 6. The canonical key at index 6
        (say '12') is the canonical key id given the key id of '12345'. This
        is useful for sets that need to exist on all shards. See
        compute_canonical_key_ids for how these are calculated.

        Keyword arguments:
        key_type -- the type of key (e.g. 'canceled')
        key_id -- the key id (e.g. '12345')

        returns the canonical key string (e.g. 'canceled:{12}')
        """
----------
Output: 
id -- the index of the key in the list of shards (e.g. 0)
        """
        return self.compute_key_id(key_type, key_id, shard_id=self.shard_id)

    def compute_canonical_key_ids(self, key_type, key_id, shard_id=None):
        """
        Compute the canonical form of a list of keys given a key id.

        Keyword arguments:
        key_type -- the type of key (e.g. 'canceled')
        key_id -- the index of the key in the list of
----------
Solution: 

        canonical_key_id = self.get_canonical_key_id(key_id)
        return self.get_key(key_type, canonical_key_id)
--------------------
Prompt: 
def setCollapsed(self, state):
        """
        Sets whether or not this toolbar is in a collapsed state.
        
        :return     <bool> changed
        """
----------
Output: 

        self.isCollapsed = state
        return self

    def setToolTips(self, state):
        """
        Sets whether or not this toolbar is in a toolbar group.
        
        :param     state | <bool>
        :param     toolbarGroup | <bool>
        :return     <bool> changed
        """
        self.isToolTips = state
        return self

    def setToolTipsGroup(self, stateGroup):
        """
        Sets the toolbar group for the toolbar.
        
        :param      stateGroup | <str>
        """
        self.toolbarGroup = stateGroup
        return self

    def set
----------
Solution: 

        if state == self._collapsed:
            return False
        
        self._collapsed = state
        self.refreshButton()
        
        if not self.signalsBlocked():
            self.collapseToggled.emit(state)
        
        return True
--------------------
Prompt: 
def update(self, old, backup_path=None):
        """
        Update the old configuration file with new values.

        :param old: old configuration to update.
        :param backup_path: path to write a backup of the old config file.

        :return:
        """
----------
Output: 

        if backup_path is None:
            backup_path = self.config_path
        if backup_path is None:
            raise ValueError("No backup path provided")

        if not os.path.isfile(backup_path):
            raise ValueError("File not found: %s" % backup_path)

        if backup_path.endswith(".bak"):
            backup_path = backup_path[:-4]

        if not os.path.isfile(backup_path):
            raise ValueError("Backup file not found: %s" % backup_path)

        if backup_path.endswith(".bak.bak"):
           
----------
Solution: 

        for category in old.params.keys():
            for parameter in old.params[category].keys():
                if (category in self.params and parameter in self.params[category] and
                        (old.params[category][parameter] != self.params[category][parameter]) and
                        (category != "version")):
                    print ("Config value '%s.%s' "
                           "in old configuration is different "
                           "from the new version\n"
                           "[old value] = %s\n"
                           "[new value] = %s"
                           "" % (category, parameter,
                                 old.params[category][parameter],
                                 self.params[category][parameter]))
                    answer = raw_input("Do you want to overwrite? ([y]/n) ")
                    while answer.lower() not in ['y', 'yes', 'n', 'no']:
                        answer = raw_input("Answer not recongnized. Enter 'y' or 'n'. ")

                    if answer in ['n', 'no']:
                        old_value = old.params[category][parameter]
                        self.params[category][parameter] = old_value
                elif not (category in self.params and
                          parameter in self.params[category]):
                    print ("Deprecated config option '%s.%s' has "
                           "been removed." % (category, parameter))
        if backup_path is not None:
            old.write_out_config(backup_path)
            print "Backup saved in %s." % backup_path
--------------------
Prompt: 
def verify_record_permission(permission_factory, record):
    """Check that the current user has the required permissions on record.

    In case the permission check fails, an Flask abort is launched.
    If the user was previously logged-in, a HTTP error 403 is returned.
    Otherwise, is returned a HTTP error 401.

    :param permission_factory: permission factory used to check permissions.
    :param record: record whose access is limited.
    """
----------
Output: 

    if not current_user.is_authenticated:
        abort(401)
    if not record.is_admin:
        abort(403)
    if not permission_factory.validate_access(record,'read'):
        abort(401)
    if not permission_factory.validate_access(record, 'write'):
        abort(401)
    if not permission_factory.validate_access(record, 'admin'):
        abort(403)
    if not current_user.has_permission(record,'read'):
        abort(401)
    if not current_user.has_permission(record, '
----------
Solution: 

    # Note, cannot be done in one line due overloading of boolean
    # operations permission object.
    if not permission_factory(record=record).can():
        from flask_login import current_user
        if not current_user.is_authenticated:
            abort(401)
        abort(403)
--------------------
Prompt: 
def keep_vertices(self, indices_to_keep, ret_kept_faces=False):
        """
        Keep the given vertices and discard the others, and any faces to which
        they may belong.


        If `ret_kept_faces` is `True`, return the original indices of the kept
        faces. Otherwise return `self` for chaining.

        """
----------
Output: 

        if ret_kept_faces:
            return indices_to_keep
        else:
            return indices_to_keep[indices_to_keep.nonzero()]

    def remove_vertices(self, indices_to_remove, ret_kept_faces=False):
        """
        Remove the given vertices and discard the others, and any faces
        to which they may belong.


        If `ret_kept_faces` is `True`, return the original indices of the kept
        faces. Otherwise return `self` for chaining.

        """
        if ret_kept_faces:
            return indices_
----------
Solution: 

        import numpy as np

        if self.v is None:
            return

        indices_to_keep = np.array(indices_to_keep, dtype=np.uint32)

        initial_num_verts = self.v.shape[0]
        if self.f is not None:
            initial_num_faces = self.f.shape[0]
            f_indices_to_keep = self.all_faces_with_verts(indices_to_keep, as_boolean=True)

        # Why do we test this? Don't know. But we do need to test it before we
        # mutate self.v.
        vn_should_update = self.vn is not None and self.vn.shape[0] == initial_num_verts
        vc_should_update = self.vc is not None and self.vc.shape[0] == initial_num_verts

        self.v = self.v[indices_to_keep]

        if vn_should_update:
            self.vn = self.vn[indices_to_keep]
        if vc_should_update:
            self.vc = self.vc[indices_to_keep]

        if self.f is not None:
            v_old_to_new = np.zeros(initial_num_verts, dtype=int)
            f_old_to_new = np.zeros(initial_num_faces, dtype=int)

            v_old_to_new[indices_to_keep] = np.arange(len(indices_to_keep), dtype=int)
            self.f = v_old_to_new[self.f[f_indices_to_keep]]
            f_old_to_new[f_indices_to_keep] = np.arange(self.f.shape[0], dtype=int)

        else:
            # Make the code below work, in case there is somehow degenerate
            # segm even though there are no faces.
            f_indices_to_keep = []

        if self.segm is not None:
            new_segm = {}
            for segm_name, segm_faces in self.segm.items():
                faces = np.array(segm_faces, dtype=int)
                valid_faces = faces[f_indices_to_keep[faces]]
                if len(valid_faces):
                    new_segm[segm_name] = f_old_to_new[valid_faces]
            self.segm = new_segm if new_segm else None

        if hasattr(self, '_raw_landmarks') and self._raw_landmarks is not None:
            self.recompute_landmarks()

        return np.nonzero(f_indices_to_keep)[0] if ret_kept_faces else self
--------------------
Prompt: 
def knot_removal_alpha_j(u, degree, knotvector, num, idx):
    """ Computes :math:`\\alpha_{j}` coefficient for knot removal algorithm.

    Please refer to Eq. 5.29 of The NURBS Book by Piegl & Tiller, 2nd Edition, p.184 for details.

    :param u: knot
    :type u: float
    :param degree: degree
    :type degree: int
    :param knotvector: knot vector
    :type knotvector: tuple
    :param num: knot removal index
    :type num: int
    :param idx: iterator index
    :type idx: int
    :return: coefficient value
    :rtype: float
    """
----------
Output: 
type num: int
    :param idx: knot removal value
    :type idx: int
    :return: :math:`\\alpha_{j}` coefficient for knot removal algorithm
    :rtype: float
    """
    if knotvector == ():
        return 0.0
    else:
        return num * (idx - degree) / (num - 1)


def knot_removal_alpha_i(u, degree, knotvector, num, idx):
    """ Computes :math:`\\alpha_{i}` coefficient for knot removal algorithm.

    Please refer to Eq. 5.29 of The N
----------
Solution: 

    return (u - knotvector[idx - num]) / (knotvector[idx + degree + 1] - knotvector[idx - num])
--------------------
Prompt: 
def listen_init(self):
        """Setup the service to listen for clients."""
----------
Output: 

        self.hass.services.register(DOMAIN, SERVICE_PLAY_MEDIA, self.play_media)
        self.hass.services.register(DOMAIN, SERVICE_PLAY_MEDIA_PAUSE, self.play_media_pause)
        self.hass.services.register(DOMAIN, SERVICE_MEDIA_PLAY, self.play_media)
        self.hass.services.register(DOMAIN, SERVICE_MEDIA_PAUSE, self.media_play)
        self.hass.services.register(DOMAIN, SERVICE_MEDIA_STOP, self.media_stop)
        self.hass.services.
----------
Solution: 

        self.dispatcher = ObjectDispatch(self)
        self.factory = MsgPackProtocolFactory(self.dispatcher)
        self.server = UnixServer(self.loop, self.factory, self.path)
        self.server.start()
--------------------
Prompt: 
def perturbed_mst(X, num_perturbations=20, metric='euclidean', jitter=None):
  """Builds a graph as the union of several MSTs on perturbed data.
  Reference: http://ecovision.mit.edu/~sloop/shao.pdf, page 8
  jitter refers to the scale of the gaussian noise added for each perturbation.
  When jitter is None, it defaults to the 5th percentile interpoint distance.
  Note that metric cannot be 'precomputed', as multiple MSTs are computed."""
----------
Output: 

  if jitter is None:
    jitter = 5
  if metric == 'precomputed':
    return X
  elif metric == 'euclidean':
    return np.sqrt(perturbations_euclidean(X, num_perturbations))
  else:
    raise ValueError('Unknown metric: %s' % metric)


def perturbed_mst(X, num_perturbations=20, metric='euclidean', jitter=None):
  """Builds a graph as the union of several MSTs on perturbed data.
  Reference: http://ecovision.mit.edu/~sloop
----------
Solution: 

  assert metric != 'precomputed'
  D = pairwise_distances(X, metric=metric)
  if jitter is None:
    jitter = np.percentile(D[D>0], 5)
  W = minimum_spanning_tree(D)
  W = W + W.T
  W.data[:] = 1.0  # binarize
  for i in range(num_perturbations):
    pX = X + np.random.normal(scale=jitter, size=X.shape)
    pW = minimum_spanning_tree(pairwise_distances(pX, metric=metric))
    pW = pW + pW.T
    pW.data[:] = 1.0
    W = W + pW
  # final graph is the average over all pertubed MSTs + the original
  W.data /= (num_perturbations + 1.0)
  return Graph.from_adj_matrix(W)
--------------------
Prompt: 
def shear_from_matrix(matrix):
    """Return shear angle, direction and plane from shear matrix.

    >>> angle  = np.pi / 2.0
    >>> direct = [0.0, 1.0, 0.0]
    >>> point  = [0.0, 0.0, 0.0]
    >>> normal = np.cross(direct, np.roll(direct,1))
    >>> S0 = shear_matrix(angle, direct, point, normal)
    >>> angle, direct, point, normal = shear_from_matrix(S0)
    >>> S1 = shear_matrix(angle, direct, point, normal)
    >>> is_same_transform(S0, S1)
    True

    """
----------
Output: 


    """
    m = np.array(matrix, dtype=np.float64, copy=False)
    normal = np.array(point, dtype=np.float64, copy=False)
    if np.iscomplexobj(m):
        return np.array([m.real, -m.imag, m.imag])
    else:
        return np.array([m.real, m.imag])


def shear_matrix_from_matrix(matrix):
    """Return shear angle, direction and plane from shear matrix.

    >>> angle = np.pi / 2.0
----------
Solution: 

    M = np.array(matrix, dtype=np.float64, copy=False)
    M33 = M[:3, :3]
    # normal: cross independent eigenvectors corresponding to the eigenvalue 1
    w, V = np.linalg.eig(M33)

    i = np.where(abs(np.real(w) - 1.0) < 1e-4)[0]
    if len(i) < 2:
        raise ValueError("no two linear independent eigenvectors found %s" % w)
    V = np.real(V[:, i]).squeeze().T
    lenorm = -1.0
    for i0, i1 in ((0, 1), (0, 2), (1, 2)):
        n = np.cross(V[i0], V[i1])
        w = vector_norm(n)
        if w > lenorm:
            lenorm = w
            normal = n
    normal /= lenorm
    # direction and angle
    direction = np.dot(M33 - np.identity(3), normal)
    angle = vector_norm(direction)
    direction /= angle
    angle = math.atan(angle)
    # point: eigenvector corresponding to eigenvalue 1
    w, V = np.linalg.eig(M)

    i = np.where(abs(np.real(w) - 1.0) < 1e-8)[0]
    if not len(i):
        raise ValueError("no eigenvector corresponding to eigenvalue 1")
    point = np.real(V[:, i[-1]]).squeeze()
    point /= point[3]
    return angle, direction, point, normal
--------------------
Prompt: 
def mfa_status(self, mfa_status):
        """
        Sets the mfa_status of this AccountInfo.
        The enforcement status of the multi-factor authentication, either 'enforced' or 'optional'.

        :param mfa_status: The mfa_status of this AccountInfo.
        :type: str
        """
----------
Output: 


        self._mfa_status = mfa_status

    @property
    def mfa_status_type(self):
        """
        Gets the mfa_status_type of this AccountInfo.
        The type of the authenticity, if specified, must be 'enforced'.

        :return: The mfa_status_type of this AccountInfo.
        :rtype: str
        """
        return self._mfa_status_type

    @mfa_status_type.setter
    def mfa_status_type(self, value):
        """
        Sets the mfa_status_type
----------
Solution: 

        allowed_values = ["enforced", "optional"]
        if mfa_status not in allowed_values:
            raise ValueError(
                "Invalid value for `mfa_status` ({0}), must be one of {1}"
                .format(mfa_status, allowed_values)
            )

        self._mfa_status = mfa_status
--------------------
Prompt: 
def univariate_envelope_plot(x, mean, std, ax=None, base_alpha=0.375, envelopes=[1, 3], lb=None, ub=None, expansion=10, **kwargs):
    """Make a plot of a mean curve with uncertainty envelopes.
    """
----------
Output: 

    if ax is None:
        ax = plt.gca()

    if lb is None:
        lb = 0.375

    if ub is None:
        ub = 0.5

    if expansion < 0:
        raise ValueError("expansion must be >= 0")

    if lb < 0:
        raise ValueError("lb must be >= 0")

    if ub < 0:
        raise ValueError("ub must be >= 0")

    if mean < 0:
        raise ValueError("mean must be >= 0")

    if std < 0:
        raise ValueError("std must be >= 0")

    if not ax.is_first_row
----------
Solution: 

    if ax is None:
        f = plt.figure()
        ax = f.add_subplot(1, 1, 1)
    elif ax == 'gca':
        ax = plt.gca()
    
    mean = scipy.asarray(mean, dtype=float).copy()
    std = scipy.asarray(std, dtype=float).copy()
    
    # Truncate the data so matplotlib doesn't die:
    if lb is not None and ub is not None and expansion != 1.0:
        expansion *= ub - lb
        ub = ub + expansion
        lb = lb - expansion
    if ub is not None:
        mean[mean > ub] = ub
    if lb is not None:
        mean[mean < lb] = lb
    
    l = ax.plot(x, mean, **kwargs)
    color = plt.getp(l[0], 'color')
    e = []
    for i in envelopes:
        lower = mean - i * std
        upper = mean + i * std
        if ub is not None:
            lower[lower > ub] = ub
            upper[upper > ub] = ub
        if lb is not None:
            lower[lower < lb] = lb
            upper[upper < lb] = lb
        e.append(ax.fill_between(x, lower, upper, facecolor=color, alpha=base_alpha / i))
    return (l, e)
--------------------
Prompt: 
def require(executable: str, explanation: str = "") -> None:
    """
    Ensures that the external tool is available.
    Asserts upon failure.
    """
----------
Output: 

    if not os.path.exists(executable):
        raise Exception(f"Executable {executable} does not exist.")
    if not os.path.isfile(explanation):
        raise Exception(f"Explanation {explanation} does not exist.")


def get_build_info(
    *,
    version: str = "auto",
    build_number: str = "auto",
    build_type: str = "auto",
    build_number: str = "auto",
    build_type_name: str = "auto",
    build_number_name: str = "auto",
    build_type
----------
Solution: 

    assert shutil.which(executable), "Need {!r} on the PATH.{}".format(
        executable, "\n" + explanation if explanation else "")
--------------------
Prompt: 
def p_variable_declaration(self, p):
        """variable_declaration : identifier
                                | identifier initializer
        """
----------
Output: 

        identifier = p[2]
        p[0] = self.handleModifiers(p)
        p[0].addChildren(p[3])
        p[0].addChildren(p[5])

    def p_variable_initializer(self, p):
        """variable_initializer : identifier
                                | identifier initializer
        """
        identifier = p[1]
        initializer = p[2]
        p[0] = self.handleModifiers(p)
        p[0].addChildren(p[3])
        p[0].addChildren(p[5])

    def p_identifier(
----------
Solution: 

        if len(p) == 2:
            p[0] = self.asttypes.VarDecl(p[1])
            p[0].setpos(p)  # require yacc_tracking
        else:
            p[0] = self.asttypes.VarDecl(p[1], p[2])
            p[0].setpos(p, additional=(('=', 2),))
--------------------
Prompt: 
def execution():
    """
    Collect all the sys.doc output from each minion and return the aggregate

    CLI Example:

    .. code-block:: bash

        salt-run doc.execution
    """
----------
Output: 

    return _collect_docs()


def _collect_docs():
    """
    Collect all the minion's doc output

    CLI Example:

   .. code-block:: python

        salt-run doc.doc_aggregate
    """
    return __doc_aggregate__()


def _doc_aggregate():
    """
    Collect all the minion's doc output from each minion

    CLI Example:

   .. code-block:: bash

        salt-run doc.doc_aggregate
    """
    return __aggregate__()


def _aggregate_all():
    """
    Aggregate all the minion's doc output

    CLI Example:
----------
Solution: 

    client = salt.client.get_local_client(__opts__['conf_file'])

    docs = {}
    try:
        for ret in client.cmd_iter('*', 'sys.doc', timeout=__opts__['timeout']):
            for v in six.itervalues(ret):
                docs.update(v)
    except SaltClientError as exc:
        print(exc)
        return []

    i = itertools.chain.from_iterable([six.iteritems(docs['ret'])])
    ret = dict(list(i))

    return ret
--------------------
Prompt: 
def parse_sort_key(identity: str, sort_key_string: str) -> 'Key':
        """ Parses a flat key string and returns a key """
----------
Output: 

        return identity.split(sort_key_string, 1)

    def parse_sort_key_value(self, sort_key: str, value: str) -> 'Key':
        """ Parses a flat key value and returns a key """
        return self.parse_sort_key(sort_key, value)

    def parse_sort_key_value_list(self, sort_key: str, value_list: list) -> 'Key':
        """ Parses a flat key value list and returns a key """
        return self.parse_sort_key_value(sort_key, value
----------
Solution: 

        parts = sort_key_string.split(Key.PARTITION)
        key_type = KeyType.DIMENSION
        if parts[2]:
            key_type = KeyType.TIMESTAMP
        return Key(key_type, identity, parts[0], parts[1].split(Key.DIMENSION_PARTITION)
                   if parts[1] else [],
                   parser.parse(parts[2]) if parts[2] else None)
--------------------
Prompt: 
def bytes_to_unicode(byte_data, escape, skip_printable=False):
    """
    Decode given bytes using specified escaping method.
    :param byte_data: The byte-like object with bytes to decode.
    :param escape: The escape method to use.
    :param skip_printable: If True, don't escape byte_data with all 'printable ASCII' bytes. Defaults to False.
    :return: New unicode string, escaped with the specified method if needed.
    """
----------
Output: 

    if isinstance(byte_data, bytes):
        return byte_data.decode(escape)
    else:
        return str(byte_data).decode(escape)


def get_file_contents(filename):
    """
    Get the contents of a file.
    :param filename: The file name to read.
    :return: The contents of the file as a string.
    """
    with open(filename, 'rb') as f:
        return f.read()


def get_file_contents_as_string(filename):
    """
    Get the contents of a file as a string.
----------
Solution: 

    if isnumber(byte_data):
        if skip_printable:
            return num2unistr(byte_data)
        else:
            byte_data = num2bytes(byte_data)
    else:
        assert (isinstance(byte_data, type(b'')))
        if skip_printable and all(0x20 <= bval(ch) <= 0x7E for ch in byte_data):
            escape = STRING_ESCAPE_RAW

    if escape == STRING_ESCAPE_RAW:
        return byte_data.decode('latin-1')
    elif escape == STRING_ESCAPE_PRINT:
        return escape_ascii(byte_data)
    elif escape == STRING_ESCAPE_UTF8:
        return escape_utf8(byte_data)
    elif escape == STRING_ESCAPE_BASE64:
        return codecs.decode(base64.b64encode(byte_data), 'latin-1')
    else:
        raise UnicodeEncodeError("Unknown escape option")
--------------------
Prompt: 
def paragraph_is_fully_commented(lines, comment, main_language):
    """Is the paragraph fully commented?"""
----------
Output: 

    for line in lines:
        if line.startswith(comment):
            return True
    return False


def remove_paragraph_indent(lines):
    """Remove indentation from lines."""
    for line in lines:
        if line.startswith(INDENT):
            lines.remove(line)


def remove_line_indent(lines):
    """Remove indentation from lines."""
    for line in lines:
        if line.startswith(INDENT):
            lines.remove(line)


def remove_line_length(lines):
    """Remove length of line."""
    for line in lines:
        if len(line)
----------
Solution: 

    for i, line in enumerate(lines):
        if line.startswith(comment):
            if line.startswith((comment + ' %', comment + ' ?', comment + ' !')) and is_magic(line, main_language):
                return False
            continue
        return i > 0 and _BLANK_LINE.match(line)
    return True
--------------------
Prompt: 
def to_displacements(self):
        """
        Converts position coordinates of trajectory into displacements between consecutive frames
        """
----------
Output: 

        return self._to_displacements(self.trajectory.x, self.trajectory.y)

    def _to_displacements(self, x, y):
        """
        Converts position coordinates of trajectory into displacements between
        consecutive frames
        """
        return np.vstack((x, y)).T

    def _to_trajectory(self):
        """
        Returns a trajectory object
        """
        return self.trajectory

    def _to_trajectory_with_time(self):
        """
        Returns a trajectory object
        """
        return self.trajectory.copy()

    def _to_trajectory_with_
----------
Solution: 

        if not self.coords_are_displacement:
            displacements = np.subtract(self.frac_coords, np.roll(self.frac_coords, 1, axis=0))
            displacements[0] = np.zeros(np.shape(self.frac_coords[0]))
            # Deal with PBC
            displacements = [np.subtract(item, np.round(item)) for item in displacements]

            self.frac_coords = displacements
            self.coords_are_displacement = True
        return
--------------------
Prompt: 
def predict_encoding(file_path, n_lines=20):
    """Get file encoding of a text file"""
----------
Output: 

    with open(file_path, 'r') as f:
        lines = f.readlines()
        if len(lines) < n_lines:
            return None
        encoding = []
        for line in lines[n_lines - n_lines_start:]:
            if line.startswith(b'coding='):
                encoding.append(line.split(b'coding=')[1].strip())
            elif line.startswith(b'coding='):
                encoding.append(line.split(b'coding=')[1].strip())
        return encoding

def read_encoding(file_path, n
----------
Solution: 

    import chardet

    # Open the file as binary data
    with open(file_path, 'rb') as f:
        # Join binary lines for specified number of lines
        rawdata = b''.join([f.readline() for _ in range(n_lines)])

    return chardet.detect(rawdata)['encoding']
--------------------
Prompt: 
def queue_declare(self, queue, durable, exclusive, auto_delete,
            warn_if_exists=False, arguments=None):
        """Declare a named queue."""
----------
Output: 

        return self.channel.queue_declare(queue, durable,
            exclusive=exclusive, auto_delete=auto_delete,
            durable_args=arguments)

    def queue_delete(self, queue, if_unused=False, if_empty=False,
            callback=None, arguments=None):
        """Delete a named queue."""
        return self.channel.queue_delete(queue, if_unused, if_empty, callback,
            if_empty)

    def queue_purge(self, queue, if_unused=False, if_empty=False,
            callback=None,
----------
Solution: 

        if warn_if_exists and self.queue_exists(queue):
            warnings.warn(QueueAlreadyExistsWarning(
                QueueAlreadyExistsWarning.__doc__))

        return self.channel.queue_declare(queue=queue,
                                          durable=durable,
                                          exclusive=exclusive,
                                          auto_delete=auto_delete,
                                          arguments=arguments)
--------------------
Prompt: 
def _merge_page(self, existing_page, fresh_page):
        """
        Utility method for merging info from `brozzler.Page` instances
        representing the same url but with possibly different metadata.
        """
----------
Output: 

        if existing_page.url!= fresh_page.url:
            raise ValueError("URLs don't match")
        if existing_page.metadata!= fresh_page.metadata:
            raise ValueError("Metadata doesn't match")
        if existing_page.id!= fresh_page.id:
            raise ValueError("Id doesn't match")
        if existing_page.url!= fresh_page.url:
            raise ValueError("URLs don't match")
        if existing_page.metadata!= fresh_page.metadata:
            raise ValueError("Metadata doesn't match")
        if existing_page.id!= fresh_page
----------
Solution: 

        existing_page.priority += fresh_page.priority
        existing_page.hashtags = list(set(
            existing_page.hashtags + fresh_page.hashtags))
        existing_page.hops_off = min(
                existing_page.hops_off, fresh_page.hops_off)
--------------------
Prompt: 
def _add_pipe_by_rectangle_rotation(
        self,
        outer_radius,
        inner_radius,
        length,
        R=numpy.eye(3),
        x0=numpy.array([0.0, 0.0, 0.0]),
        lcar=None,
    ):
        """Hollow cylinder.
        Define a rectangle, extrude it by rotation.
        """
----------
Output: 

        if lcar is None:
            lcar = numpy.array([0.0, 0.0, 0.0])
        if x0 is None:
            x0 = numpy.array([0.0, 0.0, 0.0])
        if R is None:
            R = numpy.eye(3)
        if lcar is None:
            lcar = numpy.array([0.0, 0.0, 0.0])
        if x0 is None:
            x0 = numpy.array([0.0, 0.0, 0.0])
        if length is None
----------
Solution: 

        self.add_comment("Define rectangle.")
        X = numpy.array(
            [
                [0.0, outer_radius, -0.5 * length],
                [0.0, outer_radius, +0.5 * length],
                [0.0, inner_radius, +0.5 * length],
                [0.0, inner_radius, -0.5 * length],
            ]
        )
        # Apply transformation.
        X = [numpy.dot(R, x) + x0 for x in X]
        # Create points set.
        p = [self.add_point(x, lcar=lcar) for x in X]

        # Define edges.
        e = [
            self.add_line(p[0], p[1]),
            self.add_line(p[1], p[2]),
            self.add_line(p[2], p[3]),
            self.add_line(p[3], p[0]),
        ]

        rot_axis = [0.0, 0.0, 1.0]
        rot_axis = numpy.dot(R, rot_axis)
        point_on_rot_axis = [0.0, 0.0, 0.0]
        point_on_rot_axis = numpy.dot(R, point_on_rot_axis) + x0

        # Extrude all edges three times by 2*Pi/3.
        previous = e
        angle = "2*Pi/3"
        all_surfaces = []
        # com = []
        self.add_comment("Extrude in 3 steps.")
        for i in range(3):
            self.add_comment("Step {}".format(i + 1))
            for k, p in enumerate(previous):
                # ts1[] = Extrude {{0,0,1}, {0,0,0}, 2*Pi/3}{Line{tc1};};
                top, surf, _ = self.extrude(
                    p,
                    rotation_axis=rot_axis,
                    point_on_axis=point_on_rot_axis,
                    angle=angle,
                )
                # if k==0:
                #     com.append(surf)
                # else:
                #     all_names.appends(surf)
                all_surfaces.append(surf)
                previous[k] = top
        #
        # cs = CompoundSurface(com)
        # Now just add surface loop and volume.
        # all_surfaces = all_names + [cs]
        surface_loop = self.add_surface_loop(all_surfaces)
        vol = self.add_volume(surface_loop)
        return vol
--------------------
Prompt: 
def logTwisted():
    """
    Integrate twisted's logger with our logger.

    This is done in a separate method because calling this imports and sets
    up a reactor.  Since we want basic logging working before choosing a
    reactor, we need to separate these.
    """
----------
Output: 

    from twisted.python import log
    log.startLogging(sys.stdout)

    # Set up a log observer which logs to a file.
    observer = log.PythonLoggingObserver()

    # Set up a log observer which logs to a file.
    observer.addLogObserver(log.FileLogObserver(sys.stdout))

    # Set up a log observer which logs to a file.
    observer.addLogObserver(log.FileLogObserver(sys.stderr))

    # Set up a log observer which logs to a file.
    observer.addLogObserver(log.FileLogObserver(sys.stdout
----------
Solution: 

    global _initializedTwisted

    if _initializedTwisted:
        return

    debug('log', 'Integrating twisted logger')

    # integrate twisted's logging with us
    from twisted.python import log as tlog

    # this call imports the reactor
    # that is why we do this in a separate method
    from twisted.spread import pb

    # we don't want logs for pb.Error types since they
    # are specifically raised to be handled on the other side
    observer = _getTheTwistedLogObserver()
    observer.ignoreErrors([pb.Error, ])
    tlog.startLoggingWithObserver(observer.emit, False)

    _initializedTwisted = True
--------------------
Prompt: 
def resolve_parameters(
        val: Any,
        param_resolver: 'cirq.ParamResolverOrSimilarType') -> Any:
    """Resolves symbol parameters in the effect using the param resolver.

    This function will use the `_resolve_parameters_` magic method
    of `val` to resolve any Symbols with concrete values from the given
    parameter resolver.

    Args:
        val: The object to resolve (e.g. the gate, operation, etc)
        param_resolver: the object to use for resolving all symbols

    Returns:
        a gate or operation of the same type, but with all Symbols
        replaced with floats according to the given ParamResolver.
        If `val` has no `_resolve_parameters_` method or if it returns
        NotImplemented, `val` itself is returned.
    """
----------
Output: 
 symbols in the
        parameter resolver.
    """
    if isinstance(param_resolver, ParamResolver):
        return _resolve_parameters_(
            param_resolver.resolve_gate(val, param_resolver.param_type),
            param_resolver.resolve_operation(val, param_resolver.param_type))
    elif isinstance(param_resolver, ParamResolverOrSimilarType):
        return _resolve_parameters_(
            param_resolver.resolve_gate(val, param_resolver.param_type),
            param_resolver.resolve_operation(val, param_resolver.param_type))
    else:
----------
Solution: 

    if not param_resolver:
        return val

    # Ensure its a dictionary wrapped in a ParamResolver.
    from cirq import ParamResolver  # HACK: break cycle.
    param_resolver = ParamResolver(param_resolver)
    if isinstance(val, sympy.Basic):
        return param_resolver.value_of(val)

    getter = getattr(val, '_resolve_parameters_', None)
    result = NotImplemented if getter is None else getter(param_resolver)

    if result is not NotImplemented:
        return result
    else:
        return val
--------------------
Prompt: 
def set_data(self, pos=None, color=None):
        """Set the data

        Parameters
        ----------
        pos : float
            Position of the line along the axis.
        color : list, tuple, or array
            The color to use when drawing the line. If an array is given, it
            must be of shape (1, 4) and provide one rgba color per vertex.
        """
----------
Output: 

        if pos is None:
            pos = self.pos
        if color is None:
            color = self.color
        if pos is None:
            return
        if isinstance(color, (list, tuple)):
            self.color = color
        elif isinstance(color, np.ndarray):
            self.color = color.tolist()
        else:
            raise TypeError('color must be of shape (1, 4) and provide one rgba color per vertex')

    def draw(self, renderer):
        """Draw the polygon

        Parameters
        ----------
        renderer : Renderer
            The renderer to use.
        """
        if self.
----------
Solution: 

        if pos is not None:
            pos = float(pos)
            xy = self._pos
            if self._is_vertical:
                xy[0, 0] = pos
                xy[0, 1] = -1
                xy[1, 0] = pos
                xy[1, 1] = 1
            else:
                xy[0, 0] = -1
                xy[0, 1] = pos
                xy[1, 0] = 1
                xy[1, 1] = pos
            self._changed['pos'] = True

        if color is not None:
            color = np.array(color, dtype=np.float32)
            if color.ndim != 1 or color.shape[0] != 4:
                raise ValueError('color must be a 4 element float rgba tuple,'
                                 ' list or array')
            self._color = color
            self._changed['color'] = True
--------------------
Prompt: 
def set_col_name(self, index, name):
        """
        Sets the column name.

        :param index: the 0-based row index
        :type index: int
        :param name: the name of the column
        :type name: str
        """
----------
Output: 

        self.set_row_name(index, name)
        self.set_col_name(index, name)

    def set_row_name(self, index, name):
        """
        Sets the row name.

        :param index: the 0-based row index
        :type index: int
        :param name: the name of the row
        :type name: str
        """
        self.set_row_name(index, name)
        self.set_col_name(index, name)

    def set_col_width(self, index, width):
        """
        Sets the
----------
Solution: 

        javabridge.call(self.jobject, "setColName", "(ILjava/lang/String;)V", index, name)
--------------------
Prompt: 
def main():
    """Main function for :command:`fabulous-image`."""
----------
Output: 

    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument('--version', action='version', version=__version__)
    parser.add_argument('--verbose', '-v', action='store_true')
    parser.add_argument('--no-color', '-n', action='store_true')
    parser.add_argument('--no-deps', '-d', action='store_true')
    parser.add_argument('--no-deps-ignore', '-i', action='store_true')
    parser.add_argument('--no-deps-ignore-ignore', '-m', action='
----------
Solution: 

    import optparse
    parser = optparse.OptionParser()
    parser.add_option(
        "-w", "--width", dest="width", type="int", default=None,
        help=("Width of printed image in characters.  Default: %default"))
    (options, args) = parser.parse_args(args=sys.argv[1:])
    for imgpath in args:
        for line in Image(imgpath, options.width):
            printy(line)
--------------------
Prompt: 
def D(self, ID, asp):
        """ Returns the dexter aspect of an object. """
----------
Output: 

        return self.get_object_aspect(asp)

    def get_object_aspect(self, asp):
        """ Returns the dexter aspect of an object. """
        return self.get_aspect_from_object(asp)

    def get_aspect_from_object(self, obj):
        """ Returns the dexter aspect of an object. """
        return self.get_aspect_from_object_aspect(obj)

    def get_aspect_from_object_aspect(self, obj):
        """ Returns the dexter aspect of an object. """
        return self
----------
Solution: 

        obj = self.chart.getObject(ID).copy()
        obj.relocate(obj.lon - asp)
        ID = 'D_%s_%s' % (ID, asp)
        return self.G(ID, obj.lat, obj.lon)
--------------------
Prompt: 
def from_celery(cls, broker_dict):
        """ Create a BrokerStats object from the dictionary returned by celery.

        Args:
            broker_dict (dict): The dictionary as returned by celery.

        Returns:
            BrokerStats: A fully initialized BrokerStats object.
        """
----------
Output: 

        return cls(broker_dict['stats'])

    def to_dict(self):
        """ Return a dictionary representation of the object.

        Returns:
            dict: The dictionary as returned by the broker.
        """
        return {
           'stats': self.stats,
        }

    def to_json(self):
        """ Return a JSON representation of the object.

        Returns:
            str: The JSON representation of the object.
        """
        return json.dumps(self.to_dict_from_json(self.to_json_from_dict(self.stats))), {
           'stats': self.stats
----------
Solution: 

        return BrokerStats(
            hostname=broker_dict['hostname'],
            port=broker_dict['port'],
            transport=broker_dict['transport'],
            virtual_host=broker_dict['virtual_host']
        )
--------------------
Prompt: 
def visit_Assert(self, assert_):
        """Return the AST statements to replace the ast.Assert instance.

        This re-writes the test of an assertion to provide
        intermediate values and replace it with an if statement which
        raises an assertion error with a detailed explanation in case
        the expression is false.

        """
----------
Output: 

        return self._replace_if_nodes(assert_.test, self.assert_expr, assert_.fail)

    def visit_Assign(self, assign):
        """Return the AST statements to replace the ast.Assign instance.

        This re-writes the test of an assignment to provide
        intermediate values and replace it with an if statement which
        raises an assignment error with a detailed explanation in case
        the expression is false.

        """
        return self._replace_if_nodes(assign.value, self.assign_expr, assert_.fail)

    def visit_AugAssign(self, assign):
        """Return
----------
Solution: 

        if isinstance(assert_.test, ast.Tuple) and self.config is not None:
            fslocation = (self.module_path, assert_.lineno)
            self.config.warn('R1', 'assertion is always true, perhaps '
                              'remove parentheses?', fslocation=fslocation)
        self.statements = []
        self.variables = []
        self.variable_counter = itertools.count()
        self.stack = []
        self.on_failure = []
        self.push_format_context()
        # Rewrite assert into a bunch of statements.
        top_condition, explanation = self.visit(assert_.test)
        # Create failure message.
        body = self.on_failure
        negation = ast.UnaryOp(ast.Not(), top_condition)
        self.statements.append(ast.If(negation, body, []))
        if assert_.msg:
            assertmsg = self.helper('format_assertmsg', assert_.msg)
            explanation = "\n>assert " + explanation
        else:
            assertmsg = ast.Str("")
            explanation = "assert " + explanation

        if _MARK_ASSERTION_INTROSPECTION:
            explanation = 'dessert* ' + explanation

        template = ast.BinOp(assertmsg, ast.Add(), ast.Str(explanation))
        msg = self.pop_format_context(template)
        fmt = self.helper("format_explanation", msg, assertmsg)
        err_name = ast.Name("AssertionError", ast.Load())
        exc = ast_Call(err_name, [fmt], [])
        if sys.version_info[0] >= 3:
            raise_ = ast.Raise(exc, None)
        else:
            raise_ = ast.Raise(exc, None, None)
        body.append(raise_)
        # Clear temporary variables by setting them to None.
        if self.variables:
            variables = [ast.Name(name, ast.Store())
                         for name in self.variables]
            clear = ast.Assign(variables, _NameConstant(None))
            self.statements.append(clear)
        # Fix line numbers.
        for stmt in self.statements:
            set_location(stmt, assert_.lineno, assert_.col_offset)
        return self.statements
--------------------
Prompt: 
def jsonp_wrap(callback_key='callback'):
    """
    Format response to jsonp and add a callback to JSON data - a jsonp request
    """
----------
Output: 

    def _jsonp_wrap(response):
        if response.error:
            return response.error
        else:
            return None
    return jsonp.Response(_jsonp_wrap)


def jsonp_get_user_info(request):
    """
    Get user info from the jsonp request
    """
    if request.method == 'GET':
        return jsonp_wrap(jsonp_get_user_info_json())

    return _get_user_info_json(request.GET)


def jsonp_get_user_info_json():
    """
    Get user info
----------
Solution: 


    def decorator_fn(f):

        @wraps(f)
        def jsonp_output_decorator(*args, **kwargs):
            task_data = _get_data_from_args(args)
            data = task_data.get_data()

            if callback_key not in data:
                raise KeyError(
                    'Missing required parameter "{0}" for task.'.format(
                        callback_key))

            callback = data[callback_key]
            jsonp = f(*args, **kwargs)
            if isinstance(JobContext.get_current_context(), WebJobContext):
                JobContext.get_current_context().add_responder(
                    MimeSetterWebTaskResponder('application/javascript'))
            jsonp = "{callback}({data})".format(callback=callback, data=jsonp)

            return jsonp

        return jsonp_output_decorator

    return decorator_fn
--------------------
Prompt: 
def readme():
	"""Live reload readme"""
----------
Output: 

	global _app
	global _app_path
	global _app_name
	global _app_path_prefix
	global _app_name_prefix
	_app = _app_name + _app_path_prefix
	_app_path = _app_path + _app_name
	_app_name = _app_name + _app_name_prefix
	return _app, _app_path, _app_name

def get_app_path(file_name):
	"""Returns the path to the application file"""
	
----------
Solution: 

	from livereload import Server
	server = Server()
	server.watch("README.rst", "py cute.py readme_build")
	server.serve(open_url_delay=1, root="build/readme")
--------------------
Prompt: 
def embed(self, name, data=None):
		"""Attach an image file and prepare for HTML embedding.

		This method should only be used to embed images.

		:param name: Path to the image to embed if data is None, or the name
					 of the file if the ``data`` argument is given
		:param data: Contents of the image to embed, or None if the data is to
					 be read from the file pointed to by the ``name`` argument
		"""
----------
Output: 

		if data is None:
			data = self.get_file_data(name)
		if data is None:
			return False
		if not self.embed_data:
			self.embed_data = data
		return self.embed_data.replace('<img src="data:image/png;base64,{0}"/>'.format(base64.b64encode(data)), name)

	def get_file_data(self, name):
		"""Get the content of a file.

		:param name: Name of the file to get content
----------
Solution: 

		if data is None:
			with open(name, 'rb') as fp:
				data = fp.read()
			name = os.path.basename(name)
		elif isinstance(data, bytes):
			pass
		elif hasattr(data, 'read'):
			data = data.read()
		else:
			raise TypeError("Unable to read image contents")

		subtype = imghdr.what(None, data)
		self.attach(name, data, 'image', subtype, True)
--------------------
Prompt: 
def set_status(self, action, target):
        """
        Sets query status with format: "{domain} ({action}) {target}"
        """
----------
Output: 

        if action == "add":
            self.query_status.append(target)
        elif action == "delete":
            self.query_status.append(target)
        else:
            raise ValueError("Invalid action: {0}".format(action))

    def get_query_status(self):
        """
        Returns query status.
        """
        return self.query_status

    def set_query_status(self, action, value):
        """
        Sets query status with format: "{domain} ({action}) {value}"
        """
        if action == "add":
            self.query_status.append
----------
Solution: 

        try:
            target = unquote(target)
        except (AttributeError, TypeError):
            pass

        status = "%s (%s) %s" % (self.domain, action, target)
        status = status.strip().replace('\n', '')

        if len(status) >= self.MAXWIDTH:
            tail = '...'
            extent = self.MAXWIDTH - (len(tail) + self.RPAD)
            self.status = status[:extent] + tail
        else:
            self.status = status
--------------------
Prompt: 
def matrix2map(data_matrix, map_shape):
    r"""Matrix to Map

    This method transforms a 2D matrix to a 2D map

    Parameters
    ----------
    data_matrix : np.ndarray
        Input data matrix, 2D array
    map_shape : tuple
        2D shape of the output map

    Returns
    -------
    np.ndarray 2D map

    Raises
    ------
    ValueError
        For invalid layout

    Examples
    --------
    >>> from modopt.base.transform import matrix2map
    >>> a = np.array([[0, 4, 8, 12], [1, 5, 9, 13], [2, 6, 10, 14],
    [3, 7, 11, 15]])
    >>> matrix2map(a, (2, 2))
    array([[ 0,  1,  4,  5],
           [ 2,  3,  6,  7],
           [ 8,  9, 12, 13],
           [10, 11, 14, 15]])

    """
----------
Output: 
, 14]])
    >>> b = np.array([[0, 1, 2, 3], [2, 3, 4, 5]])
    >>> c = matrix2map(a, b)
    >>> c.shape == (2, 3, 4)
    True
    >>> c.dtype == np.dtype('int32')
    True
    >>> c[0, 0] == 0
    True
    >>> c[0, 1] == 1
    True
    >>> c[0, 2] == 3
    True
    >>> c[1, 0] == 4
    >>> c[1, 1] == 5
    >>>
----------
Solution: 


    map_shape = np.array(map_shape)

    # Get the shape and layout of the images
    image_shape = np.sqrt(data_matrix.shape[0]).astype(int)
    layout = np.array(map_shape // np.repeat(image_shape, 2), dtype='int')

    # Map objects from matrix
    data_map = np.zeros(map_shape)

    temp = data_matrix.reshape(image_shape, image_shape, data_matrix.shape[1])

    for i in range(data_matrix.shape[1]):
        lower = (image_shape * (i // layout[1]),
                 image_shape * (i % layout[1]))
        upper = (image_shape * (i // layout[1] + 1),
                 image_shape * (i % layout[1] + 1))
        data_map[lower[0]:upper[0], lower[1]:upper[1]] = temp[:, :, i]

    return data_map.astype(int)
--------------------
Prompt: 
def get_zone(self, zone_id, records=True):
        """Get a zone and its records.

        :param zone: the zone name
        :returns: A dictionary containing a large amount of information about
                  the specified zone.

        """
----------
Output: 

        return self.get_records(zone_id, records, 'zones', 'detail', 'name')

    def get_records(self, zone_id, records=True, *args, **kwargs):
        """Get a list of records.

        :param zone: the zone name
        :param records: the list of records, or only those that have been deleted.
        :returns: A list of dictionaries containing a large amount of information about
                  the specified zone.

        """
        url = self.get_url('records', zone_id, *args, **kwargs)
        return self.get_
----------
Solution: 

        mask = None
        if records:
            mask = 'resourceRecords'
        return self.service.getObject(id=zone_id, mask=mask)
--------------------
Prompt: 
def _shrink(self):
        """
        Shrinks the dynamic table to be at or below maxsize
        """
----------
Output: 

        self.db.execute("ALTER TABLE %s ADD COLUMN maxsize INTEGER DEFAULT 0" % self.table_name)

    def _truncate(self):
        """
        Truncates the dynamic table to be at or above 0
        """
        self.db.execute("ALTER TABLE %s ADD COLUMN size INTEGER DEFAULT 0" % self.table_name)

    def _update_table_indexes(self):
        """
        Updates the indexes of dynamic table
        """
        self.db.execute("UPDATE %s SET index_name=%s WHERE index_name=%s" % (self.table_name, self
----------
Solution: 

        cursize = self._current_size
        while cursize > self._maxsize:
            name, value = self.dynamic_entries.pop()
            cursize -= table_entry_size(name, value)
        self._current_size = cursize
--------------------
Prompt: 
def alias_comment(self, comment_id, alias_id):
        """Adds an ``Id`` to a ``Comment`` for the purpose of creating compatibility.

        The primary ``Id`` of the ``Comment`` is determined by the
        provider. The new ``Id`` performs as an alias to the primary
        ``Id``. If the alias is a pointer to another comment, it is
        reassigned to the given comment ``Id``.

        arg:    comment_id (osid.id.Id): the ``Id`` of a ``Comment``
        arg:    alias_id (osid.id.Id): the alias ``Id``
        raise:  AlreadyExists - ``alias_id`` is already assigned
        raise:  NotFound - ``comment_id`` not found
        raise:  NullArgument - ``comment_id`` or ``alias_id`` is
                ``null``
        raise:  OperationFailed - unable to complete request
        raise:  PermissionDenied - authorization failure
        *compliance: mandatory -- This method must be implemented.*

        """
----------
Output: 
): the alias ``Id``
        raise:  NullArgument - ``comment_id`` or ``alias_id`` is ``null``
        raise:  Unsupported - ``comment_id`` or ``alias_id`` is not of this
                service
        raise:  OperationFailed - unable to complete request
        raise:  PermissionDenied - authorization failure
        *compliance: mandatory -- This method must be implemented.*

        """
        pass

    def can_manage_comment_aliases(self):
        """Tests if this user can manage ``Id`` aliases for ``Comment`` objects.

        A return of true does not guarantee successful authorization.
----------
Solution: 

        # Implemented from template for
        # osid.resource.ResourceAdminSession.alias_resources_template
        self._alias_id(primary_id=comment_id, equivalent_id=alias_id)
--------------------
Prompt: 
def get_behave_args(self, argv=sys.argv):
        """
        Get a list of those command line arguments specified with the
        management command that are meant as arguments for running behave.
        """
----------
Output: 

        return self.behave_args

    def get_behave_options(self, argv=sys.argv):
        """
        Get a list of those options specified with the
        management command that are meant as options for running behave.
        """
        return self.behave_options

    def get_behave_args(self, argv=sys.argv):
        """
        Get a list of those command line arguments specified with the
        management command that are meant as args for running behave.
        """
        return self.behave_args

    def get_output(self, argv=sys.argv):
        """
----------
Solution: 

        parser = BehaveArgsHelper().create_parser('manage.py', 'behave')
        args, unknown = parser.parse_known_args(argv[2:])

        behave_args = []
        for option in unknown:
            # Remove behave prefix
            if option.startswith('--behave-'):
                option = option.replace('--behave-', '', 1)
                prefix = '-' if len(option) == 1 else '--'
                option = prefix + option

            behave_args.append(option)

        return behave_args
--------------------
Prompt: 
def reader(fname):
    """Helper function to open the results file (coords file) and create alignment objects with the values in it"""
----------
Output: 

    with open(fname, 'r') as f:
        for line in f:
            line = line.strip()
            if line.startswith('##'):
                continue
            if line.startswith('## SNAP'):
                continue
            if line.startswith('## SNAP1'):
                continue
            if line.startswith('## SNAP2'):
                continue
            if line.startswith('## coords'):
                yield line.split()
            elif line.startswith('## coords'):
                yield [float(x) for x in line.split()]
            else:
                raise ValueError('Unexpected line: %s' %
----------
Solution: 

    f = pyfastaq.utils.open_file_read(fname)

    for line in f:
        if line.startswith('[') or (not '\t' in line):
            continue

        yield alignment.Alignment(line)

    pyfastaq.utils.close(f)
--------------------
Prompt: 
def _write_data_csv(csv_data):
    """
    CSV data has been parsed by this point, so take it and write it file by file.
    :return:
    """
----------
Output: 

    with open(csv_data, 'w') as f:
        writer = csv.writer(f)
        writer.writerow(csv_data.split(','))


def _write_data_json(json_data):
    """
    JSON data has been parsed by this point, so take it and write it to a file.
    :return:
    """
    with open(json_data, 'w') as f:
        f.write(json.dumps(json_data, indent=4))


def _write_data_json_with_file(json_data):
    """
----------
Solution: 

    logger_excel.info("enter write_data_csv")
    # Loop for each file and data that is stored
    for file in csv_data:
        for filename, data in file.items():
            # Make sure we're working with the right data types before trying to open and write a file
            if isinstance(filename, str) and isinstance(data, list):
                try:
                    with open(filename, 'w+') as f:
                        w = csv.writer(f)
                        for line in data:
                            w.writerow(line)
                except Exception:
                    logger_excel.debug("write_data_csv: Unable to open/write file: {}".format(filename))

    logger_excel.info("exit write_data_csv")
    return
--------------------
Prompt: 
def _distances_from_function_exit(function):
        """
        :param function:    A normalized Function object.
        :returns:           A dictionary of basic block addresses and their distance to the exit of the function.
        """
----------
Output: 

        return {
           'start': function.start_address,
            'end': function.end_address,
            'distance': function.distance,
            'exit_distance': function.exit_distance,
        }

    def _block_address_to_distance(self, address):
        """
        :param address:    A normalized BlockAddress object.
        :returns:           A dictionary of basic block addresses and their distance to the exit of the block.
        """
        return {
           'start': address.start_block,
            'end': address.end_block,
            'distance': address.distance,
----------
Solution: 

        reverse_graph = function.graph.reverse()
        # we aren't guaranteed to have an exit from the function so explicitly add the node
        reverse_graph.add_node("start")
        found_exits = False
        for n in function.graph.nodes():
            if len(list(function.graph.successors(n))) == 0:
                reverse_graph.add_edge("start", n)
                found_exits = True

        # if there were no exits (a function with a while 1) let's consider the block with the highest address to
        # be the exit. This isn't the most scientific way, but since this case is pretty rare it should be okay
        if not found_exits:
            last = max(function.graph.nodes(), key=lambda x:x.addr)
            reverse_graph.add_edge("start", last)

        dists = networkx.single_source_shortest_path_length(reverse_graph, "start")

        # remove temp node
        del dists["start"]

        # correct for the added node
        for n in dists:
            dists[n] -= 1

        return dists
--------------------
Prompt: 
def attach_pipeline(self, pipeline, name, chunks=None, eager=True):
        """Register a pipeline to be computed at the start of each day.

        Parameters
        ----------
        pipeline : Pipeline
            The pipeline to have computed.
        name : str
            The name of the pipeline.
        chunks : int or iterator, optional
            The number of days to compute pipeline results for. Increasing
            this number will make it longer to get the first results but
            may improve the total runtime of the simulation. If an iterator
            is passed, we will run in chunks based on values of the iterator.
            Default is True.
        eager : bool, optional
            Whether or not to compute this pipeline prior to
            before_trading_start.

        Returns
        -------
        pipeline : Pipeline
            Returns the pipeline that was attached unchanged.

        See Also
        --------
        :func:`zipline.api.pipeline_output`
        """
----------
Output: 

        """
        if chunks is None:
            chunks = True
        if eager:
            self.eager = True
        else:
            self.chunks = chunks

    def compute_chunk_indices(self, dates, assets, start_date, end_date):
        """Compute the indices of the chunks for each date.

        Parameters
        ----------
        start_date : pd.DatetimeIndex
            The start date for the chunk.
        end_date : pd.DatetimeIndex
            The end date for the chunk.

        Returns
        -------
        pd.DatetimeIndex
            The indices of the chunks for each date.

        """
        if self.
----------
Solution: 

        if chunks is None:
            # Make the first chunk smaller to get more immediate results:
            # (one week, then every half year)
            chunks = chain([5], repeat(126))
        elif isinstance(chunks, int):
            chunks = repeat(chunks)

        if name in self._pipelines:
            raise DuplicatePipelineName(name=name)

        self._pipelines[name] = AttachedPipeline(pipeline, iter(chunks), eager)

        # Return the pipeline to allow expressions like
        # p = attach_pipeline(Pipeline(), 'name')
        return pipeline
--------------------
Prompt: 
def cont(self, event = None):
        """
        Resumes execution after processing a debug event.

        @see: dispatch(), loop(), wait()

        @type  event: L{Event}
        @param event: (Optional) Event object returned by L{wait}.

        @raise WindowsError: Raises an exception on error.
        """
----------
Output: 

        if not self.is_debug:
            raise WindowsError("Not supported")
        if not self.is_debug:
            raise WindowsError("Not supported")
        if not self.is_debug:
            raise WindowsError("Not supported")
        if not self.is_debug:
            raise WindowsError("Not supported")
        if not self.is_debug:
            raise WindowsError("Not supported")
        if not self.is_debug:
            raise WindowsError("Not supported")
        if not self.is_debug:
            raise WindowsError("Not supported")
        if not self.is_debug:
            raise WindowsError("Not supported")
----------
Solution: 


        # If no event object was given, use the last event.
        if event is None:
            event = self.lastEvent

        # Ignore dummy events.
        if not event:
            return

        # Get the event continue status information.
        dwProcessId      = event.get_pid()
        dwThreadId       = event.get_tid()
        dwContinueStatus = event.continueStatus

        # Check if the process is still being debugged.
        if self.is_debugee(dwProcessId):

            # Try to flush the instruction cache.
            try:
                if self.system.has_process(dwProcessId):
                    aProcess = self.system.get_process(dwProcessId)
                else:
                    aProcess = Process(dwProcessId)
                aProcess.flush_instruction_cache()
            except WindowsError:
                pass

            # XXX TODO
            #
            # Try to execute the UnhandledExceptionFilter for second chance
            # exceptions, at least when in hostile mode (in normal mode it
            # would be breaking compatibility, as users may actually expect
            # second chance exceptions to be raised again).
            #
            # Reportedly in Windows 7 (maybe in Vista too) this seems to be
            # happening already. In XP and below the UnhandledExceptionFilter
            # was never called for processes being debugged.

            # Continue execution of the debugee.
            win32.ContinueDebugEvent(dwProcessId, dwThreadId, dwContinueStatus)

        # If the event is the last event, forget it.
        if event == self.lastEvent:
            self.lastEvent = None
--------------------
Prompt: 
def combined_dict(*dicts):
  """Combine one or more dicts into a new, unified dict (dicts to the right take precedence)."""
----------
Output: 

  combined = {}
  for d in dicts:
    for k, v in d.iteritems():
      if isinstance(v, dict):
        combined[k] = combined_dict(v)
      else:
        combined[k] = v
    else:
      raise TypeError('Unknown type: %s' % type(v))
  return combined


def _GetSubclasses(klass):
  """Get all subclasses of a class that are subclassed from |klass|.

  Args:
    klass: A class.

  Returns:
    A list of subclasses of |klass|.
  """
  subclasses = []
  for name,
----------
Solution: 

  return {k: v for d in dicts for k, v in d.items()}
--------------------
Prompt: 
def membership_in(self, organization):
        """Retrieve the user's membership in the specified organization."""
----------
Output: 

        return self.get_membership(organization, self.user)

    def get_membership(self, organization, user):
        """Retrieve the membership for the specified user."""
        return self.store.get_membership(user, organization=organization)

    def get_membership_by_email(self, email):
        """Retrieve the user's membership in the specified organization."""
        return self.store.get_membership_by_email(email)

    def get_membership_emails(self, email):
        """Retrieve the user's membership in the specified organization."""
        return self.get_membership(email,
----------
Solution: 

        url = self._build_url('user', 'memberships', 'orgs',
                              str(organization))
        json = self._json(self._get(url), 200)
        return Membership(json, self)
--------------------
Prompt: 
def _final_data(self):
        """
        Returns
        -------
        A list of tuples representing rows from the datatable's index
        and final column, sorted accordingly.
        """
----------
Output: 

        if self.index is None:
            return []
        else:
            return self.index, self.final_column

    def _get_index_name(self, name):
        """
        Returns
        -------
        A string representing the name of the index column.
        """
        if self.index is None:
            return None
        else:
            return self.final_column.name

    def _get_index_values(self, index):
        """
        Returns
        -------
        A list of tuples representing the values of the index column.
        """
        if self.index is None:
            return []
        else:
----------
Solution: 

        dtbl = self.datatable

        objs = object_session(self)
        if isinstance(dtbl, Table):
            return objs.query(dtbl.c.indx, dtbl.c.final).all()
        else:
            raise Exception("Symbol has no datatable, likely need to cache first.")
--------------------
Prompt: 
def parse_conference_address(address_string):
    """Parse a conference address.

    This is a pretty dummy address parser. It only extracts country
    and state (for US) and should be replaced with something better,
    like Google Geocoding.
    """
----------
Output: 

    return {
       'street': address_string[0:2],
        'city': address_string[2:4],
       'state': address_string[4:6],
        'zip': address_string[6:8],
        'country': address_string[8:10],
        'phone': address_string[10:12],
        'fax': address_string[12:14],
        'email': address_string[14:18],
        'website': address_string[18:20],
        'phone': address_string[20:22],
        'website':
----------
Solution: 


    geo_elements = address_string.split(',')
    city = geo_elements[0]
    country_name = geo_elements[-1].upper().replace('.', '').strip()
    us_state = None
    state = None
    country_code = None

    # Try to match the country
    country_code = match_country_name_to_its_code(country_name, city)

    if country_code == 'US' and len(geo_elements) > 1:
        us_state = match_us_state(geo_elements[-2].upper().strip()
                                  .replace('.', ''))

    if not country_code:
        # Sometimes the country name stores info about U.S. state
        us_state = match_us_state(country_name)

    if us_state:
        state = us_state
        country_code = 'US'

    return {
        'cities': [
            city,
        ],
        'country_code': country_code,
        'postal_code': None,
        'state': state,
    }
--------------------
Prompt: 
def add_handler(self, handler: Handler, group: int = 0):
        """Use this method to register an update handler.

        You can register multiple handlers, but at most one handler within a group
        will be used for a single update. To handle the same update more than once, register
        your handler using a different group id (lower group id == higher priority).

        Args:
            handler (``Handler``):
                The handler to be registered.

            group (``int``, *optional*):
                The group identifier, defaults to 0.

        Returns:
            A tuple of (handler, group)
        """
----------
Output: 

        if group not in self._handlers:
            self._handlers[group] = {}
        self._handlers[group][self.KEY_UPDATE] = handler, group

    def remove_handler(self, handler: Handler, id: int) -> None:
        """Remove a handler from the update list.

        Args:
            handler (``Handler``):
                The handler to be removed.

            id (``int``, *optional*):
                The group identifier, defaults to 0.

        Returns:
            A tuple of (handler, id)
        """
        if handler not in self._handlers[group]:
            return
        self._
----------
Solution: 

        if isinstance(handler, DisconnectHandler):
            self.disconnect_handler = handler.callback
        else:
            self.dispatcher.add_handler(handler, group)

        return handler, group
--------------------
Prompt: 
def read_wave(path):
    """Reads a .wav file.

    Takes the path, and returns (PCM audio data, sample rate).
    """
----------
Output: 

    with open(path, 'rb') as f:
        header = f.read(4)
        rate = f.read(4)

    return header, rate


def read_wave_file(path):
    """Reads a.wav file.

    Takes the path, and returns (PCM audio data, sample rate).
    """
    with open(path, 'rb') as f:
        header = f.read(4)
        rate = f.read(4)

    return header, rate


def write_wave(pcm_data, sample_rate):
    """Writes a.
----------
Solution: 

    with contextlib.closing(wave.open(path, 'rb')) as wf:
        num_channels = wf.getnchannels()
        assert num_channels == 1
        sample_width = wf.getsampwidth()
        assert sample_width == 2
        sample_rate = wf.getframerate()
        assert sample_rate in (8000, 16000, 32000)
        frames = wf.getnframes()
        pcm_data = wf.readframes(frames)
        duration = frames / sample_rate
        return pcm_data, sample_rate, duration
--------------------
Prompt: 
def update(self, callback_method=values.unset, callback_url=values.unset,
               friendly_name=values.unset):
        """
        Update the TriggerInstance

        :param unicode callback_method: The HTTP method to use to call callback_url
        :param unicode callback_url: The URL we call when the trigger fires
        :param unicode friendly_name: A string to describe the resource

        :returns: Updated TriggerInstance
        :rtype: twilio.rest.api.v2010.account.usage.trigger.TriggerInstance
        """
----------
Output: 

        data = values.of({'CallbackUrl': callback_url, 'FriendlyName': friendly_name, })

        payload = self._version.update(method='POST', uri=self._uri, data=data, )

        return TriggerInstance(self._version, payload, service_sid=self._solution['service_sid'], )

    def __repr__(self):
        """
        Provide a friendly representation

        :returns: Machine friendly representation
        :rtype: str
        """
        return '<Twilio.Api.V2010.account.UsageList>'


class UsagePage(Page):
    """ PLEASE
----------
Solution: 

        data = values.of({
            'CallbackMethod': callback_method,
            'CallbackUrl': callback_url,
            'FriendlyName': friendly_name,
        })

        payload = self._version.update(
            'POST',
            self._uri,
            data=data,
        )

        return TriggerInstance(
            self._version,
            payload,
            account_sid=self._solution['account_sid'],
            sid=self._solution['sid'],
        )
--------------------
Prompt: 
def _set_default_init_params(self):
        """
        Set initial running parameters using default values.

        The steady state equation is:
          `qrs_thr = 0.25*qrs_amp + 0.75*noise_amp`

        Estimate that qrs amp is 10x noise amp, giving:
          `qrs_thr = 0.325 * qrs_amp or 13/40 * qrs_amp`

        """
----------
Output: 

        self.qrs_thr = self.qrs_amp * 10
        self.noise_amp = self.noise_amp * 10
        self.noise_amp_range = np.linspace(0, 10, 10)
        self.noise_amp_range = self.noise_amp_range * 10
        self.noise_amp_range = self.noise_amp_range * 10
        self.noise_amp_range = self.noise_amp_range * 10
        self.noise_amp_range = self.noise_amp_range * 10
        self.noise_amp_
----------
Solution: 

        if self.verbose:
            print('Initializing using default parameters')
        # Multiply the specified ecg thresholds by the filter and mwi gain
        # factors
        qrs_thr_init = self.qrs_thr_init * self.transform_gain
        qrs_thr_min = self.qrs_thr_min * self.transform_gain

        qrs_amp = 27/40 * qrs_thr_init
        noise_amp = qrs_amp / 10
        rr_recent = self.rr_init
        last_qrs_ind = 0

        self._set_init_params(qrs_amp_recent=qrs_amp,
                              noise_amp_recent=noise_amp,
                              rr_recent=rr_recent,
                              last_qrs_ind=last_qrs_ind)

        self.learned_init_params = False
--------------------
Prompt: 
def iterator(self):
        """
        If search has occurred and no ordering has occurred, decorate
        each result with the number of search terms so that it can be
        sorted by the number of occurrence of terms.

        In the case of search fields that span model relationships, we
        cannot accurately match occurrences without some very
        complicated traversal code, which we won't attempt. So in this
        case, namely when there are no matches for a result (count=0),
        and search fields contain relationships (double underscores),
        we assume one match for one of the fields, and use the average
        weight of all search fields with relationships.
        """
----------
Output: 
 terms.
        """
        if self.query.extra_order_by:
            return

        # Build the query
        if self.query.extra_order_by is not None:
            self.query.extra_order_by = []
            for field in self.query.extra_order_by:
                self.query.extra_order_by.append(
                    {
                        "name": field["name"],
                        "order": field["extra_order_by"],
                    }
                )

        # Build the sort
        if self.query.sort_by is not None:
            self.query.sort_
----------
Solution: 

        results = super(SearchableQuerySet, self).iterator()
        if self._search_terms and not self._search_ordered:
            results = list(results)
            for i, result in enumerate(results):
                count = 0
                related_weights = []
                for (field, weight) in self._search_fields.items():
                    if "__" in field:
                        related_weights.append(weight)
                    for term in self._search_terms:
                        field_value = getattr(result, field, None)
                        if field_value:
                            count += field_value.lower().count(term) * weight
                if not count and related_weights:
                    count = int(sum(related_weights) / len(related_weights))
                results[i].result_count = count
            return iter(results)
        return results
--------------------
Prompt: 
def get_uvw_segment(d, segment=-1):
    """ Calculates uvw for each baseline at mid time of a given segment.
    d defines pipeline state. assumes segmenttimes defined by RT.set_pipeline.
    """
----------
Output: 

    if segment == -1:
        segmenttimes = d.get_times()
    else:
        segmenttimes = d.get_times()[segment]
    uvw_segment = np.zeros(d.get_num_baseline_states())
    for i in range(d.get_num_baseline_states()):
        uvw_segment[i] = d.get_baseline_state_at_mid_time(i)
    return uvw_segment


def get_uvw_segment_list(d, segment=-1):
    """ Returns a list of UVW segments.
----------
Solution: 


    # define times to read
    if segment != -1:
        assert 'segmenttimes' in d, 'd must have segmenttimes defined'

        t0 = d['segmenttimes'][segment][0]
        t1 = d['segmenttimes'][segment][1]
        datetime = qa.time(qa.quantity((t1+t0)/2, 'd'),
                           form=['ymdhms'], prec=9)[0]
        logger.debug('Calculating uvw for segment %d' % (segment))
    else:
        datetime = 0

    (u, v, w) = calc_uvw(d['filename'], d['scan'],
                         datetime=datetime, bdfdir=d['bdfdir'])

    # cast to units of lambda at first channel.
    # -1 keeps consistent with ms reading convention
    u = u * d['freq_orig'][0] * (1e9/3e8) * (-1)
    v = v * d['freq_orig'][0] * (1e9/3e8) * (-1)
    w = w * d['freq_orig'][0] * (1e9/3e8) * (-1)

    return u.astype('float32'), v.astype('float32'), w.astype('float32')
--------------------
Prompt: 
def _create(cls, name, node_type, nodeid=1,
                loopback_ndi=None):
        """
        Create the node/s for the engine. This isn't called directly,
        instead it is used when engine.create() is called

        :param str name: name of node
        :param str node_type: based on engine type specified
        :param int nodeid: used to identify which node
        :param list LoopbackInterface loopback_ndi: optional loopback
            interface for node.
        """
----------
Output: 

        return cls(name, node_type, nodeid,
                loopback_ndi=loopback_ndi)

    def _get_node_by_id(self, node_id):
        """
        Get the node object with the given node id

        :param int node_id: node id
        """
        return self.nodes[nodeid]

    def _get_node_by_name(self, node_name):
        """
        Get the node object with the given node name

        :param str node_name: node name
        """
        return self.nodes[node_name]

    def _
----------
Solution: 

        loopback = loopback_ndi if loopback_ndi else []
        node = {node_type: {
            'activate_test': True,
            'disabled': False,
            'loopback_node_dedicated_interface': loopback,
            'name': name + ' node ' + str(nodeid),
            'nodeid': nodeid}
        }
        return node
--------------------
Prompt: 
def load(self, cfgFile=None, timeout=None):
        """expect that the data file has already been established"""
----------
Output: 

        if cfgFile is None:
            cfgFile = cfgFile
        if not os.path.isfile(cfgFile):
            raise Exception("Config file %s does not exist" % cfgFile)
        if timeout is None:
            timeout = cfgfile.getInt(self.configSection, "timeout", 10)
        if not os.path.isfile(timeout):
            raise Exception("Config file %s does not exist" % timeout)
        if cfgfile.has_option(self.configSection, "logFile"):
            self.logSection = cfgfile.get(self.configSection, "log
----------
Solution: 

        #if cfgFile != None: self.cfgFile = cfgFile # if it's specified, use it
        if not cfgFile:
            cfgs = activeConfigs()
            if   len(cfgs) > 1: raise Exception("found too many configurations (%s); not clear which to load: %s"%(len(cfgs), cfgs))
            elif len(cfgs) < 1:
                if timeout: # wait for a configuration file to appear to be loaded
                    startWait = time.time()
                    timeReported = 0
                    while not cfgs:
                        timeWaited = time.time() - startWait
                        if timeWaited > timeout:
                            raise c.TimeoutExceeded("could not join game after %s seconds"%(timeout))
                        try:  cfgs = activeConfigs()
                        except:
                            if self.debug and timeWaited - timeReported >= 1:
                                timeReported += 1
                                print("second(s) waited for game to appear:  %d"%(timeReported))
                else:  raise Exception("must have a saved configuration to load or allow loading via timeout setting")
            cfgFile = cfgs.pop()
        try:
            with open(cfgFile, "rb") as f:
                data = f.read() # bytes => str
        except TypeError as e:
            print("ERROR %s: %s %s"%(e, cfgFile, type(cfgFile)))
            raise
        self.loadJson(data) # str => dict
        if self.debug:
            print("configuration loaded: %s"%(self.name))
            self.display()
--------------------
Prompt: 
def update_probs(self):
        """Update the internal probability values given the counts."""
----------
Output: 

        self.probs = np.zeros(self.num_actions)
        self.probs[self.action_index] = 1.0
        self.action_index += 1
        self.num_actions += 1

    def get_action_probs(self):
        """Return the internal probability values."""
        return self.probs[self.action_index]

    def get_action_probs_and_actions(self):
        """Return the action probabilities and actions."""
        return self.get_action_probs(), self.get_action_probs_and_actions()

    def get_action_index
----------
Solution: 

        # We deal with the prior probsfirst
        # This is a fixed assumed value for systematic error
        syst_error = 0.05
        prior_probs = {'syst': {}, 'rand': {}}
        for source, (p, n) in self.prior_counts.items():
            # Skip if there are no actual counts
            if n + p == 0:
                continue
            prior_probs['syst'][source] = syst_error
            prior_probs['rand'][source] = \
                1 - min((float(p) / (n + p), 1-syst_error)) - syst_error
        # Next we deal with subtype probs based on counts
        subtype_probs = {}
        for source, entry in self.subtype_counts.items():
            for rule, (p, n) in entry.items():
                # Skip if there are no actual counts
                if n + p == 0:
                    continue
                if source not in subtype_probs:
                    subtype_probs[source] = {}
                subtype_probs[source][rule] = \
                    1 - min((float(p) / (n + p), 1-syst_error)) - syst_error
        # Finally we propagate this into the full probability
        # data structures of the parent class
        super(BayesianScorer, self).update_probs(prior_probs, subtype_probs)
--------------------
Prompt: 
def equals(self, other):
        """ Structural equality of models.

        Args:
            other (HasProps) : the other instance to compare to

        Returns:
            True, if properties are structurally equal, otherwise False

        """
----------
Output: 

        return self.__dict__ == other.__dict__

    def __ne__(self, other):
        """ Check if two models are not equal.

        Args:
            other (HasProps) : the other instance to compare to

        Returns:
            True, if properties are not equal, otherwise False

        """
        return not self.__eq__(other)

    def __hash__(self):
        return self.__hash

    def __repr__(self):
        return "StructuralModel({})".format(self.to_json())

    def __str__(self):
        return "StructuralModel({})".format(self.to_json
----------
Solution: 


        # NOTE: don't try to use this to implement __eq__. Because then
        # you will be tempted to implement __hash__, which would interfere
        # with mutability of models. However, not implementing __hash__
        # will make bokeh unusable in Python 3, where proper implementation
        # of __hash__ is required when implementing __eq__.
        if not isinstance(other, self.__class__):
            return False
        else:
            return self.properties_with_values() == other.properties_with_values()
--------------------
Prompt: 
def imported_member(self, node, member, name):
        """verify this is not an imported class or handle it"""
----------
Output: 

        if not isinstance(member, astroid.FunctionDef):
            return False
        return isinstance(member.parent, astroid.Class) and \
               isinstance(member.parent.parent, astroid.Module) and \
               isinstance(member.parent.parent, astroid.Class) and \
               isinstance(member.parent.parent.parent, astroid.Module) and \
               node.name == name

    def test_import_from_statement(self):
        """verify this is not an imported class or handle it"""
        if not isinstance(self.module, astroid.Module):
            return False
        return isinstance(self
----------
Solution: 

        # /!\ some classes like ExtensionClass doesn't have a __module__
        # attribute ! Also, this may trigger an exception on badly built module
        # (see http://www.logilab.org/ticket/57299 for instance)
        try:
            modname = getattr(member, "__module__", None)
        except TypeError:
            modname = None
        if modname is None:
            if name in ("__new__", "__subclasshook__"):
                # Python 2.5.1 (r251:54863, Sep  1 2010, 22:03:14)
                # >>> print object.__new__.__module__
                # None
                modname = builtins.__name__
            else:
                attach_dummy_node(node, name, member)
                return True

        real_name = {"gtk": "gtk_gtk", "_io": "io"}.get(modname, modname)

        if real_name != self._module.__name__:
            # check if it sounds valid and then add an import node, else use a
            # dummy node
            try:
                getattr(sys.modules[modname], name)
            except (KeyError, AttributeError):
                attach_dummy_node(node, name, member)
            else:
                attach_import_node(node, modname, name)
            return True
        return False
--------------------
Prompt: 
def config(self, **kwargs):
        """
        Config the behavior of :class:`Logger`.

        Control the output to show :class:`Logger.verbose` log::

            log.config(verbose=True)

        Control the output to show only the :class:`Logger.warn` and
        :class:`Logger.error` log::

            log.config(quiet=True)

        """
----------
Output: 

        return Logger.verbose(**kwargs)

    def info(self, **kwargs):
        """
        Info the behavior of :class:`Logger`.

        Control the output to show :class:`Logger.info` log::

            log.info(message)

        Control the output to show only the :class:`Logger.error` and
        :class:`Logger.warn` log::

            log.info(message, level=logging.INFO)

        """
        return Logger.info(**kwargs)

    def warning(self, **kwargs):
        """
        Show a warning message.

        """
        return self.info(message=kwargs
----------
Solution: 

        if 'indent' in kwargs:
            self._indent = kwargs.get('indent', 0)
        if 'verbose' in kwargs:
            self._enable_verbose = kwargs.get('verbose', False)
        if 'quiet' in kwargs:
            self._enable_quiet = kwargs.get('quiet', False)
        return self
--------------------
Prompt: 
def _assert_obj_type(pub, name="pub", obj_type=DBPublication):
    """
    Make sure, that `pub` is instance of the `obj_type`.

    Args:
        pub (obj): Instance which will be checked.
        name (str): Name of the instance. Used in exception. Default `pub`.
        obj_type (class): Class of which the `pub` should be instance. Default
                 :class:`.DBPublication`.

    Raises:
        InvalidType: When the `pub` is not instance of `obj_type`.
    """
----------
Output: 

    if not isinstance(pub, db.Model):
        raise InvalidType(name, pub, obj_type)


def _assert_type_or_none(pub, name="pub", obj_type=None):
    """
    Make sure, that `pub` is instance of the `obj_type`.

    Args:
        pub (class): Instance which will be checked.
        name (str): Name of the instance. Used in exception. Default `pub`.
        obj_type (class): Class of which the `pub` should be instance. Default
                 :class:`.DBPublication`.

    Raises:
----------
Solution: 

    if not isinstance(pub, obj_type):
        raise InvalidType(
            "`%s` have to be instance of %s, not %s!" % (
                name,
                obj_type.__name__,
                pub.__class__.__name__
            )
        )
--------------------
Prompt: 
def _adjust_for_new_root(self, path):
        """Adjust a path given the new root directory of the output."""
----------
Output: 

        if path == self.root_dir:
            return
        if not os.path.exists(path):
            raise ValueError("Path does not exist: %s" % path)
        if not os.path.isdir(path):
            raise ValueError("Path is not a directory: %s" % path)
        for f in os.listdir(path):
            if f == self.root_dir:
                continue
            if os.path.join(path, f) == self.root_dir:
                continue
            if os.path.isdir(os.path.join(path, f)):
                self
----------
Solution: 

        if self.new_root is None:
            return path
        elif path.startswith(self.new_root):
            return path[len(self.new_root):]
        else:
            return path
--------------------
Prompt: 
def populate_request_data(self, request_args):
        """Add the authentication info to the supplied dictionary.

        We use the `requests.HTTPBasicAuth` class as the `auth` param.

        Args:
            `request_args`: The arguments that will be passed to the request.
        Returns:
            The updated arguments for the request.
        """
----------
Output: 

        if not request_args:
            return request_args

        if not isinstance(request_args, dict):
            raise TypeError('request_args must be a dict')

        request_args['auth'] = self.auth
        return request_args

    def get_auth_headers(self):
        """Get the authentication headers.

        Returns:
            The authentication headers for the current request.
        """
        if not self.auth:
            return {}

        return {
            'Authorization': 'Bearer'+ self.auth,
            'Content-Type': 'application/json',
        }

    def get_auth_credentials
----------
Solution: 

        request_args['auth'] = HTTPBasicAuth(
            self._username, self._password)
        return request_args
--------------------
Prompt: 
def _parse_player_position(self, player_info):
        """
        Parse the player's position.

        The player's position isn't contained within a unique tag and the
        player's meta information should be iterated through until 'Position'
        is found as it contains the desired text.

        Parameters
        ----------
        player_info : PyQuery object
            A PyQuery object of the player's information on the HTML stats
            page.
        """
----------
Output: 

        for tag in player_info.find_all('td'):
            if tag.has_attr('class') and tag['class'] == 'player-position':
                text = tag.text
                if text is None:
                    continue
                if text.startswith('Player'):
                    yield (tag['id'], text[4:])
                else:
                    yield (tag['id'], text)

    def _parse_player_title(self, player_info):
        """
        Parse the player's title.

        The player's title is contained within a unique tag and the
        player's meta information should be iterated through until '
----------
Solution: 

        for section in player_info('div#meta p').items():
            if 'Position' in str(section):
                position = section.text().replace('Position: ', '')
                setattr(self, '_position', position)
                break
--------------------
Prompt: 
def _reloader_observer(server, app, interval):
    """ Start a child process with identical commandline arguments and restart
        it as long as it exists with status code 3. Also create a lockfile and
        touch it (update mtime) every interval seconds.
    """
----------
Output: 

    # We need to lock the lockfile to prevent race conditions when the daemon
    # dies.
    lockfile = os.path.join(app.config['PID_DIR'], 'lock')
    try:
        os.mkfifo(lockfile)
    except OSError:
        return server.NOT_DONE_YET
    # We need to create a lockfile and touch it (update mtime) every
    # interval seconds.
    with open(lockfile, 'w') as f:
        f.write(str(interval))
        f.write(str(os.getpid()))


class Daemon(Daemon
----------
Solution: 

    fd, lockfile = tempfile.mkstemp(prefix='bottle-reloader.', suffix='.lock')
    os.close(fd) # We only need this file to exist. We never write to it
    try:
        while os.path.exists(lockfile):
            args = [sys.executable] + sys.argv
            environ = os.environ.copy()
            environ['BOTTLE_CHILD'] = 'true'
            environ['BOTTLE_LOCKFILE'] = lockfile
            p = subprocess.Popen(args, env=environ)
            while p.poll() is None: # Busy wait...
                os.utime(lockfile, None) # I am alive!
                time.sleep(interval)
            if p.poll() != 3:
                if os.path.exists(lockfile): os.unlink(lockfile)
                sys.exit(p.poll())
            elif not server.quiet:
                print("Reloading server...")
    except KeyboardInterrupt:
        pass
    if os.path.exists(lockfile): os.unlink(lockfile)
--------------------
Prompt: 
def backtick (cmd, encoding='utf-8'):
    """Return decoded output from command."""
----------
Output: 

    return _decode (cmd, encoding)

def _decode (cmd, encoding='utf-8'):
    """Decode command."""
    if not cmd:
        return None
    if not encoding:
        return cmd
    if not _is_utf8 (cmd):
        return cmd
    if not _is_utf8 (encoding):
        return cmd
    if not _is_utf8 (encoding):
        return cmd
    if not _is_utf8 (cmd):
        return cmd
    if not _is_utf8 (encoding):
        return cmd
    return _decode_str (cmd, encoding)

def
----------
Solution: 

    data = subprocess.Popen(cmd, stdout=subprocess.PIPE).communicate()[0]
    return data.decode(encoding)
--------------------
Prompt: 
def _queryset_iterator(qs):
    """
    Override default iterator to wrap returned items in a publishing
    sanity-checker "booby trap" to lazily raise an exception if DRAFT
    items are mistakenly returned and mis-used in a public context
    where only PUBLISHED items should be used.

    This booby trap is added when all of:

    - the publishing middleware is active, and therefore able to report
    accurately whether the request is in a drafts-permitted context
    - the publishing middleware tells us we are not in
    a drafts-permitted context, which means only published items
    should be used.
    """
----------
Output: 
 but we need to use the request object
    """
    return itertools.chain(
        qs.filter(publish_immediately=True, published=True)
       .exclude(publish_immediately=True, published=True)
       .filter(publish_immediately=True, published=True)
       .order_by('publish_immediately')
       .iterator()
    )


class PublishingMiddleware(object):
    """
    This middleware allows you to add headers to your request that are
    automatically generated.

    You must provide the ``headers`` argument.
    """
    def process_response(self, request
----------
Solution: 

    # Avoid double-processing draft items in our custom iterator when we
    # are in a `PublishingQuerySet` that is also a subclass of the
    # monkey-patched `UrlNodeQuerySet`
    if issubclass(type(qs), UrlNodeQuerySet):
        super_without_boobytrap_iterator = super(UrlNodeQuerySet, qs)
    else:
        super_without_boobytrap_iterator = super(PublishingQuerySet, qs)

    if is_publishing_middleware_active() \
            and not is_draft_request_context():
        for item in super_without_boobytrap_iterator.iterator():
            if getattr(item, 'publishing_is_draft', False):
                yield DraftItemBoobyTrap(item)
            else:
                yield item
    else:
        for item in super_without_boobytrap_iterator.iterator():
            yield item
--------------------
Prompt: 
def _get_group_dn(self, group_lookup_attribute_value):
        """ Searches for a group and retrieves its distinguished name.

        :param group_lookup_attribute_value: The value for the LDAP_GROUPS_GROUP_LOOKUP_ATTRIBUTE
        :type group_lookup_attribute_value: str

        :raises: **GroupDoesNotExist** if the group doesn't exist in the active directory.

        """
----------
Output: 

        group_dn = None
        try:
            group_dn = self.get_active_directory_dn(self.active_directory)
        except:
            pass
        if group_lookup_attribute_value is not None:
            group_dn = group_lookup_attribute_value.split(',')[0]
        else:
            for group in self.ldap_groups:
                if group.get('dn') == group_dn:
                    group_lookup_attribute_value = group.get('attribute')
                    break
        raise GroupDoesNotExist(group_lookup_attribute_value)
        if len(group_
----------
Solution: 

        self.ldap_connection.search(search_base=self.GROUP_SEARCH['base_dn'],
                                    search_filter=self.GROUP_SEARCH['filter_string'].format(
                                        lookup_value=escape_query(group_lookup_attribute_value)),
                                    search_scope=self.GROUP_SEARCH['scope'],
                                    attributes=self.GROUP_SEARCH['attribute_list'])
        results = [result["dn"] for result in self.ldap_connection.response if result["type"] == "searchResEntry"]

        if not results:
            raise GroupDoesNotExist("The {group_lookup_attribute} provided does not exist in the Active "
                                    "Directory.".format(group_lookup_attribute=self.group_lookup_attr))

        if len(results) > 1:
            logger.debug("Search returned more than one result: {results}".format(results=results))

        if results:
            return results[0]
        else:
            return results
--------------------
Prompt: 
def close(self):
        """Close a port on dummy_serial."""
----------
Output: 

        pass

    def get_port_name(self):
        """Return the port name for the current test.

        :returns: port name for the current test.
        """
        return "dummy_port"

    def get_port_stats(self, port_name):
        """Get statistics from the port's event buffer.

        :param port_name: port name.
        :type port_name: str
        :returns: dictionary with statistic names and values.
        """
        return self._get_event_stats(port_name, "rx_packets")

    def get_port_desc(self,
----------
Solution: 

        if VERBOSE:
            _print_out('\nDummy_serial: Closing port\n')

        if not self._isOpen:
            raise IOError('Dummy_serial: The port is already closed')
            
        self._isOpen = False
        self.port = None
--------------------
Prompt: 
def start_session(self):
        """
        Starts a Salesforce session and determines which SF instance to use for future requests.
        """
----------
Output: 

        self.session = requests.Session()
        self.session.headers.update({'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'})
        self.session.headers.update({'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'})
        self.session.headers.update({'Accept': 'application
----------
Solution: 

        if self.has_active_session():
            raise Exception("Session already in progress.")

        response = requests.post(self._get_login_url(),
                                 headers=self._get_login_headers(),
                                 data=self._get_login_xml())
        response.raise_for_status()

        root = ET.fromstring(response.text)
        for e in root.iter("%ssessionId" % self.SOAP_NS):
            if self.session_id:
                raise Exception("Invalid login attempt.  Multiple session ids found.")
            self.session_id = e.text

        for e in root.iter("%sserverUrl" % self.SOAP_NS):
            if self.server_url:
                raise Exception("Invalid login attempt.  Multiple server urls found.")
            self.server_url = e.text

        if not self.has_active_session():
            raise Exception("Invalid login attempt resulted in null sessionId [%s] and/or serverUrl [%s]." %
                            (self.session_id, self.server_url))
        self.hostname = urlsplit(self.server_url).hostname
--------------------
Prompt: 
def StartupInstance(r, instance, dry_run=False, no_remember=False):
    """
    Starts up an instance.

    @type instance: str
    @param instance: the instance to start up
    @type dry_run: bool
    @param dry_run: whether to perform a dry run
    @type no_remember: bool
    @param no_remember: if true, will not record the state change
    @rtype: string
    @return: job id
    """
----------
Output: 

    if dry_run:
      dry_run_add = dry_run
    else:
      dry_run_add = no_remember

    body = {
      'configuration': instance,
      'dry-run': dry_run_add,
     'remember-instances': no_remember,
    }

    return self._SendRequest(HTTP_POST, '/start', body)

  def StopInstance(self, instance, dry_run=False, no_remember=False):
    """
    Stops an instance.

    @type instance: str
    @param instance: the instance to stop
   
----------
Solution: 


    query = {
        "dry-run": dry_run,
        "no-remember": no_remember,
    }

    return r.request("put", "/2/instances/%s/startup" % instance, query=query)
--------------------
Prompt: 
def decipher(self,string):
        """Decipher string using FracMorse cipher according to initialised key. 

        Example::

            plaintext = FracMorse('ROUNDTABLECFGHIJKMPQSVWXYZ').decipher(ciphertext)     

        :param string: The string to decipher.
        :returns: The enciphered string.
        """
----------
Output: 

        return self.cipher.decrypt(string,self.key)

    def decipher_all(self,ciphertext):
        """Decipher all the given ciphertext using FracMorse cipher. 

        :param string: The string to decipher.
        :returns: The deciphered string.
        """
        return self.cipher.encrypt(ciphertext,self.key)

    def _encrypt_all(self,ciphertext):
        """Encrypt all the given ciphertext using FracMorse cipher. 

        :param string: The string to encrypt.
        :returns: The concatenated ciphertext.
        """
        return ''.join
----------
Solution: 
                   
        string = string.upper()
        mapping = dict(zip(self.key,self.table))
        ptext = ""
        for i in string:
            ptext += mapping[i]
        return self.demorse(ptext)
--------------------
Prompt: 
def get_host_datastore_system(host_ref, hostname=None):
    """
    Returns a host's datastore system

    host_ref
        Reference to the ESXi host

    hostname
        Name of the host. This argument is optional.
    """
----------
Output: 

    if hostname is None:
        hostname = socket.gethostname()
    if not hostname:
        hostname = socket.gethostname()
    return "esxi_host_%s" % hostname


def get_host_cluster_name(module):
    """
    Returns the cluster name of a host

    module
        Ansible module object

    Returns:
        string: Name of the cluster name
    """
    name = module.params.get('cluster_name')
    if not name:
        name = module.params.get('cluster')
    return name


def main():
    argument_spec = vmware_argument_spec()
   
----------
Solution: 


    if not hostname:
        hostname = get_managed_object_name(host_ref)
    service_instance = get_service_instance_from_managed_object(host_ref)
    traversal_spec = vmodl.query.PropertyCollector.TraversalSpec(
        path='configManager.datastoreSystem',
        type=vim.HostSystem,
        skip=False)
    objs = get_mors_with_properties(service_instance,
                                    vim.HostDatastoreSystem,
                                    property_list=['datastore'],
                                    container_ref=host_ref,
                                    traversal_spec=traversal_spec)
    if not objs:
        raise salt.exceptions.VMwareObjectRetrievalError(
            'Host\'s \'{0}\' datastore system was not retrieved'
            ''.format(hostname))
    log.trace('[%s] Retrieved datastore system', hostname)
    return objs[0]['object']
--------------------
Prompt: 
def targets_by_file(self, targets):
    """Returns a map from abs path of source, class or jar file to an OrderedSet of targets.

    The value is usually a singleton, because a source or class file belongs to a single target.
    However a single jar may be provided (transitively or intransitively) by multiple JarLibrary
    targets. But if there is a JarLibrary target that depends on a jar directly, then that
    "canonical" target will be the first one in the list of targets.
    """
----------
Output: 

    if not targets:
      return OrderedSet()

    # TODO(John Sirois): This is a temporary hack to get around a PR to remove the duplication.
    # Right now, we can use the target set to find the first duplicate target, but
    # it's not clear that there is a duplication.
    if len(targets) == 1:
      return OrderedSet(targets.values())[0]

    # TODO(John Sirois): This is a temporary hack to get around a PR to remove the duplication.
    # Right now, we can use the target set to find the first duplicate target,
----------
Solution: 

    targets_by_file = defaultdict(OrderedSet)

    for target in targets:
      for f in self.files_for_target(target):
        targets_by_file[f].add(target)

    return targets_by_file
--------------------
Prompt: 
def queuedb_findall(path, queue_id, name=None, offset=None, limit=None):
    """
    Get all queued entries for a queue and a name.
    If name is None, then find all queue entries

    Return the rows on success (empty list if not found)
    Raise on error
    """
----------
Output: 

    path = path.strip()
    if not path:
        return []
    if name:
        path = path + '/' + name
    if offset:
        path = path + '?offset=%s' % (offset)
    if limit:
        path = path + '?limit=%s' % (limit)
    return queue_get(path)


def queuedb_list(queue_id):
    """
    Get all queued entries for a queue.
    Raise on error
    """
    path = queuedb_path(queue_id)
    return queuedb_findall(path)


def queuedb_delete
----------
Solution: 

    sql = "SELECT * FROM queue WHERE queue_id = ? ORDER BY rowid ASC"
    args = (queue_id,)
    
    if name:
        sql += ' AND name = ?'
        args += (name,)

    if limit:
        sql += ' LIMIT ?'
        args += (limit,)
    
    if offset:
        sql += ' OFFSET ?'
        args += (offset,)

    sql += ';'
    
    db = queuedb_open(path)
    if db is None:
        raise Exception("Failed to open %s" % path)

    cur = db.cursor()
    rows = queuedb_query_execute(cur, sql, args)

    count = 0
    ret = []
    for row in rows:
        dat = {}
        dat.update(row)
        ret.append(dat)

    db.close()
    return ret
--------------------
Prompt: 
def _strict_match(self, struct1, struct2, fu, s1_supercell=True,
                      use_rms=False, break_on_match=False):
        """
        Matches struct2 onto struct1 (which should contain all sites in
        struct2).

        Args:
            struct1, struct2 (Structure): structures to be matched
            fu (int): size of supercell to create
            s1_supercell (bool): whether to create the supercell of
                struct1 (vs struct2)
            use_rms (bool): whether to minimize the rms of the matching
            break_on_match (bool): whether to stop search at first
                valid match
        """
----------
Output: 
on_match (bool): whether to break on the match.
        """
        if use_rms:
            return fu * 1.0 / (struct1.size + struct2.size)
        else:
            return fu * size_diff(struct1, struct2)

    def _create_supercell(self, size, supercell_class):
        """
        Creates a supercell of the given size.

        Args:
            size (int): size of supercell to create
            supercell_class (SuperCell): supercell class to be used for
                matching
        """
        if supercell_class:
           
----------
Solution: 

        if fu < 1:
            raise ValueError("fu cannot be less than 1")

        mask, s1_t_inds, s2_t_ind = self._get_mask(struct1, struct2,
                                                   fu, s1_supercell)

        if mask.shape[0] > mask.shape[1]:
            raise ValueError('after supercell creation, struct1 must '
                             'have more sites than struct2')

        # check that a valid mapping exists
        if (not self._subset) and mask.shape[1] != mask.shape[0]:
            return None

        if LinearAssignment(mask).min_cost > 0:
            return None

        best_match = None
        # loop over all lattices
        for s1fc, s2fc, avg_l, sc_m in \
                self._get_supercells(struct1, struct2, fu, s1_supercell):
            # compute fractional tolerance
            normalization = (len(s1fc) / avg_l.volume) ** (1/3)
            inv_abc = np.array(avg_l.reciprocal_lattice.abc)
            frac_tol = inv_abc * self.stol / (np.pi * normalization)
            # loop over all translations
            for s1i in s1_t_inds:
                t = s1fc[s1i] - s2fc[s2_t_ind]
                t_s2fc = s2fc + t
                if self._cmp_fstruct(s1fc, t_s2fc, frac_tol, mask):
                    inv_lll_abc = np.array(avg_l.get_lll_reduced_lattice().reciprocal_lattice.abc)
                    lll_frac_tol = inv_lll_abc * self.stol / (np.pi * normalization)
                    dist, t_adj, mapping = self._cart_dists(
                        s1fc, t_s2fc, avg_l, mask, normalization, lll_frac_tol)
                    if use_rms:
                        val = np.linalg.norm(dist) / len(dist) ** 0.5
                    else:
                        val = max(dist)
                    if best_match is None or val < best_match[0]:
                        total_t = t + t_adj
                        total_t -= np.round(total_t)
                        best_match = val, dist, sc_m, total_t, mapping
                        if (break_on_match or val < 1e-5) and val < self.stol:
                            return best_match

        if best_match and best_match[0] < self.stol:
            return best_match
--------------------
Prompt: 
def _compute_faulting_mechanism(self, C, rake, dip):
        """
        Compute faulting mechanism term (see eq. 5, page 319).

        Reverse faulting is defined as occurring on steep faults (dip > 45)
        and rake in (22.5, 157.5).

        Thrust faulting is defined as occurring on shallow dipping faults
        (dip <=45) and rake in (22.5, 157.5)
        """
----------
Output: 

        return C['faulting_mechanism'][0]

    def _compute_faulting_type(self, C, faulting_type):
        """
        Compute the faulting type term (see eq. 5, page 319).

        Faulting type is defined as occurring on the shallow dipping faults
        (dip <=45) and rake in (22.5, 157.5)
        """
        if faulting_type == 'vacant':
            return faulting_type
        elif faulting_type == 'heavy':
            return 'vacant'
        elif faulting_type ==
----------
Solution: 

        # flag for reverse faulting
        frv = float((dip > 45) and (22.5 <= rake <= 157.5))
        # flag for thrust faulting
        fth = float((dip <= 45) and (22.5 <= rake <= 157.5))

        return C['c10'] * frv + C['c11'] * fth
--------------------
Prompt: 
def get_dates_range(self, scale='auto', start=None, end=None,
                        date_max='2010-01-01'):
        """
        Returns a list of dates sampled according to the specified parameters.

        :param scale: {'auto', 'maximum', 'daily', 'weekly', 'monthly',
            'quarterly', 'yearly'}
            Scale specifies the sampling intervals.
            'auto' will heuristically choose a scale for quick processing
        :param start: First date that will be included.
        :param end: Last date that will be included
        """
----------
Output: 

        date_range = []
        if scale.lower() == 'auto':
            scale ='maximum'
        if start is not None:
            date_range.append(start)
        if end is not None:
            date_range.append(end)
        if date_max is not None:
            date_range.append(date_max)
        return date_range

    def get_dates_range_for_time(self, time):
        """
        Returns a list of dates sampled according to the specified parameters.

        :param time: {'auto', 'auto-6'}
            Time specifies the sampling
----------
Solution: 

        if scale not in ['auto', 'maximum', 'daily', 'weekly', 'monthly',
                         'quarterly', 'yearly']:
            raise ValueError('Incorrect scale: %s' % scale)
        start = Timestamp(start or self._start.min() or date_max)
        # FIXME: start != start is true for NaN objects... is NaT the same?
        start = Timestamp(date_max) if repr(start) == 'NaT' else start
        end = Timestamp(end or max(Timestamp(self._end.max()),
                                   self._start.max()))
        # FIXME: end != end ?
        end = datetime.utcnow() if repr(end) == 'NaT' else end
        start = start if self.check_in_bounds(start) else self._lbound
        end = end if self.check_in_bounds(end) else self._rbound

        if scale == 'auto':
            scale = self._auto_select_scale(start, end)
        if scale == 'maximum':
            start_dts = list(self._start.dropna().values)
            end_dts = list(self._end.dropna().values)
            dts = map(Timestamp, set(start_dts + end_dts))
            dts = filter(lambda ts: self.check_in_bounds(ts) and
                         ts >= start and ts <= end, dts)
            return dts

        freq = dict(daily='D', weekly='W', monthly='M', quarterly='3M',
                    yearly='12M')
        offset = dict(daily=off.Day(n=0), weekly=off.Week(),
                      monthly=off.MonthEnd(), quarterly=off.QuarterEnd(),
                      yearly=off.YearEnd())
        # for some reason, weekly date range gives one week less:
        end_ = end + off.Week() if scale == 'weekly' else end
        ret = list(pd.date_range(start + offset[scale], end_,
                                 freq=freq[scale]))
        ret = [dt for dt in ret if dt <= end]
        ret = [start] + ret if ret and start < ret[0] else ret
        ret = ret + [end] if ret and end > ret[-1] else ret
        ret = filter(lambda ts: self.check_in_bounds(ts), ret)
        return ret
--------------------
Prompt: 
def NonNegativeInt(n):
    """If *n* is non-negative integer returns it, otherwise an error.

    >>> print("%d" % NonNegativeInt('8'))
    8

    >>> NonNegativeInt('8.1')
    Traceback (most recent call last):
       ...
    ValueError: 8.1 is not an integer

    >>> print("%d" % NonNegativeInt('0'))
    0

    >>> NonNegativeInt('-1')
    Traceback (most recent call last):
       ...
    ValueError: -1 is not non-negative

    """
----------
Output: 

    if n < 0:
        raise ValueError("n must be non-negative")
    return n

# Map operator to precedence (for internal use by the C implementation)
operators = {
    '+': (PyrexTypes.Add, '+', '-'),
    '-': (PyrexTypes.Sub, '-', '-'),
    '*': (PyrexTypes.mul, '*', '*'),
    '/': (PyrexTypes.Div, '/'),
    '//': (PyrexTypes.FloorDiv, '//'),
    '%': (PyrexTypes.Mod, '%'),
    '**': (PyrexTypes.Pow, '**'),
    '%': (PyrexTypes.Mod
----------
Solution: 

    if not isinstance(n, str):
        raise ValueError('%r is not a string' % n)
    try:
       n = int(n)
    except:
        raise ValueError('%s is not an integer' % n)
    if n < 0:
        raise ValueError('%d is not non-negative' % n)
    else:
        return n
--------------------
Prompt: 
def _sorted_key_map(item, transform=to_bytes):
    """Creates a list of the item's key/value pairs as tuples, sorted by the keys transformed by transform.

    :param dict item: Source dictionary
    :param function transform: Transform function
    :returns: List of tuples containing transformed key, original value, and original key for each entry
    :rtype: list(tuple)
    """
----------
Output: 

    return sorted(item.keys(), key=itemgetter(0))


def _get_source_string(source):
    """Returns the string representation of the source.

    :param str source: Source string
    :returns: String representation of the source
    :rtype: str
    """
    if source is None:
        return None
    if isinstance(source, string_types):
        return source
    if hasattr(source,'read'):
        return source.read()
    return str(source)


class AnsibleDockerClient(Client):
    """Client class for Docker."""

    def __init__(self, argument_
----------
Solution: 

    sorted_items = []
    for key, value in item.items():
        _key = transform(key)
        sorted_items.append((_key, value, key))
    sorted_items = sorted(sorted_items, key=lambda x: x[0])
    return sorted_items
--------------------
Prompt: 
def download_pac(candidate_urls, timeout=1, allowed_content_types=None):
    """
    Try to download a PAC file from one of the given candidate URLs.

    :param list[str] candidate_urls: URLs that are expected to return a PAC file.
        Requests are made in order, one by one.
    :param timeout: Time to wait for host resolution and response for each URL.
        When a timeout or DNS failure occurs, the next candidate URL is tried.
    :param allowed_content_types: If the response has a ``Content-Type`` header,
        then consider the response to be a PAC file only if the header is one of these values.
        If not specified, the allowed types are
        ``application/x-ns-proxy-autoconfig`` and ``application/x-javascript-config``.
    :return: Contents of the PAC file, or `None` if no URL was successful.
    :rtype: str|None
    """
----------
Output: 
 the host will return a ``list`` of ``str``
    :return: A ``tuple`` of the form ``(url, file_size)`` where ``url`` is the PAC file
        path and ``file_size`` is the file size of the downloaded file.
    """
    if allowed_content_types is None:
        allowed_content_types = []
    host = None
    for candidate_url in candidate_urls:
        if candidate_url.endswith('.pac'):
            host = candidate_url
        elif candidate_url.endswith('.pac.gz'):
            host = candidate_url
        elif candidate_
----------
Solution: 

    if not allowed_content_types:
        allowed_content_types = {'application/x-ns-proxy-autoconfig', 'application/x-javascript-config'}

    sess = requests.Session()
    sess.trust_env = False  # Don't inherit proxy config from environment variables.
    for pac_url in candidate_urls:
        try:
            resp = sess.get(pac_url, timeout=timeout)
            content_type = resp.headers.get('content-type', '').lower()
            if content_type and True not in [allowed_type in content_type for allowed_type in allowed_content_types]:
                continue
            if resp.ok:
                return resp.text
        except (requests.exceptions.ConnectionError, requests.exceptions.Timeout):
            continue
--------------------
Prompt: 
def _perform_system_check(self):
        """
        Perform a system check to define if we need to throttle to handle 
        all the incoming messages 
        """
----------
Output: 

        if self.config.throttle_check_interval_ms is not None:
            self.throttle_check = int(self.config.throttle_check_interval_ms)
        else:
            self.throttle_check = self.config.throttle_check_interval
        if self.throttle_check > self.config.throttle_check_max_interval:
            self.throttle_check = self.config.throttle_check_max_interval
        if self.throttle_check < 0:
            self.throttle_check = 0
        if self.throttle_check > self.config.throttle_check
----------
Solution: 

        if Global.CONFIG_MANAGER.tracing_mode:
            Global.LOGGER.debug("performing a system check")

        now = datetime.datetime.now()
        sent = Global.MESSAGE_DISPATCHER.dispatched
        received = self.fetched
        queue_length = sent - received
        message_sleep_interval = Global.CONFIG_MANAGER.message_fetcher_sleep_interval

        if Global.CONFIG_MANAGER.show_stats:
            if (now - self.last_stats_check_date).total_seconds() > Global.CONFIG_MANAGER.stats_timeout:
                self.last_stats_check_date = now
                stats_string = f"showing stats\n--- [STATS] ---\nMessage Sent: {sent}\nMessage Received: {received}\nMessage Sleep Interval = {message_sleep_interval}\nQueue length = {queue_length}\n--- [ END ] ---"
                Global.LOGGER.info(stats_string)

        # if we are accumulating messages, or we have processed at least 5000 messages
        # since last check, we need to speed up the process
        messages_limit_reached = sent - self.last_queue_check_count > Global.CONFIG_MANAGER.messages_dispatched_for_system_check
        queue_limit_reached = queue_length > Global.CONFIG_MANAGER.queue_length_for_system_check
        time_limit_since_last_check_is_over = (now - self.last_queue_check_date).total_seconds() > Global.CONFIG_MANAGER.seconds_between_queue_check

        if not Global.CONFIG_MANAGER.fixed_message_fetcher_interval:
            if (messages_limit_reached) or (queue_limit_reached and time_limit_since_last_check_is_over):
                cause = "messages limit reached" if messages_limit_reached else "queue limit reached"
                Global.LOGGER.debug(f"triggering the throttle function due to {cause}")
                self._adapt_sleep_interval(sent, received, queue_length, now)
--------------------
Prompt: 
def exportTreeItem(self, sheet, cols, item):
        """
        Exports the inputed item to the given Excel worksheet for the
        given visible columns.
        
        :param      sheet | <xlwt.WorkSheet>
                    cols  | [<int>, ..]
                    item  | <QTreeWidgetItem>
        """
----------
Output: 

        if not self.isVisible(item):
            return False
        if not self.isExpanded(cols):
            return False
        if not self.isExpanded(item.childCount()):
            return False
        if not self.isExpanded(item.column()):
            return False
        if not self.isExpanded(item.parent().isValid()):
            return False
        if not self.isExpanded(item.column()):
            return False
        if not self.isExpanded(item.parent().internalPointer().parent()):
            return False
        if not self.isExpanded(item.parent().internalPointer().parent().isValid()):
----------
Solution: 

        # export item information
        for c, col in enumerate(cols):
            data = unwrapVariant(item.data(Qt.EditRole, col))
            if data:
                sheet.write(self._currrow, c, nativestring(data))
            else:
                sheet.write(self._currrow, c, nativestring(item.text(col)))
        
        self._currrow += 1
        
        # export children as rows
        for c in range(item.childCount()):
            self.exportTreeItem(sheet, cols, item.child(c))
--------------------
Prompt: 
def _set_properties(self, flags):
        """Set the properties of the message flags based on a byte input."""
----------
Output: 

        if flags & 1:
            self.flags |= flags & ~(1 << 7)
        if flags & 2:
            self.flags |= flags & ~(1 << 6)
        if flags & 4:
            self.flags |= flags & ~(1 << 7)
        if flags & 8:
            self.flags |= flags & ~(1 << 8)
        if flags & 16:
            self.flags |= flags & ~(1 << 6)
        if flags & 32:
            self.flags |= flags & ~(1 << 5)
        if flags & 64:
            self.flags |= flags & ~
----------
Solution: 

        flagByte = self._normalize(flags)

        if flagByte is not None:
            self._messageType = (flagByte[0] & 0xe0) >> 5
            self._extended = (flagByte[0] & MESSAGE_FLAG_EXTENDED_0X10) >> 4
            self._hopsLeft = (flagByte[0] & 0x0c) >> 2
            self._hopsMax = flagByte[0] & 0x03
        else:
            self._messageType = None
            self._extended = None
            self._hopsLeft = None
            self._hopsMax = None
--------------------
Prompt: 
def parse_encoding(value=None):
    """Parse a value to a valid encoding.

    This function accepts either a member of
    :py:class:`~cg:cryptography.hazmat.primitives.serialization.Encoding` or a string describing a member. If
    no value is passed, it will assume ``PEM`` as a default value. Note that ``"ASN1"`` is treated as an alias
    for ``"DER"``.

        >>> parse_encoding()
        <Encoding.PEM: 'PEM'>
        >>> parse_encoding('DER')
        <Encoding.DER: 'DER'>
        >>> parse_encoding(Encoding.PEM)
        <Encoding.PEM: 'PEM'>
    """
----------
Output: 
b'-----BEGIN CERTIFICATE-----... -----END CERTIFICATE-----... -----')
        <Encoding.PEM: '-----BEGIN CERTIFICATE-----... -----END CERTIFICATE-----... ----->
        >>> parse_encoding(b'-----BEGIN CERTIFICATE-----... -----BEGIN CERTIFICATE-----... -----END CERTIFICATE-----... -----')
        <Encoding.PEM: '-----BEGIN CERTIFICATE-----... -----END CERTIFICATE-----... ----->

    :param value: The value to parse. If ``None``, the value will be retrieved from the
        certificate.
    :type value: str
    :returns: The parsed value.
    :rtype:
----------
Solution: 

    if value is None:
        return ca_settings.CA_DEFAULT_ENCODING
    elif isinstance(value, Encoding):
        return value
    elif isinstance(value, six.string_types):
        if value == 'ASN1':
            value = 'DER'

        try:
            return getattr(Encoding, value)
        except AttributeError:
            raise ValueError('Unknown encoding: %s' % value)
    else:
        raise ValueError('Unknown type passed: %s' % type(value).__name__)
--------------------
Prompt: 
def run(self):
        """ run daemon """
----------
Output: 

        self.logger.info("Starting daemon")
        self.logger.info("PID: %s" % os.getpid())
        self.logger.info("PID_FILE: %s" % os.path.abspath(self.PID_FILE))
        self.logger.info("PID_FILE_MODE: %s" % self.PID_FILE_MODE)
        self.logger.info("PID_FILE_NAME: %s" % self.PID_FILE_NAME)
        self.logger.info("PID_FILE_PATH: %s" % self.PID_FILE_PATH
----------
Solution: 

        # FIXME - not detaching doesn't work, just run directly for now
        # self._exec(detach=False)
        try:
            with self.pidfile:
                return self._main()

        except pidfile.PidFileError:
            # this isn't exposed in pidfile :o
            self.log.error("failed to get pid lock, already running?")
            return 1

        finally:
            # call on_stop to let them clean up
            for mod in self.joins:
                self.log.debug("stopping %s", mod.name)
                mod.on_stop()
--------------------
Prompt: 
def enable_events(self):
        """enable slow wave and spindle detection if both
        annotations and channels are active.
        """
----------
Output: 

        if self.annotations_enabled and self.channels_enabled:
            self.enable_annotations()
        else:
            self.enable_channels()

    def enable_annotations(self):
        """enable the annotations on the waveform."""
        if self.annotations_enabled:
            self.annotations_enabled = False
            self.annotations_enabled_lock.acquire()
            self.annotations_enabled = True
            self.annotations_enabled_lock.release()
            self.annotations_enabled_lock.acquire()
            self.channels_enabled = True
            self.channels_enabled_lock.release()
            self.
----------
Solution: 

        if self.annot is not None and self.parent.channels.groups:
            self.action['spindle'].setEnabled(True)
            self.action['slow_wave'].setEnabled(True)
            self.action['analyze'].setEnabled(True)
        else:
            self.action['spindle'].setEnabled(False)
            self.action['slow_wave'].setEnabled(False)
            self.action['analyze'].setEnabled(False)
--------------------
Prompt: 
def get_smart_tasks(self):
        """
        Return the transitions linked to the gateway.

        Returns a Command.
        """
----------
Output: 

        return Command(self.name, self.gateway, self.ip, self.port, self.username, self.password, self.port_number, self.port_type, self.port_number, self.timeout, self.timeout_type)

    def get_smart_task_list(self):
        """
        Return the list of smart tasks.

        Returns a Command.
        """
        return Command(self.name, self.smart_gateway, self.smart_ip, self.smart_port, self.smart_username, self.smart_password, self.smart_port
----------
Solution: 

        def process_result(result):
            return [self.get_smart_task(task) for task in result]

        return Command('get', [ROOT_SMART_TASKS],
                       process_result=process_result)
--------------------
Prompt: 
async def send_rpc(self, conn_id, address, rpc_id, payload, timeout):
        """Send an RPC to a device.

        See :meth:`AbstractDeviceAdapter.send_rpc`.
        """
----------
Output: 

        return await self.send_rpc_request(conn_id, address, payload, timeout)

    async def send_rpc_request(self, conn_id, address, payload, timeout):
        """Send an RPC to a device.

        See :meth:`AbstractDeviceAdapter.send_rpc`.
        """
        raise NotImplementedError

    async def get_device_info(self, conn_id):
        """Get device info.

        See :meth:`AbstractDeviceAdapter.get_device_info`.
        """
        raise NotImplementedError

    async def get_device_info_by_id(self, device_id
----------
Solution: 


        resp = await self._execute(self._adapter.send_rpc_sync, conn_id, address, rpc_id, payload, timeout)
        _raise_error(conn_id, 'send_rpc', resp)

        status = resp.get('status')
        payload = resp.get('payload')

        # This will raise an exception if needed based on status
        return unpack_rpc_response(status, payload, rpc_id, address)
--------------------
Prompt: 
def _compile(self, target, results_dir, source):
    """Compile given source to an object file."""
----------
Output: 

    if self._options.output_buildbot_annotations:
      bisect_utils.OutputAnnotationStepStart('Running tests...')
      self._compile_annotations(target, results_dir, source)
      self._compile_annotations(target, results_dir, source)
      self._compile_annotations(target, results_dir, source)
      self._compile_annotations(target, results_dir, source)
      self._compile_annotations(target, results_dir, source)
      self._compile_annotations(target, results_dir, source)
      self._run.attrs.release_target = release_
----------
Solution: 

    obj = self._objpath(target, results_dir, source)
    safe_mkdir_for(obj)

    abs_source = os.path.join(get_buildroot(), source)

    # TODO: include dir should include dependent work dir when headers are copied there.
    include_dirs = []
    for dep in target.dependencies:
      if self.is_library(dep):
        include_dirs.extend([os.path.join(get_buildroot(), dep.target_base)])

    cmd = [self.cpp_toolchain.compiler]
    cmd.extend(['-c'])
    cmd.extend(('-I{0}'.format(i) for i in include_dirs))
    cmd.extend(['-o' + obj, abs_source])
    cmd.extend(self.get_options().cc_options)

    # TODO: submit_async_work with self.run_command, [(cmd)] as a Work object.
    with self.context.new_workunit(name='cpp-compile', labels=[WorkUnitLabel.COMPILER]) as workunit:
      self.run_command(cmd, workunit)

    self.context.log.info('Built c++ object: {0}'.format(obj))
--------------------
Prompt: 
def distribution_compatible(dist, supported_tags=None):
  """Is this distribution compatible with the given interpreter/platform combination?

  :param supported_tags: A list of tag tuples specifying which tags are supported
    by the platform in question.
  :returns: True if the distribution is compatible, False if it is unrecognized or incompatible.
  """
----------
Output: 

  if supported_tags is None:
    supported_tags = []
  else:
    supported_tags = [tag for tag in supported_tags if tag[0] in dist]
  if supported_tags:
    return any(tag[1] == supported_tags[0] for tag in zip(*supported_tags))[0]
  else:
    return False

----------
Solution: 

  if supported_tags is None:
    supported_tags = get_supported()
  package = Package.from_href(dist.location)
  if not package:
    return False
  return package.compatible(supported_tags)
--------------------
Prompt: 
def _get_parent_timestamp(dirname, mtime):
    """
    Get the timestamps up the directory tree. All the way to root.

    Because they affect every subdirectory.
    """
----------
Output: 

    return time.mktime(time.strptime(dirname, "%Y-%m-%d %H:%M:%S").timetuple())


def _get_parent_dir(dirname, mtime):
    """
    Get the parent directory up to the given timestamp.

    Because they affect every subdirectory.
    """
    return os.path.dirname(dirname)


def _get_parent_filename(dirname, mtime):
    """
    Get the filename of the parent directory up to the given timestamp.

    Because they affect every subdirectory.
    """
    return os.path.basename(dirname)


def _get_
----------
Solution: 

    parent_pathname = os.path.dirname(dirname)

    # max between the parent timestamp the one passed in
    mtime = _max_timestamps(parent_pathname, False, mtime)

    if dirname != os.path.dirname(parent_pathname):
        # this is only called if we're not at the root
        mtime = _get_parent_timestamp(parent_pathname, mtime)

    return mtime
--------------------
Prompt: 
def vertex_normals(self):
        """
        The vertex normals of the mesh. If the normals were loaded
        we check to make sure we have the same number of vertex
        normals and vertices before returning them. If there are
        no vertex normals defined or a shape mismatch we  calculate
        the vertex normals from the mean normals of the faces the
        vertex is used in.

        Returns
        ----------
        vertex_normals : (n,3) float
          Represents the surface normal at each vertex.
          Where n == len(self.vertices)
        """
----------
Output: 

        if self.vertices is None:
            return None
        if len(self.vertices)!= 3:
            raise ValueError("3D vertex array must have 3 vertices")
        if self.mean_normals is None:
            return None
        if len(self.mean_normals)!= len(self.vertices):
            raise ValueError("3D mean normals array must have "
                             "the same number of vertices")
        if self.shape_normals is None:
            return None
        if self.shape_normals.shape[0]!= 3:
            raise ValueError("3D shape mismatch")
        if self.shape_
----------
Solution: 

        # make sure we have faces_sparse
        assert hasattr(self.faces_sparse, 'dot')
        vertex_normals = geometry.mean_vertex_normals(
            vertex_count=len(self.vertices),
            faces=self.faces,
            face_normals=self.face_normals,
            sparse=self.faces_sparse)
        return vertex_normals
--------------------
Prompt: 
def _convert_for_reindex(self, key, axis=None):
        """
        Transform a list of keys into a new array ready to be used as axis of
        the object we return (e.g. including NaNs).

        Parameters
        ----------
        key : list-like
            Target labels
        axis: int
            Where the indexing is being made

        Returns
        -------
        list-like of labels
        """
----------
Output: 

        if axis is None:
            axis = self.axis or 0

        new_values = self.values.copy()

        if is_scalar(key) and not isinstance(key, (tuple, list)):
            new_values[key] = new_values[key].astype(self.dtype)

        elif isinstance(key, Index):
            new_values[key] = self.reindex(key.values).values

        elif is_list_like(key):
            # reindex
            if len(key)!= len(self.index):
                raise ValueError('Length of values does not match index length
----------
Solution: 


        if axis is None:
            axis = self.axis or 0
        labels = self.obj._get_axis(axis)

        if com.is_bool_indexer(key):
            key = check_bool_indexer(labels, key)
            return labels[key]

        if isinstance(key, Index):
            keyarr = labels._convert_index_indexer(key)
        else:
            # asarray can be unsafe, NumPy strings are weird
            keyarr = com.asarray_tuplesafe(key)

        if is_integer_dtype(keyarr):
            # Cast the indexer to uint64 if possible so
            # that the values returned from indexing are
            # also uint64.
            keyarr = labels._convert_arr_indexer(keyarr)

            if not labels.is_integer():
                keyarr = ensure_platform_int(keyarr)
                return labels.take(keyarr)

        return keyarr
--------------------
Prompt: 
def update_source_list(self):
        """
        update ubuntu 16 source list
        :return: 
        """
----------
Output: 

        self.logger.debug("Updating source list")
        self.logger.debug(self.source_list)
        self.source_list = self.update_source_list_ubuntu16(self.source_list)
        self.logger.debug("Source list updated")
        return self.source_list

    def update_source_list_ubuntu16(self, source_list):
        """
        update ubuntu 16 source list
        :param source_list: 
        :return: 
        """
        self.logger.debug("Updating source list")
        self.logger.debug(source_list)
       
----------
Solution: 

        with cd('/etc/apt'):
            sudo('mv sources.list sources.list.bak')
            put(StringIO(bigdata_conf.ubuntu_source_list_16),
                'sources.list', use_sudo=True)
            sudo('apt-get update -y --fix-missing')
--------------------
Prompt: 
def reset(self):
        """ Removes all the components of the block
        """
----------
Output: 

        self.parent.reset()

    def get_children(self):
        """ Returns a list of all of the children of this module """
        return [self.parent] + self.children

    def get_child_descriptors(self):
        """ Returns a list of all block descriptors """
        return [descriptor for descriptor in self.get_children() if descriptor.has_dynamic_children]

    def get_score(self):
        """ Returns the score of the problem when it is graded
        """
        last_submission = self.get_last_submission()
        score = last_submission.get('score
----------
Solution: 

        self._components = OrderedDict()
        self.clear_selections()
        self._logger.info("<block: %s> reset component list" % (self.name))
--------------------
Prompt: 
def parse_at_element(
            self,
            element,  # type: ET.Element
            state  # type: _ProcessorState
    ):
        # type: (...) -> Any
        """Parse the provided element as an aggregate."""
----------
Output: 

        if element.tag == "aggregate":
            self.aggregate = self._parse_aggregate(element)
        elif element.tag == "aggregate_list":
            self.aggregate_list = self._parse_aggregate_list(element)
        elif element.tag == "aggregate_list_list":
            self.aggregate_list_list = self._parse_aggregate_list_list(
                element
            )
        elif element.tag == "aggregate_map":
            self.aggregate_map = self._parse_aggregate_map(
                element
            )
        elif element.tag == "aggregate_map_list
----------
Solution: 

        parsed_dict = self._dictionary.parse_at_element(element, state)
        return self._converter.from_dict(parsed_dict)
--------------------
Prompt: 
def get_labels(self, include_freq=False, on_unicode_error='strict'):
        """
        Get the entire list of labels of the dictionary optionally
        including the frequency of the individual labels. Unsupervised
        models use words as labels, which is why get_labels
        will call and return get_words for this type of
        model.
        """
----------
Output: 

        if include_freq:
            return self.get_words(include_freq=True, on_unicode_error=on_unicode_error)
        else:
            return self.get_words(on_unicode_error=on_unicode_error)

    def get_labels_as_list(self, labels):
        """
        Convert a list of labels to a list of words.
        """
        if labels is None:
            return None
        return [self.get_words(include_freq=True, on_unicode_error=on_unicode_error) for label in labels]

    def
----------
Solution: 

        a = self.f.getArgs()
        if a.model == model_name.supervised:
            pair = self.f.getLabels(on_unicode_error)
            if include_freq:
                return (pair[0], np.array(pair[1]))
            else:
                return pair[0]
        else:
            return self.get_words(include_freq)
--------------------
Prompt: 
def _version_from_file(
        path_to_version,
        default_version=DEFAULT_VERSION,
):
    """for PyPI installed versions, just get data from file

    Args:
        path_to_version (str): abspath to dir where version.txt exists
        default_version (str): fallback version in case of error

    Returns:
        str: current working version

    """
----------
Output: 

    if path_to_version.exists():
        return path_to_version.read_text().strip()
    else:
        return default_version


def _version_from_url(url, default_version=DEFAULT_VERSION):
    """for PyPI installed versions, just get data from URL

    Args:
        url (str): url of the package
        default_version (str): fallback version in case of error

    Returns:
        str: current working version

    """
    if url.is_url():
        return url.read_text().strip()
    else:
        return default_version



----------
Solution: 

    version_filepath = os.path.join(path_to_version, 'version.txt')
    if not os.path.isfile(version_filepath):
        warnings.warn(
            'Unable to resolve current version',
            exceptions.ProsperDefaultVersionWarning)
        return default_version

    with open(version_filepath, 'r') as v_fh:
        data = v_fh.read()

    return data
--------------------
Prompt: 
def _get_utm_name_value_pair(zone, direction=_Direction.NORTH):
    """ Get name and code for UTM coordinates

    :param zone: UTM zone number
    :type zone: int
    :param direction: Direction enum type
    :type direction: Enum, optional (default=NORTH)
    :return: Name and code of UTM coordinates
    :rtype: str, str
    """
----------
Output: 

    if direction == _Direction.NORTH:
        return (0, 1)
    elif direction == _Direction.SOUTH:
        return (1, 0)
    else:
        return (0, None)


def _get_utm_name_pair(zone, direction=_Direction.NORTH):
    """ Get name and code for UTM coordinates

    :param zone: UTM zone number
    :type zone: int
    :param direction: Direction enum type
    :type direction: Enum, optional (default=NORTH)
    :return: Name and code of UTM
----------
Solution: 

    name = 'UTM_{}{}'.format(zone, direction.value)
    epsg = _get_utm_code(zone, direction)
    return name, epsg
--------------------
Prompt: 
def est_via_matching(self, weights='inv', matches=1, bias_adj=False):

		"""
		Estimates average treatment effects using nearest-
		neighborhood matching.

		Matching is done with replacement. Method supports multiple
		matching. Correcting bias that arise due to imperfect matches
		is also supported. For details on methodology, see [1]_.

		Parameters
		----------
		weights: str or positive definite square matrix
			Specifies weighting matrix used in computing
			distance measures. Defaults to string 'inv',
			which does inverse variance weighting. String
			'maha' gives the weighting matrix used in the
			Mahalanobis metric.
		matches: int
			Number of matches to use for each subject.
		bias_adj: bool
			Specifies whether bias adjustments should be
			attempted.

		References
		----------
		.. [1] Imbens, G. & Rubin, D. (2015). Causal Inference in
			Statistics, Social, and Biomedical Sciences: An
			Introduction.
		"""
----------
Output: 
 means that the distance measures are normalized
			to have 1/inv weighting.
		bias_adj: bool
			Correcting bias.

		Returns
		-------
		float
			The average treatment effects.
		"""

		if isinstance(weights, str):
			weights = self.get_weights(weights)
		if isinstance(matches, int):
			matches = self.get_matches(matches)

		return self.est_average_effect(weights, bias_adj)

	def est_average_effect(self, weights='inv', bias_adj
----------
Solution: 


		X, K = self.raw_data['X'], self.raw_data['K']
		X_c, X_t = self.raw_data['X_c'], self.raw_data['X_t']

		if weights == 'inv':
			W = 1/X.var(0)
		elif weights == 'maha':
			V_c = np.cov(X_c, rowvar=False, ddof=0)
			V_t = np.cov(X_t, rowvar=False, ddof=0)
			if K == 1:
				W = 1/np.array([[(V_c+V_t)/2]])  # matrix form
			else:
				W = np.linalg.inv((V_c+V_t)/2)
		else:
			W = weights

		self.estimates['matching'] = Matching(self.raw_data, W,
		                                      matches, bias_adj)
--------------------
Prompt: 
def draw_tree(t, df, size=10, ratio=0.6, precision=0):
    """ Draws a representation of a random forest in IPython.
    Parameters:
    -----------
    t: The tree you wish to draw
    df: The data used to train the tree. This is used to get the names of the features.
    """
----------
Output: 

    from sklearn.ensemble import RandomForestClassifier
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.ensemble import RandomForestClassifier
    from
----------
Solution: 

    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True,
                      special_characters=True, rotate=True, precision=precision)
    IPython.display.display(graphviz.Source(re.sub('Tree {',
       f'Tree {{ size={size}; ratio={ratio}', s)))
--------------------
Prompt: 
def set(self, key, val, timeout=None, using=None):
        """
        Set will be using the generational key, so if another thread
        bumps this key, the localstore version will still be invalid.
        If the key is bumped during a transaction it will be new
        to the global cache on commit, so it will still be a bump.
        """
----------
Output: 

        self.localstore.set(key, val, timeout)
        if using is not None:
            self.localstore.set(key, val, timeout)

    def get(self, key, timeout=None, using=None):
        """
        Get will be using the generational key, so if another thread
        bumps this key, the localstore version will still be invalid.
        If the key is bumped during a transaction it will be new
        to the global cache on commit, so it will still be a bump.
        """
        return self.localstore.get(key, timeout, with
----------
Solution: 

        if timeout is None:
            timeout = self.timeout
        if self.is_managed(using=using) and self._patched_var:
            self.local[key] = val
        else:
            self.cache_backend.set(key, val, timeout)
--------------------
Prompt: 
def get_authorize_callback(endpoint, provider_id):
    """Get a qualified URL for the provider to return to upon authorization

    param: endpoint: Absolute path to append to the application's host
    """
----------
Output: 

    return '{0}/oauth/authorize/{1}'.format(
        provider_url(endpoint),
        urllib.parse.quote(provider_id, safe=''))


def get_client_id(client_id):
    """Get the client id for the given client id

    param: client_id: The client id
    """
    return '{0}/oauth/token'.format(client_id)


def get_client_secret(client_secret):
    """Get the client secret for the given client id

    param: client_secret: The client secret
    """
    return '{0}/oauth/
----------
Solution: 

    endpoint_prefix = config_value('BLUEPRINT_NAME')
    url = url_for(endpoint_prefix + '.' + endpoint, provider_id=provider_id)
    return request.url_root[:-1] + url
--------------------
Prompt: 
def extract_text(input_file, pageno=1):
    """Use the txtwrite device to get text layout information out

    For details on options of -dTextFormat see
    https://www.ghostscript.com/doc/current/VectorDevices.htm#TXT

    Format is like
    <page>
    <line>
    <span bbox="left top right bottom" font="..." size="...">
    <char bbox="...." c="X"/>

    :param pageno: number of page to extract, or all pages if None
    :return: XML-ish text representation in bytes
    """
----------
Output: 
 XML
    """
    if input_file is None:
        return

    if pageno is not None:
        text_file = open(input_file, 'w')
        text_file.write('<page>%d</page>\n' % pageno)
        text_file.write('<line>%d</line>\n' % input_file.tell())
        text_file.write('<span bbox="left top right bottom" font="..." size="...">')
        text_file.write(str(text_file.read(pageno))
        text_file.write('</span
----------
Solution: 


    if pageno is not None:
        pages = ['-dFirstPage=%i' % pageno, '-dLastPage=%i' % pageno]
    else:
        pages = []

    args_gs = (
        [
            'gs',
            '-dQUIET',
            '-dSAFER',
            '-dBATCH',
            '-dNOPAUSE',
            '-sDEVICE=txtwrite',
            '-dTextFormat=0',
        ]
        + pages
        + ['-o', '-', fspath(input_file)]
    )

    p = run(args_gs, stdout=PIPE, stderr=PIPE)
    if p.returncode != 0:
        raise SubprocessOutputError(
            'Ghostscript text extraction failed\n%s\n%s\n%s'
            % (input_file, p.stdout.decode(), p.stderr.decode())
        )

    return p.stdout
--------------------
Prompt: 
def get_server_premaster_secret(self, password_verifier, server_private, client_public, common_secret):
        """S = (A * v^u) ^ b % N

        :param int password_verifier:
        :param int server_private:
        :param int client_public:
        :param int common_secret:
        :rtype: int
        """
----------
Output: 

        return self.get_premaster_secret(password_verifier, server_private, common_secret)

    def get_premaster_secret(self, password_verifier, server_private, client_public, common_secret):
        """
        Return the premaster secret for the given password verifier.

        :param int password_verifier:
        :param int server_private:
        :param int client_public:
        :param int common_secret:
        :rtype: int
        """
        return self.get_server_premaster_secret(password_verifier, server_private, client_public,
----------
Solution: 

        return pow((client_public * pow(password_verifier, common_secret, self._prime)), server_private, self._prime)
--------------------
Prompt: 
def max_id_length(p_num):
    """
    Returns the length of the IDs used, given the number of items that are
    assigned an ID. Used for padding in lists.
    """
----------
Output: 

    return len(p_num)


def get_unique_values(p_list):
    """
    Returns a list of unique values in the passed list.
    """
    unique_values = []
    for item in p_list:
        if item not in unique_values:
            unique_values.append(item)
    return p_list, unique_values


def get_unique_values_list(p_list):
    """
    Returns a list of unique values in the passed list.
    """
    unique_values = []
    for item in p_list:
        if item not in
----------
Solution: 

    try:
        alphabet = config().identifier_alphabet()
        length, _ = _get_table_size(alphabet, p_num)
    except _TableSizeException:
        length, _ = _get_table_size(_DEFAULT_ALPHABET, p_num)

    return length
--------------------
Prompt: 
def parse_dict(self, attrs):
        """Read a dict to attributes."""
----------
Output: 

        if not attrs:
            return None
        d = {}
        for key, value in attrs.items():
            d[key] = value
        return d

    def _get_value(self, attrs, key):
        """Read a value from attributes."""
        if not attrs:
            return None
        value = attrs.get(key, '')
        if not value:
            return None
        if isinstance(value, list):
            value = value[0]
        return value

    def _get_attr_from_entity(self, entity):
        """Read an attribute from the entity."""
        if not entity:
            return
----------
Solution: 

        attrs = attrs or {}
        ident = attrs.get("id", "")
        classes = attrs.get("classes", [])
        kvs = OrderedDict((k, v) for k, v in attrs.items()
                          if k not in ("classes", "id"))

        return ident, classes, kvs
--------------------
Prompt: 
def get_plugins_info(self):
        """
        Collect the current live info from all the registered plugins.

        Return a dictionary, keyed on the plugin name.

        """
----------
Output: 

        return dict((plugin.name, plugin.info) for plugin in self.plugins)

    def get_plugin_names(self):
        """
        Return a list of the currently registered plugins.

        Return a list of the names of all the plugins.

        """
        return [plugin.name for plugin in self.plugins]

    def get_plugin_info(self, name):
        """
        Return a live instance of the plugin with the given name.

        """
        return self.plugins[0].info

    def get_plugin_classes(self):
        """
        Return a list of the currently registered classes.
----------
Solution: 

        d = {}
        for p in self.plugins:
            d.update(p.get_info())
        return d
--------------------
Prompt: 
def upload_file(self, path, contents, replace=False):
        """
        Uplodas the file to its path with the given `content`, adding the
        appropriate parent directories when needed. If the path already exists
        and `replace` is `False`, the file will not be uploaded.
        """
----------
Output: 

        if os.path.exists(path):
            if replace:
                path = path.replace(self.path, self.path + '.bak')
            else:
                path = path.replace(self.path + '.bak', self.path)
            self.log('file already exists: %s' % path)
        else:
            self.log('file does not exist: %s' % self.path)
            if not os.path.exists(self.path):
                self.log('creating directory %s' % self.path)
                self.mkdir(self.path)
            self
----------
Solution: 

        f = self.get_file(path)
        f.upload(contents, replace=replace)
        self.set_cache_buster(path, f.hash())
--------------------
Prompt: 
async def home_z(self, mount: top_types.Mount = None):
        """ Home the two z-axes """
----------
Output: 

        await self._device.set_value(self._device.value_id, self._device.value_id, "z")
        await self._device.set_value(self._device.value_id, self._device.value_id, "z")
        await self._device.set_value(self._device.value_id, self._device.value_id, "z")
        await self._device.set_value(self._device.value_id, self._device.value_id, "z")
        await self._device.set_value(self._
----------
Solution: 

        if not mount:
            axes = [Axis.Z, Axis.A]
        else:
            axes = [Axis.by_mount(mount)]
        await self.home(axes)
--------------------
Prompt: 
def addfield(self, pkt, s, i):
        """
        There is a hack with the _ExtensionsField.i2len. It works only because
        we expect _ExtensionsField.i2m to return a string of the same size (if
        not of the same value) upon successive calls (e.g. through i2len here,
        then i2m when directly building the _ExtensionsField).

        XXX A proper way to do this would be to keep the extensions built from
        the i2len call here, instead of rebuilding them later on.
        """
----------
Output: 

        if i is None:
            return s
        else:
            return self.i2m(pkt, s, i)

    def i2len(self, pkt, x):
        """
        Returns the length of the interleaved data, in bits.

        :param pkt: the packet
        :param x: the interleaved length
        :return: the length of the interleaved data in bits
        """
        if x is None:
            return len(self.i2m(pkt, 0))
        else:
            return self.i2m(pkt, x, 1)

    def i2
----------
Solution: 

        if i is None:
            if self.length_of is not None:
                fld, fval = pkt.getfield_and_val(self.length_of)

                tmp = pkt.tls_session.frozen
                pkt.tls_session.frozen = True
                f = fld.i2len(pkt, fval)
                pkt.tls_session.frozen = tmp

                i = self.adjust(pkt, f)
                if i == 0:  # for correct build if no ext and not explicitly 0
                    return s
        return s + struct.pack(self.fmt, i)
--------------------
Prompt: 
def delta(self, other):
        """
        Return the error between this and another bearing. This will be an
        angle in degrees, positive or negative depending on the direction of
        the error.
        
            self   other
              \      /
               \    /
                \__/
                 \/ <- angle will be +ve

            other   self
              \      /
               \    /
                \__/
                 \/ <- angle will be -ve        

        :param other: bearing to compare to
        :type other: Bearing

        :returns: error angle
        :rtype: float
        """
----------
Output: 

        return math.degrees(math.atan2(self.x, other.x))

    def __str__(self):
        """
        Return a string representation of this bearing. This will be
        the same as the angle between the bearing and the bearing.
        
            \     /
               \    /
               \_______
               \/ <- angle will be +ve        

        :returns: a string representation of this bearing
        :rtype: str

        :returns: a string representation of the bearing
        :rtype: str

        :returns: a string representation of the bearing
        :rtype: str

        :
----------
Solution: 

        difference = float(other) - float(self)
        while difference < -180:
            difference += 360
        while difference > 180:
            difference -= 360
        return difference
--------------------
Prompt: 
def get_time_estimator(total):
    """Given a total amount of items to compute, return a function that,
    if called every time an item is computed (or every step items are computed)
    will give a time estimation for how long it will take to compute the whole
    set of itmes. The function will return two values: the first is the number
    of seconds that are still needed to compute the whole set, the second value
    is the time in the future when the operation is expected to end.
    """
----------
Output: 

    def time_estimator(self, total):
        """Given a total amount of items to compute, return a function that,
        if called every time an item is computed (or every step items are computed)
        will give a time estimation for how long it will take to compute the whole
        set of itmes. The function will return two values: the first is the number
        of seconds that are needed to compute the whole set, the second value
        is the time in the future when the operation is expected to end.
        """
        if total < 0:
            return 0
        if total > 0:
            return total -
----------
Solution: 

    t1 = time.time()
    count = [0]

    def estimate_needed_time(step=1):
        count[0] += step
        t2 = time.time()
        t3 = 1.0 * (t2 - t1) / count[0] * (total - count[0])
        return t3, t3 + t1
    return estimate_needed_time
--------------------
Prompt: 
def validate(config):
    """
    Validate the beacon configuration
    """
----------
Output: 

    # Check if the beacon is already configured
    if not config.has_section('beacon'):
        return False

    # Check if the beacon is configured
    if not config.has_option('beacon', 'name'):
        return False

    # Check if the beacon is configured with the correct values
    if not config.has_option('beacon', 'url'):
        return False

    # Check if the beacon is configured with the correct values
    if not config.has_option('beacon', 'username'):
        return False

    # Check if the beacon is configured with the correct values
   
----------
Solution: 


    # Configuration for load beacon should be a list of dicts
    if not isinstance(config, list):
        return False, ('Configuration for load beacon must be a list.')
    else:
        _config = {}
        list(map(_config.update, config))

        if 'emitatstartup' in _config:
            if not isinstance(_config['emitatstartup'], bool):
                return False, ('Configuration for load beacon option '
                               'emitatstartup must be a boolean.')

        if 'onchangeonly' in _config:
            if not isinstance(_config['onchangeonly'], bool):
                return False, ('Configuration for load beacon option '
                               'onchangeonly must be a boolean.')

        if 'averages' not in _config:
            return False, ('Averages configuration is required'
                           ' for load beacon.')
        else:

            if not any(j in ['1m', '5m', '15m'] for j
                       in _config.get('averages', {})):
                return False, ('Averages configuration for load beacon '
                               'must contain 1m, 5m or 15m items.')

            for item in ['1m', '5m', '15m']:
                if not isinstance(_config['averages'][item], list):
                    return False, ('Averages configuration for load beacon: '
                                   '1m, 5m and 15m items must be '
                                   'a list of two items.')
                else:
                    if len(_config['averages'][item]) != 2:
                        return False, ('Configuration for load beacon: '
                                       '1m, 5m and 15m items must be '
                                       'a list of two items.')

    return True, 'Valid beacon configuration'
--------------------
Prompt: 
def ensure_utf8(str_or_unicode):
    """
    tests, if the input is ``str`` or ``unicode``. if it is ``unicode``,
    it will be encoded from ``unicode`` to ``utf-8``. otherwise, the
    input string is returned.
    """
----------
Output: 

    if isinstance(str_or_unicode, unicode):
        return str_or_unicode
    else:
        return unicode(str_or_unicode, "utf-8")


class TestUtils(unittest.TestCase):
    def test_get_logger(self):
        self.assertEqual(logger, logging.getLogger('py.warnings'))
        logger.setLevel(logging.DEBUG)
        self.assertEqual(logger, logging.getLogger('py.warnings'))

    def test_get_logger_console(self):
        console = logging.StreamHandler()
        console.setLevel(logging.DEBUG)
        self.assertEqual
----------
Solution: 

    if isinstance(str_or_unicode, str):
        return str_or_unicode
    elif isinstance(str_or_unicode, unicode):
        return str_or_unicode.encode('utf-8')
    else:
        raise ValueError(
            "Input '{0}' should be a string or unicode, but it is of "
            "type {1}".format(str_or_unicode, type(str_or_unicode)))
--------------------
Prompt: 
def codingthreads(self):
        """
        Find CDS features in .gff files to filter out non-coding sequences from the analysis
        """
----------
Output: 

        self.gff_files = []
        for gff_file in self.gff_files:
            if not os.path.exists(gff_file):
                raise ValueError("GFF file %s does not exist" % gff_file)
            self.gff_files.append(gff_file)
            if not os.path.exists(gff_file + ".gff"):
                raise ValueError("GFF file %s does not exist" % gff_file + ".gff")
            if not os.path.exists(gff_file + ".g
----------
Solution: 

        printtime('Extracting CDS features', self.start)
        # Create and start threads
        for i in range(self.cpus):
            # Send the threads to the appropriate destination function
            threads = Thread(target=self.codingsequences, args=())
            # Set the daemon to true - something to do with thread management
            threads.setDaemon(True)
            # Start the threading
            threads.start()
        for sample in self.runmetadata.samples:
            self.codingqueue.put(sample)
        self.codingqueue.join()
        # Create CDS files and determine gene presence/absence
        self.corethreads()
--------------------
Prompt: 
def insert_into_shaders(self, vertex, fragment):
        """Apply the insertions to shader code."""
----------
Output: 

        self.code.append(
            self.shader_code.format(
                vertex.name,
                fragment.name,
                self.shader_code.format(
                    fragment.name,
                    vertex.name,
                    fragment.name,
                    self.shader_code.format(
                        fragment.name,
                        vertex.name,
                        fragment.name,
                    ),
                self.code.indent(2),
            )
        )

    def write_header(self):
        """Write the header."""
        self.header_text = self.text_buffer.get_text(
            self.header_text_buffer
----------
Solution: 

        to_insert = defaultdict(str)
        to_insert.update({key: '\n'.join(self._to_insert[key]) + '\n'
                          for key in self._to_insert})
        return _insert_glsl(vertex, fragment, to_insert)
--------------------
Prompt: 
def _get_stage_id(self, stage):
        """
        :param stage: A stage ID, name, or index (stage index is the number n for the nth stage, starting from 0; can be provided as an int or a string)
        :type stage: int or string
        :returns: The stage ID (this is a no-op if it was already a stage ID)
        :raises: :class:`~dxpy.exceptions.DXError` if *stage* could not be parsed, resolved to a stage ID, or it could not be found in the workflow
        """
----------
Output: 

        if isinstance(stage, basestring):
            return self.get_stage_index(stage)
        elif isinstance(stage, int):
            return self.get_stage_id(stage)
        else:
            raise DXError("Unknown stage type: %s" % stage)

    def _get_stage_name(self, stage):
        """
        :param stage: A stage name, name, or index (stage index is the number n for the nth stage, starting from 0; can be provided as an int or a string)
        :type stage: int or string
        :returns: The name (
----------
Solution: 

        # first, if it is a string, see if it is an integer
        if isinstance(stage, basestring):
            try:
                stage = int(stage)
            except:
                # we'll try parsing it as a string later
                pass

        if not isinstance(stage, basestring):
            # Try to parse as stage index; ensure that if it's not a
            # string that it is an integer at this point.
            try:
                stage_index = int(stage)
            except:
                raise DXError('DXWorkflow: the given stage identifier was neither a string stage ID nor an integer index')
            if stage_index < 0 or stage_index >= len(self.stages):
                raise DXError('DXWorkflow: the workflow contains ' + str(len(self.stages)) + \
                              ' stage(s), and the numerical value of the given stage identifier is out of range')
            return self.stages[stage_index].get("id")

        if re.compile('^([a-zA-Z_]|stage-)[0-9a-zA-Z_]*$').match(stage) is not None:
            # Check if there exists a stage with this stage id
            stage_id_exists = any([stg['id'] for stg in self.stages if stg.get('id') == stage])
            if stage_id_exists:
                return stage

        # A stage with the provided ID can't be found in the workflow, so look for it as a name
        stage_ids_matching_name = [stg['id'] for stg in self.stages if stg.get('name') == stage]
        if len(stage_ids_matching_name) == 0:
            raise DXError('DXWorkflow: the given stage identifier ' + stage + ' could not be found as a stage ID nor as a stage name')
        elif len(stage_ids_matching_name) > 1:
            raise DXError('DXWorkflow: more than one workflow stage was found to have the name "' + stage + '"')
        else:
            return stage_ids_matching_name[0]
--------------------
Prompt: 
def activate(self):
        """
        Activate the scene.
        """
----------
Output: 

        self.scene.active_object.select = True
        self.scene.active_object.hide = True
        self.scene.active_object.select_set(True)
        self.scene.objects.active = self.scene.objects[0]
        self.scene.objects.active.select = True
        self.scene.objects[0].hide = True
        self.scene.objects[0].hide_set(True)
        self.scene.objects.active.hide = False
        self.scene.objects.active.hide_set(False)
        self.scene.objects
----------
Solution: 

        response = self.api_interface.set_device_state(self, None)
        self._update_state_from_response(response)
--------------------
Prompt: 
def get_visual_content(self, id_or_uri):
        """
        Gets a list of visual content objects describing each rack within the data center. The response aggregates data
        center and rack data with a specified metric (peak24HourTemp) to provide simplified access to display data for
        the data center.

        Args:
            id_or_uri: Can be either the resource ID or the resource URI.

        Return:
            list: List of visual content objects.
        """
----------
Output: 

        if isinstance(id_or_uri, ResourceID):
            return self.get_resource_by_id(id_or_uri)
        elif isinstance(id_or_uri, str):
            return self.get_resource_by_uri(id_or_uri)
        else:
            raise TypeError("get_visual_content() expected an instance of str or ResourceID, got %s" % type(id_or_uri))

    def get_resource_by_id(self, id):
        """
        Gets a resource object from the data center.

        Args:
            id:
----------
Solution: 

        uri = self._client.build_uri(id_or_uri) + "/visualContent"
        return self._client.get(uri)
--------------------
Prompt: 
def _clone_args(self):
        """ return args to create new Dict clone
        """
----------
Output: 

        return dict(self.args)

    def _get_args(self):
        """ return arguments to create new Dict
        """
        return self.args.copy()

    def _get_value(self, key):
        """ return value for a given key
        """
        return self.args.get(key, None)

    def _get_value_from_path(self, path):
        """ return value for a given path
        """
        if path in self.args:
            return self.args[path]
        elif path in self.path_values:
            return self.path_values[path
----------
Solution: 

        keys = list(self.keys)
        kw = {}
        if self.allow_any or self.extras:
            kw['allow_extra'] = list(self.extras)
            if self.allow_any:
                kw['allow_extra'].append('*')
            kw['allow_extra_trafaret'] = self.extras_trafaret
        if self.ignore_any or self.ignore:
            kw['ignore_extra'] = list(self.ignore)
            if self.ignore_any:
                kw['ignore_any'].append('*')
        return keys, kw
--------------------
Prompt: 
def get_activity_objective_bank_session(self, proxy):
        """Gets the session for retrieving activity to objective bank mappings.

        arg:    proxy (osid.proxy.Proxy): a proxy
        return: (osid.learning.ActivityObjectiveBankSession) - an
                ``ActivityObjectiveBankSession``
        raise:  NullArgument - ``proxy`` is ``null``
        raise:  OperationFailed - unable to complete request
        raise:  Unimplemented - ``supports_activity_objective_bank()``
                is ``false``
        *compliance: optional -- This method must be implemented if
        ``supports_activity_objective_bank()`` is ``true``.*

        """
----------
Output: 
_activity_objective_bank()`` is ``true``.*

        """
        return  # osid.learning.ActivityObjectiveBankSession

    activity_objective_bank_session = property(fget=get_activity_objective_bank_session)

    def get_activity_objective_bank_assignment_session(self):
        """Gets the session for assigning activity to objective banks.

        return: (osid.learning.ActivityObjectiveBankAssignmentSession)
                an ``ActivityObjectiveBankAssignmentSession``
        raise:  NullArgument - ``session`` is ``null``
        raise:  OperationFailed - unable to complete request
----------
Solution: 

        if not self.supports_activity_objective_bank():
            raise errors.Unimplemented()
        # pylint: disable=no-member
        return sessions.ActivityObjectiveBankSession(proxy=proxy, runtime=self._runtime)
--------------------
Prompt: 
def _build(self, input_modules, middle_modules, head_modules):
        """
        TBD
        """
----------
Output: 

        self.modules = []
        self.middle_modules = []
        self.head_modules = []
        for module in input_modules:
            self.modules.append(module)
            self.middle_modules.append(module)
            self.head_modules.append(head_modules[module])

    def _build_module(self, module):
        """
        TBD
        """
        self.modules.append(module)
        self.middle_modules.append(module)
        self.head_modules.append(module)

    def _build_middle_module(self, module):
       
----------
Solution: 

        self.input_layer = self._build_input_layer(input_modules)
        self.middle_layers = self._build_middle_layers(middle_modules)
        self.heads = self._build_task_heads(head_modules)

        # Construct loss module
        reduction = self.config["train_config"]["loss_fn_reduction"]
        self.criteria = SoftCrossEntropyLoss(reduction=reduction)
--------------------
Prompt: 
def write(self, data):
        """Write data to connection

        Write data as string of bytes.

        Arguments:
        data -- buffer containing data to be written

        Return value:
        number of bytes actually transmitted
        """
----------
Output: 

        self.sock.sendall(data)
        return self.sock.recv_into(self.buf)

    def writelines(self, lines):
        """Write data to connection

        Write data as list of lines of bytes.

        Arguments:
        lines -- list of lines to be written

        Return value:
        number of lines actually transmitted
        """
        for line in lines:
            self.write(line)
        return self.write

    def close(self):
        """Close connection

        Close the connection to the MySQL server.
        """
        if self.sock:
            self.sock.close()
            self.sock
----------
Solution: 


        try:
            ret = self._wrap_socket_library_call(
                lambda: SSL_write(self._ssl.value, data), ERR_WRITE_TIMEOUT)
        except openssl_error() as err:
            if err.ssl_error == SSL_ERROR_SYSCALL and err.result == -1:
                raise_ssl_error(ERR_PORT_UNREACHABLE, err)
            raise
        if ret:
            self._handshake_done = True
        return ret
--------------------
Prompt: 
def get_ancestors(self):
        """
        :returns: A *list* containing the current node object's ancestors,
            starting by the root node and descending to the parent.
        """
----------
Output: 

        return self.get_ancestors_from_path(self.path)

    def get_ancestors_from_path(self, path):
        """
        :returns: A list of node objects that are ancestors of `path`.
        """
        return [self.get_node(p) for p in self.get_ancestors() if path == self.path]

    def get_ancestors_from_path_and_parents(self, path, parents):
        """
        :returns: A list of node objects that are ancestors of `path` and `parents`.
            The first node in the list is the
----------
Solution: 

        ancestors = []
        if self._meta.proxy_for_model:
            # the current node is a proxy model; our result set
            # should use the same proxy model, so we need to
            # explicitly fetch instances of that model
            # when following the 'parent' relation
            cls = self.__class__
            node = self
            while node.parent_id:
                node = cls.objects.get(pk=node.parent_id)
                ancestors.insert(0, node)
        else:
            node = self.parent
            while node:
                ancestors.insert(0, node)
                node = node.parent
        return ancestors
--------------------
Prompt: 
def process_request(self, request):
        """
        Adds a ``subdomain`` attribute to the ``request`` parameter.
        """
----------
Output: 

        self.request.subdomain = self.subdomain
        self.request.subdomain_specified = self.subdomain_specified
        return self.add_subdomain_cookie(request)

    def add_subdomain_cookie(self, request):
        """
        Adds a ``subdomain`` attribute to the ``request`` parameter.
        """
        self.add_cookie_header(request)
        self.request.set_cookie(
            settings.SESSION_COOKIE_NAME,
            request.session.session_key,
            max_age=settings.SESSION_COOKIE_AGE,
            expires=settings.SESSION_COOKIE_AGE,
----------
Solution: 

        domain, host = map(lower,
            (self.get_domain_for_request(request), request.get_host()))

        pattern = r'^(?:(?P<subdomain>.*?)\.)?%s(?::.*)?$' % re.escape(domain)
        matches = re.match(pattern, host)

        if matches:
            request.subdomain = matches.group('subdomain')
        else:
            request.subdomain = None
            logger.warning('The host %s does not belong to the domain %s, '
                'unable to identify the subdomain for this request',
                request.get_host(), domain)
--------------------
Prompt: 
def set_review_whether_correct(self,
                                   during_attempt=None,
                                   after_attempt=None,
                                   before_deadline=None,
                                   after_deadline=None):
        """stub"""
----------
Output: 

        if during_attempt is not None:
            self.assertTrue(isinstance(during_attempt, bool))
        if after_attempt is not None:
            self.assertTrue(isinstance(after_attempt, bool))
        if before_deadline is not None:
            self.assertTrue(isinstance(before_deadline, datetime))
        if after_deadline is not None:
            self.assertTrue(isinstance(after_deadline, datetime))

    def test_set_review_whether_correct(self):
        """Test if review is correct"""
        self.set_review_whether_correct(self.during_attempt
----------
Solution: 

        whether_correct = self.my_osid_object_form._my_map['reviewOptions']['whetherCorrect']
        if during_attempt is not None:
            whether_correct['duringAttempt'] = bool(during_attempt)
        if after_attempt is not None:
            whether_correct['afterAttempt'] = bool(after_attempt)
        if before_deadline is not None:
            whether_correct['beforeDeadline'] = bool(before_deadline)
        if after_deadline is not None:
            whether_correct['afterDeadline'] = bool(after_deadline)
--------------------
Prompt: 
def toversa(htmlsource, model, source_uri):
    """
    >>> import urllib
    >>> from versa.reader import rdfalite
    >>> from versa.driver import memory
    >>> m = memory.connection()
    >>> burl = 'http://link.delawarelibrary.org/'
    >>> with urllib.request.urlopen(burl) as resourcefp: rdfalite.toversa(resourcefp.read(), m, burl)

    """
----------
Output: 

    return _toversa(htmlsource, model, source_uri)


def _toversa(htmlsource, modeluri, source_uri=None):
    """
    >>> import urllib
    >>> from versa.reader import toversa
    >>> from versa.driver import memory
    >>> burl = 'http://link.delawarelibrary.org/'
    >>> with urllib.request.urlopen(burl) as resourcefp:
    >>>     burl = 'http://link.delawarelibrary.org/'
    >>>     with urllib.request.urlopen(burl)
----------
Solution: 

    sink = versalinks(model)
    next(sink) #Prime the coroutine
    return parse(htmlsource, sink, source_uri)
--------------------
Prompt: 
def add_filehandler(level, fmt, filename, mode, backup_count, limit, when):
    """Add a file handler to the global logger."""
----------
Output: 

    global logger
    logger = logging.getLogger(level)
    logger.addHandler(logging.FileHandler(filename, mode, delay=True))
    return


def add_stderr_logger(level, fmt, stderr, when):
    """Add a stderr logger to the global logger."""
    global logger
    logger = logging.getLogger(level)
    logger.addHandler(logging.StreamHandler(stderr))
    return


def add_file_logger(filename, fmt, level, when):
    """Add a file logger to the global logger."""
    global logger
    logger = logging.getLogger(filename)
    logger.addHandler
----------
Solution: 

    kwargs = {}
 
    # If the filename is not set, use the default filename
    if filename is None:
        filename = getattr(sys.modules['__main__'], '__file__', 'log.py')
        filename = os.path.basename(filename.replace('.py', '.log'))
        filename = os.path.join('/tmp', filename)

    if not os.path.exists(os.path.dirname(filename)):
        os.mkdir(os.path.dirname(filename))

    kwargs['filename'] = filename
 
    # Choose the filehandler based on the passed arguments
    if backup_count == 0:  # Use FileHandler
        cls = logging.FileHandler
        kwargs['mode'] = mode
    elif when is None:  # Use RotatingFileHandler
        cls = logging.handlers.RotatingFileHandler
        kwargs['maxBytes'] = limit
        kwargs['backupCount'] = backup_count
        kwargs['mode'] = mode
    else:  # Use TimedRotatingFileHandler
        cls = logging.handlers.TimedRotatingFileHandler
        kwargs['when'] = when
        kwargs['interval'] = limit
        kwargs['backupCount'] = backup_count
 
    return add_handler(cls, level, fmt, False, **kwargs)
--------------------
Prompt: 
def integer_based_slice(self, ts):
        """
        Transform a :class:`TimeSlice` into integer indices that numpy can work
        with

        Args:
            ts (slice, TimeSlice): the time slice to translate into integer
                indices
        """
----------
Output: 

        return ts.start, ts.stop

    def slice_by_time(self, ts):
        """
        Return the index of the time slice that the time slice is
        mapped to

        Args:
            ts (TimeSlice): the time slice to translate into integer indices
        """
        return self.integer_based_slice(ts.start)

    def slice_by_time_and_offset(self, ts, offset):
        """
        Transform a :class:`TimeSlice` into integer indices that numpy can work
        with

        Args:
            ts (slice, TimeSlice): the time slice to translate into integer
               
----------
Solution: 


        if isinstance(ts, slice):
            try:
                start = Seconds(0) if ts.start is None else ts.start
                if start < Seconds(0):
                    start = self.end + start
                stop = self.end if ts.stop is None else ts.stop
                if stop < Seconds(0):
                    stop = self.end + stop
                duration = stop - start
                ts = TimeSlice(start=start, duration=duration)
            except (ValueError, TypeError):
                pass

        if not isinstance(ts, TimeSlice):
            return ts

        diff = self.duration - self.frequency
        start_index = \
            max(0, np.floor((ts.start - diff) / self.frequency))
        end = self.end if ts.duration is None else ts.end


        # KLUDGE: This is basically arbitrary, but the motivation is that we'd
        # like to differentiate between cases where the slice
        # actually/intentionally overlaps a particular sample, and cases where
        # the slice overlaps the sample by a tiny amount, due to rounding or
        # lack of precision (e.g. Seconds(1) / SR44100().frequency).
        ratio = np.round(end / self.frequency, 2)


        stop_index = np.ceil(ratio)
        return slice(int(start_index), int(stop_index))
--------------------
Prompt: 
def populate_obj(self, obj, name):
        """Store file."""
----------
Output: 

        if not self.obj_type:
            self.obj_type = 'file'
        self.obj_type_id = obj.id
        self.obj_type_name = name
        self.obj_type_description = obj.description
        self.obj_type_type = obj.type_id
        self.obj_type_size = obj.size
        self.obj_type_md5 = obj.md5
        self.obj_type_sha1 = obj.sha1
        self.obj_type_sha256 = obj.sha256
        self.obj_type_
----------
Solution: 

        from abilian.core.models.blob import Blob

        delete_value = self.allow_delete and self.delete_files_index

        if not self.has_file() and not delete_value:
            # nothing uploaded, and nothing to delete
            return

        state = sa.inspect(obj)
        mapper = state.mapper
        if name not in mapper.relationships:
            # directly store in database
            return super().populate_obj(obj, name)

        rel = getattr(mapper.relationships, name)
        if rel.uselist:
            raise ValueError("Only single target supported; else use ModelFieldList")

        if delete_value:
            setattr(obj, name, None)
            return

        #  FIXME: propose option to always create a new blob
        cls = rel.mapper.class_
        val = getattr(obj, name)

        if val is None:
            val = cls()
            setattr(obj, name, val)

        data = ""
        if self.has_file():
            data = self.data
            if not issubclass(cls, Blob):
                data = data.read()

        setattr(val, self.blob_attr, data)
--------------------
Prompt: 
def list_l3_agent_hosting_routers(self, router, **_params):
        """Fetches a list of L3 agents hosting a router."""
----------
Output: 

        return self.get(self.l3_agents_path % (router),
                        params=_params)

    def list_routers_on_l3_agent(self, agent, **_params):
        """Fetches a list of routers on a particular agent."""
        return self.get(self.l3_agent_path % (agent),
                        params=_params)

    def update_router(self, router_id, **_params):
        """Updates a router."""
        return self.put(self.l3_agents_path % (router_id),
                        body={'router': update_dict
----------
Solution: 

        return self.get((self.router_path + self.L3_AGENTS) % router,
                        params=_params)
--------------------
Prompt: 
def build_from_table(self):
        """
        Generates the sql for the FROM portion of the query

        :return: the FROM portion of the query
        :rtype: str
        """
----------
Output: 

        return "FROM " + self.table_name + " WHERE " + self.where_clause + " ORDER BY " + self.order_by

    def build_where_clause(self, where_clause):
        """
        Generates the sql for the WHERE portion of the query

        :param where_clause: the WHERE portion of the query
        :type where_clause: str
        :return: the WHERE portion of the query
        :rtype: str
        """
        return where_clause

    def build_order_by(self, order_by):
        """
        Generates the sql for the ORDER BY portion of
----------
Solution: 

        table_parts = []

        # get the table sql for each table
        for table in self.tables:
            sql = table.get_sql()
            if len(sql):
                table_parts.append(sql)

        # combine all table sql separated by a comma
        sql = 'FROM {0} '.format(', '.join(table_parts))

        return sql
--------------------
Prompt: 
def publish_scene_velocity(self, scene_id, velocity):
        """publish a changed scene velovity"""
----------
Output: 

        self.send_message(
            "publish_scene_velocity",
            {"scene_id": scene_id, "velocity": velocity},
            room=self.room
        )

    def set_scene_velocity(self, scene_id, velocity):
        """set a changed scene velocity"""
        self.send_message(
            "set_scene_velocity",
            {"scene_id": scene_id, "velocity": velocity},
            room=self.room
        )

    def get_scene_velocity(self, scene_id):
        """get a changed scene velocity"""
        self.send_message
----------
Solution: 

        self.sequence_number += 1
        self.publisher.send_multipart(msgs.MessageBuilder.scene_velocity(self.sequence_number, scene_id, velocity))
        return self.sequence_number
--------------------
Prompt: 
def gen_ascii_docs(src='fastai'):
    """Generate documentation for fastai library in HTML (asciidoctor required)
    :param str src: The absolute/relative path of source file/dir
    """
----------
Output: 

    if src == 'fastai':
        return "fastai.html"
    else:
        return "html.html"


def gen_html_docs(src='html'):
    """Generate documentation for html files in HTML (asciidoctor required)
    :param str src: The absolute/relative path of source file/dir
    """
    if src == 'fastai':
        return "fastai.html"
    else:
        return "html.html"


def gen_html_docs_with_index(src='fastai'):
    """Generate documentation for html files with index.html
----------
Solution: 

    os.chdir(Path(__file__).absolute().parent)
    with working_directory('..'):
        path = Path(src)
        if path.is_dir():
            file_paths = list(path.glob('**/*.py'))
        else:
            file_paths = [path]

    pat = re.compile('^(?!__init__).*.py\Z')
    for file_path in file_paths:
        if pat.match(file_path.name):
            file_path.parent.mkdir(parents=True, exist_ok=True)
            with working_directory('..'):
                tmpl_str = parse_module(file_path)

            (file_path.parent/(file_path.name.rsplit('.',1)[0] + '.adoc.tmpl')).write_text(tmpl_str)
            (file_path.parent/(file_path.name.rsplit('.',1)[0] + '.adoc')).write_text(re.sub(r"{{(.*?)}}", parse_tmpl, tmpl_str, flags=re.DOTALL))
    if path.is_dir():
        subprocess.call(['asciidoctor', str(path) + '/**/*.adoc'])
    else:
        subprocess.call(['asciidoctor', str(path).rsplit('.',1)[0] + '.adoc'])
--------------------
Prompt: 
def set_mode_rtl(self):
        """enter RTL mode"""
----------
Output: 

        self.set_rc_mode(self.rc_mode)
        self.set_rc_mode(self.rc_mode)

    def set_mode_tone(self):
        """enter Tone mode"""
        self.set_rc_mode(self.rc_mode)
        self.set_rc_mode(self.rc_mode)
        self.set_rc_mode(self.rc_mode)
        self.set_rc_mode(self.rc_mode)
        self.set_rc_mode(self.rc_mode)
        self.set_
----------
Solution: 

        if self.mavlink10():
            self.mav.command_long_send(self.target_system, self.target_component,
                                       mavlink.MAV_CMD_NAV_RETURN_TO_LAUNCH, 0, 0, 0, 0, 0, 0, 0, 0)
        else:
            MAV_ACTION_RETURN = 3
            self.mav.action_send(self.target_system, self.target_component, MAV_ACTION_RETURN)
--------------------
Prompt: 
def validate_arguments(self, view_class, kwargs):
        """
        view_class : View Class used to render content popup dialog
        view_class must be subclass of django.views.generic.View
        """
----------
Output: 

        pass

    def get_context_data(self, **kwargs):
        """
        kwargs :
            - 'form' : form class used to create the popup
        """
        context = super(PopupView, self).get_context_data(**kwargs)
        context['popup_form'] = self.popup_form
        context['popup_title'] = self.popup_title
        context['popup_url'] = self.popup_url
        context['popup_id'] = self.popup_id
        context['popup_class'] = self.popup_class
        return context

    def get_context_data
----------
Solution: 


        # Check view_class inherit from django View
        if not issubclass(view_class, View):
            raise PopupViewIsNotSubclassView()

        self.view_class_name = view_class.__name__
        self.popup_dialog_title = kwargs.pop("popup_dialog_title", _("Popup Dialog: Select value"))

        self.callback_data = kwargs.pop("callback_data", {})
        if not isinstance(self.callback_data, dict):
            raise AttributeError("callback_data argument must be a dictionary")
        try:
            self.callback_data = urllib.urlencode(self.callback_data)
        except AttributeError:
            self.callback_data = urllib.parse.urlencode(self.callback_data)
--------------------
Prompt: 
def generate_paillier_keypair(private_keyring=None, n_length=DEFAULT_KEYSIZE):
    """Return a new :class:`PaillierPublicKey` and :class:`PaillierPrivateKey`.

    Add the private key to *private_keyring* if given.

    Args:
      private_keyring (PaillierPrivateKeyring): a
        :class:`PaillierPrivateKeyring` on which to store the private
        key.
      n_length: key size in bits.

    Returns:
      tuple: The generated :class:`PaillierPublicKey` and
      :class:`PaillierPrivateKey`
    """
----------
Output: 
PrivateKeyring` as a tuple.
    """
    if private_keyring is None:
      private_keyring = self.default_keyring
    return PaillierPublicKey(private_keyring, n_length)

  def _Generate(self, key_type, key_length, key_name, key_value, private_key_type,
              public_key_type, private_key_length, private_key_name, key_format):
    """Generate a new :class:`PaillierPublicKey` and :class:`PaillierPrivateKey`.

    Args:
      key_type (int):
        :
----------
Solution: 

    p = q = n = None
    n_len = 0
    while n_len != n_length:
        p = getprimeover(n_length // 2)
        q = p
        while q == p:
            q = getprimeover(n_length // 2)
        n = p * q
        n_len = n.bit_length()

    public_key = PaillierPublicKey(n)
    private_key = PaillierPrivateKey(public_key, p, q)

    if private_keyring is not None:
        private_keyring.add(private_key)

    return public_key, private_key
--------------------
Prompt: 
def get_dvcs_info():
    """Gets current repository info from git"""
----------
Output: 

    try:
        return subprocess.check_output(['git', 'log', '--format=%H', '--quiet', '--pretty=format:%s%n%b', '--date=short' % GIT_LOG_FORMAT], shell=True)
    except subprocess.CalledProcessError:
        return None


def get_repo_name():
    """Gets the name of the current repository"""
    return subprocess.check_output(['git', 'config', '--get','remote.origin.url'], shell=True).strip()


def get_repo_path():
    """Gets the path of the current repository"""
----------
Solution: 

    cmd = "git rev-list --count HEAD"
    commit_count = str(
        int(subprocess.check_output(shlex.split(cmd)).decode("utf8").strip())
    )
    cmd = "git rev-parse HEAD"
    commit = str(subprocess.check_output(shlex.split(cmd)).decode("utf8").strip())
    return {Constants.COMMIT_FIELD: commit, Constants.COMMIT_COUNT_FIELD: commit_count}
--------------------
Prompt: 
def randomize(self):
        """Create a new motif with shuffled positions.

        Shuffle the positions of this motif and return a new Motif instance.

        Returns
        -------
        m : Motif instance
            Motif instance with shuffled positions.
        """
----------
Output: 

        if self.shuffle:
            return self.shuffle_motif(motif=self.shuffle_motif)
        else:
            return Motif(self.motif_list, self.pos_list, self.pos_list, motif_type=self.motif_type, pos_list=self.pos_list, pos_list_name=self.pos_list_name, pos_list_type=self.pos_list_type, pos_list_type=self.pos_list_type, pos_list_name=self.pos_list_
----------
Solution: 

        random_pfm = [[c for c in row] for row in self.pfm]
        random.shuffle(random_pfm)
        m = Motif(pfm=random_pfm)
        m.id = "random"
        return m
--------------------
Prompt: 
def hasReaders(self, ulBuffer):
        """inexpensively checks for readers to allow writers to fast-fail potentially expensive copies and writes."""
----------
Output: 

        return self.hasReaders(ulBuffer) and self.reader.hasFailed()

    def hasReaders(self, ulBuffer):
        """inexpensively checks for readers to allow writers to slow-fail copies and writes."""
        return self.hasReaders(ulBuffer)

    def getLog(self):
        """returns the log of the last executed statement"""
        return self.log

    def getLogName(self):
        """returns the name of the log"""
        return self.logName

    def getLogStart(self):
        """returns the start time of the log"""
       
----------
Solution: 


        fn = self.function_table.hasReaders
        result = fn(ulBuffer)
        return result
--------------------
Prompt: 
def blank_object(obj: T, fieldlist: Sequence[str]) -> None:
    """Within "obj", sets all fields in the fieldlist to None."""
----------
Output: 

    if not obj.is_hidden:
        fieldlist = fieldlist + (None,) * len(fieldlist)


def get_field_type(field: str) -> str:
    """Get the type of the field."""
    if field.startswith("int"):
        return "int"
    elif field.startswith("float"):
        return "float"
    elif field.startswith("bool"):
        return "bool"
    elif field.startswith("enum"):
        return "enum"
    elif field.startswith("enum_list"):
        return "enum_list"
    elif field.startswith("enum_dict
----------
Solution: 

    for f in fieldlist:
        setattr(obj, f, None)
--------------------
Prompt: 
def iter_ancestors(self):
        """ 
        Iterates over the list of all ancestor nodes from 
        current node to the current tree root.
        """
----------
Output: 

        if self.is_root():
            yield self
        else:
            for child in self.children:
                for ancestor in iter_ancestors(child):
                    yield ancestor

    def iter_descendants(self):
        """ 
        Iterates over the list of all descendant nodes from 
        current node to the current tree root.
        """
        if self.is_root():
            yield self
        else:
            for child in self.children:
                for descendant in iter_descendants(descendant):
                    yield descendant

    def is_descendant_of(self, node):
        """ 
        Returns True if the node is a descendant of the current
----------
Solution: 

        node = self
        while node.up is not None:
            yield node.up
            node = node.up
--------------------
Prompt: 
def random_letters(self, length=16):
        """Returns a random letter (between a-z and A-Z)."""
----------
Output: 

        return ''.join(random.choice(self.letters) for _ in range(length))

    def random_letters_and_digits(self, length=16):
        """Returns a random letter (between A-Z and A-Z and digits)."""
        return ''.join(random.choice(self.letters + self.digits) for _ in range(length))

    def test_get_random_string(self):
        """Test getting a random string."""
        random_string = self.random_letters_and_digits()
        self.assertEqual(len(random_string), length)
        self
----------
Solution: 

        return self.random_choices(
            getattr(string, 'letters', string.ascii_letters),
            length=length,
        )
--------------------
Prompt: 
def update_status(self, id_number, new_value):
        """
        Update a status name

        :type id_number: int
        :param id_number: status ID number

        :type new_value: str
        :param new_value: The new status name

        :rtype: dict
        :return: an empty dictionary
        """
----------
Output: 

        return self.update_status_with_defaults(id_number, new_value)

    def update_status_with_defaults(self, id_number, defaults):
        """
        Update a status with default values

        :type id_number: int
        :param id_number: status ID number

        :type defaults: dict
        :param defaults: The default status values

        :type id_number: int
        :param id_number: status ID number

        :type defaults: dict
        :param id_number: status default values

        :rtype: dict
        :return: an empty dictionary
        """
----------
Solution: 

        data = {
            'id': id_number,
            'new_value': new_value
        }

        return self.post('updateStatus', data)
--------------------
Prompt: 
def save_index(self, filename):
        """ Save the current Layout's index to a .json file.

        Args:
            filename (str): Filename to write to.

        Note: At the moment, this won't serialize directory-specific config
        files. This means reconstructed indexes will only work properly in
        cases where there aren't multiple layout specs within a project.
        """
----------
Output: 

        self.index_json_filename = filename
        self.index_json_filename_json = filename + '.json'
        self.index_json_filename_json_path = os.path.join(
            self.index_json_dir, self.index_json_filename_json)
        self.index_json_filename_json_path_json = os.path.join(
            self.index_json_dir, self.index_json_filename_json_json)
        self.index_json_filename_json_path_json_json = os.path.join
----------
Solution: 

        data = {}
        for f in self.files.values():
            entities = {v.entity.id: v.value for k, v in f.tags.items()}
            data[f.path] = {'domains': f.domains, 'entities': entities}
        with open(filename, 'w') as outfile:
            json.dump(data, outfile)
--------------------
Prompt: 
def get_dep(self, name: str) -> str:
        """
        Return the uuid of the dependency identified with "name".

        :param name:
        :return: UUID
        """
----------
Output: 

        return self._get_dep_uuid(name)

    def _get_dep_uuid(self, name: str) -> str:
        """
        Return the uuid of the dependency identified with "name".

        :param name:
        :return: UUID
        """
        return self.get_dep_uuid_from_name(name)

    def get_dep_uuid_from_name(self, name: str) -> str:
        """
        Return the uuid of the dependency identified with "name".

        :param name:
        :return: UUID
        """
        return self._get_dep_name
----------
Solution: 

        deps = self.meta["dependencies"]
        for d in deps:
            if d["model"] == name:
                return d
        raise KeyError("%s not found in %s." % (name, deps))
--------------------
Prompt: 
def isempty(path):
    """Check if a path has zero length (also true if it's a directory)

    :param path: A string for the path.  This should not have any wildcards.
    :returns: True if the path has zero length, False otherwise.
    """
----------
Output: 

    if path == '':
        return True
    if path[0] == '/':
        return False
    return path.startswith(os.path.sep)

def _get_default_directory():
    """Get the default directory for the current platform.

    :returns: The default directory for the current platform.
    """
    if sys.platform == 'darwin':
        return os.path.expanduser('~/Library/Caches/MacOSX')
    elif sys.platform == 'win32':
        return os.path.expanduser('~/Library/Caches/Windows')
    elif sys.platform == 'cygwin
----------
Solution: 

    cmd = "hadoop fs -test -z %s"
    p = _hadoop_fs_command(cmd % (path))
    p.communicate()
    rcode = p.returncode
    return bool(int(rcode == 0))
--------------------
Prompt: 
def findwithin(data):
    """
Returns an integer representing a binary vector, where 1=within-
subject factor, 0=between.  Input equals the entire data 2D list (i.e.,
column 0=random factor, column -1=measured values (those two are skipped).
Note: input data is in |Stat format ... a list of lists ("2D list") with
one row per measured value, first column=subject identifier, last column=
score, one in-between column per factor (these columns contain level
designations on each factor).  See also stats.anova.__doc__.

Usage:   lfindwithin(data)     data in |Stat format
"""
----------
Output: 
.fisher_exact

"""
        return self.stat_data[data[0][0]][0][0]

    def findwithin2(self,data):
        """
Returns an integer representing a 2D list, where 1=within-
subject factor, 0=between.  Input equals the entire data column
(i.e., column 0=random factor, column -1=measured values (those two are skipped).
Note: input data is in |Stat format... a list of lists ("2D list") with
one row per measured value, first column=subject identifier,
----------
Solution: 


    numfact = len(data[0]) - 1
    withinvec = 0
    for col in range(1, numfact):
        examplelevel = pstat.unique(pstat.colex(data, col))[0]
        rows = pstat.linexand(data, col, examplelevel)  # get 1 level of this factor
        factsubjs = pstat.unique(pstat.colex(rows, 0))
        allsubjs = pstat.unique(pstat.colex(data, 0))
        if len(factsubjs) == len(allsubjs):  # fewer Ss than scores on this factor?
            withinvec = withinvec + (1 << col)
    return withinvec
--------------------
Prompt: 
def process_response(self, request, response):
        """
        Adds WWW-Authenticate: Basic headers to 401 responses, and rewrites
        redirects the login page to be 401 responses if it's a non-browser
        agent.
        """
----------
Output: 

        if not self.is_browser_agent:
            return response

        # If the user is already logged in, we're good
        if request.user.is_authenticated():
            return response

        # If we're not using the same browser, we need to use the same
        # browser for all requests
        if not self.is_browser_agent:
            return response

        # If we're using the same agent, we need to use the same request
        # for all requests
        if not self.is_browser_agent:
            return response

        # If we're using the same request, we need to use the
----------
Solution: 

        process = False

        # Don't do anything for unsecure requests, unless DEBUG is on
        if not self.allow_http and not request.is_secure():
            return response

        if response.status_code == UNAUTHORIZED:
            pass
        elif response.status_code == FOUND:
            location = urllib_parse.urlparse(response['Location'])
            if location.path != settings.LOGIN_URL:
                # If it wasn't a redirect to the login page, we don't touch it.
                return response
            elif not self.is_agent_a_robot(request):
                # We don't touch requests made in order to be shown to humans.
                return response

        realm = getattr(settings, 'BASIC_AUTH_REALM', request.META.get('HTTP_HOST', 'restricted'))
        
        if response.status_code == FOUND:
            response = self.unauthorized_view(request)

        authenticate = response.get('WWW-Authenticate', None)
        if authenticate:
            authenticate = 'Basic realm="%s", %s' % (realm, authenticate)
        else:
            authenticate = 'Basic realm="%s"' % realm
        response['WWW-Authenticate'] = authenticate

        return response
--------------------
Prompt: 
def indent_selection(self, cursor):
        """
        Indent selected text

        :param cursor: QTextCursor
        """
----------
Output: 

        cursor.beginEditBlock()
        cursor.insertText(self.selected_text)
        cursor.endEditBlock()
        self.setTextCursor(cursor)

    def indent_to_end(self):
        """
        Indent selected text

        :return: QTextBlock
        """
        cursor = self.textCursor()
        cursor.beginEditBlock()
        cursor.insertText(cursor.selectedText())
        cursor.endEditBlock()
        self.setTextCursor(cursor)

    def set_selected_text(self, text):
        """
        Set selected text

        :param text: str
        """
       
----------
Solution: 

        doc = self.editor.document()
        tab_len = self.editor.tab_length
        cursor.beginEditBlock()
        nb_lines = len(cursor.selection().toPlainText().splitlines())
        c = self.editor.textCursor()
        if c.atBlockStart() and c.position() == c.selectionEnd():
            nb_lines += 1
        block = doc.findBlock(cursor.selectionStart())
        i = 0
        # indent every lines
        while i < nb_lines:
            nb_space_to_add = tab_len
            cursor = QtGui.QTextCursor(block)
            cursor.movePosition(cursor.StartOfLine, cursor.MoveAnchor)
            if self.editor.use_spaces_instead_of_tabs:
                for _ in range(nb_space_to_add):
                    cursor.insertText(" ")
            else:
                cursor.insertText('\t')
            block = block.next()
            i += 1
        cursor.endEditBlock()
--------------------
Prompt: 
def match_envs(declared_envs, desired_envs, passthru):
    """Determine the envs that match the desired_envs.

    If ``passthru` is True, and none of the declared envs match the
    desired envs, then the desired envs will be used verbatim.

    :param declared_envs: The envs that are declared in the tox config.
    :param desired_envs: The envs desired from the tox-travis config.
    :param bool passthru: Whether to used the ``desired_envs`` as a
                          fallback if no declared envs match.
    """
----------
Output: 
 for the ``env_fallback``
                          ``env_fallback`` value.
    :return: A list of envs matching the desired config.
    :rtype: list[str]
    """
    if not isinstance(passthru, bool):
        raise TypeError('passthru must be True or False')
    return [
        env for env in
        [
            'TOX_ENV_NAME',
            'TOX_ENV_NAME_OVERRIDE',
            'TOX_ENV_NAME_OVERRIDE_OVERRIDE',
            'TOX_ENV_OVERRIDE',
            'TOX_ENV_
----------
Solution: 

    matched = [
        declared for declared in declared_envs
        if any(env_matches(declared, desired) for desired in desired_envs)
    ]
    return desired_envs if not matched and passthru else matched
--------------------
Prompt: 
def stop_archive(self, archive_id):
        """
        Stops an OpenTok archive that is being recorded.

        Archives automatically stop recording after 90 minutes or when all clients have disconnected
        from the session being archived.

        @param [String] archive_id The archive ID of the archive you want to stop recording.

        :rtype: The Archive object corresponding to the archive being stopped.
        """
----------
Output: 

        self.send_command(Command.STOP_ARCHIVE, [archive_id])
        return self.wait_for_command_complete(Command.OK)

    def get_archive_id(self, archive_id):
        """
        Gets the archive ID of the given OpenTok archive.

        @param [String] archive_id The ID of the OpenTok archive you want to stop recording.

        @rtype: [String] The ID of the OpenTok object corresponding to the archive being stopped.
        """
        self.send_command(Command.GET_ARCHIVE, [archive_id])
        return self
----------
Solution: 

        response = requests.post(self.endpoints.archive_url(archive_id) + '/stop', headers=self.json_headers(), proxies=self.proxies, timeout=self.timeout)

        if response.status_code < 300:
            return Archive(self, response.json())
        elif response.status_code == 403:
            raise AuthError()
        elif response.status_code == 404:
            raise NotFoundError("Archive not found")
        elif response.status_code == 409:
            raise ArchiveError("Archive is not in started state")
        else:
            raise RequestError("An unexpected error occurred", response.status_code)
--------------------
Prompt: 
def topic_detail(request, slug):
    """
    A detail view of a Topic

    Templates:
        :template:`faq/topic_detail.html`
    Context:
        topic
            An :model:`faq.Topic` object.
        question_list
            A list of all published :model:`faq.Question` objects that relate
            to the given :model:`faq.Topic`.

    """
----------
Output: 

    question_list = []
    if request.method == 'POST':
        form = TopicDetailForm(request.POST)
        if form.is_valid():
            question_list = form.cleaned_data['question_list']
            return HttpResponseRedirect(reverse('faq:topic_detail', args=[slug]))
    else:
        form = TopicDetailForm()
        return render(request, 'faq/topic_detail.html', {'form': form})


def topic_edit(request, slug):
    """
    A edit view of a Topic

    Templates:
        :template:`faq/topic
----------
Solution: 

    extra_context = {
        'question_list': Question.objects.published().filter(topic__slug=slug),
    }

    return object_detail(request, queryset=Topic.objects.published(),
        extra_context=extra_context, template_object_name='topic', slug=slug)
--------------------
Prompt: 
def generate_builder(self, entry_point, export_target):
        """
        Yields exactly one builder if both the provided entry point and
        export target satisfies the checks required.
        """
----------
Output: 

        if entry_point.is_literal():
            yield self.compile_literal(entry_point)
        else:
            yield self.compile_target(entry_point)

    def generate_type(self, entry_point):
        """
        Returns the type (class) of a field's value.
        """
        return entry_point.type

    def generate_size(self, entry_point):
        """
        Yields the number of bytes in the value.
        """
        yield entry_point.size

    def generate_value(self, entry_point):
        """
        Returns the value of the field
----------
Solution: 


        try:
            builder = entry_point.resolve()
        except ImportError:
            logger.error(
                "unable to import the target builder for the entry point "
                "'%s' from package '%s' to generate artifact '%s'",
                entry_point, entry_point.dist, export_target,
            )
            return

        if not self.verify_builder(builder):
            logger.error(
                "the builder referenced by the entry point '%s' "
                "from package '%s' has an incompatible signature",
                entry_point, entry_point.dist,
            )
            return

        # CLEANUP see deprecation notice below
        verifier = self.verify_export_target(export_target)
        if not verifier:
            logger.error(
                "the export target '%s' has been rejected", export_target)
            return

        toolchain, spec = self.extract_builder_result(builder(
            [entry_point.dist.project_name], export_target=export_target))
        if not toolchain:
            logger.error(
                "the builder referenced by the entry point '%s' "
                "from package '%s' failed to produce a valid "
                "toolchain",
                entry_point, entry_point.dist,
            )
            return

        if spec.get(EXPORT_TARGET) != export_target:
            logger.error(
                "the builder referenced by the entry point '%s' "
                "from package '%s' failed to produce a spec with the "
                "expected export_target",
                entry_point, entry_point.dist,
            )
            return

        if callable(verifier):
            warnings.warn(
                "%s:%s.verify_export_target returned a callable, which "
                "will no longer be passed to spec.advise by calmjs-4.0.0; "
                "please instead override 'setup_export_location' or "
                "'prepare_export_location' in that class" % (
                    self.__class__.__module__, self.__class__.__name__),
                DeprecationWarning
            )
            spec.advise(BEFORE_PREPARE, verifier, export_target)
        else:
            spec.advise(
                BEFORE_PREPARE,
                self.prepare_export_location, export_target)
        yield entry_point, toolchain, spec
--------------------
Prompt: 
def finalize(self):
        """
        Add title and modify axes to make the image ready for display.
        """
----------
Output: 

        self.axes.set_title(self.title)
        self.axes.set_xlabel(self.xlabel)
        self.axes.set_ylabel(self.ylabel)
        self.axes.set_aspect(self.aspect)
        self.axes.set_zorder(self.zorder)
        self.axes.set_zorder(self.zorder)
        self.axes.set_xlabel(self.xlabel)
        self.axes.set_ylabel(self.ylabel)
        self.axes.set_aspect(self.aspect)
        self.axes.set_
----------
Solution: 

        self.set_title(
            '{} Manifold (fit in {:0.2f} seconds)'.format(
                self._name, self.fit_time_.interval
            )
        )
        self.ax.set_xticklabels([])
        self.ax.set_yticklabels([])

        if self._target_color_type == DISCRETE:
            # Add the legend
            manual_legend(self, self.classes_, self._colors, frameon=True)

        elif self._target_color_type == CONTINUOUS:
            # Add the color bar
            plt.colorbar(self._scatter, ax=self.ax)
--------------------
Prompt: 
def add_gate_option_group(parser):
    """Adds the options needed to apply gates to data.

    Parameters
    ----------
    parser : object
        ArgumentParser instance.
    """
----------
Output: 

    parser.add_argument('gate_option_group',
                        help='Gates to add to the model.')
    parser.add_argument('--model_name',
                        help='Name of the model. If not given, the model name will be '
                             'generated automatically from the model directory.')
    parser.add_argument('--model_dir',
                        help='Directory where the generated model will be saved.')
    parser.add_argument('--save_dir',
                        help='Directory where the generated model will be saved.')
    parser.add_argument('--save_prefix',
                        help='Prefix for the generated model names
----------
Solution: 

    gate_group = parser.add_argument_group("Options for gating data.")

    gate_group.add_argument("--gate", nargs="+", type=str,
                            metavar="IFO:CENTRALTIME:HALFDUR:TAPERDUR",
                            help="Apply one or more gates to the data before "
                                 "filtering.")
    gate_group.add_argument("--gate-overwhitened", action="store_true",
                            help="Overwhiten data first, then apply the "
                                 "gates specified in --gate. Overwhitening "
                                 "allows for sharper tapers to be used, "
                                 "since lines are not blurred.")
    gate_group.add_argument("--psd-gate", nargs="+", type=str,
                            metavar="IFO:CENTRALTIME:HALFDUR:TAPERDUR",
                            help="Apply one or more gates to the data used "
                                 "for computing the PSD. Gates are applied "
                                 "prior to FFT-ing the data for PSD "
                                 "estimation.")
    return gate_group
--------------------
Prompt: 
def add_atom_data(data_api, data_setters, atom_names, element_names, atom_charges, group_atom_ind):
    """Add the atomic data to the DataTransferInterface.
    :param data_api the data api from where to get the data
    :param data_setters the class to push the data to
    :param atom_nams the list of atom names for the group
    :param element_names the list of element names for this group
    :param atom_charges the list formal atomic charges for this group
    :param group_atom_ind the index of this atom in the group"""
----------
Output: 
_ind the list of group atom indices for this group
    :return the number of atoms added
    """
    if group_atom_ind is None:
        group_atom_ind = group_atom_names

    # Add the atomic data to the DataTransferInterface
    for i in range(len(data_setters)):
        data_setters[i].add_atom(atom_nams[group_atom_ind[i]])
        data_setters[i].add_element(element_names[group_atom_ind[i]])
        data_setters[i].add_atom(
----------
Solution: 

    atom_name = atom_names[group_atom_ind]
    element = element_names[group_atom_ind]
    charge = atom_charges[group_atom_ind]
    alternative_location_id = data_api.alt_loc_list[data_api.atom_counter]
    serial_number = data_api.atom_id_list[data_api.atom_counter]
    x = data_api.x_coord_list[data_api.atom_counter]
    y = data_api.y_coord_list[data_api.atom_counter]
    z = data_api.z_coord_list[data_api.atom_counter]
    occupancy = data_api.occupancy_list[data_api.atom_counter]
    temperature_factor = data_api.b_factor_list[data_api.atom_counter]
    data_setters.set_atom_info(atom_name, serial_number, alternative_location_id,
                               x, y, z, occupancy, temperature_factor, element, charge)
--------------------
Prompt: 
def save(self, path: Union[str, bytes, int]) -> None:
        """Write QASM output to a file specified by path."""
----------
Output: 

        with open(path, "wb") as f:
            f.write(self.qasm_header)
            f.write(self.qasm_body)
            f.write(self.qasm_footer)
            f.write(self.qasm_body)
            f.write(self.qasm_footer)
            f.write(self.qasm_body)
            f.write(self.qasm_footer)

    def load(self, path: Union[str, bytes, int]) -> None:
        """Read QASM output from a file specified by path."""
----------
Solution: 

        with open(path, 'w') as f:
            def write(s: str) -> None:
                f.write(s)
            self._write_qasm(write)
--------------------
Prompt: 
def get_members(self, api=None):
        """Retrieve dataset members
        :param api: Api instance
        :return: Collection object
        """
----------
Output: 

        if not api:
            api = self.api
        if not self.is_valid_id(api.dataset_id):
            raise ValueError('Invalid dataset id')
        if not self.is_valid_id(api.dataset_name):
            raise ValueError('Invalid dataset name')
        if not self.is_valid_name(api.dataset_name):
            raise ValueError('Invalid dataset name')
        if not self.is_valid_name(api.dataset_name):
            raise ValueError('Invalid dataset name')
        if not self.is_valid_name(api.dataset_name):
----------
Solution: 

        api = api or self._API

        response = api.get(url=self._URL['members'].format(id=self.id))

        data = response.json()
        total = response.headers['x-total-matching-query']
        members = [Member(api=api, **member) for member in data['items']]
        links = [Link(**link) for link in data['links']]
        href = data['href']

        return Collection(
            resource=Member,
            href=href,
            total=total,
            items=members,
            links=links,
            api=api
        )
--------------------
Prompt: 
def ReadAttachments(self, document_link, options=None):
        """Reads all attachments in a document.

        :param str document_link:
            The link to the document.
        :param dict options:
            The request options for the request.

        :return:
            Query Iterable of Attachments.
        :rtype:
            query_iterable.QueryIterable

        """
----------
Output: 

        return self.client.ReadAttachments(document_link, options)

    def ReadAttachmentsWithText(self, document_text, options=None):
        """Reads all attachments with a given text.

        :param str document_text:
            The text to search for.
        :param dict options:
            The request options for the request.

        :return:
            Query Iterable of AttachmentsWithText.
        :rtype:
            query_iterable.QueryIterable

        """
        return self.client.ReadAttachmentsWithText(document_text, options)

    def ReadAttachmentsWithImage(self, document
----------
Solution: 

        if options is None:
            options = {}

        return self.QueryAttachments(document_link, None, options)
--------------------
Prompt: 
def discover(cls, path, depth="0"):
        """Discover a list of collections under the given ``path``.

        If ``depth`` is "0", only the actual object under ``path`` is
        returned.

        If ``depth`` is anything but "0", it is considered as "1" and direct
        children are included in the result.

        The ``path`` is relative.

        The root collection "/" must always exist.

        """
----------
Output: 

        return cls.objects.get(path=path, depth=depth)

    @classmethod
    def get_all_objects(cls):
        """Get all collections.

        Returns:
            Collections: A list of all collections.

        """
        return cls.objects.filter(depth=0)

    @classmethod
    def get_all_children(cls):
        """Get all collections children.

        Returns:
            Collections: A list of all collections children.

        """
        return cls.objects.filter(depth=1)

    @classmethod
    def get_all_children_count(cls):
        """Get
----------
Solution: 


        # Path should already be sanitized
        attributes = _get_attributes_from_path(path)

        try:
            if len(attributes) == 3:
                # If an item, create a collection for the item.
                item = attributes.pop()
                path = "/".join(attributes)
                collection = cls(path, _is_principal(path))
                yield collection.get(item)
                return

            collection = cls(path, _is_principal(path))
        except api.exceptions.DoesNotExist:
            return

        yield collection

        if depth == "0":
            return

        if len(attributes) == 0:
            yield cls(posixpath.join(path, cls.user), principal=True)
        elif len(attributes) == 1:
            for journal in cls.etesync.list():
                if journal.collection.TYPE in (api.AddressBook.TYPE, api.Calendar.TYPE, api.TaskList.TYPE):
                    yield cls(posixpath.join(path, journal.uid), principal=False)
        elif len(attributes) == 2:
            for item in collection.list():
                yield collection.get(item)

        elif len(attributes) > 2:
            raise RuntimeError("Found more than one attribute. Shouldn't happen")
--------------------
Prompt: 
def Release(self):
    """Remove the lock.

    Note that this only resets the lock if we actually hold it since
    lock_expiration == self.expires and lock_owner = self.lock_token.
    """
----------
Output: 

    if self.expires < self.lock_expiration:
      self.expires = self.lock_expiration
      self.owner = None
    else:
      self.expires = None
      self.owner = None
    return self.expires

  def __init__(self, lock_token, lease_seconds=0):
    """Constructor.

    Args:
      lock_token: A string used as the token to use to validate the lock.
      lease_seconds: The time to live for the lock.
    """
    self.lock_token = lock_token
    self.lease_seconds = lease_seconds
    self.
----------
Solution: 

    if self.locked:
      query = ("UPDATE locks SET lock_expiration=0, lock_owner=0 "
               "WHERE lock_expiration=%s "
               "AND lock_owner=%s "
               "AND subject_hash=unhex(md5(%s))")
      args = [self.expires, self.lock_token, self.subject]
      self.store.ExecuteQuery(query, args)
      self.locked = False
--------------------
Prompt: 
async def _quit(self):
        """Quits the bot."""
----------
Output: 

        await self.bot.say("Goodbye.")

    @commands.command(pass_context=True, no_pm=True)
    async def _start(self, ctx):
        """Starts the bot."""
        if ctx.message.server.id not in self.bot.servers:
            await self.bot.say("This server is not registered.")
            return

        self.bot.servers[ctx.message.server.id].start()
        await self.bot.say("Logged in as")

    @commands.command(pass_context=True, no_pm=True)
    async def
----------
Solution: 

        await self.bot.responses.failure(message="Bot shutting down")
        await self.bot.logout()
--------------------
Prompt: 
def _set_textarea(el, value):
        """
        Set content of given textarea element `el` to `value`.

        Args:
            el (obj): Reference to textarea element you wish to set.
            value (obj/list): Value to which the `el` will be set.
        """
----------
Output: 

        if not isinstance(value, list):
            value = [value]
        for el in value:
            if el not in el:
                raise ValueError('Textarea element %s not in %s' % (el, value))
            el = el.replace(el, el.get(el, el))
        el.text = value

    def _set_textbox(self, value):
        """
        Set content of given textbox element `value` to `textbox`.

        Args:
            value (str): Reference to textbox element you wish to set.
        """
        if not isinstance(value,
----------
Solution: 

        if isinstance(value, dict):
            el.text = value["val"]
        elif type(value) in [list, tuple]:
            el.text = "\n\n".join(
                "-- %s --\n%s" % (item["source"], item["val"])
                for item in value
            )
        else:
            el.text = value
--------------------
Prompt: 
def load_obs(self,  mask_threshold=0.5):
        """
        Loads observations and masking grid (if needed).

        :param mask_threshold: Values greater than the threshold are kept, others are masked.
        :return:
        """
----------
Output: 

        self.mask_threshold = mask_threshold
        self.obs_mask = np.zeros(self.grid.shape, dtype=np.bool)
        self.obs_mask[self.grid] = True
        self.mask_grid = np.zeros(self.grid.shape, dtype=np.bool)

    def load_obs_grid(self,  grid_size=None, mask_threshold=0.5):
        """
        Loads grid (if needed).

        :param grid_size: Values greater than the size of the grid (if None, the size of the whole grid
----------
Solution: 

        start_date = self.run_date + timedelta(hours=self.start_hour)
        end_date = self.run_date + timedelta(hours=self.end_hour)
        mrms_grid = MRMSGrid(start_date, end_date, self.mrms_variable, self.mrms_path)
        mrms_grid.load_data()
        if len(mrms_grid.data) > 0:
            self.raw_obs[self.mrms_variable] = np.where(mrms_grid.data > 100, 100, mrms_grid.data)
            self.window_obs[self.mrms_variable] = np.array([self.raw_obs[self.mrms_variable][sl].max(axis=0)
                                                            for sl in self.hour_windows])
            if self.obs_mask:
                mask_grid = MRMSGrid(start_date, end_date, self.mask_variable, self.mrms_path)
                mask_grid.load_data()
                self.raw_obs[self.mask_variable] = np.where(mask_grid.data >= mask_threshold, 1, 0)
                self.window_obs[self.mask_variable] = np.array([self.raw_obs[self.mask_variable][sl].max(axis=0)
                                                               for sl in self.hour_windows])
--------------------
Prompt: 
def generate(self, file, validatedata = None,  inputdata=None, user = None):
        """Convert the template into instantiated metadata, validating the data in the process and returning errors otherwise. inputdata is a dictionary-compatible structure, such as the relevant postdata. Return (success, metadata, parameters), error messages can be extracted from parameters[].error. Validatedata is a (errors,parameters) tuple that can be passed if you did validation in a prior stage, if not specified, it will be done automatically."""
----------
Output: 

        self.template = Template(file, self.template_vars)
        self.parameters = self.template.parameters
        self.error = validatedata[0]
        self.error_message = validatedata[1]
        self.parameters_error = validatedata[2]
        self.parameters_parameters = self.template.parameters_parameters
        self.parameters_valid = self.template.parameters_parameters_valid
        self.parameters_valid_message = validatedata[3]
        self.parameters_valid_error = validatedata[4]
        self.parameters_valid_parameters = self.
----------
Solution: 


        metadata = {}

        if not validatedata:
            assert inputdata
            errors, parameters = self.validate(inputdata,user) #pylint: disable=unused-variable
        else:
            errors, parameters = validatedata #pylint: disable=unused-variable

        #scan errors and set metadata
        success = True
        for parameter in parameters:
            assert isinstance(parameter, clam.common.parameters.AbstractParameter)
            if parameter.error:
                success = False
            else:
                metadata[parameter.id] = parameter.value

        if not success:
            metadata = None
        else:
            try:
                metadata = self.formatclass(file, **metadata)
            except:
                raise

        return success, metadata, parameters
--------------------
Prompt: 
def nlmsg_len(self, value):
        """Length setter."""
----------
Output: 

        self._nlmsg_len = value

    @property
    def nlmsg_type(self):
        """Type setter."""
        return self._nlmsg_type

    @property
    def nlmsg_flags(self):
        """Flags setter."""
        return self._nlmsg_flags

    @property
    def nlmsg_len(self):
        """Length setter."""
        return self._nlmsg_len

    @property
    def nlmsg_type(self):
        """Type setter."""
        return self._nlmsg_type

    @property
    def nlmsg_flags(self):
        """Flags setter."""
        return
----------
Solution: 

        self.bytearray[self._get_slicers(0)] = bytearray(c_uint32(value or 0))
--------------------
Prompt: 
def user_absent(name):
    """
    Ensure a user is not present

    name
        username to remove if it exists

    Examples:

    .. code-block:: yaml

        delete:
          onyx.user_absent:
            - name: daniel
    """
----------
Output: 

    try:
        if name in get_user_list():
            del_user(name)
    except KeyError:
        pass


def del_user(name):
    """
    Delete a user

    name
        username to remove

    Examples:

   .. code-block:: yaml

        delete:
          onyx.user_absent:
            - name: daniel
    """
    try:
        if name in get_user_list():
            del_user(name)
    except KeyError:
        pass


def get_user_list():
    """
    Return a list of all user names

    Returns:
        list of
----------
Solution: 


    ret = {'name': name,
           'result': False,
           'changes': {},
           'comment': ''}

    old_user = __salt__['onyx.cmd']('get_user', username=name)

    if not old_user:
        ret['result'] = True
        ret['comment'] = 'User does not exist'
        return ret

    if __opts__['test'] is True and old_user:
        ret['result'] = None
        ret['comment'] = 'User will be removed'
        ret['changes']['old'] = old_user
        ret['changes']['new'] = ''
        return ret

    __salt__['onyx.cmd']('remove_user', username=name)

    if __salt__['onyx.cmd']('get_user', username=name):
        ret['comment'] = 'Failed to remove user'
    else:
        ret['result'] = True
        ret['comment'] = 'User removed'
        ret['changes']['old'] = old_user
        ret['changes']['new'] = ''
    return ret
--------------------
Prompt: 
def relations_of(self, target, include_object=False):
        """ list all relations the originate from target """
----------
Output: 

        if include_object:
            return [rel for rel in self.model.objects.filter(target=target) if rel.from_field == target]
        return [rel for rel in self.model.objects.filter(target=target) if rel.to_field == target]

    def get_related_objects(self, target, relation_name):
        """ returns a list of objects that match the given relation """
        return [rel for rel in self.relations_of(target, include_object=True) if rel.to_field == relation_name]

    def get_related
----------
Solution: 

        relations = (target if isinstance(target, RamGraphDBNode) else self._get_item_node(target)).outgoing
        if include_object:
            for k in relations:
                for v in relations[k]:
                    if hasattr(v, 'obj'): # filter dead links
                        yield k, v.obj
        else:
            yield from relations
--------------------
Prompt: 
def _convert_md_type(self, type_to_convert: str):
        """Metadata types are not consistent in Isogeo API. A vector dataset is
         defined as vector-dataset in query filter but as vectorDataset in
         resource (metadata) details.

        see: https://github.com/isogeo/isogeo-api-py-minsdk/issues/29
        """
----------
Output: 

        if type_to_convert =='vector':
            return self._convert_vector_dataset(
                self.resource_type, self.vector_dataset_id)
        elif type_to_convert =='resource':
            return self._convert_resource_dataset(
                self.resource_type, self.resource_id)
        else:
            raise ValueError(
                'Unknown dataset type: {0}'.format(type_to_convert))

    def _convert_vector_dataset(self, vector_type: str, dataset_id: str):
        """
        Convert a vector dataset type to a vector
----------
Solution: 

        if type_to_convert in FILTER_TYPES:
            return FILTER_TYPES.get(type_to_convert)
        elif type_to_convert in FILTER_TYPES.values():
            return [k for k, v in FILTER_TYPES.items() if v == type_to_convert][0]
        else:
            raise ValueError(
                "Incorrect metadata type to convert: {}".format(type_to_convert)
            )
--------------------
Prompt: 
def check_stripe_api_version(app_configs=None, **kwargs):
	"""Check the user has configured API version correctly."""
----------
Output: 

	if not app_configs:
		app_configs = frappe.get_all("Stripe Settings")
		frappe.cache().hset("stripe_settings", "api_version", "v1")
		frappe.cache().hset("stripe_settings", "api_version", "v2")
		frappe.cache().hset("stripe_settings", "user_id", "test@example.com")
		frappe.db.commit()
		frappe.cache().hset("stripe_settings", "user_id", "test@example.com")
	
----------
Solution: 

	from . import settings as djstripe_settings

	messages = []
	default_version = djstripe_settings.DEFAULT_STRIPE_API_VERSION
	version = djstripe_settings.get_stripe_api_version()

	if not validate_stripe_api_version(version):
		msg = "Invalid Stripe API version: {}".format(version)
		hint = "STRIPE_API_VERSION should be formatted as: YYYY-MM-DD"
		messages.append(checks.Critical(msg, hint=hint, id="djstripe.C004"))

	if version != default_version:
		msg = (
			"The Stripe API version has a non-default value of '{}'. "
			"Non-default versions are not explicitly supported, and may "
			"cause compatibility issues.".format(version)
		)
		hint = "Use the dj-stripe default for Stripe API version: {}".format(default_version)
		messages.append(checks.Warning(msg, hint=hint, id="djstripe.W001"))

	return messages
--------------------
Prompt: 
def iter(self, dir_entries):
        """
        :param dir_entries: list of os.DirEntry() instances
        """
----------
Output: 

        for entry in dir_entries:
            if entry.name.startswith('.') or entry.is_dir:
                yield entry

    def find_plugin(self, name):
        """
        :param name: the name of the plugin to load
        """
        if self.plugins is not None and name in self.plugins:
            return self.plugins[name]
        else:
            if name in self.plugin_modules:
                module = self.plugin_modules[name]
                if hasattr(module,'setup'):
                    yield module
                else:
                    yield self.load_plugin(name)
            else:
               
----------
Solution: 

        filter = self.filter
        for entry in dir_entries:
            path = filter(Path2(entry.path))
            if path != False:
                yield path
--------------------
Prompt: 
def temporary_file(contents, suffix='', prefix='tmp'):
    """Creates a temporary file with specified contents that persists for the context.

    :param contents: binary string that will be written to the file.
    :param prefix: will be prefixed to the filename.
    :param suffix: will be appended to the filename.
    :returns path of the created file.
    """
----------
Output: 

    if not isinstance(contents, six.binary_type):
        contents = contents.encode('utf-8')
    with tempfile.NamedTemporaryFile(prefix=prefix, suffix=suffix,
                                  delete=False) as f:
        f.write(contents)
        f.flush()
        os.fsync(f.fileno())
    return path


def _get_disk_of_partition(session, partition_path):
    device_path = utils.make_dev_path(session, partition_path)
    if session.is_local_connection:
        return device_path
    try:
        partitions
----------
Solution: 

    tmp = tempfile.NamedTemporaryFile(suffix=suffix, prefix=prefix, delete=False)
    with tmp:
        tmp.write(contents)

    try:
        yield tmp.name
    finally:
        os.unlink(tmp.name)
--------------------
Prompt: 
def parent(self):
        """
        Return the parent scope.

        :return: FoldScope or None
        """
----------
Output: 

        return self._parent

    @property
    def parent_scope(self):
        """
        Return the parent scope.

        :return: FoldScope or None
        """
        return self._parent_scope

    @property
    def scope(self):
        """
        Return the scope.

        :return: FoldScope
        """
        return self._scope

    def __init__(self, parent=None, scope=None):
        """
        Create a new FoldScope object.

        :param parent: The parent scope.
        :type parent: FoldScope or None
        :param scope: The scope.
        :type
----------
Solution: 

        if TextBlockHelper.get_fold_lvl(self._trigger) > 0 and \
                self._trigger.blockNumber():
            block = self._trigger.previous()
            ref_lvl = self.trigger_level - 1
            while (block.blockNumber() and
                    (not TextBlockHelper.is_fold_trigger(block) or
                     TextBlockHelper.get_fold_lvl(block) > ref_lvl)):
                block = block.previous()
            try:
                return FoldScope(block)
            except ValueError:
                return None
        return None
--------------------
Prompt: 
def parse(input):
    """
    parse input to datetime
    """
----------
Output: 

    return datetime.strptime(input, '%Y-%m-%d %H:%M:%S')


def get_date_range(start, end):
    """
    get date range from start to end
    """
    start = datetime.strptime(start, '%Y-%m-%d %H:%M:%S')
    end = datetime.strptime(end, '%Y-%m-%d %H:%M:%S')
    return start, end


def get_date_range_str(start, end):
    """
    get date range from start to end
    """
    start = datetime.strptime
----------
Solution: 

    if isinstance(input, datetime):
        return input
    if isinstance(input, date):
        return date_to_datetime(input)
    if isinstance(input, time):
        return time_to_datetime(input)
    if isinstance(input, (int, float)):
        return timestamp_to_datetime(input)
    if isinstance(input, (str)):
        return string_to_data_time(input)
    return None
--------------------
Prompt: 
def tb_capture(func):
    """A decorator which captures worker tracebacks.

    Tracebacks in particular, are captured. Inspired by an example in
    https://bugs.python.org/issue13831.

    This decorator wraps rio-mucho worker tasks.

    Parameters
    ----------
    func : function
        A function to be decorated.

    Returns
    -------
    func

    """
----------
Output: 

    def wrapper(func):
        @functools.wraps(func)
        def wrapper_task(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                logger.error("Exception in worker: %s", e)
                raise
        return wrapper_task
    return wrapper


def _get_worker_name(worker_id):
    """Get the name of the rio-mucho worker.

    Parameters
    ----------
    worker_id : str
        The name of the rio-mucho worker.

    Returns
    -------
    str
        The name
----------
Solution: 


    @wraps(func)
    def wrapper(*args, **kwds):
        try:
            return func(*args, **kwds)

        except Exception:
            raise MuchoChildError()

    return wrapper
--------------------
Prompt: 
def xmltreefromfile(filename):
    """Internal function to read an XML file"""
----------
Output: 

    return ElementTree.parse(filename)

def xmltreeread(filename, events):
    """Read an XML file and return a list of events"""
    return xmltreefromfile(filename)

def xmltreewrite(filename, events):
    """Internal function to write an XML file"""
    return ElementTree.write(filename, events, encoding="utf-8")

def xmltreewritexml(filename, events):
    """Write an XML file"""
    return xmltreewrite(filename, events, encoding="utf-8")

def xmltreewriteevents(events):
    """Write an events list
----------
Solution: 

    try:
        return ElementTree.parse(filename, ElementTree.XMLParser(collect_ids=False))
    except TypeError:
        return ElementTree.parse(filename, ElementTree.XMLParser())
--------------------
Prompt: 
def asluav_status_encode(self, LED_status, SATCOM_status, Servo_status, Motor_rpm):
                """
                Extended state information for ASLUAVs

                LED_status                : Status of the position-indicator LEDs (uint8_t)
                SATCOM_status             : Status of the IRIDIUM satellite communication system (uint8_t)
                Servo_status              : Status vector for up to 8 servos (uint8_t)
                Motor_rpm                 : Motor RPM (float)

                """
----------
Output: 

                return asluav_status_encode(self.status, LED_status, SATCOM_status,
                                              self.servo_rpm_to_asluav(Motor_rpm),
                                              self.asluav_position_indicator_rpm_to_servo(LED_position_indicator_rpm),
                                              self.asluav_servo_position_sensor_rpm_to_asluav(Servo_position_sensor_rpm),
                                              self.asluav_servo_asluav_position_sensor_rpm(Servo_status))

        @staticmethod
        def
----------
Solution: 

                return MAVLink_asluav_status_message(LED_status, SATCOM_status, Servo_status, Motor_rpm)
--------------------
Prompt: 
def config(self):
        """ Writeable configuration parameters.

        :rtype:     dict
        """
----------
Output: 

        return {
            'name': self.name,
           'version': self.version,
            'description': self.description,
            'author': self.author,
            'author_email': self.author_email,
            'url': self.url,
            'download_url': self.download_url,
            'issue_tracker': self.issue_tracker,
            'issue_tracker_email': self.issue_tracker_email,
            'issue_tracker_name': self.issue_tracker_name,
            'project_id': self.project_id,
            'issue_id
----------
Solution: 

        config = self._get_config()
        return {section: {itm.name: itm for itm in config[section].values()
                          if not itm.readonly}
                for section in config
                if 'settings' in section or section == 'other'}
--------------------
Prompt: 
def _parse_bbox_list(bbox_list):
        """ Helper method for parsing a list of bounding boxes
        """
----------
Output: 

        bbox_list = bbox_list.split(",")
        bbox_list = [float(x) for x in bbox_list]
        return bbox_list

    def _parse_bbox_string(bbox_string):
        """ Helper method for parsing a string of bboxes
        """
        bbox_list = bbox_string.split(",")
        bbox_string = bbox_string.replace(" ", "")
        return bbox_list

    # -------------------------------------------------------------------------
    def _get_features(self, row):
        """
            Get the features for the specified Row

            @param row: the gis_location Row
        """
        return row
----------
Solution: 

        if isinstance(bbox_list, BBoxCollection):
            return bbox_list.bbox_list, bbox_list.crs

        if not isinstance(bbox_list, list) or not bbox_list:
            raise ValueError('Expected non-empty list of BBox objects')

        for bbox in bbox_list:
            if not isinstance(bbox, BBox):
                raise ValueError('Elements in the list should be of type {}, got {}'.format(BBox.__name__, type(bbox)))

        crs = bbox_list[0].crs
        for bbox in bbox_list:
            if bbox.crs is not crs:
                raise ValueError('All bounding boxes should have the same CRS')

        return bbox_list, crs
--------------------
Prompt: 
def fetch(self, failures=True, wait=0):
        """
        get the task result objects from the chain when it finishes. blocks until timeout.
        :param failures: include failed tasks
        :param int wait: how many milliseconds to wait for a result
        :return: an unsorted list of task objects
        """
----------
Output: 

        if not self.tasks:
            return []

        # sort tasks by priority
        tasks = sorted(self.tasks, key=lambda t: t.priority)

        # loop through the tasks
        for task in tasks:
            if task.status == 'failed':
                # if the task is a failed task, we don't want to wait
                if wait and task.status!= 'timeout':
                    return []
            elif task.status == 'finished':
                # if the task is finished, we don't want to wait
                if task.finished:
                    return []

            # if the task is a running task, we
----------
Solution: 

        if self.started:
            return fetch_group(self.group, failures=failures, wait=wait, count=self.length(), cached=self.cached)
--------------------
Prompt: 
def update_serviceprofile(self, host_id, vlan_id):
        """Top level method to update Service Profiles on UCS Manager.

        Calls all the methods responsible for the individual tasks that
        ultimately result in a vlan_id getting programed on a server's
        ethernet ports and the Fabric Interconnect's network ports.
        """
----------
Output: 

        return self.api.update_port(host_id, {'vlan_id': vlan_id})

    def delete_serviceprofile(self, host_id):
        """Top level method to delete the Service Profiles on UCS Manager.

        Calls all the methods responsible for the individual tasks that
        ultimately result in a host_id getting network ports on a server's
        fabric ports.
        """
        return self.api.delete_port(host_id, {'vlan_id': host_id})

    def get_serviceprofile(self, host_id):
        """Retrieves the network port
----------
Solution: 

        ucsm_ip = self.get_ucsm_ip_for_host(host_id)
        if not ucsm_ip:
            LOG.info('UCS Manager network driver does not have UCSM IP '
                     'for Host_id %s', str(host_id))
            return False

        service_profile = self.ucsm_sp_dict.get((ucsm_ip, host_id))
        if service_profile:
            LOG.debug('UCS Manager network driver Service Profile : %s',
                      service_profile)
        else:
            LOG.info('UCS Manager network driver does not support '
                     'Host_id %s', host_id)
            return False

        with self.ucsm_connect_disconnect(ucsm_ip) as handle:
            # Create Vlan Profile
            if not self._create_vlanprofile(handle, vlan_id, ucsm_ip):
                LOG.error('UCS Manager network driver failed to create '
                          'Vlan Profile for vlan %s', str(vlan_id))
                return False

            # Update Service Profile
            if not self._update_service_profile(handle,
                                                service_profile,
                                                vlan_id,
                                                ucsm_ip):
                LOG.error('UCS Manager network driver failed to update '
                          'Service Profile %(service_profile)s in UCSM '
                          '%(ucsm_ip)s',
                    {'service_profile': service_profile, 'ucsm_ip': ucsm_ip})
                return False

        return True
--------------------
Prompt: 
def build_traversal(self, traversal):
        """
        traverse a relationship from a node to a set of nodes
        """
----------
Output: 

        if traversal.direction == 'left':
            traversal.direction = 'right'
        elif traversal.direction == 'right':
            traversal.direction = 'left'
        else:
            traversal.direction = 'right'
        return traversal

    def build_path(self, path):
        """
        build a path from a path to a set of nodes
        """
        if path.type == 'way':
            self.ways.append(path)
        elif path.type =='relation':
            self.relations.append(path)
        elif path.type == 'path':
            self.paths.append(path)
----------
Solution: 

        # build source
        rhs_label = ':' + traversal.target_class.__label__

        # build source
        lhs_ident = self.build_source(traversal.source)
        rhs_ident = traversal.name + rhs_label
        self._ast['return'] = traversal.name
        self._ast['result_class'] = traversal.target_class

        rel_ident = self.create_ident()
        stmt = _rel_helper(lhs=lhs_ident, rhs=rhs_ident, ident=rel_ident, **traversal.definition)
        self._ast['match'].append(stmt)

        if traversal.filters:
            self.build_where_stmt(rel_ident, traversal.filters)

        return traversal.name
--------------------
Prompt: 
def _derX(self,x,y,z):
        """
        Returns the derivative with respect to x of the interpolated function
        at each value in x,y,z. Only called internally by HARKinterpolator3D.derivativeX.
        """
----------
Output: 

        return self.HARKinterpolator3D._derX(x,y,z)

    def _derY(self,x,y,z):
        """
        Returns the derivative with respect to x of the interpolated function
        at each value in x,y,z. Only called internally by HARKinterpolator3D.derivativeY.
        """
        return self.HARKinterpolator3D._derY(x,y,z)

    def _derZ(self,x,y,z):
        """
        Returns the derivative with respect to x of the interpolated function
        at each
----------
Solution: 

        if _isscalar(x):
            x_pos = max(min(self.xSearchFunc(self.x_list,x),self.x_n-1),1)
            y_pos = max(min(self.ySearchFunc(self.y_list,y),self.y_n-1),1)
            z_pos = max(min(self.zSearchFunc(self.z_list,z),self.z_n-1),1)
        else:
            x_pos = self.xSearchFunc(self.x_list,x)
            x_pos[x_pos < 1] = 1
            x_pos[x_pos > self.x_n-1] = self.x_n-1
            y_pos = self.ySearchFunc(self.y_list,y)
            y_pos[y_pos < 1] = 1
            y_pos[y_pos > self.y_n-1] = self.y_n-1
            z_pos = self.zSearchFunc(self.z_list,z)
            z_pos[z_pos < 1] = 1
            z_pos[z_pos > self.z_n-1] = self.z_n-1
        beta = (y - self.y_list[y_pos-1])/(self.y_list[y_pos] - self.y_list[y_pos-1])
        gamma = (z - self.z_list[z_pos-1])/(self.z_list[z_pos] - self.z_list[z_pos-1])
        dfdx = (
           (  (1-beta)*(1-gamma)*self.f_values[x_pos,y_pos-1,z_pos-1]
           +  (1-beta)*gamma*self.f_values[x_pos,y_pos-1,z_pos]
           +  beta*(1-gamma)*self.f_values[x_pos,y_pos,z_pos-1]
           +  beta*gamma*self.f_values[x_pos,y_pos,z_pos]) -
           (  (1-beta)*(1-gamma)*self.f_values[x_pos-1,y_pos-1,z_pos-1]
           +  (1-beta)*gamma*self.f_values[x_pos-1,y_pos-1,z_pos]
           +  beta*(1-gamma)*self.f_values[x_pos-1,y_pos,z_pos-1]
           +  beta*gamma*self.f_values[x_pos-1,y_pos,z_pos]))/(self.x_list[x_pos] - self.x_list[x_pos-1])
        return dfdx
--------------------
Prompt: 
def check_failed_login(self):
        """
        'Private method', check failed logins, it's used for wath_login decorator
        """
----------
Output: 

        if self.request.user.is_authenticated():
            return True
        return False

    def wath_login(self):
        """
        'Private method', waths up the login process
        """
        if self.request.user.is_authenticated():
            return self.request.user.get_profile().wath_login()
        else:
            return False

    def get_context_data(self, **kwargs):
        """
        Returns context data used for template rendering
        """
        context = super(LoginRequiredMixin, self).get_context_data(**kwargs)
        context['login_url
----------
Solution: 

        last_attempt = self.get_last_failed_access_attempt()

        if not last_attempt:
            # create a new entry
            user_access = self._FailedAccessAttemptModel(ip_address=self.ip)
        elif last_attempt:
            user_access = last_attempt

        if self.request.method == 'POST':
            if self.username is None:
                raise DobermanImproperlyConfigured(
                    "Bad username form field, if you are using a custom field please configure: "
                    "DOBERMAN_USERNAME_FORM_FIELD via settings."
                )

            if self.response.status_code != 302:

                user_access.user_agent = self.request.META.get('HTTP_USER_AGENT', '<unknown user agent>')[:255]
                user_access.username = self.username
                user_access.failed_attempts += 1
                user_access.params_get = self.request.GET
                user_access.params_post = self.request.POST

                if user_access.failed_attempts >= self.max_failed_attempts:
                    user_access.is_locked = True

                user_access.save()

            elif self.response.status_code == 302 and not user_access.is_locked:
                user_access.is_expired = True
                user_access.save()

        return user_access
--------------------
Prompt: 
def note_content_data(note, role):
    """Return the data for content

    :param note: the note that holds the data
    :type note: :class:`jukeboxcore.djadapter.models.Note`
    :param role: item data role
    :type role: QtCore.Qt.ItemDataRole
    :returns: data for the created date
    :rtype: depending on role
    :raises: None
    """
----------
Output: 

    if role == QtCore.Qt.DisplayRole:
        return note.content.display_name
    elif role == QtCore.Qt.EditRole:
        return note.content.edit_description
    elif role == QtCore.Qt.ActionRole:
        return note_action_data(note)
    elif role == QtCore.Qt.StatusTipRole:
        return note_tip_data(note)
    elif role == QtCore.Qt.ToolTipRole:
        return note_content_data(note, role)
    elif role == QtCore.Qt.UserRole:
        return note_user_data(note)
   
----------
Solution: 

    if role == QtCore.Qt.DisplayRole or role == QtCore.Qt.EditRole:
        return note.content
--------------------
Prompt: 
def _prepare_aggregate(cls, *args, **kw):
		"""Generate and execute an aggregate query pipline using combined plain and parametric query generation.
		
		Additionally, performs argument case normalization, refer to the `_prepare_query` method's docstring.
		
		This provides a find-like interface for generating aggregate pipelines with a few shortcuts that make
		aggregates behave more like "find, optionally with more steps". Positional arguments that are not Filter
		instances are assumed to be aggregate pipeline stages.
		
		https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.aggregate
		"""
----------
Output: 
collection.html#pymongo.collection.Collection.aggregate
		
		:param args: Arguments to the aggregate query.
		:param kw: Keyword arguments to the aggregate query.
		:return: A list of dictionaries, one for each step in the pipeline, and the other for each step
		"""
		return []

	def _prepare_query(self, *args, **kw):
		"""Generate and execute a query pipline using combined plain and parametric query generation.
		
		Additionally, performs argument case normalization, refer to the `_prepare_aggregate` method's docstring.
		
		:param
----------
Solution: 

		
		stages = []
		stage_args = []
		fragments = []
		
		for arg in args:  # Split the positional arguments into filter fragments and projection stages.
			(fragments if isinstance(arg, Filter) else stage_args).append(arg)
		
		cls, collection, query, options = cls._prepare_query(
				cls.AGGREGATE_MAPPING,
				cls.AGGREGATE_OPTIONS,
				*fragments,
				**kw
			)
		
		if query:
			stages.append({'$match': query})
		
		stages.extend(stage_args)
		
		if 'sort' in options:  # Convert the find-like option to a stage with the correct semantics.
			stages.append({'$sort': odict(options.pop('sort'))})
		
		if 'skip' in options:  # Note: Sort + limit memory optimization invalidated when skipping.
			stages.append({'$skip': options.pop('skip')})
		
		if 'limit' in options:
			stages.append({'$limit': options.pop('limit')})
		
		if 'projection' in options:
			stages.append({'$project': options.pop('projection')})
		
		return cls, collection, stages, options
--------------------
Prompt: 
def restore(name, run_path=None, replace=False, root="."):
    """ Downloads the specified file from cloud storage into the current run directory
    if it doesn exist.

    name: the name of the file
    run_path: optional path to a different run to pull files from
    replace: whether to download the file even if it already exists locally
    root: the directory to download the file to.  Defaults to the current 
        directory or the run directory if wandb.init was called.

    returns None if it can't find the file, otherwise a file object open for reading
    raises wandb.CommError if it can't find the run
    """
----------
Output: 
andb.WandbError if the file could not be found.
    """
    if run_path is None:
        run_path = os.getcwd()
    if os.path.exists(run_path + "/" + name):
        return run_path + "/" + name
    else:
        raise wandb.WandbError("File " + name + " not found")

def download(run_path=None, replace=False, root="."):
    """ Downloads the specified file from cloud storage into the current run directory
    if it doesn exist.

    run_path: optional path
----------
Solution: 

    if run_path is None and run is None:
        raise ValueError(
            "You must call `wandb.init` before calling restore or specify a run_path")
    api = Api()
    api_run = api.run(run_path or run.path)
    root = run.dir if run else root
    path = os.path.exists(os.path.join(root, name))
    if path and replace == False:
        return open(path, "r")
    files = api_run.files([name])
    if len(files) == 0:
        return None
    return files[0].download(root=root, replace=True)
--------------------
Prompt: 
def batchccn(args):
    """
    %prog batchccn test.csv

    Run CCN script in batch. Write makefile.
    """
----------
Output: 

    if args.csv:
        with open(args.csv, 'w') as f:
            for row in test_data:
                f.write(row + '\n')
        return

    if args.test:
        with open(args.test, 'w') as f:
            for row in test_data:
                row = row.strip()
                if not row:
                    continue
                if row[0] == '#':
                    continue
                if row[0] == ';':
                    row = row.split(';')
                f.write(row[0] + '\n')
                f.write(row[
----------
Solution: 

    p = OptionParser(batchccn.__doc__)
    opts, args = p.parse_args(args)

    if len(args) != 1:
        sys.exit(not p.print_help())

    csvfile, = args
    mm = MakeManager()
    pf = op.basename(csvfile).split(".")[0]
    mkdir(pf)

    header = next(open(csvfile))
    header = None if header.strip().endswith(".bam") else "infer"
    logging.debug("Header={}".format(header))
    df = pd.read_csv(csvfile, header=header)
    cmd = "perl /mnt/software/ccn_gcn_hg38_script/ccn_gcn_hg38.pl"
    cmd += " -n {} -b {}"
    cmd += " -o {} -r hg38".format(pf)
    for i, (sample_key, bam) in df.iterrows():
        cmdi = cmd.format(sample_key, bam)
        outfile = "{}/{}/{}.ccn".format(pf, sample_key, sample_key)
        mm.add(csvfile, outfile, cmdi)
    mm.write()
--------------------
Prompt: 
def create_dev_cert(name, dest):
    """
    A command to generate a self signed certificate for dev purposes.
    :param name: name ro be given to the cert and key
    :param dest: location on local filesystem to store the generated files
    :return:
    """
----------
Output: 

    if not dest:
        dest = os.path.join(name, "cert.pem")
    if not os.path.exists(dest):
        os.makedirs(dest)
    cert = open(dest, "w")
    cert.write(generate_cert(name, dest))
    cert.close()


def generate_cert(name, dest):
    """
    Generate a self signed certificate for dev purposes.
    :param name: name ro be given to the cert and key
    :param dest: location on local filesystem to store the generated files
    :return:
    """
    if not dest
----------
Solution: 

    if not os.path.exists(dest):
        os.makedirs(dest)
    keygen.gen_ca_cert(name, dest, 3650)
--------------------
Prompt: 
def _capture_stdout(callable: Callable[..., CaptureResult], *args, **kwargs) -> CaptureResult:
        """
        Captures content written to standard out.
        :param callable: the callable to wrap
        :param args: positional arguments passed to the callable
        :param kwargs: keyword arguments passed to the callable
        :return: the captured result
        """
----------
Output: 

        if callable.__name__ == "get_stdout":
            return callable(args, **kwargs)
        else:
            return callable(args, **kwargs)

    def _capture_stderr(
        self,
        callable: Callable[..., CaptureResult],
        *args,
        **kwargs: Any,
    ) -> CaptureResult:
        """
        Captures content written to standard err.
        :param callable: the callable to wrap
        :param args: positional arguments passed to the callable
        :param kwargs: keyword arguments passed to the callable
        :return: the captured result
        """
        if callable.__name__ == "get_
----------
Solution: 

        stream = StringIO()
        with redirect_stdout(stream):
            return_value = callable(*args, **kwargs)
        assert return_value.stdout is None, "stdout appears to have already been captured"
        return CaptureResult(return_value=return_value.return_value, stdout=stream.getvalue(),
                             stderr=return_value.stderr, exception=return_value.exception)
--------------------
Prompt: 
def create_eager_metrics_for_problem(problem, model_hparams):
  """See create_eager_metrics."""
----------
Output: 

  metrics = {}
  for name, metric in problem.metrics.items():
    metrics[name] = metric.create_metric(
        model_hparams,
        metric.RegressionOutput(
            loss=metric.loss,
            metrics=metric.get_config(),
            weighted_metrics=metric.get_weighted_metrics(),
            sample_weight=metric.sample_weight,
            masks=metric.get_config().get('mask_targets', None),
            masks_interval=metric.get_config().get('mask_interval', None),
            masks_target=name in mask_targets,
           
----------
Solution: 

  metric_fns = problem.eval_metric_fns(model_hparams)
  problem_hparams = problem.get_hparams(model_hparams)
  target_modality = problem_hparams.modality["targets"]
  weights_fn = model_hparams.weights_fn.get(
      "targets",
      modalities.get_weights_fn(target_modality))
  return create_eager_metrics_internal(metric_fns, weights_fn=weights_fn)
--------------------
Prompt: 
def Arrows(startPoints, endPoints=None, s=None, scale=1, c="r", alpha=1, res=12):
    """
    Build arrows between two lists of points `startPoints` and `endPoints`.
    `startPoints` can be also passed in the form ``[[point1, point2], ...]``.
    
    Color can be specfied as a colormap which maps the size of the arrows.

    :param float s: fix aspect-ratio of the arrow and scale its cross section
    :param float scale: apply a rescaling factor to the length
    :param c: color or array of colors
    :param str cmap: color arrows by size using this color map
    :param float alpha: set transparency
    :param int res: set arrow resolution

    .. hint:: |glyphs_arrow| |glyphs_arrow.py|_
    """
----------
Output: 
 int startPoints: start point
    :param int endPoints: end point
    :param int startAngle: start angle
    :param int endAngle: end angle
    :param int startPoint: start point
    :param int endPoint: end point
    :param int startAngle: start angle
    :param int endAngle: end angle
    :param int res: resolution of the arrow
    :return: list of arrow objects
    :rtype: list
    """
    if endPoints is None:
        endPoints = [[0,0]]
    if startPoints[0][0] == endPoints[0][0]:
       
----------
Solution: 

    startPoints = np.array(startPoints)
    if endPoints is None:
        strt = startPoints[:,0]
        endPoints = startPoints[:,1]
        startPoints = strt
    
    arr = vtk.vtkArrowSource()
    arr.SetShaftResolution(res)
    arr.SetTipResolution(res)
    if s:
        sz = 0.02 * s
        arr.SetTipRadius(sz*2)
        arr.SetShaftRadius(sz)
        arr.SetTipLength(sz * 10)
    arr.Update()
    pts = Points(startPoints)
    orients = (endPoints - startPoints) * scale
    arrg = Glyph(pts, arr.GetOutput(), 
                 orientationArray=orients, scaleByVectorSize=True, 
                 c=c, alpha=alpha)
    settings.collectable_actors.append(arrg)
    return arrg
--------------------
Prompt: 
def rasterize_pdf(
    input_file,
    output_file,
    xres,
    yres,
    raster_device,
    log,
    pageno=1,
    page_dpi=None,
    rotation=None,
    filter_vector=False,
):
    """Rasterize one page of a PDF at resolution (xres, yres) in canvas units.

    The image is sized to match the integer pixels dimensions implied by
    (xres, yres) even if those numbers are noninteger. The image's DPI will
     be overridden with the values in page_dpi.

    :param input_file: pathlike
    :param output_file: pathlike
    :param xres: resolution at which to rasterize page
    :param yres:
    :param raster_device:
    :param log:
    :param pageno: page number to rasterize (beginning at page 1)
    :param page_dpi: resolution tuple (x, y) overriding output image DPI
    :param rotation: 0, 90, 180, 270: clockwise angle to rotate page
    :param filter_vector: if True, remove vector graphics objects
    :return:
    """
----------
Output: 
 in the DPI.

    Args:
        input_file (str): The input PDF file.
        output_file (str): The output PDF file.
        xres (int): The x resolution of the image.
        yres (int): The y resolution of the image.
        raster_device (str): The name of the raster device.
        log (logging.Logger): The logger to use for logging.
        pageno (int): The number of pages to use. If None, use all pages.
        page_dpi (int): The resolution of the page in dots per inch.
        rotation (
----------
Solution: 

    res = xres, yres
    int_res = round(xres), round(yres)
    if not page_dpi:
        page_dpi = res

    with NamedTemporaryFile(delete=True) as tmp:
        args_gs = (
            [
                'gs',
                '-dQUIET',
                '-dSAFER',
                '-dBATCH',
                '-dNOPAUSE',
                f'-sDEVICE={raster_device}',
                f'-dFirstPage={pageno}',
                f'-dLastPage={pageno}',
                f'-r{str(int_res[0])}x{str(int_res[1])}',
            ]
            + (['-dFILTERVECTOR'] if filter_vector else [])
            + [
                '-o',
                tmp.name,
                '-dAutoRotatePages=/None',  # Probably has no effect on raster
                '-f',
                fspath(input_file),
            ]
        )

        log.debug(args_gs)
        p = run(args_gs, stdout=PIPE, stderr=STDOUT, universal_newlines=True)
        if _gs_error_reported(p.stdout):
            log.error(p.stdout)
        else:
            log.debug(p.stdout)

        if p.returncode != 0:
            log.error('Ghostscript rasterizing failed')
            raise SubprocessOutputError()

        # Ghostscript only accepts integers for output resolution
        # if the resolution happens to be fractional, then the discrepancy
        # would change the size of the output page, especially if the DPI
        # is quite low. Resize the image to the expected size

        tmp.seek(0)
        with Image.open(tmp) as im:
            expected_size = (
                round(im.size[0] / int_res[0] * res[0]),
                round(im.size[1] / int_res[1] * res[1]),
            )
            if expected_size != im.size or page_dpi != (xres, yres):
                log.debug(
                    f"Ghostscript: resize output image {im.size} -> {expected_size}"
                )
                im = im.resize(expected_size)

            if rotation is not None:
                log.debug("Rotating output by %i", rotation)
                # rotation is a clockwise angle and Image.ROTATE_* is
                # counterclockwise so this cancels out the rotation
                if rotation == 90:
                    im = im.transpose(Image.ROTATE_90)
                elif rotation == 180:
                    im = im.transpose(Image.ROTATE_180)
                elif rotation == 270:
                    im = im.transpose(Image.ROTATE_270)
                if rotation % 180 == 90:
                    page_dpi = page_dpi[1], page_dpi[0]
            im.save(fspath(output_file), dpi=page_dpi)
--------------------
Prompt: 
def _color_noise(x, s_freq, coef=0):
    """Add some color to the noise by changing the power spectrum.

    Parameters
    ----------
    x : ndarray
        one vector of the original signal
    s_freq : int
        sampling frequency
    coef : float
        coefficient to apply (0 -> white noise, 1 -> pink, 2 -> brown,
                              -1 -> blue)

    Returns
    -------
    ndarray
        one vector of the colored noise.
    """
----------
Output: 

    return _color_power(x, s_freq, s_freq, s_freq, coef)


def _color_power(x, s_freq, s_freq, s_freq, coef=0):
    """Add some power to the power spectrum by changing the power spectrum.

    Parameters
    ----------
    x : ndarray
        one vector of the original signal
    s_freq : int
        sampling frequency
    s_freq : int
        power spectrum
    coef : float
        power coefficient to apply (0 -> white noise, 1 -> pink, 2 -> brown,
                              -1 -> blue)
----------
Solution: 

    # convert to freq domain
    y = fft(x)
    ph = angle(y)
    m = abs(y)

    # frequencies for each fft value
    freq = linspace(0, s_freq / 2, int(len(m) / 2) + 1)
    freq = freq[1:-1]

    # create new power spectrum
    m1 = zeros(len(m))
    # leave zero alone, and multiply the rest by the function
    m1[1:int(len(m) / 2)] = m[1:int(len(m) / 2)] * f(freq, coef)
    # simmetric around nyquist freq
    m1[int(len(m1) / 2 + 1):] = m1[1:int(len(m1) / 2)][::-1]

    # reconstruct the signal
    y1 = m1 * exp(1j * ph)
    return real(ifft(y1))
--------------------
Prompt: 
def manifest_download(self):
        """download manifest files"""
----------
Output: 

        if self.config.get('update', False):
            self.update_manifest()
        else:
            self.update_manifest_path()

    def update_manifest_path(self):
        """update manifest file paths"""
        self.manifest_path = os.path.join(self.config['destination'], 'dist','servoapp.json')
        self.config['manifest_path'] = self.manifest_path

    def get_log_path(self):
        """return path to the mozlog.txt"""
        return self.config.get('path', os.path.join
----------
Solution: 

        if self.downloaders_lock.acquire(False):
            if len(self.downloaders):
                # there already exist downloader threads
                self.downloaders_lock.release()
                return

            for url in ['http://firmware.ardupilot.org/manifest.json']:
                filename = self.make_safe_filename_from_url(url)
                path = mp_util.dot_mavproxy("manifest-%s" % filename)
                self.downloaders[url] = threading.Thread(target=self.download_url, args=(url, path))
                self.downloaders[url].start()
            self.downloaders_lock.release()
        else:
            print("fw: Failed to acquire download lock")
--------------------
Prompt: 
def create_bookmark_action(parent, url, title, icon=None, shortcut=None):
    """Create bookmark action"""
----------
Output: 

    return Action(parent, url, title, icon, shortcut)


class Action(object):
    """Action"""
    def __init__(self, parent, url, title, icon=None, shortcut=None):
        self.parent = parent
        self.url = url
        self.title = title
        self.icon = icon
        self.shortcut = shortcut
        self.parent.add_action(create_action(parent, url, icon, shortcut))

    def get_url(self):
        """Get URL"""
        return self.url

    def get_title(self):
        """Get title"""
----------
Solution: 

    
    @Slot()
    def open_url():
        return programs.start_file(url)
    
    return create_action( parent, title, shortcut=shortcut, icon=icon,
                          triggered=open_url)
--------------------
Prompt: 
def _font_of_mention(m):
    """
    Returns the font type and size of the first alphanumeric
    char in the text or None if there isn't any.
    """
----------
Output: 

    if m.group(0) == "a":
        return "a"
    elif m.group(0) == "i":
        return "i"
    elif m.group(0) == "u":
        return "u"
    elif m.group(0) == "s":
        return "s"
    elif m.group(0) == "t":
        return "t"
    else:
        return None, None


def _get_line_length(line):
    """
    Returns the length of the line.
    """
    if line.startswith("L"):
        return len
----------
Solution: 

    for ch in m:
        if isinstance(ch, LTChar) and ch.get_text().isalnum():
            return (ch.fontname, _font_size_of(ch))
    return (None, 0)
--------------------
Prompt: 
def listContents(self):
        """ Return list of volumes or diffs in this Store's selected directory. """
----------
Output: 

        return self.getSelectedVolumes()

    def getSelectedVolumes(self):
        """ Return list of selected files in this Store's selected directory. """
        return self.getSelectedFiles()

    def getSelectedFiles(self):
        """ Return list of selected files in this Store's selected directory. """
        return self.getSelectedFilesByType(self.getSelectedFilesByType(self.getSelectedFilesByDiff))

    def getSelectedFilesByType(self, files):
        """ Return list of files in this Store's selected directory. """
        return self.getSelectedFilesByType(files)

    def
----------
Solution: 

        items = list(self.extraKeys.items())
        items.sort(key=lambda t: t[1])

        (count, size) = (0, 0)

        for (diff, path) in items:
            if path.startswith("/"):
                continue
            yield str(diff)
            count += 1
            size += diff.size

        yield "TOTAL: %d diffs %s" % (count, humanize(size))
--------------------
Prompt: 
def compare_variant_type_plot(data):
    """ Return HTML for the Variant Counts barplot """
----------
Output: 

    return """
    <div class="barplot" id="counts-bar" style="width: 100%; height: 100%;">
        <div class="bar-group">
            <div class="bar-header">
                <div class="bar-group-header">
                    <div class="bar-group-title">Counts</div>
                </div>
                <div class="bar-group-header">
                    <div class="bar-group-title">Total</div>
                </div>
            </div>
        </div>
    </div>
    """

def compare_variant_
----------
Solution: 

    keys = OrderedDict()
    keys['snps'] = {'name': 'SNPs', 'color': '#7cb5ec'}
    keys['indels'] = {'name': 'InDels', 'color': '#90ed7d'}
    keys['multiallelic_snps'] = {'name': 'multi-allelic SNP', 'color': 'orange'}
    keys['complex_indels'] = {'name': 'Complex InDels', 'color': '#8085e9'}

    total_variants = dict()
    known_variants = dict()
    novel_variants = dict()
    for s_name, values in data.items():
        total_variants[s_name] = {
            'snps': values['TOTAL_SNPS'],
            'indels': values['TOTAL_INDELS'],
            'multiallelic_snps': values['TOTAL_MULTIALLELIC_SNPS'],
            'complex_indels': values['TOTAL_COMPLEX_INDELS'],
        }

        known_variants[s_name] = {
            'snps': values['NUM_IN_DB_SNP'],
            'indels': int(values['TOTAL_INDELS'])-int(values['NOVEL_INDELS']),
            'multiallelic_snps': values['NUM_IN_DB_SNP_MULTIALLELIC'],
            'complex_indels': values['NUM_IN_DB_SNP_COMPLEX_INDELS'],
        }

        novel_variants[s_name] = {
            'snps': values['NOVEL_SNPS'],
            'indels': values['NOVEL_INDELS'],
            'multiallelic_snps': int(values['TOTAL_MULTIALLELIC_SNPS'])-int(values['NUM_IN_DB_SNP_MULTIALLELIC']),
            'complex_indels': int(values['TOTAL_COMPLEX_INDELS'])-int(values['NUM_IN_DB_SNP_COMPLEX_INDELS']),
        }

    plot_conf = {
        'id': 'picard_variantCallingMetrics_variant_type',
        'title': 'Picard: Variants Called',
        'ylab': 'Counts of Variants',
        'hide_zero_cats': False,
        'data_labels': [
            {'name': 'Total'},
            {'name': 'Known'},
            {'name': 'Novel'}
        ],
    }
    return bargraph.plot(data=[total_variants, known_variants, novel_variants],
                         cats=[keys, keys, keys],
                         pconfig=plot_conf)
--------------------
Prompt: 
def export_template(access_token, subscription_id, rgname):
    """Capture the specified resource group as a template

    Args:
        access_token (str): A valid Azure authentication token.
        subscription_id (str): Azure subscription id.
        rgname (str): Azure resource group name.

    Returns:
        HTTP response. JSON body.
    """
----------
Output: 

    return _get_request(access_token, 'exportTemplate', subscription_id, rgname)


def _get_request(access_token, operation, subscription_id, rgname):
    """Get a request object for the specified subscription

    Args:
        access_token (str): A valid Azure authentication token.
        operation (str): The operation to execute.
        subscription_id (str): Azure subscription id.
        rgname (str): Azure resource group name.

    Returns:
        HTTP response.
    """
    return _get_request_with_token(access_token, 'get',
----------
Solution: 

    endpoint = ''.join([get_rm_endpoint(),
                        '/subscriptions/', subscription_id,
                        '/resourcegroups/', rgname,
                        '/exportTemplate',
                        '?api-version=', RESOURCE_API])
    rg_body = {'options':'IncludeParameterDefaultValue', 'resources':['*']}
    body = json.dumps(rg_body)
    return do_post(endpoint, body, access_token)
--------------------
Prompt: 
def signal_alias_exists(alias: str) -> bool:
        """
        Checks if signal alias exists.
        :param alias: Signal alias.
        :return:
        """
----------
Output: 

        return alias in self.signal_aliases

    def get_signal_alias(self, signal_name: str) -> str:
        """
        Gets signal alias.
        :param signal_name: Signal alias name.
        :return:
        """
        return self.signal_aliases[signal_name]

    def get_signal_alias_name(self, signal_name: str) -> str:
        """
        Gets signal alias name.
        :param signal_name: Signal alias name.
        :return:
        """
        return self.signal_aliases[signal_name]

    def signal_alias_
----------
Solution: 

        if SignalDispatcher.signals.get(alias):
            return True

        return False
--------------------
Prompt: 
def update_frame(self, key, ranges=None):
        """
        Update the internal state of the Plot to represent the given
        key tuple (where integers represent frames). Returns this
        state.
        """
----------
Output: 

        if ranges is None:
            ranges = self.plot_ranges
        if key in self.frame_ranges:
            self.frame_ranges[key].update(ranges)
        else:
            self.frame_ranges[key] = ranges
        return self.frame_ranges[key].update(key)

    def get_data(self, key):
        """
        Return the data for the given key.
        """
        return self.plot_data[key]

    def get_data_range(self, key):
        """
        Return the range for the given key.
        """
        return self.plot
----------
Solution: 

        ranges = self.compute_ranges(self.layout, key, ranges)
        for coord in self.layout.keys(full_grid=True):
            subplot = self.subplots.get(wrap_tuple(coord), None)
            if subplot is not None:
                subplot.update_frame(key, ranges)
        title = self._get_title_div(key)
        if title:
            self.handles['title']
--------------------
Prompt: 
def _publish_status(self, slug, data):
        """Publish a status message for a device

        Args:
            slug (string): The device slug that we are publishing on behalf of
            data (dict): The status message data to be sent back to the caller
        """
----------
Output: 

        self.device_status_changed.emit(slug, data)

    def device_status_changed(self, slug, state):
        """Update the status message for a device

        Args:
            slug (string): The device slug that we are updating
            state (dict): The new status message state
            self.device_status_changed (str): The new status message data
        """
        self._set_status(slug, state)
        self.device_status_changed.emit(slug, data)

    def _set_status(self, slug, state):
        """Set the message status

        Args
----------
Solution: 


        status_topic = self.topics.prefix + 'devices/{}/data/status'.format(slug)

        self._logger.debug("Publishing status message: (topic=%s) (message=%s)", status_topic, str(data))
        self.client.publish(status_topic, data)
--------------------
Prompt: 
def short_timesince(date):
    """
    A shorter version of Django's built-in timesince filter.
    Selects only the first part of the returned string,
    splitting on the comma.

    Falls back on default Django timesince if it fails.

    Example: 3 days, 20 hours becomes "3 days".

    """
----------
Output: 

    try:
        return timesince(date)
    except (ValueError, TypeError):
        return date.replace(hour=0, minute=0, second=0, microsecond=0)

def timeuntil(date):
    """
    A Django version of Django's built-in timeuntil filter.
    Selects only the first part of the returned string,
    splitting on the comma.

    Falls back on default Django timesuntil if it fails.

    Example: 3 days, 20 hours becomes "3 days".

    """
    try:
        return timesince(date, time_zone=pytz.utc)
   
----------
Solution: 

    try:
        t = timesince(date).split(", ")[0]
    except IndexError:
        t = timesince(date)
    return t
--------------------
Prompt: 
def _clob_end_handler_factory():
    """Generates the handler for the end of a clob value. This includes anything from the data's closing quote through
    the second closing brace.
    """
----------
Output: 

    def _end_handler(value):
        return value.replace('\\\\', '\\').replace('\\"', '"')
    return _end_handler_factory


class Clob(object):
    """
    A wrapper for the CLOB type.
    """
    def __init__(self, data):
        self.data = data

    def __call__(self, name, *args, **kwargs):
        return self.data.get_value(name, *args, **kwargs)

    def __getattr__(self, name):
        return getattr(self.data, name)

    def __getitem__(self, key
----------
Solution: 

    def action(c, ctx, prev, res, is_first):
        if is_first and ctx.is_self_delimiting and c == _DOUBLE_QUOTE:
            assert c is prev
            return res
        _illegal_character(c, ctx)

    return _lob_end_handler_factory(IonType.CLOB, action)
--------------------
Prompt: 
def data(self, column, role):
        """Return the data for the specified column and role

        Column 0: The name of the action
        Column 1: The description of the action
        Column 2: The status value
        Column 3: The status message
        Column 4: The traceback

        :param column: the data column
        :type column: int
        :param role: the data role
        :type role: QtCore.Qt.ItemDataRole
        :returns: data depending on the role, or None if the column is out of range
        :rtype: depending on the role or None
        :raises: None
        """
----------
Output: 

        if role == QtCore.Qt.DisplayRole:
            return self.display_data[0]
        elif role == QtCore.Qt.EditRole:
            return self.display_data[1]
        elif role == QtCore.Qt.ToolTipRole:
            return self.display_data[2]
        elif role == QtCore.Qt.WhatsThisRole:
            return self.display_data[3]
        elif role == QtCore.Qt.WhatsThisRole:
            return self.display_data[4]
        else:
            raise ValueError("Invalid role")

    def display_data_item(self
----------
Solution: 

        if role == QtCore.Qt.DisplayRole:
            if column == 0:
                return self._au.name
            if column == 1:
                return self._au.description
            if column == 2:
                return self._au.status.value
            if column == 3:
                return self._au.status.message
            if column == 4:
                return self._au.status.traceback

        if role == QtCore.Qt.ForegroundRole:
            if column == 2:
                return self.colormapping.get(self._au.status.value)
--------------------
Prompt: 
def meta(self, meta):
        """Extract model metadata for lua script stdnet/lib/lua/odm.lua"""
----------
Output: 

        if self.version >= (4,0,0):
            self.version = 4
            self.lua_script = lua_script
        else:
            self.version = 6
            self.lua_script = lua_script
        self.lua_script_path = self.lua_script.split(";")
        self.version_info = self.version_info[0:self.version]
        self.lua_script_path.reverse()
        self.lua_script_path = self.lua_script_path[::-1]
        self.l
----------
Solution: 

        data = meta.as_dict()
        data['namespace'] = self.basekey(meta)
        return data
--------------------
Prompt: 
def pick(self, *args):
        """
        Return a copy of the object only containing the
        whitelisted properties.
        """
----------
Output: 

        return self.__class__(self.name, self.value, self.type, self.default, self.required, self.default_from_file, self.default_from_file_location, *args)

    def __repr__(self):
        return "PickledObject(name=%s, value=%s, type=%s, default=%s, required=%s, default_from_file=%s, default_from_line=%s)" % (
            self.name, self.value, self.type, self.default, self.required, self.default_from_file,
----------
Solution: 

        ns = self.Namespace()
        ns.result = {}

        def by(key, *args):
            if key in self.obj:
                ns.result[key] = self.obj[key]

        _.each(self._flatten(args, True, []), by)
        return self._wrap(ns.result)
--------------------
Prompt: 
def clock(rpc):
    """
    This task runs forever and notifies all clients subscribed to
    'clock' once a second.
    """
----------
Output: 

    global _client_clock
    _client_clock = None
    return rpc.call('clock')


def get_client_clock():
    """
    Return the current task's client clock.
    """
    global _client_clock
    if _client_clock is None:
        _client_clock = clock(get_client_client)
    return _client_clock


def get_client_client():
    """
    Return the current task's client client.
    """
    global _client_client
    if _client_client is None:
        _client_client = get_client_clock()
       
----------
Solution: 


    while True:
        yield from rpc.notify('clock', str(datetime.datetime.now()))
        yield from asyncio.sleep(1)
--------------------
Prompt: 
def suppress_keyboard_interrupt_message():
    """Register a new excepthook to suppress KeyboardInterrupt
    exception messages, and exit with status code 130.

    """
----------
Output: 

    sys.excepthook = _sys.excepthook

def enable_automatic_gc():
    """Register an automatic garbage collector to be used by Psyco.

    """
    _sys.set_automatic_gc(True)

def disable_automatic_gc():
    """Register an automatic garbage collector to be used by Psyco.

    """
    _sys.set_automatic_gc(False)

def enable_automatic_gc_after_fork():
    """Register an automatic garbage collector to be used by Psyco.

    """
    _sys.set_automatic
----------
Solution: 

    old_excepthook = sys.excepthook

    def new_hook(type, value, traceback):
        if type != KeyboardInterrupt:
            old_excepthook(type, value, traceback)
        else:
            sys.exit(130)

    sys.excepthook = new_hook
--------------------
Prompt: 
def getDownloadUrls(self):
		"""Return a list of the urls to download from"""
----------
Output: 

		return self.downloadUrls

	def getDownloadSize(self):
		"""Return the size of the file in bytes"""
		return self.downloadSize

	def getDownloadUrl(self, url):
		"""Return the url to download"""
		return url.replace(self.getDownloadUrls(), '')

	def getDownloadSize(self, size):
		"""Return the size of the file in bytes"""
		return size

	def getDownloadUrlAndSize(self, url, size):
		"""Return the url to download"""
		return self.get
----------
Solution: 

		data = self.searchIndex(False)
		fileUrls = []
		for datum in data:
			fileUrl = self.formatDownloadUrl(datum[0])
			fileUrls.append(fileUrl)
		return fileUrls
--------------------
Prompt: 
def add_context(self, isc_name, isc_policy_id, isc_traffic_tag):
        """
        Create the VSS Context within the VSSContainer

        :param str isc_name: ISC name, possibly append policy name??
        :param str isc_policy_id: Policy ID in SMC (the 'key' attribute)
        :param str isc_traffic_tag: NSX groupId (serviceprofile-145)
        :raises CreateElementFailed: failed to create
        :return: VSSContext
        """
----------
Output: 

        if not self.is_valid_iscs_xml_tag(iscs_name):
            raise ValueError("Invalid ISC name %s" % iscs_name)
        if not self.is_valid_isc_xml_tag(isc_name):
            raise ValueError("Invalid ISC name %s" % isc_name)
        if not self.is_valid_isc_xml_tag(isc_name):
            raise ValueError("Invalid ISC name %s" % isc_name)
        if not self.is_valid_isc_xml_
----------
Solution: 

        if 'add_context' in self.data.links: # SMC >=6.5
            element = ElementCreator(
                VSSContext,
                href=self.get_relation('add_context'),
                json = {
                    'name': isc_name,
                    'vc_isc': {
                        'isc_name': isc_name,
                        'isc_policy_id': isc_policy_id,
                        'isc_traffic_tag': isc_traffic_tag
                    }
                })
        else: # SMC < 6.5
            element = VSSContext.create(
                isc_name=isc_name,
                isc_policy_id=isc_policy_id,
                isc_traffic_tag=isc_traffic_tag,
                vss_container=self)
        
        # Delete cache since the virtualResources node is attached to
        # the engine json
        self._del_cache()
        return element
--------------------
Prompt: 
def _edges_to_adjacency_list(edges):
    """Convert a list of edges into an adjacency list."""
----------
Output: 

    return [tuple(sorted(e)) for e in edges]


def _get_all_possible_edges(G, source):
    """Return all possible edges in the graph from source to all nodes in G.

    Parameters
    ----------
    G : NetworkX graph

    source : node
       Starting node for path.

    Returns
    -------
    adjacency_list : iterator
       A postorder iterator over all possible edges in the graph.

    Examples
    --------
    >>> G = nx.path_graph(5)
    >>> for n in G:
   ...     print(n, n.edges())
    (0
----------
Solution: 

    adj = {}
    for i, j in edges:
        if i in adj:  # pragma: no cover
            ni = adj[i]
        else:
            ni = adj[i] = set()
        if j in adj:
            nj = adj[j]
        else:
            nj = adj[j] = set()
        ni.add(j)
        nj.add(i)
    return adj
--------------------
Prompt: 
def display_molecule(mol, style='ball-and-stick'):
    """Display the molecule *mol* with the default viewer.

    """
----------
Output: 

    if style =='stick':
        print(style_to_string(style))
    elif style == 'ball':
        print(style_to_string(style))
    elif style =='stick':
        print('stick')
    elif style == 'ball-and-stick':
        print('ball-and-stick')
    elif style == 'ball-and-stick-default':
        style_to_string(style)
    else:
        raise ValueError('Unknown molecule viewer style: %s' % (style))


def display_molecule_default(mol, style='ball-and-stick'):
----------
Solution: 

    v = QtViewer()

    
    if style == 'ball-and-stick':
        bs = v.add_renderer(BallAndStickRenderer,
                            mol.r_array,
                            mol.type_array,
                            mol.bonds)
    elif style == 'vdw':
        sr = v.add_renderer(AtomRenderer, mol.r_array, mol.type_array,
                            backend='impostors')
    else:
        raise Exception("Rendering style unknown")
    
    v.widget.camera.autozoom(mol.r_array)
    v.run()
--------------------
Prompt: 
def get_ip(source='aws'):
    
    """ a method to get current public ip address of machine """
----------
Output: 

    
    # get current public ip
    try:
        return boto.ec2.get_public_ip(source)
    except boto.exception.EC2ResponseError, e:
        return None

def get_instance_metadata(instance):
    """ a method to get instance metadata from an instance """
    
    # get instance metadata
    try:
        return boto.ec2.get_instance_metadata(instance['InstanceId'])
    except boto.exception.BotoServerError, e:
        return None

def main():
    """ main """
    
    # get command line arguments
    parser = argparse.ArgumentParser(description='Create a
----------
Solution: 

    
    if source == 'aws':
        source_url = 'http://checkip.amazonaws.com/'
    else:
        raise Exception('get_ip currently only supports queries to aws')
    
    import requests
    try:
        response = requests.get(url=source_url)
    except Exception as err:
        from labpack.handlers.requests import handle_requests
        from requests import Request
        request_object = Request(method='GET', url=source_url)
        request_details = handle_requests(request_object)
        raise Exception(request_details['error'])
    current_ip = response.content.decode()
    current_ip = current_ip.strip()
    
    return current_ip
--------------------
Prompt: 
def bigtable_users(self):
        """Access to bigtable.user role memebers

        For example:

        .. literalinclude:: snippets.py
            :start-after: [START bigtable_users_policy]
            :end-before: [END bigtable_users_policy]
        """
----------
Output: 

        return self.service.bigtable_client.list_user_policies(
            request={"name": self.full_name},
            project_id=self.project_id,
            zone=self.zone,
            cluster_id=self.cluster_id,
            table_id=self.table_id,
            max_results=self.max_results,
            page_token=self.page_token,
            start_index=self.start_index,
            end_index=self.end_index,
            retry=self.retry,
            timeout=self.timeout,
            metadata={"
----------
Solution: 

        result = set()
        for member in self._bindings.get(BIGTABLE_USER_ROLE, ()):
            result.add(member)
        return frozenset(result)
--------------------
Prompt: 
def TemporaryDirectory(suffix=None, prefix=None, dir=None, on_error='ignore'):  # @ReservedAssignment
    """
    An extension to `tempfile.TemporaryDirectory`.

    Unlike with `python:tempfile`, a :py:class:`~pathlib.Path` is yielded on
    ``__enter__``, not a `str`.

    Parameters
    ----------
    suffix : str
        See `tempfile.TemporaryDirectory`.
    prefix : str
        See `tempfile.TemporaryDirectory`.
    dir : ~pathlib.Path
        See `tempfile.TemporaryDirectory`, but pass a :py:class:`~pathlib.Path` instead.
    on_error : str
        Handling of failure to delete directory (happens frequently on NFS), one of:

        raise
            Raise exception on failure.
        ignore
            Fail silently.
    """
----------
Output: 

        object instead of a string.
    on_error : str
        See `python:os.listdir` for details.

    Yields
    ------
    :py:class:`~pathlib.Path`
        The yielded path.
    """
    if suffix is None:
        yield from tempfile.TemporaryDirectory(dir=dir, suffix=suffix, prefix=prefix)
    else:
        yield from tempfile.TemporaryDirectory(dir=dir, suffix=suffix, prefix=prefix)


def get_data_path(name):
    """
    Returns the absolute path to the data directory.

    Parameters
    ----------
    name : str
----------
Solution: 

    if dir:
        dir = str(dir)  # @ReservedAssignment
    temp_dir = tempfile.TemporaryDirectory(suffix, prefix, dir)
    try:
        yield Path(temp_dir.name)
    finally:
        try:
            temp_dir.cleanup()
        except OSError as ex:
            print(ex)
            # Suppress relevant errors if ignoring failed delete
            if on_error != 'ignore' or ex.errno != errno.ENOTEMPTY:
                raise
--------------------
Prompt: 
def variable_device(device, name):
  """Fix the variable device to colocate its ops."""
----------
Output: 

  return device


def _is_indexed_slices_list(inputs):
  """Returns whether any tensor in `inputs` is IndexedSlices."""
  for input_tensor in inputs:
    if isinstance(input_tensor, ops.Tensor):
      if input_tensor.dense_shape is not None:
        return False
      if input_tensor.indices is not None:
        return True
  return True


def _check_data_types(inputs, expected_types):
  """Asserts tensors are all the expected types."""
  # Note: not using sparse_to_dense since these are expected to be
----------
Solution: 

  if callable(device):
    var_name = tf.get_variable_scope().name + '/' + name
    var_def = tf.NodeDef(name=var_name, op='Variable')
    device = device(var_def)
  if device is None:
    device = ''
  return device
--------------------
Prompt: 
def register_endpoints(self, backend_names):
        """
        See super class satosa.frontends.base.FrontendModule
        :type backend_names: list[str]
        :rtype: list[(str, ((satosa.context.Context, Any) -> satosa.response.Response, Any))]
        :raise ValueError: if more than one backend is configured
        """
----------
Output: 

        self.backend_names = backend_names
        return self.backend_names

    def get_backend_names(self):
        """
        Returns the list of available SATOS backends.
        :rtype: list[str]
        """
        return self.backend_names

    def get_response_type(self):
        """
        Returns the response type.
        :rtype: str
        """
        return self.response_type

    def get_response_types(self):
        """
        Returns the list of available SATOS responses.
        :rtype: list[str]
        """
        return self.response_
----------
Solution: 

        backend_name = None
        if len(backend_names) != 1:
            # only supports one backend since there currently is no way to publish multiple authorization endpoints
            # in configuration information and there is no other standard way of authorization_endpoint discovery
            # similar to SAML entity discovery
            # this can be circumvented with a custom RequestMicroService which handles the routing based on something
            # in the authentication request
            logger.warn("More than one backend is configured, make sure to provide a custom routing micro service to "
                        "determine which backend should be used per request.")
        else:
            backend_name = backend_names[0]

        endpoint_baseurl = "{}/{}".format(self.base_url, self.name)
        self._create_provider(endpoint_baseurl)

        provider_config = ("^.well-known/openid-configuration$", self.provider_config)
        jwks_uri = ("^{}/jwks$".format(self.name), self.jwks)

        if backend_name:
            # if there is only one backend, include its name in the path so the default routing can work
            auth_endpoint = "{}/{}/{}/{}".format(self.base_url, backend_name, self.name, AuthorizationEndpoint.url)
            self.provider.configuration_information["authorization_endpoint"] = auth_endpoint
            auth_path = urlparse(auth_endpoint).path.lstrip("/")
        else:
            auth_path = "{}/{}".format(self.name, AuthorizationEndpoint.url)
        authentication = ("^{}$".format(auth_path), self.handle_authn_request)
        url_map = [provider_config, jwks_uri, authentication]

        if any("code" in v for v in self.provider.configuration_information["response_types_supported"]):
            self.provider.configuration_information["token_endpoint"] = "{}/{}".format(endpoint_baseurl,
                                                                                       TokenEndpoint.url)
            token_endpoint = ("^{}/{}".format(self.name, TokenEndpoint.url), self.token_endpoint)
            url_map.append(token_endpoint)

            self.provider.configuration_information["userinfo_endpoint"] = "{}/{}".format(endpoint_baseurl,
                                                                                          UserinfoEndpoint.url)
            userinfo_endpoint = ("^{}/{}".format(self.name, UserinfoEndpoint.url), self.userinfo_endpoint)
            url_map.append(userinfo_endpoint)
        if "registration_endpoint" in self.provider.configuration_information:
            client_registration = ("^{}/{}".format(self.name, RegistrationEndpoint.url), self.client_registration)
            url_map.append(client_registration)

        return url_map
--------------------
Prompt: 
def user_lookup(self, cloudflare_email=None, unique_id=None):
        """
        Lookup user data based on either his cloudflare_email or his
        unique_id.

        :param    cloudflare_email: email associated with user
        :type     cloudflare_email: str
        :param    unique_id:        unique id associated with user
        :type     unique_id:        str

        :returns:
        :rtype:   dict
        """
----------
Output: 

        if not cloudflare_email and not unique_id:
            raise ValueError("One of 'cloudflare_email' or 'unique_id' must be specified")
        elif not cloudflare_email and unique_id:
            raise ValueError("One of 'cloudflare_email' or 'unique_id' must be specified")
        elif not cloudflare_email and not cloudflare_email:
            raise ValueError("One of 'cloudflare_email' or 'cloudflare_email' must be specified")
        elif not cloudflare_email and not cloudflare_
----------
Solution: 

        if not cloudflare_email and not unique_id:
            raise KeyError(
                'Either cloudflare_email or unique_id must be present')

        params = {'act': 'user_lookup'}
        if cloudflare_email:
            params['cloudflare_email'] = cloudflare_email
        else:
            params['unique_id'] = unique_id

        return self._request(params)
--------------------
Prompt: 
def parse_hicup_logs(self, f):
        """ Parse a HiCUP summary report """
----------
Output: 

        self.hicup_logs = []
        for line in f:
            if line.startswith('H'):
                self.hicup_logs.append(HiCUPSummaryReport(line))
            elif line.startswith('I'):
                self.hicup_logs.append(IlluminaSummaryReport(line))
        return self.hicup_logs

    def parse_hicup_report(self, f):
        """ Parse a HiCUP report """
        self.hicup_report = None
        for line in f:
            if line.startswith('H'):
                self.hicup
----------
Solution: 

        if not f['fn'].endswith('.txt'):
            return None
        header = []
        lines = f['f'].splitlines()
        for l in lines:
            s = l.split("\t")
            if len(header) == 0:
                if s[0] != 'File':
                    return None
                header = s[1:]
            else:
                s_name = self.clean_s_name(s[0], f['root']).lstrip('HiCUP_output/')
                parsed_data = {}
                for idx, num in enumerate(s[1:]):
                    try:
                        parsed_data[header[idx]] = float(num)
                    except:
                        parsed_data[header[idx]] = num
                parsed_data['Duplicate_Read_Pairs'] = parsed_data['Valid_Pairs'] - parsed_data['Deduplication_Read_Pairs_Uniques']
                if s_name in self.hicup_data:
                    log.debug("Duplicate sample name found! Overwriting: {}".format(s_name))
                self.add_data_source(f, s_name)
                self.hicup_data[s_name] = parsed_data
--------------------
Prompt: 
def check_repeated_exclamations(text):
    """Check the text."""
----------
Output: 

    return text.count(r"(")!= text.count(")")


def check_no_exclamation(text):
    """Check the text."""
    return text.count(r"(") == text.count(")")


def check_no_space(text):
    """Check the text."""
    return text.count(" ") == 0


def check_space(text):
    """Check the text."""
    return text.count("(") == text.count("(")


def check_space_before_and_after(text):
    """Check the text."""
----------
Solution: 

    err = "leonard.hell"
    msg = u"Never use the words 'all hell broke loose'."

    regex = r"all hell broke loose"

    return existence_check(
        text, [regex], err, msg, max_errors=1)
--------------------
Prompt: 
def write_publication(self, values):
        """
        Write publication info to db

        Parameters
        ----------
        values: dict with entries
            {'pub_id': str (short name for publication),
            'authors': list of str ()
            'journal': str,
            'volume': str,
            'number': str,
            'pages': 'str'
            'year': int,
            'publisher': str,
            'doi': str,
            'tags': list of str}
        """
----------
Output: 

        if not self.is_valid_publication_id(values['pub_id']):
            raise ValueError('Invalid publication id: %s' % values['pub_id'])

        self.db.execute('UPDATE books SET title = %s, authors = %s, volume = %s, number = %s, pages = %s, year = %s, publisher = %s, doi = %s, tags = %s WHERE id = %s', values['title'], values['authors'], values['volume'], values['number'], values['pages'], values['number'], self.id)
        self
----------
Solution: 

        con = self.connection or self._connect()
        self._initialize(con)
        cur = con.cursor()

        values = (values['pub_id'],
                  values['title'],
                  json.dumps(values['authors']),
                  values['journal'],
                  values['volume'],
                  values['number'],
                  values['pages'],
                  values['year'],
                  values['publisher'],
                  values['doi'],
                  json.dumps(values['tags']))

        q = self.default + ',' + ', '.join('?' * len(values))
        cur.execute('INSERT OR IGNORE INTO publication VALUES ({})'.format(q),
                    values)

        pid = self.get_last_id(cur, table='publication')

        if self.connection is None:
            con.commit()
            con.close()

        return pid
--------------------
Prompt: 
def _set_intf_router_isis(self, v, load=False):
    """
    Setter method for intf_router_isis, mapped from YANG variable /routing_system/interface/loopback/ip/intf_router_isis (container)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_intf_router_isis is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_intf_router_isis() directly.
    """
----------
Output: 

    if hasattr(v, "_utype"):
        v = v._utype(v)
    try:
        t = YANGDynClass(
            v,
            base=container_isis_map_entry.container_isis_map_entry,
            is_container="container",
            yang_name="isis",
            parent=self,
            path_helper=self._path_helper,
            extmethods=self._extmethods,
            register_paths=True,
            namespace="http://openconfig.net/yang/network-instance",
            defining_module="openconfig-network-instance",
            yang_
----------
Solution: 

    if hasattr(v, "_utype"):
      v = v._utype(v)
    try:
      t = YANGDynClass(v,base=intf_router_isis.intf_router_isis, is_container='container', presence=False, yang_name="intf-router-isis", rest_name="", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'cli-drop-node-name': None, u'callpoint': u'IsisLoopbackInterfaceIpRouter'}}, namespace='urn:brocade.com:mgmt:brocade-isis', defining_module='brocade-isis', yang_type='container', is_config=True)
    except (TypeError, ValueError):
      raise ValueError({
          'error-string': 
--------------------
Prompt: 
def transformToNative(obj):
        """
        Turn obj.value into a list of dates, datetimes, or
        (datetime, timedelta) tuples.
        """
----------
Output: 

        if isinstance(obj, (datetime, timedelta)):
            return list(obj)
        elif isinstance(obj, (list, tuple)):
            return map(transformToNative, obj)
        else:
            return obj

    def transformToNative(data):
        """
        Turn data.value into a list of dates, datetimes, or
        (datetime, timedelta) tuples.
        """
        if isinstance(data, datetime):
            return list(data)
        elif isinstance(data, timedelta):
            return map(transformToNative, data)
        else:
            return data

    def transformToNativeList(data):
        """
       
----------
Solution: 

        if obj.isNative:
            return obj
        obj.isNative = True
        if obj.value == '':
            obj.value = []
            return obj
        tzinfo = getTzid(getattr(obj, 'tzid_param', None))
        valueParam = getattr(obj, 'value_param', "DATE-TIME").upper()
        valTexts = obj.value.split(",")
        if valueParam == "DATE":
            obj.value = [stringToDate(x) for x in valTexts]
        elif valueParam == "DATE-TIME":
            obj.value = [stringToDateTime(x, tzinfo) for x in valTexts]
        elif valueParam == "PERIOD":
            obj.value = [stringToPeriod(x, tzinfo) for x in valTexts]
        return obj
--------------------
Prompt: 
def _from_signer_and_info(cls, signer, info, **kwargs):
        """Creates a Credentials instance from a signer and service account
        info.

        Args:
            signer (google.auth.crypt.Signer): The signer used to sign JWTs.
            info (Mapping[str, str]): The service account info.
            kwargs: Additional arguments to pass to the constructor.

        Returns:
            google.auth.jwt.Credentials: The constructed credentials.

        Raises:
            ValueError: If the info is not in the expected format.
        """
----------
Output: 

        if not isinstance(info, dict):
            raise ValueError('info must be a mapping mapping str to str '
                             'or a mapping mapping str to str')

        return cls(
            signer._credentials,
            **info
        )

    @classmethod
    def from_signing_credentials(cls, info, **kwargs):
        """Creates a Credentials instance from a JWT signing credentials
        info.

        Args:
            info (Mapping[str, str]): The service account info.
            kwargs: Additional arguments to pass to the constructor.

        Returns:
            google.auth.jwt.credentials.Credentials: The constructed credentials.

        Raises
----------
Solution: 

        kwargs.setdefault('subject', info['client_email'])
        kwargs.setdefault('issuer', info['client_email'])
        return cls(signer, **kwargs)
--------------------
Prompt: 
def git_check():
    """
    Check that all changes , besides versioning files, are committed
    :return:
    """
----------
Output: 

    if not os.path.exists(GIT_DIR):
        return False
    if not os.path.isdir(GIT_DIR):
        return False
    if not os.access(GIT_DIR, os.W_OK):
        return False
    return True


def git_clone(git_url, target_dir):
    """
    Clone a git repository
    :param git_url:
    :param target_dir:
    :return:
    """
    if not os.path.exists(git_url):
        return False
    if not os.path.isdir(target_dir):
       
----------
Solution: 


    # check that changes staged for commit are pushed to origin
    output = local('git diff --name-only | egrep -v "^(pynb/version.py)|(version.py)$" | tr "\\n" " "',
                   capture=True).strip()
    if output:
        fatal('Stage for commit and commit all changes first: {}'.format(output))

    output = local('git diff --cached --name-only | egrep -v "^(pynb/version.py)|(version.py)$" | tr "\\n" " "',
                   capture=True).strip()
    if output:
        fatal('Commit all changes first: {}'.format(output))
--------------------
Prompt: 
def nohtml(s):
    """Return copy of ``s`` that will not treat ``'<...>'`` as DOT HTML string in quoting.

    Args:
        s: String in which leading ``'<'`` and trailing ``'>'`` should be treated as literal.
    Raises:
        TypeError: If ``s`` is not a ``str`` on Python 3, or a ``str``/``unicode`` on Python 2.

    >>> quote('<>-*-<>')
    '<>-*-<>'

    >>> quote(nohtml('<>-*-<>'))
    '"<>-*-<>"'
    """
----------
Output: 

    if isinstance(s, unicode):
        return s.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')
    elif isinstance(s, bytes):
        return s.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')
    else:
        raise TypeError("Expected str or bytes, got %r" % (type(s),))

def unescape(s):
    """Return string with unescaped HTML entities removed.

    Args:
        s: String with HTML entities removed.
    Raises:
        TypeError: If ``s`` is not
----------
Solution: 

    try:
        subcls = NOHTML[type(s)]
    except KeyError:
        raise TypeError('%r does not have one of the required types: %r' %
                        (s, list(NOHTML)))
    return subcls(s)
--------------------
Prompt: 
def parse_file(self, name):
        """Parse the content of a file.

        See 'parse' method for information.

        :param name: the pathname of the file to parse
        :return: True on success (no error detected), False otherwise
        """
----------
Output: 

        try:
            return self.parse(name)
        except Exception:
            return False

    def parse(self, name):
        """Parse the content of a file.

        :param name: the pathname of the file to parse
        :return: True on success, False otherwise
        """
        try:
            return self.parse_file(name)
        except Exception:
            return False

    def get_file_content(self, name):
        """Get the content of a file.

        :param name: the pathname of the file to parse
        :return: the content of the file
        """
        try:
            return
----------
Solution: 

        with open(name, "rb") as fp:
            return self.parse(fp.read())
--------------------
Prompt: 
def until(self, regex):
        """Wait until the regex encountered
        """
----------
Output: 

        while True:
            self.wait_for_match(regex)
            if self.match_count == 0:
                break
            time.sleep(self.match_interval)

    def wait_for_match(self, regex):
        """Wait for a regex match
        """
        self.logger.debug("Waiting for %s in %s seconds", regex, self.match_interval)
        time.sleep(self.match_interval)
        self.logger.debug("Regex %s found in %s seconds", regex, self.match_count)

    def get_match_count(self):
----------
Solution: 

        logger.debug('waiting for %s', regex)
        r = re.compile(regex, re.M)
        self.tn.expect([r])
--------------------
Prompt: 
def shape(self):
        """Number of grid points per axis."""
----------
Output: 

        return self._shape

    @property
    def n_dims(self):
        """Number of dimensions."""
        return self._n_dims

    @property
    def n_points(self):
        """Number of points per axis."""
        return self._n_points

    @property
    def n_dims_x(self):
        """Number of dimensions in x."""
        return self.n_dims * self.n_dims_y

    @property
    def n_dims_y(self):
        """Number of dimensions in y."""
        return self.n_points * self.n_points

    @property
----------
Solution: 

        try:
            return self.__shape
        except AttributeError:
            shape = tuple(len(vec) for vec in self.coord_vectors)
            self.__shape = shape
            return shape
--------------------
Prompt: 
def moduleInfo( module ):
        """
        Generates HTML information to display for the about info for a module.
        
        :param      module  | <module>
        """
----------
Output: 

        return """
        <h2>Module info</h2>
        <p>
        <table width="100%" border="0" cellspacing="0">
          <tr>
            <td width="50%">
              <table class="tablesorter" border="0" cellspacing="1">
                <tr>
                  <th width="50%">
                    <th align="center">
                      <th align="left">
                        <a href="http://www.w3.org/1999/xhtml">
                            <img src="http://www.w3.org/1999/xhtml" alt
----------
Solution: 

        data = module.__dict__
        
        html = []
        html.append( '<h2>%s</h2>' % data.get('__name__', 'Unknown') )
        html.append( '<hr/>' )
        ver = data.get('__version__', '0')
        html.append( '<small>version: %s</small>' % ver)
        html.append( '<br/>' )
        html.append( nativestring(data.get('__doc__', '')) )
        html.append( '<br/><br/><b>Authors</b><ul/>' )
        
        for author in data.get('__authors__', []):
            html.append( '<li>%s</li>' % author )
            
        html.append( '</ul>' )
        html.append( '<br/><br/><b>Depends on:</b>' )
        for depends in data.get('__depends__', []):
            html.append( '<li>%s</li>' % depends )
            
        html.append( '</ul>' )
        html.append( '' )
        html.append( '<br/><br/><b>Credits</b></ul>' )
        
        for credit in data.get('__credits__', []):
            html.append('<li>%s: %s</li>' % credit)
            
        html.append( '</ul>' )
        
        opts = (data.get('__maintainer__', ''), data.get('__email__', ''))
        html.append('<br/><br/><small>maintained by: %s email: %s</small>' % opts)
        
        opts = (data.get('__copyright__', ''), data.get('__license__', ''))
        html.append('<br/><small>%s | license: %s</small>' % opts)
        
        return '\n'.join(html)
--------------------
Prompt: 
def dataset_list(self,
                     sort_by=None,
                     size=None,
                     file_type=None,
                     license_name=None,
                     tag_ids=None,
                     search=None,
                     user=None,
                     mine=False,
                     page=1):
        """ return a list of datasets!

            Parameters
            ==========
            sort_by: how to sort the result, see valid_sort_bys for options
            size: the size of the dataset, see valid_sizes for string options
            file_type: the format, see valid_file_types for string options
            license_name: string descriptor for license, see valid_license_names
            tag_ids: tag identifiers to filter the search
            search: a search term to use (default is empty string)
            user: username to filter the search to
            mine: boolean if True, group is changed to "my" to return personal
            page: the page to return (default is 1)
        """
----------
Output: 
name: the license name, see valid_licenses for options
            tag_ids: the tags ids, see valid_tags for options
            search: the search query, see valid_search_bys for options
            user: the user, see valid_user_bys for options
            mine: if True, return a list of datasets, otherwise return a list of datasets

            Returns
            =======
            A list of datasets, each with the following fields:

            - id: the dataset id
            - name: the dataset name
            - description: the dataset description
            - tags: the dataset tags
            - tags_string: the
----------
Solution: 

        valid_sort_bys = ['hottest', 'votes', 'updated', 'active', 'published']
        if sort_by and sort_by not in valid_sort_bys:
            raise ValueError('Invalid sort by specified. Valid options are ' +
                             str(valid_sort_bys))

        valid_sizes = ['all', 'small', 'medium', 'large']
        if size and size not in valid_sizes:
            raise ValueError('Invalid size specified. Valid options are ' +
                             str(valid_sizes))

        valid_file_types = ['all', 'csv', 'sqlite', 'json', 'bigQuery']
        if file_type and file_type not in valid_file_types:
            raise ValueError('Invalid file type specified. Valid options are '
                             + str(valid_file_types))

        valid_license_names = ['all', 'cc', 'gpl', 'odb', 'other']
        if license_name and license_name not in valid_license_names:
            raise ValueError('Invalid license specified. Valid options are ' +
                             str(valid_license_names))

        if int(page) <= 0:
            raise ValueError('Page number must be >= 1')

        group = 'public'
        if mine:
            group = 'my'
            if user:
                raise ValueError('Cannot specify both mine and a user')
        if user:
            group = 'user'

        datasets_list_result = self.process_response(
            self.datasets_list_with_http_info(
                group=group,
                sort_by=sort_by or 'hottest',
                size=size or 'all',
                filetype=file_type or 'all',
                license=license_name or 'all',
                tagids=tag_ids or '',
                search=search or '',
                user=user or '',
                page=page))
        return [Dataset(d) for d in datasets_list_result]
--------------------
Prompt: 
def fixed_legend_filter_sort(self, fixed_legend_filter_sort):
        """Sets the fixed_legend_filter_sort of this ChartSettings.

        Whether to display \"Top\"- or \"Bottom\"-ranked series in the fixed legend  # noqa: E501

        :param fixed_legend_filter_sort: The fixed_legend_filter_sort of this ChartSettings.  # noqa: E501
        :type: str
        """
----------
Output: 


        self._fixed_legend_filter_sort = fixed_legend_filter_sort

    @property
    def fixed_legend_title(self):
        """Gets the fixed_legend_title of this ChartSettings.  # noqa: E501

        The fixed legend title for the series with the title for the chart  # noqa: E501

        :return: The fixed_legend_title of this ChartSettings.  # noqa: E501
        :rtype: str
        """
        return self._fixed_legend_title

    @fixed_legend_title.setter
    def fixed_legend_title(self, fixed
----------
Solution: 

        allowed_values = ["TOP", "BOTTOM"]  # noqa: E501
        if fixed_legend_filter_sort not in allowed_values:
            raise ValueError(
                "Invalid value for `fixed_legend_filter_sort` ({0}), must be one of {1}"  # noqa: E501
                .format(fixed_legend_filter_sort, allowed_values)
            )

        self._fixed_legend_filter_sort = fixed_legend_filter_sort
--------------------
Prompt: 
def bowtie(sam, btd, f, r, u, opt, no_shrink, threads):
    """
    generate bowtie2 command
    """
----------
Output: 

    if not isinstance(threads, int):
        threads = 1
    if not no_shrink:
        cmd = "bowtie2 -q -f %d -r %d -u %s -opt %s -noshrink %s" % (threads, r, u, opt, no_shrink)
    else:
        cmd = "bowtie2 -q -f %d -r %d -u %s -opt %s -noshrink %s" % (f, q, no_shrink, no_shrink)
    return cmd

def bowtie2_build(sam,
----------
Solution: 

    bt2 = 'bowtie2 -x %s -p %s ' % (btd, threads)
    if f is not False:
        bt2 += '-1 %s -2 %s ' % (f, r)
    if u is not False:
        bt2 += '-U %s ' % (u)
    bt2 += opt
    if no_shrink is False:
        if f is False:
            bt2 += ' | shrinksam -u -k %s-shrunk.sam ' % (sam)
        else:
            bt2 += ' | shrinksam -k %s-shrunk.sam ' % (sam)
    else:
        bt2 += ' > %s.sam' % (sam)
    return bt2
--------------------
Prompt: 
def parse(self, elt, ps):
        """elt -- the DOM element being parsed
        ps -- the ParsedSoap object.
        """
----------
Output: 

        if elt.tagName == 'r':
            self.parse_r(elt, ps)
        elif elt.tagName =='s':
            self.parse_s(elt, ps)
        elif elt.tagName == 'r':
            self.parse_r(elt, ps)
        elif elt.tagName =='s':
            self.parse_s(elt, ps)
        elif elt.tagName == 'r':
            self.parse_r(elt, ps)
        elif elt.tagName =='s':
            self.parse_s(elt, ps)
        elif elt.tagName == 'r
----------
Solution: 

        self.checkname(elt, ps)
        if len(_children(elt)) == 0:
            href = _find_href(elt)
            if not href:
                if self.nilled(elt, ps) is False:
                    return []
                if self.nillable is True: 
                    return Nilled
                raise EvaluateException('Required string missing',
                        ps.Backtrace(elt))
            if href[0] != '#':
                return ps.ResolveHREF(href, self)
            elt = ps.FindLocalHREF(href, elt)
            self.checktype(elt, ps)

        if self.nilled(elt, ps): return Nilled
        if len(_children(elt)) == 0: return []

        v = self.simple_value(elt, ps)
        return self.text_to_data(v, elt, ps)
--------------------
Prompt: 
def _clean_file(self, f):
        """this will take a given file path, scrub it accordingly, and save a new copy of the file
        in the same location"""
----------
Output: 

        if os.path.isfile(f):
            os.remove(f)
        os.rename(f, self.file)

    def _get_file_path(self, path):
        """returns the full path to the given file"""
        return os.path.join(self.file, path)

    def _get_file_contents(self, path):
        """returns the contents of the given file"""
        with open(self._get_file_path(path), 'r') as f:
            return f.read()

    def _get_file_lines(self, path):
----------
Solution: 

        if os.path.exists(f) and not os.path.islink(f):
            tmp_file = tempfile.TemporaryFile(mode='w+b')
            try:
                fh = open(f, 'r')
                data = fh.readlines()
                fh.close()
                if len(data) > 0: #if the file isn't empty:
                    for l in data:
                        new_l = self._clean_line(l)
                        tmp_file.write(new_l.encode('utf-8'))

                    tmp_file.seek(0)

            except Exception as e: # pragma: no cover
                self.logger.exception(e)
                raise Exception("CleanFile Error: Cannot Open File For Reading - %s" % f)

            try:
                if len(data) > 0:
                    new_fh = open(f, 'wb')
                    for line in tmp_file:
                        new_fh.write(line)
                    new_fh.close()
            except Exception as e: # pragma: no cover
                self.logger.exception(e)
                raise Exception("CleanFile Error: Cannot Write to New File - %s" % f)

            finally:
                tmp_file.close()
--------------------
Prompt: 
def distinct(self):
        """Return sets of indices for each distinct haplotype."""
----------
Output: 

        return set(self.haplotypes)

    def __str__(self):
        """Return string representation of the haplotype."""
        return 'Haplotype(haplotype_id={}, name={}, type={},'\
               'name={}, name_type={}, name_type_name={}, name_value={})'.format(self.haplotype_id, self.name, self.type, self.name_id, self.type_name, self.name_type, self.name_id, self.value)

    def __repr__(self):
        """Return string
----------
Solution: 


        # setup collection
        d = collections.defaultdict(set)

        # iterate over haplotypes
        for i in range(self.shape[1]):

            # hash the haplotype
            k = hash(self.values[:, i].tobytes())

            # collect
            d[k].add(i)

        # extract sets, sorted by most common
        return sorted(d.values(), key=len, reverse=True)
--------------------
Prompt: 
def is_dir_url(link):
    # type: (Link) -> bool
    """Return whether a file:// Link points to a directory.

    ``link`` must not have any other scheme but file://. Call is_file_url()
    first.

    """
----------
Output: 

    # type: (Link) -> bool
    """Return whether a file:// link points to a directory.

    """
    # type: (Link) -> bool
    """Return whether a file:// link points to a directory.

    # We use the following heuristic to figure out if a given path is a
    # link to a directory, but not a directory itself.
    #   - We don't allow for a trailing slash, but if we find one,
    #     we assume that we're a link to a directory.
    #   - We have to do this because we remove the trailing slash,
    #     which
----------
Solution: 

    link_path = url_to_path(link.url_without_fragment)
    return os.path.isdir(link_path)
--------------------
Prompt: 
def by_visits(self, event_kind=None):
        """
        Gets Venues in order of how many Events have been held there.
        Adds a `num_visits` field to each one.

        event_kind filters by kind of Event, e.g. 'theatre', 'cinema', etc.
        """
----------
Output: 

        if event_kind is not None:
            event_kind = self.event_kind_from_enum(event_kind)
        return self.get_venues(event_kind)

    def by_time(self, time_string):
        """
        Gets a list of events that occurred in a given time string.

        time_string filters by time string.
        """
        if time_string is not None:
            time_string = self.convert_time(time_string)
            return self.get_events(time_string)

    def convert_time(self, time_string):
----------
Solution: 

        qs = self.get_queryset()

        if event_kind is not None:
            qs = qs.filter(event__kind=event_kind)

        qs = qs.annotate(num_visits=Count('event')) \
                .order_by('-num_visits', 'name_sort')

        return qs
--------------------
Prompt: 
def create_registry(interface):
    """
    Create a new registry for an extensible interface.

    Parameters
    ----------
    interface : type
        The abstract data type for which to create a registry,
        which will manage registration of factories for this type.

    Returns
    -------
    interface : type
        The data type specified/decorated, unaltered.
    """
----------
Output: 

    return interface(
        'test_{}'.format(interface.__name__),
        lambda x: x.name,
        lambda x: x.name,
        lambda x: x.name,
        lambda x: x.name,
        lambda x: x.name,
        lambda x: x.name,
        lambda x: x.name,
        lambda x: x.name,
        lambda x: x.name,
        lambda x: x.name,
        lambda x: x.name,
        lambda x: x.name,
        lambda x: x.name,
        lambda x: x.name,
       
----------
Solution: 

    if interface in custom_types:
        raise ValueError('there is already a Registry instance '
                         'for the specified type')
    custom_types[interface] = Registry(interface)
    return interface
--------------------
Prompt: 
def regex_parse(regex, text, fromstart=True):
    r"""
    regex_parse

    Args:
        regex (str):
        text (str):
        fromstart (bool):

    Returns:
        dict or None:

    Example:
        >>> # DISABLE_DOCTEST
        >>> from utool.util_regex import *  # NOQA
        >>> regex = r'(?P<string>\'[^\']*\')'
        >>> text = " 'just' 'a' sentance with 'strings' in it "
        >>> fromstart = False
        >>> result = regex_parse(regex, text, fromstart)['string']
        >>> print(result)

    """
----------
Output: 
start=fromstart)
        >>> assert result is not None
        >>> assert'string' in result
        >>> assert'sentance' in result
        >>> assert 'from' in result
        >>> assert fromstart is True
        >>> result = regex_parse(regex, text, fromstart=True)
        >>> assert result is not None
        >>> assert'string' in result
        >>> assert'sentance' in result
        >>> assert 'from' in result
        >>> assert fromstart is True
        >>> assert result['from'] == 'just'
        >>> assert result['string'] == 'a'
        >>> result = regex_
----------
Solution: 

    match = regex_get_match(regex, text, fromstart=fromstart)
    if match is not None:
        parse_dict = match.groupdict()
        return parse_dict
    return None
--------------------
Prompt: 
def _scale_to_dtype(self, data, dtype):
        """Scale provided data to dtype range assuming a 0-1 range.

        Float input data is assumed to be normalized to a 0 to 1 range.
        Integer input data is not scaled, only clipped. A float output
        type is not scaled since both outputs and inputs are assumed to
        be in the 0-1 range already.

        """
----------
Output: 

        if data.size == 0:
            return data

        if dtype == 0:
            return data.dtype.type(0)

        if dtype == 1:
            return data.dtype.type(1)

        if dtype == 2:
            return data.dtype.type(2)

        if dtype == 3:
            return data.dtype.type(3)

        if dtype == 4:
            return data.dtype.type(4)

        if dtype == 5:
            return data.dtype.type(5)

        if dtype == 6:
            return data.dtype.type(6)

        if dtype == 7
----------
Solution: 

        if np.issubdtype(dtype, np.integer):
            if np.issubdtype(data, np.integer):
                # preserve integer data type
                data = data.clip(np.iinfo(dtype).min, np.iinfo(dtype).max)
            else:
                # scale float data (assumed to be 0 to 1) to full integer space
                dinfo = np.iinfo(dtype)
                data = data.clip(0, 1) * (dinfo.max - dinfo.min) + dinfo.min
            data = data.round()
        return data
--------------------
Prompt: 
def start_notifications(self):
        """Start the notifications thread.

        If an external callback is not set up (using `update_webhook`) then
        calling this function is mandatory to get or set resource.

        .. code-block:: python

            >>> api.start_notifications()
            >>> print(api.get_resource_value(device, path))
            Some value
            >>> api.stop_notifications()

        :returns: void
        """
----------
Output: 

        self._notifications_thread = threading.Thread(target=self._notifications_loop, name='start_notifications')
        self._notifications_thread.daemon = True
        self._notifications_thread.start()

    def stop_notifications(self):
        """Stop the notifications thread.

        :returns: void
        """
        self._notifications_thread.join()

    def _get_resource_value(self, device, path):
        """Get the value of a specific resource.

        :param device: device name
        :param path: path to the resource
        :returns: resource value
        :rtype: dict
       
----------
Solution: 

        with self._notifications_lock:
            if self.has_active_notification_thread:
                return
            api = self._get_api(mds.NotificationsApi)
            self._notifications_thread = NotificationsThread(
                self._db,
                self._queues,
                b64decode=self.b64decode,
                notifications_api=api,
                subscription_manager=self.subscribe,
            )
            self._notifications_thread.daemon = True
            self._notifications_thread.start()
--------------------
Prompt: 
def check_imported(module):
        """Check whether the provide module can be imported (package installed).

        Args:
            module (str): The name of the module to check availability.

        Returns:
            bool: True if the module can be imported, False otherwise.
        """
----------
Output: 

        return module in sys.modules

    def get_module_name(module):
        """Get the name of the provide module.

        Args:
            module (str): The name of the provide module.

        Returns:
            str: The name of the provide module.
        """
        return get_provide_module_name(module)

    def get_module_path(module):
        """Get the path of the provide module.

        Args:
            module (str): The name of the module to check availability.

        Returns:
            str: The path of the provide module.
        """
        return get_provide_module
----------
Solution: 

        imported = True
        module_info = ('', '', '')
        # TODO: if possible, update to a cleaner method that doesn't require importing the module
        # and running inline code.
        try:
            importlib.import_module(module)
            module_info = imp.find_module(module)
        except ImportError:
            imported = False

        # get module path
        module_path = module_info[1]
        description = module_info[2]

        if not description:
            # if description is None or empty string the module could not be imported
            imported = False
        elif not description and not module_path:
            # if description/module_path are None or empty string the module could not be imported
            imported = False
        elif module_path is not None and (
            'dist-packages' in module_path or 'site-packages' in module_path
        ):
            # if dist-packages|site-packages in module_path the import doesn't count
            imported = False
        return imported
--------------------
Prompt: 
def step4(self, encrypted_data):
        """Last pairing step."""
----------
Output: 

        self.data = encrypted_data
        self.encrypted_data = encrypted_data
        self.step = 2
        self.state = 'WAIT FOR STARTUP'
        self.data_received = False
        self.encrypted_data_received = False
        self.step_count = 0
        self.encrypted_data_count = 0
        self.state = 'WAIT FOR STARTUP'

    def step5(self, encrypted_data):
        """Last pairing step."""
        self.data = encrypted_data
        self.encrypted_data_count += 1
        self.encrypted_data_received = True
----------
Solution: 

        chacha = chacha20.Chacha20Cipher(self._session_key, self._session_key)
        decrypted_tlv_bytes = chacha.decrypt(
            encrypted_data, nounce='PS-Msg06'.encode())
        if not decrypted_tlv_bytes:
            raise Exception('data decrypt failed')  # TODO: new exception
        decrypted_tlv = tlv8.read_tlv(decrypted_tlv_bytes)
        _LOGGER.debug('PS-Msg06: %s', decrypted_tlv)

        atv_identifier = decrypted_tlv[tlv8.TLV_IDENTIFIER]
        atv_signature = decrypted_tlv[tlv8.TLV_SIGNATURE]
        atv_pub_key = decrypted_tlv[tlv8.TLV_PUBLIC_KEY]
        log_binary(_LOGGER,
                   'Device',
                   Identifier=atv_identifier,
                   Signature=atv_signature,
                   Public=atv_pub_key)

        # TODO: verify signature here

        return Credentials(atv_pub_key, self._signing_key.to_seed(),
                           atv_identifier, self.pairing_id)
--------------------
Prompt: 
def eth_getStorageAt(self, address, position):
        """Get account storage data at position.

        :param address:
        :param position:
        :return:
        """
----------
Output: 

        return self.get_account_storage(address, position)

    def eth_getStorage(self, address, storageType):
        """Get account storage data.

        :param address:
        :param storageType:
        :return:
        """
        return self.get_account_storage(address, storageType)

    def eth_getStorages(self, address):
        """Get account storages.

        :param address:
        :return:
        """
        return self.get_account_storages(address)

    def eth_get_account_storages(self, address, storagesType
----------
Solution: 

        account = self.reader._get_account(address)
        return _encode_hex(
            utils.zpad(utils.encode_int(account.get_storage_data(position)), 32)
        )
--------------------
Prompt: 
def log(message, type):
    """Log notices to stdout and errors to stderr"""
----------
Output: 

    print(message, file=sys.stderr)
    sys.exit(1)


def main():
    """Main function"""
    parser = argparse.ArgumentParser(description='Generate the documentation for the '
                                     'the %s documentation' % __name__,
                                     epilog='See the %s source for more details.' %
                                     __name__)
    parser.add_argument('--version', action='version', version=__version__)
    parser.add_argument('--html', action='store_true',
                        help='Generate the documentation')
    args = parser.parse_args()
    generate_docs(args.version,
----------
Solution: 


    (sys.stdout if type == 'notice' else sys.stderr).write(message + "\n")
--------------------
Prompt: 
def process_instrs(queue, stack, body, context):
    """
    Process instructions from the instruction queue.
    """
----------
Output: 

    while body:
        opcode = body.pop(0)
        if opcode == 'I':
            if len(body) == 1:
                # 'I' is the start of a new instruction.
                # 'I' is the end of a new instruction.
                # 'I' is the start of a new instruction.
                # 'I' is the end of a new instruction.
                # 'I' is the start of a new instruction.
                # 'I' is the end of a new instruction.
                # 'I' is the start of a new instruction.
                # 'I' is the end
----------
Solution: 

    next_instr = queue.popleft
    while queue:
        newcontext = _process_instr(next_instr(), queue, stack, body, context)
        if newcontext is not None:
            context = newcontext
--------------------
Prompt: 
def _read_txt(self, stream):
        """
        Load a PLY element from an ASCII-format PLY file.  The element
        may contain list properties.

        """
----------
Output: 

        # Read the header
        line = stream.readline()
        if line.startswith(b'PLY'):
            header = line.split(b' ', 1)[1]
            if header.startswith(b'Ply'):
                self.element_name = header[1:]
                self.element_type = header[0]
            else:
                raise PLYParseError('Expected PLY element name after PLY')
        elif line.startswith(b'End'):
            self.element_name = ''
            self.element_type = ''
            self.element_name = ''
        elif line.startswith(
----------
Solution: 

        self._data = _np.empty(self.count, dtype=self.dtype())

        k = 0
        for line in _islice(iter(stream.readline, b''), self.count):
            fields = iter(line.strip().split())
            for prop in self.properties:
                try:
                    self._data[prop.name][k] = prop._from_fields(fields)
                except StopIteration:
                    raise PlyElementParseError("early end-of-line",
                                               self, k, prop)
                except ValueError:
                    raise PlyElementParseError("malformed input",
                                               self, k, prop)
            try:
                next(fields)
            except StopIteration:
                pass
            else:
                raise PlyElementParseError("expected end-of-line",
                                           self, k)
            k += 1

        if k < self.count:
            del self._data
            raise PlyElementParseError("early end-of-file", self, k)
--------------------
Prompt: 
def calcKYratioDifference(self):
        """
        Returns the difference between the simulated capital to income ratio and the target ratio.
        Can only be run after solving all AgentTypes and running makeHistory.

        Parameters
        ----------
        None

        Returns
        -------
        diff : float
            Difference between simulated and target capital to income ratio.
        """
----------
Output: 

        return self.getKyrRatioDifference(None, None)

    def calcKyrRatioDifference(self):
        """
        Returns the difference between the KyrRatio and the simulated capital to income ratio.
        Can only be run after solving all AgentTypes and running makeHistory.

        Parameters
        ----------
        None

        Returns
        -------
        diff : float
            Difference between simulated and target capital to income ratio.
        """
        return self.getKyrRatioDifference(None, None)

    def calcKyrRatioDifference_old(self):
        """
        Returns the difference between the KyrRatio
----------
Solution: 

        # Ignore the first X periods to allow economy to stabilize from initial conditions
        KYratioSim = np.mean(np.array(self.KtoYnow_hist)[self.ignore_periods:])
        diff = KYratioSim - self.KYratioTarget
        return diff
--------------------
Prompt: 
def to_funset(self, lname="clamping", cname="clamped"):
        """
        Converts the list of clampings to a set of `gringo.Fun`_ instances

        Parameters
        ----------
        lname : str
            Predicate name for the clamping id

        cname : str
            Predicate name for the clamped variable

        Returns
        -------
        set
            Representation of all clampings as a set of `gringo.Fun`_ instances


        .. _gringo.Fun: http://potassco.sourceforge.net/gringo.html#Fun
        """
----------
Output: 

    """

    def to_funset_list(self, lname="funset", cname="funset"):
        """
        Converts the list of funtions to a set of `gringo.Fun`_ instances

        Parameters
        ----------
        lname : str
            Predicate name for the funtion id

        cname : str
            Predicate name for the funset variable

        Returns
        -------
        set
            Representation of all funtions as a set of `gringo.Fun`_ instances


       .. _potassco.models.funset.funset_list: http://potassco.sourceforge.net/
----------
Solution: 

        fs = set()
        for i, clamping in enumerate(self):
            fs.add(gringo.Fun(lname, [i]))
            fs = fs.union(clamping.to_funset(i, cname))

        return fs
--------------------
Prompt: 
def as_dict(self):
        """
        Returns the CTRL as a dictionary. "SITE" and "CLASS" are of
        the form {'CATEGORY': {'TOKEN': value}}, the rest is of the
        form 'TOKEN'/'CATEGORY': value. It gets the conventional standard
        structure because primitive cells use the conventional
        a-lattice parameter as the scaling factor and not the a-lattice
        parameter of the primitive cell.
        """
----------
Output: 

        return {'version': self.version, 'nb_sets': self.nb_sets,
               'symbol_count': self.symbol_count,'scaling_factor': self.scaling_factor,
                'units': self.units,'sites': self.sites,
                'class_count': self.class_count,'sites_count': self.nb_sets_count,
                'comment': self.comment, 'n_atoms': self.n_atoms_count,
                'n_residues': self.n_residues_count,'scaling_factor_count': self
----------
Solution: 

        ctrl_dict = {"@module": self.__class__.__module__,
                     "@class": self.__class__.__name__}
        if self.header is not None:
            ctrl_dict["HEADER"] = self.header
        if self.version is not None:
            ctrl_dict["VERS"] = self.version
        sga = SpacegroupAnalyzer(self.structure)
        alat = sga.get_conventional_standard_structure().lattice.a
        plat = self.structure.lattice.matrix/alat

        
--------------------
Prompt: 
def remove_mediators(tree, columns):
    """
    Removes intermediate nodes that are just mediators between their parent and child states.
    :param columns: list of characters
    :param tree: ete3.Tree
    :return: void, modifies the input tree
    """
----------
Output: 

    for i in range(0, len(columns)):
        if columns[i] == tree.root:
            columns[i] = tree.root.children[0]
            break
    for i in range(0, len(columns)):
        if columns[i] == tree.root:
            columns[i] = tree.root.children[0]
            break
    for i in range(0, len(columns)):
        if columns[i] == tree.root:
            columns[i] = tree.root.children[0]
            break
    for i in range(0, len(columns
----------
Solution: 

    num_removed = 0
    for n in tree.traverse('postorder'):
        if getattr(n, METACHILD, False) or n.is_leaf() or len(n.children) > 1 or n.is_root() \
                or getattr(n, NUM_TIPS_INSIDE) > 0:
            continue

        parent = n.up
        child = n.children[0]

        compatible = True
        for column in columns:
            states = getattr(n, column, set())
            parent_states = getattr(parent, column, set())
            child_states = getattr(child, column, set())
            # if mediator has unresolved states, it should hesitate between the parent and the child:
            if states != child_states | parent_states:
                compatible = False
                break

        if compatible:
            parent.remove_child(n)
            parent.add_child(child)
            num_removed += 1
    if num_removed:
        logging.getLogger('pastml').debug("Removed {} internal node{}"
                                          " with the state unresolved between the parent's and the only child's."
                                          .format(num_removed, '' if num_removed == 1 else 's'))
--------------------
Prompt: 
def ambigFilter(in_fh1, in_fh2, out_fh1, out_fh2, verbose=False, best=False):
  """
    @summary: take reads from in_fh1 and output to out_fh1 if they don't
              appear also in in_fh2 (ditto for in_fh2)

    @param in_fh1: BED formated stream of reads
    @param in_fh2: BED formated stream of reads
    @param out_fh1: Output reads that pass from in_fh1 to this stream
    @param out_fh2: Output reads that pass from in_fh2 to this stream
    @param verbose: output additional messages to sys.stderr if True
    @param best: Given two items that have the same name, try to output the one
                 with the best score

    @return: None (out streams have BED format)
  """
----------
Output: 
_fh2: Output reads that pass from in_fh2 to this stream
    @param verbose: Prints out the best/worst reads
  """
  # TODO: check if the two BED files are the same
  if verbose:
    print "Merging reads from %s and %s" % (in_fh1, in_fh2)

  # TODO: check if the two BED files are the same
  if verbose:
    print "Merging reads from %s and %s" % (out_fh1, out_fh2)

  # TODO: check if the two BED
----------
Solution: 

  for r1, r2 in BEDUniqueIterator(in_fh1, in_fh2, verbose, best, dropAfter=6):
    if r1 is not None:
      out_fh1.write(str(r1) + "\n")
    if r2 is not None:
      out_fh2.write(str(r2) + "\n")
--------------------
Prompt: 
def get_trainer(name):
    """return the unique id for a trainer, determined by the md5 sum
    """
----------
Output: 

    return hashlib.md5(name).hexdigest()


def get_trainers(trainer_id):
    """return a list of unique trainers
    """
    return [get_trainer(trainer_id) for trainer_id in get_trainer_ids(trainer_id)]


def get_trainer_by_id(trainer_id):
    """return the unique id for a trainer, determined by the md5 sum
    """
    return hashlib.md5(trainer_id).hexdigest()


def get_trainer_ids(trainer
----------
Solution: 

    name = name.lower()
    return int(hashlib.md5(name.encode('utf-8')).hexdigest(), 16) % 10**8
--------------------
Prompt: 
def to_dict(self):
        """
        This method converts the DictCell into a python `dict`.  This is useful
        for JSON serialization.
        """
----------
Output: 

        return dict(zip(self.columns, self.values))

    def __str__(self):
        """
        This method returns a string representation of the cell.
        """
        return json.dumps(self.to_dict(), indent=4)

    def __repr__(self):
        """
        This method returns a string representation of the cell.
        """
        return self.__str__()

    def __eq__(self, other):
        """
        This method is used to check if two cells are the same.
        """
        return self.to_dict() == other.to_dict()

    def __ne__(self
----------
Solution: 

        output = {}
        for key, value in self.__dict__['p'].iteritems():
            if value is None or isinstance(value, SIMPLE_TYPES):
                output[key] = value
            elif hasattr(value, 'to_dot'):
                output[key] = value.to_dot()
            elif hasattr(value, 'to_dict'):
                output[key] = value.to_dict()
            elif isinstance(value, datetime.date):
                # Convert date/datetime to ms-since-epoch ("new Date()").
                ms = time.mktime(value.utctimetuple()) * 1000
                ms += getattr(value, 'microseconds', 0) / 1000
                output[key] = int(ms)
            elif isinstance(value, dict):
                output[key] = []
            else:
                raise ValueError('cannot encode ' + repr(key))

        return output
--------------------
Prompt: 
def absolute_coords(self):
		"""The current absolute coordinates of the pointer event,
		in mm from the top left corner of the device.

		To get the corresponding output screen coordinate, use
		:meth:`transform_absolute_coords`.

		For pointer events that are not of type
		:attr:`~libinput.constant.EventType.POINTER_MOTION_ABSOLUTE`,
		this property raises :exc:`AttributeError`.

		Returns:
			(float, float): The current absolute coordinates.
		Raises:
			AttributeError
		"""
----------
Output: 

		raise AttributeError('absolute_coords')

	def transform_absolute_coords(self, x, y):
		"""Transform the screen coordinate from the absolute coordinates
		to the current absolute coordinates.

		Parameters:
			x (float): The x-coordinate of the absolute coordinates.
			y (float): The y-coordinate of the current absolute coordinates.

		Returns:
			(float, float): The x-coordinate of the absolute coordinates.
		Raises:
			ValueError: If the absolute coordinates are not of type
			ValueError: If the current absolute coordinates are of type
			
----------
Solution: 


		if self.type != EventType.POINTER_MOTION_ABSOLUTE:
			raise AttributeError(_wrong_prop.format(self.type))
		abs_x = self._libinput.libinput_event_pointer_get_absolute_x(
			self._handle)
		abs_y = self._libinput.libinput_event_pointer_get_absolute_y(
			self._handle)
		return abs_x, abs_y
--------------------
Prompt: 
def _populate_trace(self, graph: TraceGraph, trace_frame_ids: List[int]) -> None:
        """ Populates (from the given trace graph) the forward and backward
        traces reachable from the given traces (including input trace frames).
        Make sure to respect trace kind in successors
        """
----------
Output: 

        for trace_id in trace_frame_ids:
            trace = trace_graph.get_trace(trace_id)
            if trace.get('status') =='success':
                self.successors.append(trace_id)
            elif trace.get('status') == 'failure':
                self.failureors.append(trace_id)
            else:
                raise ValueError(f'Unexpected status: {trace.get("status")}')

    def _get_trace_id(self, trace_id: str) -> str:
        """ Returns the trace ID of the given trace (or None
----------
Solution: 

        while len(trace_frame_ids) > 0:
            trace_frame_id = trace_frame_ids.pop()
            if trace_frame_id in self._visited_trace_frame_ids:
                continue

            trace_frame = graph._trace_frames[trace_frame_id]
            self._add_trace_frame(graph, trace_frame)
            self._visited_trace_frame_ids.add(trace_frame_id)

            key = (trace_frame.callee_id.local_id, trace_frame.callee_port)
            trace_frame_ids.extend(
                [
                    trace_frame_id
                    for trace_frame_id in graph._trace_frames_map[key]
                    if trace_frame_id not in self._visited_trace_frame_ids
                    and graph._trace_frames[trace_frame_id].kind == trace_frame.kind
                ]
            )
--------------------
Prompt: 
def main(fast=False):
    """Run all benchmarks and print report to the console."""
----------
Output: 

    if not fast:
        print("Running benchmarks in fast mode.")
        benchmarks = [
            Benchmark(
                name=name,
                description=description,
                options=options,
                args=args,
                env=env,
                cwd=cwd,
                env_vars=env_vars,
                env_options=env_options,
                env_args=env_args,
                env_options_vars=env_options_vars,
                env_options_options=env_options_options,
                env_options_options_vars=env_options_options_vars,
                env_options_
----------
Solution: 

    print('Running benchmarks...\n')
    results = bench_run(fast=fast)
    bench_report(results)
--------------------
Prompt: 
def get_2mass_epoch (tmra, tmdec, debug=False):
    """Given a 2MASS position, look up the epoch when it was observed.

    This function uses the CDS Vizier web service to look up information in
    the 2MASS point source database. Arguments are:

    tmra
      The source's J2000 right ascension, in radians.
    tmdec
      The source's J2000 declination, in radians.
    debug
      If True, the web server's response will be printed to :data:`sys.stdout`.

    The return value is an MJD. If the lookup fails, a message will be printed
    to :data:`sys.stderr` (unconditionally!) and the :data:`J2000` epoch will
    be returned.

    """
----------
Output: 
.
    """
    # Get the epoch from the database
    epoch = float(tmra) + float(tmdec) / 2.0

    # If the epoch is less than 2.0, then we have to use the web service to get the
    # declination from the database
    if epoch > 2.0:
        # Get the epoch from the database
        tm_epoch = get_J2000_epoch (epoch, debug)
        # If the epoch is less than 2.0, we have to use the CDS Vizier web service to get the
        # declination from the database
        tmdec
----------
Solution: 

    import codecs
    try:
        from urllib.request import urlopen
    except ImportError:
        from urllib2 import urlopen
    postdata = b
--------------------
Prompt: 
def get_repository_config_acl(namespace, config, snapshot_id):
    """Get configuration permissions.

    The configuration should exist in the methods repository.

    Args:
        namespace (str): Configuration namespace
        config (str): Configuration name
        snapshot_id (int): snapshot_id of the method

    Swagger:
        https://api.firecloud.org/#!/Method_Repository/getConfigACL
    """
----------
Output: 

    permission_list = []
    if config is not None:
        for item in config.split(','):
            if item.startswith('method.'):
                permission_list.append(item[len('method.'):])
            elif item.startswith('method_repository.'):
                permission_list.append(item[len('method_repository.'):])
            else:
                raise ValueError('Unknown config item: %s' % item)
    return permission_list


def get_repository_config_acl_for_method(namespace, method):
    """Get the configuration ACL for a method.

    Args:
        namespace (str):
----------
Solution: 

    uri = "configurations/{0}/{1}/{2}/permissions".format(namespace,
                                                    config, snapshot_id)
    return __get(uri)
--------------------
Prompt: 
def _surfdens(self,R,z,phi=0.,t=0.):
        """
        NAME:
           _surfdens
        PURPOSE:
           evaluate the surface density for this potential
        INPUT:
           R - Galactocentric cylindrical radius
           z - vertical height
           phi - azimuth
           t - time
        OUTPUT:
           the surface density
        HISTORY:
           2018-08-19 - Written - Bovy (UofT)
        """
----------
Output: 

        return (R-self.R0) / (self.R0+z)

    def _surfdens_from_z(self,z,t=0.):
        """
        NAME:
           _surfdens_from_z
        PURPOSE:
           evaluate the surface density for this potential
        INPUT:
           z - vertical height
           t - time
        OUTPUT:
           the surface density
        HISTORY:
           2018-08-19 - Written - Bovy (UofT)
        """
        return _surfdens(self,R=self.R0,z=z,phi=phi,t
----------
Solution: 

        r= numpy.sqrt(R**2.+z**2.)
        Rma= numpy.sqrt(R**2.-self.a**2.+0j)
        if Rma == 0.:
            return (-12.*self.a**3-5.*self.a*z**2
                      +numpy.sqrt(1.+z**2/self.a**2)\
                         *(12.*self.a**3-self.a*z**2+2/self.a*z**4))\
                          /30./numpy.pi*z**-5.
        else:
            return self.a*((2.*self.a**2.+R**2.)*Rma**-5\
                               *(numpy.arctan(z/Rma)-numpy.arctan(self.a*z/r/Rma))
                           +z*(5.*self.a**3.*r-4.*self.a**4
                               +self.a**2*(2.*r**2.+R**2)
                               -self.a*r*(5.*R**2.+3.*z**2.)+R**2.*r**2.)
                           /(self.a**2.-R**2.)**2.
                           /(r**2-self.a**2.)**2.).real/4./numpy.pi
--------------------
Prompt: 
def datagetter(cls):
        """ example datagetter function, make any local modifications here """
----------
Output: 

        def datagetter_decorator(func):
            def _datagetter(self):
                return func
            return _datagetter

        return dict(
            (name, datagetter(name))
            for name in dir(cls)
            if not name.startswith('_') and name.isupper()
        )


class TestDataGetter(unittest.TestCase):
    """
    Tests for DataGetter
    """

    def test_datagetter_with_default(self):
        """
        Test that default is passed to datagetter
        """
        self.assertEqual(datagetter(None), None)

    def test_datagetter_with_
----------
Solution: 

        with open('myfile', 'rt') as f:
            rows = [r for r in csv.reader(f)]
        dothing = lambda _: [i for i, v in enumerate(_)]
        rows = [dothing(_) for _ in rows]
        raise NotImplementedError('You need to implement this yourlself!')
        return rows
--------------------
Prompt: 
def cowbat(self):
        """
        Run all the methods
        """
----------
Output: 

        self.logger.info("Running cowbat...")
        self.logger.info("Method: %s" % self.method)
        self.logger.info("Method arguments: %s" % self.method_args)
        self.logger.info("Method return: %s" % self.method_return)
        self.logger.info("Method arguments: %s" % self.method_args)
        self.logger.info("Method return: %s" % self.method_return)

        # Run the method
        self.logger.info("Running method...")
        self.logger
----------
Solution: 

        logging.info('Beginning COWBAT database downloads')
        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'genesippr')):
            self.sipprverse_targets(databasepath=self.databasepath)
        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'coregenome')):
            self.cowbat_targets(databasepath=self.databasepath)
        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'ConFindr')):
            self.confindr_targets()
        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'mash')):
            self.mash(databasepath=self.databasepath)
        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'MLST')):
            self.mlst(databasepath=self.databasepath)
        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'rMLST')):
            self.rmlst(databasepath=self.databasepath,
                       credentials=self.credentials)
        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'univec')):
            self.univec(databasepath=self.databasepath)
        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'resfinder')):
            self.cge_db_downloader(databasepath=self.databasepath,
                                   analysistype='resfinder',
                                   dbname='resfinder_db')
        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'virulence')):
            self.cge_db_downloader(databasepath=self.databasepath,
                                   analysistype='virulence',
                                   dbname='virulencefinder_db')
        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'serosippr')):
            self.cge_db_downloader(databasepath=self.databasepath,
                                   analysistype='serosippr',
                                   dbname='serotypefinder_db')
        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'pointfinder')):
            self.cge_db_downloader(databasepath=self.databasepath,
                                   analysistype='pointfinder',
                                   dbname='pointfinder_db')
        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'clark')):
            self.clark(databasepath=self.databasepath)
        if self.overwrite or not os.path.isdir(os.path.join(self.databasepath, 'mob_suite')):
            self.mob_suite_targets()
--------------------
Prompt: 
def authcode_post(self, path, **kwargs):
        """Perform an HTTP POST to okcupid.com using this profiles session
        where the authcode is automatically added as a form item.
        """
----------
Output: 

        return self._post(path, kwargs)

    def authcode_get(self, path, **kwargs):
        """Perform an HTTP GET to okcupid.com using this profiles session
        where the authcode is automatically added as a form item.
        """
        return self._get(path, kwargs)

    def _get(self, path, **kwargs):
        """Perform an HTTP GET to okcupid.com using this profiles session
        where the authcode is automatically added as a form item.
        """
        return self.session.get(self.urls[path], **kwargs)

    def _
----------
Solution: 

        kwargs.setdefault('data', {})['authcode'] = self.authcode
        return self._session.okc_post(path, **kwargs)
--------------------
Prompt: 
def get_sls_config_file(path, stage, region):
    """Determine Serverless config file name."""
----------
Output: 

    return path + ".sls" if path.endswith(".sls") else path


def get_sls_home(path):
    """Determine Serverless home directory."""
    return get_sls_config_file(path, 'home', 'home') else None


def get_sls_data_home(path):
    """Determine Serverless data directory."""
    return get_sls_config_file(path, 'home', 'data') else None


def get_sls_data_home_no_file(path):
    """Determine Serverless data file name
----------
Solution: 

    for name in gen_sls_config_files(stage, region):
        if os.path.isfile(os.path.join(path, name)):
            return name
    return "config-%s.json" % stage
--------------------
Prompt: 
def validateStringInput(input_key,input_data, read=False):
    """
    To check if a string has the required format. This is only used for POST APIs.
    """
----------
Output: 

    if not isinstance(input_data,basestring):
        raise TypeError("validateStringInput() argument must be a string")
    if not isinstance(input_key,basestring):
        raise TypeError("validateStringInput() argument must be a string")
    if read:
        return input_data.read()
    else:
        return input_data

def validateIntegerInput(input_key,input_data,read=False):
    """
    To check if an integer has the required format. This is only used for POST APIs.
    """
    if not isinstance(input_data,int):
        raise TypeError("validate
----------
Solution: 

    log = clog.error_log
    func = None
    if '*' in input_data or '%' in input_data:
        func = validationFunctionWildcard.get(input_key)
        if func is None:
            func = searchstr
    elif input_key == 'migration_input' :
        if input_data.find('#') != -1 : func = block
        else : func = dataset
    else:
        if not read:
            func = validationFunction.get(input_key)
            if func is None:
                func = namestr
        else:
            if input_key == 'dataset':
                func = reading_dataset_check
            elif input_key == 'block_name':
                func = reading_block_check
            elif input_key == 'logical_file_name':
                func = reading_lfn_check
            else:
                func = namestr
    try:
        func(input_data)
    except AssertionError as ae:
        serverLog = str(ae) + " key-value pair (%s, %s) cannot pass input checking" %(input_key, input_data)
        #print serverLog
        dbsExceptionHandler("dbsException-invalid-input2", message="Invalid Input Data %s...:  Not Match Required Format" %input_data[:10], \
            logger=log.error, serverError=serverLog)
    return input_data
--------------------
Prompt: 
def release_port(upnp, external_port):
    """Try to release the port mapping for `external_port`.

    Args:
        external_port (int): the port that was previously forwarded to.

    Returns:
        success (boolean): if the release was successful.
    """
----------
Output: 

    if not upnp.is_running:
        return False

    if not upnp.is_port_bound:
        return False

    if not upnp.is_port_bound.is_set():
        return False

    if not upnp.is_port_bound.is_set():
        return False

    if not upnp.is_port_bound.is_set():
        return False

    if not upnp.is_running_port:
        return False

    return upnp.release_port(external_port)


def get_port_mappings(upnp):
    """Get
----------
Solution: 

    mapping = upnp.getspecificportmapping(external_port, 'UDP')

    if mapping is None:
        log.error('could not find a port mapping', external=external_port)
        return False
    else:
        log.debug('found existing port mapping', mapping=mapping)

    if upnp.deleteportmapping(external_port, 'UDP'):
        log.info('successfully released port mapping', external=external_port)
        return True

    log.warning(
        'could not release port mapping, check your router for stale mappings',
    )
    return False
--------------------
Prompt: 
def disambiguate_entity(key, text):
    """Resolve ambiguity between entities with same dimensionality."""
----------
Output: 

    if len(text)!= len(key):
        return text
    if len(text)!= len(self.dim_names):
        return text
    if len(text) == 0:
        return text
    if len(text) == len(self.dim_names):
        return text
    if key!= self.dim_names[0]:
        return text
    if self.dim_names[1]!= self.dim_names[key]:
        return text
    if len(text)!= len(self.dim_names[self.dim_names[0]]]):
        return text
    return
----------
Solution: 

    new_ent = l.DERIVED_ENT[key][0]

    if len(l.DERIVED_ENT[key]) > 1:
        transformed = TFIDF_MODEL.transform([text])
        scores = CLF.predict_proba(transformed).tolist()[0]
        scores = sorted(zip(scores, TARGET_NAMES), key=lambda x: x[0],
                        reverse=True)
        names = [i.name for i in l.DERIVED_ENT[key]]
        scores = [i for i in scores if i[1] in names]
        try:
            new_ent = l.ENTITIES[scores[0][1]]
        except IndexError:
            logging.debug('\tAmbiguity not resolved for "%s"', str(key))

    return new_ent
--------------------
Prompt: 
def get_nn_info(self, structure, n):
        """
        Get all near-neighbor sites as well as the associated image locations
        and weights of the site with index n using the closest relative
        neighbor distance-based method with VIRE atomic/ionic radii.

        Args:
            structure (Structure): input structure.
            n (integer): index of site for which to determine near
                neighbors.

        Returns:
            siw (list of tuples (Site, array, float)): tuples, each one
                of which represents a neighbor site, its image location,
                and its weight.
        """
----------
Output: 

        if self.is_spin_polarized:
            return self._get_spin_polarized_info(structure, n)
        else:
            return self._get_nn_info(structure, n)

    def _get_spin_polarized_info(self, structure, n):
        """
        Get all near-neighbor sites as well as the associated image
        locations and atomic weights of the site with index n using the
        VIRE atomic/ionic radii.

        Args:
            structure (Structure): input structure.
            n (integer): index of site for which to determine near
                neighbors.
----------
Solution: 


        vire = ValenceIonicRadiusEvaluator(structure)
        site = vire.structure[n]
        neighs_dists = vire.structure.get_neighbors(site, self.cutoff)
        rn = vire.radii[vire.structure[n].species_string]

        reldists_neighs = []
        for neigh, dist in neighs_dists:
            reldists_neighs.append([dist / (
                    vire.radii[neigh.species_string] + rn), neigh])

        siw = []
        min_reldist = min([reldist for reldist, neigh in reldists_neighs])
        for reldist, s in reldists_neighs:
            if reldist < (1.0 + self.tol) * min_reldist:
                w = min_reldist / reldist
                siw.append({'site': s,
                            'image': self._get_image(vire.structure, s),
                            'weight': w,
                            'site_index': self._get_original_site(
                                vire.structure, s)})

        return siw
--------------------
Prompt: 
def get_token_async(self, refresh=False):
    """Get an authentication token.

    The token is cached in memcache, keyed by the scopes argument.
    Uses a random token expiration headroom value generated in the constructor
    to eliminate a burst of GET_ACCESS_TOKEN API requests.

    Args:
      refresh: If True, ignore a cached token; default False.

    Yields:
      An authentication token. This token is guaranteed to be non-expired.
    """
----------
Output: 

    if not refresh:
      raise NotImplementedError('Refresh is not implemented.')
    if not self._cache_control:
      raise NotImplementedError('CacheControl is not implemented.')
    if not self._cache_control.is_admin():
      raise NotImplementedError('CacheControl is not implemented.')
    if not self._cache_control.is_moderator():
      raise NotImplementedError('CacheControl is not implemented.')
    if not self._cache_control.is_admin():
      raise NotImplementedError('CacheControl is not implemented.')
    if not self._cache.has_key(self._get_current_token_key()):
      raise KeyError('No
----------
Solution: 

    key = '%s,%s' % (self.service_account_id, ','.join(self.scopes))
    ts = yield _AE_TokenStorage_.get_by_id_async(
        key,
        use_cache=True,
        use_memcache=self.retry_params.memcache_access_token,
        use_datastore=self.retry_params.save_access_token)
    if refresh or ts is None or ts.expires < (
        time.time() + self.expiration_headroom):
      token, expires_at = yield self.make_token_async(
          self.scopes, self.service_account_id)
      timeout = int(expires_at - time.time())
      ts = _AE_TokenStorage_(id=key, token=token, expires=expires_at)
      if timeout > 0:
        yield ts.put_async(memcache_timeout=timeout,
                           use_datastore=self.retry_params.save_access_token,
                           force_writes=True,
                           use_cache=True,
                           use_memcache=self.retry_params.memcache_access_token)
    raise ndb.Return(ts.token)
--------------------
Prompt: 
def build_catalog(site, datasets, format=None):
    """Build the DCAT catalog for this site"""
----------
Output: 

    if format is None:
        format = 'xml'
    if format == 'xml':
        return build_xml_catalog(site, datasets)
    elif format == 'json':
        return build_json_catalog(site, datasets)
    elif format == 'jsonp':
        return build_json_catalog(site, datasets)
    elif format == 'xml':
        return build_xml_catalog(site, datasets)
    elif format == 'xmlp':
        return build_xml_catalog(site, datasets)
    elif format == 'xml':
        return build_xml_catalog(site, datasets)
----------
Solution: 

    site_url = url_for('site.home_redirect', _external=True)
    catalog_url = url_for('site.rdf_catalog', _external=True)
    graph = Graph(namespace_manager=namespace_manager)
    catalog = graph.resource(URIRef(catalog_url))

    catalog.set(RDF.type, DCAT.Catalog)
    catalog.set(DCT.title, Literal(site.title))
    catalog.set(DCT.language,
                Literal(current_app.config['DEFAULT_LANGUAGE']))
    catalog.set(FOAF.homepage, URIRef(site_url))

    publisher = graph.resource(BNode())
    publisher.set(RDF.type, FOAF.Organization)
    publisher.set(FOAF.name, Literal(current_app.config['SITE_AUTHOR']))
    catalog.set(DCT.publisher, publisher)

    for dataset in datasets:
        catalog.add(DCAT.dataset, dataset_to_rdf(dataset, graph))

    if isinstance(datasets, Paginable):
        if not format:
            raise ValueError('Pagination requires format')
        catalog.add(RDF.type, HYDRA.Collection)
        catalog.set(HYDRA.totalItems, Literal(datasets.total))
        kwargs = {
            'format': format,
            'page_size': datasets.page_size,
            '_external': True,
        }

        first_url = url_for('site.rdf_catalog_format', page=1, **kwargs)
        page_url = url_for('site.rdf_catalog_format',
                           page=datasets.page, **kwargs)
        last_url = url_for('site.rdf_catalog_format',
                           page=datasets.pages, **kwargs)
        pagination = graph.resource(URIRef(page_url))
        pagination.set(RDF.type, HYDRA.PartialCollectionView)

        pagination.set(HYDRA.first, URIRef(first_url))
        pagination.set(HYDRA.last, URIRef(last_url))
        if datasets.has_next:
            next_url = url_for('site.rdf_catalog_format',
                               page=datasets.page + 1, **kwargs)
            pagination.set(HYDRA.next, URIRef(next_url))
        if datasets.has_prev:
            prev_url = url_for('site.rdf_catalog_format',
                               page=datasets.page - 1, **kwargs)
            pagination.set(HYDRA.previous, URIRef(prev_url))

        catalog.set(HYDRA.view, pagination)

    return catalog
--------------------
Prompt: 
def add_template_global(self, func: Callable, name: Optional[str]=None) -> None:
        """Add a template global.

        This is designed to be used on the application directly. An
        example usage,

        .. code-block:: python

            def five():
                return 5

            app.add_template_global(five)

        Arguments:
            func: The function that is the global.
            name: The global name (defaults to function name).
        """
----------
Output: 

        if name is None:
            name = self.get_global_name()
        if name in self.templates:
            raise ValueError(f"Duplicate template name: {name}")
        self.templates[name] = func

    def add_template_global_with_default_context(self, func: Callable, name: Optional[str]=None) -> None:
        """Add a template global with default context.

        This is designed to be used on the application directly. An
        example usage,

       .. code-block:: python

            def five():
                return 5

            app.add_template_global_
----------
Solution: 

        self.jinja_env.globals[name or func.__name__] = func
--------------------
Prompt: 
def load(parser, serializer):
    """Returns a dictionary of builtin functions for Fortran. Checks the
    cache first to see if we have a serialized version. If we don't, it
    loads it from the XML file.

    :arg parser: the DocParser instance for parsing the XML tags.
    :arg serializer: a Serializer instance from the CodeParser to cache
      the loaded XML file.
    """
----------
Output: 

    return dict((name, func) for name, func in serializer.get_functions().items())


class XMLParser(object):
    """A class that parses XML files.

    :arg file: the XML file to parse.
    :arg version: the XML version to parse.
    :arg functions: a dictionary of builtin functions.
    """

    def __init__(self, file, version, version_map, version_class=XMLParserVersion):
        """Initialize the XMLParser.

        :arg file: the XML file to parse.
        :arg version: the XML version to parse.
        :arg
----------
Solution: 

    fortdir = os.path.dirname(fortpy.__file__)
    xmlpath = os.path.join(fortdir, "isense", "builtin.xml")
    if not os.path.isfile(xmlpath):
        return {}

    changed_time = os.path.getmtime(xmlpath)
    cached = serializer.load_module("builtin.xml", changed_time)
    if cached is None:
        result = _load_builtin_xml(xmlpath, parser)
        serializer.save_module("builtin.xml", result, changed_time)
    else:
        result = cached

    return result
--------------------
Prompt: 
def resource(ref: str, delimiter: str = None) -> str:
    """
    Given a (URI) reference, return up to its delimiter (exclusively), or all of it if there is none.

    :param ref: reference
    :param delimiter: delimiter character (default None maps to '#', or ';' introduces identifiers)
    """
----------
Output: 

    if delimiter is None:
        return ref
    return delimiter.join(ref.split(delimiter))


def uri(uri: str) -> str:
    """
    Given a URI, return the URI.

    :param uri: URI
    :return: URI
    """
    return uri.split('#')[0]


def uri_to_path(uri: str) -> str:
    """
    Given a URI, return the path portion of the URI.

    :param uri: URI
    :return: path portion of the URI
    """
    if uri is None:
        return ''
    return uri.split
----------
Solution: 


    return ref.split(delimiter if delimiter else '#')[0]
--------------------
Prompt: 
def target_types_by_alias(self):
    """Returns a mapping from target alias to the target types produced for that alias.

    Normally there is 1 target type per alias, but macros can expand a single alias to several
    target types.

    :API: public

    :rtype: dict
    """
----------
Output: 

    return dict((t.type, t.target_types[t.type]) for t in self.types)

  def __repr__(self):
    return 'Target(name=%s, types=%s)' % (self.name, self.target_types)


class Rule(object):
  """A rule specifies how to process a single rule.

  :API: public

  :param name: The name of the rule.
  :param types: The mapping of target types to the aliases.
  :param targets: The targets to process the rule against.
  :type targets: dict
  :param
----------
Solution: 

    target_types_by_alias = defaultdict(set)
    for alias, target_type in self.target_types.items():
      target_types_by_alias[alias].add(target_type)
    for alias, target_macro_factory in self.target_macro_factories.items():
      target_types_by_alias[alias].update(target_macro_factory.target_types)
    return dict(target_types_by_alias)
--------------------
Prompt: 
def _http_get(self, url, query):
        """
        Performs the HTTP GET Request.
        """
----------
Output: 

        try:
            response = requests.get(url, params=query, headers=self.headers)
        except requests.exceptions.ConnectionError:
            raise ConnectionError(
                "Unable to connect to the server. Is the server running?"
            )

        if response.status_code!= 200:
            raise ConnectionError(
                "Unable to connect to the server. Is the server running?"
            )

        return response.json()

    def _http_post(self, url, data):
        """
        Performs the HTTP POST Request.
        """
        try:
            response = requests.post(url, data=json.dumps(
----------
Solution: 


        if not self.authorization_as_header:
            query.update({'access_token': self.access_token})

        response = None

        self._normalize_query(query)

        kwargs = {
            'params': query,
            'headers': self._request_headers()
        }

        if self._has_proxy():
            kwargs['proxies'] = self._proxy_parameters()

        response = requests.get(
            self._url(url),
            **kwargs
        )

        if response.status_code == 429:
            raise RateLimitExceededError(response)

        return response
--------------------
Prompt: 
def create_socketpair(size=None):
    """
    Create a :func:`socket.socketpair` to use for use as a child process's UNIX
    stdio channels. As socket pairs are bidirectional, they are economical on
    file descriptor usage as the same descriptor can be used for ``stdin`` and
    ``stdout``. As they are sockets their buffers are tunable, allowing large
    buffers to be configured in order to improve throughput for file transfers
    and reduce :class:`mitogen.core.Broker` IO loop iterations.
    """
----------
Output: 

    if size is None:
        size = os.sysconf('SC_OPEN_MAX')
    if size is None:
        size = int(os.sysconf('SC_OPEN_MAX_FILESIZE'))
    if size > 0:
        return socket.socketpair(size, broker_file=broker_file)
    else:
        return None


def create_socketpair_with_files(size=None):
    """
    Create a :func:`socket.socketpair` to use for use as a child process's UNIX
    stdio channels. As socket pairs are bidirectional, they are e
----------
Solution: 

    parentfp, childfp = socket.socketpair()
    parentfp.setsockopt(socket.SOL_SOCKET,
                        socket.SO_SNDBUF,
                        size or mitogen.core.CHUNK_SIZE)
    childfp.setsockopt(socket.SOL_SOCKET,
                       socket.SO_RCVBUF,
                       size or mitogen.core.CHUNK_SIZE)
    return parentfp, childfp
--------------------
Prompt: 
def shiftLeft(col, numBits):
    """Shift the given value numBits left.

    >>> spark.createDataFrame([(21,)], ['a']).select(shiftLeft('a', 1).alias('r')).collect()
    [Row(r=42)]
    """
----------
Output: 

    sc = SparkContext._active_spark_context
    return Column(sc._jvm.functions.shiftLeft(_to_java_column(col), numBits))


@ignore_unicode_prefix
@since(1.5)
def shiftRight(col, numBits):
    """In a string column, this returns the value that is `numBits` shifted right.

    >>> df = spark.createDataFrame([('ab',)], ['a'])
    >>> df.select(shiftRight('a', 3).alias('r')).collect()
    [Row(r=42)]
    """
    sc = SparkContext._
----------
Solution: 

    sc = SparkContext._active_spark_context
    return Column(sc._jvm.functions.shiftLeft(_to_java_column(col), numBits))
--------------------
Prompt: 
def generate_diary(self):
        """
        extracts event information from core tables into diary files
        """
----------
Output: 

        # get the list of events
        events = self.core_tables.find_all('event')

        # get the list of events with the same name
        if self.name!= events[0].find('name').text:
            self.name = events[0].find('name').text
            self.description = events[0].find('description').text
        else:
            self.name = None
            self.description = None

        # get the list of events with the same description
        if self.description!= events[0].find('description').text:
            self.description = events[0].
----------
Solution: 

        print('Generate diary files from Event rows only')
        for r in self.table:
            print(str(type(r)) + ' = ', r)
--------------------
Prompt: 
def pkg_upgrade(repo, skip, flag):
    """Checking packages for upgrade
    """
----------
Output: 

    if not skip:
        cmd = "yum update -y --nogpgcheck %s" % flag
    else:
        cmd = "yum remove -y --nogpgcheck %s" % flag
    rc, out = run_command(cmd)
    if rc == 0:
        return out.split('\n')
    else:
        return []


def repo_install(module, items, repoq, yum_basecmd, conf_file, en_repos, dis_repos):
    """Install one or more repos
    """
    pkgs = []
    res = {}
    res['results'] = []
----------
Solution: 

    Msg().checking()
    PACKAGES_TXT = RepoInit(repo).fetch()[0]
    pkgs_for_upgrade = []
    # name = data[0]
    # location = data[1]
    # size = data[2]
    # unsize = data[3]
    data = repo_data(PACKAGES_TXT, repo, flag="")
    for pkg in installed():
        status(0.0005)
        inst_pkg = split_package(pkg)
        for name in data[0]:
            if name:    # this tips because some pkg_name is empty
                repo_pkg = split_package(name[:-4])
            if (repo_pkg[0] == inst_pkg[0] and
                LooseVersion(repo_pkg[1]) > LooseVersion(inst_pkg[1]) and
                repo_pkg[3] >= inst_pkg[3] and
                    inst_pkg[0] not in skip and
                    repo_pkg[1] != "blacklist"):
                pkgs_for_upgrade.append(repo_pkg[0])
    Msg().done()
    if "--checklist" in flag:
        pkgs_for_upgrade = choose_upg(pkgs_for_upgrade)
    return pkgs_for_upgrade
--------------------
Prompt: 
def ensure_table(self, cls):
        """Ensure table's existence - as per the gludb spec."""
----------
Output: 

        if cls.table_exists(cls.table_name):
            self.log.info("Table '%s' already exists.", cls.table_name)
        else:
            self.log.info("Creating table '%s'...", cls.table_name)
            self.execute("CREATE TABLE %s (id INTEGER PRIMARY KEY, "
                         "name TEXT NOT NULL, "
                         "type INTEGER NOT NULL, "
                         "description TEXT NOT NULL, "
                         "unit_id INTEGER NOT NULL, "
                         "description_html TEXT, "
                         "unit_class TEXT NOT NULL, "
                         "unit_
----------
Solution: 

        id_len = len(uuid())
        index_names = cls.index_names() or []
        cols = [
            'id char(%d) primary key' % (id_len,),
            'value jsonb'
        ] + [
            name + ' text' for name in index_names
        ]

        table_name = cls.get_table_name()

        with self._conn() as conn:
            with conn.cursor() as cur:
                cur.execute('create table if not exists %s (%s);' % (
                    table_name,
                    ','.join(cols)
                ))
                for name in index_names:
                    cur.execute('create index if not exists %s on %s(%s);' % (
                        table_name + '_' + name + '_idx',
                        table_name,
                        name
                    ))
--------------------
Prompt: 
def process(self, sched, coro):
        """If there aren't enough coroutines waiting for the signal as the
        recipicient param add the calling coro in another queue to be activated
        later, otherwise activate the waiting coroutines."""
----------
Output: 

        if len(self.coroutines) < self.max_coroutines:
            self.coroutines.append(coro)
            return
        self.coroutines.append(coro)
        self.queue.put(coro)
        self.coro_running.put(True)
        self.coro = []
        self.coro_coro = []
        self.coro_queue = []
        self.coro_coro = []
        self.max_coroutines = len(self.coroutines)
        self.max_coro_
----------
Solution: 

        super(Signal, self).process(sched, coro)
        self.result = len(sched.sigwait[self.name])
        if self.result < self.recipients:
            sched.signals[self.name] = self
            self.coro = coro
            return

        for waitop, waitcoro in sched.sigwait[self.name]:
            waitop.result = self.value
        if self.prio & priority.OP:
            sched.active.extendleft(sched.sigwait[self.name])
        else:
            sched.active.extend(sched.sigwait[self.name])

        if self.prio & priority.CORO:
            sched.active.appendleft((None, coro))
        else:
            sched.active.append((None, coro))

        del sched.sigwait[self.name]
--------------------
Prompt: 
def transform_annotation(self, ann, duration):
        """Apply the structure agreement transformation.

        Parameters
        ----------
        ann : jams.Annotation
            The segment annotation

        duration : number > 0
            The target duration

        Returns
        -------
        data : dict
            data['agree'] : np.ndarray, shape=(n, n), dtype=bool
        """
----------
Output: 

        # TODO: This is a hack to get the duration from the segment
        #       and use it to compute the duration.
        #       We should be able to use the same number of frames as the
        #       segment, but we don't want to have to compute the same duration
        #       for the same frame.
        #       We should be able to use the same number of frames as the
        #       segment, but we don't want to have to compute the same duration for
        #       different frames.
        data = {'agree': np.zeros(ann.num_frames, dtype=bool),
----------
Solution: 


        intervals, values = ann.to_interval_values()

        intervals, values = adjust_intervals(intervals, values,
                                             t_min=0, t_max=duration)
        # Re-index the labels
        ids, _ = index_labels(values)

        rate = float(self.hop_length) / self.sr
        # Sample segment labels on our frame grid
        _, labels = intervals_to_samples(intervals, ids, sample_size=rate)

        # Make the agreement matrix
        return {'agree': np.equal.outer(labels, labels)}
--------------------
Prompt: 
def sync(self):
        """
        Syncs the items on this layer with the current layer settings.
        """
----------
Output: 

        self.clear_cache()
        self.update_geometry()
        self.update_attributes()
        self.update_geometry_from_data()
        self.update_geometry_from_data_and_attributes()

    def __str__(self):
        "Returns the string representation of this layer."
        return self.name

    def __repr__(self):
        "For `sync`."
        return '<%s %r>' % (self.__class__.__name__, self.name)

    def __init__(self, data, source, name=None):
        self.data = data
        self.source =
----------
Solution: 

        layerData = self.layerData()
        for item in self.scene().items():
            try:
                if item._layer == self:
                    item.syncLayerData(layerData)
            except AttributeError:
                continue
--------------------
Prompt: 
def text(text, message='', title=''):
    """
    This function is suitable for displaying general text, which can be longer
    than in :func:`message`

    :ref:`screenshots<text>`

    :param text: (long) text to be displayed
    :param message: (short) message to be displayed.
    :param title: window title
    :rtype: None
    """
----------
Output: 

    if not message:
        message = ''
    if not title:
        title = ''
    win = sublime.active_window()
    if win:
        win.focus_view(win.active_view())
    else:
        win = sublime.active_window()
    if win:
        win.run_command('insert', {'characters': message,'start': text})
    else:
        win.run_command('insert', {'characters': message,'start': text, 'end': text})


def get_window_title(window):
    """
    This function returns the title of
----------
Solution: 

    return backend_api.opendialog("text", dict(text=text, message=message, title=title))
--------------------
Prompt: 
def render_page(path):
    """Internal interface to the page view.

    :param path: Page path.
    :returns: The rendered template.
    """
----------
Output: 

    return render_template('page.html', path=path)


@app.route('/<path:path>')
def page_view(path):
    """Internal interface to the page view.

    :param path: Page path.
    :returns: The rendered template.
    """
    return render_page(path)


@app.route('/<path:path>')
def page_view_path(path):
    """Internal interface to the page view path.

    :param path: Page path.
    :returns: The page path.
    """
    return render_path(path, path
----------
Solution: 

    try:
        page = Page.get_by_url(request.path)
    except NoResultFound:
        abort(404)

    return render_template(
        [page.template_name, current_app.config['PAGES_DEFAULT_TEMPLATE']],
        page=page)
--------------------
Prompt: 
def update(self, *args, **kwargs):
        """Update ConfigMap from mapping/iterable.

        If the key exists the entry is updated else it is added.

        Args:
            *args: variable length argument list.  A valid argument is a two item
                tuple/list.  The first item is the key and the second is the value.
            **kwargs: Arbitrary keyword arguments representing the config.
        """
----------
Output: 

        if len(args)!= 2:
            raise TypeError, "expected at least 2 arguments, got %s" % len(args)
        if not isinstance(kwargs, dict):
            raise TypeError, "expected keyword arguments, got %s" % kwargs
        for k, v in kwargs.items():
            self[k] = v

    def setdefault(self, key, default=None):
        """Set a variable's value from a dictionary.

        Args:
            *key: variable name.
            *default: default value to set.
        """
        if key not in self:
            self[key] = default
----------
Solution: 

        for k, v in args:
            self[k] = v
        for k, v in kwargs.items():
            self[k] = v
--------------------
Prompt: 
def verification_events(self):
        """
        Events related to command verification.

        :type: List[:class:`.CommandHistoryEvent`]
        """
----------
Output: 

        return [CommandHistoryEvent(command=command, timestamp=timestamp) for command, timestamp in self._command_history_events]

    def _get_command_history_events(self):
        return [CommandHistoryEvent(command=command) for command in self._command_history_events]

    def _get_command_line_events(self):
        return [CommandLineEvent(command=command) for command in self._command_line_events]

    def _get_command_line_options(self):
        return [
            Option(
                name='timeout',
                type=int,
               
----------
Solution: 

        queued = self._assemble_event('Verifier_Queued')
        started = self._assemble_event('Verifier_Started')
        return [x for x in [queued, started] if x]
--------------------
Prompt: 
def get_channel(device, ch_name, channel_dict, loader, resource_dict):
    """Get a channels from a channels dictionary.

    :param name: name of the device
    :param device_dict: device dictionary
    :rtype: Device
    """
----------
Output: 

    if not isinstance(device_dict, dict):
        raise TypeError("device_dict must be a dictionary")
    if not isinstance(loader, dict):
        raise TypeError("loader must be a dictionary")
    if not isinstance(resource_dict, dict):
        raise TypeError("resource_dict must be a dictionary")

    channels = []
    for ch in channel_dict:
        if ch not in device:
            raise ValueError("Channel %s not found in device dictionary" % ch)
        if not loader.get(ch):
            raise ValueError("Channel %s not found in loader dictionary" % ch)
        channels.
----------
Solution: 

    channel_dict = get_bases(channel_dict, loader)

    r_ids = resource_dict.get('channel_ids', {}).get(ch_name, [])
    ids = r_ids if r_ids else channel_dict.get('ids', {})

    can_select = False if channel_dict.get('can_select') == 'False' else True
    channels = Channels(device, ids, can_select)

    update_component(ch_name, channels, channel_dict)

    return channels
--------------------
Prompt: 
def focus_next_unfolded(self):
        """focus next unfolded message in depth first order"""
----------
Output: 

        self.set_depth(1)
        self.set_mode(self.mode)
        self.set_tags(self.tags)
        self.set_flags(self.flags)
        self.set_flags(self.flags)
        self.set_flags(self.flags)
        self.set_flags(self.flags)
        self.set_flags(self.flags)
        self.set_flags(self.flags)
        self.set_flags(self.flags)
        self.set_flags(self.flags)
        self.set_flags(self.
----------
Solution: 

        self.focus_property(lambda x: not x.is_collapsed(x.root),
                            self._tree.next_position)
--------------------
Prompt: 
def gatk_remove_missingalt(in_file, data):
    """
    GATK 4.1.0.0 outputs variants that have missing ALTs, which breaks downstream
    tools, this filters those out.
    """
----------
Output: 

    out_file = "%s.gatk" % in_file
    if not utils.file_exists(out_file):
        return out_file

    if not utils.file_exists(in_file):
        return out_file

    if not utils.file_exists(utils.get_ref_file(in_file)):
        return out_file

    ref_file = utils.get_ref_file(in_file)
    if not utils.file_exists(ref_file):
        return ref_file

    utils.system("rm -f %s" % ref_file
----------
Solution: 

    base = in_file.split('.vcf.gz')[0]
    out_file = "%s-nomissingalt%s" % (base, '.vcf.gz')
    if utils.file_exists(out_file):
        return out_file
    no_gzip_out = out_file.replace(".vcf.gz", ".vcf")
    with file_transaction(no_gzip_out) as tx_out_file:
        with utils.open_gzipsafe(in_file) as in_handle, open(tx_out_file, "w") as out_handle:
            for line in in_handle:
                line = remove_missingalt(line)
                if line:
                    out_handle.write(line)
    return vcfutils.bgzip_and_index(no_gzip_out, data["config"])
--------------------
Prompt: 
def parse_pylint_output(output):
    """
    Parses pylint output, counting number of errors, conventions, etc
    :param output: output list generated by run_pylint()
    :return:
    """
----------
Output: 

    errors = 0
    for line in output:
        if line.startswith('ERROR'):
            errors += 1
        elif line.startswith('CONVERTER'):
            conversions = 0
            for i in range(0, len(line.split('\n'))):
                if line.split('\n')[i] == 'CONVERT':
                    conversions += int(line.split('\n')[i+1])
                elif line.split('\n')[i] == 'CONVERT_TO_NONE':
                    conversions += int(line.split('\n')[i+1])
                else:
                    raise Exception('Unknown pylint output:
----------
Solution: 


    stripped_output = [x[0] for x in output]

    counter = Counter(stripped_output)

    return counter
--------------------
Prompt: 
def on_channel_open(self, channel):
        """
        Input channel creation callback
        Queue declaration done here

        Args:
            channel: input channel
        """
----------
Output: 

        self.channels.append(channel)

    def on_channel_close(self, channel):
        """
        Input channel termination callback
        Queue declaration done here

        Args:
            channel: input channel
        """
        self.channels.remove(channel)

    def on_channel_open_error(self, channel, error):
        """
        Input channel creation callback
        Queue declaration done here

        Args:
            channel: input channel
            error: error message
        """
        self.channels.append(channel)

    def on_channel_close_error(self, channel, error):
        """
        Input channel termination callback
       
----------
Solution: 

        self.in_channel.exchange_declare(exchange='input_exc', type='topic', durable=True)
        channel.queue_declare(callback=self.on_input_queue_declare, queue=self.INPUT_QUEUE_NAME)
--------------------
Prompt: 
def compute_jaccard_index(x_set, y_set):
    """Return the Jaccard similarity coefficient of 2 given sets.

    Args:
        x_set (set): first set.
        y_set (set): second set.

    Returns:
        float: Jaccard similarity coefficient.

    """
----------
Output: 

    return jaccard_similarity_coefficient(x_set, y_set, 2)


def compute_jaccard_index_2(x_set, y_set):
    """Return the Jaccard similarity coefficient of 2 given sets.

    Args:
        x_set (set): first set.
        y_set (set): second set.

    Returns:
        float: Jaccard similarity coefficient.

    """
    return compute_jaccard_index(x_set, y_set) / 2.0


def compute_jaccard_index_3(x
----------
Solution: 

    if not x_set or not y_set:
        return 0.0

    intersection_cardinal = len(x_set & y_set)
    union_cardinal = len(x_set | y_set)

    return intersection_cardinal / float(union_cardinal)
--------------------
Prompt: 
def return_hdr(self):
        """Return the header for further use.

        Returns
        -------
        subj_id : str
            subject identification code
        start_time : datetime
            start time of the dataset
        s_freq : float
            sampling frequency
        chan_name : list of str
            list of all the channels
        n_samples : int
            number of samples in the dataset
        orig : dict
            additional information taken directly from the header

        Notes
        -----
        It only reads hdf5 matlab files and the VARiable needs to be called
        'data'

        h5py is necessary for this function
        """
----------
Output: 
 """
        return self.data

    def read_h5(self, h5_file):
        """Read a matlab file.

        Parameters
        ----------
        h5_file : str
            name of the hdf5 file

        Returns
        -------
        subj_id : str
            subject identification code
        start_time : datetime
            start time of the dataset
        s_freq : float
            sampling frequency
        chan_name : list of str
            all the channels
        n_samples : int
            number of samples in the dataset
        orig : dict
            additional information taken directly from the header

        Notes
        -----
        This function reads
----------
Solution: 

        # fieldtrip does not have this information
        orig = dict()
        subj_id = str()
        start_time = datetime.fromordinal(1)  # fake

        try:
            ft_data = loadmat(self.filename, struct_as_record=True,
                              squeeze_me=True)
            if VAR not in ft_data:
                raise KeyError('Save the FieldTrip variable as ''{}
--------------------
Prompt: 
def _update_plotting_params(self, **kwargs):
        """Some plotting parameters can be changed through the tool; this
        updataes those plotting parameters.
        """
----------
Output: 

        for key, value in kwargs.items():
            if key == 'x':
                self.x_label = value
            elif key == 'y':
                self.y_label = value
            elif key == 'z':
                self.z_label = value
            elif key == 'color':
                self.color = value
            elif key =='marker':
                self.marker = value
            elif key =='markersize':
                self.markersize = value
            elif key =='markerfacecolor':
                self.markerfacecolor = value
            elif key =='markerfacecolor':
                self.markerfacecolor
----------
Solution: 

        scalars = kwargs.get('scalars', None)
        if scalars is not None:
            old = self.display_params['scalars']
            self.display_params['scalars'] = scalars
            if old != scalars:
                self.plotter.subplot(*self.loc)
                self.plotter.remove_actor(self._data_to_update, reset_camera=False)
                self._need_to_update = True
                self.valid_range = self.input_dataset.get_data_range(scalars)
                # self.display_params['rng'] = self.valid_range
        cmap = kwargs.get('cmap', None)
        if cmap is not None:
            self.display_params['cmap'] = cmap
--------------------
Prompt: 
def course_is_user_registered(self, course, username=None):
        """
        Checks if a user is registered
        :param course: a Course object
        :param username: The username of the user that we want to check. If None, uses self.session_username()
        :return: True if the user is registered, False else
        """
----------
Output: 

        try:
            user = self.session_username(username)
            return user in course.enrollment_set.all()
        except ObjectDoesNotExist:
            return False

    def get_user_by_email(self, email):
        """
        Gets the user object from the User model, if exists.
        :param email: The email of the user we want to check.
        :return: The user object if exists, None otherwise
        """
        try:
            return User.objects.get(email=email)
        except ObjectDoesNotExist:
            return None

    def get_user_by_username(self, username):
       
----------
Solution: 

        if username is None:
            username = self.session_username()

        if self.has_staff_rights_on_course(course, username):
            return True

        return self._database.aggregations.find_one({"students": username, "courseid": course.get_id()}) is not None
--------------------
Prompt: 
def _create_opt_rule(self, rulename):
        """ Given a rule name, creates an optional ply.yacc rule
            for it. The name of the optional rule is
            <rulename>_opt
        """
----------
Output: 

        if not self.rules:
            self.rules = {}
            self.rulesname = rulename
        self.rules[rulename] = {
            'type': 'p_rule',
            'name': rulename,
            'p_pattern': '',
            'action': 'permit',
            'p_name': '',
            'p_args': [],
            'p_file': '',
            'p_fileloc': '',
            'p_line': '',
            'p_col': '',
            'p_error': '',
            'p_errorloc': '',
            'p_errorline': '',
----------
Solution: 

        optname = rulename + '_opt'

        def optrule(self, p):
            p[0] = p[1]

        optrule.__doc__ = '%s : empty\n| %s' % (optname, rulename)
        optrule.__name__ = 'p_%s' % optname
        setattr(self.__class__, optrule.__name__, optrule)
--------------------
Prompt: 
def disconnect_node(node, src=True, dst=True):
    """Disconnect all connections from node

    :param node: the node to disconnect
    :type node: str
    :returns: None
    :rtype: None
    :raises: None
    """
----------
Output: 

    for i in range(len(node.remotes)):
        if node.remotes[i] == src:
            node.remotes[i] = dst
        else:
            if not i == len(node.remotes)-1:
                node.remotes[i+1].disconnect()
            if not src:
                node.remotes[i].disconnect()
            if dst:
                node.remotes[i+1].disconnect(remotes[i+1)

    if node.remotes[0] == dst:
        return

    for i in range(len(node.remotes
----------
Solution: 

    if dst:
        destconns = cmds.listConnections(node, connections=True, plugs=True, source=False) or []
        for i in range(0, len(destconns), 2):
            source, dest = destconns[i], destconns[i+1]
            cmds.disconnectAttr(source, dest)
    if src:
        srcconns = cmds.listConnections(node, connections=True, plugs=True, destination=False) or []
        for i in range(0, len(srcconns), 2):
            source, dest = srcconns[i+1], srcconns[i]
            cmds.disconnectAttr(source, dest)
--------------------
Prompt: 
def toil_get_file(file_store, index, existing, file_store_id):
    """Get path to input file from Toil jobstore."""
----------
Output: 

    if existing:
        return existing
    else:
        return file_store_id


def toil_get_file_id(file_id, existing):
    """Get path to input file from Toil jobid."""
    if existing:
        return existing
    else:
        return file_id


def toil_get_file_path(file_id):
    """Get path to input file from Toil jobid."""
    return file_id


def toil_get_file_path_id(file_path_id):
    """Get path to input file from Toil job
----------
Solution: 


    if not file_store_id.startswith("toilfs:"):
        return file_store.jobStore.getPublicUrl(file_store.jobStore.importFile(file_store_id))
    src_path = file_store.readGlobalFile(file_store_id[7:])
    index[src_path] = file_store_id
    existing[file_store_id] = src_path
    return schema_salad.ref_resolver.file_uri(src_path)
--------------------
Prompt: 
def get_broadcast(self, broadcast_id):
        """
        Use this method to get details on a broadcast that is in-progress.

        :param String broadcast_id: The ID of the broadcast you want to stop

        :rtype A Broadcast object, which contains information of the broadcast: id, sessionId
        projectId, createdAt, updatedAt, resolution, broadcastUrls and status
        """
----------
Output: 

        return self._get_resource(
            "getBroadcast",
            {"broadcast_id": broadcast_id},
            False,
            "GET",
            False,
            "calendar",
            "broadcasts/{calendar_id}/stop".format(
                calendar_id=self._calendar_id
            )
        )

    def get_calendar_events(self):
        """
        Use this method to get details on all calendar events.

        :rtype A list of CalendarEvent objects, each representing a single calendar event
        """
        return self._get_resource(
            "getCalendarEvents",
            {"calendar_id": self._
----------
Solution: 

        endpoint = self.endpoints.broadcast_url(broadcast_id)
        response = requests.get(
            endpoint,
            headers=self.json_headers(),
            proxies=self.proxies,
            timeout=self.timeout
        )

        if response.status_code == 200:
            return Broadcast(response.json())
        elif response.status_code == 400:
            raise BroadcastError(
                'Invalid request. This response may indicate that data in your request '
                'data is invalid JSON.')
        elif response.status_code == 403:
            raise AuthError('Authentication error.')
        elif response.status_code == 409:
            raise BroadcastError('No matching broadcast found (with the specified ID).')
        else:
            raise RequestError('OpenTok server error.', response.status_code)
--------------------
Prompt: 
def parse_corant(blob):
    """Creates new blob entries for the given blob keys"""
----------
Output: 

    return [
        {
            'key': k,
            'value': v,
        } for k, v in six.iteritems(blob)
    ]


def parse_blob(blob):
    """Creates new blob entries for the given blob keys"""
    return [
        {
            'key': k,
            'value': v,
        } for k, v in six.iteritems(blob)
    ]


def get_blob(key):
    """Returns the entry for the given key"""
    return [
        {
            'key': k,
            'value': v,
        } for k, v in
----------
Solution: 


    if 'track_seamuon' in blob.keys():

        muon = blob['track_seamuon']

        blob['Muon'] = Table({
            'id': np.array(muon)[:, 0].astype(int),
            'pos_x': np.array(muon)[:, 1],
            'pos_y': np.array(muon)[:, 2],
            'pos_z': np.array(muon)[:, 3],
            'dir_x': np.array(muon)[:, 4],
            'dir_y': np.array(muon)[:, 5],
            'dir_z': np.array(muon)[:, 6],
            'energy': np.array(muon)[:, 7],
            'time': np.array(muon)[:, 8],
            'particle_id': np.array(muon)[:, 9].astype(int),
            'is_charm': np.array(muon)[:, 10].astype(int),
            'mother_pid': np.array(muon)[:, 11].astype(int),
            'grandmother_pid': np.array(muon)[:, 11].astype(int),
        },
                             h5loc='muon')

        blob['MuonMultiplicity'] = Table({
            'muon_multiplicity': len(np.array(muon)[:, 6])
        },
                                         h5loc='muon_multiplicity')

    if 'track_seaneutrino' in blob.keys():

        nu = blob['track_seaneutrino']

        blob['Neutrino'] = Table({
            'id': np.array(nu)[:, 0].astype(int),
            'pos_x': np.array(nu)[:, 1],
            'pos_y': np.array(nu)[:, 2],
            'pos_z': np.array(nu)[:, 3],
            'dir_x': np.array(nu)[:, 4],
            'dir_y': np.array(nu)[:, 5],
            'dir_z': np.array(nu)[:, 6],
            'energy': np.array(nu)[:, 7],
            'time': np.array(nu)[:, 8],
            'particle_id': np.array(nu)[:, 9].astype(int),
            'is_charm': np.array(nu)[:, 10].astype(int),
            'mother_pid': np.array(nu)[:, 11].astype(int),
            'grandmother_pid': np.array(nu)[:, 11].astype(int),
        },
                                 h5loc='nu')
        blob['NeutrinoMultiplicity'] = Table({
            'total': len(np.array(nu)[:, 6]),
            'nue': len(np.array(nu)[:, 6][np.array(nu)[:, 9] == 66]),
            'anue': len(np.array(nu)[:, 6][np.array(nu)[:, 9] == 67]),
            'numu': len(np.array(nu)[:, 6][np.array(nu)[:, 9] == 68]),
            'anumu': len(np.array(nu)[:, 6][np.array(nu)[:, 9] == 69]),
        },
                                             h5loc='nu_multiplicity')

    if ('track_seamuon' or 'track_seaneutrino') in blob.keys():

        blob['Weights'] = Table({
            'w1': blob['weights'][0][0],
            'w2': blob['weights'][0][1],
            'w3': blob['weights'][0][2],
        },
                                h5loc='weights')

    if 'track_primary' in blob.keys():

        primary = blob['track_primary']

        blob['Primary'] = Table({
            'id': np.array(primary)[:, 0].astype(int),
            'pos_x': np.array(primary)[:, 1],
            'pos_y': np.array(primary)[:, 2],
            'pos_z': np.array(primary)[:, 3],
            'dir_x': np.array(primary)[:, 4],
            'dir_y': np.array(primary)[:, 5],
            'dir_z': np.array(primary)[:, 6],
            'energy': np.array(primary)[:, 7],
            'time': np.array(primary)[:, 8],
            'particle_id': np.array(primary)[:, 9].astype(int)
        },
                                h5loc='primary')

    return blob
--------------------
Prompt: 
def _prepare_wsdl_objects(self):
        """
        Create the data structure and get it ready for the WSDL request.
        """
----------
Output: 

        wsdl = self.wsdl
        wsdl_url = wsdl.url
        wsdl_version = wsdl.version

        # Create the root element
        root = wsdl.root
        root.setAttribute("xmlns:wsdl", WSDL_NAMESPACE)
        root.setAttribute("xmlns:xsi", "http://www.w3.org/2001/XMLSchema-instance")
        root.setAttribute("xmlns:xsd", "http://www.w3.org/2001/XMLSchema")

        # Create the wsdl:Service element
        service = wsdl.services[wsdl_url]
        wsdl_
----------
Solution: 


        # Service defaults for objects that are required.
        self.MultipleMatchesAction = 'RETURN_ALL'
        self.Constraints = self.create_wsdl_object_of_type('SearchLocationConstraints')
        self.Address = self.create_wsdl_object_of_type('Address')
        self.LocationsSearchCriterion = 'ADDRESS'
        self.SortDetail = self.create_wsdl_object_of_type('LocationSortDetail')
--------------------
Prompt: 
def profiler():
    """Profile the block"""
----------
Output: 

    import cProfile
    return cProfile.runctx('s', globals(), locals(), 'block.prof')

def _get_type(name):
    """Returns the type of the named type"""
    return type_dict[name]

def _get_type_name(name):
    """Returns the type name of the named type"""
    return type_dict[name]

def _get_type_name_and_args(name, args):
    """Returns the type name and arguments of a named type"""
    return name, args

def _get_type_name_
----------
Solution: 

    import cProfile
    import pstats
    pr = cProfile.Profile()
    pr.enable()
    yield
    pr.disable()
    ps = pstats.Stats(pr).sort_stats('tottime')
    ps.print_stats()
--------------------
Prompt: 
def popn(self, buffer_type, count):
        """Remove and return the oldest count values from the named buffer

        Args:
            buffer_type (str): The buffer to pop from (either u"storage" or u"streaming")
            count (int): The number of readings to pop

        Returns:
            list(IOTileReading): The values popped from the buffer
        """
----------
Output: 

        return [reading.pop(count) for reading in self.readings if reading.buffer_type == buffer_type]

    def get_latest_reading(self, buffer_id):
        """Get the latest reading from the named buffer

        Args:
            buffer_id (str): The buffer id to get the latest reading from

        Returns:
            IOTileReading: The latest reading from the buffer
        """
        return self.readings[0]

    def get_latest_buffer(self, buffer_type, count):
        """Get the latest buffer values from the named buffer

        Args:
            buffer
----------
Solution: 


        buffer_type = str(buffer_type)

        if buffer_type == u'streaming':
            chosen_buffer = self.streaming_data
        else:
            chosen_buffer = self.storage_data

        if count > len(chosen_buffer):
            raise StreamEmptyError("Not enough data in buffer for popn command", requested=count, stored=len(chosen_buffer), buffer=buffer_type)

        popped = chosen_buffer[:count]
        remaining = chosen_buffer[count:]

        if buffer_type == u'streaming':
            self.streaming_data = remaining
        else:
            self.storage_data = remaining

        return popped
--------------------
Prompt: 
def arrival_delay_greater_than(item_id, delay, namespace="_expected_arrival"):
    """Check if an item arrival is delayed more than a given amount."""
----------
Output: 

    return _arrival_delay_greater_than(item_id, delay, namespace)


def arrival_delay_less_than(delay, value, namespace="_expected_arrival"):
    """Check if an item arrival is delayed more than a given amount."""
    return _arrival_delay_greater_than(item_id, delay, namespace)


def arrival_delay_greater_than_or_equal(delay, value, namespace="_expected_arrival"):
    """Check if an item arrival is delayed less than or equal amount."""
    return _arrival_
----------
Solution: 

    expected = get_state(item_id, namespace=namespace)
    now = time.time()
    if expected and (now - expected) > delay:
        logger.error("Timeout: waited %s seconds for parent.", delay)
        return True
    elif expected:
        logger.info("Still out of order but no timeout: %s-%s <= %s.",
                    now, expected, delay)
        return False
    elif delay > 0:
        logger.info("Storing expected arrival time (%s) for context '%s'",
                    datetime.fromtimestamp(now).isoformat(), item_id)
        set_state(item_id, now, namespace=namespace)
        return False
    else:
        logger.info("Event is out of order but not waiting for parent.")
        return True
--------------------
Prompt: 
def save_profile_as(self):
        """Save the minimum needs under a new profile name.
        """
----------
Output: 

        self.profile_name = self.min_profile_name + " " + self.min_profile_name_length
        self.save()

    def save_profile_as_profile_id(self):
        """Save the new profile id.
        """
        self.profile_id = self.min_profile_id + " " + self.min_profile_name_length
        self.save()

    def save_profile_as_user(self):
        """Save the minimum needs under a new username.
        """
        self.user = self.create_account_and_fetch_
----------
Solution: 

        # noinspection PyCallByClass,PyTypeChecker
        file_name_dialog = QFileDialog(self)
        file_name_dialog.setAcceptMode(QFileDialog.AcceptSave)
        file_name_dialog.setNameFilter(self.tr('JSON files (*.json *.JSON)'))
        file_name_dialog.setDefaultSuffix('json')
        dir = os.path.join(QgsApplication.qgisSettingsDirPath(),
                           'inasafe', 'minimum_needs')
        file_name_dialog.setDirectory(expanduser(dir))
        if file_name_dialog.exec_():
            file_name = file_name_dialog.selectedFiles()[0]
        else:
            return
        file_name = basename(file_name)
        file_name = file_name.replace('.json', '')
        minimum_needs = {'resources': []}
        self.mark_current_profile_as_saved()
        for index in range(self.resources_list.count()):
            item = self.resources_list.item(index)
            minimum_needs['resources'].append(item.resource_full)
        minimum_needs['provenance'] = self.provenance.text()
        minimum_needs['profile'] = file_name
        self.minimum_needs.update_minimum_needs(minimum_needs)
        self.minimum_needs.save()
        self.minimum_needs.save_profile(file_name)
        if self.profile_combo.findText(file_name) == -1:
            self.profile_combo.addItem(file_name)
        self.profile_combo.setCurrentIndex(
            self.profile_combo.findText(file_name))
--------------------
Prompt: 
def connect_array(self, address, connection_key, connection_type, **kwargs):
        """Connect this array with another one.

        :param address: IP address or DNS name of other array.
        :type address: str
        :param connection_key: Connection key of other array.
        :type connection_key: str
        :param connection_type: Type(s) of connection desired.
        :type connection_type: list
        :param \*\*kwargs: See the REST API Guide on your array for the
                           documentation on the request:
                           **POST array/connection**
        :type \*\*kwargs: optional

        :returns: A dictionary describing the connection to the other array.
        :rtype: ResponseDict

        .. note::

            Currently, the only type of connection is "replication".

        .. note::

            Requires use of REST API 1.2 or later.

        """
----------
Output: 
kwargs: dict
        :return: None
                 If the method is called asynchronously,
                 returns the request thread.
        """
        kwargs['_return_http_data_only'] = True
        if kwargs.get('callback'):
            return self.put_with_http_info(address, 'connect_array', connection_key, **kwargs)
        else:
            (data) = self.put_with_http_info(address, 'connect_array', connection_type, **kwargs)
            return data

    def disconnect_array(self, **kwargs):
        """Disconnect this array from the DNS server.

       
----------
Solution: 

        data = {"management_address": address,
                "connection_key": connection_key,
                "type": connection_type}
        data.update(kwargs)
        return self._request("POST", "array/connection", data)
--------------------
Prompt: 
def create_cloudwatch_event(app_name, env, region, rules):
    """Create cloudwatch event for lambda from rules.

    Args:
        app_name (str): name of the lambda function
        env (str): Environment/Account for lambda function
        region (str): AWS region of the lambda function
        rules (dict): Trigger rules from the settings
    """
----------
Output: 

    return {
        "name": app_name,
        "event_source": "lambda",
        "event_version": "1.0",
        "event_source_version": "1.0",
        "event_version_version": "1.0",
        "version": "1.0",
        "region": region,
        "rules": rules,
        "event_version_id": "1.0",
        "event_source_id": "lambda",
        "event_version_name": "AWS Lambda",
        "event_version_name": "AWS Lambda",
        "
----------
Solution: 

    session = boto3.Session(profile_name=env, region_name=region)
    cloudwatch_client = session.client('events')

    rule_name = rules.get('rule_name')
    schedule = rules.get('schedule')
    rule_description = rules.get('rule_description')
    json_input = rules.get('json_input', {})

    if schedule is None:
        LOG.critical('Schedule is required and no schedule is defined!')
        raise InvalidEventConfiguration('Schedule is required and no schedule is defined!')

    if rule_name is None:
        LOG.critical('Rule name is required and no rule_name is defined!')
        raise InvalidEventConfiguration('Rule name is required and no rule_name is defined!')
    else:
        LOG.info('%s and %s', app_name, rule_name)
        rule_name = "{}_{}".format(app_name, rule_name.replace(' ', '_'))

    if rule_description is None:
        rule_description = "{} - {}".format(app_name, rule_name)

    lambda_arn = get_lambda_arn(app=app_name, account=env, region=region)

    # Add lambda permissions
    account_id = get_env_credential(env=env)['accountId']
    principal = "events.amazonaws.com"
    statement_id = '{}_cloudwatch_{}'.format(app_name, rule_name)
    source_arn = 'arn:aws:events:{}:{}:rule/{}'.format(region, account_id, rule_name)
    add_lambda_permissions(
        function=lambda_arn,
        statement_id=statement_id,
        action='lambda:InvokeFunction',
        principal=principal,
        source_arn=source_arn,
        env=env,
        region=region, )

    # Create Cloudwatch rule
    cloudwatch_client.put_rule(
        Name=rule_name,
        ScheduleExpression=schedule,
        State='ENABLED',
        Description=rule_description, )

    targets = []
    # TODO: read this one from file event-config-*.json
    json_payload = '{}'.format(json.dumps(json_input))

    target = {
        "Id": app_name,
        "Arn": lambda_arn,
        "Input": json_payload,
    }

    targets.append(target)

    put_targets_response = cloudwatch_client.put_targets(Rule=rule_name, Targets=targets)
    LOG.debug('Cloudwatch put targets response: %s', put_targets_response)

    LOG.info('Created Cloudwatch event "%s" with schedule: %s', rule_name, schedule)
--------------------
Prompt: 
def getAllConfig(self, fmt='json'):
        """
            return all element configurations as json string file.
            could be further processed by beamline.Lattice class

            :param fmt: 'json' (default) or 'dict'
        """
----------
Output: 

        if fmt == 'dict':
            return json.dumps(self.config, indent=4)
        elif fmt == 'list':
            return json.dumps(self.config, indent=4)
        elif fmt == 'beamline':
            return json.dumps(self.beamline, indent=4)
        else:
            raise ValueError("Unknown format: %s" % (fmt))

    def getAllConfigJson(self, fmt='json'):
        """
            return all element configurations as json string file.
            could be further processed by beamline.Lattice class

            :param fmt: 'json' (
----------
Solution: 

        for e in self.getCtrlConf(msgout=False):
            self._lattice_confdict.update(e.dumpConfig(type='simu'))
        self._lattice_confdict.update(self._lattice.dumpConfig())
        if fmt == 'json':
            return json.dumps(self._lattice_confdict)
        else:
            return self._lattice_confdict
--------------------
Prompt: 
def set_headers(self):
        """
        Sets the content headers on the response.
        """
----------
Output: 

        self.headers = self.response.headers
        self.status_code = self.response.status_code
        self.reason = self.response.reason
        self.body = self.response.body
        self.body_length = len(self.body)
        self.body_type = self.response.body_type
        self.body_length = len(self.body)
        self.body_type_length = len(self.body_type)
        self.body_type_type = self.body_type_type
        self.body_length_type = self.
----------
Solution: 

        self.set_header("Accept-Ranges", "bytes")

        content_type = self.get_content_type()
        if content_type:
            self.set_header("Content-Type", content_type)
--------------------
Prompt: 
def iterfd(fd, size=10000000):
    """
    Generator which yields bytes from a file descriptor.

    Args:
        fd (file): A file-like object to read bytes from.
        size (int): Size, in bytes, of the number of bytes to read from the
        fd at a given time.

    Notes:
        If the first read call on the file descriptor is a empty bytestring,
        that zero length bytestring will be yielded and the generator will
        then be exhuasted. This behavior is intended to allow the yielding of
        contents of a zero byte file.

    Yields:
        bytes: Bytes from the file descriptor.
    """
----------
Output: 
: The next `size` bytes from the file.

    Notes:
        If the first read call on the file descriptor is a bytes object,
        that string will be returned instead of the generator.
    """
    while True:
        chunk = fd.read(size)
        if not chunk:
            break
        yield chunk


def _get_file_contents(path):
    """
    Read the contents of a file.

    Args:
        path (str): The path to the file.

    Returns:
        str: The contents of the file.
    """
    with open(path, 'rb') as f:
       
----------
Solution: 

    fd.seek(0)
    byts = fd.read(size)
    # Fast path to yield b''
    if len(byts) == 0:
        yield byts
        return
    while byts:
        yield byts
        byts = fd.read(size)
--------------------
Prompt: 
def register_integration(package_folder):
    """Register a honeycomb integration.

    :param package_folder: Path to folder with integration to load
    :returns: Validated integration object
    :rtype: :func:`honeycomb.utils.defs.Integration`
    """
----------
Output: 

    if not package_folder.endswith(os.path.sep):
        package_folder += os.path.sep
    with open(package_folder, 'r') as f:
        return f.read()


def get_honeycomb_config(package_folder):
    """Get honeycomb configuration.

    :param package_folder: Path to folder with honeycomb configuration to load
    :returns: honeycomb configuration
    :rtype: :func:`honeycomb.utils.defs.HoneycombConfig`
    """
    if not package_folder.endswith(os.path.
----------
Solution: 

    logger.debug("registering integration %s", package_folder)
    package_folder = os.path.realpath(package_folder)
    if not os.path.exists(package_folder):
        raise IntegrationNotFound(os.path.basename(package_folder))

    json_config_path = os.path.join(package_folder, CONFIG_FILE_NAME)
    if not os.path.exists(json_config_path):
        raise ConfigFileNotFound(json_config_path)

    with open(json_config_path, "r") as f:
        config_json = json.load(f)

    # Validate integration and alert config
    validate_config(config_json, defs.INTEGRATION_VALIDATE_CONFIG_FIELDS)
    validate_config_parameters(config_json,
                               defs.INTEGRATION_PARAMETERS_ALLOWED_KEYS,
                               defs.INTEGRATION_PARAMETERS_ALLOWED_TYPES)

    integration_type = _create_integration_object(config_json)

    return integration_type
--------------------
Prompt: 
def initialize_private_canvas(self, private_canvas):
        """Initialize the private canvas used by this instance.
        """
----------
Output: 

        self.private_canvas = private_canvas
        self.canvas = private_canvas.get_tk_widget()
        self.canvas.pack(side='top', fill='both', expand=1)
        self.canvas.create_window(0, 0, window=self.canvas.winfo_toplevel())
        self.canvas.bind('<Configure>', self.on_configure)
        self.canvas.bind('<Map>', self.on_map)
        self.canvas.bind('<Button-4>', self.on_click_event)
        self.canvas.bind('<Button-5>', self
----------
Solution: 

        if self.t_.get('show_pan_position', False):
            self.show_pan_mark(True)

        if self.t_.get('show_focus_indicator', False):
            self.show_focus_indicator(True)
--------------------
Prompt: 
def run_pileup(job, tumor_bam, univ_options, somaticsniper_options):
    """
    Runs a samtools pileup on the tumor bam.

    :param dict tumor_bam: Dict of bam and bai for tumor DNA-Seq
    :param dict univ_options: Dict of universal options used by almost all tools
    :param dict somaticsniper_options: Options specific to SomaticSniper
    :return: fsID for the pileup file
    :rtype: toil.fileStore.FileID
    """
----------
Output: 
.FileID
    """
    # Get the path to the somatic sniper output file
    fs_id = job.fileStore.getLocalTempDir() + "/somaticsniper_output.txt"

    # Get the options specific to SomaticSniper
    if "options" in tumor_bam:
        options = tumor_bam["options"]
        options_dict = {}
        for key in options:
            options_dict[key] = options[key]
        options = tumor_bam["options"]
        options_dict = options_dict.copy()
        options
----------
Solution: 

    work_dir = os.getcwd()
    input_files = {
        'tumor.bam': tumor_bam['tumor_dna_fix_pg_sorted.bam'],
        'tumor.bam.bai': tumor_bam['tumor_dna_fix_pg_sorted.bam.bai'],
        'genome.fa.tar.gz': somaticsniper_options['genome_fasta'],
        'genome.fa.fai.tar.gz': somaticsniper_options['genome_fai']}
    input_files = get_files_from_filestore(job, input_files, work_dir, docker=False)

    for key in ('genome.fa', 'genome.fa.fai'):
        input_files[key] = untargz(input_files[key + '.tar.gz'], work_dir)
    input_files = {key: docker_path(path) for key, path in input_files.items()}

    parameters = ['pileup',
                  '-cvi',
                  '-f', docker_path(input_files['genome.fa']),
                  docker_path(input_files['tumor.bam'])]

    with open(os.path.join(work_dir, 'pileup.txt'), 'w') as pileup_file:
        docker_call(tool='samtools', tool_parameters=parameters, work_dir=work_dir,
                    dockerhub=univ_options['dockerhub'], outfile=pileup_file,
                    tool_version=somaticsniper_options['samtools']['version'])
    outfile = job.fileStore.writeGlobalFile(pileup_file.name)
    job.fileStore.logToMaster('Ran samtools pileup on %s successfully' % univ_options['patient'])
    return outfile
--------------------
Prompt: 
def get_alpha_or_number(number, template):
    """Returns an Alphanumber that represents the number passed in, expressed
    as defined in the template. Otherwise, returns the number
    """
----------
Output: 

    if number is None:
        return None
    if isinstance(number, int):
        return template.get_alpha_or_number(number)
    if isinstance(number, Alphanumber):
        return number
    if isinstance(template, str):
        return template
    if isinstance(template, dict):
        return Alphanumber(number, template)
    if isinstance(template, list):
        return [Alphanumber(number, x) for x in template]
    if isinstance(template, Alphanumber):
        return template
    raise ValueError("Invalid type of template: %s" % type(template))


----------
Solution: 

    match = re.match(r".*\{alpha:(\d+a\d+d)\}$", template.strip())
    if match and match.groups():
        format = match.groups()[0]
        return to_alpha(number, format)
    return number
--------------------
Prompt: 
def filenameToModname(self, filename):
        """Convert a filename to a module name."""
----------
Output: 

        return filename.replace(os.path.sep, '.')

    def moduleNameToFilename(self, moduleName):
        """Convert a module name to a filename."""
        return moduleName.replace('.', os.path.sep) + '.py'

    def modulePathToModuleName(self, modulePath):
        """Convert a module path to a module name."""
        return modulePath.replace(os.path.sep, '.')

    def modulePathToModulePath(self, modulePath):
        """Convert a module path to a filename."""
        return modulePath.replace(os.path.sep, '.')

   
----------
Solution: 

        for ext in reversed(self._exts):
            if filename.endswith(ext):
                filename = filename[:-len(ext)]
                break
        else:
            self.warn(filename, '%s: unknown file name extension', filename)
        filename = os.path.abspath(filename)
        elements = filename.split(os.path.sep)
        modname = []
        while elements:
            modname.append(elements[-1])
            del elements[-1]
            if not os.path.exists(os.path.sep.join(elements + ['__init__.py'])):
                break
        modname.reverse()
        modname = ".".join(modname)
        return modname
--------------------
Prompt: 
def extract_archive(filepath):
    """
    Returns the path of the archive

    :param str filepath: Path to file to extract or read

    :return: path of the archive

    :rtype: str
    """
----------
Output: 

    with open(filepath, 'rb') as f:
        return f.read()


def extract_file_from_stream(stream, filepath):
    """
    Extracts a file from a stream

    :param stream: Stream to extract from
    :param str filepath: Path to file to extract from

    :return: path of the file

    :rtype: str
    """
    with open(filepath, 'rb') as f:
        return stream.extractfile(f, encoding='binary', errors='replace')


def extract_path(filepath):
    """
    Returns the path of the file

    :
----------
Solution: 


    # Checks if file path is a directory
    if os.path.isdir(filepath):
        path = os.path.abspath(filepath)
        print("Archive already extracted. Viewing from {}...".format(path))
        return path

    # Checks if the filepath is a zipfile and continues to extract if it is
    # if not it raises an error
    elif not zipfile.is_zipfile(filepath):
        # Misuse of TypeError? :P
        raise TypeError("{} is not a zipfile".format(filepath))

    archive_sha = SHA1_file(
        filepath=filepath,
        # Add version of slackviewer to hash as well so we can invalidate the cached copy
        #  if there are new features added
        extra=to_bytes(slackviewer.__version__)
    )

    extracted_path = os.path.join(SLACKVIEWER_TEMP_PATH, archive_sha)

    if os.path.exists(extracted_path):
        print("{} already exists".format(extracted_path))
    else:
        # Extract zip
        with zipfile.ZipFile(filepath) as zip:
            print("{} extracting to {}...".format(filepath, extracted_path))
            zip.extractall(path=extracted_path)

        print("{} extracted to {}".format(filepath, extracted_path))

        # Add additional file with archive info
        create_archive_info(filepath, extracted_path, archive_sha)

    return extracted_path
--------------------
Prompt: 
def addModel(D, models):
    """
    Insert model data into a LiPD dataset

    Examples of model naming:
    chron0model0
    chron0model1
    chron1model0

    Example of 'models' variable:
    {
       chron0model0: {
                        "method": {...},
                        "summaryTable": [...],
                        "ensembleTable": [...],
                        "distributionTable: [...]
                    },
       chron0model1:...
    }

    :param dict D: Metadata (dataset)
    :param dict models: Model data to add
    :return dict D: Metadata (dataset)
    """
----------
Output: 
: Metadata (dataset)
    """
    # Add the model data to the dataset
    D = addMetadata(D, models)

    # Add the dataset to the dataset
    return D

----------
Solution: 

    try:
        # Loop for each model that needs to  be added
        for _model_name, _model_data in models.items():
            # split the table name into a path that we can use
            _m = re.match(re_model_name, _model_name)
            if _m:
                D = _put_model(D, _model_name, _model_data, _m)
            else:
                print("The table name found in the given model data isn't valid for automatic placement")
                _placement_name = _prompt_placement(D, "model")
                _m = re.match(re_model_name, _placement_name)
                if _m:
                    D = _put_model(D, _placement_name, _model_data, _m)
                else:
                    print("Oops. This shouldn't happen. That table name doesn't look right. Please report this error")
                    return

    except Exception as e:
        print("addModel: Model data NOT added, {}".format(e))

    return D
--------------------
Prompt: 
def publish(self, topic, message, subject=None):
        """
        Get properties of a Topic

        :type topic: string
        :param topic: The ARN of the new topic.

        :type message: string
        :param message: The message you want to send to the topic.
                        Messages must be UTF-8 encoded strings and
                        be at most 4KB in size.

        :type subject: string
        :param subject: Optional parameter to be used as the "Subject"
                        line of the email notifications.

        """
----------
Output: 

        return self._client.publish(topic, message)

    def send(self, to, subject=None, content=None, html=None):
        """
        Send an email to a recipient.

        :type to: string
        :param to: The recipient's email address.

        :type subject: string
        :param subject: Optional parameter to be used as the "Subject"
                        line of the email notifications.

        :type content: string
        :param content: Optional parameter to be used as the "Content-Type"
                        header of the email notifications.

        :rtype: bool
        :return: True if
----------
Solution: 

        params = {'ContentType' : 'JSON',
                  'TopicArn' : topic,
                  'Message' : message}
        if subject:
            params['Subject'] = subject
        response = self.make_request('Publish', params, '/', 'GET')
        body = response.read()
        if response.status == 200:
            return json.loads(body)
        else:
            boto.log.error('%s %s' % (response.status, response.reason))
            boto.log.error('%s' % body)
            raise self.ResponseError(response.status, response.reason, body)
--------------------
Prompt: 
def schema_get(name, profile=None):
    """
    Known valid names of schemas are:
      - image
      - images
      - member
      - members

    CLI Example:

    .. code-block:: bash

        salt '*' glance.schema_get name=f16-jeos
    """
----------
Output: 

    if profile is not None:
        return __salt__['cmd.run']('image', name, '--profile', profile)
    else:
        return __salt__['cmd.run']('image', name)


def schema_list():
    """
    List all available schemas.

    CLI Example:

   .. code-block:: bash

        salt '*' glance.schema_list
    """
    return __salt__['cmd.run']('image','schema')


def schema_info(name):
    """
    Known valid schemas are:
      - image
      - images
      - member
      - members

    CLI Example:

   
----------
Solution: 

    g_client = _auth(profile)
    schema_props = {}
    for prop in g_client.schemas.get(name).properties:
        schema_props[prop.name] = prop.description
    log.debug(
        'Properties of schema %s:\n%s',
        name, pprint.PrettyPrinter(indent=4).pformat(schema_props)
    )
    return {name: schema_props}
--------------------
Prompt: 
def _compute_curve(labels, predictions, num_thresholds, weights=None):
    """This function is another implementation of functions in
    https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/pr_curve/summary.py"""
----------
Output: 

    if weights is None:
        weights = tf.ones_like(labels)
    else:
        weights = tf.constant(weights, dtype=tf.float32)
        for i in range(num_thresholds):
            weights[i] = tf.reduce_sum(
                tf.multiply(predictions[i], weights[i]),
                axis=1,
                keep_dims=True)
    return tf.reduce_sum(weights, axis=1, keep_dims=True)


def _compute_auc(labels, predictions, num_thresholds, weights=None):
    """This function is
----------
Solution: 


    if weights is None:
        weights = 1.0

    # Compute bins of true positives and false positives.
    bucket_indices = np.int32(np.floor(predictions * (num_thresholds - 1)))
    float_labels = labels.astype(np.float)
    histogram_range = (0, num_thresholds - 1)
    tp_buckets, _ = np.histogram(
        bucket_indices,
        bins=num_thresholds,
        range=histogram_range,
        weights=float_labels * weights)
    fp_buckets, _ = np.histogram(
        bucket_indices,
        bins=num_thresholds,
        range=histogram_range,
        weights=(1.0 - float_labels) * weights)

    # Obtain the reverse cumulative sum.
    tp = np.cumsum(tp_buckets[::-1])[::-1]
    fp = np.cumsum(fp_buckets[::-1])[::-1]
    tn = fp[0] - fp
    fn = tp[0] - tp
    precision = tp / np.maximum(_MINIMUM_COUNT, tp + fp)
    recall = tp / np.maximum(_MINIMUM_COUNT, tp + fn)
    return np.stack((tp, fp, tn, fn, precision, recall))
--------------------
Prompt: 
def get_block_header(self, block_hash, id=None, endpoint=None):
        """
        Get the corresponding block header information according to the specified script hash.
        Args:
            block_hash: (str) the block scripthash (e.g. 'a5508c9b6ed0fc09a531a62bc0b3efcb6b8a9250abaf72ab8e9591294c1f6957')
            id: (int, optional) id to use for response tracking
            endpoint: (RPCEndpoint, optional) endpoint to specify to use

        Returns:
            json object of the result or the error encountered in the RPC call
        """
----------
Output: 
 the script hash to use for response tracking
        Returns:
            (dict) the block header information
        """
        return self._get_block_header(block_hash, id, endpoint)

    def _get_block_header(self, block_hash, id=None, endpoint=None):
        """
        Get the corresponding block header information according to the specified script hash.
        Args:
            block_hash: (str) the block scripthash (e.g. 'a5508c9b6ed0fc09a531a62bc0b3efcb6b8a9
----------
Solution: 

        return self._call_endpoint(GET_BLOCK_HEADER, params=[block_hash, 1], id=id, endpoint=endpoint)
--------------------
Prompt: 
def BuscarCertConSaldoDisponible(self, cuit_depositante=None,
                        cod_grano=2, campania=1314, coe=None, 
                        fecha_emision_des=None,
                        fecha_emision_has=None,
                 ):
        """Devuelve los certificados de depósito en los que un productor tiene
        saldo disponible para Liquidar/Retirar/Transferir"""
----------
Output: 

        if cuit_depositante is None:
            cuit_depositante = self.cuit
        if coe is None:
            coe = self.coe
        if fecha_emision_des is None:
            fecha_emision_des = self.fecha_emision_des
        if fecha_emision_has is None:
            fecha_has = self.fecha_has
        if cod_depositante is None:
            cod_depositante = self.cod_depositante
        if coe is None
----------
Solution: 

        
        ret = self.client.cgBuscarCertConSaldoDisponible(
                    auth={
                        'token': self.Token, 'sign': self.Sign,
                        'cuit': self.Cuit, },
                    cuitDepositante=cuit_depositante or self.Cuit,
                    codGrano=cod_grano, campania=campania,
                    coe=coe,
                    fechaEmisionDes=fecha_emision_des,
                    fechaEmisionHas=fecha_emision_has,
                        )['oReturn']
        self.__analizar_errores(ret)
        array = ret.get('certificado', [])
        self.Excepcion = self.Traceback = ""
        self.params_out['certificados'] = []
        for cert in array:
            self.params_out['certificados'].append(dict(
                coe=cert['coe'],
                tipo_certificado=cert['tipoCertificado'],
                campania=cert['campania'],
                cuit_depositante=cert['cuitDepositante'],
                cuit_depositario=cert['cuitDepositario'],
                nro_planta=cert['nroPlanta'],
                kilos_disponibles=cert['kilosDisponibles'],
                cod_grano=cert['codGrano'],
            ))
        return True
--------------------
Prompt: 
def getIntervals(fn, tierName, filterFunc=None,
                 includeUnlabeledRegions=False):
    """
    Get information about the 'extract' tier, used by several merge scripts
    """
----------
Output: 

    if filterFunc is None:
        filterFunc = lambda x: True
    if includeUnlabeledRegions:
        filterFunc = lambda x: False
    return _getIntervals(fn, tierName, filterFunc, False, False)


def _getIntervals(fn, tierName, func, includeUnlabeledRegions=False):
    """
    Get information about the 'extract' tier, used by several merge scripts
    """
    if includeUnlabeledRegions:
        filterFunc = lambda x: False
    if tierName is None:
        return [True]
    if tierName in self.get
----------
Solution: 


    tg = tgio.openTextgrid(fn)
    
    tier = tg.tierDict[tierName]
    if includeUnlabeledRegions is True:
        tier = tgio._fillInBlanks(tier)

    entryList = tier.entryList
    if filterFunc is not None:
        entryList = [entry for entry in entryList if filterFunc(entry)]

    return entryList
--------------------
Prompt: 
def get_plugin_actions(self):
        """Return a list of actions related to plugin."""
----------
Output: 

        return []

    def get_plugin_options(self, plugin, plugin_type):
        """Return a list of options related to a plugin."""
        return []

    def get_plugin_options_list(self, plugin):
        """Return a list of options available in a plugin."""
        return []

    def get_plugin_options_dict(self, plugin):
        """Return a dict of options available in a plugin."""
        return {}

    def get_plugin_options_help(self, plugin):
        """Return a help text for the given plugin."""
        return help_text

    def get_plugin_
----------
Solution: 

        create_client_action = create_action(
                                   self,
                                   _("New console (default settings)"),
                                   icon=ima.icon('ipython_console'),
                                   triggered=self.create_new_client,
                                   context=Qt.WidgetWithChildrenShortcut)
        self.register_shortcut(create_client_action, context="ipython_console",
                               name="New tab")

        create_pylab_action = create_action(
                                   self,
                                   _("New Pylab console (data plotting)"),
                                   icon=ima.icon('ipython_console'),
                                   triggered=self.create_pylab_client,
                                   context=Qt.WidgetWithChildrenShortcut)

        create_sympy_action = create_action(
                                   self,
                                   _("New SymPy console (symbolic math)"),
                                   icon=ima.icon('ipython_console'),
                                   triggered=self.create_sympy_client,
                                   context=Qt.WidgetWithChildrenShortcut)

        create_cython_action = create_action(
                                   self,
                                   _("New Cython console (Python with "
                                     "C extensions)"),
                                   icon=ima.icon('ipython_console'),
                                   triggered=self.create_cython_client,
                                   context=Qt.WidgetWithChildrenShortcut)
        special_console_action_group = QActionGroup(self)
        special_console_actions = (create_pylab_action, create_sympy_action,
                                   create_cython_action)
        add_actions(special_console_action_group, special_console_actions)
        special_console_menu = QMenu(_("New special console"), self)
        add_actions(special_console_menu, special_console_actions)

        restart_action = create_action(self, _("Restart kernel"),
                                       icon=ima.icon('restart'),
                                       triggered=self.restart_kernel,
                                       context=Qt.WidgetWithChildrenShortcut)

        reset_action = create_action(self, _("Remove all variables"),
                                     icon=ima.icon('editdelete'),
                                     triggered=self.reset_kernel,
                                     context=Qt.WidgetWithChildrenShortcut)

        if self.interrupt_action is None:
            self.interrupt_action = create_action(
                self, _("Interrupt kernel"),
                icon=ima.icon('stop'),
                triggered=self.interrupt_kernel,
                context=Qt.WidgetWithChildrenShortcut)

        self.register_shortcut(restart_action, context="ipython_console",
                               name="Restart kernel")

        connect_to_kernel_action = create_action(self,
               _("Connect to an existing kernel"), None, None,
               _("Open a new IPython console connected to an existing kernel"),
               triggered=self.create_client_for_kernel)
        
        rename_tab_action = create_action(self, _("Rename tab"),
                                       icon=ima.icon('rename'),
                                       triggered=self.tab_name_editor)
        
        # Add the action to the 'Consoles' menu on the main window
        main_consoles_menu = self.main.consoles_menu_actions
        main_consoles_menu.insert(0, create_client_action)
        main_consoles_menu += [special_console_menu, connect_to_kernel_action,
                               MENU_SEPARATOR,
                               self.interrupt_action, restart_action,
                               reset_action]

        # Plugin actions
        self.menu_actions = [create_client_action, special_console_menu,
                             connect_to_kernel_action,
                             MENU_SEPARATOR,
                             self.interrupt_action,
                             restart_action, reset_action, rename_tab_action]

        self.update_execution_state_kernel()

        # Check for a current client. Since it manages more actions.
        client = self.get_current_client()
        if client:
            return client.get_options_menu()
        return self.menu_actions
--------------------
Prompt: 
def from_birth_date(birth_date):
        """Take a person's birth date (datetime.date) and return a new DOB 
        object suitable for him."""
----------
Output: 

        if not isinstance(birth_date, datetime.date):
            raise TypeError("Expected a date object, got %s" % type(birth_date))
        if birth_date.year < 1900:
            raise ValueError("Expected a date object, got %s" % birth_date)
        if birth_date.month < 1:
            raise ValueError("Expected a date object, got %s" % birth_date)
        if birth_date.day < 1:
            raise ValueError("Expected a date object, got %s" % birth_date)
        if birth_date.day > 1:
----------
Solution: 

        if birth_date > datetime.date.today():
            raise ValueError('birth_date can\'t be in the future')
        date_range = DateRange(birth_date, birth_date)
        return DOB(date_range=date_range)
--------------------
Prompt: 
def ISBNValidator(raw_isbn):
    """ Check string is a valid ISBN number"""
----------
Output: 

    try:
        int(raw_isbn)
    except ValueError:
        return False
    else:
        return True


def ISBNValidator_ISBN_valid(isbn):
    """ Check ISBN is a valid ISBN number"""
    try:
        int(isbn)
    except ValueError:
        return False
    else:
        return True


def ISBNValidator_ISBN_invalid(isbn):
    """ Check ISBN is a valid ISBN number"""
    try:
        int(isbn)
    except ValueError:
        return False
    else:
        return True


def ISBNValidator
----------
Solution: 

    isbn_to_check = raw_isbn.replace('-', '').replace(' ', '')

    if not isinstance(isbn_to_check, string_types):
        raise ValidationError(_(u'Invalid ISBN: Not a string'))

    if len(isbn_to_check) != 10 and len(isbn_to_check) != 13:
        raise ValidationError(_(u'Invalid ISBN: Wrong length'))
    
    if not isbn.is_valid(isbn_to_check):
        raise ValidationError(_(u'Invalid ISBN: Failed checksum'))

    if isbn_to_check != isbn_to_check.upper():
        raise ValidationError(_(u'Invalid ISBN: Only upper case allowed'))

    return True
--------------------
Prompt: 
def get_unique_name(name='', collection=()):
    """
    Generate a unique name (str type) by appending a sequence number to
    the original name so that it is not contained in the collection.
    ``collection`` has a __contains__ method (tuple, list, dict, etc.)
    """
----------
Output: 

    if not name:
        return ''
    if isinstance(name, str):
        name = (name,)
    seen = set()
    for i in xrange(len(collection)):
        if name[i] not in seen:
            seen.add(name[i])
            name[i] = '%s_%s' % (name[i], i)
    return '/'.join(collection)


def get_unique_name_sequence(name_sequence, collection=()):
    """
    Generate a unique name (str type) by appending a sequence number to
    the original name so that it is not contained in the
----------
Solution: 

    if name not in collection:
        return name
    i = 1
    while True:
        i += 1
        name2 = '%s_%s' % (name, i)
        if name2 not in collection:
            return name2
--------------------
Prompt: 
def viewkeys(obj, **kwargs):
    """
    Function for iterating over dictionary keys with the same set-like
    behaviour on Py2.7 as on Py3.

    Passes kwargs to method."""
----------
Output: 

    for k in obj:
        if not k.startswith('__') and k.endswith('__'):
            k = k[1:-1]
            yield k, k[1:-1], k[1:-1]


def viewvalues(obj, **kwargs):
    """
    Function for iterating over dictionary values with the same set-like
    behaviour on Py2.7.

    Passes kwargs to method."""
    for v in viewkeys(obj, **kwargs):
        if not v.startswith('__') and v.endswith('__'):
            v = v[1:-1]
            yield v, v[1
----------
Solution: 

    func = getattr(obj, "viewkeys", None)
    if not func:
        func = obj.keys
    return func(**kwargs)
--------------------
Prompt: 
def fit(self, X, y=None):
        """
        X : ANTsImage | string | list of ANTsImage types | list of strings
            images to register to fixed image

        y : string | list of strings
            labels for images
        """
----------
Output: 

        if y is None:
            if isinstance(X, list):
                self.fixed_labels = [str(i) for i in range(len(X[0]))]
            else:
                self.fixed_labels = [str(i) for i in range(len(X))]
        else:
            self.fixed_labels = [str(i) for i in range(len(X))]
        self.fixed_labels = [self.fixed_labels[i] for i in self.fixed_labels]
        self.labels = self.fixed_labels
        self.fixed_labels =
----------
Solution: 

        moving_images = X if isinstance(X, (list,tuple)) else [X]
        moving_labels = y if y is not None else [i for i in range(len(moving_images))]
        fixed_image = self.fixed_image

        self.fwdtransforms_ = {}
        self.invtransforms_ = {}
        self.warpedmovout_ = {}
        self.warpedfixout_ = {}
        
        for moving_image, moving_label in zip(moving_images, moving_labels):
            fit_result = interface.registration(fixed_image,
                                                moving_image,
                                                type_of_transform=self.type_of_transform,
                                                initial_transform=None,
                                                outprefix='',
                                                mask=None,
                                                grad_step=0.2,
                                                flow_sigma=3,
                                                total_sigma=0,
                                                aff_metric='mattes',
                                                aff_sampling=32,
                                                syn_metric='mattes',
                                                syn_sampling=32,
                                                reg_iterations=(40,20,0),
                                                verbose=False)

            self.fwdtransforms_[moving_label] = fit_result['fwdtransforms']
            self.invtransforms_[moving_label] = fit_result['invtransforms']
            self.warpedmovout_[moving_label]  = fit_result['warpedmovout']
            self.warpedfixout_[moving_label]  = fit_result['warpedfixout']

        return self
--------------------
Prompt: 
def get_one(self, section, key):
        """Retrieve the first value for a section/key.

        Raises:
            KeyError: If no line match the given section/key.
        """
----------
Output: 

        for line in self.lines:
            if line.key == key:
                return line.value
        raise KeyError(key)

    def get_all(self, section):
        """Retrieve all values for a section/key.

        Args:
            section: The section to retrieve values for.

        Returns:
            A list of values matching the given section/key.
        """
        return [self.get_one(section, i) for i in range(len(self.lines) - 1)]

    def get_all_until(self, section, key):
        """Retrieve all values until a section/
----------
Solution: 

        lines = iter(self.get(section, key))
        try:
            return next(lines)
        except StopIteration:
            raise KeyError("Key %s not found in %s" % (key, section))
--------------------
Prompt: 
def compute_attention_component(antecedent,
                                total_depth,
                                filter_width=1,
                                padding="VALID",
                                name="c",
                                vars_3d_num_heads=0,
                                layer_collection=None):
  """Computes attention compoenent (query, key or value).

  Args:
    antecedent: a Tensor with shape [batch, length, channels]
    total_depth: an integer
    filter_width: An integer specifying how wide you want the attention
      component to be.
    padding: One of "VALID", "SAME" or "LEFT". Default is VALID: No padding.
    name: a string specifying scope name.
    vars_3d_num_heads: an optional integer (if we want to use 3d variables)
    layer_collection: A tensorflow_kfac.LayerCollection. Only used by the
      KFAC optimizer. Default is None.

  Returns:
    c : [batch, length, depth] tensor
  """
----------
Output: 
 is "LEFT".
    name: string, name scope for ops managed by this function.
    vars_3d_num_heads: an integer specifying how many heads you want to use for
      3D attention. Default is 0.
    layer_collection: string, name of the layer collection to use.

  Returns:
    antecedent: a Tensor with shape [batch, length, channels]
    - query: a Tensor with shape [batch, length, filters]
    - key: a Tensor with shape [batch, length, key]
    - value: a Tensor with shape [batch, length, value]
----------
Solution: 

  if layer_collection is not None:
    if filter_width != 1 or vars_3d_num_heads != 0:
      raise ValueError(
          "KFAC implementation only supports filter_width=1 (actual: {}) and "
          "vars_3d_num_heads=0 (actual: {}).".format(
              filter_width, vars_3d_num_heads))
  if vars_3d_num_heads > 0:
    assert filter_width == 1
    input_depth = antecedent.get_shape().as_list()[-1]
    depth_per_head = total_depth // vars_3d_num_heads
    initializer_stddev = input_depth ** -0.5
    if "q" in name:
      initializer_stddev *= depth_per_head ** -0.5
    var = tf.get_variable(
        name, [input_depth,
               vars_3d_num_heads,
               total_depth // vars_3d_num_heads],
        initializer=tf.random_normal_initializer(stddev=initializer_stddev))
    var = tf.cast(var, antecedent.dtype)
    var = tf.reshape(var, [input_depth, total_depth])
    return tf.tensordot(antecedent, var, axes=1)
  if filter_width == 1:
    return common_layers.dense(
        antecedent, total_depth, use_bias=False, name=name,
        layer_collection=layer_collection)
  else:
    return common_layers.conv1d(
        antecedent, total_depth, filter_width, padding=padding, name=name)
--------------------
Prompt: 
def _evaluate(self,R,phi=0.,t=0.):
        """
        NAME:
           _evaluate
        PURPOSE:
           evaluate the potential at R,phi,t
        INPUT:
           R - Galactocentric cylindrical radius
           phi - azimuth
           t - time
        OUTPUT:
           Phi(R,phi,t)
        HISTORY:
           2011-03-27 - Started - Bovy (NYU)
        """
----------
Output: 

        return self._evaluate_scalar(R,phi,t)

    def _evaluate_scalar(self,R,phi,t):
        """
        NAME:
           _evaluate_scalar
        PURPOSE:
           evaluate the potential at R,phi,t
        INPUT:
           R - Galactocentric cylindrical radius
           phi - azimuth
           t - time
        OUTPUT:
           Phi(R,phi,t)
        HISTORY:
           2011-03-27 - Bovy - Added optional argument 't' to specify the time at which the potential is evaluated
        """
        if isinstance(t
----------
Solution: 

        if not self._tform is None:
            if t < self._tform:
                smooth= 0.
            elif t < self._tsteady:
                deltat= t-self._tform
                xi= 2.*deltat/(self._tsteady-self._tform)-1.
                smooth= (3./16.*xi**5.-5./8*xi**3.+15./16.*xi+.5)
            else: #spiral is fully on
                smooth= 1.
        else:
            smooth= 1.
        return smooth*self._A/self._alpha*math.cos(self._alpha*math.log(R)
                                                   -self._m*(phi-self._omegas*t
                                                             -self._gamma))
--------------------
Prompt: 
def system_resolve_data_objects(input_params={}, always_retry=True, **kwargs):
    """
    Invokes the /system/resolveDataObjects API method.

    For more info, see: https://wiki.dnanexus.com/API-Specification-v1.0.0/System-Methods#API-method:-/system/resolveDataObjects
    """
----------
Output: 

    if not input_params:
        input_params = {}
    if not always_retry:
        kwargs['always_retry'] = True
    return self._system_resolve_with_http_info('get', '/system/resolveDataObjects', input_params, **kwargs)

  def system_get_log(self, **kwargs):
    """
    Retrieves the logs of the current user.

    Args:
      kwargs: Other parameters to pass to the /system/log API method.
    Returns:
      A dict respresentation of the logs.
    """
    return self._system_with_http_info('
----------
Solution: 

    return DXHTTPRequest('/system/resolveDataObjects', input_params, always_retry=always_retry, **kwargs)
--------------------
Prompt: 
def google_storage(self, scene, path):
        """
        Google Storage Downloader.

        :param scene:
            The scene id
        :type scene:
            String
        :param path:
            The directory path to where the image should be stored
        :type path:
            String

        :returns:
            Boolean
        """
----------
Output: 

        if not self.is_alive():
            return False

        if not path.endswith('/'):
            path += '/'

        if not path.startswith(self.base_path):
            path = self.base_path + path

        if not path.startswith(self.base_path):
            path = self.base_path + path

        if not path.endswith(self.image_ext):
            path += self.image_ext

        if not path.startswith(self.image_path):
            path = self.image_path + path

        if not path.startswith(self.image_path):
            path
----------
Solution: 


        sat = self.scene_interpreter(scene)
        url = self.google_storage_url(sat)

        self.remote_file_exists(url)

        self.output('Source: Google Storage', normal=True, arrow=True)
        return self.fetch(url, path)
--------------------
Prompt: 
def receive(socket, num_bytes=20):
    """Receive data frame from open socket.

    :param socket: open socket instance
    :param num_bytes: number of bytes to read

    :return: received data
    """
----------
Output: 

    data = ''
    while len(data) < num_bytes:
        data += sock.recv(num_bytes - len(data))
        if not data:
            break

    return data


def send_data(socket, data):
    """Send data to open socket.

    :param socket: open socket instance
    :param data: data to send

    :return: None
    """
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.send(data)
    sock.close()


def send_data_with_retries(socket, data):
----------
Solution: 

    length = int(_receive_all(socket, num_bytes).decode())
    serialized_data = _receive_all(socket, length)
    return pickle.loads(serialized_data)
--------------------
Prompt: 
def get(self, name, failobj=None):
        """Get a header value.

        Like __getitem__() but return failobj instead of None when the field
        is missing.
        """
----------
Output: 

        return self._headers.get(name)

    def get_all(self):
        """Get all header values.

        :return: A list of all header values.
        """
        return self._headers.values()

    def get_all_if_match(self, name, failobj=None):
        """Get all header values if a header value is matched.

        :return: A list of all header values if a header value is matched,
            else None.
        """
        return get(self, name, failobj).get_all_if_match(name)

    def get_
----------
Solution: 

        name = name.lower()
        for k, v in self._headers:
            if k.lower() == name:
                return self.policy.header_fetch_parse(k, v)
        return failobj
--------------------
Prompt: 
def XYZ_to_lbd(X,Y,Z,degree=False):
    """
    NAME:

       XYZ_to_lbd

    PURPOSE:

       transform from rectangular Galactic coordinates to spherical Galactic coordinates (works with vector inputs)

    INPUT:

       X - component towards the Galactic Center (in kpc; though this obviously does not matter))

       Y - component in the direction of Galactic rotation (in kpc)

       Z - component towards the North Galactic Pole (kpc)

       degree - (Bool) if True, return l and b in degrees

    OUTPUT:

       [l,b,d] in (rad or degree,rad or degree,kpc)

       For vector inputs [:,3]

    HISTORY:

       2009-10-24 - Written - Bovy (NYU)

       2014-06-14 - Re-written w/ numpy functions for speed and w/ decorators for beauty - Bovy (IAS)

    """
----------
Output: 
bd coordinates (in degrees, otherwise)

    OUTPUT:

       lbd coordinates (in degrees, otherwise)
    """
    if degree:
        return np.degrees(X)
    else:
        return np.radians(X)

def lbd_to_xyz(lbd, degree=False):
    """
    NAME:

       lbd_to_xyz

    PURPOSE:

       transform from rectangular Galactic coordinates to spherical Galactic pole (works with vector inputs)

    INPUT:

       lbd - (Bool) if True, return the spherical Galactic pole (in kpc;
----------
Solution: 

    #Whether to use degrees and scalar input is handled by decorators
    d= nu.sqrt(X**2.+Y**2.+Z**2.)
    b=nu.arcsin(Z/d)
    cosl= X/d/nu.cos(b)
    sinl= Y/d/nu.cos(b)
    l= nu.arcsin(sinl)
    l[cosl < 0.]= nu.pi-l[cosl < 0.]
    l[(cosl >= 0.)*(sinl < 0.)]+= 2.*nu.pi
    out= nu.empty((len(l),3))
    out[:,0]= l
    out[:,1]= b
    out[:,2]= d
    return out
--------------------
Prompt: 
def screenshot(self, png_filename=None, format='raw'):
        """
        Screenshot with PNG format

        Args:
            png_filename(string): optional, save file name
            format(string): return format, pillow or raw(default)
        Returns:
            raw data or PIL.Image
        
        Raises:
            WDAError
        """
----------
Output: 

        if not png_filename:
            png_filename = self.get_png_filename()
        if not format:
            format = 'png'
        if format == 'raw':
            return self.raw_screenshot()
        elif format == 'pillow':
            return self.pillow_screenshot()
        else:
            raise WDAError("Unknown screenshot format: %s" % format)

    def save_screenshot(self, filename=None):
        """
        Save screenshot to file

        Args:
            filename(string): optional, save file name
        Returns:
            None
        
        Raises:
            WDAError
----------
Solution: 

        value = self.http.get('screenshot').value
        raw_value = base64.b64decode(value)
        png_header = b"\x89PNG\r\n\x1a\n"
        if not raw_value.startswith(png_header) and png_filename:
            raise WDAError(-1, "screenshot png format error")

        if png_filename:
            with open(png_filename, 'wb') as f:
                f.write(raw_value)

        if format == 'raw':
            return raw_value
        elif format == 'pillow':
            from PIL import Image
            buff = io.BytesIO(raw_value)
            return Image.open(buff)
        else:
            raise ValueError("unknown format")
--------------------
Prompt: 
def lin_interp(x, rangeX, rangeY):
    """
    Interpolate linearly variable x in rangeX onto rangeY.
    """
----------
Output: 

    return np.interp(x, rangeX, rangeY)

def lin_interp_2d(x, rangeX, rangeY):
    """
    Interpolate linearly variable x in rangeX onto rangeY.
    """
    return np.interp(x, rangeX, rangeY)

def lin_interp_3d(x, rangeX, rangeY):
    """
    Interpolate linearly variable x in rangeX onto rangeY.
    """
    return np.interp(x, rangeX, rangeY)

def lin_interp_4d(x, rangeX
----------
Solution: 

    s = (x - rangeX[0]) / mag(rangeX[1] - rangeX[0])
    y = rangeY[0] * (1 - s) + rangeY[1] * s
    return y
--------------------
Prompt: 
def _onCompletionListItemSelected(self, index):
        """Item selected. Insert completion to editor
        """
----------
Output: 

        self.editor.insert(index, self.completion)
        self.editor.setFocus()

    def _onCompletionListDoubleClicked(self, index):
        """Double click on a list of completions
        """
        self.editor.selectAll()

    def _onCompletionListContextMenu(self, index):
        """Context menu. Insert completion to editor
        """
        self.editor.insert(index, self.completion)
        self.editor.setFocus()

    def _onCompletionListContextMenuTriggered(self, index):
        """Context menu. Insert a context menu item
        """
        self.
----------
Solution: 

        model = self._widget.model()
        selectedWord = model.words[index]
        textToInsert = selectedWord[len(model.typedText()):]
        self._qpart.textCursor().insertText(textToInsert)
        self._closeCompletion()
--------------------
Prompt: 
def SetCoreGRRKnowledgeBaseValues(kb, client_obj):
  """Set core values from GRR into the knowledgebase."""
----------
Output: 

  if not kb.IsValid(constants.IP4_ADDRESS):
    raise errors.OpPrereqError("Invalid IP address '%s'" % constants.IP4_ADDRESS,
                               errors.ECODE_INVAL)
  client_obj.Set(constants.IP4_ADDRESS,
                compat.any_from_bytes(self.cfg.GetMasterNode(), "master_candidate_config",
                                     constants.DEFAULT_NODE_GROUP_NAME))
  client_obj.Set(constants.IP4_VERSION,
                compat.any_from_str(self.cfg.GetVersion(), cl=
----------
Solution: 

  client_schema = client_obj.Schema
  kb.fqdn = utils.SmartUnicode(client_obj.Get(client_schema.FQDN, ""))
  if not kb.fqdn:
    kb.fqdn = utils.SmartUnicode(client_obj.Get(client_schema.HOSTNAME, ""))
  versions = client_obj.Get(client_schema.OS_VERSION)
  if versions and versions.versions:
    try:
      kb.os_major_version = versions.versions[0]
      kb.os_minor_version = versions.versions[1]
    except IndexError:
      # Some OSs don't have a minor version.
      pass
  client_os = client_obj.Get(client_schema.SYSTEM)
  if client_os:
    kb.os = utils.SmartUnicode(client_obj.Get(client_schema.SYSTEM))
--------------------
Prompt: 
def append_pair(self, tag, value, header=False):
        """Append a tag=value pair to this message.

        :param tag: Integer or string FIX tag number.
        :param value: FIX tag value.
        :param header: Append to header if True; default to body.

        Both parameters are explicitly converted to strings before
        storage, so it's ok to pass integers if that's easier for
        your program logic.

        Note: a Python 'None' value will be silently ignored, and
        no field is appended."""
----------
Output: 

        if header:
            self.body.append(tag)
            self.header_written = True
        else:
            self.body.append(value)
            self.header_written = False

    def get_header(self):
        """Return the header message as a string.

        :rtype: String
        :returns: Header message."""
        if self.header_written:
            return self.header_message.encode('ascii')
        else:
            return None

    def get_body(self):
        """Return the body of this message.

        :rtype: List of String
        :returns: List of String
----------
Solution: 


        if tag is None or value is None:
            return

        if int(tag) == 8:
            self.begin_string = fix_val(value)

        if int(tag) == 35:
            self.message_type = fix_val(value)

        if header:
            self.pairs.insert(self.header_index,
                              (fix_tag(tag),
                               fix_val(value)))
            self.header_index += 1
        else:
            self.pairs.append((fix_tag(tag), fix_val(value)))
        return
--------------------
Prompt: 
def _get_and_set_force_cache_miss(request):
        """
        Gets value for request query parameter FORCE_CACHE_MISS
        and sets it in the default request cache.

        This functionality is only available for staff.

        Example:
            http://clobert.com/api/v1/resource?force_cache_miss=true

        """
----------
Output: 

        self.default_request_cache.append(request.GET.get('force_cache_miss', False))
        return self.default_request_cache

    def _get_and_set_cache_hit(self, request):
        """
        Gets value for request query parameter FORCE_CACHE_HIT
        and sets it in the default request cache.

        This functionality is only available for staff.

        Example:
            http://clobert.com/api/v1/resource?hit=true

        """
        self.default_request_cache.append(request.GET.get('hit',
----------
Solution: 

        if not (request.user and request.user.is_active and request.user.is_staff):
            force_cache_miss = False
        else:
            force_cache_miss = request.GET.get(FORCE_CACHE_MISS_PARAM, 'false').lower() == 'true'
        DEFAULT_REQUEST_CACHE.set(SHOULD_FORCE_CACHE_MISS_KEY, force_cache_miss)
--------------------
Prompt: 
def umap(path, name=None, include=None, namespace=None, priority=None):
    """
        Maps a given URL path, name and namespace to a view.
        Arguments:
            - path: the URL regex, e.g.: '^teste/(?P<pk>[0-9])/$'.

        Optional arguments:
            - name: the URL name, which Django uses to identify the URL;
            - include: A custom URL list, previously
                       set on the module's urls.py;
            - namespace: the URL's namespace;
            - priority: the URL's priority;
    """
----------
Output: 

    if namespace is None:
        namespace = getattr(settings, 'URL_NAMESPACE', None)
    if name is None:
        name = getattr(settings, 'URL_NAME', None)
    if include is None:
        include = getattr(settings, 'URL_INCLUDE', None)
    if not name:
        return None
    if path.startswith('django/'):
        return reverse('django_view_with_app_lookup', args=[namespace, name])
    else:
        return reverse('django_view_with_model', args=[namespace, name])


def reverse_lazy(view_name,
----------
Solution: 

    def url_wrapper(view):
        # gets the module name
        module = _find_urls_module(view)
        # gets the view function (checking if it's a class-based view)
        fn = view.as_view() if hasattr(view, 'as_view') else view

        if namespace and include:
            raise TypeError(
                'You can\'t use \'namespace\' and \'include\''
                ' at the same time!'
            )

        if namespace:
            # imports the urlpatterns object
            base = import_string('{}.urls.urlpatterns'.format(module))
            # searchs for the namespace
            urlpatterns_list = [
                x for x in base
                if getattr(x, 'namespace', None) == namespace
            ]
            # if the list length is different than 1,
            # then the namespace is either duplicated or doesn't exist
            if len(urlpatterns_list) != 1:
                raise ValueError(
                    'Namespace \'{}\' not in list.'.format(namespace)
                )
            # if the namespace was found, get its object
            urlpatterns = urlpatterns_list.pop(0).url_patterns
        else:
            # imports the urlpatterns object
            urlpatterns = import_string('{}.urls.{}'.format(
                module,
                include or 'urlpatterns'
            ))
        # appends the url with its given name
        call = (
            urlpatterns.append if priority is None
            else partial(urlpatterns.insert, priority)
        )
        call(url(path, fn, name=name))
        return view
    return url_wrapper
--------------------
Prompt: 
def generate(env):
    """Add Builders and construction variables for SunPRO C++."""
----------
Output: 

    SCons.Tool.createSharedLibBuilder(env)
    SCons.Tool.createProgBuilder(env)

    env['AR']          = 'ar'
    env['ARFLAGS']     = SCons.Util.CLVar('-c')
    env['ARCOM']       = '$AR --create $TARGET $SOURCES'
    env['AS']          = 'as'
    env['ASFLAGS']    = SCons.Util.CLVar('-s')
    env['RC']          = 'rc'
    env['RCFLAGS']    = SCons.Util.CLVar('-s')
    env['RCCOM']
----------
Solution: 

    path, cxx, shcxx, version = get_cppc(env)
    if path:
        cxx = os.path.join(path, cxx)
        shcxx = os.path.join(path, shcxx)

    cplusplus.generate(env)

    env['CXX'] = cxx
    env['SHCXX'] = shcxx
    env['CXXVERSION'] = version
    env['SHCXXFLAGS']   = SCons.Util.CLVar('$CXXFLAGS -KPIC')
    env['SHOBJPREFIX']  = 'so_'
    env['SHOBJSUFFIX']  = '.o'
--------------------
Prompt: 
def copy_configs(configs, source, target):
    """
    Copy dynamips configs to converted topology

    :param configs: Configs to copy
    :param str source: Source topology directory
    :param str target: Target topology files directory
    :return: True when a config cannot be found, otherwise false
    :rtype: bool
    """
----------
Output: 

    if not os.path.exists(target):
        os.makedirs(target)

    for config in configs:
        if not os.path.exists(os.path.join(config, 'dynamips.yml')):
            return False

        with open(os.path.join(config, 'dynamips.yml'), 'r') as f:
            try:
                converted = yaml.load(f)
            except yaml.YAMLError as exc:
                raise ConfigurationError(
                    'invalid yaml file, contains invalid structure, '
                   'must be a valid yaml dictionary'
                ) from exc

        for key in
----------
Solution: 

    config_err = False
    if len(configs) > 0:
        config_dir = os.path.join(target, 'dynamips', 'configs')
        os.makedirs(config_dir)
        for config in configs:
            old_config_file = os.path.join(source, config['old'])
            new_config_file = os.path.join(config_dir,
                                           os.path.basename(config['new']))
            if os.path.isfile(old_config_file):
                # Copy and rename the config
                shutil.copy(old_config_file, new_config_file)
            else:
                config_err = True
                logging.error('Unable to find %s' % config['old'])
    return config_err
--------------------
Prompt: 
def avail_images(conn=None, call=None):
    """
    Return a list of the server appliances that are on the provider
    """
----------
Output: 

    if conn is None:
        conn = get_ems_inventory_conn()
    return conn.list_images(image_group_name=image_group_name, datacenter_name=datacenter_name,
                         filters={'tag:Name': image_name})


def get_ems_hosts(conn=None, call=None):
    """
    Return a list of EMS host names
    """
    if conn is None:
        conn = get_ems_conn()
    return conn.list_host(host_name=None, datacenter_name=datacenter_name,
                         host
----------
Solution: 

    if call == 'action':
        raise SaltCloudSystemExit(
            'The avail_images function must be called with '
            '-f or --function, or with the --list-images option'
        )

    if not conn:
        conn = get_conn()

    ret = {}

    for appliance in conn.list_appliances():
        ret[appliance['name']] = appliance

    return ret
--------------------
Prompt: 
def setup_prefix_suffix(self):
        """
        Set up the compile prefix, sourcepath and the targetpath suffix
        attributes, which are the prefix to the function name and the
        suffixes to retrieve the values from for creating the generator
        function.
        """
----------
Output: 

        self.prefix, self.sourcepath = self.split_prefix(self.prefix)
        self.ext = '.c'
        if self.prefix == '':
            self.prefix = ''
        if self.sourcepath == '':
            self.sourcepath = ''
        if self.targetpath == '':
            self.targetpath = ''
        self.target = self.prefix + self.targetpath
        self.outputs = [self.target]
        self.inputs = [self.sourcepath, self.targetpath]

    def split_prefix(self, s):
        """
        Splits the given string
----------
Solution: 


        self.compile_prefix = 'compile_'
        self.sourcepath_suffix = '_sourcepath'
        self.modpath_suffix = '_modpaths'
        self.targetpath_suffix = '_targetpaths'
--------------------
Prompt: 
def _g(self, h, xp, s):
        """Density function for blow and hop moves"""
----------
Output: 

        return self.get_density(h, xp, s)

    def _g_from_density(self, density):
        """Density function from density"""
        return self.get_density(h, density)

    def _g_from_energy(self, energy):
        """Energy function from energy"""
        return self.get_energy(energy)

    def _g_from_energy_per_atom(self, energy_per_atom):
        """Energy per atom"""
        return self.get_energy_per_atom(energy_per_atom)

    def get_density(self
----------
Solution: 


        nphi = sum(self.phi)

        return (nphi / 2.0) * log(2 * pi) + nphi * \
            log(s) + 0.5 * sum((h - xp) ** 2) / (s ** 2)
--------------------
Prompt: 
def get_instance_route53_names(self, instance):
        """ Check if an instance is referenced in the records we have from
        Route53. If it is, return the list of domain names pointing to said
        instance. If nothing points to it, return an empty list. """
----------
Output: 

        if instance is None:
            return []
        name_list = []
        if 'Name' in instance:
            name_list = instance['Name'].split(',')
        if 'Name Authority' in instance:
            name_list = instance['NameAuthority'].split(',')
        if 'Name Suffix' in instance:
            name_list = instance['NameSuffix'].split(',')
        if 'Name Suffix Authority' in instance:
            name_list = instance['NameSuffixAuthority'].split(',')
        if 'Name Suffix Suffix' in instance:
            name_list = instance['NameSuffixSuffix
----------
Solution: 


        instance_attributes = [ 'public_dns_name', 'private_dns_name',
                                'ip_address', 'private_ip_address' ]

        name_list = set()

        for attrib in instance_attributes:
            try:
                value = getattr(instance, attrib)
            except AttributeError:
                continue

            if value in self.route53_records:
                name_list.update(self.route53_records[value])

        return list(name_list)
--------------------
Prompt: 
def _transform_row(self, in_row, out_row):
        """
        Transforms an input row to an output row (i.e. (partial) dimensional data).

        :param dict[str,str] in_row: The input row.
        :param dict[str,T] out_row: The output row.

        :rtype: (str,str)
        """
----------
Output: 

        return {
            'in_row': in_row,
            'out_row': out_row,
           'mask': self.mask,
            'label': self.label,
            'type': self.type,
           'shape': self.shape,
            'fill_color': self.fill_color,
            'fill_opacity': self.fill_opacity,
            'fill_opacity': self.fill_opacity,
           'stroke_color': self.stroke_color,
           'stroke_opacity': self.stroke_opacity,
           'stroke_opacity': self.stroke_opacity,
           
----------
Solution: 

        tmp_row = {}

        for step in self._steps:
            park_info, ignore_info = step(in_row, tmp_row, out_row)
            if park_info or ignore_info:
                return park_info, ignore_info

        return None, None
--------------------
Prompt: 
def at(self, t):
        """At time ``t``, compute the target's position relative to the center.

        If ``t`` is an array of times, then the returned position object
        will specify as many positions as there were times.  The kind of
        position returned depends on the value of the ``center``
        attribute:

        * Solar System Barycenter: :class:`~skyfield.positionlib.Barycentric`
        * Center of the Earth: :class:`~skyfield.positionlib.Geocentric`
        * Difference: :class:`~skyfield.positionlib.Geometric`
        * Anything else: :class:`~skyfield.positionlib.ICRF`

        """
----------
Output: 
.Difference`
        * Skyfield: :class:`~skyfield.skyfield.Skyfield`
        * Skyfield: :class:`~skyfield.skyfield.Position`

        :param t: array of times
        :type t: :class:`numpy.ndarray`

        :returns: position object
        :rtype: :class:`SkyPosition`

        """
        if not isinstance(t, np.ndarray):
            t = np.array(t)
        if not t.shape == (2,):
            raise ValueError("at() requires 2D input")
        if t.shape[0]!=
----------
Solution: 

        if not isinstance(t, Time):
            raise ValueError('please provide the at() method with a Time'
                             ' instance as its argument, instead of the'
                             ' value {0!r}'.format(t))
        observer_data = ObserverData()
        observer_data.ephemeris = self.ephemeris
        p, v, observer_data.gcrs_position, message = self._at(t)
        center = self.center
        if center == 0:
            observer_data.bcrs_position = p
            observer_data.bcrs_velocity = v
        self._snag_observer_data(observer_data, t)
        position = build_position(p, v, t, center, self.target, observer_data)
        position.message = message
        return position
--------------------
Prompt: 
def SerializeExclusiveData(self, writer):
        """
        Serialize object.

        Args:
            writer (neo.IO.BinaryWriter):
        """
----------
Output: 

        self.serialize_binary(writer, exclusive=True)

    def Serialize(self, writer):
        """
        Serialize object.

        Args:
            writer (neo.IO.BinaryWriter):
        """
        self.serialize_binary(writer, exclusive=True)
        writer.write(self.serialize())

    def serialize_binary(self, writer, exclusive=False):
        """
        Serialize object.

        Args:
            writer (neo.IO.BinaryWriter):
        """
        if not self.binary:
            return

        if exclusive:
            writer.write_uint(exclusive)
        else
----------
Solution: 

        writer.WriteVarBytes(self.Script)
        if self.Version >= 1:
            writer.WriteFixed8(self.Gas)
--------------------
Prompt: 
def git_branch(self):
        """Git branch name."""
----------
Output: 

        return self.get_branch_name()

    def git_branch_name(self):
        """Git branch name."""
        return self.get_branch_name()

    def git_branch_path(self):
        """Git branch path."""
        return self.get_branch_path()

    def git_branch_path_name(self):
        """Git branch path name."""
        return self.git_branch_path()

    def git_branch_path_path(self):
        """Git branch path path."""
        return self.git_branch_path()

    def git_branch_path_
----------
Solution: 

        info = self.info
        return 'rpm-{major}.{minor}.x'.format(
            major=info[0], minor=info[1])
--------------------
Prompt: 
def determine_start_end_timestep(lsm_file_list,
                                 file_re_match=None,
                                 file_datetime_pattern=None,
                                 expected_time_step=None,
                                 lsm_grid_info=None):
    """
    Determine the start and end date from LSM input files
    """
----------
Output: 

    if file_re_match is None:
        file_re_match = lsm_file_list[0]
    if file_datetime_pattern is None:
        file_datetime_pattern = lsm_file_list[1]
    if expected_time_step is None:
        expected_time_step = determine_timestep(lsm_file_list)
    if file_re_match.start()!= expected_time_step.start():
        raise ValueError("File start date is not in expected format")
    if file_re_match.end()!= expected_time_
----------
Solution: 

    if lsm_grid_info is None:
        lsm_grid_info = identify_lsm_grid(lsm_file_list[0])

    if None in (lsm_grid_info['time_var'], lsm_grid_info['time_dim'])\
            or lsm_grid_info['model_name'] in ('era_20cm', 'erai'):
        # NOTE: the ERA20CM and ERA 24hr time variables
        # in the tests are erroneous
        if None in (file_re_match, file_datetime_pattern):
            raise ValueError("LSM files missing time dimension and/or "
                             "variable.To mitigate this, add the "
                             "'file_re_match' and 'file_datetime_pattern' "
                             "arguments.")

        if lsm_grid_info['time_dim'] is None:
            print("Assuming time dimension is 1")
            file_size_time = 1
        else:
            lsm_example_file = Dataset(lsm_file_list[0])
            file_size_time = \
                len(lsm_example_file.dimensions[lsm_grid_info['time_dim']])
            lsm_example_file.close()

        total_num_time_steps = int(file_size_time * len(lsm_file_list))

        # determine the start time from the existing files
        actual_simulation_start_datetime = \
            datetime.strptime(file_re_match.search(lsm_file_list[0]).group(0),
                              file_datetime_pattern)

        # check to see if the time step matches expected
        if len(lsm_file_list) > 1:
            time_step = \
                int((datetime.strptime(
                    file_re_match.search(lsm_file_list[1]).group(0),
                    file_datetime_pattern) -
                    actual_simulation_start_datetime).total_seconds()
                    / float(file_size_time))

        elif expected_time_step is not None:
            time_step = int(expected_time_step)
        else:
            raise ValueError("Only one LSM file with one timestep present. "
                             "'expected_time_step' parameter required to "
                             "continue.")

        # determine the end datetime
        actual_simulation_end_datetime = \
            datetime.strptime(file_re_match.search(lsm_file_list[-1]).group(0),
                              file_datetime_pattern) \
            + timedelta(seconds=(file_size_time-1) * time_step)
    else:
        with pangaea.open_mfdataset(lsm_file_list,
                                    lat_var=lsm_grid_info['latitude_var'],
                                    lon_var=lsm_grid_info['longitude_var'],
                                    time_var=lsm_grid_info['time_var'],
                                    lat_dim=lsm_grid_info['latitude_dim'],
                                    lon_dim=lsm_grid_info['longitude_dim'],
                                    time_dim=lsm_grid_info['time_dim']) as xds:

            datetime_arr = [pd.to_datetime(dval) for dval in
                            xds.lsm.datetime.values]
            actual_simulation_start_datetime = datetime_arr[0]
            actual_simulation_end_datetime = datetime_arr[-1]
            total_num_time_steps = len(datetime_arr)

            if total_num_time_steps <= 1:
                if expected_time_step is not None:
                    time_step = int(expected_time_step)
                else:
                    raise ValueError("Only one LSM file with one timestep "
                                     "present. 'expected_time_step' parameter "
                                     "required to continue.")
            else:
                time_step = int(np.diff(xds.lsm.datetime.values)[0]
                                / np.timedelta64(1, 's'))

    if expected_time_step is not None:
        if time_step != int(expected_time_step):
            print("WARNING: The time step used {0} is different than "
                  "expected {1}".format(time_step, expected_time_step))

    return (actual_simulation_start_datetime, actual_simulation_end_datetime,
            time_step, total_num_time_steps)
--------------------
Prompt: 
def wait_and_start_browser(host, port=None, cancel_event=None):
    """
    Waits for the server to run and then opens the specified address in
    the browser. Set cancel_event to cancel the wait.
    """
----------
Output: 

    if port is None:
        port = _DEFAULT_PORT
    if cancel_event is None:
        cancel_event = threading.Event()
    if host is None:
        host = _DEFAULT_HOST
    if port is None:
        port = _DEFAULT_PORT
    if host is None:
        raise Exception("Could not find a valid host.")
    if cancel_event.wait(timeout=5):
        raise Exception("Timed out waiting for server to start.")
    if cancel_event.isSet():
        raise Exception("Server did not start.")
    return cancel_event.isSet()


def wait
----------
Solution: 

    if host == '0.0.0.0':
        host = 'localhost'
    if port is None:
        port = 80

    if wait_for_server(host, port, cancel_event):
        start_browser('http://{0}:{1}/'.format(host, port))
--------------------
Prompt: 
def _set_data(self, action):
        """
        marshals response data into page data
        """
----------
Output: 

        self.response_data = {
            'action': action,
            'action_url': self.get_action_url(action),
            'action_text': self.get_action_text(action),
            'action_url_text': self.get_action_url_text(action),
            'action_text_url': self.get_action_text_url(action),
            'action_text_text': self.get_action_text_text(action),
            'action_url_text': self.get_action_url_url(action),
            '
----------
Solution: 

        if 'query' in action:
            self._set_query_data(action)
        elif action == 'imageinfo':
            self._set_imageinfo_data()
        elif action == 'parse':
            self._set_parse_data()
        elif action == 'random':
            self._set_random_data()
        elif action == 'labels':
            self._set_labels()
        elif action == 'wikidata':
            self._set_wikidata()
            self.get_labels()
        elif action == 'restbase':
            self._set_restbase_data()

        self._update_imageinfo()
        self._update_params()
--------------------
Prompt: 
def denied_response(self, req):
        """Returns a standard WSGI response callable with the status of 403 or 401
        depending on whether the REMOTE_USER is set or not.
        """
----------
Output: 

        if req.environ.get('swift.authorize_override') is not None:
            return HTTPForbidden()(req)
        else:
            return self.app(req)

    def handle_request(self, req):
        """
        Entry point for auth requests (two steps)

        If the request is a user authentication request we try to authenticate
        against the user and if successful, we return the response.  Otherwise, we
        update the cookie.
        """
        if not req.path.startswith(self.reseller_prefix):
            return self.denied_response(req)

        if not req.path
----------
Solution: 

        if not hasattr(req, 'credentials_valid'):
            req.credentials_valid = None
        if req.remote_user or req.credentials_valid:
            return HTTPForbidden(request=req)
        else:
            return HTTPUnauthorized(request=req)
--------------------
Prompt: 
def enter_room(self, sid, room, namespace=None):
        """Enter a room.

        This function adds the client to a room. The :func:`emit` and
        :func:`send` functions can optionally broadcast events to all the
        clients in a room.

        :param sid: Session ID of the client.
        :param room: Room name. If the room does not exist it is created.
        :param namespace: The Socket.IO namespace for the event. If this
                          argument is omitted the default namespace is used.
        """
----------
Output: 

        if namespace is None:
            namespace = self.default_namespace
        if namespace is None:
            namespace = self.client_namespace
        self.emit('room-enter', sid, room, namespace)

    def send_event(self, event, data=None):
        """Send a single event.

        This function adds the client to a single event. The :func:`emit` and
        :func:`send` functions can optionally broadcast events to all the
        clients in a single event.

        :param event: Event name.
        :param data: Optional data to send.
        """
        if data is None
----------
Solution: 

        namespace = namespace or '/'
        self.logger.info('%s is entering room %s [%s]', sid, room, namespace)
        self.manager.enter_room(sid, namespace, room)
--------------------
Prompt: 
def parse(readDataInstance):
        """
        Returns a new L{NetMetaDataHeader} object.
        
        @type readDataInstance: L{ReadData}
        @param readDataInstance: A L{ReadData} object with data to be parsed as a L{NetMetaDataHeader} object.
        
        @rtype: L{NetMetaDataHeader}
        @return: A new L{NetMetaDataHeader} object.
        """
----------
Output: 

        header = NetMetaDataHeader()
        header.read(readDataInstance)
        header.read(readDataInstance)
        header.read(readDataInstance)
        header.read(readDataInstance)
        header.read(readDataInstance)
        header.read(readDataInstance)
        header.read(readDataInstance)
        header.read(readDataInstance)
        header.read(readDataInstance)
        header.read(readDataInstance)
        header.read(readDataInstance)
        header.read(readDataInstance)
        header.read(readDataInstance)
        header.read
----------
Solution: 

        nmh = NetMetaDataHeader()
        
        nmh.signature.value = readDataInstance.readDword()
        nmh.majorVersion.value = readDataInstance.readWord()
        nmh.minorVersion.value = readDataInstance.readWord()
        nmh.reserved.value = readDataInstance.readDword()
        nmh.versionLength.value = readDataInstance.readDword()
        nmh.versionString.value = readDataInstance.readAlignedString()
        nmh.flags.value = readDataInstance.readWord()
        nmh.numberOfStreams.value = readDataInstance.readWord()
        return nmh
--------------------
Prompt: 
def _get_args_contents(self):
        """
        Mimic the argument formatting behaviour of
        ActionBase._execute_module().
        """
----------
Output: 

        return self._execute_module().contents

    def _get_kwargs(self):
        """
        Mimic the keyword arguments.
        """
        return self._execute_module().contents

    def _get_action_class(self):
        """
        Return the Action subclass to use for this action.
        """
        return self._execute_module().actions[self.action_name]

    def _get_action_name(self):
        """
        Return the name of the action (string)
        """
        return self._get_action_name_from_kwargs()

    def _get_action_name
----------
Solution: 

        return ' '.join(
            '%s=%s' % (key, shlex_quote(str(self.args[key])))
            for key in self.args
        ) + ' '
--------------------
Prompt: 
def fly(self):
        """
        Generate doc tree.
        """
----------
Output: 

        self.doc_tree = DocTree()
        self.doc_tree.set_root_node(self.root_node)
        self.doc_tree.set_root_node(self.root_node)
        self.doc_tree.set_root_node(self.root_node)
        self.doc_tree.set_root_node(self.root_node)
        self.doc_tree.set_root_node(self.root_node)
        self.root_node.children.append(self.doc_tree)
        self.root_node
----------
Solution: 

        dst_dir = Path(self.conf_file).parent.abspath

        package_dir = Path(dst_dir, self.package.shortname)

        # delete existing api document
        try:
            if package_dir.exists():
                shutil.rmtree(package_dir.abspath)
        except Exception as e:
            print("'%s' can't be removed! Error: %s" % (package_dir, e))

        # create .rst files
        for pkg, parent, sub_packages, sub_modules in self.package.walk():
            if not is_ignored(pkg, self.ignored_package):
                dir_path = Path(*([dst_dir, ] + pkg.fullname.split(".")))
                init_path = Path(dir_path, "__init__.rst")

                make_dir(dir_path.abspath)
                make_file(
                    init_path.abspath,
                    self.generate_package_content(pkg),
                )

                for mod in sub_modules:
                    if not is_ignored(mod, self.ignored_package):
                        module_path = Path(dir_path, mod.shortname + ".rst")
                        make_file(
                            module_path.abspath,
                            self.generate_module_content(mod),
                        )
--------------------
Prompt: 
def taper(self):
        """Taper the spectrum by adding zero flux to each end.
        This is similar to :meth:`SpectralElement.taper`.

        There is no check to see if the spectrum is already tapered.
        Hence, calling this on a tapered spectrum will result in
        multiple zero-flux entries at both ends.

        The wavelengths to use for the new first and last points are
        calculated by using the same ratio as for the two interior points
        used at each end.

        Returns
        -------
        OutSpec : `TabularSourceSpectrum`
            Tapered spectrum.

        """
----------
Output: 
 with zero-flux entries.
        """
        return self.taper_with_zero_flux(taper_type='taper')

    def taper_with_zero_flux(self, taper_type='taper'):
        """Tapered tapered spectrum with zero-flux entries.

        This method is similar to :meth:`SpectralElement.taper_with_zero_flux`.

        There is no check to see if the tapered spectrum is already tapered.
        Hence, calling this on a tapered tapered spectrum will result in
        multiple zero-
----------
Solution: 

        OutSpec = TabularSourceSpectrum()
        wcopy = N.zeros(self._wavetable.size+2, dtype=N.float64)
        fcopy = N.zeros(self._fluxtable.size+2, dtype=N.float64)
        wcopy[1:-1] = self._wavetable
        fcopy[1:-1] = self._fluxtable
        fcopy[0] = 0.0
        fcopy[-1] = 0.0

        # The wavelengths to use for the first and last points are
        # calculated by using the same ratio as for the 2 interior points
        wcopy[0] = wcopy[1]*wcopy[1]/wcopy[2]
        wcopy[-1] = wcopy[-2]*wcopy[-2]/wcopy[-3]

        OutSpec._wavetable = wcopy
        OutSpec._fluxtable = fcopy
        OutSpec.waveunits = units.Units(str(self.waveunits))
        OutSpec.fluxunits = units.Units(str(self.fluxunits))

        return OutSpec
--------------------
Prompt: 
def install(runas=None, path=None):
    """
    Install pyenv systemwide

    CLI Example:

    .. code-block:: bash

        salt '*' pyenv.install
    """
----------
Output: 

    if runas is None:
        runas = _get_runas()
    if path is None:
        path = _get_path()
    if runas is None:
        runas = _get_runas()
    if runas is None:
        return False
    if runas.is_file('/etc/rc.conf'):
        rc_conf = '/etc/rc.conf'
    else:
        rc_conf = '/etc/rc.conf.d/rc.conf'
    if not os.path.exists(rc_conf):
        return False
    if not os.path
----------
Solution: 

    path = path or _pyenv_path(runas)
    path = os.path.expanduser(path)
    return _install_pyenv(path, runas)
--------------------
Prompt: 
def create_domain_record(self, domain_id, record_type, data, name=None,
                             priority=None, port=None, weight=None):
        """
        This method creates a new domain name with an A record for the specified
        [ip_address].

        Required parameters

            domain_id:
                Integer or Domain Name (e.g. domain.com), specifies the domain
                for which to create a record.

            record_type:
                String, the type of record you would like to create.
                'A', 'CNAME', 'NS', 'TXT', 'MX' or 'SRV'

            data:
                String, this is the value of the record

        Optional parameters
            name:
                String, required for 'A', 'CNAME', 'TXT' and 'SRV' records

            priority:
                Integer, required for 'SRV' and 'MX' records

            port:
                Integer, required for 'SRV' records

            weight:
                Integer, required for 'SRV' records
        """
----------
Output: 
 'NS', the type of record you would like to create.

            data:
                String, the data you would like to create.
                String, the data you would like to create.

            name:
                String, the name you would like to create.
                String, the name you would like to create.

            priority:
                Integer, the priority you would like to create.

            port:
                Integer, the port you would like to create.

            weight:
                Integer, the weight you would like to create.
        """
        return self.api_request(path='/domains/{0}/records/{1}'.format(domain
----------
Solution: 

        params = dict(record_type=record_type, data=data)

        if name:
            params.update({'name': name})
        if priority:
            params.update({'priority': priority})
        if port:
            params.update({'port': port})
        if weight:
            params.update({'weight': weight})

        json = self.request('/domains/%s/records/new' % domain_id, method='GET', params=params)
        status = json.get('status')
        if status == 'OK':
            domain_record_json = json.get('domain_record')
            domain_record = Record.from_json(domain_record_json)
            return domain_record
        else:
            message = json.get('message')
            raise DOPException('[%s]: %s' % (status, message))
--------------------
Prompt: 
def install_labels(cls, quiet=True, stdout=None):
    """
    Setup labels with indexes and constraints for a given class

    :param cls: StructuredNode class
    :type: class
    :param quiet: (default true) enable standard output
    :param stdout: stdout stream
    :type: bool
    :return: None
    """
----------
Output: 

    if stdout is None:
        stdout = sys.stdout
    if not stdout:
        stdout = sys.stdout
    if not quiet:
        stdout.write("Creating index files for %s\n" % cls.name)
        stdout.write("  - %s\n" % cls.name)
        stdout.write("  - %s\n" % cls.name)
        stdout.write("  - %s\n" % cls.name)
        stdout.write("  - %s\n" % cls.name)
        stdout.write("  - %s\n" %
----------
Solution: 


    if not hasattr(cls, '__label__'):
        if not quiet:
            stdout.write(' ! Skipping class {0}.{1} is abstract\n'.format(cls.__module__, cls.__name__))
        return

    for name, property in cls.defined_properties(aliases=False, rels=False).items():
        db_property = property.db_property or name
        if property.index:
            if not quiet:
                stdout.write(' + Creating index {0} on label {1} for class {2}.{3}\n'.format(
                    name, cls.__label__, cls.__module__, cls.__name__))

            db.cypher_query("CREATE INDEX on :{0}({1}); ".format(
                cls.__label__, db_property))

        elif property.unique_index:
            if not quiet:
                stdout.write(' + Creating unique constraint for {0} on label {1} for class {2}.{3}\n'.format(
                    name, cls.__label__, cls.__module__, cls.__name__))

            db.cypher_query("CREATE CONSTRAINT "
                            "on (n:{0}) ASSERT n.{1} IS UNIQUE; ".format(
                cls.__label__, db_property))
--------------------
Prompt: 
def consult_filters(self, url_info: URLInfo, url_record: URLRecord, is_redirect: bool=False) \
            -> Tuple[bool, str, dict]:
        """Consult the URL filter.

        Args:
            url_record: The URL record.
            is_redirect: Whether the request is a redirect and it is
                desired that it spans hosts.

        Returns
            tuple:

            1. bool: The verdict
            2. str: A short reason string: nofilters, filters, redirect
            3. dict: The result from :func:`DemuxURLFilter.test_info`
        """
----------
Output: 
is_redirect`
        """
        if is_redirect:
            if not url_info.is_redirect:
                url_info.is_redirect = True
            return (True, url_info.is_redirect)
        else:
            if not url_record.is_redirect:
                url_record.is_redirect = True
            return (True, '', url_info.is_redirect)

    def get_demux_url_filter(self, url_info: URLInfo) \
            -> DemuxURLFilter:
        """Get a :class:`DemuxURLFilter` for the given URL
----------
Solution: 

        if not self._url_filter:
            return True, 'nofilters', None

        test_info = self._url_filter.test_info(url_info, url_record)

        verdict = test_info['verdict']

        if verdict:
            reason = 'filters'
        elif is_redirect and self.is_only_span_hosts_failed(test_info):
            verdict = True
            reason = 'redirect'
        else:
            reason = 'filters'

        return verdict, reason, test_info
--------------------
Prompt: 
def write_csv(path, rows, dialect='excel', fieldnames=None, quoting=csv.QUOTE_ALL, extrasaction='ignore', *args, **kwargs):
        """ Write rows data to a CSV file (with or without fieldnames) """
----------
Output: 

        if not quoting:
            quoting = csv.QUOTE_MINIMAL

        writer = csv.DictWriter(path, fieldnames=fieldnames, extrasaction=extrasaction)
        if quoting:
            writer.writeheader()
        for row in rows:
            if dialect == 'excel':
                row = self.convert_rows(row)
            writer.writerow(row)

        return path

    def write_table(self, table, path, fields, delimiter=',',
            quotechar='"', quoting=csv.QUOTE_ALL,
            header=True, quotechar='"'):
        """ Write a table data structure to
----------
Solution: 

        if not quoting:
            quoting = csv.QUOTE_MINIMAL
        if 'lineterminator' not in kwargs:
            kwargs['lineterminator'] = '\n'  # use \n to fix double-line in Windows
        with open(path, mode='wt', newline='') as csvfile:
            if fieldnames:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames, dialect=dialect, quoting=quoting, extrasaction=extrasaction, *args, **kwargs)
                writer.writeheader()
                for row in rows:
                    writer.writerow(row)
            else:
                writer = csv.writer(csvfile, dialect=dialect, quoting=quoting, *args, **kwargs)
                for row in rows:
                    writer.writerow(row)
--------------------
Prompt: 
def _GetCh(self):
    """Read a single character from the user.

    Returns:
      A string, the character read.
    """
----------
Output: 

    self._Read(1)
    return self._Read(2)

  def _GetC(self):
    """Read a character from the user.

    Returns:
      A string, the character read.
    """
    self._Read(1)
    return self._Read(2)

  def _Read(self, n):
    """Read n bytes from the input stream.

    Args:
      n: The number of bytes to read.

    Returns:
      A string, the number of bytes read.
    """
    self._stream.seek(n)
    result = self._stream.read()
    self._
----------
Solution: 

    fd = self._tty.fileno()
    old = termios.tcgetattr(fd)
    try:
      tty.setraw(fd)
      ch = self._tty.read(1)
      # Also support arrow key shortcuts (escape + 2 chars)
      if ord(ch) == 27:
        ch += self._tty.read(2)
    finally:
      termios.tcsetattr(fd, termios.TCSADRAIN, old)
    return ch
--------------------
Prompt: 
def to_python(self, value):
        """Validates that the input can be converted to a list of decimals."""
----------
Output: 

        if value is None:
            value = []
        return value

    def to_url(self, value):
        """
        Validates that the input can be converted to a URL.
        """
        if value is None:
            value = ''
        return value

    def to_python_list(self, value):
        """
        Validates that the input is a list of strings.
        """
        if value is None:
            value = []
        return value

    def to_url_args(self, value):
        """
        Converts the input value into a dictionary.
        """
        if value is None:
            value =
----------
Solution: 

        if not value:
            return None

        if isinstance(value, list):
            for index, position_val in enumerate(value):
                val = super(MultipleDecimalField, self).to_python(position_val)
                value[index] = val

        return value
--------------------
Prompt: 
def parse(self, response):
        """
        根据对 ``start_urls`` 中提供链接的请求响应包内容，解析生成具体文章链接请求

        :param Response response: 由 ``Scrapy`` 调用并传入的请求响应对象
        """
----------
Output: 

        self.start_urls = [url for url in start_urls if url.startswith(self.base_url)]
        return self

    def get_urls(self):
        """
        获取具体的urls

        :return: 根据对象的urls
        """
        return self.start_urls

    def get_start_urls(self):
        """
        获取具体的start_urls

        :return: 根据对象的urls
        """
        return self.start_urls

    def get_urls_from
----------
Solution: 

        content_raw = response.body.decode()
        self.logger.debug('响应body原始数据：{}'.format(content_raw))
        content = json.loads(content_raw, encoding='UTF-8')
        self.logger.debug(content)

        # 文章发布日期
        date = datetime.datetime.strptime(content['date'], '%Y%m%d')

        strftime = date.strftime("%Y-%m-%d")
        self.logger.info('日期：{}'.format(strftime))

        # 处理头条文章列表，将其 `top` 标记到相应 __story__ 中
        if 'top_stories' in content:
            self.logger.info('处理头条文章')
            for item in content['top_stories']:
                for story in content['stories']:
                    if item['id'] == story['id']:
                        story['top'] = 1
                        break
                self.logger.debug(item)

        # 处理今日文章，并抛出具体文章请求
        post_num = len(content['stories'])
        self.logger.info('处理今日文章，共{:>2}篇'.format(post_num))
        for item in content['stories']:
            self.logger.info(item)
            post_num = 0 if post_num < 0 else post_num
            pub_time = date + datetime.timedelta(minutes=post_num)
            post_num -= 1

            url = 'http://news-at.zhihu.com/api/4/news/{}'.format(item['id'])
            request = scrapy.Request(url, callback=self.parse_post)
            post_dict = {
                'spider': ZhihuDailySpider.name,
                'date': pub_time.strftime("%Y-%m-%d %H:%M:%S"),
                'meta': {
                    'spider.zhihu_daily.id': str(item.get('id', ''))
                }
            }
            if item.get('top'):
                post_dict['meta']['spider.zhihu_daily.top'] = \
                    str(item.get('top', 0))
            request.meta['post'] = post_dict
            self.item_list.append(post_dict)
            yield request
--------------------
Prompt: 
def iter_steps(g, steps):
    """
    iterate over 'g' in blocks with a length of the given 'step' count.

    >>> for v in iter_steps([1,2,3,4,5], steps=2): v
    [1, 2]
    [3, 4]
    [5]
    >>> for v in iter_steps([1,2,3,4,5,6,7,8,9], steps=3): v
    [1, 2, 3]
    [4, 5, 6]
    [7, 8, 9]

                                 12345678        12345678
                                         12345678
    >>> bits = [int(i) for i in "0101010101010101111000"]
    >>> for v in iter_steps(bits, steps=8): v
    [0, 1, 0, 1, 0, 1, 0, 1]
    [0, 1, 0, 1, 0, 1, 0, 1]
    [1, 1, 1, 0, 0, 0]
    """
----------
Output: 
 ^
        ^
    [1, 2, 3, 4, 5, 6, 7, 8, 9]
    """
    for i in xrange(0, len(g), steps):
        yield g[i:i+steps]

def iter_steps_from_list(l, steps):
    """
    iterate over 'l' in blocks of'steps' (a list of blocks)

    >>> for v in iter_steps_from_list([1,2,3,4,5], steps=2): v
    [1, 2, 3, 4, 5]
    >>> for v
----------
Solution: 

    values = []
    for value in g:
        values.append(value)
        if len(values) == steps:
            yield list(values)
            values = []
    if values:
        yield list(values)
--------------------
Prompt: 
def wait_key(keys=None):
    """ Waits for a keypress at the console and returns it.
        "Where's the any key?"

        Arguments:
            keys - if passed, wait for this specific key, e.g. ESC.
                   may be a tuple.
        Returns:
            char or ESC - depending on key hit.
            None - immediately under i/o redirection, not an interactive tty.
    """
----------
Output: 

    if keys is None:
        keys = ()
    for key in keys:
        if key is None:
            return None
        if isinstance(key, str):
            if key == "esc":
                return True
            elif key == "tab":
                return False
        if isinstance(key, tuple):
            if len(key) == 2:
                char = key[0]
                if char == " ":
                    return None
                elif char == "\t":
                    return char
                elif char == "\n":
                    return None
                else:
                    return None
        if key is None:
            return None
        if isinstance(key, str
----------
Solution: 

    if is_a_tty():
        if keys:
            if not isinstance(keys, tuple):
                keys = (keys,)
            while True:
                key = _getch()
                if key in keys:
                    return key
        else:
            return _getch()
--------------------
Prompt: 
def _WriteStorageMetadata(self):
    """Writes the storage metadata."""
----------
Output: 

    self._WriteMetadata(self._storage_writer, self._metadata)

  def _WriteMetadata(self, writer, metadata):
    """Writes the storage metadata."""
    if metadata is None:
      return
    writer.Write("  %s: %s\n" % (metadata.filename, metadata.value))

  def _GetMetadata(self, filename, _with_private=False):
    """Reads a file containing metadata for a given filename."""
    metadata = rdf_client.StatEntry(
        pathspec=rdf_paths.PathSpec(
            path="/".join(filename.split("/")[:-
----------
Solution: 

    self._cursor.execute(self._CREATE_METADATA_TABLE_QUERY)

    query = 'INSERT INTO metadata (key, value) VALUES (?, ?)'

    key = 'format_version'
    value = '{0:d}'.format(self._FORMAT_VERSION)
    self._cursor.execute(query, (key, value))

    key = 'compression_format'
    value = self.compression_format
    self._cursor.execute(query, (key, value))

    key = 'serialization_format'
    value = self.serialization_format
    self._cursor.execute(query, (key, value))

    key = 'storage_type'
    value = self.storage_type
    self._cursor.execute(query, (key, value))
--------------------
Prompt: 
def childgroup(self, field):
        """
        Return a list of fields stored by row regarding the configured grid

        :param field: The original field this widget is attached to
        """
----------
Output: 

        return self.childgroups[field]

    def _get_field_options(self, field):
        """
        Return a list of options for the given field

        :param field: The original field this widget is attached to
        """
        return self.childoptions[field]

    def _get_field_display(self, field):
        """
        Return a display string for the given field

        :param field: The original field this widget is attached to
        """
        return self.childdisplay[field]

    def _get_field_icon(self, field):
        """
        Return a QIcon for the
----------
Solution: 

        grid = getattr(self, "grid", None)
        named_grid = getattr(self, "named_grid", None)

        if grid is not None:
            childgroup = self._childgroup(field.children, grid)
        elif named_grid is not None:
            childgroup = self._childgroup_by_name(field.children, named_grid)
        else:
            raise AttributeError(u"Missing the grid or named_grid argument")

        return childgroup
--------------------
Prompt: 
def permission_set(self, name, func=None):
        """Define a new permission set (directly, or as a decorator).

        E.g.::

            @authz.permission_set('HTTP')
            def is_http_perm(perm):
                return perm.startswith('http.')

        """
----------
Output: 

        if func is None:
            func = self.permission_get
        self.permissions[name] = func

    def permission_get(self, name):
        """Return the permission for the given name.

        :param name: The name of the permission.
        :type name: str

        :returns: The permission for the given name.
        :rtype: str

        :raises ValueError: If the given name is not a valid permission.
        :raises TypeError: If the given name is not a callable.
        :raises ValueError: If the given name is not a valid permission.
        :raises ValueError: If the given
----------
Solution: 

        if func is None:
            return functools.partial(self.predicate, name)
        self.permission_sets[name] = func
        return func
--------------------
Prompt: 
def evaluate(self, node: InstanceNode) -> XPathValue:
        """Evaluate the receiver and return the result.

        Args:
            node: Context node for XPath evaluation.

        Raises:
            XPathTypeError: If a subexpression of the receiver is of a wrong
                type.
        """
----------
Output: 

        if isinstance(node, XPathValue):
            return node.evaluate(self)
        elif isinstance(node, XPathValue):
            return self.evaluate(node.context)
        else:
            raise XPathTypeError(node)

    def __str__(self) -> str:
        """Get the XPath value as a string.

        Returns:
            str: The XPath value.

        Raises:
            XPathTypeError: If a subexpression of the receiver is of a wrong
                type.
        """
        if isinstance(self.subexpression, str):
            return self.subexpression
        elif isinstance(self.
----------
Solution: 

        return self._eval(XPathContext(node, node, 1, 1))
--------------------
Prompt: 
def get_time_slice(time, z, zdot=None, timeStart=None, timeEnd=None):
    """
    Get slice of time, z and (if provided) zdot from timeStart to timeEnd.

    Parameters
    ----------
    time : ndarray
        array of time values 
    z : ndarray
        array of z values
    zdot : ndarray, optional
        array of zdot (velocity) values.
    timeStart : float, optional
        time at which to start the slice.
        Defaults to beginnging of time trace
    timeEnd : float, optional
        time at which to end the slide.
        Defaults to end of time trace

    Returns
    -------
    time_sliced : ndarray
        array of time values from timeStart to timeEnd
    z_sliced : ndarray
        array of z values from timeStart to timeEnd
    zdot_sliced : ndarray
        array of zdot values from timeStart to timeEnd.
        None if zdot not provided

    """
----------
Output: 
.
        Defaults to end of time trace.

    Returns
    -------
    timeSlice : ndarray
        array of time values, if provided, the slice of z and zdot from timeStart to timeEnd.
    """
    if zdot is None:
        if timeStart is None:
            timeStart = 0.0
        if timeEnd is None:
            timeEnd = 0.0
        zdot = np.asarray(zdot)
        if timeStart is not None:
            timeStart = float(timeStart)
        if timeEnd is not None:
            timeEnd = float(timeEnd)
        if timeEnd is
----------
Solution: 

    if timeStart == None:
        timeStart = time[0]
    if timeEnd == None:
        timeEnd = time[-1]

    StartIndex = _np.where(time == take_closest(time, timeStart))[0][0]
    EndIndex = _np.where(time == take_closest(time, timeEnd))[0][0]

    time_sliced = time[StartIndex:EndIndex]
    z_sliced = z[StartIndex:EndIndex]

    if zdot != None:
        zdot_sliced = zdot[StartIndex:EndIndex]
    else:
        zdot_sliced = None    
    
    return time_sliced, z_sliced, zdot_sliced
--------------------
Prompt: 
def howPlotAsk(goodFormat):
    """plots using inquirer prompts

    Arguments:
        goodFormat {dict} -- module : [results for module]
    """
----------
Output: 

    import matplotlib.pyplot as plt
    import numpy as np

    plt.figure(1)
    plt.plot(goodFormat['results']['inquirer'], goodFormat['results']['inquirer_prompt'], 'r', label='inquirer')
    plt.plot(goodFormat['results']['inquirer'], goodFormat['results']['inquirer_prompt_prompt'], 'r', label='inquirer_prompt')
    plt.plot(goodFormat['results']['inquirer'], goodFormat['results']['inquirer_prompt_prompt'], 'r', label='inquirer_prompt
----------
Solution: 

    plotAnswer = askPlot()
    if "Save" in plotAnswer['plotQ']:
        exportPlotsPath = pathlib.Path(askSave())
        if "Show" in plotAnswer['plotQ']:
            plotter(exportPlotsPath, True, goodFormat)
        else:
            plotter(exportPlotsPath, False, goodFormat)
    elif "Show" in plotAnswer['plotQ']:
        plotter(None, True, goodFormat)
--------------------
Prompt: 
async def message_data(request):
    """
    As an example of aiohttp providing a non-html response, we load the actual messages for the "messages" view above
    via ajax using this endpoint to get data. see static/message_display.js for details of rendering.
    """
----------
Output: 

    return web.Response(text=json.dumps(request.app['messages']),
                        status=200,
                        headers={
                            'Content-Type': 'application/json',
                            'X-Requested-With': 'XMLHttpRequest',
                            'X-CSRF-Token': request.app['csrf_token'],
                        },
                        status=200)


@routes.api_route('/message/display.js')
async def message_display_js(request):
    """
    As an example of aiohttp providing a non-html response, we load the actual messages for the "messages" view above
    via ajax
----------
Solution: 

    messages = []
    # {% if database.is_none %}
    if request.app['settings'].MESSAGE_FILE.exists():
        # read the message file, process it and populate the "messages" list
        with request.app['settings'].MESSAGE_FILE.open() as msg_file:
            for line in msg_file:
                if not line:
                    # ignore blank lines eg. end of file
                    continue
                # split the line into it constituent parts, see process_form above
                username, ts, message = line.split('|', 2)
                # parse the datetime string and render it in a more readable format.
                ts = '{:%Y-%m-%d %H:%M:%S}'.format(datetime.strptime(ts, '%Y-%m-%dT%H:%M:%S.%f'))
                messages.append({'username': username, 'timestamp': ts, 'message': message})
        messages.reverse()
    # {% elif database.is_pg_sqlalchemy %}

    async with request.app['pg_engine'].acquire() as conn:
        async for row in conn.execute(sa_messages.select().order_by(sa_messages.c.timestamp.desc())):
            ts = '{:%Y-%m-%d %H:%M:%S}'.format(row.timestamp)
            messages.append({'username': row.username, 'timestamp': ts, 'message': row.message})
    # {% endif %}
    return json_response(messages)
--------------------
Prompt: 
def _extract_gcs_api_response_error(message):
  """ A helper function to extract user-friendly error messages from service exceptions.

  Args:
    message: An error message from an exception. If this is from our HTTP client code, it
        will actually be a tuple.

  Returns:
    A modified version of the message that is less cryptic.
  """
----------
Output: 

  if not isinstance(message, tuple):
    return (message, None)

  if len(message) > 1:
    return (message, None)

  return (message, None)


def _extract_gcs_api_error_details(exc):
  """ A helper function to extract user-friendly details from service exceptions.

  Args:
    exc: An error message from an exception. If this is from our HTTP client code, it
        will be a tuple.

  Returns:
    A modified version of the message that is less cryptic.
  """
  if not isinstance(exc, tuple):
----------
Solution: 

  try:
    if len(message) == 3:
      # Try treat the last part as JSON
      data = json.loads(message[2])
      return data['error']['errors'][0]['message']
  except Exception:
    pass
  return message
--------------------
Prompt: 
def _run_happy_eval(vrn_file, rm_file, rm_interval_file, base_dir, data):
    """Validation with hap.py: https://github.com/Illumina/hap.py

    XXX Does not yet parse out metrics for plotting.
    """
----------
Output: 

    # TODO: Add support for other metrics.
    # TODO: Add support for other metrics.
    # TODO: Add support for other metrics.
    # TODO: Add support for other metrics.
    # TODO: Add support for other metrics.
    # TODO: Add support for other metrics.
    # TODO: Add support for other metrics.
    # TODO: Add support for other metrics.
    # TODO: Add support for other metrics.
    # TODO: Add support for other metrics.
    # TODO: Add support for other metrics.
    # TODO: Add support for other metrics.
    # TODO: Add support for other
----------
Solution: 

    out_dir = utils.safe_makedir(os.path.join(base_dir, "happy"))
    out_prefix = os.path.join(out_dir, "val")
    if not utils.file_exists(out_prefix + ".summary.csv"):
        vrn_file, rm_file, interval_bed = _prepare_inputs(vrn_file, rm_file, rm_interval_file, base_dir, data)
        cmd = ["hap.py", "-V", "-f", interval_bed, "-r", dd.get_ref_file(data),
               "-l", ",".join(_get_location_list(interval_bed)),
               "-o", out_prefix, rm_file, vrn_file]
        do.run(cmd, "Validate calls using hap.py", data)
    return {"vcf": out_prefix + ".vcf.gz"}
--------------------
Prompt: 
def start(self):
        """
        Start rule poller and target poller once X-Ray daemon address
        and context manager is in place.
        """
----------
Output: 

        self.log.info("Starting X-Ray daemon")
        self.log.info("  - Waiting for X-Ray daemon to start")
        self.log.info("  - Waiting for X-Ray daemon to start")
        self.log.info("  - Waiting for X-Ray daemon to start")
        self.log.info("  - Waiting for X-Ray daemon to start")
        self.log.info("  - Waiting for X-Ray daemon to start")
        self.log.info("  - Waiting for X-Ray daemon to start")
       
----------
Solution: 

        if not global_sdk_config.sdk_enabled():
            return

        with self._lock:
            if not self._started:
                self._rule_poller.start()
                self._target_poller.start()
                self._started = True
--------------------
Prompt: 
def weld_unique(array, weld_type):
    """Return the unique elements of the array.

    Parameters
    ----------
    array : numpy.ndarray or WeldObject
        Input array.
    weld_type : WeldType
        Type of each element in the input array.

    Returns
    -------
    WeldObject
        Representation of this computation.

    """
----------
Output: 

    if weld_type == WeldType.PSEUDO_ARRAY:
        return array
    elif weld_type == WeldType.PSEUDO_ARRAY_LIST:
        return WeldObject.list_weld_arrays(array)
    elif weld_type == WeldType.PSEUDO_ARRAY_LIST_LIST:
        return WeldObject.list_weld_arrays_list(array)
    elif weld_type == WeldType.PSEUDO_ARRAY_LIST_LIST:
        return WeldObject.list_weld_
----------
Solution: 

    obj_id, weld_obj = create_weld_object(array)

    weld_template = 
--------------------
Prompt: 
def _ProcessZipFileWithPlugins(self, parser_mediator, zip_file):
    """Processes a zip file using all compound zip files.

    Args:
      parser_mediator (ParserMediator): mediates interactions between parsers
          and other components, such as storage and dfvfs.
      zip_file (zipfile.ZipFile): the zip file. It should not be closed in
          this method, but will be closed in ParseFileObject().
    """
----------
Output: 

    for plugin in self._plugins:
      plugin.ProcessZipFile(zip_file, parser_mediator)

  def ParseFileObject(self):
    """Parses a file-like object.

    Returns:
      bool: True if the file-like object was parsed.
    """
    try:
      self._ProcessZipFileWithPlugins(self._parser_mediator, self._zip_file_object)
    except (IOError, OSError) as exception:
      logging.error(exception)
      return False

    return True

  def GetFileObject(self):
    """Retrieves a file-like object.

    Returns:
     
----------
Solution: 

    archive_members = zip_file.namelist()
    for plugin in self._plugins:
      try:
        plugin.UpdateChainAndProcess(
            parser_mediator, zip_file=zip_file, archive_members=archive_members)
      except errors.WrongCompoundZIPPlugin as exception:
        logger.debug('[{0:s}] wrong plugin: {1!s}'.format(
            self.NAME, exception))
--------------------
Prompt: 
def load_from_array(self, keys, data_array):
        """
        This loads the data contained in an array into the catalogue object

        :param keys:
            A list of keys explaining the content of the columns in the array
        :type list:
        """
----------
Output: 

        for key in keys:
            if key not in data_array:
                raise KeyError("Key %s not found in catalogue" % key)
            if key not in self.columns:
                raise KeyError("Column %s not found in catalogue" % key)
            if key not in self.columns_dict:
                raise KeyError("Column %s not found in catalogue" % key)
            if key not in self.columns_dict_ordered:
                self.columns_dict_ordered.append(key)
            if key not in self.columns_dict_ordered_reversed:
                self.columns
----------
Solution: 


        if len(keys) != np.shape(data_array)[1]:
            raise ValueError('Key list does not match shape of array!')

        for i, key in enumerate(keys):
            if key in self.INT_ATTRIBUTE_LIST:
                self.data[key] = data_array[:, i].astype(int)
            else:
                self.data[key] = data_array[:, i]
            if key not in self.TOTAL_ATTRIBUTE_LIST:
                print('Key %s not a recognised catalogue attribute' % key)

        self.update_end_year()
--------------------
Prompt: 
def _on_progress(adapter, operation, conn_id, done, total):
    """Callback when progress is reported."""
----------
Output: 

    if done == 0:
        log.info('Processed %d rows.', len(self.rows))
    elif total == 0:
        log.info('No rows found.')
    else:
        log.info('Processed %d rows (%d%%).', done, 100 * done / total)


class _BatchedSerializer(object):
    """Batched serializer for use with the old mongoengine-backed search API."""

    def __init__(self, queryset, batch_size):
        self.queryset = queryset
        self.batch_size = batch_size

    def __len__(self):
----------
Solution: 


    conn_string = adapter._get_property(conn_id, 'connection_string')
    if conn_string is None:
        return

    adapter.notify_progress(conn_string, operation, done, total)
--------------------
Prompt: 
def read_lsm_timestamps(fh):
    """Read LSM time stamps from file and return as list."""
----------
Output: 

    lsm_timestamps = []
    for line in fh:
        if line.startswith('Time: '):
            lsm_timestamps.append(line[len('Time: '):line.find(' ')])
    return lsm_timestamps

def read_lsm_times(fh):
    """Read LSM time stamps from file and return as list."""
    lsm_times = []
    for line in fh:
        if line.startswith('Time: '):
            lsm_times.append(line[len('Time: '):line.find(' ')])
    return lsm_times

def
----------
Solution: 

    size, count = struct.unpack('<ii', fh.read(8))
    if size != (8 + 8 * count):
        log.warning('read_lsm_timestamps: invalid LSM TimeStamps block')
        return []
    # return struct.unpack('<%dd' % count, fh.read(8*count))
    return fh.read_array('<f8', count=count)
--------------------
Prompt: 
def get_validated_types(object_types: Set[Type], set_name: str) -> Set[Type]:
    """
    Utility to validate a set of types :
    * None is not allowed as a whole or within the set,
    * object and Any are converted into AnyObject
    * if AnyObject is in the set, it must be the only element

    :param object_types: the set of types to validate
    :param set_name: a name used in exceptions if any
    :return: the fixed set of types
    """
----------
Output: 

    if not object_types:
        return None
    if not isinstance(object_types, set):
        raise TypeError(f"object_types must be a set, got {type(object_types)}")
    return object_types - set_name


def get_validated_types_for_type(
    type_name: str, object_types: AnyObject, set_name: str
) -> Set[Type]:
    """
    Utility to validate a set of types :
    * AnyObject is not allowed as a whole or within the set,
    * object and Any are converted into AnyObject
----------
Solution: 

    check_var(object_types, var_types=set, var_name=set_name)
    res = {get_validated_type(typ, set_name + '[x]') for typ in object_types}
    if AnyObject in res and len(res) > 1:
        raise ValueError('The set of types contains \'object\'/\'Any\'/\'AnyObject\', so no other type must be present '
                         'in the set')
    else:
        return res
--------------------
Prompt: 
def from_key_bytes(cls, algorithm, key_bytes):
        """Creates a `Verifier` object based on the supplied algorithm and raw verification key.

        :param algorithm: Algorithm on which to base verifier
        :type algorithm: aws_encryption_sdk.identifiers.Algorithm
        :param bytes encoded_point: Raw verification key
        :returns: Instance of Verifier generated from encoded point
        :rtype: aws_encryption_sdk.internal.crypto.Verifier
        """
----------
Output: 

        return Verifier(
            algorithm=algorithm,
            key_bytes=key_bytes,
            key_info=cls._parse_key_info(key_bytes),
            key_info_digest=cls._parse_key_digest(key_bytes),
            key_info_verify=cls._parse_key_verify(key_bytes),
            key_info_encrypt=cls._parse_key_encrypt(key_bytes),
            key_info_decrypt=cls._parse_key_decrypt(key_bytes),
            key_info_encrypt_digest=cls._parse_key_digest
----------
Solution: 

        return cls(
            algorithm=algorithm, key=serialization.load_der_public_key(data=key_bytes, backend=default_backend())
        )
--------------------
Prompt: 
def _get_oauth_token(self):
        """
        Get Monzo access token via OAuth2 `authorization code` grant type.

        Official docs:
            https://monzo.com/docs/#acquire-an-access-token

        :returns: OAuth 2 access token
        :rtype: dict
        """
----------
Output: 

        return {
            'access_token': self.access_token,
            'expires_in': self.expires_in,
           'scope': self.scope,
            'token_type': 'Bearer',
            'expires_in_seconds': self.expires_in_seconds,
           'refresh_token': self.refresh_token,
           'scope_type': 'Bearer',
           'refresh_token_seconds': self.refresh_token_seconds,
            'client_id': self.client_id,
            'client_secret': self.client_secret,
            'code': self
----------
Solution: 

        url = urljoin(self.api_url, '/oauth2/token')

        oauth = OAuth2Session(
            client_id=self._client_id,
            redirect_uri=config.REDIRECT_URI,
        )

        token = oauth.fetch_token(
            token_url=url,
            code=self._auth_code,
            client_secret=self._client_secret,
        )

        return token
--------------------
Prompt: 
def xml_decode(string):
    """ Returns the string with special characters decoded.
    """
----------
Output: 

    if not string:
        return ""
    if not string.startswith("'") and not string.endswith("'"):
        return string
    return string[1:-1]

def xml_encode(string):
    """ Returns the string with special characters encoded.
    """
    if not string:
        return ""
    if not string.startswith('"') and not string.endswith("'"):
        return string
    return string[1:-1].encode('utf-8')

def xml_escape(string):
    """ Returns the string with special characters escaped.
    """
    if not string:
        return ""
    if not string.
----------
Solution: 

    string = string.replace("&amp;", "&")
    string = string.replace("&lt;",  "<")
    string = string.replace("&gt;",  ">")
    string = string.replace("&quot;","\"")
    string = string.replace("/", SLASH)
    return string
--------------------
Prompt: 
def convert_python_regex_to_ecma(value, flags=[]):
    """Convert Python regex to ECMA 262 regex.

    If given value is already ECMA regex it will be returned unchanged.

    :param string value: Python regex.
    :param list flags: List of flags (allowed flags: `re.I`, `re.M`)
    :return: ECMA 262 regex
    :rtype: str

    """
----------
Output: 

    if isinstance(value, str):
        return re.compile(value, flags)
    else:
        return value


def convert_to_bytes(value, flags=[]):
    """Convert value to bytes.

    :param bytes value: Value to be converted.
    :param list flags: List of flags (allowed flags: `re.I`, `re.M`)
    :return: Bytes value.
    :rtype: int

    """
    if isinstance(value, int):
        return value
    else:
        return int.from_bytes(value, flags)


def convert_to_bits(value
----------
Solution: 

    if is_ecma_regex(value):
        return value

    result_flags = [PYTHON_TO_ECMA_FLAGS[f] for f in flags]
    result_flags = ''.join(result_flags)

    return '/{value}/{flags}'.format(value=value, flags=result_flags)
--------------------
Prompt: 
def _independent_lattice(self, shape, lattice=None):
        """ Helper to construct the list of nodes and edges. """
----------
Output: 

        if lattice is None:
            lattice = self.lattice
        return [self.node(i) for i in shape]

    def _get_lattice_nodes(self, lattice):
        """
        Returns the list of nodes in the lattice.
        """
        if lattice.is_periodic:
            return [self.node(i) for i in range(self.n_nodes)]
        else:
            return [self.node(i) for i in lattice.get_nodes()]

    def _get_lattice_edges(self, lattice):
        """
        Returns the list of edges in the lattice.
       
----------
Solution: 

        I, J = shape

        if lattice is not None:
            end_I = min(I, max(lattice[..., 3])) - 1
            end_J = min(J, max(lattice[..., 4])) - 1
            unvisited_nodes = deque([(i, j, s)
                                     for i in range(end_I)
                                     for j in range(end_J)
                                     for s in self._start_states])
            lattice = lattice.tolist()
        else:
            lattice = []
            unvisited_nodes = deque([(0, 0, s) for s in self._start_states])
        lattice += _grow_independent_lattice(self._transitions, 
                                             self.n_states, (I, J), 
                                             unvisited_nodes)
        lattice = np.array(sorted(lattice), dtype='int64')
        return lattice
--------------------
Prompt: 
def _textOutput(self, gaObjects):
        """
        Prints out the specified Variant objects in a VCF-like form.
        """
----------
Output: 

        print("## INFO: vcf2pileup.py: textOutput() called\n")
        for gaObject in gaObjects:
            print("## %s\n" % gaObject.getName())
            _textOutput(gaObject.getVariantList())

    def _getVcfHeader(self, gaObjects):
        """
        Prints out the VCF header for the specified Variant object.
        """
        print("## INFO: vcf2pileup.py: getVcfHeader() called\n")
        for gaObject in gaObjects:
            print("## %s\
----------
Solution: 

        for variantAnnotation in gaObjects:
            print(
                variantAnnotation.id, variantAnnotation.variant_id,
                variantAnnotation.variant_annotation_set_id,
                variantAnnotation.created, sep="\t", end="\t")
            for effect in variantAnnotation.transcript_effects:
                print(effect.alternate_bases, sep="|", end="|")
                for so in effect.effects:
                    print(so.term, sep="&", end="|")
                    print(so.term_id, sep="&", end="|")
                print(effect.hgvs_annotation.transcript,
                      effect.hgvs_annotation.protein, sep="|", end="\t")
            print()
--------------------
Prompt: 
def security_group_rule_delete(auth=None, **kwargs):
    """
    Delete a security group

    name_or_id
        The unique ID of the security group rule

    CLI Example:

    .. code-block:: bash

        salt '*' neutronng.security_group_rule_delete name_or_id=1dcac318a83b4610b7a7f7ba01465548

    """
----------
Output: 

    if auth is None:
        auth = current_app.config['DEFAULT_AUTH']
    if auth is None:
        return 'ERROR: Must provide a valid auth'
    if 'name_or_id' in kwargs:
        name_or_id = kwargs['name_or_id']
    else:
        name_or_id = current_app.config['DEFAULT_TENANT_NAME']
    if name_or_id in ('1', '2', '3'):
        return 'ERROR: Cannot delete a security group with ID 1 or 2'
    if 'name_or_id'
----------
Solution: 

    cloud = get_operator_cloud(auth)
    kwargs = _clean_kwargs(**kwargs)
    return cloud.delete_security_group_rule(**kwargs)
--------------------
Prompt: 
def _ParseInformationalOptions(self, options):
    """Parses the informational options.

    Args:
      options (argparse.Namespace): command line arguments.
    """
----------
Output: 

    self._Parse(options.infoal_options, 'infoal_options')
    self._Parse(options.infoal_args, 'infoal_args')
    self._Parse(options.infoal_env_options, 'infoal_env_options')
    self._Parse(options.infoal_env_args, 'infoal_env_args')
    self._Parse(options.infoal_log_options, 'infoal_log_options')
    self._Parse(options.infoal_log_env_options, 'infoal_log_env_options')
----------
Solution: 

    self._debug_mode = getattr(options, 'debug', False)
    self._quiet_mode = getattr(options, 'quiet', False)

    if self._debug_mode and self._quiet_mode:
      logger.warning(
          'Cannot use debug and quiet mode at the same time, defaulting to '
          'debug output.')
--------------------
Prompt: 
def add(self, document):
        """
        Add a document to the database.
        """
----------
Output: 

        self.db.add(document)
        self.db.commit()
        return self.db.get(document.id)

    def get_document(self, document_id):
        """
        Get a document by its id.
        """
        return self.documents.get(document_id)

    def get_document_by_id(self, document_id):
        """
        Get a document by its id.
        """
        return self.documents.get(document_id)

    def get_documents_by_id(self, document_ids):
        """
        Get all documents.
       
----------
Solution: 

        docid = int(document.uniqueIdentifier())
        text = u' '.join(document.textParts())

        self.store.executeSQL(self.addSQL, (docid, text))
--------------------
Prompt: 
def generate_version(
    current_version: str, increment: str, prerelease: Optional[str] = None
) -> Version:
    """Based on the given increment a proper semver will be generated.

    For now the rules and versioning scheme is based on
    python's PEP 0440.
    More info: https://www.python.org/dev/peps/pep-0440/

    Example:
        PATCH 1.0.0 -> 1.0.1
        MINOR 1.0.0 -> 1.1.0
        MAJOR 1.0.0 -> 2.0.0
    """
----------
Output: 
.0
        PATCH 1.0.0 -> 2.0.1
        MINOR 1.1.0 -> 2.1.0
        MAJOR 2.0.0 -> 2.0.1
        PATCH 1.0.0 -> 2.0.1
        MINOR 2.1.0 -> 2.0.0
        MAJOR 2.1.0 -> 2.0.0
    """
    if prerelease:
        current_version = prerelease + " " + increment
    return Version(current_version)


def get_version_from_changelog(changelog: str)
----------
Solution: 

    pre_version = prerelease_generator(current_version, prerelease=prerelease)
    semver = semver_generator(current_version, increment=increment)
    # TODO: post version
    # TODO: dev version
    return Version(f"{semver}{pre_version}")
--------------------
Prompt: 
def vec(self):
        """:obj:`numpy.ndarray` : Vector representation for this camera.
        """
----------
Output: 

        return self._vector

    @vec.setter
    def vector(self, value):
        """:obj:`vec.Set` :obj:`value` :obj:`vec.Vec` representation for this camera.
        """
        self._vector = value

    @property
    def name(self):
        """:obj:`str` : The name of this camera.
        """
        return self._name

    @name.setter
    def name(self, value):
        """:obj:`str.Set` :obj:`value` :obj:`name` representation for this camera.
        """
        self._name = value

   
----------
Solution: 

        return np.r_[self.fx, self.fy, self.cx, self.cy, self.skew, self.height, self.width]
--------------------
Prompt: 
def AddFareObject(self, fare, problem_reporter=None):
    """Deprecated. Please use AddFareAttributeObject."""
----------
Output: 

    return self.AddFareAttributeObject(fare, 'fare_object', problem_reporter)

  def AddFareObjectAttribute(self, fare, name, value, problem_reporter=None):
    """Deprecated. Please use AddFareAttributeObjectAttribute."""
    return self.AddFareAttributeObjectAttributeObject(
        self.AddFareAttributeAttribute(fare, name, value, 'fare_object'),
        problem_reporter)

  def AddFareAttributeAttribute(self, fare, name, value, problem_reporter=None):
    """Deprecated. Please use Add
----------
Solution: 

    warnings.warn("No longer supported. The Fare class was renamed to "
                  "FareAttribute, and all related functions were renamed "
                  "accordingly.", DeprecationWarning)
    self.AddFareAttributeObject(fare, problem_reporter)
--------------------
Prompt: 
def get(self, path, default=None):
        """
        Returns given path value.

        :param path: Path name.
        :type path: unicode
        :param default: Default value if path is not found.
        :type default: object
        :return: Action.
        :rtype: QAction
        """
----------
Output: 

        return QAction(self.tr('Select'), self.tr('Select a layer'), self)

    def setData(self, path, value, default=None):
        """
        Sets given path value.

        :param path: Path name.
        :type path: unicode
        :param value: Value.
        :type value: unicode
        :param default: Default value if path is not found.
        :type default: object
        :return: self.
        :rtype: QAction
        """
        return QAction(self.tr('Set data'), self.tr('Set data'), self)

    def add
----------
Solution: 


        try:
            return self.__getitem__(path)
        except KeyError as error:
            return default
--------------------
Prompt: 
def pullup(self, pin, enabled):
        """Turn on the pull-up resistor for the specified pin if enabled is True,
        otherwise turn off the pull-up resistor.
        """
----------
Output: 

        if enabled:
            self.write_register(pin, 0)
        else:
            self.write_register(pin, 1)

    def pulldown(self, pin, enabled):
        """Turn on the pull-down resistor for the specified pin if enabled is True,
        otherwise turn off the pull-down resistor.
        """
        if enabled:
            self.write_register(pin, 0)
        else:
            self.write_register(pin, -1)

    def set_pin_state(self, pin, state):
        """Set the pin state for the specified pin.
       
----------
Solution: 

        self._validate_channel(pin)
        if enabled:
            self.gppu[int(pin/8)] |= 1 << (int(pin%8))
        else:
            self.gppu[int(pin/8)] &= ~(1 << (int(pin%8)))
        self._write_gppu()
--------------------
Prompt: 
def get_url_reports(self, resources):
        """Retrieves a scan report on a given URL.

        Args:
            resources: list of URLs.
        Returns:
            A dict with the URL as key and the VT report as value.
        """
----------
Output: 

        report = {}
        for resource in resources:
            if resource.url == self.url:
                report[resource.id] = resource.report
            else:
                report[resource.id] = resource.report_url
        return report

    def get_url_reports_by_id(self, report_id):
        """Retrieves a scan report on a given URL.

        Args:
            report_id: URL id.
        Returns:
            A dict with the URL as key and the VT report as value.
        """
        return get_url_reports(self.url_reports, report_id
----------
Solution: 

        api_name = 'virustotal-url-reports'

        (all_responses, resources) = self._bulk_cache_lookup(api_name, resources)
        resource_chunks = self._prepare_resource_chunks(resources, '\n')
        response_chunks = self._request_reports("resource", resource_chunks, 'url/report')
        self._extract_response_chunks(all_responses, response_chunks, api_name)

        return all_responses
--------------------
Prompt: 
def read_caffemodel(prototxt_fname, caffemodel_fname):
    """Return a caffe_pb2.NetParameter object that defined in a binary
    caffemodel file
    """
----------
Output: 

    with open(caffemodel_fname, 'rb') as f:
        caffe_pb2.NetParameter.ParseFromString(f.read())
    return caffe_pb2.NetParameter.from_proto(caffemodel["proto"])


def read_caffemodel_from_file(prototxt_fname, caffemodel_fname):
    """Return a caffe_pb2.NetParameter object that defined in a binary
    caffemodel file
    """
    with open(caffemodel_fname, 'rb') as f:
        caffe_pb2.NetParameter.ParseFromString(f
----------
Solution: 

    if use_caffe:
        caffe.set_mode_cpu()
        net = caffe.Net(prototxt_fname, caffemodel_fname, caffe.TEST)
        layer_names = net._layer_names
        layers = net.layers
        return (layers, layer_names)
    else:
        proto = caffe_pb2.NetParameter()
        with open(caffemodel_fname, 'rb') as f:
            proto.ParseFromString(f.read())
        return (get_layers(proto), None)
--------------------
Prompt: 
def fileToMD5(filename, block_size=256*128, binary=False):
    """
    A function that calculates the MD5 hash of a file.

    Args:
    -----
        filename: Path to the file.
        block_size: Chunks of suitable size. Block size directly depends on
            the block size of your filesystem to avoid performances issues.
            Blocks of 4096 octets (Default NTFS).
        binary: A boolean representing whether the returned info is in binary
            format or not.

    Returns:
    --------
        string: The  MD5 hash of the file.
    """
----------
Output: 

    if binary:
        return hashlib.md5(filename).hexdigest()
    else:
        return hashlib.md5(filename.encode('utf-8')).hexdigest()

def get_file_md5(filename, block_size=256*128, binary=False):
    """
    A function that returns the MD5 hash of a file.

    Args:
    -----
        filename: Path to the file.
        block_size: Chunks of suitable size. Block size directly depends on
            the block size of your filesystem to avoid performances issues.
            Blocks of 4096 octets (Default NTFS).
----------
Solution: 

    md5 = hashlib.md5()
    with open(filename,'rb') as f:
        for chunk in iter(lambda: f.read(block_size), b''):
             md5.update(chunk)
    if not binary:
        return md5.hexdigest()
    return md5.digest()
--------------------
Prompt: 
def _request(self, action='GET', url='/', data=None, query_params=None):
        """
        Requests to Hetzner by current session and returns the response.
        """
----------
Output: 

        if data is None:
            data = {}
        if query_params is None:
            query_params = {}
        if self.session.cookies:
            for c in self.session.cookies:
                if c.name == 'csrftoken':
                    query_params['csrfmiddlewaretoken'] = c.value
                elif c.name =='sessionid':
                    query_params['sessionid'] = c.value
                elif c.name == 'url':
                    url = c.value
                else:
                    raise ValueError("Unknown cookie: %s" % c.name)

        if self.session.cookies:
----------
Solution: 

        if data is None:
            data = {}
        if query_params is None:
            query_params = {}
        response = self.session.request(action, self.api[self.account]['endpoint'] + url,
                                        params=query_params, data=data)
        response.raise_for_status()
        return response
--------------------
Prompt: 
def get_networks(base_file, ipaddr_file):
    """ Gather network and host information from ipplan export files. """
----------
Output: 

    # TODO: Add support for multiple networks
    # TODO: Add support for multiple hosts
    # TODO: Add support for multiple hosts
    # TODO: Add support for multiple hosts
    # TODO: Add support for multiple hosts
    # TODO: Add support for multiple hosts
    # TODO: Add support for multiple hosts
    # TODO: Add support for multiple hosts
    # TODO: Add support for multiple hosts
    # TODO: Add support for multiple hosts
    # TODO: Add support for multiple hosts
    # TODO: Add support for multiple hosts
    # TODO: Add support for multiple hosts
    # TODO: Add support for multiple hosts
    #
----------
Solution: 

    networks = []

    base = open(base_file, 'r')

    csv_reader = csv.reader(base, delimiter='\t')

    buffer = ""
    for row in csv_reader:

        # Fixes quotation bug in ipplan exporter for base.txt
        if len(networks) > 0 and len(buffer) > 0:
            networks[-1]['comment'] += " ".join(buffer)
            buffer = ""
        if len(row) < 3:
            buffer = row
        else:

            network = {
                'network': ipaddress.ip_network("{}/{}".format(row[0], row[2])),
                'description': row[1],
                'hosts': [],
                'comment': ""
            }

            if len(row) > 3:
                network['additional'] = " ".join(row[3:])

            networks.append(network)

    base.close()

    ipaddr = open(ipaddr_file, 'r')

    csv_reader = csv.reader(ipaddr, delimiter='\t')
    for row in csv_reader:

        host = {
            'ipaddr': ipaddress.ip_address(row[0]),
            'user': row[1],
            'location': row[2],
            'description': row[3],
            'fqdn': row[4],
            'phone': row[5],
            'mac': row[6]
        }

        if len(row) > 7:
            host['additional'] = " ".join(row[7:])

        add_ip_to_net(networks, host)

    ipaddr.close()

    return networks
--------------------
Prompt: 
def bdd(*keywords):
    """
    Run tests matching keywords.
    """
----------
Output: 

    global _BDD_TESTS
    for kw in keywords:
        _BDD_TESTS.append(bdd_test(kw))
    return _BDD_TESTS

def bdd_test(name):
    """
    BDD tests are a test suite that is run in a specific order.
    """
    test = globals().get('BDD_%s' % name)
    if test is None:
        test = _BDD_TEST
    return test(test(test(keywords[0])) or test(keywords[1])

def bdd_keywords():
    """
    Run bdd
----------
Solution: 

    settings = _personal_settings().data
    _storybook().with_params(
        **{"python version": settings["params"]["python version"]}
    ).only_uninherited().shortcut(*keywords).play()
--------------------
Prompt: 
async def get_me(self, input_peer=False):
        """
        Gets "me" (the self user) which is currently authenticated,
        or None if the request fails (hence, not authenticated).

        Args:
            input_peer (`bool`, optional):
                Whether to return the :tl:`InputPeerUser` version or the normal
                :tl:`User`. This can be useful if you just need to know the ID
                of yourself.

        Returns:
            Your own :tl:`User`.
        """
----------
Output: 

        return None

    @abc.abstractmethod
    def get_me_by_id(self, input_peer_id):
        """
        Gets the self user object from your account.

        Args:
            input_peer_id (`int`, optional):
                The :tl:`InputPeerID` to return. This can be useful if you just
                need to know the ID of yourself.

        Returns:
            :class:`InputPeerUser`:
                The :tl:`InputPeerUser` object.
        """
        return None

    @abc.abstractmethod
    async def get_me_by_name(self, input_name
----------
Solution: 

        if input_peer and self._self_input_peer:
            return self._self_input_peer

        try:
            me = (await self(
                functions.users.GetUsersRequest([types.InputUserSelf()])))[0]

            self._bot = me.bot
            if not self._self_input_peer:
                self._self_input_peer = utils.get_input_peer(
                    me, allow_self=False
                )

            return self._self_input_peer if input_peer else me
        except errors.UnauthorizedError:
            return None
--------------------
Prompt: 
def parse_libxc_docs(path):
    """
    Parse libxc_docs.txt file, return dictionary with mapping:
    libxc_id --> info_dict
    """
----------
Output: 

    doc_dict = {}
    with open(path, 'r') as f:
        for line in f:
            if line.startswith('libxc'):
                doc_id, info_dict = line.strip().split(' ', 2)
                doc_dict[doc_id] = {}
                doc_dict[doc_id]['id'] = int(doc_id)
                doc_dict[doc_id]['name'] = info_dict.strip()
                doc_dict[doc_id]['id'] = int(doc_id)
                doc_dict[doc_id]['version'] =
----------
Solution: 

    def parse_section(section):
	d = {}
	for l in section:
	    key, value = l.split(":")
	    key = key.strip()
	    d[key] = value.strip()

	return int(d["Number"]), d

    d = OrderedDict()
    with open(path, "rt") as fh:
	section = []
	for line in fh:
	    if not line.startswith("-"):
		section.append(line)
	    else:
		num, entry = parse_section(section)
		assert num not in d
		d[num] = entry
		section = []
	assert not section

    return d
--------------------
Prompt: 
def _create_application_version_request(app_metadata, application_id, template):
    """
    Construct the request body to create application version.

    :param app_metadata: Object containing app metadata
    :type app_metadata: ApplicationMetadata
    :param application_id: The Amazon Resource Name (ARN) of the application
    :type application_id: str
    :param template: A packaged YAML or JSON SAM template
    :type template: str
    :return: SAR CreateApplicationVersion request body
    :rtype: dict
    """
----------
Output: 

    request_body = {
        'ApplicationName': application_id,
        'TemplateName': template,
        'VersionLabel': app_metadata.version_label,
        'VersionId': app_metadata.version_id,
        'IsDefaultVersion': True if app_metadata.is_default_version else False
    }
    if is_s3_enabled(app_metadata):
        request_body['VersionId'] = app_metadata.version_id
    if is_aws_enabled(app_metadata):
        request_body['IsDefaultVersion'] = True if app_metadata.is_
----------
Solution: 

    app_metadata.validate(['semantic_version'])
    request = {
        'ApplicationId': application_id,
        'SemanticVersion': app_metadata.semantic_version,
        'SourceCodeUrl': app_metadata.source_code_url,
        'TemplateBody': template
    }
    return {k: v for k, v in request.items() if v}
--------------------
Prompt: 
def crypt(word, salt=None, rounds=_ROUNDS_DEFAULT):
    """Return a string representing the one-way hash of a password, with a salt
    prepended.
    If ``salt`` is not specified or is ``None``, the strongest
    available method will be selected and a salt generated.  Otherwise,
    ``salt`` may be one of the ``crypt.METHOD_*`` values, or a string as
    returned by ``crypt.mksalt()``.
    """
----------
Output: 

    if salt is None:
        salt = get_random_bytes(8)
    return b2a_hex(sha1(word + salt).digest())


def gen_password(length=20, chars=string.letters + string.digits):
    """Generate a random password with the specified ``length`` and
    ``chars`` characters."""
    return ''.join(random.choice(chars) for _ in range(length))


def gen_salt(length=20, chars=string.letters + string.digits):
    """Generate a random salt with the specified ``length`` and
    ``
----------
Solution: 

    if salt is None or isinstance(salt, _Method):
        salt = mksalt(salt, rounds)

    algo, rounds, salt = extract_components_from_salt(salt)
    if algo == 5:
        hashfunc = hashlib.sha256
    elif algo == 6:
        hashfunc = hashlib.sha512
    else:
        raise ValueError('Unsupported algorithm, must be either 5 (sha256) or 6 (sha512)')

    return sha2_crypt(word, salt, hashfunc, rounds)
--------------------
Prompt: 
def generate_phonetic_representation(self, word):
        """
        Returns a generated phonetic representation for a word.

        :param str word: a word to be phoneticized.
        :return: A list of phonemes representing the phoneticized word.

        This method is used for words for which there is no pronunication
        entry in the CMU dictionary. The function generates a
        pronunication for the word in the standard CMU format. This can then
        be converted to a compact phonetic representation using
        modify_phonetic_representation().

        """
----------
Output: 

        if word in self.CMU_PHASES:
            return self.CMU_PHASES[word]
        else:
            return []

    def modify_phonetic_representation(self, phonetic_representation, word):
        """
        Modifies a phonetic representation for a word.

        :param str phonetic_representation: a phonememe representation
            of the word.

        This method is used for words for which there is no pronunication
        entry in the CMU dictionary. The function generates a
        compact representation for the phonememe in the standard CMU
        format. This can
----------
Solution: 

        with NamedTemporaryFile() as temp_file:
            # Write the word to a temp file
            temp_file.write(word)
            #todo - clean up this messy t2p path
            t2pargs = [os.path.abspath(os.path.join(os.path.dirname(__file__),'t2p/t2p')),
                       '-transcribe', os.path.join(data_path, 'cmudict.0.7a.tree'),
                       temp_file.name]
            temp_file.seek(0)
            output, error = subprocess.Popen(
                t2pargs, stdout=subprocess.PIPE,
                stderr=subprocess.PIPE
            ).communicate()
            output = output.split()
            phonetic_representation = output[1:]

        return phonetic_representation
--------------------
Prompt: 
def _evaluate(self,R,z,phi=0.,t=0.):
        """
        NAME:
           _evaluate
        PURPOSE:
           evaluate the potential at R,z
        INPUT:
           R - Galactocentric cylindrical radius
           z - vertical height
           phi - azimuth
           t - time
        OUTPUT:
           Phi(R,z)
        HISTORY:
           2013-09-08 - Written - Bovy (IAS)
        """
----------
Output: 

        R = np.atleast_1d(R)
        z = np.atleast_1d(z)
        phi = np.atleast_1d(phi)
        t = np.atleast_1d(t)
        if phi.shape!= (self.n_phi,self.n_azimuth):
            raise ValueError("phi must be a square matrix")
        if phi.shape[0]!= self.n_time:
            raise ValueError("time must be a scalar")
        if phi.shape[1]!= self.n_phi:
            raise ValueError("number of points must be
----------
Solution: 

        r2= R**2.+z**2.
        rb= nu.sqrt(r2+self.b2)
        return -1./(self.b+rb)
--------------------
Prompt: 
def category_helper(form_tag=True):
    """
    Category's form layout helper
    """
----------
Output: 

    if form_tag:
        return 'category_form_layout.html'
    else:
        return 'category_form.html'


@register.simple_tag
def category_form_layout(parser, token):
    """
    Category's form layout helper
    """
    if token.contents == 'category_form_layout.html':
        return 'category_form_layout.html'
    else:
        return 'category_form.html'


@register.simple_tag
def category_form_layout_js(parser, token):
    """
    Category's form layout helper
   
----------
Solution: 

    helper = FormHelper()
    helper.form_action = '.'
    helper.attrs = {'data_abide': ''}
    helper.form_tag = form_tag
    
    helper.layout = Layout(
        Row(
            Column(
                'title',
                css_class='small-12'
            ),
        ),
        Row(
            Column(
                'slug',
                css_class='small-12 medium-10'
            ),
            Column(
                'order',
                css_class='small-12 medium-2'
            ),
        ),
        Row(
            Column(
                'description',
                css_class='small-12'
            ),
        ),
        Row(
            Column(
                'visible',
                css_class='small-12'
            ),
        ),
        ButtonHolderPanel(
            Submit('submit', _('Submit')),
            css_class='text-right',
        ),
    )
    
    return helper
--------------------
Prompt: 
def load(self, modules):
        """Load Python modules and check their usability

        :param modules: list of the modules that must be loaded
        :return:
        """
----------
Output: 

        for module in modules:
            self.check_module(module)

    def check_module(self, module):
        """Check if the module is a valid module

        :param module: module name
        :return:
        """
        if module not in self.modules:
            raise ValueError("Module %s not found" % module)

    def load_all(self):
        """Load all modules

        :return:
        """
        for module in self.modules:
            self.load_module(module)

    def load_all_modules(self):
        """Load all modules

        :return:
        """
       
----------
Solution: 

        self.modules_assoc = []
        for module in modules:
            if not module.enabled:
                logger.info("Module %s is declared but not enabled", module.name)
                # Store in our modules list but do not try to load
                # Probably someone else will load this module later...
                self.modules[module.uuid] = module
                continue
            logger.info("Importing Python module '%s' for %s...", module.python_name, module.name)
            try:
                python_module = importlib.import_module(module.python_name)

                # Check existing module properties
                # Todo: check all mandatory properties
                if not hasattr(python_module, 'properties'):  # pragma: no cover
                    self.configuration_errors.append("Module %s is missing a 'properties' "
                                                     "dictionary" % module.python_name)
                    raise AttributeError
                logger.info("Module properties: %s", getattr(python_module, 'properties'))

                # Check existing module get_instance method
                if not hasattr(python_module, 'get_instance') or \
                        not isinstance(getattr(python_module, 'get_instance'),
                                       collections.Callable):  # pragma: no cover
                    self.configuration_errors.append("Module %s is missing a 'get_instance' "
                                                     "function" % module.python_name)
                    raise AttributeError

                self.modules_assoc.append((module, python_module))
                logger.info("Imported '%s' for %s", module.python_name, module.name)
            except ImportError as exp:  # pragma: no cover, simple protection
                self.configuration_errors.append("Module %s (%s) can't be loaded, Python "
                                                 "importation error: %s" % (module.python_name,
                                                                            module.name,
                                                                            str(exp)))
            except AttributeError:  # pragma: no cover, simple protection
                self.configuration_errors.append("Module %s (%s) can't be loaded, "
                                                 "module configuration" % (module.python_name,
                                                                           module.name))
            else:
                logger.info("Loaded Python module '%s' (%s)", module.python_name, module.name)
--------------------
Prompt: 
def __software_to_pkg_id(self, publisher, name, is_component, is_32bit):
        """
        Determine the Package ID of a software/component using the
        software/component ``publisher``, ``name``, whether its a software or a
        component, and if its 32bit or 64bit archiecture.

        Args:
            publisher (str): Publisher of the software/component.
            name (str): Name of the software.
            is_component (bool): True if package is a component.
            is_32bit (bool): True if the software/component is 32bit architecture.

        Returns:
            str: Package Id
        """
----------
Output: 
.

        Returns:
            int: Package ID of the specified software/component.

        Raises:
            ValueError: If the specified package is not a valid software/component.

        """
        if not is_component:
            raise ValueError("Invalid argument 'is_component'.")

        if not self.is_valid_package_name(name):
            raise ValueError("Invalid argument 'name'.")

        if not self.is_valid_package_publisher(publisher):
            raise ValueError("Invalid argument 'publisher'.")

        if not self.is_valid_package_is_32bit_component(is_32bit):
            raise ValueError
----------
Solution: 

        if publisher:
            # remove , and lowercase as , are used as list separators
            pub_lc = publisher.replace(',', '').lower()

        else:
            # remove , and lowercase
            pub_lc = 'NoValue'  # Capitals/Special Value

        if name:
            name_lc = name.replace(',', '').lower()
            # remove ,   OR we do the URL Encode on chars we do not want e.g. \\ and ,
        else:
            name_lc = 'NoValue'  # Capitals/Special Value

        if is_component:
            soft_type = 'comp'
        else:
            soft_type = 'soft'

        if is_32bit:
            soft_type += '32'  # Tag only the 32bit only

        default_pkg_id = pub_lc+'\\\\'+name_lc+'\\\\'+soft_type

        # Check to see if class was initialise with pkg_obj with a method called
        # to_pkg_id, and if so use it for the naming standard instead of the default
        if self.__pkg_obj and hasattr(self.__pkg_obj, 'to_pkg_id'):
            pkg_id = self.__pkg_obj.to_pkg_id(publisher, name, is_component, is_32bit)
            if pkg_id:
                return pkg_id

        return default_pkg_id
--------------------
Prompt: 
def to_netcdf(data, filename, *, group="posterior", coords=None, dims=None):
    """Save dataset as a netcdf file.

    WARNING: Only idempotent in case `data` is InferenceData

    Parameters
    ----------
    data : InferenceData, or any object accepted by `convert_to_inference_data`
        Object to be saved
    filename : str
        name or path of the file to load trace
    group : str (optional)
        In case `data` is not InferenceData, this is the group it will be saved to
    coords : dict (optional)
        See `convert_to_inference_data`
    dims : dict (optional)
        See `convert_to_inference_data`

    Returns
    -------
    str
        filename saved to
    """
----------
Output: 
 `convert_to_inference_data` for more information.
    dims : dict (optional)
        See `convert_to_inference_data` for more information.

    """
    if isinstance(data, InferenceData):
        data = data.copy()
    if isinstance(data, str):
        data = to_netcdf_string(data)
    if isinstance(group, str):
        group = to_netcdf(group)
    if isinstance(coords, dict):
        coords = coords.copy()
    if isinstance(dims, str):
        dims = to_netcdf_string(dims)
   
----------
Solution: 

    inference_data = convert_to_inference_data(data, group=group, coords=coords, dims=dims)
    file_name = inference_data.to_netcdf(filename)
    return file_name
--------------------
Prompt: 
def main():
    """Create and use a logger."""
----------
Output: 

    # Create a logger with a higher log level
    logger = logging.getLogger('py3canvas')
    logger.setLevel(logging.DEBUG)

    # Create a new window
    win = gtk.Window()

    # Create a new canvas
    canvas = gtk.DrawingArea()
    canvas.connect("draw", lambda *args: win.draw_rectangle(0, 0, 400, 400, fill="red"))

    # Create a new canvas widget
    win.connect("destroy", lambda *args: canvas.destroy())

    # Create a new canvas widget
    win.add(canvas)

    # Create a new
----------
Solution: 

    logger = setup_logger()

    logger.debug('a debug message')
    logger.info('an info message')
    logger.warning('a warning message')
    logger.error('an error message')
    logger.critical('a critical message')
--------------------
Prompt: 
def _node_to_dict(cls, node, json, json_fields):
        """ Helper method for ``get_tree``.
        """
----------
Output: 

        return {
            'id': node._id,
            'title': node.title,
            'category': node.category,
            'is_registration': node.is_registration,
            'is_collection': node.is_collection,
            'contributors': [
                contributor._id
                for contributor in node.contributors
                if contributor.is_registered
            ],
            'is_fork': True,
            'is_registration_of': node.is_registration_of(json_fields),
            'is_forked_from': node.is_forked_from,
            'is_registration_
----------
Solution: 

        if json:
            pk_name = node.get_pk_name()
            # jqTree or jsTree format
            result = {'id': getattr(node, pk_name), 'label': node.__repr__()}
            if json_fields:
                result.update(json_fields(node))
        else:
            result = {'node': node}
        return result
--------------------
Prompt: 
async def route_wrapper(self, route, request, context, request_args,
                            request_kw, *decorator_args, with_context=None,
                            **decorator_kw):
        """This is the function that is called when a route is decorated with
           your plugin decorator. Context will normally be None, but the user
           can pass use_context=True so the route will get the plugin
           context
        """
----------
Output: 

        if with_context is None:
            with_context = False
        if decorator_args is None:
            decorator_args = []
        if decorator_kw is None:
            decorator_kw = {}
        if with_context:
            return await self.plugin_manager.async_call(
                'GET', '/plugin/{0}/'.format(route.name),
                request,
                context,
                request_args,
                request_kw,
                *decorator_args,
                with_context=with_context,
                **decorator_kw)
        else:
            return await self.plugin_manager.async_
----------
Solution: 

        # by default, do nothing, just run the wrapped function
        if with_context:
            resp = route(request, context, *request_args, **request_kw)
        else:
            resp = route(request, *request_args, **request_kw)
        if isawaitable(resp):
            resp = await resp
        return resp
--------------------
Prompt: 
def convert(model, input_features, output_features):
    """Convert a boosted tree model to protobuf format.

    Parameters
    ----------
    decision_tree : GradientBoostingRegressor
        A trained scikit-learn tree model.

    input_feature: [str]
        Name of the input columns.

    output_features: str
        Name of the output column.

    Returns
    -------
    model_spec: An object of type Model_pb.
        Protobuf representation of the model
    """
----------
Output: 

    model_spec = Model_pb.ModelSpec(
        name=model.name,
        input_columns=model.input_columns,
        output_columns=model.output_columns,
        loss=model.loss,
        train_op=model.train_op,
        training_steps=len(input_features),
        steps=len(output_features),
        train_step_kwargs=dict(
            learning_rate=LEARNING_RATE,
            gradient_clipping_threshold=LEARNING_STEP_THRESHOLD,
            gradient_clipping_window_size=1,
           
----------
Solution: 

    if not(_HAS_SKLEARN):
        raise RuntimeError('scikit-learn not found. scikit-learn conversion API is disabled.')

    _sklearn_util.check_expected_type(model, _ensemble.GradientBoostingRegressor)
    def is_gbr_model(m):
        if len(m.estimators_) == 0:
            return False
        if hasattr(m, 'estimators_') and m.estimators_ is not None:
            for t in m.estimators_.flatten():
                if not hasattr(t, 'tree_') or t.tree_ is None:
                    return False
            return True
        else:
            return False

    _sklearn_util.check_fitted(model, is_gbr_model)

    base_prediction = model.init_.mean

    return _MLModel(_convert_tree_ensemble(model, input_features, output_features,
            base_prediction = base_prediction))
--------------------
Prompt: 
def from_pytime(cls, pytime):
        """
        Converts Python time object to sql time object
        ignoring timezone
        @param pytime: Python time object
        @return: sql time object
        """
----------
Output: 

        return pytime.replace(tzinfo=None)

    @classmethod
    def to_pytime(cls, pytime):
        """
        Converts SQL time object to python time object
        ignoring timezone
        @param pytime: SQL time object
        @return: pytime object
        """
        return pytime.replace(tzinfo=None)

    @classmethod
    def to_sql(cls, sql_type):
        """
        Converts a time object to a string
        @param sql_type: time object type
        @return: string
        """
        return sql_type.replace(":", "_")

    @classmethod
    def to
----------
Solution: 

        secs = pytime.hour * 60 * 60 + pytime.minute * 60 + pytime.second
        nsec = secs * 10 ** 9 + pytime.microsecond * 1000
        return cls(nsec=nsec)
--------------------
Prompt: 
def _get_mappedids(entry, graph):
        """
        Extract the Orphanet and UMLS ids as equivalences from the entry
        :param entry:
        :return:
        """
----------
Output: 

        orphet_id = entry.get('id')
        umls_id = entry.get('name')
        if orphet_id:
            return (orphet_id, umls_id)
        else:
            return None, None

    def _get_orphet_id(self, graph, entry):
        """
        Extract the Orphanet and UMLS ids as equivalences from the entry
        :param graph:
        :param entry:
        :return:
        """
        orphet_id = entry.get('id')
        if orph
----------
Solution: 

        model = Model(graph)
        omim_num = str(entry['mimNumber'])
        omim_curie = 'OMIM:' + omim_num
        orpha_mappings = []
        if 'externalLinks' in entry:
            links = entry['externalLinks']
            if 'orphanetDiseases' in links:
                # triple semi-colon delimited list of
                # double semi-colon delimited orphanet ID/disease pairs
                # 2970;;566;;Prune belly syndrome
                items = links['orphanetDiseases'].strip().split(';;;')
                for item in items:
                    orphdis = item.strip().split(';;')
                    orpha_num = orphdis[0].strip()
                    orpha_label = orphdis[2].strip()
                    orpha_curie = 'ORPHA:' + orpha_num
                    orpha_mappings.append(orpha_curie)
                    model.addClassToGraph(orpha_curie, orpha_label)
                    model.addXref(omim_curie, orpha_curie)

            if 'umlsIDs' in links:
                umls_mappings = links['umlsIDs'].split(',')
                for umls in umls_mappings:
                    umls_curie = 'UMLS:' + umls
                    model.addClassToGraph(umls_curie, None)
                    model.addXref(omim_curie, umls_curie)
--------------------
Prompt: 
def genes_with_homology_models(self):
        """DictList: All genes that have at least one homology model."""
----------
Output: 

        return self._genes.values()

    def get_homology_models(self, hom_id):
        """DictList: All genes that have at least one homology model."""
        return self._homology_models.get(hom_id, [])

    def add_het_model(self, het_id, model):
        """DictList: All genes that have at least one het model."""
        self._hets.append(model)
        self._genes[model.id].hom_id = het_id
        self._genes
----------
Solution: 

        return DictList(x for x in self.genes_with_structures if x.protein.num_structures_homology > 0)
--------------------
Prompt: 
def kill_session(self, target_session=None):
        """
        Kill the tmux session with ``$ tmux kill-session``, return ``self``.

        Parameters
        ----------
        target_session : str, optional
            target_session: str. note this accepts ``fnmatch(3)``. 'asdf' will
            kill 'asdfasd'.

        Returns
        -------
        :class:`Server`

        Raises
        ------
        :exc:`exc.BadSessionName`
        """
----------
Output: 

        if target_session is None:
            target_session = self.get_target_session()
        if target_session is None:
            raise BadSessionName("No tmux session")
        if target_session is None:
            raise BadSessionName("No tmux session")
        if target_session is None:
            raise BadSessionName("No tmux session")

        if target_session == 'all':
            return self.get_server_all()
        elif target_session == 'kill':
            return self.kill_server_all(target_session)
        elif target_session == 'kill
----------
Solution: 

        session_check_name(target_session)

        proc = self.cmd('kill-session', '-t%s' % target_session)

        if proc.stderr:
            raise exc.LibTmuxException(proc.stderr)

        return self
--------------------
Prompt: 
def iam_device_info(self, apdu):
        """Create a device information record based on the contents of an
        IAmRequest and put it in the cache."""
----------
Output: 

        return self.cache.add_iam_device_info(apdu)

    def get_iam_device_info(self, apdu):
        """Get the device information record for the given IAmRequest."""
        return self.cache.get_iam_iam_device_info(apdu)

    def get_iam_role_id(self, apdu):
        """Get the role ID for the given IAmRequest."""
        return self.cache.get_iam_role_id(apdu)

    def get_iam_role_name(self, apdu):
        """Get
----------
Solution: 

        if _debug: DeviceInfoCache._debug("iam_device_info %r", apdu)

        # make sure the apdu is an I-Am
        if not isinstance(apdu, IAmRequest):
            raise ValueError("not an IAmRequest: %r" % (apdu,))

        # get the device instance
        device_instance = apdu.iAmDeviceIdentifier[1]

        # get the existing cache record if it exists
        device_info = self.cache.get(device_instance, None)

        # maybe there is a record for this address
        if not device_info:
            device_info = self.cache.get(apdu.pduSource, None)

        # make a new one using the class provided
        if not device_info:
            device_info = self.device_info_class(device_instance, apdu.pduSource)

        # jam in the correct values
        device_info.deviceIdentifier = device_instance
        device_info.address = apdu.pduSource
        device_info.maxApduLengthAccepted = apdu.maxAPDULengthAccepted
        device_info.segmentationSupported = apdu.segmentationSupported
        device_info.vendorID = apdu.vendorID

        # tell the cache this is an updated record
        self.update_device_info(device_info)
--------------------
Prompt: 
def visualize_tensors(name, imgs, scale_func=lambda x: (x + 1.) * 128., max_outputs=1):
    """Generate tensor for TensorBoard (casting, clipping)

    Args:
        name: name for visualization operation
        *imgs: multiple tensors as list
        scale_func: scale input tensors to fit range [0, 255]

    Example:
        visualize_tensors('viz1', [img1])
        visualize_tensors('viz2', [img1, img2, img3], max_outputs=max(30, BATCH))
    """
----------
Output: 


    Args:
        name: name for operation
        *imgs: multiple tensors as list
        *scale_func: scale input tensors to fit range [0, 255]

    Returns:
        TensorBoard object
    """
    if len(imgs) == 1:
        return imgs[0]
    else:
        return visualize_tensor(name, imgs, max_outputs)


def visualize_tensors_from_csv(name, csv_file, scale_func=lambda x: (x + 1.) * 128., max_outputs=1):
    """Generate tensor for TensorBoard (cast
----------
Solution: 

    xy = scale_func(tf.concat(imgs, axis=2))
    xy = tf.cast(tf.clip_by_value(xy, 0, 255), tf.uint8, name='viz')
    tf.summary.image(name, xy, max_outputs=30)
--------------------
Prompt: 
def get_ip_mac_arp_list(auth, url, devid=None, devip=None):
    """
    function takes devid of specific device and issues a RESTFUL call to get the IP/MAC/ARP list
    from the target device.

    :param auth: requests auth object #usually auth.creds from auth pyhpeimc.auth.class

    :param url: base url of IMC RS interface #usually auth.url from pyhpeimc.auth.authclass

    :param devid: int or str value of the target device.

    :param devip: str of ipv4 address of the target device

    :return: list of dictionaries containing the IP/MAC/ARP list of the target device.

    :rtype: list

    >>> from pyhpeimc.auth import *

    >>> from pyhpeimc.plat.termaccess import *

    >>> auth = IMCAuth("http://", "10.101.0.203", "8080", "admin", "admin")

    >>> ip_mac_list = get_ip_mac_arp_list( auth.creds, auth.url, devid='10')

    >>> ip_mac_list = get_ip_mac_arp_list( auth.creds, auth.url, devip='10.101.0.221')

    >>> assert type(ip_mac_list) is list

    >>> assert 'deviceId' in ip_mac_list[0]

    """
----------
Output: 
 devip: int or str value of the target device.
    """
    url = url + "/arp"
    if devid:
        url = url + "/dev/" + str(devid) + "/ipaddr"
    if devip:
        url = url + "/ip/" + str(devip) + "/arp"
    return get_request(url, auth=auth)


def get_ip_mac_address(auth, devid=None, address=None):
    """
    function takes auth object and issues a RESTFUL call to get the IP/MAC address
    from the target device
----------
Solution: 

    if devip is not None:
        dev_details = get_dev_details(devip, auth, url)
        if isinstance(dev_details, str):
            print("Device not found")
            return 403
        else:
            devid = get_dev_details(devip, auth, url)['id']
    f_url = url + "/imcrs/res/access/ipMacArp/" + str(devid)
    response = requests.get(f_url, auth=auth, headers=HEADERS)
    try:
        if response.status_code == 200:
            ipmacarplist = (json.loads(response.text))
            if 'ipMacArp' in ipmacarplist:
                return ipmacarplist['ipMacArp']
            else:
                return ['this function is unsupported']
    except requests.exceptions.RequestException as error:
        return "Error:\n" + str(error) + " get_ip_mac_arp_list: An Error has occured"
--------------------
Prompt: 
def set_password_prompt(self, regex=None):
        """
        Defines a pattern that is used to monitor the response of the
        connected host for a password prompt.

        :type  regex: RegEx
        :param regex: The pattern that, when matched, causes an error.
        """
----------
Output: 

        self.password_prompt = regex

    def set_ssh_options(self, **kwargs):
        """
        Defines a set of ssh options that are passed to the underlying
        :class:`SSHClient <plumbum.machines.remote.RemoteClient>` instance.

        :type  kwargs: dict
        :param kwargs: The set of ssh options that, when matched, causes an
            error.
        """
        self.ssh_options = kwargs

    def set_ssh_key(self, **kwargs):
        """
        Defines a key that can be used to connect to the remote host.

        :type  kwargs
----------
Solution: 

        if regex is None:
            self.manual_password_re = regex
        else:
            self.manual_password_re = to_regexs(regex)
--------------------
Prompt: 
def get_context_object_name(self, object_list):
        """Get the name of the item to be used in the context.

        See original in ``django.views.generic.list.MultipleObjectMixin``.
        """
----------
Output: 

        if len(object_list) == 1:
            return object_list[0].name
        return '%s-%s' % (object_list[0].name, object_list[0].pk)

    def get_context_data(self, **kwargs):
        context = super(ItemAdmin, self).get_context_data(**kwargs)
        context['item'] = self.get_object()
        return context

    def get_urls(self):
        """Get the admin urls.

        This is used to generate URLs for admins: use {% url 'admin:index' %}
        to
----------
Solution: 

        if self.context_object_name:
            return self.context_object_name
        elif hasattr(object_list, 'model'):
            object_name = object_list.model._meta.object_name.lower()
            return smart_str('{0}_list'.format(object_name))
        else:
            return None
--------------------
Prompt: 
def get_experiment_summ_ids( self, coinc_event_id ):
		"""
		Gets all the experiment_summ_ids that map to a given coinc_event_id.
		"""
----------
Output: 

		return self.experiment_summ_ids( coinc_event_id )

	def experiment_summ_ids( self, coinc_event_id ):
		"""
		Gets all the experiment_summ_ids that map to a given coinc_event_id.
		"""
		return [ coinc_event_id ]

	def get_coinc_event_id( self, coinc_event_id ):
		"""
		Gets the coinc_event_id that maps to a given coinc_event_id.
		"""
	
----------
Solution: 

		experiment_summ_ids = []
		for row in self:
			if row.coinc_event_id == coinc_event_id:
				experiment_summ_ids.append(row.experiment_summ_id)
		if len(experiment_summ_ids) == 0:
			raise ValueError("'%s' could not be found in the experiment_map table" % coinc_event_id)
		return experiment_summ_ids
--------------------
Prompt: 
def register(self, auth_body=None, kind="user", bind_email=None,
                 username=None, password=None, device_id=None,
                 initial_device_display_name=None, inhibit_login=None):
        """Performs /register.

        Args:
            auth_body (dict): Authentication Params.
            kind (str): Specify kind of account to register. Can be 'guest' or 'user'.
            bind_email (bool): Whether to use email in registration and authentication.
            username (str): The localpart of a Matrix ID.
            password (str): The desired password of the account.
            device_id (str): ID of the client device.
            initial_device_display_name (str): Display name to be assigned.
            inhibit_login (bool): Whether to login after registration. Defaults to false.
        """
----------
Output: 
str): The localpart of a Password.
            device_id (str): The device ID.
            initial_device_display_name (str): The initial display name.
            inhibit_login (str): The inhibit login.
        """
        self.auth_body = auth_body
        self.kind = kind
        self.bind_email = bind_email
        self.username = username
        self.password = password
        self.account_id = device_id
        self.username_locked = False
        self.password_locked = False
        self.device_id = initial_device_
----------
Solution: 

        content = {}
        content["kind"] = kind
        if auth_body:
            content["auth"] = auth_body
        if username:
            content["username"] = username
        if password:
            content["password"] = password
        if device_id:
            content["device_id"] = device_id
        if initial_device_display_name:
            content["initial_device_display_name"] = \
                    initial_device_display_name
        if bind_email:
            content["bind_email"] = bind_email
        if inhibit_login:
            content["inhibit_login"] = inhibit_login
        return self._send(
            "POST",
            "/register",
            content=content,
            query_params={'kind': kind}
        )
--------------------
Prompt: 
def contrast(self, color, step):
        """if color is dark, will return a lighter one, otherwise darker"""
----------
Output: 

        if step == 0:
            return self.darker
        else:
            return self.lighter

    def set_contrast(self, contrast):
        """if contrast is a float, will set the step to a float, otherwise float"""
        if isinstance(contrast, float):
            self.contrast = contrast
        else:
            self.contrast = contrast

    def set_darker(self, darker):
        """if darker is a float, will set the step to a float, otherwise float"""
        if isinstance(darker, float):
            self.darker = darker
        else
----------
Solution: 

        hls = colorsys.rgb_to_hls(*self.rgb(color))
        if self.is_light(color):
            return colorsys.hls_to_rgb(hls[0], hls[1] - step, hls[2])
        else:
            return colorsys.hls_to_rgb(hls[0], hls[1] + step, hls[2])
--------------------
Prompt: 
def set(self, *components):
        """ Set the possible components of the block
        
        :param components: components to append Optionables or Composables
        """
----------
Output: 

        if not components:
            return
        
        if len(components) == 1:
            self.add_option(Option(self, 'component', components[0]))
        else:
            self.add_option(Option(self, 'component',''.join(components)))

    def get_option_by_name(self, name):
        """ Get an Option instance by its name
        
        :param name: name of the option
        """
        return self.options.get_by_name(name)

    def get_option_by_id(self, id):
        """ Get an Option instance by its id
----------
Solution: 

        self.reset()
        if len(components) == 1:
            self.append(components[0])
        else:
            for comp in components:
                self.append(comp)
--------------------
Prompt: 
def multidict(ordered_pairs):
    """Convert duplicate keys values to lists."""
----------
Output: 

    return [dict(pair) for pair in ordered_pairs]


def get_unique_values(values):
    """Return unique values."""
    seen = set()
    unique_values = set()
    for value in values:
        if value not in seen:
            unique_values.add(value)
            unique_values.add(seen.add(value))
    return unique_values


def get_unique_values_and_order(values):
    """Return unique values and order."""
    return [multidict(pair) for pair in values] + [multidict(order) for order
----------
Solution: 

    # read all values into lists
    d = defaultdict(list)
    for k, v in ordered_pairs:
        d[k].append(v)
    # unpack lists that have only 1 item
    dict_copy = deepcopy(d)
    for k, v in iteritems(dict_copy):
        if len(v) == 1:
            d[k] = v[0]
    return dict(d)
--------------------
Prompt: 
def search_form(context, search_model_names=None):
    """
    Includes the search form with a list of models to use as choices
    for filtering the search by. Models should be a string with models
    in the format ``app_label.model_name`` separated by spaces. The
    string ``all`` can also be used, in which case the models defined
    by the ``SEARCH_MODEL_CHOICES`` setting will be used.
    """
----------
Output: 

    search_model_names = SEARCH_MODEL_CHOICES if SEARCH_MODEL_CHOICES else []
    return context['choices'] + [(k, v) for k, v in get_app_choices(context)] \
        + [(k, v) for k, v in get_search_model_choices(context, k))] \
        + search_model_names


def get_app_choices(context):
    """
    Returns a list of tuples containing the name and model for
    ``app_label.model`` if there are choices for the given model.
    """
    app_labels
----------
Solution: 

    template_vars = {
        "request": context["request"],
    }
    if not search_model_names or not settings.SEARCH_MODEL_CHOICES:
        search_model_names = []
    elif search_model_names == "all":
        search_model_names = list(settings.SEARCH_MODEL_CHOICES)
    else:
        search_model_names = search_model_names.split(" ")
    search_model_choices = []
    for model_name in search_model_names:
        try:
            model = apps.get_model(*model_name.split(".", 1))
        except LookupError:
            pass
        else:
            verbose_name = model._meta.verbose_name_plural.capitalize()
            search_model_choices.append((verbose_name, model_name))
    template_vars["search_model_choices"] = sorted(search_model_choices)
    return template_vars
--------------------
Prompt: 
def update(self, fields=None, **kwargs):
        """Update the current entity.

        Make an HTTP PUT call to ``self.path('base')``. Return the response.

        :param fields: An iterable of field names. Only the fields named in
            this iterable will be updated. No fields are updated if an empty
            iterable is passed in. All fields are updated if ``None`` is passed
            in.
        :return: A ``requests.response`` object.

        """
----------
Output: 

        path = self.path('update')
        return self._put(path, fields, **kwargs)

    def update_with_body(self, path, body, **kwargs):
        """Update the current entity with a body.

        Make an HTTP PUT call to ``self.path('base')``. Return the response.

        :param path: The path to the resource.
        :param body: The body to update.
        :param kwargs: The optional arguments for the ``body`` parameter.
            See :py:meth:`update` for more details.
        :return: A ``requests.response`` object
----------
Solution: 

        kwargs = kwargs.copy()  # shadow the passed-in kwargs
        kwargs.update(self._server_config.get_client_kwargs())
        # a content upload is always multipart
        headers = kwargs.pop('headers', {})
        headers['content-type'] = 'multipart/form-data'
        kwargs['headers'] = headers
        return client.put(
            self.path('self'),
            fields,
            **kwargs
        )
--------------------
Prompt: 
def _get_elementwise_name_from_keras_layer(keras_layer):
    """
    Get the keras layer name from the activation name.
    """
----------
Output: 

    if keras_layer is None:
        return None
    else:
        return keras_layer.name


def _get_default_variable_store():
    """
    Return the default VariableStore instance used by keras.
    """
    if keras_defaults is None:
        from tensorflow.python.keras.initializers import VariableStore
        keras_defaults = VariableStore()
    return keras_defaults


def _get_partitioner(shape, num_partitions, num_rows, num_cols, padding='valid'):
    """
    Return a function to create a partitioner for the given shape.
    """
   
----------
Solution: 

    if isinstance(keras_layer, _keras.layers.Add):
        return 'ADD'
    elif isinstance(keras_layer, _keras.layers.Multiply):
        return 'MULTIPLY'
    elif isinstance(keras_layer, _keras.layers.Concatenate):
        if len(keras_layer.input_shape[0]) == 3 and (keras_layer.axis == 1 or keras_layer.axis == -2):
            return 'SEQUENCE_CONCAT'
        elif len(keras_layer.input_shape[0]) == 4 and (keras_layer.axis == 3 or keras_layer.axis == -1):
            return 'CONCAT'
        elif len(keras_layer.input_shape[0]) == 2 and (keras_layer.axis == 1 or keras_layer.axis == -1):
            return 'CONCAT'
        else:
            raise ValueError('Only channel and sequence concatenation are supported.')
    elif isinstance(keras_layer, _keras.layers.Dot):
        if len(keras_layer.input_shape[0]) == 2:
            if type(keras_layer.axes) is list or type(keras_layer.axes) is tuple:
                if len(keras_layer.axes) > 1:
                    raise ValueError('Only vector dot-product is supported.')
                else:
                    axis = keras_layer.axes[0]
            else:
                axis = keras_layer.axes
            if axis != -1 and axis != 1:
                 raise ValueError('Only vector dot-product is supported.')

            if keras_layer.normalize:
                return 'COS'
            else:
                return 'DOT'
        else:
            raise ValueError('Only vector dot-product is supported.')
    elif isinstance(keras_layer, _keras.layers.Maximum):
        return 'MAX'
    elif isinstance(keras_layer, _keras.layers.Average):
        return 'AVE'
    else:
        _utils.raise_error_unsupported_option(str(type(keras_layer)), 'merge',
                keras_layer.name)
--------------------
Prompt: 
def is_match(subject: Expression, pattern: Expression) -> bool:
    """
    Check whether the given *subject* matches given *pattern*.

    Args:
        subject:
            The subject.
        pattern:
            The pattern.

    Returns:
        True iff the subject matches the pattern.
    """
----------
Output: 

    return (
        subject.matches(pattern)
        and subject.matches_pattern(pattern)
        and subject.matches_pattern_str(pattern)
    )


def is_match_str(subject: Expression, pattern: Expression) -> bool:
    """
    Check whether the given *subject* matches given *pattern*.

    Args:
        subject:
            The subject.
        pattern:
            The pattern.

    Returns:
        True iff the subject matches the pattern.
    """
    return (
        pattern.matches_pattern_str(subject)
        and subject.matches_pattern_str_str(subject)
----------
Solution: 

    return any(True for _ in match(subject, pattern))
--------------------
Prompt: 
def list_cubes(self):
        """ List all available JSON files. """
----------
Output: 

        return self.get_json_files()

    def list_cubes_json(self):
        """ List all available JSON files as a JSON string. """
        return json.dumps(self.list_cubes(), cls=JSONEncoder)

    def list_templates(self):
        """ List all available template files. """
        return self.get_template_files()

    def list_templates_json(self):
        """ List all available JSON files as a JSON string. """
        return json.dumps(self.list_templates_json(), cls=JSONEncoder)

    def list_templates_template(self
----------
Solution: 

        for file_name in os.listdir(self.directory):
            if '.' in file_name:
                name, ext = file_name.rsplit('.', 1)
                if ext.lower() == 'json':
                    yield name
--------------------
Prompt: 
def run_forever(self, **kwargs):
        """
        Run the motor until another command is sent.
        """
----------
Output: 

        self.logger.debug("run_forever called")
        while True:
            try:
                self.logger.debug("sending command")
                self.send_command(self.command_dict[self.command])
                self.logger.debug("sent command")
                break
            except KeyError:
                self.logger.debug("no command")
                self.send_command(self.command_dict[self.command])
                self.logger.debug("no command")
                self.logger.debug("sending command")
                self.send_command(self.command_dict[self.send_command])
----------
Solution: 

        for key in kwargs:
            setattr(self, key, kwargs[key])
        self.command = self.COMMAND_RUN_FOREVER
--------------------
Prompt: 
def get_cell_rect(self, row, col, tab):
        """Returns rectangle of cell on canvas"""
----------
Output: 

        return tab.bbox(row, col)

    def get_cell_rect_color(self, row, col, color):
        """Returns rectangle of cell on canvas"""
        return self.get_cell_rect(row, col, color)

    def get_cell_rect_width(self, row, color):
        """Returns rectangle of cell on canvas"""
        return self.get_cell_rect(row, COL_CELL_W, color)

    def get_cell_rect_height(self, color):
        """Returns rectangle of cell on canvas"""
        return color.bbox(row
----------
Solution: 


        top_row = self.row_tb[0]
        left_col = self.col_rl[0]

        pos_x = self.x_offset
        pos_y = self.y_offset

        merge_area = self._get_merge_area((row, col, tab))

        for __row in xrange(top_row, row):
            __row_height = self.code_array.get_row_height(__row, tab)
            pos_y += __row_height

        for __col in xrange(left_col, col):
            __col_width = self.code_array.get_col_width(__col, tab)
            pos_x += __col_width

        if merge_area is None:
            height = self.code_array.get_row_height(row, tab)
            width = self.code_array.get_col_width(col, tab)
        else:
            # We have a merged cell
            top, left, bottom, right = merge_area
            # Are we drawing the top left cell?
            if top == row and left == col:
                # Set rect to merge area
                heights = (self.code_array.get_row_height(__row, tab)
                           for __row in xrange(top, bottom+1))
                widths = (self.code_array.get_col_width(__col, tab)
                          for __col in xrange(left, right+1))
                height = sum(heights)
                width = sum(widths)
            else:
                # Do not draw the cell because it is hidden
                return

        return pos_x, pos_y, width, height
--------------------
Prompt: 
def ssn(self, taxpayer_identification_number_type=SSN_TYPE):
        """ Generate a random United States Taxpayer Identification Number of the specified type.

        If no type is specified, a US SSN is returned.
        """
----------
Output: 

        if not ssn_type:
            ssn_type = random.choice(self.random_state_types)
        return self.random_state_generator.generate_random_state_number(
            taxpayer_identification_number_type=taxpayer_identification_number_type)

    def get_random_number_generator(self):
        """ Generate a random United States Taxpayer Identification Number generator.

        Returns:
            UnitedStatesTaxpayerIdentificationNumberGenerator:
        """
        return UnitedStatesTaxpayerNumberGenerator(
            self.random_
----------
Solution: 


        if taxpayer_identification_number_type == self.ITIN_TYPE:
            return self.itin()
        elif taxpayer_identification_number_type == self.EIN_TYPE:
            return self.ein()
        elif taxpayer_identification_number_type == self.SSN_TYPE:

            # Certain numbers are invalid for United States Social Security
            # Numbers. The area (first 3 digits) cannot be 666 or 900-999.
            # The group number (middle digits) cannot be 00. The serial
            # (last 4 digits) cannot be 0000.

            area = self.random_int(min=1, max=899)
            if area == 666:
                area += 1
            group = self.random_int(1, 99)
            serial = self.random_int(1, 9999)

            ssn = "{0:03d}-{1:02d}-{2:04d}".format(area, group, serial)
            return ssn

        else:
            raise ValueError("taxpayer_identification_number_type must be one of 'SSN', 'EIN', or 'ITIN'.")
--------------------
Prompt: 
def _merge_list_of_dict(first, second, prepend=True):
    """
    Merge lists of dictionaries.
    Each element of the list is a dictionary having one single key.
    That key is then used as unique lookup.
    The first element list has higher priority than the second.
    When there's an overlap between the two lists,
    it won't change the position, but the content.
    """
----------
Output: 

    if not prepend:
        return first + second
    else:
        return first + second + [None] + second + [None]


def _get_content_type_from_path(path):
    """
    Get content type from path.
    """
    return mimetypes.guess_type(path)[0] or 'application/octet-stream'


def _get_content_type_from_headers(headers):
    """
    Get content type from headers.
    """
    content_type = headers.get('content-type')
    if not content_type:
        content_type = _
----------
Solution: 

    first = _cleanup(first)
    second = _cleanup(second)
    if not first and not second:
        return []
    if not first and second:
        return second
    if first and not second:
        return first
    # Determine overlaps
    # So we dont change the position of the existing terms/filters
    overlaps = []
    merged = []
    appended = []
    for ele in first:
        if _lookup_element(second, ele.keys()[0]):
            overlaps.append(ele)
        elif prepend:
            merged.append(ele)
        elif not prepend:
            appended.append(ele)
    for ele in second:
        ele_key = ele.keys()[0]
        if _lookup_element(overlaps, ele_key):
            # If theres an overlap, get the value from the first
            # But inserted into the right position
            ele_val_first = _lookup_element(first, ele_key)
            merged.append({ele_key: ele_val_first})
        else:
            merged.append(ele)
    if not prepend:
        merged.extend(appended)
    return merged
--------------------
Prompt: 
def update_ref(profile, ref, sha):
    """Point a ref to a new SHA.

    Args:

        profile
            A profile generated from ``simplygithub.authentication.profile``.
            Such profiles tell this module (i) the ``repo`` to connect to,
            and (ii) the ``token`` to connect with.

        ref
            The ref to update, e.g., ``heads/my-feature-branch``.

        sha
            The SHA of the commit to point the ref to.

    Returns
        A dict with data about the ref.

    """
----------
Output: 

    data = {}
    data['ref'] = repo.get_ref(ref).get_data()
    data['sha'] = sha
    return data


def update_branch(branch, profile, token):
    """Point a branch to a new profile.

    Args:

        branch
            A branch to point.

        profile
            A profile generated from ``simplygithub.authentication.profile``.
            Such profiles tell this module (i) the ``branch`` to connect to,
            and (ii) the ``token`` to connect with.

    Returns
        A dict with data about the branch.

    """
    data =
----------
Solution: 

    resource = "/refs/" + ref
    payload = {"sha": sha}
    data = api.patch_request(profile, resource, payload)
    return prepare(data)
--------------------
Prompt: 
def run_analysis(self, argv):
        """Run this analysis"""
----------
Output: 

        self.logger.info("Running analysis")
        self.logger.info("argv = %s" % argv)
        self.logger.debug("Analysis parameters = %s" % self.analysis_parameters)
        self.logger.debug("Analysis results = %s" % self.analysis_results)
        self.logger.debug("Analysis results_dir = %s" % self.analysis_results_dir)
        self.logger.debug("Analysis_dir = %s" % self.analysis_results_dir)
        self.logger.debug("Analysis_dir = %s" % self
----------
Solution: 

        args = self._parser.parse_args(argv)

        name_keys = dict(target_type=args.ttype,
                         target_name=args.target,
                         sim_name=args.sim,
                         fullpath=True)

        orig_dir = NAME_FACTORY.targetdir(**name_keys)
        dest_dir = NAME_FACTORY.sim_targetdir(**name_keys)
        self.copy_target_dir(orig_dir, dest_dir,
                             args.roi_baseline, args.extracopy)
--------------------
Prompt: 
def image_information_response(self):
        """Parse image information request and create response."""
----------
Output: 

        return self._parse_image_info(self._image_info_response)

    def _parse_image_info(self, response):
        """Parse image info request."""
        image_info = {}

        if response.status_code == 200:
            image_info['image_url'] = response.json()['image_url']
            image_info['image_url_https'] = response.json()['image_url_https']
            image_info['image_url_https_https'] = response.json()['image_url_https_https']
            image_info['image_url
----------
Solution: 

        dr = degraded_request(self.identifier)
        if (dr):
            self.logger.info("image_information: degraded %s -> %s" %
                             (self.identifier, dr))
            self.degraded = self.identifier
            self.identifier = dr
        else:
            self.logger.info("image_information: %s" % (self.identifier))
        # get size
        self.manipulator.srcfile = self.file
        self.manipulator.do_first()
        # most of info.json comes from config, a few things specific to image
        info = {'tile_height': self.config.tile_height,
                'tile_width': self.config.tile_width,
                'scale_factors': self.config.scale_factors
                }
        # calculate scale factors if not hard-coded
        if ('auto' in self.config.scale_factors):
            info['scale_factors'] = self.manipulator.scale_factors(
                self.config.tile_width, self.config.tile_height)
        i = IIIFInfo(conf=info, api_version=self.api_version)
        i.server_and_prefix = self.server_and_prefix
        i.identifier = self.iiif.identifier
        i.width = self.manipulator.width
        i.height = self.manipulator.height
        if (self.api_version >= '2.0'):
            # FIXME - should come from manipulator
            i.qualities = ["default", "color", "gray"]
        else:
            # FIXME - should come from manipulator
            i.qualities = ["native", "color", "gray"]
        i.formats = ["jpg", "png"]  # FIXME - should come from manipulator
        if (self.auth):
            self.auth.add_services(i)
        return self.make_response(i.as_json(),
                                  headers={"Content-Type": self.json_mime_type})
--------------------
Prompt: 
def submodules(self):
        """
        Returns all documented sub-modules in the module sorted
        alphabetically as a list of `pydoc.Module`.
        """
----------
Output: 

        modules = []
        for name, module in self.modules.items():
            if not module.__doc__:
                modules.append((name, module))
        return modules

    def get_submodules(self, name):
        """
        Returns all documented modules in the module sorted
        alphabetically as a list of module names.
        """
        return sorted(self.submodules.get(name, []))

    def get_object(self, name):
        """
        Returns all documented objects in the module sorted
        alphabetically as a list of `pydoc.Text` objects.
        """
        objects = []
        for name,
----------
Solution: 

        p = lambda o: isinstance(o, Module) and self._docfilter(o)
        return sorted(filter(p, self.doc.values()))
--------------------
Prompt: 
def rebin(a, newshape):
    """Rebin an array to a new shape."""
----------
Output: 

    # Only supports int32 for now
    if a.dtype == 'int32':
        return np.lib.stride_tricks.as_strided(a, shape=newshape)
    # Otherwise, fall back to int64
    else:
        return np.lib.stride_tricks.as_strided(a, shape=newshape).copy()


def _get_strides_from_shape(shape):
    """Return strides from the shape of a tensor.

    Parameters
    ----------
    shape : tuple of int
        The shape of the tensor.

    Returns
    -------
    tuple of int
----------
Solution: 


    slices = [slice(0, old, float(old)/new)
              for old, new in zip(a.shape, newshape)]
    coordinates = numpy.mgrid[slices]
    # choose the biggest smaller integer index
    indices = coordinates.astype('i')
    return a[tuple(indices)]
--------------------
Prompt: 
def interpolate2dStructuredPointSpreadIDW(grid, mask, kernel=15, power=2,
                                          maxIter=1e5, copy=True):
    """
    same as interpolate2dStructuredIDW but using the point spread method
    this is faster if there are bigger connected masked areas and the border
    length is smaller

    replace all values in [grid] indicated by [mask]
    with the inverse distance weighted interpolation of all values within
    px+-kernel

    [power] -> distance weighting factor: 1/distance**[power]
    [copy] -> False: a bit faster, but modifies 'grid' and 'mask'
    """
----------
Output: 
 but uses the same method as interpolate2dStructuredIDW

    """
    # find the nearest neighbor of the grid
    # px = np.array(grid)
    # mask = np.array(mask)
    # kernel = np.array(kernel)
    # if copy:
    #     return np.array(grid), np.array(mask)
    # else:
    #     return np.array(grid), np.array(mask)
    # # find the nearest kernel
    # # kernel = np.array(kernel)
    # # if power > 0:
    # #     return np.array
----------
Solution: 

    assert grid.shape == mask.shape, 'grid and mask shape are different'

    border = np.zeros(shape=mask.shape, dtype=np.bool)
    if copy:
        # copy mask as well because if will be modified later:
        mask = mask.copy()
        grid = grid.copy()
    return _calc(grid, mask, border, kernel, power, maxIter)
--------------------
Prompt: 
def converged_ionic(self):
        """
        Checks that ionic step convergence has been reached, i.e. that vasp
        exited before reaching the max ionic steps for a relaxation run
        """
----------
Output: 

        self.converged = True
        self.converged_ionic_steps = 0
        self.converged_ionic_steps_max = self.max_ionic_steps
        self.converged_ionic_steps_converged = 0
        self.converged_ionic_steps_converged_max = self.converged_ionic_steps
        return self.converged_ionic_steps >= self.converged_ionic_steps_max

    def get_potcar_symbols(self):
        """
        Returns the list of symbols that are present in the v
----------
Solution: 

        nsw = self.parameters.get("NSW", 0)
        return nsw <= 1 or len(self.ionic_steps) < nsw
--------------------
Prompt: 
def object_to_bytes(obj):
    """
    Convert a object to a bytearray or call get_raw() of the object
    if no useful type was found.
    """
----------
Output: 

    if isinstance(obj, dict):
        return obj
    elif isinstance(obj, list):
        return bytearray(obj)
    elif isinstance(obj, str):
        return obj.encode('utf-8')
    else:
        return bytearray(obj)


def get_raw_bytes(obj):
    """
    Convert a list of bytes to a bytearray or call get_raw() of the object
    if no useful type was found.
    """
    if isinstance(obj, dict):
        return obj
    elif isinstance(obj, list):
        return bytearray(obj)
    elif isinstance(obj, str):
        return
----------
Solution: 

    if isinstance(obj, str):
        return bytearray(obj, "UTF-8")
    elif isinstance(obj, bool):
        return bytearray()
    elif isinstance(obj, int):
        return pack("<L", obj)
    elif obj is None:
        return bytearray()
    elif isinstance(obj, bytearray):
        return obj
    else:
        return obj.get_raw()
--------------------
Prompt: 
def login(self):
        """
        View function to log a user in. Supports html and json requests.
        """
----------
Output: 

        if not self.is_browser_on_page(HomePage):
            self.location('/login', wait_for_page_load=False)

    def logout(self):
        """
        Log out function
        """
        if self.is_browser_on_page(HomePage):
            self.location('/logout', wait_for_page_load=False)
        else:
            self.location('/logout', wait_for_page_load=True)

    def get_current_user(self):
        """
        Returns the currently logged in user.
        """
        return self.get('/users
----------
Solution: 

        form = self._get_form('SECURITY_LOGIN_FORM')
        if form.validate_on_submit():
            try:
                self.security_service.login_user(form.user, form.remember.data)
            except AuthenticationError as e:
                form._errors = {'_error': [str(e)]}
            else:
                self.after_this_request(self._commit)
                if request.is_json:
                    return self.jsonify({'token': form.user.get_auth_token(),
                                         'user': form.user})
                self.flash(_('flask_unchained.bundles.security:flash.login'),
                           category='success')
                return self.redirect('SECURITY_POST_LOGIN_REDIRECT_ENDPOINT')
        else:
            # FIXME-identity
            identity_attrs = app.config.SECURITY_USER_IDENTITY_ATTRIBUTES
            msg = f"Invalid {', '.join(identity_attrs)} and/or password."

            # we just want a single top-level form error
            form._errors = {'_error': [msg]}
            for field in form._fields.values():
                field.errors = None

        if form.errors and request.is_json:
            return self.jsonify({'error': form.errors.get('_error')[0]},
                                code=HTTPStatus.UNAUTHORIZED)

        return self.render('login',
                           login_user_form=form,
                           **self.security.run_ctx_processor('login'))
--------------------
Prompt: 
def _place_tables_section(skeleton_section, sheet, keys_section):
    """
    Place data into skeleton for either a paleo or chron section.
    :param dict skeleton_section: Empty or current progress of skeleton w/ data
    :param dict sheet: Sheet metadata
    :param list keys_section: Paleo or Chron specific keys
    :return dict: Skeleton section full of data
    """
----------
Output: 

    progress = 0
    if not keys_section:
        keys_section = [
            "paleo_id",
            "paleo_name",
            "progress",
            "paleo_progress",
            "paleo_progress_percent",
            "progress_percent",
            "progress_percent_percent",
            "progress_percent_percent",
            "progress_percent_percent",
            "progress_percent_percent",
            "progress_percent_percent",
            "progress_percent_percent",
            "progress_percent_percent",
            "progress_percent_percent",
            "progress
----------
Solution: 

    logger_excel.info("enter place_tables_section")
    try:
        logger_excel.info("excel: place_tables_section: placing table: {}".format(sheet["new_name"]))
        new_name = sheet["new_name"]
        logger_excel.info("placing_tables_section: {}".format(new_name))
        # get all the sheet metadata needed for this function
        idx_pc = sheet["idx_pc"] - 1
        idx_model = sheet["idx_model"]
        idx_table = sheet["idx_table"]
        table_type = sheet["table_type"]
        data = sheet["data"]
        # paleoMeas or chronMeas key
        key_1 = keys_section[0]
        # paleoModel or chronModel key
        key_2 = keys_section[1]
        # Is this a measurement, or distribution table?
        if idx_table:
            # Yes, a table idx exists, so decrement it.
            idx_table = sheet["idx_table"] - 1
        # Is this a ensemble, dist, or summary table?
        if idx_model:
            # Yes, a model idx exists, so decrement it.
            idx_model -= 1
    except Exception as e:
        logger_excel.debug("excel: place_tables_section: error during setup, {}".format(e))

    # If it's measurement table, it goes in first.
    try:
        if table_type == "measurement":
            skeleton_section[idx_pc][key_1][idx_table] = data
        # Other types of tables go one step below
        elif table_type in ["ensemble", "distribution", "summary"]:
            if table_type == "summary":
                skeleton_section[idx_pc][key_2][idx_model]["summaryTable"] = data
            elif table_type == "ensemble":
                skeleton_section[idx_pc][key_2][idx_model]["ensembleTable"] = data
            elif table_type == "distribution":
                skeleton_section[idx_pc][key_2][idx_model]["distributionTable"][idx_table] = data
    except Exception as e:
        logger_excel.warn("excel: place_tables_section: Unable to place table {}, {}".format(new_name, e))
    logger_excel.info("exit place_tables_section")
    return skeleton_section
--------------------
Prompt: 
def crossValidation(self,seed=0,n_folds=10,fullVector=True,verbose=None,D=None,**keywords):
        """
        Split the dataset in n folds, predict each fold after training the model on all the others

        Args:
            seed:        seed
            n_folds:     number of folds to train the model on
            fullVector:  Bolean indicator, if true it stops if no convergence is observed for one of the folds, otherwise goes through and returns a pheno matrix with missing values
            verbose:     if true, prints the fold that is being used for predicitons
            **keywords:  params to pass to the function optimize
        Returns:
            Matrix of phenotype predictions [N,P]
        """
----------
Output: 

            D:           if not None, this is the training set, if not None, the model will be trained on all the others
        """
        if D is None:
            D = self.D
        if verbose is None:
            verbose = self.verbose
        if self.model is None:
            self.model = self.createModel()
        if self.model is None:
            self.model = self.model
        if fullVector:
            return self.model.crossValidation(seed,n_folds,D,**keywords)
        else:
            return self.model.crossValidation(seed,n_folds
----------
Solution: 

        verbose = dlimix.getVerbose(verbose)

        # split samples into training and test
        sp.random.seed(seed)
        r = sp.random.permutation(self.Y.shape[0])
        nfolds = 10
        Icv = sp.floor(((sp.ones((self.Y.shape[0]))*nfolds)*r)/self.Y.shape[0])

        RV = {}
        if self.P==1:  RV['var'] = sp.zeros((nfolds,self.n_randEffs))
        else:          RV['var'] = sp.zeros((nfolds,self.P,self.n_randEffs))

        Ystar = sp.zeros_like(self.Y)

        for fold_j in range(n_folds):

            if verbose:
                print((".. predict fold %d"%fold_j))

            Itrain  = Icv!=fold_j
            Itest   = Icv==fold_j
            Ytrain  = self.Y[Itrain,:]
            Ytest   = self.Y[Itest,:]
            vc = VarianceDecomposition(Ytrain)
            vc.setTestSampleSize(Itest.sum())
            for term_i in range(self.n_fixedEffs):
                F      = self.vd.getFixed(term_i)
                Ftest  = F[Itest,:]
                Ftrain = F[Itrain,:]
                if self.P>1:    A = self.vd.getDesign(term_i)
                else:           A = None
                vc.addFixedEffect(F=Ftrain,Ftest=Ftest,A=A)
            for term_i in range(self.n_randEffs):
                if self.P>1:
                    tct  = self.trait_covar_type[term_i]
                    rank = self.rank[term_i]
                    ftc  = self.fixed_tc[term_i]
                    jitt = self.jitter[term_i]
                    if tct=='lowrank_diag1' or tct=='freeform1':
                        d = D[fold_j,:,term_i]
                    else:
                        d = None
                else:
                    tct  = None
                    rank = None
                    ftc  = None
                    jitt = None
                    d    = None
                if term_i==self.noisPos:
                    vc.addRandomEffect(is_noise=True,trait_covar_type=tct,rank=rank,jitter=jitt,fixed_trait_covar=ftc,d=d)
                else:
                    R = self.vd.getTerm(term_i).getK()
                    Rtrain = R[Itrain,:][:,Itrain]
                    Rcross = R[Itrain,:][:,Itest]
                    vc.addRandomEffect(K=Rtrain,Kcross=Rcross,trait_covar_type=tct,rank=rank,jitter=jitt,fixed_trait_covar=ftc,d=d)
            conv = vc.optimize(verbose=False,**keywords)
            if self.P==1:
                RV['var'][fold_j,:] = vc.getVarianceComps()[0,:]
            else:
                RV['var'][fold_j,:,:] = vc.getVarianceComps()

            if fullVector:
                assert conv, 'VarianceDecompositon:: not converged for fold %d. Stopped here' % fold_j
            if conv:
                Ystar[Itest,:] = vc.predictPhenos()
            else:
                warnings.warn('not converged for fold %d' % fold_j)
                Ystar[Itest,:] = sp.nan

        return Ystar,RV
--------------------
Prompt: 
def load_pubkeys(loadpath, pubkeys):

    """Append the file contents in loadpath directory onto pubkeys list"""
----------
Output: 


    # read in the public key file
    with open(loadpath + ".pub", 'a') as f:
        f.write(pubkeys)


def load_privatekeys(loadpath):

    """Read in the private key file"""

    with open(loadpath + ".private", 'r') as f:
        privatekeys = f.read()


def save_privatekeys(loadpath, privatekeys):

    """Write out the file contents in loadpath directory onto private key list"""

    # write out the file contents
    with open(loadpath + ".private", 'w') as f
----------
Solution: 


    filenames = os.listdir(loadpath)
    logger.debug('loading authorized pubkeys {0}'.format(filenames))
    for filename in filenames:
        pubkeys.append(open(join(loadpath, filename)).read())
--------------------
Prompt: 
def main():
    """
    Run the main CLI program.

    Initializes the stack, parses command line arguments, and fires requested
    logic.
    """
----------
Output: 

    # Parse command line arguments
    parser = argparse.ArgumentParser(description='Run the Mesos test suite.')
    parser.add_argument('-c', '--config', help='Path to Mesos test suite config file', required=True)
    parser.add_argument('-p', '--port', help='Port to run Mesos test suite on', type=int, default=9090)
    parser.add_argument('-s', '--skip', help='Skip the specified number of tests', type=int, default=0)
    parser.add_argument('-d', '--debug', help='Enable
----------
Solution: 

    plugins = DefaultPluginManager()
    plugins.load_plugins()

    parser, _ = create_argparser()
    # add the updater protocol options to the CLI:
    for kls in updater_classes():
        kls.register_arguments(parser)

    for kls in detector_classes():
        kls.register_arguments(parser)

    # add the plugin options to the CLI:
    from os import environ
    plugins.options(parser, environ)

    args = parser.parse_args()

    if args.debug:
        args.verbose_count = 5  # some high number

    log_level = max(int(logging.WARNING / 10) - args.verbose_count, 0) * 10
    # print(log_level)
    logging.basicConfig(level=log_level, format="%(levelname)s %(message)s")
    # logging.debug("args %r", args)

    if args.version:
        from . import __version__
        print("dyndnsc %s" % __version__)  # noqa
        return 0

    # silence 'requests' logging
    requests_log = logging.getLogger("requests")
    requests_log.setLevel(logging.WARNING)

    logging.debug(parser)
    cfg = get_configuration(args.config)

    if args.listpresets:
        list_presets(cfg)
        return 0

    if args.config:
        collected_configs = collect_config(cfg)
    else:
        parsed_args = parse_cmdline_args(args, updater_classes().union(detector_classes()))
        logging.debug("parsed_args %r", parsed_args)

        collected_configs = {
            "cmdline": {
                "interval": int(args.sleeptime)
            }
        }
        collected_configs["cmdline"].update(parsed_args)

    plugins.configure(args)
    plugins.initialize()

    logging.debug("collected_configs: %r", collected_configs)
    dyndnsclients = []
    for thisconfig in collected_configs:
        logging.debug("Initializing client for '%s'", thisconfig)
        # done with options, bring on the dancing girls
        dyndnsclient = getDynDnsClientForConfig(
            collected_configs[thisconfig], plugins=plugins)
        if dyndnsclient is None:
            return 1
        # do an initial synchronization, before going into endless loop:
        dyndnsclient.sync()
        dyndnsclients.append(dyndnsclient)

    run_forever_callable = partial(run_forever, dyndnsclients)

    if args.daemon:
        import daemonocle
        daemon = daemonocle.Daemon(worker=run_forever_callable)
        daemon.do_action("start")
        args.loop = True

    if args.loop:
        run_forever_callable()

    return 0
--------------------
Prompt: 
def _get_buildout_ver(directory='.'):
    """Check for buildout versions.

    In any cases, check for a version pinning
    Also check for buildout.dumppickedversions which is buildout1 specific
    Also check for the version targeted by the local bootstrap file
    Take as default buildout2

    directory
        directory to execute in
    """
----------
Output: 

    if not os.path.isdir(directory):
        raise ValueError('Directory %s not found' % directory)
    ver = None
    for line in os.popen('git ls-tree -r %s' % directory):
        if line.startswith('version'):
            ver = line.split()[2]
            break
    if not ver:
        raise ValueError('Cannot find git version')
    return ver


def _get_git_version(directory='.'):
    """Get the git version.

    In any cases, check for a version pinning
    Also get the version from the local bootstrap file
    Take as
----------
Solution: 

    directory = os.path.abspath(directory)
    buildoutver = 2
    try:
        files = _find_cfgs(directory)
        for f in files:
            with salt.utils.files.fopen(f) as fic:
                buildout1re = re.compile(r'^zc\.buildout\s*=\s*1', RE_F)
                dfic = salt.utils.stringutils.to_unicode(fic.read())
                if (
                        ('buildout.dumppick' in dfic)
                        or
                        (buildout1re.search(dfic))
                ):
                    buildoutver = 1
        bcontent = _get_bootstrap_content(directory)
        if (
            '--download-base' in bcontent
            or '--setup-source' in bcontent
            or '--distribute' in bcontent
        ):
            buildoutver = 1
    except (OSError, IOError):
        pass
    return buildoutver
--------------------
Prompt: 
def _extract_extra_keywords(self,params):
        """
        Return any items in params that are not also
        parameters of the overridden object.
        """
----------
Output: 

        return [param for param in params if param.name not in self.extra_keywords]

    def _get_extra_keywords_from_method(self,method):
        """
        Return any items in method that are not also
        methods of the overridden object.
        """
        return [method.im_func.func_code.co_varnames[i] for i in self.im_func.func_code.co_varnames_list if i.startswith(self.extra_keywords)]

    def _get_extra_keywords_from_method_list(self,method_list
----------
Solution: 

        extra_keywords = {}
        overridden_object_params = list(self._overridden.param)
        for name, val in params.items():
            if name not in overridden_object_params:
                extra_keywords[name]=val
                # CEBALERT: should we remove name from params
                # (i.e. del params[name]) so that it's only available
                # via extra_keywords()?
        return extra_keywords
--------------------
Prompt: 
def _parse_non_entities(self):
        """create data for all non-entities in the babelfied text
        set self._all_entities with merged entity and non-entity data
        """
----------
Output: 

        self._all_entities = {}
        for match in re.finditer(u"&(\w+);", self.text):
            self._all_entities[match.group(1)] = match.group(2)
        self.text = self.text.replace(u"&amp;", u'&')
        self.text = self.text.replace(u"&lt;", u'&')
        self.text = self.text.replace(u"&gt;", u'&')

    def _replace_entity(self, match):
        """
        replace the character entity in the babelf
----------
Solution: 

        def _differ(tokens):
            inner, outer = tokens
            not_same_start = inner.get('start') != outer.get('start')
            not_same_end = inner.get('end') != outer.get('end')
            return not_same_start or not_same_end

        def _get_dot_token():
            dot_token = dict()
            dot_token['start'] = (len(self._text) - 1)
            dot_token['end'] = dot_token['start']
            dot_token['text'] = '.'
            dot_token['isEntity'] = False
            return dot_token

        if self._text.endswith('.'):
            text = self._text[:-1]
            add_dot_token = True
        else:
            text = self._text
            add_dot_token = False

        index = 0
        all_tokens = list()
        for token in text.split():

            comma_token = False
            if token.endswith(','):
                comma_token = True
                token = token[:-1]

            start = index
            end = (start + len(token))
            index += (len(token) + 1)

            all_tokens.append({
                'start': start,
                'end': end - 1,
                'text': self._text[start: end],
                'isEntity': False,
            })

            if comma_token:
                all_tokens.append({
                    'start': index,
                    'end': index,
                    'text': ',',
                    'isEntity': False,
                })
                index += 1

        token_tuples = list(product(all_tokens, self.entities))
        redundant = [
            tokens[0] for tokens in token_tuples if not _differ(tokens)]
        non_entity_tokens = [
            item for item in all_tokens if item not in redundant]

        if add_dot_token:
            non_entity_tokens.append(_get_dot_token())

        self._all_entities = sorted(
            self._entities + non_entity_tokens, key=itemgetter('start'))
--------------------
Prompt: 
def acls(self):
        """The instance bound ACLs operations layer."""
----------
Output: 

        return self.acl_manager.acl_class(self.session)

    def delete(self, session=None):
        """Delete this ACL.

        :param session: The session to use for the request.
        """
        if session is None:
            self.session = self.acl_manager.session
        else:
            self.session = session
        return self.session.delete(self)

    def add(self, session=None, acl_type=None, acl_id=None, action=None):
        """Add a new ACL.

        :param session: The session to use for the
----------
Solution: 

        if self._acls is None:
            self._acls = InstanceAcls(instance=self)
        return self._acls
--------------------
Prompt: 
def _rotate_and_chop(self, verts, normal, axis=[0, 0, 1]):
        r"""
        Method to rotate a set of vertices (or coords) to align with an axis
        points must be coplanar and normal must be given
        Chops axis coord to give vertices back in 2D
        Used to prepare verts for printing or calculating convex hull in order
        to arrange them in hull order for calculations and printing
        """
----------
Output: 

        # TODO: check if this is correct
        if self.is_valid_vertex_list(verts):
            # TODO: check if this is correct
            if normal == 'x':
                verts = rotate_x_2d(verts, axis)
            elif normal == 'y':
                verts = rotate_y_2d(verts, axis)
            self._rotate_and_coords(verts)
        else:
            raise ValueError("The given set of vertices is not valid")

    def _rotate_and_coords(self, verts):
        r"""
        Method to rotate a set of vertices
----------
Solution: 

        xaxis = [1, 0, 0]
        yaxis = [0, 1, 0]
        zaxis = [0, 0, 1]
        angle = tr.angle_between_vectors(normal, axis)
        if angle == 0.0 or angle == np.pi:
            # We are already aligned
            facet = verts
        else:
            M = tr.rotation_matrix(tr.angle_between_vectors(normal, axis),
                                   tr.vector_product(normal, axis))
            try:
                facet = np.dot(verts, M[:3, :3].T)
            except ValueError:
                pass
        try:
            x = facet[:, 0]
            y = facet[:, 1]
            z = facet[:, 2]
        except IndexError:
            x = facet[0]
            y = facet[1]
            z = facet[2]
        # Work out span of points and set axes scales to cover this and be
        # equal in both dimensions
        if axis == xaxis:
            output = np.column_stack((y, z))
        elif axis == yaxis:
            output = np.column_stack((x, z))
        elif axis == zaxis:
            output = np.column_stack((x, y))
        else:
            output = facet
        return output
--------------------
Prompt: 
def image_from_docker_args(args):
    """This scans docker run args and attempts to find the most likely docker image argument.
    If excludes any argments that start with a dash, and the argument after it if it isn't a boolean
    switch.  This can be improved, we currently fallback gracefully when this fails.
    """
----------
Output: 

    if args.get('format') == 'json':
        return json.loads(args.get('docker_args'))
    elif args.get('format') == 'docker':
        return docker_image_from_args(docker_args_to_dict(args['docker_args']))
    else:
        raise Exception("Unknown format {}".format(args['format']))


def docker_image_from_args(args):
    """This scans docker run args and attempts to find the most likely docker image argument.
    If excludes any argments that start with a dash, and the argument after it if
----------
Solution: 

    bool_args = ["-t", "--tty", "--rm","--privileged", "--oom-kill-disable","--no-healthcheck", "-i",
        "--interactive", "--init", "--help", "--detach", "-d", "--sig-proxy", "-it", "-itd"]
    last_flag = -2
    last_arg = ""
    possible_images = []
    if len(args) > 0 and args[0] == "run":
        args.pop(0)
    for i, arg in enumerate(args):
        if arg.startswith("-"):
            last_flag = i
            last_arg = arg
        elif "@sha256:" in arg:
            # Because our regex doesn't match digests
            possible_images.append(arg)
        elif docker_image_regex(arg):
            if last_flag == i - 2:
                possible_images.append(arg)
            elif "=" in last_arg:
                possible_images.append(arg)
            elif last_arg in bool_args and last_flag == i - 1:
                possible_images.append(arg)
    most_likely = None
    for img in possible_images:
        if ":" in img or "@" in img or "/" in img:
            most_likely = img
            break
    if most_likely == None and len(possible_images) > 0:
        most_likely = possible_images[0]
    return most_likely
--------------------
Prompt: 
def basic(type='artist-radio', artist_id=None, artist=None, song_id=None, song=None, track_id=None, dmca=False,
          results=15, buckets=None, limit=False,genres=None,):
    """Get a basic playlist
    
    Args:
    
    Kwargs:
        type (str): a string representing the playlist type ('artist-radio' or 'song-radio')
        
        artist_id (str): the artist_id to seed the playlist
        
        artist (str): the name of an artist to seed the playlist
        
        song_id (str): a song_id to seed the playlist
        
        song (str): the name of a song to seed the playlist
        
        track_id (str): the name of a track to seed the playlist
        
        dmca (bool): make the playlist dmca-compliant
        
        results (int): desired length of the playlist
        
        buckets (list): A list of strings specifying which buckets to retrieve
        
        limit (bool): Whether results should be restricted to any idspaces given in the buckets parameter
    """
----------
Output: 
str): the song_id to seed the playlist
        
        song (str): the name of an artist to seed the playlist
    
    Returns:
        A list of the form (name, artist, url)
    """
    if type == 'artist-radio':
        return basic_artist_radio(type='artist-radio', artist_id=artist_id, results=results)
    elif type =='song-radio':
        return basic_song_radio(type='song-radio', artist_id=artist_id, results=results)
    elif type == 'track-radio':
        return basic_track_radio
----------
Solution: 


    limit = str(limit).lower()
    dmca = str(dmca).lower()

    kwargs = locals()
    kwargs['bucket'] = kwargs['buckets']
    del kwargs['buckets']
    kwargs['genre'] = kwargs['genres']
    del kwargs['genres']

    result = util.callm("%s/%s" % ('playlist', 'basic'), kwargs)
    return [Song(**util.fix(s_dict)) for s_dict in result['response']['songs']]
--------------------
Prompt: 
def permissions(self):
        """Dynamically generate dictionary of privacy options
        """
----------
Output: 

        return {
           'read': self.read_privacy,
            'write': self.write_privacy,
            'delete': self.delete_privacy,
            'undelete': self.undelete_privacy,
            'delete_all': self.delete_privacy,
            'undelete_all': self.undelete_privacy,
        }

    def get_context_data(self, **kwargs):
        """Add the context data to the context
        """
        context = super(UserSettingsView, self).get_context_data(**kwargs)
        context['read'] = self.request.user.
----------
Solution: 

        # TODO: optimize this, it's kind of a bad solution for listing a mostly
        # static set of files.
        # We could either add a permissions dict as an attribute or cache this
        # in some way. Creating a dict would be another place we have to define
        # the permission, so I'm not a huge fan, but it would definitely be the
        # easier option.
        permissions_dict = {"self": {}, "parent": {}}

        for field in self.properties._meta.get_fields():
            split_field = field.name.split('_', 1)
            if len(split_field) <= 0 or split_field[0] not in ['self', 'parent']:
                continue
            permissions_dict[split_field[0]][split_field[1]] = getattr(self.properties, field.name)

        return permissions_dict
--------------------
Prompt: 
def _init_h2us(self, fnc_most_specific):
        """Given a set of user GO ids, return GO ids grouped under the "GO high" terms.

        Example of a grouped go list:

            gos = ['GO:0044464':[    # grp_term: D1 cell part
                       'GO:0005737', # child:    D3 cytoplasm
                       'GO:0048471', # child:    D4 perinuclear region of cytoplasm
                   'GO:0016020':[    # grp_term: D1 membrane
                       'GO:0098589', # child:    D2 membrane region
                       'GO:0005886', # child:    D2 plasma membrane
                  ]
        """
----------
Output: 
mbrane
                       'GO:0005502', # child:    D5 cytoplasm
                       'GO:0005503', # child:    D6 perinuclear region of perinuclear region of membrane
                   ],
               'D1': [    # grp_term: D1 cell part
                       'D1:0000305', # child:    D2 cytoplasm
                       'D1:0000305', # child:    D3 cytoplasm
                       'D1:0000305', # child:    D4 cytoplasm
                       'D1:0000305
----------
Solution: 

        # Header GO IDs are main. User GO IDs are as specified by the user
        hdrgo2usrgos = cx.defaultdict(set)
        # Contains user GO IDs which are also header GO IDs, plus user main GO if needed
        hdrgo_is_usrgo = set()
        _go2nt = self.gosubdag.go2nt
        objhi = GrouperInit.GetGoidHigh(self.gosubdag, self.hdrobj.hdrgos,
                                        self.most_specific_fncs[fnc_most_specific])
        for goid_usr in self.usrgos:
            goid_main = _go2nt[goid_usr].id
            # Add current GO ID to parents_all in case curr GO ID is a high GO.
            goid_high = objhi.get_goid_high(goid_main)
            # Don't add user GO ID if it is also the GO header
            if goid_main != goid_high:
                hdrgo2usrgos[goid_high].add(goid_usr)
            elif goid_high not in hdrgo2usrgos:
                hdrgo2usrgos[goid_high] = set()
            if goid_main == goid_high:
                hdrgo_is_usrgo.add(goid_main)
                if goid_main != goid_usr:
                    hdrgo_is_usrgo.add(goid_usr)
        # Initialize data members
        self.hdrgo2usrgos = hdrgo2usrgos
        self.hdrgo_is_usrgo = hdrgo_is_usrgo
--------------------
Prompt: 
def write_service_double_file(target_root, service_name, rendered):
    """Render syntactically valid python service double code."""
----------
Output: 

    with open(target_root /'services/' + service_name / 'double.js', 'w') as f:
        f.write(rendered)


def write_service_template(target_root, service_name, template_name, rendered):
    """Write a template for a python service."""
    with open(target_root / 'templates/' + service_name / template_name, 'w') as f:
        f.write(rendered)


def write_service_script(target_root, service_name, script_name, rendered):
    """Render a script
----------
Solution: 

    target_path = os.path.join(
        target_root,
        'snapstore_schemas', 'service_doubles', '%s.py' % service_name
    )
    with open(target_path, 'w') as target_file:
        target_file.write(rendered)
--------------------
Prompt: 
def dot(r1, r2):
    """Compute the dot product

       Arguments:
        | ``r1``, ``r2``  -- two :class:`Vector3` objects

       (Returns a Scalar)
    """
----------
Output: 

    return dot_product(r1.dot(r2), r1.dot(r2))

def cross(v1, v2):
    """Compute the cross product of two vectors

       Arguments:
        | ``v1``, ``v2``  -- a 2-dimensional vector

    (Returns a Scalar)
    """
    return cross_product(v1.cross(v2), v1.cross(v2))

def dot_product(v1, v2):
    """Compute the dot product of two vectors

       Arguments:
        | ``v1``, ``v2`` 
----------
Solution: 

    if r1.size != r2.size:
        raise ValueError("Both arguments must have the same input size.")
    if r1.deriv != r2.deriv:
        raise ValueError("Both arguments must have the same deriv.")
    return r1.x*r2.x + r1.y*r2.y + r1.z*r2.z
--------------------
Prompt: 
def match_objective_id(self, objective_id, match):
        """Sets the objective ``Id`` for this query.

        arg:    objective_id (osid.id.Id): an objective ``Id``
        arg:    match (boolean): ``true`` for a positive match,
                ``false`` for a negative match
        raise:  NullArgument - ``objective_id`` is ``null``
        *compliance: mandatory -- This method must be implemented.*

        """
----------
Output: 

        pass

    @abc.abstractmethod
    def clear_objective_bank_id_terms(self):
        """Clears the bank ``Id`` terms.



        *compliance: mandatory -- This method must be implemented.*

        """
        pass

    @abc.abstractmethod
    def match_any_objective_bank_id(self, match):
        """Matches an ``Id`` that has any objective banks.



        :param match: ``true`` to match any objective banks with any
                term
        :type: ``boolean``
        *compliance: mandatory -- This method must be implemented.*

        """
        pass
----------
Solution: 

        if not isinstance(objective_id, Id):
            raise errors.InvalidArgument()
        self._add_match('objectiveId', str(objective_id), match)
--------------------
Prompt: 
def is_convertible_with(self, other):
        """Returns true if `other` is convertible with this Dimension.

        Two known Dimensions are convertible if they have the same value.
        An unknown Dimension is convertible with all other Dimensions.

        Args:
          other: Another Dimension.

        Returns:
          True if this Dimension and `other` are convertible.
        """
----------
Output: 

        return (self.value == other.value and
                self.format == other.format and
                self.value == other.value)

    def __ne__(self, other):
        """Returns true if `self`!= `other`."""
        return not self.__eq__(other)

    def __eq__(self, other):
        """Returns true if `self` and `other` are equal."""
        return self.value == other.value

    def __ne__(self, other):
        """Returns true if `self`!= `other`."""
        return not self.__eq__(other)

    def __hash
----------
Solution: 

        other = as_dimension(other)
        return self._value is None or other.value is None or self._value == other.value
--------------------
Prompt: 
def file_command(self, value):
        """
        Setter for **self.__file_command** attribute.

        :param value: Attribute value.
        :type value: unicode
        """
----------
Output: 


        self.__file_command = value

    @property
    def file_name(self):
        """
        Getter for **self.__file_name** attribute.

        :return: Attribute value.
        :rtype: unicode
        """

        return self.__file_name

    @file_name.setter
    def file_name(self, value):
        """
        Setter for **self.__file_name** attribute.

        :param value: Attribute value.
        :type value: unicode
        """

        self.__file_name = value

    @property
    def file_path(self):
        """
        Getter for
----------
Solution: 


        if value is not None:
            assert type(value) is unicode, "'{0}' attribute: '{1}' type is not 'unicode'!".format(
                "file_command", value)
        self.__file_command = value
--------------------
Prompt: 
def revoke_permission(user, permission_name):
    """
    Revoke a specified permission from a user.

    Permissions are only revoked if they are in the scope any of the user's
    roles. If the permission is out of scope, a RolePermissionScopeException
    is raised.
    """
----------
Output: 

    if not user.is_anonymous():
        raise RolePermissionDenied("Cannot revoke permissions for anonymous users")
    if not permission_name.lower() in ["read", "write", "admin", "administrator"]:
        raise RolePermissionInvalid("Invalid permission name")
    if permission_name in get_roles_with_permissions(user, permission_name):
        raise RoleAccessError("You cannot revoke the rights for the requested role")
    if not user.is_admin():
        raise RoleAccessError("Cannot revoke permissions for administrators")
    if not permission_name.lower() in ["admin", "
----------
Solution: 

    roles = get_user_roles(user)

    for role in roles:
        if permission_name in role.permission_names_list():
            permission = get_permission(permission_name)
            user.user_permissions.remove(permission)
            return

    raise RolePermissionScopeException(
        "This permission isn't in the scope of "
        "any of this user's roles.")
--------------------
Prompt: 
def make_redirect_url(self, path_info, query_args=None, domain_part=None):
        """Creates a redirect URL.

        :internal:
        """
----------
Output: 

        url = self.build_absolute_uri(path_info, self.redirect_field_name)
        if query_args:
            url += '?' + self.redirect_field_value(query_args[0])
        if query_args:
            url += '&' + self.redirect_field_value(query_args[1])
        if self.redirect_url:
            url = url + '&' + self.redirect_url
        return url

    def build_absolute_uri(self, path_info, path):
        """Creates an absolute URI from the path.

        :internal:
        """
----------
Solution: 

        suffix = ''
        if query_args:
            suffix = '?' + self.encode_query_args(query_args)
        return str('%s://%s/%s%s' % (
            self.url_scheme,
            self.get_host(domain_part),
            posixpath.join(self.script_name[:-1].lstrip('/'),
                           url_quote(path_info.lstrip('/'), self.map.charset,
                                     safe='/:|+')),
            suffix
        ))
--------------------
Prompt: 
def contains(self, name: str) -> List[str]:
        """Return a list of all keywords containing the given string.

        >>> from hydpy.core.devicetools import Keywords
        >>> keywords = Keywords('first_keyword', 'second_keyword',
        ...                     'keyword_3', 'keyword_4',
        ...                     'keyboard')
        >>> keywords.contains('keyword')
        ['first_keyword', 'keyword_3', 'keyword_4', 'second_keyword']
        """
----------
Output: 

        return self.keywords(name)

    def __contains__(self, name: str) -> bool:
        """Return whether the given string contains the given keyword.

        >>> from hydpy.core.devicetools import Keywords
        >>> keywords = Keywords('first_keyword','second_keyword',
       ...                     'keyword_3', 'keyword_4',
       ...                     'keyboard')
        >>> keywords.contains('keyword')
        True
        >>> keywords.contains('keyword_3')
        True
        >>> keywords.contains('keyword_4')
        False
        >>> keywords.contains('keyboard')
        False
       
----------
Solution: 

        return sorted(keyword for keyword in self if name in keyword)
--------------------
Prompt: 
async def fetch_member(self, member_id):
        """|coro|

        Retreives a :class:`Member` from a guild ID, and a member ID.

        .. note::

            This method is an API call. For general usage, consider :meth:`get_member` instead.

        Parameters
        -----------
        member_id: :class:`int`
            The member's ID to fetch from.

        Raises
        -------
        Forbidden
            You do not have access to the guild.
        HTTPException
            Getting the guild failed.

        Returns
        --------
        :class:`Member`
            The member from the member ID.
        """
----------
Output: 
 member's data.
        """
        await self.http.delete_member(member_id)
        return await self.http.get_member(member_id)

    async def get_members(self, *, member_id: int):
        """|coro|

        Retrieves a :class:`Member` from a guild ID, and a member ID.

       .. note::

            This method is an API call. For general usage, consider :meth:`get_guild_members` instead.

        Parameters
        -----------
        member_id: :class:`int`
            The guild ID to fetch members from
----------
Solution: 

        data = await self._state.http.get_member(self.id, member_id)
        return Member(data=data, state=self._state, guild=self)
--------------------
Prompt: 
def _add_delta_tdi(self, other):
        """
        Parameters
        ----------
        other : TimedeltaArray or ndarray[timedelta64]

        Returns
        -------
        result : ndarray[int64]
        """
----------
Output: 

        if isinstance(other, TimedeltaArray):
            return other._data.view('i8') + other._data
        elif is_timedelta64_dtype(other):
            return other + self._shift

        return NotImplemented

    def _sub_datelike(self, other):
        """
        Parameters
        ----------
        other : TimedeltaArray or ndarray[timedelta64]

        Returns
        -------
        result : ndarray[int64]
        """
        if isinstance(other, TimedeltaArray):
            return other - self._data
        elif is_timedelta64_dtype(other):
            return other - self._shift
        return NotImplemented
----------
Solution: 

        assert isinstance(self.freq, Tick)  # checked by calling function

        delta = self._check_timedeltalike_freq_compat(other)
        return self._addsub_int_array(delta, operator.add).asi8
--------------------
Prompt: 
def get_wulff_shape(self, material_id):
        """
        Constructs a Wulff shape for a material.

        Args:
            material_id (str): Materials Project material_id, e.g. 'mp-123'.
        Returns:
            pymatgen.analysis.wulff.WulffShape
        """
----------
Output: 

        return WulffShape(material_id, self.name, self.version, self.author, self.version_comment, self.url, self.name_comment)

    def __str__(self):
        return self.name

    def __repr__(self):
        return self.name

    def __eq__(self, other):
        return self.name == other.name

    def __ne__(self, other):
        return self.name!= other.name

    def __hash__(self):
        return self.name.__hash__()

    def __eq__(self, other):
        return self.
----------
Solution: 

        from pymatgen.symmetry.analyzer import SpacegroupAnalyzer
        from pymatgen.analysis.wulff import WulffShape, hkl_tuple_to_str

        structure = self.get_structure_by_material_id(material_id)
        surfaces = self.get_surface_data(material_id)["surfaces"]
        lattice = (SpacegroupAnalyzer(structure)
                   .get_conventional_standard_structure().lattice)
        miller_energy_map = {}
        for surf in surfaces:
            miller = tuple(surf["miller_index"])
            # Prefer reconstructed surfaces, which have lower surface energies.
            if (miller not in miller_energy_map) or surf["is_reconstructed"]:
                miller_energy_map[miller] = surf["surface_energy"]
        millers, energies = zip(*miller_energy_map.items())
        return WulffShape(lattice, millers, energies)
--------------------
Prompt: 
def extract_tables(sql):
    """Extract the table names from an SQL statment.
    Returns a list of (schema, table, alias) tuples
    """
----------
Output: 

    return [
        (
            schema,
            table,
            alias
        )
        for schema, table, alias in sql.items()
        if alias.startswith('pg_')
    ]


class PGTable(object):
    """Class for handling PostgreSQL table names.
    """

    def __init__(self, name, columns, schema=None):
        self.name = name
        self.columns = columns
        self.schema = schema
        self.table = None

    def __repr__(self):
        return self.name

    def __str__(self):
        if self.schema:
            return self.schema +''
----------
Solution: 

    parsed = sqlparse.parse(sql)
    if not parsed:
        return []

    # INSERT statements must stop looking for tables at the sign of first
    # Punctuation. eg: INSERT INTO abc (col1, col2) VALUES (1, 2)
    # abc is the table name, but if we don't stop at the first lparen, then
    # we'll identify abc, col1 and col2 as table names.
    insert_stmt = parsed[0].token_first().value.lower() == 'insert'
    stream = extract_from_part(parsed[0], stop_at_punctuation=insert_stmt)
    return list(extract_table_identifiers(stream))
--------------------
Prompt: 
def plot_transaction_rate_heterogeneity(
    model,
    suptitle="Heterogeneity in Transaction Rate",
    xlabel="Transaction Rate",
    ylabel="Density",
    suptitle_fontsize=14,
    **kwargs
):
    """
    Plot the estimated gamma distribution of lambda (customers' propensities to purchase).

    Parameters
    ----------
    model: lifetimes model
        A fitted lifetimes model, for now only for BG/NBD
    suptitle: str, optional
        Figure suptitle
    xlabel: str, optional
        Figure xlabel
    ylabel: str, optional
        Figure ylabel
    kwargs
        Passed into the matplotlib.pyplot.plot command.

    Returns
    -------
    axes: matplotlib.AxesSubplot

    """
----------
Output: 
 optional
        Figure ylabel
    suptitle_fontsize: int, optional
        Font size for the suptitle
    **kwargs: dict, optional
        Additional keyword arguments

    Returns
    -------
    None
    """
    if model.fit_method == "BGL":
        if "alpha" in kwargs:
            alpha = kwargs["alpha"]
            kwargs["alpha"] = alpha / (alpha + 1)
        if "beta" in kwargs:
            beta = kwargs["beta"]
            kwargs["beta"] = beta / (beta + 1)
        if "gamma" in kwargs:
            gamma = kwargs["gamma"]
            kwargs["gamma"]
----------
Solution: 

    from matplotlib import pyplot as plt

    r, alpha = model._unload_params("r", "alpha")
    rate_mean = r / alpha
    rate_var = r / alpha ** 2

    rv = stats.gamma(r, scale=1 / alpha)
    lim = rv.ppf(0.99)
    x = np.linspace(0, lim, 100)

    fig, ax = plt.subplots(1)
    fig.suptitle("Heterogeneity in Transaction Rate", fontsize=suptitle_fontsize, fontweight="bold")

    ax.set_title("mean: {:.3f}, var: {:.3f}".format(rate_mean, rate_var))
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)

    fig.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.plot(x, rv.pdf(x), **kwargs)
    return ax
--------------------
Prompt: 
def moments(data, n_neighbors=30, n_pcs=30, mode='connectivities', method='umap', metric='euclidean', use_rep=None,
            recurse_neighbors=False, renormalize=False, copy=False):
    """Computes moments for velocity estimation.

    Arguments
    ---------
    data: :class:`~anndata.AnnData`
        Annotated data matrix.
    n_neighbors: `int` (default: 30)
        Number of neighbors to use.
    n_pcs: `int` (default: 30)
        Number of principal components to use.
    mode: `'connectivities'` or `'distances'`  (default: `'connectivities'`)
        Distance metric to use for moment computation.
    renormalize: `bool` (default: `False`)
        Renormalize the moments by total counts per cell to its median.
    copy: `bool` (default: `False`)
        Return a copy instead of writing to adata.

    Returns
    -------
    Returns or updates `adata` with the attributes
    Ms: `.layers`
        dense matrix with first order moments of spliced counts.
    Mu: `.layers`
        dense matrix with first order moments of unspliced counts.
    """
----------
Output: 
 to use.
    mode: `str` (default: 'connectivities')
        Mode to compute the moments.
    method: `str` (default: 'umap')
        Method to compute the moments.
    metric: `str` (default: 'euclidean')
        Metric to compute the moments.
    use_rep: `bool` (default: False)
        Whether to use the repulsion matrix.
    recurse_neighbors: `bool` (default: False)
        Whether to recurse the neighbors.
    renormalize: `bool` (default: False)
        Whether to renormalize the data matrix
----------
Solution: 

    adata = data.copy() if copy else data

    if 'spliced' not in adata.layers.keys() or 'unspliced' not in adata.layers.keys():
        raise ValueError('Could not find spliced / unspliced counts.')
    if any([not_yet_normalized(adata.layers[layer]) for layer in {'spliced', 'unspliced'}]):
        normalize_per_cell(adata)
    if 'neighbors' not in adata.uns.keys() or neighbors_to_be_recomputed(adata, n_neighbors=n_neighbors):
        if use_rep is None: use_rep = 'X_pca'
        neighbors(adata, n_neighbors=n_neighbors, use_rep=use_rep, n_pcs=n_pcs, method=method, metric=metric)
    if mode not in adata.uns['neighbors']:
        raise ValueError('mode can only be \'connectivities\' or \'distances\'')

    logg.info('computing moments based on ' + str(mode), r=True)

    connectivities = get_connectivities(adata, mode, n_neighbors=n_neighbors, recurse_neighbors=recurse_neighbors)

    adata.layers['Ms'] = csr_matrix.dot(connectivities, csr_matrix(adata.layers['spliced'])).astype(np.float32).A
    adata.layers['Mu'] = csr_matrix.dot(connectivities, csr_matrix(adata.layers['unspliced'])).astype(np.float32).A
    if renormalize: normalize_per_cell(adata, layers={'Ms', 'Mu'}, enforce=True)

    logg.info('    finished', time=True, end=' ' if settings.verbosity > 2 else '\n')
    logg.hint(
        'added \n'
        '    \'Ms\' and \'Mu\', moments of spliced/unspliced abundances (adata.layers)')
    return adata if copy else None
--------------------
Prompt: 
def effective_balance(self, address: Address, block_identifier: BlockSpecification) -> Balance:
        """ The user's balance with planned withdrawals deducted. """
----------
Output: 

        return self.get_balance(address, planned=True)

    def get_balance(self, address: Address, planned: bool) -> Balance:
        """ Get the planned balance of the user. """
        return self.get_balance(address, balance_type=BalanceType.PLAN, planned=planned)

    def get_balance_type(self, address: Address) -> BalanceType:
        """ Get the balance type of the user. """
        return self.get_balance(address, balance_type=BalanceType.DEDUCT)

    def get_balance
----------
Solution: 

        fn = getattr(self.proxy.contract.functions, 'effectiveBalance')
        balance = fn(address).call(block_identifier=block_identifier)

        if balance == b'':
            raise RuntimeError(f"Call to 'effectiveBalance' returned nothing")

        return balance
--------------------
Prompt: 
def equals(self, other):
        """
        Equality of two SSAFiles.

        Compares :attr:`SSAFile.info`, :attr:`SSAFile.styles` and :attr:`SSAFile.events`.
        Order of entries in OrderedDicts does not matter. "ScriptType" key in info is
        considered an implementation detail and thus ignored.

        Useful mostly in unit tests. Differences are logged at DEBUG level.

        """
----------
Output: 

        if self.info.info.mode == 'SCRIPT':
            return self.styles == other.styles
        elif self.info.info.mode == 'STYLE':
            return self.events == other.events
        else:
            raise ValueError('Cannot compare SSAFiles with mode: %s' %
                             self.info.info.mode)

    def __ne__(self, other):
        return not self.__eq__(other)

    def __eq__(self, other):
        """
        Equality of two SSAFiles.

        Compares :attr:`SSAFile.styles` and :attr:`SSA
----------
Solution: 


        if isinstance(other, SSAFile):
            for key in set(chain(self.info.keys(), other.info.keys())) - {"ScriptType"}:
                sv, ov = self.info.get(key), other.info.get(key)
                if sv is None:
                    logging.debug("%r missing in self.info", key)
                    return False
                elif ov is None:
                    logging.debug("%r missing in other.info", key)
                    return False
                elif sv != ov:
                    logging.debug("info %r differs (self=%r, other=%r)", key, sv, ov)
                    return False

            for key in set(chain(self.styles.keys(), other.styles.keys())):
                sv, ov = self.styles.get(key), other.styles.get(key)
                if sv is None:
                    logging.debug("%r missing in self.styles", key)
                    return False
                elif ov is None:
                    logging.debug("%r missing in other.styles", key)
                    return False
                elif sv != ov:
                    for k in sv.FIELDS:
                        if getattr(sv, k) != getattr(ov, k): logging.debug("difference in field %r", k)
                    logging.debug("style %r differs (self=%r, other=%r)", key, sv.as_dict(), ov.as_dict())
                    return False

            if len(self) != len(other):
                logging.debug("different # of subtitles (self=%d, other=%d)", len(self), len(other))
                return False

            for i, (se, oe) in enumerate(zip(self.events, other.events)):
                if not se.equals(oe):
                    for k in se.FIELDS:
                        if getattr(se, k) != getattr(oe, k): logging.debug("difference in field %r", k)
                    logging.debug("event %d differs (self=%r, other=%r)", i, se.as_dict(), oe.as_dict())
                    return False

            return True
        else:
            raise TypeError("Cannot compare to non-SSAFile object")
--------------------
Prompt: 
def set_style(style, mpl=False, **kwargs):
    """
    If mpl is False accept either style name or a TStyle instance.
    If mpl is True accept either style name or a matplotlib.rcParams-like
    dictionary
    """
----------
Output: 

    if mpl:
        if isinstance(style, TStyle):
            style = style.copy()
        else:
            raise TypeError("Invalid argument for set_style")
    else:
        style = TStyle(style)
    return _set_style(style, mpl=mpl, **kwargs)


def _set_style(style, mpl=False, **kwargs):
    """
    Set the style of the given name to the given matplotlib rc setting.
    If mpl is False accept either name or rcParams name.
    """
    if mpl:
        if isinstance(style, TStyle):
            style = style.
----------
Solution: 

    if mpl:
        import matplotlib as mpl

        style_dictionary = {}
        if isinstance(style, string_types):
            style_dictionary = get_style(style, mpl=True, **kwargs)
            log.info("using matplotlib style '{0}'".format(style))
        elif isinstance(style, dict):
            style_dictionary = style
            log.info("using user-defined matplotlib style")
        else:
            raise TypeError("style must be a matplotlib style name or dict")
        for k, v in style_dictionary.items():
            mpl.rcParams[k] = v
    else:
        if isinstance(style, string_types):
            style = get_style(style, **kwargs)
        log.info("using ROOT style '{0}'".format(style.GetName()))
        style.cd()
--------------------
Prompt: 
def delete_object(self, mpath):
        """DeleteObject
        https://apidocs.joyent.com/manta/api.html#DeleteObject

        @param mpath {str} Required. A manta path, e.g. '/trent/stor/myobj'.
        """
----------
Output: 

        self.mpath = mpath
        self.path = mpath
        self.name = self.path.split('/')[-1]
        self.path = mpath
        self.name = self.path.split('/')[-1].replace('/', '_')
        self.path = mpath
        self.path = mpath
        self.path = mpath.replace('/', '_')
        self.path = mpath.replace('/', '_')
        self.path = mpath.replace('/', '_')
        self.path = mpath.replace('/', '_')
        self.path = mpath.replace('/', '_')
        self.path = mpath.
----------
Solution: 

        log.debug('DeleteObject %r', mpath)
        res, content = self._request(mpath, "DELETE")
        if res["status"] != "204":
            raise errors.MantaAPIError(res, content)
        return res
--------------------
Prompt: 
def dedent(self, node, dirty=True):
        """Dedent an item. Does nothing if the target is not indented under this item.

        Args:
            node (gkeepapi.node.ListItem): Item to dedent.
            dirty (bool): Whether this node should be marked dirty.
        """
----------
Output: 

        if not dirty:
            node.text = self.text
        else:
            node.text = self.text + "\n" + self.text
        node.text = self.text
        node.is_dirty = True
        node.is_open = True
        node.is_dirty = True

    def insert_child(self, node, index):
        """Insert a child. Does nothing if the target is already a child.

        Args:
            node (gkeepapi.node.ListItem): Child to insert.
            index (int): Index of the child to insert.
        """
        if
----------
Solution: 

        if node.id not in self._subitems:
            return

        del self._subitems[node.id]
        node.super_list_item_id = None
        node.parent_item = None
        if dirty:
            node.touch(True)
--------------------
Prompt: 
def _process_phenotype_cvterm(self):
        """
        These are the qualifiers for the phenotype location itself.
        But are just the qualifiers.
        The actual "observable" part of the phenotype is only in
        the phenotype table. These get added to a lookup variable used to
        augment a phenotype association statement.
        :return:

        """
----------
Output: 

        return self._get_observable_part(self.get_observable_id_from_lookup_variable())

    def get_observable_id_from_lookup_variable(self):
        """
        :param self:
        :param lookup_variable:
        :return:
        """
        return self.get_lookup_variable_observable_id()

    def get_lookup_variable_observable_id(self):
        """
        :param self:
        :param lookup_variable:
        :return:
        """
        return self.get_lookup_variable_observable_id_
----------
Solution: 


        line_counter = 0
        raw = '/'.join((self.rawdir, 'phenotype_cvterm'))
        LOG.info("processing phenotype cvterm mappings")

        with open(raw, 'r') as f:
            f.readline()  # read the header row; skip
            filereader = csv.reader(f, delimiter='\t', quotechar='\"')
            for line in filereader:
                line_counter += 1

                (phenotype_cvterm_id, phenotype_id, cvterm_id, rank) = line

                # 4532	8507	60793	0
                # 4533	8513	60830	0

                # add the internal genotype to pub mapping
                phenotype_key = phenotype_id
                cvterm_key = cvterm_id
                phenotype_id = self.idhash['phenotype'][phenotype_key]
                if cvterm_key in self.idhash['cvterm']:
                    cvterm_id = self.idhash['cvterm'][cvterm_key]
                    if phenotype_key not in self.phenocv:
                        self.phenocv[phenotype_id] = set()
                    self.phenocv[phenotype_id].add(cvterm_id)
                else:
                    LOG.info("Not storing the cvterm info for %s", cvterm_key)

        return
--------------------
Prompt: 
def SInt64(value, min_value=None, max_value=None, encoder=ENC_INT_DEFAULT, fuzzable=True, name=None, full_range=False):
    """Signed 64-bit field"""
----------
Output: 

    if min_value is None:
        min_value = value
    if max_value is None:
        max_value = value
    if value < min_value or value > max_value:
        raise ValueError("Value out of range: %s" % full_range)
    if encoder is None:
        encoder = int
    if fuzzable:
        return encoder(value)
    else:
        return int(value)


class SInt32(SInt64):
    """Signed 32-bit integer field"""
    def __init__(self, value, encoder=ENC_INT_DEFAULT, fu
----------
Solution: 

    return BitField(value, 64, signed=True, min_value=min_value, max_value=max_value, encoder=encoder, fuzzable=fuzzable, name=name, full_range=full_range)
--------------------
Prompt: 
def delete_resource(self, resource_id):
        """
        Remove a specific resource by its identifier.
        """
----------
Output: 

        self.delete_resource_by_id(resource_id)

    def delete_resource_by_id(self, resource_id):
        """
        Delete a specific resource by its identifier.
        """
        self.delete_resource(resource_id)

    def delete_resource_by_name(self, name):
        """
        Delete a specific resource by its name.
        """
        self.delete_resource_by_name(name)

    def delete_resource_by_name(self, name):
        """
        Delete a specific resource by its name.
        """
        self.delete_resource
----------
Solution: 

        # resource_id could be a path such as '/asset/123' so quote
        uri = self._get_resource_uri(guid=resource_id)
        return self.service._delete(uri)
--------------------
Prompt: 
def _getScalesRand(self):
        """
        Internal function for parameter initialization
        Return a vector of random scales
        """
----------
Output: 

        return self.randomScales

    def _setScalesRand(self, randomScales):
        """
        Internal function for parameter update
        """
        self.randomScales = randomScales

    def _getSlope(self):
        """
        Internal function for parameter initialization
        Return a random slope
        """
        return random.uniform(0, 1)

    def _setSlope(self, slope):
        """
        Internal function for parameter update
        """
        self.randomScales = randomScales

    def _getRandomScale(self):
        """
        Internal function for parameter random scale
        Return a
----------
Solution: 

        if self.P>1:
            scales = []
            for term_i in range(self.n_randEffs):
                _scales = sp.randn(self.diag[term_i].shape[0])
                if self.jitter[term_i]>0:
                    _scales = sp.concatenate((_scales,sp.array([sp.sqrt(self.jitter[term_i])])))
                scales.append(_scales)
            scales = sp.concatenate(scales)
        else:
            scales=sp.randn(self.vd.getNumberScales())
        return scales
--------------------
Prompt: 
def read_resource_list(self, uri):
        """Read resource list from specified URI else raise exception."""
----------
Output: 

        try:
            return self.get_resource_list(uri)
        except Exception as e:
            raise e

    def get_resource_list(self, uri):
        """Get list of resources from specified URI."""
        try:
            return self.resource_list[uri]
        except Exception as e:
            raise e

    def get_resource_list_from_uri(self, uri):
        """Get list of resources from specified URI."""
        try:
            return self.resource_list[uri.split('/')[-1]]
        except Exception as e:
            raise e

    def get_resource_list_
----------
Solution: 

        self.logger.info("Reading resource list %s" % (uri))
        try:
            resource_list = ResourceList(allow_multifile=self.allow_multifile,
                                         mapper=self.mapper)
            resource_list.read(uri=uri)
        except Exception as e:
            raise ClientError("Can't read source resource list from %s (%s)" %
                              (uri, str(e)))
        self.logger.debug("Finished reading resource list")
        return(resource_list)
--------------------
Prompt: 
def get_score(self, member, default=None, pipe=None):
        """
        Return the score of *member*, or *default* if it is not in the
        collection.
        """
----------
Output: 

        return self.scores.get(member, default)

    def get_score_display(self, pipe):
        """
        Return the score display value for *pipe* if it is not in the
        collection.
        """
        return self.scores.get(pipe, None)

    def get_score_display_value(self, pipe_value):
        """
        Return the score display value for *pipe_value*, or *default* if
        it is not in the collection.
        """
        return self.scores.get(pipe_value, None)

    def get_score_description(self,
----------
Solution: 

        pipe = self.redis if pipe is None else pipe
        score = pipe.zscore(self.key, self._pickle(member))

        if (score is None) and (default is not None):
            score = float(default)

        return score
--------------------
Prompt: 
def get_one_meth_type(self, mtype, method_list):
        """
        Get all codes of one type (i.e., 'anisotropy_estimation')
        """
----------
Output: 

        if mtype == 'anisotropy_estimation':
            return self.code_is_estimation(method_list)
        elif mtype == 'code_is_estimation':
            return self.code_is_estimation(method_list)
        elif mtype == 'code_is_estimation':
            return self.code_is_estimation(method_list)
        elif mtype == 'code_is_estimation':
            return self.code_is_estimation(method_list)
        else:
            raise ValueError('Invalid mtype: %s' % mtype)

    def code
----------
Solution: 

        cond = method_list['dtype'] == mtype
        codes = method_list[cond]
        return codes
--------------------
Prompt: 
def read_value(self, varname, path="/", cmode=None, default=NO_DEFAULT):
        """
        Returns the values of variable with name varname in the group specified by path.

        Args:
            varname: Name of the variable
            path: path to the group.
            cmode: if cmode=="c", a complex ndarrays is constructed and returned
                (netcdf does not provide native support from complex datatype).
            default: returns default if varname is not present.
                self.Error is raised if default is default is NO_DEFAULT

        Returns:
            numpy array if varname represents an array, scalar otherwise.
        """
----------
Output: 
 otherwise numpy array.
        """
        if cmode is None:
            cmode = self.default_cmode
        try:
            return np.array(self.group_variables[path][varname])
        except KeyError:
            if default is NO_DEFAULT:
                return np.array(self.group_variables[path][varname])
            else:
                raise self.Error("No group variable %s" % varname)

    def get_value(self, varname, path="/", cmode=None, default=NO_DEFAULT):
        """
        Returns the values of variable with name varname in the group specified by
----------
Solution: 

        try:
            var = self.read_variable(varname, path=path)
        except self.Error:
            if default is NO_DEFAULT: raise
            return default

        if cmode is None:
            # scalar or array
            # getValue is not portable!
            try:
                return var.getValue()[0] if not var.shape else var[:]
            except IndexError:
                return var.getValue() if not var.shape else var[:]

        else:
            assert var.shape[-1] == 2
            if cmode == "c":
                return var[...,0] + 1j*var[...,1]
            else:
                raise ValueError("Wrong value for cmode %s" % cmode)
--------------------
Prompt: 
def solve_each(expr, vars):
    """Return True if RHS evaluates to a true value with each state of LHS.

    If LHS evaluates to a normal IAssociative object then this is the same as
    a regular let-form, except the return value is always a boolean. If LHS
    evaluates to a repeared var (see efilter.protocols.repeated) of
    IAssociative objects then RHS will be evaluated with each state and True
    will be returned only if each result is true.
    """
----------
Output: 

    if expr.is_repeating:
        return True
    if expr.is_normal:
        return False
    if expr.is_repeating:
        return True
    if expr.is_repeated:
        return True
    if expr.is_normal:
        return True
    if expr.is_repeating:
        return False
    if expr.is_repeated:
        return True
    return False


def efilter_protocol_is_in(p, p_type):
    """Return True if the given protocol is in the given list of protocols.

    If p_type is a list of protocols,
----------
Solution: 

    lhs_values, _ = __solve_for_repeated(expr.lhs, vars)

    for lhs_value in repeated.getvalues(lhs_values):
        result = solve(expr.rhs, __nest_scope(expr.lhs, vars, lhs_value))
        if not result.value:
            # Each is required to return an actual boolean.
            return result._replace(value=False)

    return Result(True, ())
--------------------
Prompt: 
def predict(self, x, add_intercept=False):
        """For some x, predict the bn(x) for each base

        Arguments:
            x: np.array; Vector of dimension 1
            add_intercept: bool; should we add the intercept to the final array

        Returns:
            np.array, of shape (len(x), n_bases + (add_intercept))
        """
----------
Output: 

        if add_intercept:
            return self.add_intercept(x)
        else:
            return self.predict_proba(x)

    def add_intercept(self, x):
        """Add a column of ones to the linear model

        Arguments:
            x: np.array; Vector of dimension n_bases

        Returns:
            np.array; Column of ones
        """
        return x + np.random.normal(0, self.n_classes) * self.intercept_alpha

    def predict_proba(self, x, add_intercept=False):
        """For some x, predict the pro
----------
Solution: 

        # sanity check
        if x.min() < self.start:
            raise Warning("x.min() < self.start")
        if x.max() > self.end:
            raise Warning("x.max() > self.end")

        return get_X_spline(x=x,
                            knots=self.knots,
                            n_bases=self.n_bases,
                            spline_order=self.spline_order,
                            add_intercept=add_intercept)
--------------------
Prompt: 
def removeCMSPadding(str, blocksize=AES_blocksize):
  """CMS padding: Remove padding with bytes containing the number of padding bytes """
----------
Output: 

  return _core_.AES_removeCMSPadding(str, blocksize)

def removeCMS(str, pos=0):
  """
    Removes the specified string from the end of the string,
    where pos is the position of the end of the string.
    """
  return _core_.AES_removeCMS(str, pos)

def removeHuffmanPadding(str, pad, pos=0):
  """
    Removes the specified string from the end of the string,
    where pos is the position of the end of the string.
    """
  return _core_.AES_removeHuffmanPadding
----------
Solution: 

  try:
    pad_len = ord(str[-1]) # last byte contains number of padding bytes
  except TypeError:
    pad_len = str[-1]
  assert pad_len <= blocksize, 'padding error' 
  assert pad_len <= len(str), 'padding error'
    
  return str[:-pad_len]
--------------------
Prompt: 
def _set_gos_stats(self, v, load=False):
    """
    Setter method for gos_stats, mapped from YANG variable /cpu_interface_state/gos_stats (container)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_gos_stats is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_gos_stats() directly.
    """
----------
Output: 

    if hasattr(v, "_utype"):
        v = v._utype(v)
    try:
        t = YANGDynClass(
            v,
            base=stats.GosStats,
            is_container="container",
            yang_name="gos-stats",
            parent=self,
            path_helper=self._path_helper,
            extmethods=self._extmethods,
            register_paths=True,
            namespace="http://openconfig.net/yang/network-instance",
            defining_module="openconfig-network-instance",
            yang_type="container",
            is_
----------
Solution: 

    if hasattr(v, "_utype"):
      v = v._utype(v)
    try:
      t = YANGDynClass(v,base=gos_stats.gos_stats, is_container='container', presence=False, yang_name="gos-stats", rest_name="gos-stats", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'callpoint': u'sysdiag-stats-gos-stats-1'}}, namespace='urn:brocade.com:mgmt:brocade-sysdiag-operational', defining_module='brocade-sysdiag-operational', yang_type='container', is_config=False)
    except (TypeError, ValueError):
      raise ValueError({
          'error-string': 
--------------------
Prompt: 
def describe_token_map(self, ):
    """
    get the mapping between token->node ip
    without taking replication into consideration
    https://issues.apache.org/jira/browse/CASSANDRA-4092
    """
----------
Output: 

    return dict(self.cassandra_server_list)

  def get_keyspace_names(self, ksname=None):
    if ksname is None:
      ksname = self.conn.keyspace

    if ksname in ksname_to_value:
      return ksname_to_value[ksname]
    else:
      return []

  def get_keyspace_meta(self, keyspace_name):
    ksmeta = self.get_keyspace_meta_dict()
    ksmeta[keyspace_name] = keyspace_name
    return ksmeta

  def get_keyspace_meta_dict(
----------
Solution: 

    self._seqid += 1
    d = self._reqs[self._seqid] = defer.Deferred()
    self.send_describe_token_map()
    return d
--------------------
Prompt: 
def get_rating_id(self):
        """Gets the ``Id`` of the ``Grade``.

        return: (osid.id.Id) - the ``Agent``  ``Id``
        raise:  IllegalState - ``has_rating()`` is ``false``
        *compliance: mandatory -- This method must be implemented.*

        """
----------
Output: 

        return  # osid.id.Id

    rating_id = property(fget=get_rating_id)

    def get_agent_id_for_gradebook_column(self, gradebook_column_id):
        """Gets the ``Id`` of the ``Agent`` for the given gradebook column.

        arg:    gradebook_column_id (osid.id.Id): the ``Id`` of the ``GradebookColumn``
        return: (osid.id.Id) - the ``Agent``  ``Id``
        raise:  IllegalState - ``has
----------
Solution: 

        # Implemented from template for osid.resource.Resource.get_avatar_id_template
        if not bool(self._my_map['ratingId']):
            raise errors.IllegalState('this Comment has no rating')
        else:
            return Id(self._my_map['ratingId'])
--------------------
Prompt: 
def download(date_array, tag, sat_id, data_path=None, user=None, password=None):
    """Downloads data from Madrigal.
    
    The user's names should be provided in field user. John Malkovich should be 
    entered as John+Malkovich 
    
    The password field should be the user's email address. These parameters
    are passed to Madrigal when downloading.
    
    The affiliation field is set to pysat to enable tracking of pysat downloads.
    
    Parameters
    ----------
    
    
    """
----------
Output: 

    # Check if the data path is given
    if data_path is None:
        data_path = get_data_path()
    # Check if the user and password are given
    if user is None:
        user = getpass.getuser()
    if password is None:
        password = getpass.getpass()
    # Download the data
    url = get_data_url(date_array, tag, sat_id)
    # Download the affiliation
    try:
        response = urllib2.urlopen(url)
    except urllib2.HTTPError, e:
        print "Error: %s" % e.
----------
Solution: 

    import subprocess
    
    # currently passes things along if no user and password supplied
    # need to do this for testing
    # TODO, implement user and password values in test code
    # specific to DMSP
    if user is None:
        print ('No user information supplied for download.')
        user = 'pysat_testing'
    if password is None:
        print ('Please provide email address in password field.')
        password = 'pysat_testing@not_real_email.org'

    a = subprocess.check_output(["globalDownload.py", "--verbose", 
                    "--url=http://cedar.openmadrigal.org",
                    '--outputDir='+data_path,
                    '--user_fullname='+user,
                    '--user_email='+password,
                    '--user_affiliation=pysat',
                    '--format=hdf5',
                    '--startDate='+date_array[0].strftime('%m/%d/%Y'),
                    '--endDate='+date_array[-1].strftime('%m/%d/%Y'),
                    '--inst=8100',
                    '--kindat='+str(madrigal_tag[sat_id])])
    print ('Feedback from openMadrigal ', a)
--------------------
Prompt: 
def update(self):
        """
        Updates the non-primitive objects needed for solution, using primitive
        attributes.  This includes defining a "utility from conformity" function
        conformUtilityFunc, a grid of population punk proportions, and an array
        of future punk proportions (for each value in the grid).  Results are
        stored as attributes of self.

        Parameters
        ----------
        none

        Returns
        -------
        none
        """
----------
Output: 

        self.attributes = {}
        for key in self.population:
            self.attributes[key] = self.population[key]
        self.attributes['utility'] = self.utilityFunc.update()
        return self

    def get_attributes(self):
        """
        Returns the attributes of the solution.

        Parameters
        ----------
        none

        Returns
        -------
        dict
        """
        return self.attributes

    def get_pops(self):
        """
        Returns the pops of the solution.

        Parameters
        ----------
        none

        Returns
        -------
        list
        """
        return self.pops

    def get_value
----------
Solution: 

        self.conformUtilityFunc = lambda x : stats.beta.pdf(x,self.uParamA,self.uParamB)
        self.pGrid = np.linspace(0.0001,0.9999,self.pCount)
        self.updateEvolution()
--------------------
Prompt: 
def rotate_texture(texture, rotation, x_offset=0.5, y_offset=0.5):
    """Rotates the given texture by a given angle.

    Args:
        texture (texture): the texture to rotate
        rotation (float): the angle of rotation in degrees
        x_offset (float): the x component of the center of rotation (optional)
        y_offset (float): the y component of the center of rotation (optional)

    Returns:
        texture: A texture.
    """
----------
Output: 

    return texture.rotate(rotation, x_offset, y_offset)


def rotate_y(rotation, x_offset=0.5, y_offset=0.5):
    """Rotates the given texture by a given angle.

    Args:
        rotation (float): the angle of rotation in degrees
        x_offset (float): the x component of the center of rotation (optional)
        y_offset (float): the y component of the center of rotation (optional)

    Returns:
        texture: A texture.
    """
    return rotation.rotate(x_offset, y_offset,
----------
Solution: 

    x, y = texture
    x = x.copy() - x_offset
    y = y.copy() - y_offset
    angle = np.radians(rotation)
    x_rot = x * np.cos(angle) + y * np.sin(angle)
    y_rot = x * -np.sin(angle) + y * np.cos(angle)
    return x_rot + x_offset, y_rot + y_offset
--------------------
Prompt: 
def getDecimalMark(self):
        """Return the decimal mark to be used on reports for this client

        If the client has DefaultDecimalMark selected, the Default value from
        the LIMS Setup will be returned.

        Otherwise, will return the value of DecimalMark.
        """
----------
Output: 

        return self.defaultDecimalMark

    def setDecimalMark(self, decimalMark):
        """Set the decimal mark for this report

        :param decimalMark: the decimal mark to be used on reports
        """
        self.defaultDecimalMark = decimalMark

    def getReportType(self):
        """Return the type of report

        :return: the type of report
        """
        return self.reportType

    def setReportType(self, reportType):
        """Set the type of report for this client

        :param reportType: the type of report
        """
        self.reportType = reportType

    def getReport
----------
Solution: 

        if self.getDefaultDecimalMark() is False:
            return self.Schema()["DecimalMark"].get(self)
        return self.bika_setup.getDecimalMark()
--------------------
Prompt: 
def _persist(self):
        """
        Run the command inside a thread so that we can catch output for each
        line as it comes in and display it.
        """
----------
Output: 

        self.stdout.write("Running command: %s\n" % self.command)
        self.stdout.flush()
        self.command = self.command.strip()
        self.stdout.write("Command: %s\n" % self.command)
        self.stdout.write("Thread: %s\n" % self.thread)
        self.stdout.write("Command: %s\n" % self.command)
        self.stdout.write("Thread: %s\n" % self.thread)
        self.stdout.flush()
        self.stdout.write("
----------
Solution: 

        # run the block/command
        for command in self.commands:
            try:
                process = Popen(
                    [command],
                    stdout=PIPE,
                    stderr=PIPE,
                    universal_newlines=True,
                    env=self.env,
                    shell=True,
                )
            except Exception as e:
                retcode = process.poll()
                msg = "Command '{cmd}' {error} retcode {retcode}"
                self.py3.log(msg.format(cmd=command, error=e, retcode=retcode))

            # persistent blocklet output can be of two forms.  Either each row
            # of the output is on a new line this is much easier to deal with)
            # or else the output can be continuous and just flushed when ready.
            # The second form is more tricky, if we find newlines then we
            # switch to easy parsing of the output.

            # When we have output we store in self.persistent_output and then
            # trigger the module to update.

            fd = process.stdout.fileno()
            fl = fcntl.fcntl(fd, fcntl.F_GETFL)
            has_newlines = False
            while True:
                line = process.stdout.read(1)
                # switch to a non-blocking read as we do not know the output
                # length
                fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)
                line += process.stdout.read(1024)
                # switch back to blocking so we can wait for the next output
                fcntl.fcntl(fd, fcntl.F_SETFL, fl)
                if process.poll():
                    break
                if self.py3.is_python_2():
                    line = line.decode("utf-8")
                self.persistent_output = line
                self.py3.update()
                if line[-1] == "\n":
                    has_newlines = True
                    break
                if line == "":
                    break
            if has_newlines:
                msg = "Switch to newline persist method {cmd}"
                self.py3.log(msg.format(cmd=command))
                # just read the output in a sane manner
                for line in iter(process.stdout.readline, b""):
                    if process.poll():
                        break
                    if self.py3.is_python_2():
                        line = line.decode("utf-8")
                    self.persistent_output = line
                    self.py3.update()
        self.py3.log("command exited {cmd}".format(cmd=command))
        self.persistent_output = "Error\nError\n{}".format(
            self.py3.COLOR_ERROR or self.py3.COLOR_BAD
        )
        self.py3.update()
--------------------
Prompt: 
def get_state_in_ec_string(self, ec_index, add_colour=True):
        """Get the state of the component in an execution context as a string.

        @param ec_index The index of the execution context to check the state
                        in. This index is into the total array of contexts,
                        that is both owned and participating contexts. If the
                        value of ec_index is greater than the length of @ref
                        owned_ecs, that length is subtracted from ec_index and
                        the result used as an index into @ref
                        participating_ecs.

        """
----------
Output: 

        @param add_colour If True, add a colour to the result. If False, add
                        a colour to the result.
        @return The state of the component in an EC string.
        """
        if add_colour:
            return self.ref_state_colours[ec_index]
        else:
            return self.ref_state[ec_index]

    def get_state_in_ec_index(self, ec_index):
        """Get the state of the component in an execution context as an index.

        @param ec_index The index of the execution context to check the state
----------
Solution: 

        with self._mutex:
            if ec_index >= len(self.owned_ecs):
                ec_index -= len(self.owned_ecs)
                if ec_index >= len(self.participating_ecs):
                    raise exceptions.BadECIndexError(ec_index)
                state = self.participating_ec_states[ec_index]
            else:
                state = self.owned_ec_states[ec_index]
        if state == self.INACTIVE:
            result = 'Inactive', ['bold', 'blue']
        elif state == self.ACTIVE:
            result = 'Active', ['bold', 'green']
        elif state == self.ERROR:
            result = 'Error', ['bold', 'white', 'bgred']
        elif state == self.UNKNOWN:
            result = 'Unknown', ['bold', 'red']
        elif state == self.CREATED:
            result = 'Created', ['reset']
        if add_colour:
            return utils.build_attr_string(result[1], supported=add_colour) + \
                    result[0] + utils.build_attr_string('reset',
                    supported=add_colour)
        else:
            return result[0]
--------------------
Prompt: 
def done(self):
        """ Check if we should stop returning objects """
----------
Output: 

        return self.state =='stopping'

    def stop(self):
        """
        Stop the job
        """
        self.state ='stopped'
        self.stop_time = datetime.now()
        self.save()

    def save(self):
        """
        Save the job
        """
        self.state ='saving'
        self.save_time = datetime.now()
        self.state_changed = True
        self.save_time = datetime.now()
        self.state = 'done'
        self.save_time = datetime.now()
        self.save()

    def get_
----------
Solution: 

        if self._done:
            return self._done

        if self._limit is None:
            self._done = False
        elif self.itemcount >= self._limit:
            self._done = True

        return self._done
--------------------
Prompt: 
def bulk_cache_of_tag(self, tag):
        """ Caches all the symbols by a certain tag.

        For now, there is no different, than 
        caching each symbol individually.  In the future,
        this functionality could have speed improvements.

        Parameters
        ----------
        tag : str
            Use '%' to enable SQL's "LIKE" functionality.

        Returns
        -------
        TrumpReport
        """
----------
Output: 

        self.cache_of_symbol_of_tag[tag] = {}
        for symbol in self.symbols:
            self.cache_of_symbol_of_tag[symbol][tag] = {}

        for line in self.lines:
            if line.tag == tag:
                self.cache_of_symbol_of_tag[line.symbol][tag][line.date] = line.amount
                self.cache_of_symbol_of_tag[line.symbol][tag][line.amount] = line.date.strftime('%d')
                self.cache_of_symbol_
----------
Solution: 


        syms = self.search_tag(tag)
        
        name = 'Bulk Cache of Symbols tagged {}'.format(tag)
        tr = TrumpReport(name)
        for sym in syms:
            sr = sym.cache()
            tr.add_symbolreport(sr)
        
        return tr
--------------------
Prompt: 
def active_inactive(self):
        """The indexes of the active and the inactive cell vectors"""
----------
Output: 

        return self.active_cells.keys()

    def get_cell(self, row, col):
        """Returns the cell at the given row and column"""
        return self.active_cells[row][col]

    def get_cell_by_index(self, row, col):
        """Returns the active and inactive cell at the given column"""
        return self.get_cell_by_index(row, col)

    def get_cell_by_name(self, name):
        """Returns the active and inactive cell with the given name"""
        return self.get_cell_by_
----------
Solution: 

        active_indices = []
        inactive_indices = []
        for index, active in enumerate(self.active):
            if active:
                active_indices.append(index)
            else:
                inactive_indices.append(index)
        return active_indices, inactive_indices
--------------------
Prompt: 
def get_variables_in_scope(scope, collection=tf.GraphKeys.TRAINABLE_VARIABLES):
  """Returns a tuple `tf.Variable`s in a scope for a given collection.

  Args:
    scope: `tf.VariableScope` or string to retrieve variables from.
    collection: Collection to restrict query to. By default this is
        `tf.Graphkeys.TRAINABLE_VARIABLES`, which doesn't include non-trainable
        variables such as moving averages.

  Returns:
    A tuple of `tf.Variable` objects.
  """
----------
Output: 

  if isinstance(collection, tf.Graphkeys.TRAINABLE_VARIABLES):
    return collection
  elif isinstance(collection, tf.Variable):
    return (tf.Variable, tf.Variable)
  else:
    raise ValueError("Invalid collection type: %s" % collection)


def get_variables_in_scope_with_name(name, collection=None):
  """Returns a tuple `tf.Variable`s in a scope with the given name.

  Args:
    name: `string` name to retrieve variables from.
    collection: Collection to restrict query to. By default this is
        `tf
----------
Solution: 

  scope_name = get_variable_scope_name(scope)

  if scope_name:
    # Escape the name in case it contains any "." characters. Add a closing
    # slash so we will not search any scopes that have this scope name as a
    # prefix.
    scope_name = re.escape(scope_name) + "/"

  return tuple(tf.get_collection(collection, scope_name))
--------------------
Prompt: 
def column_definition(table_name, col_name):
    """
    Get the source of a column function.

    If a column is a registered Series and not a function then all that is
    returned is {'type': 'series'}.

    If the column is a registered function then the JSON returned has keys
    "type", "filename", "lineno", "text", and "html". "text" is the raw
    text of the function, "html" has been marked up by Pygments.

    """
----------
Output: 

    try:
        return _column_source[table_name][col_name]
    except KeyError:
        raise ValueError("No source for column '%s' in %s" % (col_name, table_name))


def _get_lexer_by_name(name):
    """
    Get a lexer by name.

    If a name is not a registered function then the lexer returned has
    "type", "filename", "lineno", "filename", and "lineno". "type" is the name of a
    registered function then "html" has been marked up by Pygments.

    If the name is
----------
Solution: 

    col_type = orca.get_table(table_name).column_type(col_name)

    if col_type != 'function':
        return jsonify(type=col_type)

    filename, lineno, source = \
        orca.get_raw_column(table_name, col_name).func_source_data()

    html = highlight(source, PythonLexer(), HtmlFormatter())

    return jsonify(
        type='function', filename=filename, lineno=lineno, text=source,
        html=html)
--------------------
Prompt: 
def bloomfilter(collection, on, column, capacity=3000, error_rate=0.01):
    """
    Filter collection on the `on` sequence by BloomFilter built by `column`

    :param collection:
    :param on: sequence or column name
    :param column: instance of Column
    :param capacity: numbers of capacity
    :type capacity: int
    :param error_rate: error rate
    :type error_rate: float
    :return: collection

    :Example:

    >>> df1 = DataFrame(pd.DataFrame({'a': ['name1', 'name2', 'name3', 'name1'], 'b': [1, 2, 3, 4]}))
    >>> df2 = DataFrame(pd.DataFrame({'a': ['name1']}))
    >>> df1.bloom_filter('a', df2.a)
           a  b
    0  name1  1
    1  name1  4
    """
----------
Output: 
name3'],
   ...                    'b': ['name1', 'name2', 'name3'],
   ...                    'c': [1, 2, 3]}, columns=['a', 'b', 'c']))
    >>> df2 = DataFrame(pd.DataFrame({'a': ['name1', 'name2', 'name3'],
   ...                    'b': ['name1', 'name2', 'name3'],
   ...                    'c': [1, 2, 3]}, columns=['a', 'b', 'c']))
    >>> bloomfilter(df1, 'a', 'b', error_rate
----------
Solution: 



    if not isinstance(column, Column):
        raise TypeError('bloomfilter can only filter on the column of a collection')

    # to make the class pickled right by the cloudpickle
    with open(os.path.join(path, 'lib', 'bloomfilter.py')) as bloomfilter_file:
        local = {}
        six.exec_(bloomfilter_file.read(), local)
        BloomFilter = local['BloomFilter']

        col_name = column.source_name or column.name

        on_name = on.name if isinstance(on, SequenceExpr) else on
        rand_name = '%s_%s'% (on_name, str(uuid.uuid4()).replace('-', '_'))
        on_col = collection._get_field(on).rename(rand_name)
        src_collection = collection
        collection = collection[collection, on_col]

        @output(src_collection.schema.names, src_collection.schema.types)
        class Filter(object):
            def __init__(self, resources):
                table = resources[0]

                bloom = BloomFilter(capacity, error_rate)
                for row in table:
                    bloom.add(str(getattr(row, col_name)))

                self.bloom = bloom

            def __call__(self, row):
                if str(getattr(row, rand_name)) not in self.bloom:
                    return
                return row[:-1]

        return collection.apply(Filter, axis=1, resources=[column.input, ])
--------------------
Prompt: 
def enabled(name='allprofiles'):
    """
    Enable all the firewall profiles (Windows only)

    Args:
        profile (Optional[str]): The name of the profile to enable. Default is
            ``allprofiles``. Valid options are:

            - allprofiles
            - domainprofile
            - privateprofile
            - publicprofile

    Example:

    .. code-block:: yaml

        # To enable the domain profile
        enable_domain:
          win_firewall.enabled:
            - name: domainprofile

        # To enable all profiles
        enable_all:
          win_firewall.enabled:
            - name: allprofiles
    """
----------
Output: 

    ret = {'name': name,
           'changes': {},
          'result': True,
           'comment': ''}

    if profile:
        ret['comment'] = 'Enabled'
        ret['changes']['enabled'] = profile

    return ret


def disable_domain(name='winfirewall'):
    """
    Disable all the windows firewall profiles (Windows only)

    Args:
        name (Optional[str]): The name of the profile to disable. Default is
            ``winfirewall``. Valid options are:

            - winfirewall
            - domainprofile
            - privateprofile
            - allprofiles
            - profile
            - allprofiles
            -
----------
Solution: 

    ret = {'name': name,
           'result': True,
           'changes': {},
           'comment': ''}

    profile_map = {'domainprofile': 'Domain',
                   'privateprofile': 'Private',
                   'publicprofile': 'Public',
                   'allprofiles': 'All'}

    # Make sure the profile name is valid
    if name not in profile_map:
        raise SaltInvocationError('Invalid profile name: {0}'.format(name))

    current_config = __salt__['firewall.get_config']()
    if name != 'allprofiles' and profile_map[name] not in current_config:
        ret['result'] = False
        ret['comment'] = 'Profile {0} does not exist in firewall.get_config' \
                         ''.format(name)
        return ret

    for key in current_config:
        if not current_config[key]:
            if name == 'allprofiles' or key == profile_map[name]:
                ret['changes'][key] = 'enabled'

    if __opts__['test']:
        ret['result'] = not ret['changes'] or None
        ret['comment'] = ret['changes']
        ret['changes'] = {}
        return ret

    # Enable it
    if ret['changes']:
        try:
            ret['result'] = __salt__['firewall.enable'](name)
        except CommandExecutionError:
            ret['comment'] = 'Firewall Profile {0} could not be enabled' \
                             ''.format(profile_map[name])
    else:
        if name == 'allprofiles':
            msg = 'All the firewall profiles are enabled'
        else:
            msg = 'Firewall profile {0} is enabled'.format(name)
        ret['comment'] = msg

    return ret
--------------------
Prompt: 
def get_f_ty_callback(self):
        """ Generates a callback for evaluating ``self.exprs``. """
----------
Output: 

        return lambda x: self.exprs[x]

    def get_f_ty_expr(self, f):
        """ Evaluates ``f`` on the expression tree. """
        if isinstance(f, ExprId):
            return f
        elif isinstance(f, ExprInt):
            return f.arg
        elif isinstance(f, ExprId):
            return f.id
        elif isinstance(f, ExprOp):
            return f.op
        elif isinstance(f, ExprId):
            return f.id
        elif isinstance(f, ExprOp):
            return f.args[0]
        else:
           
----------
Solution: 

        cb = self._callback_factory(self.exprs)
        lb = self.lower_bounds
        ub = self.upper_bounds
        if lb is not None or ub is not None:
            def _bounds_wrapper(t, y, p=(), be=None):
                if lb is not None:
                    if np.any(y < lb - 10*self._current_integration_kwargs['atol']):
                        raise RecoverableError
                    y = np.array(y)
                    y[y < lb] = lb[y < lb]
                if ub is not None:
                    if np.any(y > ub + 10*self._current_integration_kwargs['atol']):
                        raise RecoverableError
                    y = np.array(y)
                    y[y > ub] = ub[y > ub]
                return cb(t, y, p, be)
            return _bounds_wrapper
        else:
            return cb
--------------------
Prompt: 
def load(self):
        """load and return the module/attribute."""
----------
Output: 

        # TODO: add a test for this
        if self.module is not None:
            return self.module
        return None

    def get_attribute(self, attribute):
        """Get a module attribute."""
        if attribute is None:
            return None
        if attribute == 'name':
            return self.name
        elif attribute == 'path':
            return self.path
        elif attribute =='module':
            return self.module
        elif attribute == 'path':
            return self.path
        elif attribute =='module_args':
            return self.module_args
        elif attribute == 'path_args':
            return self
----------
Solution: 

        module = importlib.import_module(self.import_path)
        if self.attr:
            return getattr(module, self.attr)
        return module
--------------------
Prompt: 
def child(self, group_name, page_size=500):
        """ Returns the child ad group that matches the provided group_name or none if the child does not exist.

        :param group_name: The name of the child group. NOTE: A name does not contain 'CN=' or 'OU='
        :type group_name: str
        :param page_size (optional): Many servers have a limit on the number of results that can be returned.
                                     Paged searches circumvent that limit. Adjust the page_size to be below the
                                     server's size limit. (default: 500)
        :type page_size: int

        """
----------
Output: 
)
        :type page_size: int
        """
        if group_name.lower() == 'cn':
            return self.get_cn_ad_group()
        elif group_name.lower() == 'ou':
            return self.get_ou_ad_group()
        else:
            return None

    def get_cn_ad_group(self):
        """ Returns the ad group that matches the provided group_name or none if the child does not exist.

        :return: The ad group that matches the provided group_name or none if the child does not exist.
        :rtype: str
       
----------
Solution: 


        object_class = self.get_attribute("objectClass")
        group_type = object_class[-1] if object_class else None

        if group_type == "group":
            connection_dict = self.GROUP_SINGLE_CHILD_SEARCH
        elif group_type == "organizationalUnit":
            connection_dict = self.OU_SINGLE_CHILD_SEARCH
        else:
            logger.debug("Unable to process child {child} of group {group_dn} with type {group_type}.".format(
                child=group_name, group_dn=self.group_dn, group_type=group_type
            ))
            return []

        entry_list = self.ldap_connection.extend.standard.paged_search(
            search_base=connection_dict['base_dn'],
            search_filter=connection_dict['filter_string'].format(child_group_name=escape_query(group_name)),
            search_scope=connection_dict['scope'],
            attributes=connection_dict['attribute_list'],
            paged_size=page_size
        )

        results = [result["dn"] for result in entry_list if result["type"] == "searchResEntry"]

        if len(results) != 1:
            logger.debug("Search returned {count} results: {results}".format(count=len(results), results=results))

        if results:
            return ADGroup(
                group_dn=results[0], server_uri=self.server_uri, base_dn=self.base_dn,
                user_lookup_attr=self.user_lookup_attr, group_lookup_attr=self.group_lookup_attr,
                attr_list=self.attr_list, bind_dn=self.bind_dn, bind_password=self.bind_password,
                user_search_base_dn=self.user_search_base_dn, group_search_base_dn=self.user_search_base_dn
            )
        else:
            return None
--------------------
Prompt: 
def list_nodes_full(**kwargs):
    """
    Return all data on nodes
    """
----------
Output: 

    nodes = []
    for node in NODES:
        nodes.append(node)
    return nodes


def list_nodes_select(name, **kwargs):
    """
    Return all data on nodes
    """
    nodes = list_nodes_full(**kwargs)
    return nodes[name]


def list_nodes_select_by_name(name, **kwargs):
    """
    Return data on nodes
    """
    nodes = list_nodes_select(name, **kwargs)
    return nodes


def list_nodes_select_by_id(id, **kwargs):
    """
    Return
----------
Solution: 

    nodes = _query('server/list')
    ret = {}

    for node in nodes:
        name = nodes[node]['label']
        ret[name] = nodes[node].copy()
        ret[name]['id'] = node
        ret[name]['image'] = nodes[node]['os']
        ret[name]['size'] = nodes[node]['VPSPLANID']
        ret[name]['state'] = nodes[node]['status']
        ret[name]['private_ips'] = nodes[node]['internal_ip']
        ret[name]['public_ips'] = nodes[node]['main_ip']

    return ret
--------------------
Prompt: 
def train_df(self, df, drop=False):
        """
        Train scales from a dataframe
        """
----------
Output: 

        self.scale_df = df
        self.drop = drop

    def scale_df_to_dict(self):
        """
        Convert a scale to a dictionary
        """
        self.scale_dict = {}
        for key, scale in self.scale_dict.items():
            self.scale_dict[key] = scale

    def get_scale(self, key):
        """
        Get a scale
        """
        return self.scale_dict[key]

    def get_scale_by_name(self, name):
        """
        Get a scale by name
        """
        return self.scale_
----------
Solution: 

        if (len(df) == 0) or (len(self) == 0):
            return df

        # Each scale trains the columns it understands
        for sc in self:
            sc.train_df(df)
        return df
--------------------
Prompt: 
def validatePopElement(self, ctxt, elem, qname):
        """Pop the element end from the validation stack. """
----------
Output: 

        if not self.isNil():
            if elem.get("name") == "body":
                self.body = self.getBody()
            elif elem.get("name") == "head":
                self.head = self.getHead()
            elif elem.get("name") == "body-end":
                self.body_end = self.getBodyEnd()
            elif elem.get("name") == "head-end":
                self.head_end = self.getHeadEnd()
            elif elem.get("name") == "body-end":
                self.body_end =
----------
Solution: 

        if ctxt is None: ctxt__o = None
        else: ctxt__o = ctxt._o
        if elem is None: elem__o = None
        else: elem__o = elem._o
        ret = libxml2mod.xmlValidatePopElement(ctxt__o, self._o, elem__o, qname)
        return ret
--------------------
Prompt: 
def set_error(self, error_shortmsg: str, error_longmsg: str):
        """ Set the stage to error and add a message """
----------
Output: 

        self.error_shortmsg = error_shortmsg
        self.error_longmsg = error_longmsg

    def __str__(self):
        """ String representation of the job """
        return "Job: {0}, {1}, {2}, {3}, {4}, {5}, {6}, {7}, {8}, {9}, {10}, {11}, {12}, {13}, {14}, {15}, {16}, {17}, {18}, {19}, {20}" \
           .format(self.name, self.job_id, self.error_shortmsg
----------
Solution: 

        LOG.error(f"Update session: error in stage {self._stage.name}: "
                  f"{error_shortmsg}: {error_longmsg}")
        self._error = Value(error_shortmsg, error_longmsg)
        self.set_stage(Stages.ERROR)
--------------------
Prompt: 
def create_package(package_format, owner, repo, **kwargs):
    """Create a new package in a repository."""
----------
Output: 

    if kwargs.get('name'):
        name = kwargs.get('name')
    else:
        name = package_format.name
    if kwargs.get('version'):
        version = kwargs.get('version')
    else:
        version = package_format.version
    if kwargs.get('summary'):
        summary = kwargs.get('summary')
    else:
        summary = package_format.summary
    if kwargs.get('description'):
        description = kwargs.get('description')
    else:
        description = package_format.description
    if kwargs.get('url'):
        url = kwargs.get('url
----------
Solution: 

    client = get_packages_api()

    with catch_raise_api_exception():
        upload = getattr(client, "packages_upload_%s_with_http_info" % package_format)

        data, _, headers = upload(
            owner=owner, repo=repo, data=make_create_payload(**kwargs)
        )

    ratelimits.maybe_rate_limit(client, headers)
    return data.slug_perm, data.slug
--------------------
Prompt: 
def get_device(ads, **kwargs):
    """Finds a unique AndroidDevice instance from a list that has specific
    attributes of certain values.

    Deprecated, use `get_devices(ads, **kwargs)[0]` instead.
    This method will be removed in 1.8.

    Example:
        get_device(android_devices, label='foo', phone_number='1234567890')
        get_device(android_devices, model='angler')

    Args:
        ads: A list of AndroidDevice instances.
        kwargs: keyword arguments used to filter AndroidDevice instances.

    Returns:
        The target AndroidDevice instance.

    Raises:
        Error: None or more than one device is matched.
    """
----------
Output: 
Device instance.
    """
    warnings.warn(
        'get_device is deprecated, use `get_devices(ads, **kwargs)[0]` instead.',
        DeprecationWarning)
    return get_devices(ads, **kwargs)[0]


def get_device_by_name(name):
    """Finds a unique AndroidDevice instance by name.

    Deprecated, use `get_device(android_devices, name)[0]` instead.
    This method will be removed in 1.8.

    Example:
        get_device_by_name('foo')
        get_device_by_name('angler
----------
Solution: 


    filtered = get_devices(ads, **kwargs)
    if len(filtered) == 1:
        return filtered[0]
    else:
        serials = [ad.serial for ad in filtered]
        raise Error('More than one device matched: %s' % serials)
--------------------
Prompt: 
def querystring(self):
        """Return original querystring but containing only managed keys

        :return dict: dict of managed querystring parameter
        """
----------
Output: 

        return self._querystring

    def get_parameter(self, name):
        """Get parameter value

        :param name: name of the parameter
        :return str: parameter value
        """
        return self._parameters[name]

    def get_parameter_value(self, name):
        """Get parameter value

        :param name: name of the parameter
        :return str: parameter value
        """
        return self._parameters[name]

    def get_parameter_names(self):
        """Return the names of the parameters

        :return list: list of parameter names
        """
        return self._parameters.keys()

   
----------
Solution: 

        return {key: value for (key, value) in self.qs.items()
                if key.startswith(self.MANAGED_KEYS) or self._get_key_values('filter[')}
--------------------
Prompt: 
def reshape(x,input_dim):
    """
    Reshapes x into a matrix with input_dim columns

    """
----------
Output: 

    return tf.reshape(x,(-1,input_dim))

def batch_norm(x,input_dim):
    """
    Batch Normalization

    """
    return tf.nn.batch_norm(x,input_dim),input_dim

def conv2d(x,filter_shape,strides,padding='SAME'):
    """
    Convenience function to apply a 2D convolution to a tensor

    """
    return tf.nn.conv2d(x,filter_shape,strides=strides,padding=padding)

def max_pool(x,ksize,strides
----------
Solution: 

    x = np.array(x)
    if x.size ==input_dim:
        x = x.reshape((1,input_dim))
    return x
--------------------
Prompt: 
def nas_auto_qos_set_cos(self, **kwargs):
        """Auto Generated Code
        """
----------
Output: 

        config = ET.Element("config")
        auto_qos_set = ET.SubElement(config, "auto-qos-set")
        auto_qos_set_cos.text = kwargs.pop('cos', None)
        if kwargs:
            for k, v in kwargs.items():
                auto_qos_set_cos_xml = ET.SubElement(auto_qos_set, "auto-qos-set-cos")
                auto_qos_set_cos_xml.text = str(v)
                ET.SubElement(auto_qos_set_cos, "value").text = str
----------
Solution: 

        config = ET.Element("config")
        nas = ET.SubElement(config, "nas", xmlns="urn:brocade.com:mgmt:brocade-qos")
        auto_qos = ET.SubElement(nas, "auto-qos")
        set = ET.SubElement(auto_qos, "set")
        cos = ET.SubElement(set, "cos")
        cos.text = kwargs.pop('cos')

        callback = kwargs.pop('callback', self._callback)
        return callback(config)
--------------------
Prompt: 
def extract_objects(self, fname, type_filter=None):
    """Extract objects from a source file

    Args:
      fname(str): Name of file to read from
      type_filter (class, optional): Object class to filter results
    Returns:
      List of objects extracted from the file.
    """
----------
Output: 

    return extract_objects_from_file(fname, type_filter)

  def get_objects(self, type_filter=None):
    """Get objects from a source file

    Args:
      type_filter(class, optional): Object class to filter results
    Returns:
      List of objects extracted from the file.
    """
    return get_objects_from_file(self.fname_to_objects, type_filter)

  def fname_to_objects(self, fname):
    """Convert a source file name to a list of objects

    Args:
      fname(str): Source file name to convert
----------
Solution: 

    objects = []
    if fname in self.object_cache:
      objects = self.object_cache[fname]
    else:
      with io.open(fname, 'rt', encoding='utf-8') as fh:
        text = fh.read()
        objects = parse_verilog(text)
        self.object_cache[fname] = objects

    if type_filter:
      objects = [o for o in objects if isinstance(o, type_filter)]

    return objects
--------------------
Prompt: 
def encrypt_file(cls, key, in_filename, out_filename=None, chunksize=64 * 1024):
        """ Encrypts a file using AES (CBC mode) with the
            given key.
    
            key:
                The encryption key - a string that must be
                either 16, 24 or 32 bytes long. Longer keys
                are more secure.
    
            in_filename:
                Name of the input file
    
            out_filename:
                If None, '<in_filename>.enc' will be used.
    
            chunksize:
                Sets the size of the chunk which the function
                uses to read and encrypt the file. Larger chunk
                sizes can be faster for some files and machines.
                chunksize must be divisible by 16.
        """
----------
Output: 
 and encrypt. The default is 64MB.
    
            Returns:
                A tuple (status, ciphertext) where status is a
                string indicating the status of the operation.
                The ciphertext is a string of bytes.
        """
        if out_filename is None:
            out_filename = in_filename
        if out_filename is None:
            out_filename = in_filename + '.enc'
        if out_filename is None:
            out_filename = out_filename + '.enc'
        if out_filename is None:
            out_filename = out_filename + '.enc'
        if not os.path.
----------
Solution: 

        if not out_filename:
            out_filename = in_filename + '.enc'

        iv = ''.join(chr(random.randint(0, 0xFF)) for _ in range(16))
        encryptor = AES.new(key, AES.MODE_CBC, iv)
        filesize = os.path.getsize(in_filename)

        with open(in_filename, 'rb') as infile:
            with open(out_filename, 'wb') as outfile:
                outfile.write(struct.pack('<Q', filesize))
                outfile.write(iv)

                while True:
                    chunk = infile.read(chunksize)
                    if len(chunk) == 0:
                        break
                    elif len(chunk) % 16 != 0:
                        chunk += ' ' * (16 - len(chunk) % 16)

                    outfile.write(encryptor.encrypt(chunk))
--------------------
Prompt: 
def off_datastream(self, datastream):
        """
        To turn off datastream
        :param datastream: string
        """
----------
Output: 

        self.datastream = datastream
        self.datastream_name = datastream

    def get_datastream(self):
        """
        Get the datastream name
        :return: string
        """
        return self.datastream

    def get_datastream_name(self):
        """
        Get the datastream name
        :return: string
        """
        return self.datastream_name

    def get_name(self):
        """
        Get the name of the file
        :return: string
        """
        return self.name

    def get_name_with_ext(self, ext):
        """
       
----------
Solution: 

        url = '/datastream/' + str(datastream) + '/off'
        response = self.http.post(url,"")
        return response
--------------------
Prompt: 
def get_expired_tasks(self, key_prefix, time, batch_size, client=None):
        """
        Returns a list of expired tasks (older than ``time``) by looking at all
        active queues. The list is capped at ``batch_size``. The list contains
        tuples (queue, task_id).
        """
----------
Output: 

        now = datetime.now()
        expired_tasks = []
        for queue in self.active_queues:
            if client is None:
                client = self.get_client()
            expired_tasks.append((queue, now))
        expired_tasks = expired_tasks[:batch_size]
        return expired_tasks

    def get_expired_tasks_by_prefix(self, key_prefix, batch_size, client=None):
        """
        Returns a list of expired tasks (older than ``time``) by looking at all
        active queues. The list is capped at ``batch_size``. The
----------
Solution: 

        result = self._get_expired_tasks(args=[key_prefix, time, batch_size],
                                         client=client)

        # [queue1, task1, queue2, task2] -> [(queue1, task1), (queue2, task2)]
        return list(zip(result[::2], result[1::2]))
--------------------
Prompt: 
def gather_votes(self):
        """Gather votes from all the underlying slave environments for the
        current list of candidates.

        The votes are stored in :attr:`votes`, overriding any previous votes.
        """
----------
Output: 

        self.votes = [
            self.get_next_candidate(env) for env in self.slave_environments
        ]

    def get_next_candidate(self, env):
        """Get the next candidate for the given environment.

        :param env: The environment to get the next candidate for.
        :type env: :class:`robottelo.constants.Environment`

        :returns: The next candidate for the given environment.
        :rtype: :class:`robottelo.types.Candidate`

        :raises: :class:`robottelo.ssh.exception.PasswordFileFailedToCreateError
----------
Solution: 

        async def slave_task(addr, candidates):
            r_manager = await self.env.connect(addr)
            return await r_manager.gather_votes(candidates)

        if len(self.candidates) == 0:
            self._log(logging.DEBUG, "Could not gather votes because there "
                      "are no candidates!")
            self._votes = []
            return
        self._log(logging.DEBUG, "Gathering votes for {} candidates."
                  .format(len(self.candidates)))

        if self._single_env:
            self._votes = self.env.gather_votes(self.candidates)
        else:
            mgrs = self.get_managers()
            tasks = create_tasks(slave_task, mgrs, self.candidates)
            self._votes = run(tasks)
--------------------
Prompt: 
def unfold_lines(string):
    """Join lines that are wrapped.

    Any line that starts with a space or tab is joined to the previous
    line.
    """
----------
Output: 

    return re.sub(r'\s+','', string)


def _get_indent(line):
    """Return the number of spaces to indent the given line.

    This is like _get_indent but doesn't indent the lines.
    """
    return len(line) - len(line.lstrip())


def _get_indent_spaces(line):
    """Return number of spaces to indent the given line.

    This is like _get_indent but doesn't indent the lines.
    """
    return len(line) - len(line.rstrip())


def _get_indent
----------
Solution: 

    assert isinstance(string, str), 'Expect str. Got {}'.format(type(string))
    lines = string.splitlines()
    line_buffer = io.StringIO()

    for line_number in range(len(lines)):
        line = lines[line_number]
        if line and line[0:1] in (' ', '\t'):
            line_buffer.write(' ')
        elif line_number != 0:
            line_buffer.write('\r\n')
        line_buffer.write(line.strip())

    line_buffer.write('\r\n')

    return line_buffer.getvalue()
--------------------
Prompt: 
def _extract_symbols(self, symbols, default=None):
        """! @brief Fill 'symbols' field with required flash algo symbols"""
----------
Output: 

        if default is not None:
            return default
        return [
            "0x%x" % (self.address + i) for i in range(0x100)
        ]

    def _get_flash_algo_name(self):
        """! @brief Return the flash algo name"""
        return self.flash_algo.name

    def _set_flash_algo_name(self, new_name):
        """! @brief Set the flash algo name"""
        self.flash_algo.name = new_name

    def _get_flash_algo_address(self):
        """!
----------
Solution: 

        to_ret = {}
        for symbol in symbols:
            symbolInfo = self.elf.symbol_decoder.get_symbol_for_name(symbol)
            if symbolInfo is None:
                if default is not None:
                    to_ret[symbol] = default
                    continue
                raise FlashAlgoException("Missing symbol %s" % symbol)
            to_ret[symbol] = symbolInfo.address
        return to_ret
--------------------
Prompt: 
def _dataframe_fields(self):
        """
        Creates a dictionary of all fields to include with DataFrame.

        With the result of the calls to class properties changing based on the
        class index value, the dictionary should be regenerated every time the
        index is changed when the dataframe property is requested.

        Returns
        -------
        dictionary
            Returns a dictionary where the keys are the shortened ``string``
            attribute names and the values are the actual value for each
            attribute for the specified index.
        """
----------
Output: 

        return _dataframe_fields(self) + _dataframe_fields(self.index)

    def __repr__(self):
        return self.string

    def __str__(self):
        return self._dataframe_fields()

    def __getitem__(self, key):
        """
        Returns a sliced view of the view corresponding to the given
        key.

        Parameters
        ----------
        key : str
            The name of the view to be sliced.

        Returns
        -------
        sliced view
            Returns a view of the sliced ``key`` with the given ``string``
            attribute set to the sliced ``key``.
       
----------
Solution: 

        fields_to_include = {
            'adjusted_net_yards_per_attempt_index':
            self.adjusted_net_yards_per_attempt_index,
            'adjusted_net_yards_per_pass_attempt':
            self.adjusted_net_yards_per_pass_attempt,
            'adjusted_yards_per_attempt': self.adjusted_yards_per_attempt,
            'adjusted_yards_per_attempt_index':
            self.adjusted_yards_per_attempt_index,
            'all_purpose_yards': self.all_purpose_yards,
            'approximate_value': self.approximate_value,
            'assists_on_tackles': self.assists_on_tackles,
            'attempted_passes': self.attempted_passes,
            'birth_date': self.birth_date,
            'blocked_punts': self.blocked_punts,
            'catch_percentage': self.catch_percentage,
            'completed_passes': self.completed_passes,
            'completion_percentage_index': self.completion_percentage_index,
            'espn_qbr': self.espn_qbr,
            'extra_point_percentage': self.extra_point_percentage,
            'extra_points_attempted': self.extra_points_attempted,
            'extra_points_made': self.extra_points_made,
            'field_goal_percentage': self.field_goal_percentage,
            'field_goals_attempted': self.field_goals_attempted,
            'field_goals_made': self.field_goals_made,
            'fifty_plus_yard_field_goal_attempts':
            self.fifty_plus_yard_field_goal_attempts,
            'fifty_plus_yard_field_goals_made':
            self.fifty_plus_yard_field_goals_made,
            'fourth_quarter_comebacks': self.fourth_quarter_comebacks,
            'fourty_to_fourty_nine_yard_field_goal_attempts':
            self.fourty_to_fourty_nine_yard_field_goal_attempts,
            'fourty_to_fourty_nine_yard_field_goals_made':
            self.fourty_to_fourty_nine_yard_field_goals_made,
            'fumbles': self.fumbles,
            'fumbles_forced': self.fumbles_forced,
            'fumbles_recovered': self.fumbles_recovered,
            'fumbles_recovered_for_touchdown':
            self.fumbles_recovered_for_touchdown,
            'game_winning_drives': self.game_winning_drives,
            'games': self.games,
            'games_started': self.games_started,
            'height': self.height,
            'interception_percentage': self.interception_percentage,
            'interception_percentage_index':
            self.interception_percentage_index,
            'interceptions': self.interceptions,
            'interceptions_returned_for_touchdown':
            self.interceptions_returned_for_touchdown,
            'interceptions_thrown': self.interceptions_thrown,
            'kickoff_return_touchdown': self.kickoff_return_touchdown,
            'kickoff_return_yards': self.kickoff_return_yards,
            'kickoff_returns': self.kickoff_returns,
            'less_than_nineteen_yards_field_goal_attempts':
            self.less_than_nineteen_yards_field_goal_attempts,
            'less_than_nineteen_yards_field_goals_made':
            self.less_than_nineteen_yards_field_goals_made,
            'longest_field_goal_made': self.longest_field_goal_made,
            'longest_interception_return': self.longest_interception_return,
            'longest_kickoff_return': self.longest_kickoff_return,
            'longest_pass': self.longest_pass,
            'longest_punt': self.longest_punt,
            'longest_punt_return': self.longest_punt_return,
            'longest_reception': self.longest_reception,
            'longest_rush': self.longest_rush,
            'name': self.name,
            'net_yards_per_attempt_index': self.net_yards_per_attempt_index,
            'net_yards_per_pass_attempt': self.net_yards_per_pass_attempt,
            'passer_rating_index': self.passer_rating_index,
            'passes_defended': self.passes_defended,
            'passing_completion': self.passing_completion,
            'passing_touchdown_percentage': self.passing_touchdown_percentage,
            'passing_touchdowns': self.passing_touchdowns,
            'passing_yards': self.passing_yards,
            'passing_yards_per_attempt': self.passing_yards_per_attempt,
            'player_id': self.player_id,
            'position': self.position,
            'punt_return_touchdown': self.punt_return_touchdown,
            'punt_return_yards': self.punt_return_yards,
            'punt_returns': self.punt_returns,
            'punts': self.punts,
            'qb_record': self.qb_record,
            'quarterback_rating': self.quarterback_rating,
            'receiving_touchdowns': self.receiving_touchdowns,
            'receiving_yards': self.receiving_yards,
            'receiving_yards_per_game': self.receiving_yards_per_game,
            'receiving_yards_per_reception':
            self.receiving_yards_per_reception,
            'receptions': self.receptions,
            'receptions_per_game': self.receptions_per_game,
            'rush_attempts': self.rush_attempts,
            'rush_attempts_per_game': self.rush_attempts_per_game,
            'rush_touchdowns': self.rush_touchdowns,
            'rush_yards': self.rush_yards,
            'rush_yards_per_attempt': self.rush_yards_per_attempt,
            'rush_yards_per_game': self.rush_yards_per_game,
            'rushing_and_receiving_touchdowns':
            self.rushing_and_receiving_touchdowns,
            'sack_percentage': self.sack_percentage,
            'sack_percentage_index': self.sack_percentage_index,
            'sacks': self.sacks,
            'safeties': self.safeties,
            'season': self.season,
            'tackles': self.tackles,
            'team_abbreviation': self.team_abbreviation,
            'thirty_to_thirty_nine_yard_field_goal_attempts':
            self.thirty_to_thirty_nine_yard_field_goal_attempts,
            'thirty_to_thirty_nine_yard_field_goals_made':
            self.thirty_to_thirty_nine_yard_field_goals_made,
            'times_pass_target': self.times_pass_target,
            'times_sacked': self.times_sacked,
            'total_punt_yards': self.total_punt_yards,
            'touchdown_percentage_index': self.touchdown_percentage_index,
            'touches': self.touches,
            'twenty_to_twenty_nine_yard_field_goal_attempts':
            self.twenty_to_twenty_nine_yard_field_goal_attempts,
            'twenty_to_twenty_nine_yard_field_goals_made':
            self.twenty_to_twenty_nine_yard_field_goals_made,
            'weight': self.weight,
            'yards_from_scrimmage': self.yards_from_scrimmage,
            'yards_lost_to_sacks': self.yards_lost_to_sacks,
            'yards_per_attempt_index': self.yards_per_attempt_index,
            'yards_per_completed_pass': self.yards_per_completed_pass,
            'yards_per_game_played': self.yards_per_game_played,
            'yards_per_kickoff_return': self.yards_per_kickoff_return,
            'yards_per_punt': self.yards_per_punt,
            'yards_per_punt_return': self.yards_per_punt_return,
            'yards_per_touch': self.yards_per_touch,
            'yards_recovered_from_fumble': self.yards_recovered_from_fumble,
            'yards_returned_from_interception':
            self.yards_returned_from_interception
        }
        return fields_to_include
--------------------
Prompt: 
def update_metric(self, eval_metric, labels, pre_sliced):
        """Accumulate the performance according to `eval_metric` on all devices
        by comparing outputs from [begin, end) to labels. By default use all
        outputs.

        Parameters
        ----------
        eval_metric : EvalMetric
            The metric used for evaluation.
        labels : list of NDArray
            Typically comes from `label` of a `DataBatch`.
        pre_sliced : bool
            Whether labels are already sliced.
        begin : int
            Starting index of used outputs.
        end : int or None
            Ending index of used outputs.
        """
----------
Output: 
 of used outputs.
        """
        if self.binded:
            self.binded = False
            self.bin_ids = None
            self.bin_names = None
            self.bin_ids = None
            self.binded_names = None
        else:
            self.binded = True
            self.bin_ids = labels
            self.bin_names = [x[1] for x in self.bin_names]
            self.binded_names = [x[0] for x in self.bin_names]
            if end is None:
                end = len(self.bin
----------
Solution: 

        for current_exec, (texec, islice) in enumerate(zip(self.execs, self.slices)):
            if not pre_sliced:
                labels_slice = []
                for label, axis in zip(labels, self.label_layouts):
                    if axis == 0:
                        # slicing NDArray along axis 0 can avoid copying
                        labels_slice.append(label[islice])
                    elif axis > 0:
                        # pylint: disable=no-member
                        label_my_slice = nd.slice_axis(label, axis=axis, begin=islice.start,
                                                       end=islice.stop).as_in_context(label.context)
                        # pylint: enable=no-member
                        labels_slice.append(label_my_slice)
                    else:
                        labels_slice.append(label)
            else:
                labels_slice = labels[current_exec]

            labels_ = OrderedDict(zip(self.label_names, labels_slice))
            preds = OrderedDict(zip(self.output_names, texec.outputs))
            eval_metric.update_dict(labels_, preds)
--------------------
Prompt: 
def get_number(s, cast=int):
    """
    Try to get a number out of a string, and cast it.
    """
----------
Output: 

    try:
        return int(s)
    except ValueError:
        return cast(s, 0)


def get_string(s, cast=str):
    """
    Try to get a string out of a string, and cast it.
    """
    try:
        return cast(s, s)
    except ValueError:
        return s


def get_number_of_lines(s):
    """
    Try to get the number of lines in a string, and return the
    number of lines in the string.
    """
    try:
        return str(s).count('\n')
    except ValueError:
----------
Solution: 

    import string
    d = "".join(x for x in str(s) if x in string.digits)
    return cast(d)
--------------------
Prompt: 
def _to_vcard(self, entry):
        """Return a vCard of the Abook entry"""
----------
Output: 

        return self._vcard

    @property
    def vcard_id(self):
        """Return the vCard ID"""
        return self._vcard_id

    @vcard_id.setter
    def vcard_id(self, vcard_id):
        """Set the vCard ID"""
        self._vcard_id = vcard_id

    @property
    def vcard_name(self):
        """Return the vCard name"""
        return self._vcard

    @vcard_name.setter
    def vcard_name(self, vcard_name):
        """Set the
----------
Solution: 

        card = vCard()

        card.add('uid').value = Abook._gen_uid(entry)
        card.add('fn').value = entry['name']
        card.add('n').value = Abook._gen_name(entry['name'])

        if 'email' in entry:
            for email in entry['email'].split(','):
                card.add('email').value = email

        addr_comps = ['address', 'address2', 'city', 'country', 'zip', 'country']
        if any(comp in entry for comp in addr_comps):
            card.add('adr').value = Abook._gen_addr(entry)

        if 'other' in entry:
            tel = card.add('tel')
            tel.value = entry['other']

        if 'phone' in entry:
            tel = card.add('tel')
            tel.type_param = 'home'
            tel.value = entry['phone']

        if 'workphone' in entry:
            tel = card.add('tel')
            tel.type_param = 'work'
            tel.value = entry['workphone']

        if 'mobile' in entry:
            tel = card.add('tel')
            tel.type_param = 'cell'
            tel.value = entry['mobile']

        if 'nick' in entry:
            card.add('nickname').value = entry['nick']

        if 'url' in entry:
            card.add('url').value = entry['url']

        if 'notes' in entry:
            card.add('note').value = entry['notes']

        self._add_photo(card, entry['name'])

        return card
--------------------
Prompt: 
def bgp_normalize_table_data(bgp_table):
    """The 'show bgp all summary vrf all' table can have entries that wrap multiple lines.

    2001:db8:4:701::2
                4 65535  163664  163693      145    0    0     3w2d 3
    2001:db8:e0:dd::1
                4    10  327491  327278      145    0    0     3w1d 4

    Normalize this so the line wrap doesn't exit.
    """
----------
Output: 

    bgp_summary_vrf = {}
    for entry in bgp_table:
        bgp_summary_vrf[entry] = bgp_summary_vrf.get(entry, 0) + \
            bgp_summary_vrf.get(entry, 65535)


def bgp_normalize_table(bgp_table):
    """The'show bgp all summary vrf all' table can have entries that wrap multiple lines.

    2001:db8:4:701::2
                4 65535  163664  163693      145    0    0     3w2d 3
    2001:db8:e0
----------
Solution: 

    bgp_table = bgp_table.strip()
    bgp_multiline_pattern = r"({})\s*\n".format(IPV4_OR_IPV6_REGEX)
    # Strip out the newline
    return re.sub(bgp_multiline_pattern, r"\1", bgp_table)
--------------------
Prompt: 
def update_config(config_new, config_default):

    """
    Updates the loaded method configuration with default values.
    """
----------
Output: 


    if config_new is None:
        config_new = {}

    if config_default is None:
        config_default = {}

    if not isinstance(config_new, dict):
        raise ValueError("config_new must be a dictionary")

    if not isinstance(config_default, dict):
        raise ValueError("config_default must be a dictionary")

    if'method' not in config_new:
        config_new['method'] = {}

    if'method' not in config_default:
        config_default['method'] = {}

    if'method' not in config_new:
        config_
----------
Solution: 

    if any([isinstance(v, dict) for v in list(config_new.values())]):
        for k,v in list(config_new.items()):
            if isinstance(v,dict) and k in config_default:
                update_config(config_new[k],config_default[k])
            else:
                config_default[k] = v
    else:
        config_default.update(config_new)
    return config_default
--------------------
Prompt: 
def run_analysis(self, argv):
        """Run this analysis"""
----------
Output: 

        self.set_options(self.options.keys())
        self.options.update(self.options.get(self.options['--options'], {}))
        self.options['--output'] = self.options['--output']
        self.options['--verbose'] = self.options['--verbose']
        self.options['--force'] = self.options['--force']
        self.options['--no-check'] = self.options['--no-check']
        self.options['--no-check-unstable'] = self.options['--no-check-unstable']
----------
Solution: 

        args = self._parser.parse_args(argv)

        if not HAVE_ST:
            raise RuntimeError(
                "Trying to run fermipy analysis, but don't have ST")

        workdir = os.path.dirname(args.config)
        _config_file = self._clone_config_and_srcmaps(args.config, args.seed)

        gta = GTAnalysis(_config_file, logging={'verbosity': 3},
                         fileio={'workdir_regex': '\.xml$|\.npy$'})
        gta.load_roi(args.roi_baseline)

        simfile = os.path.join(workdir, 'sim_%s_%s.yaml' %
                               (args.sim, args.sim_profile))

        mcube_file = "%s_%s_%06i" % (args.sim, args.sim_profile, args.seed)
        sim_config = utils.load_yaml(simfile)

        injected_source = sim_config.get('injected_source', None)
        if injected_source is not None:
            src_dict =  injected_source['source_model']
            src_dict['ra'] = gta.config['selection']['ra']
            src_dict['dec'] = gta.config['selection']['dec']
            injected_name = injected_source['name']
            gta.add_source(injected_name, src_dict)
            gta.write_model_map(mcube_file)
            mc_spec_dict = dict(true_counts=gta.model_counts_spectrum(injected_name),
                                energies=gta.energies,
                                model=src_dict)
            mcspec_file = os.path.join(workdir,
                                       "mcspec_%s_%06i.yaml" % (mcube_file, args.seed))
            utils.write_yaml(mc_spec_dict, mcspec_file)
        else:
            injected_name = None

        gta.write_roi('sim_baseline_%06i' % args.seed)

        test_sources = []
        for profile in args.profiles:
            profile_path = os.path.join(workdir, 'profile_%s.yaml' % profile)
            test_source = load_yaml(profile_path)
            test_sources.append(test_source)
            first = args.seed
            last = first + args.nsims
            for seed in range(first, last):
                self._run_simulation(gta, args.roi_baseline,
                                     injected_name, test_sources, first, seed,
                                     non_null_src=args.non_null_src)
--------------------
Prompt: 
def get_check(self, check):
        """
        Returns an instance of the specified check.
        """
----------
Output: 

        # This is the main check.
        if check.name in self.checks:
            return self.checks[check.name](self)

        # Check if the check is a string.
        if not isinstance(check, str):
            raise ValueError("The check should be a string, not %s" % check)

        # Check if the check has an attribute.
        if check.name in self.checks_attrs:
            return self.checks_attrs[check.name](self)

        # Check if the check is a method.
        if not hasattr(check,'method'):
            raise ValueError("The
----------
Solution: 

        chk = self._check_manager.get(check)
        chk.set_entity(self)
        return chk
--------------------
Prompt: 
def GetRunlevelsLSB(states):
  """Accepts a string and returns a list of strings of numeric LSB runlevels."""
----------
Output: 

  return [s.strip() for s in states.split(',')]


def GetRunlevels(states):
  """Accepts a string and returns a list of strings of numeric runlevels."""
  return [s.strip() for s in states.split(',')]


def GetRunlevel(runlevel):
  """Accepts a numeric runlevel, returns a numeric runlevel."""
  if runlevel == 'None':
    return None
  return int(runlevel)


def GetRunlevelLSB(runlevels):
  """Accepts a list of runlevels, returns a list of strings of numeric runlevel
----------
Solution: 

  if not states:
    return set()
  valid = set(["0", "1", "2", "3", "4", "5", "6"])
  _LogInvalidRunLevels(states, valid)
  return valid.intersection(set(states.split()))
--------------------
Prompt: 
def add_coupon(self, coupon, idempotency_key=None):
		"""
		Add a coupon to a Customer.

		The coupon can be a Coupon object, or a valid Stripe Coupon ID.
		"""
----------
Output: 

		if not isinstance(coupon, StripeCoupon):
			raise ValueError("Coupon ID must be a valid Stripe Coupon object.")
		if not isinstance(idempotency_key, str):
			raise ValueError("Stripe Coupon ID must be a string.")
		if not idempotency_key:
			raise ValueError("Customer ID must be specified.")
		if not isinstance(coupon.id, int):
			raise ValueError("Stripe Coupon ID must be an integer.")
		if not isinstance(idempotency_key, str):
			raise ValueError("Stri
----------
Solution: 

		if isinstance(coupon, StripeModel):
			coupon = coupon.id

		stripe_customer = self.api_retrieve()
		stripe_customer["coupon"] = coupon
		stripe_customer.save(idempotency_key=idempotency_key)
		return self.__class__.sync_from_stripe_data(stripe_customer)
--------------------
Prompt: 
def on_privmsg(self, c, e):
        """
        This function runs when the bot receives a private message (query).
        """
----------
Output: 

        if len(e.arguments) < 2:
            return
        if e.arguments[1] == 'PRIVMSG':
            if len(e.arguments[2].split()) > 1:
                self.log.info('privmsg: %s', e.arguments[2])
            else:
                self.log.info('privmsg: %s', e.arguments[2])
        else:
            self.log.info('privmsg: %s', e.arguments[1])
            self.log.info('privmsg: %s', e.arguments[2])

    def on_pub
----------
Solution: 

        text = e.arguments[0]
        logger.debug('{0!s}'.format(e.source))
        metadata = self.set_metadata(e)
        metadata['is_private_message'] = True
        message = Message(text=text, metadata=metadata).__dict__
        self.baseplate.tell(message)
--------------------
Prompt: 
def delete_vnet(access_token, subscription_id, resource_group, name):
    """Delete a virtual network.

    Args:
        access_token (str): A valid Azure authentication token.
        subscription_id (str): Azure subscription id.
        resource_group (str): Azure resource group name.
        name (str): Name of the VNet.

    Returns:
        HTTP response. VNet JSON body.
    """
----------
Output: 

    return _delete_vnet_from_subscription(access_token, subscription_id, resource_group, name)


def _delete_vnet_from_subscription(access_token, subscription_id, resource_group, name):
    """Delete a body from a subscription.

    Args:
        access_token (str): A valid Azure authentication token.
        subscription_id (str): Azure subscription id.
        resource_group (str): Azure resource group name.
        name (str): Name of the VNet.
    """
    return _delete_vnet_from_subscription_with_
----------
Solution: 

    endpoint = ''.join([get_rm_endpoint(),
                        '/subscriptions/', subscription_id,
                        '/resourceGroups/', resource_group,
                        '/providers/Microsoft.Network/virtualNetworks/', name,
                        '?api-version=', NETWORK_API])
    return do_delete(endpoint, access_token)
--------------------
Prompt: 
def savefig(
    filename, width=None, height=None, fig=None, timeout_seconds=10, output_widget=None, headless=False, devmode=False
):
    """Save the figure to an image file.

    :param str filename: must have extension .png, .jpeg or .svg
    :param int width: the width of the image in pixels
    :param int height: the height of the image in pixels
    :type fig: ipyvolume.widgets.Figure or None
    :param fig: if None use the current figure
    :param float timeout_seconds: maximum time to wait for image data to return
    :param ipywidgets.Output output_widget: a widget to use as a context manager for capturing the data
    :param bool headless: if True, use headless chrome to save figure
    :param bool devmode: if True, attempt to get index.js from local js/dist folder
    """
----------
Output: 
 the timeout in seconds
    :param bool headless: if True, the figure is not shown until it is closed
    :param bool devmode: if True, the figure is opened in dev mode
    :type headless: bool
    :return: None
    :rtype: None

    """
    if fig is None:
        fig = ipyvolume.Figure()
    if headless:
        fig.show_headless = True
    if devmode:
        fig.show_devmode = True
    if output_widget is None:
        output_widget = ipyvolume.widgets.Image()
    if fig.show
----------
Solution: 

    __, ext = os.path.splitext(filename)
    format = ext[1:]
    assert format in ['png', 'jpeg', 'svg'], "image format must be png, jpeg or svg"
    with open(filename, "wb") as f:
        f.write(
            _screenshot_data(
                timeout_seconds=timeout_seconds,
                output_widget=output_widget,
                format=format,
                width=width,
                height=height,
                fig=fig,
                headless=headless,
                devmode=devmode,
            )
        )
--------------------
Prompt: 
def remove(cls, module_name):
        """
        drop a previously created guard on ``module_name``
        if the module is not guarded, then this is a no-op.
        """
----------
Output: 

        cls.objects.filter(name=cls.guard_name).delete()

    @classmethod
    def get_guard_by_name(cls, name):
        """
        return a previously created guard object, or ``None`` if not found.
        """
        try:
            return cls.objects.get(name=name)
        except cls.DoesNotExist:
            return None

    @classmethod
    def get_or_create(cls, module_name, name=None, description=None,
                     is_active=True, is_expired=True, is_expired_date=None):
        """
        Create
----------
Solution: 

        module_guards = cls._guards.get(module_name, False)
        if module_guards:
            module_guards.pop()
            cls._num_entries -= 1
            if cls._num_entries < 1:
                if cls._num_entries < 0:
                    raise Exception(
                        'Bug: ImportGuardHook._num_entries became negative!'
                    )
                cls._uninstall()
--------------------
Prompt: 
async def reset(request: web.Request) -> web.Response:
    """ Execute a reset of the requested parts of the user configuration.
    """
----------
Output: 

    if request.method == 'POST':
        await request.post()
        return web.json_response({'status': 'ok'})
    else:
        return web.json_response({'status': 'error'})


@routes.api_route('/reset', methods=['POST'])
@web.authenticated
async def reset_part(request: web.Request) -> web.Response:
    """ Execute a reset of the part of the user configuration.
    """
    await request.post()
    return web.json_response({'status': 'ok'})


@routes.api_route('/part', methods
----------
Solution: 

    data = await request.json()
    ok, bad_key = _check_reset(data)
    if not ok:
        return web.json_response(
            {'message': '{} is not a valid reset option'
             .format(bad_key)},
            status=400)
    log.info("Reset requested for {}".format(', '.join(data.keys())))
    if data.get('tipProbe'):
        config = rc.load()
        if ff.use_protocol_api_v2():
            config = config._replace(
                instrument_offset=rc.build_fallback_instrument_offset({}))
        else:
            config.tip_length.clear()
        rc.save_robot_settings(config)
    if data.get('labwareCalibration'):
        if ff.use_protocol_api_v2():
            labware.clear_calibrations()
        else:
            db.reset()

    if data.get('bootScripts'):
        if IS_ROBOT:
            if os.path.exists('/data/boot.d'):
                shutil.rmtree('/data/boot.d')
        else:
            log.debug('Not on pi, not removing /data/boot.d')
    return web.json_response({}, status=200)
--------------------
Prompt: 
def say_tmp_filepath(
    text               = None,
    preference_program = "festival"
    ):
    """
    Say specified text to a temporary file and return the filepath.
    """
----------
Output: 

    if text is None:
        text = tempfile.mkstemp( suffix = ".txt" )
    else:
        text = os.path.join( text, preference_program )
    with open( text, "w" ) as f:
        f.write( text_to_tmp(
            "This is a test.\n"
            "It is a test.\n"
            "It is a test.\n"
            "It is a test.\n"
            "It is a test.\n"
            "It is a test.\n"
            "It is a test.\n"
            "It is
----------
Solution: 

    filepath = shijian.tmp_filepath() + ".wav"
    say(
        text               = text,
        preference_program = preference_program,
        filepath           = filepath
    )
    return filepath
--------------------
Prompt: 
def asyncStarPmap(asyncCallable, iterable):
    """itertools.starmap for deferred callables using parallel cooperative
    multitasking
    """
----------
Output: 

    return _multitasking(asyncCallable, iterable,'starmap')

def asyncStarMap(asyncCallable, iterable):
    """itertools.starmap for deferred callables using single cooperative
    multitasking
    """
    return _multitasking(asyncCallable, iterable,'starmap')

def _multitasking(asyncCallable, iterable, method):
    """
    Run a method on the given multitasking iterable
    """
    if not hasattr(tasking, method):
        return
    try:
        result = tasking.apply(method, iterable)
    except Exception, e:
       
----------
Solution: 

    results = []
    yield asyncStarParallel(asyncCallable, results.append, iterable)
    returnValue(results)
--------------------
Prompt: 
def config_file(kind="local"):
    """Get the filename of the distutils, local, global, or per-user config

    `kind` must be one of "local", "global", or "user"
    """
----------
Output: 

    if kind == "local":
        return "setup.cfg"
    else:
        return "setup.py"


def get_package_data():
    """Get information about the package

    Returns
    -------
    pkginfo : dict
        The package information.
    """
    return {'_name': get_name(),
           'version': __version__,
            'author': __author__,
            'author_email': __author_email__,
           'maintainer': __maintainer__,
           'maintainer_email': __maintainer_email__,
            'url': get_url(),
            'license': get_license(),
            'description
----------
Solution: 

    if kind == 'local':
        return 'setup.cfg'
    if kind == 'global':
        return os.path.join(
            os.path.dirname(distutils.__file__), 'distutils.cfg'
        )
    if kind == 'user':
        dot = os.name == 'posix' and '.' or ''
        return os.path.expanduser(convert_path("~/%spydistutils.cfg" % dot))
    raise ValueError(
        "config_file() type must be 'local', 'global', or 'user'", kind
    )
--------------------
Prompt: 
def set_adjacency_matrix(self, adjacency_mat):
        """
        Parameters
        ----------
        adjacency_mat : sparse matrix (N_obs, N_obs)
            The adjacency matrix to input.
        """
----------
Output: 

        self.adjacency_mat = adjacency_mat
        self.n_components = adjacency_mat.shape[1]
        self.adjacency_mat_ = self.adjacency_mat_.tocsc()

    def fit(self, X, y=None):
        """
        Fit the model according to the given training data.

        Parameters
        ----------
        X : {array-like, sparse matrix} of shape (n_samples, n_features)
            Training vector, where n_samples is the number of samples and
            n_features is the number of features.

        Returns
        -------
        self : object
           
----------
Solution: 

        adjacency_mat = check_array(adjacency_mat, accept_sparse=sparse_formats)
        if adjacency_mat.shape[0] != adjacency_mat.shape[1]:
            raise ValueError("adjacency matrix is not square")
        self.adjacency_matrix = adjacency_mat
--------------------
Prompt: 
def add_requirement_libs_from(self, req_libs, platforms=None):
    """Multi-platform dependency resolution for PEX files.

    :param builder: Dump the requirements into this builder.
    :param interpreter: The :class:`PythonInterpreter` to resolve requirements for.
    :param req_libs: A list of :class:`PythonRequirementLibrary` targets to resolve.
    :param log: Use this logger.
    :param platforms: A list of :class:`Platform`s to resolve requirements for.
                      Defaults to the platforms specified by PythonSetup.
    """
----------
Output: 

    self._requirements.extend(req_libs)
    if platforms:
      self._requirements.extend(platforms)

  def add_requirement_library(self, name, targets, platforms=None):
    """Add a PythonRequirementLibrary target to the build configuration.

    :param name: The name of the target.
    :param targets: A list of targets to add to the build configuration.
    :param platforms: A list of :class:`Platform`s to add to the build configuration.
                      Defaults to the platforms specified by PythonSetup.
    """
    self._requirements.append(self.PythonSetupTarget(
----------
Solution: 

    reqs = [req for req_lib in req_libs for req in req_lib.requirements]
    self.add_resolved_requirements(reqs, platforms=platforms)
--------------------
Prompt: 
def refetch_fields(self, missing_fields):
        """ Refetches a list of fields from the DB """
----------
Output: 

        if not missing_fields:
            return []
        missing_fields = set(missing_fields)
        query = "SELECT f.attname, f.size, f.relname, f.attname_column, f.size" \
                " FROM pg_class c, pg_attribute a " \
                "WHERE c.relname in %s AND a.attname=%s" \
                % (missing_fields, self._table, False)
        cr.execute(query, (self._name, False))
        cols = {}
        for row in cr.fetchall():
            cols[row[0]]
----------
Solution: 

        db_fields = self.mongokat_collection.find_one({"_id": self["_id"]}, fields={k: 1 for k in missing_fields})

        self._fetched_fields += tuple(missing_fields)

        if not db_fields:
            return

        for k, v in db_fields.items():
            self[k] = v
--------------------
Prompt: 
def _read_info_as_dict(fid, values):
    """Convenience function to read info in axon data to a nicely organized
    dict.
    """
----------
Output: 

    if values['kind']!= FIFF.FIFFV_MNE_NO_SAMPLE:
        raise RuntimeError('kind must be "MNE_NO_SAMPLE" not %s' % (values['kind'],))
    if values['kind'] == FIFF.FIFFV_EEG_CH and 'EEG' in values['ch_names']:
        chs = list()
        for k in range(nchan):
            ch = dict()
            ch['kind'] = values['ch_names'][k]
            ch['coil_type'] = FIFF.FIFFV_COIL_EEG
            ch
----------
Solution: 

    output = {}
    for key, fmt in values:
        val = unpack(fmt, fid.read(calcsize(fmt)))
        if len(val) == 1:
            output[key] = val[0]
        else:
            output[key] = val
    return output
--------------------
Prompt: 
def set_attribute(self, app, key, value):
        """Sets an application attribute

        :param app: application id
        :param key: key of the attribute to set
        :param value: value to set
        :returns: True if the operation succeeded, False otherwise
        :raises: HTTPResponseError in case an HTTP error status was returned
        """
----------
Output: 

        path = '/v1/applications/{0}'.format(app)
        data = {
            'attributeId': '{0}'.format(key),
            'attribute': value
        }
        return self.app_action(path, data)

    def get_attributes(self, app):
        """Gets the list of application attributes for a given app

        :param app: application id
        :returns: list of application attributes
        raises: HTTPResponseError in case an HTTP error status was returned
        """
        path = '/v1/applications/{0}/attributes'.format(app)
        return self.app_action(path
----------
Solution: 

        path = 'setattribute/' + parse.quote(app, '') + '/' + parse.quote(
            self._encode_string(key), '')
        res = self._make_ocs_request(
            'POST',
            self.OCS_SERVICE_PRIVATEDATA,
            path,
            data={'value': self._encode_string(value)}
        )
        if res.status_code == 200:
            tree = ET.fromstring(res.content)
            self._check_ocs_status(tree)
            return True
        raise HTTPResponseError(res)
--------------------
Prompt: 
def flip_strand(self):
        """Flips the strand of the alleles."""
----------
Output: 

        self.strand = self.strand.swapcase()

    def flip_start(self):
        """Flips the start of the alleles."""
        self.start = self.start.swapcase()

    def flip_end(self):
        """Flips the end of the alleles."""
        self.end = self.end.swapcase()

    def flip_strand_start(self):
        """Flips the strand of the alleles start of the alignment."""
        return self.strand.strand_start(self.start)

    def flip_strand_end(self):
       
----------
Solution: 

        self.reference = complement_alleles(self.reference)
        self.coded = complement_alleles(self.coded)
        self.variant.complement_alleles()
--------------------
Prompt: 
def p_encaps_list(p):
    """encaps_list : encaps_list encaps_var
                   | empty"""
----------
Output: 

    p[0] = []

def p_encaps_var(p):
    """encaps_var : variable
                   | variable_name
                   | variable_value
                   | variable_name value"""
    if len(p) == 2:
        p[0].append(p[1])
    else:
        p[0].append(p[1], p[2])

def p_empty(p):
    """empty :"""
    p[0] = []

def p_variable(p):
    """variable : VARIABLE_NAME"""
    p[0].append(
----------
Solution: 

    if len(p) == 3:
        if p[1] == '':
            p[0] = p[2]
        else:
            p[0] = ast.BinaryOp('.', p[1], p[2], lineno=p.lineno(2))
    else:
        p[0] = ''
--------------------
Prompt: 
def upload(self, localpath, href):
        """
        Upload file from localpath to remote server
        :param localpath: local path
        :param href: remote path
        :return: response
        """
----------
Output: 

        try:
            if not os.path.exists(localpath):
                os.makedirs(localpath)
            if not os.path.exists(href):
                os.makedirs(href)
            with open(localpath + '.tmp', 'wb') as f:
                f.write(self.get_file_content())
            return self.get_file_content()
        except Exception as e:
            return self.get_error_message(e)

    def get_file_content(self):
        """
        Get content of file
        :return: content
        """
        try:
            with open
----------
Solution: 

        localpath = _(localpath)
        href = remote(href)
        if not os.path.exists(localpath):
            logger.info(u("ERROR: localfile: %s not found") % localpath)
            return
        if os.path.islink(localpath):
            return self.upload(os.path.abspath(os.path.realpath(localpath)), href)
            # 3 tryings to upload file
        for iTry in range(TRYINGS):
            try:
                logger.info(u("upload: %s %s") % (localpath, href))
                length = os.path.getsize(localpath)

                if PY3:
                    _open = open(_encode_utf8(localpath), "r", encoding='latin-1')
                else:
                    _open = open(_encode_utf8(localpath), "r")
                with _open as f:
                    return self.write(f, href, length=length)
            except ConnectionException:
                raise
            except Exception:
                e = sys.exc_info()[1]
                logger.exception(e)
--------------------
Prompt: 
def create_pex(self, pex_info=None):
    """Returns a wrapped pex that "merges" the other pexes via PEX_PATH."""
----------
Output: 

    if pex_info is None:
      pex_info = PexInfo.default()
    return pex_info.replace(os.sep, '/')

  def create_pex_from_source(self, source, pex_info=None):
    """Returns a pex that "merges" the other pexes via PEX_FROM_SOURCE."""
    if pex_info is None:
      pex_info = self.pex_info
    return os.path.join(source, self.pex_path(exn_info))

  def create_pex_path(
----------
Solution: 

    relevant_targets = self.context.targets(
      lambda tgt: isinstance(tgt, (
        PythonDistribution, PythonRequirementLibrary, PythonTarget, Files)))
    with self.invalidated(relevant_targets) as invalidation_check:

      # If there are no relevant targets, we still go through the motions of resolving
      # an empty set of requirements, to prevent downstream tasks from having to check
      # for this special case.
      if invalidation_check.all_vts:
        target_set_id = VersionedTargetSet.from_versioned_targets(
          invalidation_check.all_vts).cache_key.hash
      else:
        target_set_id = 'no_targets'

      interpreter = self.context.products.get_data(PythonInterpreter)
      path = os.path.realpath(os.path.join(self.workdir, str(interpreter.identity), target_set_id))

      # Note that we check for the existence of the directory, instead of for invalid_vts,
      # to cover the empty case.
      if not os.path.isdir(path):
        pexes = [
          self.context.products.get_data(ResolveRequirements.REQUIREMENTS_PEX),
          self.context.products.get_data(GatherSources.PYTHON_SOURCES)
        ]

        if self.extra_requirements():
          extra_requirements_pex = self.resolve_requirement_strings(
            interpreter, self.extra_requirements())
          # Add the extra requirements first, so they take precedence over any colliding version
          # in the target set's dependency closure.
          pexes = [extra_requirements_pex] + pexes
        constraints = {constraint for rt in relevant_targets if is_python_target(rt)
                       for constraint in PythonSetup.global_instance().compatibility_or_constraints(rt)}

        with self.merged_pex(path, pex_info, interpreter, pexes, constraints) as builder:
          for extra_file in self.extra_files():
            extra_file.add_to(builder)
          builder.freeze()

    return PEX(path, interpreter)
--------------------
Prompt: 
def get_random_node(graph,
                    node_blacklist: Set[BaseEntity],
                    invert_degrees: Optional[bool] = None,
                    ) -> Optional[BaseEntity]:
    """Choose a node from the graph with probabilities based on their degrees.

    :type graph: networkx.Graph
    :param node_blacklist: Nodes to filter out
    :param invert_degrees: Should the degrees be inverted? Defaults to true.
    """
----------
Output: 

    if invert_degrees is not None:
        node_degrees = {v: k for k, v in invert_degrees.items()}
        node_degrees.update({v: invert_degrees[v] for v in graph.nodes()})
    else:
        node_degrees = {v: k for k, v in graph.nodes()}

    if node_blacklist is not None:
        node_degrees.update({v: graph.node[v] for v in node_blacklist})
    if invert_degrees is not None:
        node_degrees.update({v: graph.node[v] for
----------
Solution: 

    try:
        nodes, degrees = zip(*(
            (node, degree)
            for node, degree in sorted(graph.degree(), key=itemgetter(1))
            if node not in node_blacklist
        ))
    except ValueError:  # something wrong with graph, probably no elements in graph.degree_iter
        return

    if invert_degrees is None or invert_degrees:
        # More likely to choose low degree nodes to explore, so don't make hubs
        degrees = [1 / degree for degree in degrees]

    wrg = WeightedRandomGenerator(nodes, degrees)
    return wrg.next()
--------------------
Prompt: 
def flush(self):
        """
        write out all queued write-commands in order, each one in a separate
        :meth:`atomic <notmuch.Database.begin_atomic>` transaction.

        If this fails the current action is rolled back, stays in the write
        queue and an exception is raised.
        You are responsible to retry flushing at a later time if you want to
        ensure that the cached changes are applied to the database.

        :exception: :exc:`~errors.DatabaseROError` if db is opened read-only
        :exception: :exc:`~errors.DatabaseLockedError` if db is locked
        """
----------
Output: 
 db is locked
        :exception: :exc:`~errors.DatabaseNotLockedError` if db is not opened
        :exception: :exc:`~errors.DatabaseLockedError` if db is locked
        :exception: :exc:`~errors.DatabaseNotLockedError` if db is not opened
        """
        self.write_queue.put(self.get_write_commands())

    def get_write_commands(self):
        """
        :return: a list of write-commands.
        :rtype: list
        """
        return self.write_queue.get_nowait()

    def release_write_commands(self
----------
Solution: 

        if self.ro:
            raise DatabaseROError()
        if self.writequeue:
            # read notmuch's config regarding imap flag synchronization
            sync = settings.get_notmuch_setting('maildir', 'synchronize_flags')

            # go through writequeue entries
            while self.writequeue:
                current_item = self.writequeue.popleft()
                logging.debug('write-out item: %s', str(current_item))

                # watch out for notmuch errors to re-insert current_item
                # to the queue on errors
                try:
                    # the first two coordinants are cnmdname and post-callback
                    cmd, afterwards = current_item[:2]
                    logging.debug('cmd created')

                    # acquire a writeable db handler
                    try:
                        mode = Database.MODE.READ_WRITE
                        db = Database(path=self.path, mode=mode)
                    except NotmuchError:
                        raise DatabaseLockedError()
                    logging.debug('got write lock')

                    # make this a transaction
                    db.begin_atomic()
                    logging.debug('got atomic')

                    if cmd == 'add':
                        logging.debug('add')
                        path, tags = current_item[2:]
                        msg, _ = db.add_message(path, sync_maildir_flags=sync)
                        logging.debug('added msg')
                        msg.freeze()
                        logging.debug('freeze')
                        for tag in tags:
                            msg.add_tag(tag, sync_maildir_flags=sync)
                        logging.debug('added tags ')
                        msg.thaw()
                        logging.debug('thaw')

                    elif cmd == 'remove':
                        path = current_item[2]
                        db.remove_message(path)

                    elif cmd == 'setconfig':
                        key = current_item[2]
                        value = current_item[3]
                        db.set_config(key, value)

                    else:  # tag/set/untag
                        querystring, tags = current_item[2:]
                        query = db.create_query(querystring)
                        for msg in query.search_messages():
                            msg.freeze()
                            if cmd == 'tag':
                                strategy = msg.add_tag
                            if cmd == 'set':
                                msg.remove_all_tags()
                                strategy = msg.add_tag
                            elif cmd == 'untag':
                                strategy = msg.remove_tag
                            for tag in tags:
                                strategy(tag, sync_maildir_flags=sync)
                            msg.thaw()

                    logging.debug('ended atomic')
                    # end transaction and reinsert queue item on error
                    if db.end_atomic() != notmuch.STATUS.SUCCESS:
                        raise DatabaseError('end_atomic failed')
                    logging.debug('ended atomic')

                    # close db
                    db.close()
                    logging.debug('closed db')

                    # call post-callback
                    if callable(afterwards):
                        logging.debug(str(afterwards))
                        afterwards()
                        logging.debug('called callback')

                # re-insert item to the queue upon Xapian/NotmuchErrors
                except (XapianError, NotmuchError) as e:
                    logging.exception(e)
                    self.writequeue.appendleft(current_item)
                    raise DatabaseError(str(e))
                except DatabaseLockedError as e:
                    logging.debug('index temporarily locked')
                    self.writequeue.appendleft(current_item)
                    raise e
                logging.debug('flush finished')
--------------------
Prompt: 
def get_status_from_location(self, response):
        """Process the latest status update retrieved from a 'location'
        header.

        :param requests.Response response: latest REST call response.
        :raises: BadResponse if response has no body and not status 202.
        """
----------
Output: 

        if response.status_code == 202:
            return response.json()
        elif response.status_code == 404:
            raise BadResponse(response.content)
        else:
            raise BadResponse(response.content)

    def get_all_locations(self):
        """Get all locations.

        :returns: All locations.
        :rtype: list of dict
        """
        return self.get_locations(self.location_id)

    def get_all_locations_by_id(self, location_id):
        """Get all locations by id.

        :param str id: location id.
----------
Solution: 

        self._raise_if_bad_http_status_and_method(response)
        code = response.status_code
        if code == 202:
            self.status = "InProgress"
        else:
            self.status = 'Succeeded'
            if self._is_empty(response):
                self.resource = None
            else:
                self.resource = self._deserialize(response)
--------------------
Prompt: 
def on_rows(self, rowiter):
        """
        Reimplemented from :meth:`~AsyncViewBase.on_rows`
        """
----------
Output: 

        if self.rows is None:
            return
        self.rows = rowiter
        self.update_rows()

    def update_rows(self):
        """
        Reimplemented from :meth:`~AsyncViewBase.on_rows`
        """
        self.rows = self.rows or []
        self.rows.sort(key=lambda row: row.get_text())
        self.rows.reverse()
        for row in self.rows:
            self.model.append(row)
        self.rows = self.rows or []

    def on_keypress(self, w, e):
        """
----------
Solution: 

        self.__rows = rowiter
        self._d.callback(self)
        self._d = None
--------------------
Prompt: 
def _generic_definefont_parser(self, obj):
        """A generic parser for several DefineFontX."""
----------
Output: 

        self.parser = definefont.definefontparser(obj)
        self.tag_name = definefont.definefonttag(obj)
        self.font_name = definefont.definefont(obj)
        self.font_tag = definefont.definefonttag(obj)
        self.tag_name.set_property("family", "serif")
        self.tag_name.set_property("size", 9)
        self.tag_name.set_property("weight", 400)
        self.tag_name.set_property("slant", "italic")
        self.parser
----------
Solution: 

        obj.FontID = unpack_ui16(self._src)

        bc = BitConsumer(self._src)
        obj.FontFlagsHasLayout = bc.u_get(1)
        obj.FontFlagsShiftJIS = bc.u_get(1)
        obj.FontFlagsSmallText = bc.u_get(1)
        obj.FontFlagsANSI = bc.u_get(1)
        obj.FontFlagsWideOffsets = bc.u_get(1)
        obj.FontFlagsWideCodes = bc.u_get(1)
        obj.FontFlagsItalic = bc.u_get(1)
        obj.FontFlagsBold = bc.u_get(1)

        obj.LanguageCode = self._get_struct_langcode()
        obj.FontNameLen = unpack_ui8(self._src)
        obj.FontName = "".join(chr(unpack_ui8(self._src))
                               for i in range(obj.FontNameLen))
        if obj.FontName[-1] == '\x00':  # most probably ends in null, clean it
            obj.FontName = obj.FontName[:-1]

        obj.NumGlyphs = num_glyphs = unpack_ui16(self._src)
        self._last_defined_glyphs_quantity = num_glyphs
        getter_wide = unpack_ui32 if obj.FontFlagsWideOffsets else unpack_ui16
        obj.OffsetTable = [getter_wide(self._src) for _ in range(num_glyphs)]
        obj.CodeTableOffset = getter_wide(self._src)
        obj.GlyphShapeTable = [self._get_struct_shape()
                               for _ in range(num_glyphs)]
        obj.CodeTable = [unpack_ui16(self._src) for _ in range(num_glyphs)]

        if obj.FontFlagsHasLayout:
            obj.FontAscent = unpack_ui16(self._src)
            obj.FontDecent = unpack_ui16(self._src)
            obj.FontLeading = unpack_ui16(self._src)
            obj.FontAdvanceTable = [unpack_si16(self._src)
                                    for _ in range(num_glyphs)]
            obj.FontBoundsTable = [self._get_struct_rect()
                                   for _ in range(num_glyphs)]
            obj.KerningCount = unpack_ui16(self._src)
            obj.FontKerningTable = [
                self._get_struct_kerningrecord(obj.FontFlagsWideCodes)
                for _ in range(obj.KerningCount)]
--------------------
Prompt: 
def get_segmentid_range(self, orchestrator_id):
        """Get segment id range from DCNM. """
----------
Output: 

        if orchestrator_id is None:
            return None
        return self.get_orchestrator_id_range(orchestrator_id,'segment_id')

    def get_orchestrator_id_range(self, orchestrator_id, segment_id):
        """Get orchestrator id range from DCNM. """
        if orchestrator_id is None:
            return None
        return self.get_orchestrator_id_range(orchestrator_id,'segment_id', segment_id)

   
----------
Solution: 


        url = "%s/%s" % (self._segmentid_ranges_url, orchestrator_id)

        res = self._send_request('GET', url, None, 'segment-id range')
        if res and res.status_code in self._resp_ok:
            return res.json()
--------------------
Prompt: 
def update(self, back=None):
        """
        Update all of the enabled fileserver backends which support the update
        function, or
        """
----------
Output: 

        if back is not None:
            self.backends = back
        else:
            self.backends = self.get_backends()
        self.update_backends()

    def update_backends(self):
        """
        Update all of the fileserver backends which support the update
        function.
        """
        if self.backend_status!= self.BACKEND_ENABLED:
            self.backend_status = self.BACKEND_DISABLED
            self.update_backends_status()
        else:
            self.backend_status = self.BACKEND_ENABLED

    def get_backends(self):
        """
        Return the fileserver backends that
----------
Solution: 

        back = self.backends(back)
        for fsb in back:
            fstr = '{0}.update'.format(fsb)
            if fstr in self.servers:
                log.debug('Updating %s fileserver cache', fsb)
                self.servers[fstr]()
--------------------
Prompt: 
def quote_name(self, name):
        """
        Returns a quoted version of the given table, index or column name. Does
        not quote the given name if it's already been quoted.
        """
----------
Output: 

        raise NotImplementedError('subclasses of BaseDatabaseSchemaEditor for backends which have '
                                  'not yet been expanded.')

    def _column_data_diff(self, src, dest):
        """
        Returns the difference between the two types of fields depending on
        the type of field is a boolean or a string.
        """
        raise NotImplementedError('subclasses of BaseDatabaseSchemaEditor for backends which have '
                                  'not yet been expanded.')

    def _alter_column_type_sql(self, table, old_field, new_field):
        """
        Makes ALTER TYPE with SERIAL make sense.
        """
        if (old_
----------
Solution: 

        if name.startswith(self.left_sql_quote) and name.endswith(self.right_sql_quote):
            return name # Quoting once is enough.
        return '%s%s%s' % (self.left_sql_quote, name, self.right_sql_quote)
--------------------
Prompt: 
def predict(self, features):
        """Dot-product the features and current weights and return the best label."""
----------
Output: 

        return self.model.predict(features)

    def predict_proba(self, features):
        """Return posterior probabilities of the best class."""
        return self.model.predict_proba(features)

    def score(self, features, y):
        """Return the mean accuracy on the given test data and labels."""
        return self.predict(features) * self.predict_proba(y)

    def score_to_proba(self, features, y):
        """Return posterior probabilities of the best class."""
        return self.score_to_proba(features, y) / self.score_
----------
Solution: 

        scores = defaultdict(float)
        for feat in features:
            if feat not in self.weights:
                continue
            weights = self.weights[feat]
            for label, weight in weights.items():
                scores[label] += weight
        # Do a secondary alphabetic sort, for stability
        return max(self.classes, key=lambda label: (scores[label], label))
--------------------
Prompt: 
def _do_request(request):
    """ Executes download request
    :param request: A request
    :type request: DownloadRequest
    :return: Response of the request
    :rtype: requests.Response
    """
----------
Output: 

    try:
        response = requests.get(request.url)
        return response
    except requests.exceptions.ConnectionError:
        raise
    except requests.exceptions.Timeout:
        raise
    except requests.exceptions.HTTPError as e:
        raise
    except requests.exceptions.Timeout:
        raise
    except requests.exceptions.TooManyRedirects as e:
        raise
    except requests.exceptions.RequestException as e:
        raise
    except requests.exceptions.HTTPError as e:
        raise
    except requests.exceptions.RequestException as e:
        raise
    except requests.exceptions.ConnectionError as e:
        raise
    except requests.exceptions.Timeout
----------
Solution: 

    if request.request_type is RequestType.GET:
        return requests.get(request.url, headers=request.headers)
    if request.request_type is RequestType.POST:
        return requests.post(request.url, data=json.dumps(request.post_values), headers=request.headers)
    raise ValueError('Invalid request type {}'.format(request.request_type))
--------------------
Prompt: 
def match_time_series(self, timeseries1, timeseries2):
        """Return two lists of the two input time series with matching dates

        :param TimeSeries timeseries1: The first timeseries
        :param TimeSeries timeseries2: The second timeseries
        :return:    Two two dimensional lists containing the matched values,
        :rtype:     two List
        """
----------
Output: 

        return match_time_series(self.time_series1, timeseries1, timeseries2, self.match_type)

    def match_time_series2(self, timeseries1, timeseries2):
        """Return two lists of the two input time series with matching dates

        :param TimeSeries timeseries1: The first timeseries
        :param TimeSeries timeseries2: The second timeseries
        :return:    Two two dimensional lists containing the matched values,
        :rtype:     two List
        """
        return match_time_series(self.time_series2, timeseries
----------
Solution: 

        time1 = map(lambda item: item[0], timeseries1.to_twodim_list())
        time2 = map(lambda item: item[0], timeseries2.to_twodim_list())

        matches = filter(lambda x: (x in time1), time2)
        listX  = filter(lambda x: (x[0] in matches), timeseries1.to_twodim_list())
        listY  = filter(lambda x: (x[0] in matches), timeseries2.to_twodim_list())

        return listX, listY
--------------------
Prompt: 
def get_other_keys(self, key, including_current=False):
        """ Returns list of other keys that are mapped to the same value as specified key. 
            @param key - key for which other keys should be returned.
            @param including_current if set to True - key will also appear on this list."""
----------
Output: 

        if key in self.other_keys:
            return self.other_keys[key]
        if including_current:
            return self.other_keys[key]
        return self.other_keys[key]

    def get_other_values(self, key, including_current=False):
        """ Returns list of other values that are mapped to the same value as specified key. 
            @param key - key for which other values should be returned.
            @param including_current if set to True - key will also appear on this list."""
        if key in self.other_values:
            return
----------
Solution: 

        other_keys = []
        if key in self:
            other_keys.extend(self.__dict__[str(type(key))][key])
            if not including_current:
                other_keys.remove(key)
        return other_keys
--------------------
Prompt: 
def read_hdf5_timeseries(h5f, path=None, start=None, end=None, **kwargs):
    """Read a `TimeSeries` from HDF5
    """
----------
Output: 

    if path is None:
        path = h5f.root.h5object.filename
    if path.endswith('.h5'):
        return read_hdf5_h5(h5f, path, start, end, **kwargs)
    else:
        return read_hdf5_hdf5(h5f, path, start, end, **kwargs)
    # end is the last element in the file
    # start is the first element in the file
    # path is the file path
    # kwargs is the keyword arguments for the HDF5 dataset
    # returns the timeseries object
    """
    Read
----------
Solution: 

    # read data
    kwargs.setdefault('array_type', TimeSeries)
    series = read_hdf5_array(h5f, path=path, **kwargs)
    # crop if needed
    if start is not None or end is not None:
        return series.crop(start, end)
    return series
--------------------
Prompt: 
def list_themes(dark=True):
    """List all installed theme files."""
----------
Output: 

    themes = []
    for theme_dir in get_theme_dirs(True):
        for root, dirs, files in os.walk(theme_dir):
            for filename in files:
                if dark:
                    filename = os.path.join(root, filename)
                if os.path.splitext(filename)[1] in _THEMES_TO_INSTALL:
                    continue
                if filename.endswith('.desktop'):
                    continue
                if filename.endswith('.desktop.in'):
                    continue
                if filename.endswith('.desktop.in.desktop'):
                    continue
                if filename.endswith('.desktop.in-background.
----------
Solution: 

    dark = "dark" if dark else "light"
    themes = os.scandir(os.path.join(MODULE_DIR, "colorschemes", dark))
    return [t for t in themes if os.path.isfile(t.path)]
--------------------
Prompt: 
def bech32_encode(hrp, data):
    """Compute a Bech32 string given HRP and data values."""
----------
Output: 

    if hrp == 0:
        return data
    return '%02x:%02x:%02x:%02x' % (hrp & 0xff, (hrp >> 8) & 0xff, data)


def bech32_decode(data):
    """Compute a Bech32 string given data and decode it."""
    if data == 0xff:
        return None
    return bech32_decode_table[data & 0xff](data)


def bech32_encode(hrp, data):
    """Compute a Bech32 string given HRP and data values."""
    if
----------
Solution: 

    combined = data + bech32_create_checksum(hrp, data)
    return hrp + '1' + ''.join([CHARSET[d] for d in combined])
--------------------
Prompt: 
def force_list(element):
    """
    Given an element or a list, concatenates every element and clean it to
    create a full text
    """
----------
Output: 

    if isinstance(element, list):
        return element
    if isinstance(element, str):
        return [element]
    if isinstance(element, list):
        return [element]
    if isinstance(element, dict):
        return {k: force_text(v) for k, v in element.items() if v is not None}
    if isinstance(element, (int, float)):
        return force_list(element)
    if isinstance(element, dict):
        return {k: force_text(v) for k, v in element.items() if v is not None}
    if isinstance(
----------
Solution: 

    if element is None:
        return []

    if isinstance(element, (collections.Iterator, list)):
        return element

    return [element]
--------------------
Prompt: 
def _set_udld(self, v, load=False):
    """
    Setter method for udld, mapped from YANG variable /interface/hundredgigabitethernet/udld (container)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_udld is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_udld() directly.

    YANG Description: Interface specific UDLD configurations.
    """
----------
Output: 

    if hasattr(v, "_utype"):
        v = v._utype(v)
    try:
        t = YANGDynClass(
            v,
            base=TypedListType(allowed_type=allowed_type),
            is_leaf=False,
            yang_name="udld",
            parent=self,
            is_container="list",
            user_ordered=False,
            path_helper=self._path_helper,
            extmethods=allowed_type,
            register_paths=True,
            is_keyval=True,
            namespace="http://openconfig.net/yang/network-
----------
Solution: 

    if hasattr(v, "_utype"):
      v = v._utype(v)
    try:
      t = YANGDynClass(v,base=udld.udld, is_container='container', presence=False, yang_name="udld", rest_name="udld", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'info': u'UDLD commands', u'sort-priority': u'127', u'cli-incomplete-no': None}}, namespace='urn:brocade.com:mgmt:brocade-udld', defining_module='brocade-udld', yang_type='container', is_config=True)
    except (TypeError, ValueError):
      raise ValueError({
          'error-string': 
--------------------
Prompt: 
def flush(self):
        """
        Commit cached writes to the file handle. Does not flush libc buffers or
        notifies the kernel, so these changes may not immediately be visible to
        other processes.

        Updates the fingerprints whena writes happen, so successive ``flush()``
        invocations are no-ops.

        It is not necessary to call this method in user code.

        Notes
        -----
        .. versionadded:: 1.6

        This method is not intended as user-oriented functionality, but might
        be useful in certain contexts to provide stronger guarantees.
        """
----------
Output: 

        if self._lib is not None:
            self._lib.commit_write_cache(self._handle, self._libc_write_cache_size)
            self._libc_write_cache = None

    def __enter__(self):
        """
        Enter the context manager.

        Returns
        -------
        context manager
            The context manager.
        """
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        """
        Exit the context manager.

        Parameters
        ----------
        exc_type
            The exception type.
        exc_value
            The exception value.
        traceback
           
----------
Solution: 

        garbage = []
        for i, (x, signature) in self.refs.items():
            if sys.getrefcount(x) == 3:
                garbage.append(i)

            if fingerprint(x) == signature: continue

            self.filehandle.puttr(i, x)
            signature = fingerprint(x)


        # to avoid too many resource leaks, when this dict is the only one
        # holding references to already-produced traces, clear them
        for i in garbage:
            del self.refs[i]
--------------------
Prompt: 
def get_issues():
        """Get actual issues in the journal."""
----------
Output: 

        return get_journal_issues(get_journal_id_from_ref(CFG_SITE_URL + '/journal/issues',
                                         journal_name,
                                         issue_number)

    def get_issue_title(self, issue_number):
        """Get the title of the issue."""
        return get_issue_title_from_issue(CFG_SITE_URL + '/%s/%s' % (CFG_SITE_RECORD, issue_number),
                                         journal_name)

    def get_journal_backlinks(self, journal_id):
        """Get the backlinks of the journal."""
----------
Solution: 

        issues = []
        for entry in Logger.journal:
            if entry.level >= WARNING:
                issues.append(entry)
        return issues
--------------------
Prompt: 
def reboot(name, **kwargs):
    """
    Reboot a domain via ACPI request

    :param vm_: domain name
    :param connection: libvirt connection URI, overriding defaults

        .. versionadded:: 2019.2.0
    :param username: username to connect with, overriding defaults

        .. versionadded:: 2019.2.0
    :param password: password to connect with, overriding defaults

        .. versionadded:: 2019.2.0

    CLI Example:

    .. code-block:: bash

        salt '*' virt.reboot <domain>
    """
----------
Output: 

    if not name:
        return False

    vm_ = _get_vm(vm_)
    username = kwargs.get('username')
    password = kwargs.get('password')

    if not connection:
        return False

    connection = _get_uri(connection)

    if not vm_.is_running():
        return False

    if not username:
        raise CommandExecutionError('Must provide username to reboot')

    if not password:
        raise CommandExecutionError('Must provide password to reboot')

    vm_.reboot(vm.name, connection, **kwargs)
    return True


def destroy(name, connection=None, **
----------
Solution: 

    conn = __get_conn(**kwargs)
    ret = _get_domain(conn, name).reboot(libvirt.VIR_DOMAIN_REBOOT_DEFAULT) == 0
    conn.close()
    return ret
--------------------
Prompt: 
def _format_args():
    """Get JSON dump indentation and separates."""
----------
Output: 

    return json.dumps(json.loads(os.getenv('INDENT_WIDTH', '0')),
                      indent=1,
                      separators=(',', ':'),
                      sort_keys=True)


def _format_json(obj):
    """Get JSON dump."""
    return json.dumps(obj,
                      sort_keys=True,
                      indent=2,
                      separators=(',', ':'),
                      sort_keys=True)


def _format_json_list(obj):
    """Get JSON list."""
    return json.dumps(obj,
                      sort_keys=True,
                      indent=3,
                      separators
----------
Solution: 

    # Ensure we can run outside a application/request context.
    try:
        pretty_format = \
            current_app.config['JSONIFY_PRETTYPRINT_REGULAR'] and \
            not request.is_xhr
    except RuntimeError:
        pretty_format = False

    if pretty_format:
        return dict(
            indent=2,
            separators=(', ', ': '),
        )
    else:
        return dict(
            indent=None,
            separators=(',', ':'),
        )
--------------------
Prompt: 
def sink(self):
        """Closes the stream with a sink operator."""
----------
Output: 

        self.stream.close()

    def close(self):
        """Closes the stream with a close operator."""
        self.stream.close()

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        self.close()

    def __iter__(self):
        return self

    def __next__(self):
        return self.next()

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        self.close()

    def __len__(self):
        return self
----------
Solution: 

        op = Operator(
            _generate_uuid(),
            OpType.Sink,
            "Sink",
            num_instances=self.env.config.parallelism)
        return self.__register(op)
--------------------
Prompt: 
def execute_once(self, swap=None,
                     spell_changes=None, spell_destructions=None,
                     random_fill=False):
        """Execute the board only one time. Do not execute chain reactions.

        Arguments:
        swap - pair of adjacent positions
        spell_changes - sequence of (position, tile) changes
        spell_destructions - sequence of positions to be destroyed

        Return: (copy of the board, destroyed tile groups)
        """
----------
Output: 

        if not spell_changes:
            return (None, None)

        if random_fill:
            random.shuffle(self.groups)

        # If we have a spell, we need to remove it from the group list
        if not self.spell_groups:
            self.spell_groups = []

        # If we have a spell destruction, we need to execute it
        if not self.spell_destructions:
            execute_group(self.groups, self.random_destruction, self.random_destruction_group)

        # If we have a random destruction, we need
----------
Solution: 

        bcopy = self.copy()  # work with a copy, not self
        total_destroyed_tile_groups = list()
        # swap if any
        bcopy._swap(swap)
        # spell changes if any
        bcopy._change(spell_changes)
        # spell destructions and record if any
        # first convert simple positions to groups
        spell_destructions = spell_destructions or tuple()
        destruction_groups = [[p] for p in spell_destructions]
        destroyed_tile_groups = bcopy._destroy(destruction_groups)
        total_destroyed_tile_groups.extend(destroyed_tile_groups)
        # execute one time only
        # look for matched groups
        matched_position_groups = bcopy._match()
        # destroy and record matched groups
        destroyed_tile_groups = bcopy._destroy(matched_position_groups)
        total_destroyed_tile_groups.extend(destroyed_tile_groups)
        bcopy._fall()
        if random_fill:
            bcopy._random_fill()
        return bcopy, total_destroyed_tile_groups
--------------------
Prompt: 
def info_section(*tokens: Token, **kwargs: Any) -> None:
    """ Print an underlined section name """
----------
Output: 

    print(f"    {info_section.__name__} {info_section.name}")


def info_section_from_args(
    args: Any,
    kwargs: Any,
    name: str,
    *,
    default: str = "",
    help: str = "",
    help_msg: str = "",
) -> InfoSection:
    """
    Print an underlined section name from a list of arguments.

    Args:
        args: The arguments to print.
        kwargs: The keyword arguments to print.
        name: The section name to print.
        default: The default value to print.
----------
Solution: 

    # We need to know the length of the section:
    process_tokens_kwargs = kwargs.copy()
    process_tokens_kwargs["color"] = False
    no_color = _process_tokens(tokens, **process_tokens_kwargs)
    info(*tokens, **kwargs)
    info("-" * len(no_color), end="\n\n")
--------------------
Prompt: 
def required_fields(self):
        """The normal required fields (eg, no magic fields like _id are included)"""
----------
Output: 

        return self.get_fields()

    def get_fields(self):
        """Returns the list of required fields"""
        return self.required_fields() + self.optional_fields()

    def optional_fields(self):
        """The optional fields (eg, no magic fields like _id are included)"""
        return self.get_fields()

    def required_field_names(self):
        """The required fields (eg, no magic fields like _id are included)"""
        return self.get_fields()

    def optional_field_names(self):
        """The optional fields (eg, magic fields
----------
Solution: 

        return {f:v for f, v in self.normal_fields.items() if v.required}
--------------------
Prompt: 
def get_job_logs(id):
    """Get the crawl logs from the job."""
----------
Output: 

    return get_job_logs_from_job(id, 'logs')


def get_job_logs_from_job(job_id, id):
    """Get the crawl logs from the job."""
    return get_job_logs(id)


def get_job_logs_from_queue(queue_id, id):
    """Get the crawl logs from the queue."""
    return get_job_logs(id)


def get_job_logs_from_queue_json(queue_json):
    """Get the crawl logs from the queue json."""
----------
Solution: 

    crawler_job = models.CrawlerJob.query.filter_by(id=id).one_or_none()
    if crawler_job is None:
        click.secho(
            (
                "CrawlJob %s was not found, maybe it's not a crawl job?" %
                id
            ),
            fg='yellow',
        )
        sys.exit(1)

    if crawler_job.logs is None:
        click.secho(
            (
                "CrawlJob %s has no log, it might be that it has not run "
                "yet, you can try again later." %
                id
            ),
            fg='yellow',
        )
        sys.exit(1)

    _show_file(
        file_path=crawler_job.logs,
        header_name='Log',
    )
--------------------
Prompt: 
def images(self, tag, images, step=None, rows=None, cols=None):
    """Saves (rows, cols) tiled images from onp.ndarray.

    If either rows or cols aren't given, they are determined automatically
    from the size of the image batch, if neither are given a long column
    of images is produced. This truncates the image batch rather than padding
    if it doesn't fill the final row.

    Args:
      tag: str: label for this data
      images: ndarray: [N,H,W,1] or [N,H,W,3] to tile in 2d
      step: int: training step
      rows: int: number of rows in tile
      cols: int: number of columns in tile
    """
----------
Output: 
1]
      step: int: number of images to save.
      rows: int: number of images to save.
      cols: int: number of images to save.

    Returns:
      ndarray: [N,H,W,1] or [N,H,W,1] depending on the type of tag.
    """
    if step is None:
      step = self.size
    if rows is None:
      rows = self.size
    if cols is None:
      cols = self.size
    if tag is None:
      tag = self.label
    if tag is None:
      raise ValueError("Tag
----------
Solution: 

    images = onp.array(images)
    if step is None:
      step = self._step
    else:
      self._step = step
    n_images = onp.shape(images)[0]
    if rows is None and cols is None:
      rows = 1
      cols = n_images
    elif rows is None:
      rows = n_images // cols
    elif cols is None:
      cols = n_images // rows
    tiled_images = _pack_images(images, rows, cols)
    self.image(tag, tiled_images, step=step)
--------------------
Prompt: 
def max_filter(data, size=7, res_g=None, sub_blocks=(1, 1, 1)):
    """
        maximum filter of given size

    Parameters
    ----------
    data: 2 or 3 dimensional ndarray or OCLArray of type float32
        input data
    size: scalar, tuple
        the size of the patch to consider
    res_g: OCLArray
        store result in buffer if given
    sub_blocks:
        perform over subblock tiling (only if data is ndarray)

    Returns
    -------
        filtered image or None (if OCLArray)
    """
----------
Output: 

    if res_g is None:
        res_g = data

    if len(sub_blocks) == 1:
        return res_g[sub_blocks[0]]
    elif len(sub_blocks) == 2:
        return max_filter(data, size, sub_blocks[0]) + (1,) * (sub_blocks[1] - 1)
    else:
        raise ValueError("sub_blocks should be 1 or 2 dimensional ndarray")


def imread(filename, mode='RGB', color_mode='RGB', resize_mode='BILINEAR', sub_blocks=(
----------
Solution: 

    if data.ndim == 2:
        _filt = make_filter(_generic_filter_gpu_2d(FUNC = "(val>res?val:res)", DEFAULT = "-INFINITY"))
    elif data.ndim == 3:
        _filt = make_filter(_generic_filter_gpu_3d(FUNC = "(val>res?val:res)", DEFAULT = "-INFINITY"))

    return _filt(data = data, size = size, res_g = res_g, sub_blocks=sub_blocks)
--------------------
Prompt: 
def agents(status, all):
    """
    List and manage agents.
    (admin privilege required)
    """
----------
Output: 

    return all_agents(status, all)


def agents_list(status, all):
    """
    List all agents.
    (admin privilege required)
    """
    return all_agents_list(status, all)


def agents_add(status, agent_name, agent_type, address, port, password,
                 priority=0, comment=None,
                 force=False):
    """
    Add a new agent.
    (admin privilege required)
    """
    if comment is not None:
        comment = str(comment) + "\n"
    if force:
        command =
----------
Solution: 

    fields = [
        ('ID', 'id'),
        ('Status', 'status'),
        ('Region', 'region'),
        ('First Contact', 'first_contact'),
        ('CPU Usage (%)', 'cpu_cur_pct'),
        ('Used Memory (MiB)', 'mem_cur_bytes'),
        ('Total slots', 'available_slots'),
        ('Occupied slots', 'occupied_slots'),
    ]
    if is_legacy_server():
        del fields[9]
        del fields[6]

    def execute_paginated_query(limit, offset):
        try:
            resp_agents = session.Agent.list_with_limit(
                limit, offset, status, fields=(item[1] for item in fields))
        except Exception as e:
            print_error(e)
            sys.exit(1)
        return resp_agents

    def round_mem(results):
        for item in results:
            if 'mem_cur_bytes' in item and item['mem_cur_bytes'] is not None:
                item['mem_cur_bytes'] = round(item['mem_cur_bytes'] / 2 ** 20, 1)
        return results

    def _generate_paginated_results(interval):
        offset = 0
        is_first = True
        total_count = -1
        while True:
            limit = (interval if is_first else
                    min(interval, total_count - offset))
            try:
                result = execute_paginated_query(limit, offset)
            except Exception as e:
                print_error(e)
                sys.exit(1)
            offset += interval
            total_count = result['total_count']
            items = result['items']
            items = round_mem(items)
            table = tabulate((item.values() for item in items),
                                headers=(item[0] for item in fields))
            if is_first:
                is_first = False
            else:
                table_rows = table.split('\n')
                table = '\n'.join(table_rows[2:])
            yield table + '\n'

            if not offset < total_count:
                break

    with Session() as session:
        paginating_interval = 10
        if all:
            click.echo_via_pager(_generate_paginated_results(paginating_interval))
        else:
            result = execute_paginated_query(paginating_interval, offset=0)
            total_count = result['total_count']
            if total_count == 0:
                print('There are no matching agents.')
                return
            items = result['items']
            items = round_mem(items)
            fields = [field for field in fields if field[1] in items[0]]
            print(tabulate((item.values() for item in items),
                            headers=(item[0] for item in fields)))
            if total_count > paginating_interval:
                print("More agents can be displayed by using --all option.")
--------------------
Prompt: 
def set_pwm(self, led_num, value):
        """Set PWM value for the specified LED

        :param led_num: LED number (0-15)
        :param value: the 12 bit value (0-4095)
        """
----------
Output: 

        self.write_cmd(self.PWR_SET_PWM, led_num, value)

    def set_pwm_freq(self, led_num, freq):
        """Set the PWM frequency for the specified LED

        :param led_num: LED number (0-15)
        :param freq: the frequency to set (0-4095)
        """
        self.write_cmd(self.PWR_SET_PWM_FREQ, led_num, freq)

    def set_pwm_dutycycle(self, led_num, dutycycle
----------
Solution: 

        self.__check_range('led_number', led_num)
        self.__check_range('led_value', value)

        register_low = self.calc_led_register(led_num)

        self.write(register_low, value_low(value))
        self.write(register_low + 1, value_high(value))
--------------------
Prompt: 
def _get_recursive_iterator(self, recursive=False):
        """
        Basic recursive iterator whose only purpose is to yield all items
        and sections in order, with their full paths as keys.

        Main challenge is to de-duplicate items and sections which
        have aliases.

        Do not add any new features to this iterator, instead
        build others that extend this one.
        """
----------
Output: 

        for path in self.paths:
            if recursive:
                for alias in path.get_all_aliases():
                    yield alias
            else:
                yield path

    def _get_paths(self):
        """
        Return the full paths to all the paths in the current
        configuration, including the ones that are not present.
        """
        paths = []
        for section in self._get_recursive_iterator():
            paths.extend(section.get_paths())
        return paths

    def _get_path_to_section(self, path):
        """
        Return the section where the path is defined.
        """
----------
Solution: 


        names_yielded = set()

        for obj_alias, obj in self._tree.items():
            if obj.is_section:
                if obj.alias in names_yielded:
                    continue
                names_yielded.add(obj.alias)

                yield (obj.alias,), obj

                if not recursive:
                    continue

                for sub_item_path, sub_item in obj._get_recursive_iterator(recursive=recursive):
                    yield (obj_alias,) + sub_item_path, sub_item

            else:
                # _tree contains duplicates so that we can have multiple aliases point
                # to the same item. We have to de-duplicate here.
                if obj.name in names_yielded:
                    continue
                names_yielded.add(obj.name)

                yield (obj.name,), obj
--------------------
Prompt: 
def set_gcc():
    """Try to use GCC on OSX for OpenMP support."""
----------
Output: 

    if sys.platform == 'darwin':
        return
    try:
        out = subprocess.check_output(['gcc', '-dumpversion'], stderr=subprocess.STDOUT)
        out = out.decode().strip()
        if out:
            gcc_version = out
        else:
            gcc_version = 'unknown'
    except (subprocess.CalledProcessError, OSError):
        gcc_version = 'unknown'
    if not out:
        raise Exception('Could not find the GCC')
    subprocess.check_call(['make', 'clean'])
    subprocess.check_call(['make', 'install'])
    subprocess.check_call
----------
Solution: 

    # For macports and homebrew

    if 'darwin' in platform.platform().lower():
        gcc = extract_gcc_binaries()

        if gcc is not None:
            os.environ["CC"] = gcc
            os.environ["CXX"] = gcc

        else:
            global use_openmp
            use_openmp = False
            logging.warning('No GCC available. Install gcc from Homebrew '
                            'using brew install gcc.')
--------------------
Prompt: 
def downsample_rand(a, samples, remove_invalid=False, ret_idx=False):
    """Downsampling by randomly removing points

    Parameters
    ----------
    a: 1d ndarray
        The input array to downsample
    samples: int
        The desired number of samples
    remove_invalid: bool
        Remove nan and inf values before downsampling
    ret_idx: bool
        Also return a boolean array that corresponds to the
        downsampled indices in `a`.

    Returns
    -------
    dsa: 1d ndarray of size `samples`
        The pseudo-randomly downsampled array `a`
    idx: 1d boolean array with same shape as `a`
        Only returned if `ret_idx` is True.
        A boolean array such that `a[idx] == dsa`
    """
----------
Output: 
 indices.
    """
    if ret_idx:
        return a[ret_idx]
    else:
        return a[a > 0] + 1


def _downsample_inner(a, samples, axis=0):
    """Inner downsampling

    Parameters
    ----------
    a: 1d ndarray
        The input array to downsample
    samples: int
        The desired number of samples
    axis: int
        The axis along which to downsample

    Returns
    -------
    dsa: 1d ndarray of size `samples`
        The inner downsampled indices.
    """
    return np.random
----------
Solution: 

    # fixed random state for this method
    rs = np.random.RandomState(seed=47).get_state()
    np.random.set_state(rs)

    samples = int(samples)

    if remove_invalid:
        # slice out nans and infs
        bad = np.isnan(a) | np.isinf(a)
        pool = a[~bad]
    else:
        pool = a

    if samples and (samples < pool.shape[0]):
        keep = np.zeros_like(pool, dtype=bool)
        keep_ids = np.random.choice(np.arange(pool.size),
                                    size=samples,
                                    replace=False)
        keep[keep_ids] = True
        dsa = pool[keep]
    else:
        keep = np.ones_like(pool, dtype=bool)
        dsa = pool

    if remove_invalid:
        # translate the kept values back to the original array
        idx = np.zeros(a.size, dtype=bool)
        idx[~bad] = keep
    else:
        idx = keep

    if ret_idx:
        return dsa, idx
    else:
        return dsa
--------------------
Prompt: 
def get_random_connection(self):
        """
        Open new connection to random redis server.
        """
----------
Output: 

        self.client = redis.StrictRedis(
            host=self.host, port=self.port, db=self.db, password=self.password)

    def get_redis_connection(self):
        """
        Create new redis connection.
        """
        self.client.set(self.key, self.value)

    def get_random_key(self):
        """
        Get random key.
        """
        return self.key

    def get_random_value(self):
        """
        Get random value.
        """
        return self.value

    def close(self):
        """
        Close
----------
Solution: 

        if self._available_connections:
            node_name = random.choice(list(self._available_connections.keys()))
            conn_list = self._available_connections[node_name]
            # check it in case of empty connection list
            if conn_list:
                return conn_list.pop()
        for node in self.nodes.random_startup_node_iter():
            connection = self.get_connection_by_node(node)

            if connection:
                return connection

        raise Exception("Cant reach a single startup node.")
--------------------
Prompt: 
def apply(self, func, workers=1, job_size=10000):
    """Apply `func` to lines of text in parallel or sequential.

    Args:
      func : a function that takes a list of lines.
    """
----------
Output: 

    if workers == 1:
      self.run_parallel(func)
    else:
      self.run_sequential(func, workers, job_size)

  def run_parallel(self, func):
    """Run `func` in parallel.

    Args:
      func : a function that takes a list of lines.
    """
    if self.is_parallel:
      self.pool.map(func, self.lines)
    else:
      self.pool.map(func, self.lines)

  def run_parallel_raw(self, func):
    """Run `func` in parallel.

   
----------
Solution: 

    if workers == 1:
      for lines in self.iter_chunks(job_size):
        yield func(lines)
    else:
      with ProcessPoolExecutor(max_workers=workers) as executor:
        for result in executor.map(func, self.iter_chunks(job_size)):
          yield result
--------------------
Prompt: 
def copy_spline_array(a):
    """
    This returns an instance of a new spline_array with all the fixins, and the data from a.
    """
----------
Output: 

    return SplineArray(a.copy(), a.copy())


def copy_spline_points(a):
    """
    This returns an instance of a new spline_points with all the fixins, and the data from a.
    """
    return SplinePoints(a.copy(), a.copy())


def copy_spline_lines(a):
    """
    This returns an instance of a new spline_lines with all the fixins, and the data from a.
    """
    return SplineLines(a.copy(), a.copy())


def copy_spline_curves(a
----------
Solution: 


    b = spline_array()

    b.x_splines = a.x_splines
    b.y_splines = a.y_splines
    b.max_y_splines = a.max_y_splines
    b.xmin = a.xmin     
    b.xmax = a.xmax
    b.ymin = a.ymin
    b.ymax = a.ymax
    b.xlabel = a.xlabel
    b.ylabel = a.ylabel
    b.zlabel = a.zlabel
    b.simple = a.simple
    

    b.generate_y_values()    

    return b
--------------------
Prompt: 
def validate():
    """Display error messages and exit if no lore environment can be found.
    """
----------
Output: 

    if not os.path.exists(os.environ.get('LOOSE_ENVIRONMENT_PATH')):
        print 'No lore environment found.  Run loose-env to generate a new one.'
        sys.exit(1)
    if not os.path.exists(os.environ.get('LOOSE_ENVIRONMENT_PATH_DEBUG')):
        print 'Debug mode.  Run loose-env to generate a new one.'
        sys.exit(1)
    if not os.path.exists(os.environ.get('LOOSE_ENVIRON
----------
Solution: 

    if not os.path.exists(os.path.join(ROOT, APP, '__init__.py')):
        message = ansi.error() + ' Python module not found.'
        if os.environ.get('LORE_APP') is None:
            message += ' $LORE_APP is not set. Should it be different than "%s"?' % APP
        else:
            message += ' $LORE_APP is set to "%s". Should it be different?' % APP
        sys.exit(message)

    if exists():
        return

    if len(sys.argv) > 1:
        command = sys.argv[1]
    else:
        command = 'lore'
    sys.exit(
        ansi.error() + ' %s is only available in lore '
                       'app directories (missing %s)' % (
            ansi.bold(command),
            ansi.underline(VERSION_PATH)
        )
    )
--------------------
Prompt: 
def get_current_task_info(self):
        """Return dictionary of current task function info for this object."""
----------
Output: 

        return self._task_info

    def get_task_info(self):
        """Return dictionary of current task function info for this object."""
        return self._task_info

    def get_task_id(self):
        """Return current task id for this object."""
        return self._task_id

    def get_task_name(self):
        """Return current task name for this object."""
        return self._task_info.name

    def get_task_id_str(self):
        """Return current task id as a string."""
        return self.get_task_id()

    def get
----------
Solution: 

        name = self.model.workflow.name
        if not name:
            return

        current_task = workflows[name].workflow
        for step in self.callback_pos:
            current_task = current_task[step]
            if callable(current_task):
                return get_func_info(current_task)
--------------------
Prompt: 
def commit(self):
        """
        Insert the text at the current cursor position.
        """
----------
Output: 

        self.textCursor().insertText(self.document().lastBlock().text())
        self.textCursor().setPosition(self.textCursor().position() + 1, QTextCursor.KeepAnchor)

    def findBlock(self, text):
        """
        Find the block containing the given text.
        """
        return self.document().findBlock(text)

    def findText(self, text):
        """
        Find the text at the current cursor position.
        """
        return self.textCursor().position() + 1, QTextCursor.KeepAnchor, text

    def findBlockAt(self,
----------
Solution: 


        tc = self.qteWidget.textCursor()

        # If this is the first ever call to this undo/redo element then
        # backup the current cursor position and the selected text (may be
        # none). This information will be required for the redo operation
        # to position the cursor (and selected text) where it was at the
        # very first call.
        if self.cursorPos0 is None:
            self.cursorPos0 = tc.position()
            self.selText = tc.selection().toHtml()
            self.selStart = tc.selectionStart()
            self.selEnd = tc.selectionEnd()
        else:
            tc.setPosition(self.cursorPos0, QtGui.QTextCursor.MoveAnchor)

        # Remove the originally selected text (may be none).
        tc.setPosition(self.selStart, QtGui.QTextCursor.MoveAnchor)
        tc.setPosition(self.selEnd, QtGui.QTextCursor.KeepAnchor)
        tc.removeSelectedText()

        # Move to the start of the (just deleted) text block and insert
        # the characters there.
        if len(self.selText) > 0:
            pos = self.selStart
        else:
            pos = tc.position()
        tc.setPosition(pos)

        # Backup the cursor position before the insertion operation,
        # insert the new character(s), move the cursor forward, and
        # backup the new cursor position as well.
        self.cursorPos1 = tc.position()
        tc.insertText(self.text)
        self.cursorPos2 = tc.position()

        self.qteWidget.setTextCursor(tc)
--------------------
Prompt: 
def camelToSnake(s):
    """
    https://gist.github.com/jaytaylor/3660565
    Is it ironic that this function is written in camel case, yet it
    converts to snake case? hmm..
    """
----------
Output: 

    return re.sub(r'([A-Z])', lambda m: m.group(1).upper(), s)


def snakeToSpace(s):
    """
    https://gist.github.com/jaytaylor/3660565
    Is it a space that is not a multiple of 4?
    """
    return re.sub(r'([A-Z])', lambda m: m.group(1).upper(), s)


def spaceToSke(space):
    """
    https://gist.github.com/jaytaylor/3660565
   
----------
Solution: 

    _underscorer1 = re.compile(r'(.)([A-Z][a-z]+)')
    _underscorer2 = re.compile('([a-z0-9])([A-Z])')

    subbed = _underscorer1.sub(r'\1_\2', s)
    return _underscorer2.sub(r'\1_\2', subbed).lower()
--------------------
Prompt: 
def yank_fields_from_attrs(attrs, _as=None, sort=True):
    """
    Extract all the fields in given attributes (dict)
    and return them ordered
    """
----------
Output: 

    if _as is not None:
        return _as(attrs, sort=sort)
    else:
        return sorted(attrs.items())


def yank_fields_to_str(yank_fields):
    """
    Convert a dictionary of attributes to a string
    """
    return json.dumps(yank_fields, indent=4, sort_keys=True)


def yank_fields_to_int(yank_fields):
    """
    Convert a dictionary of attributes to an int
    """
    return int(yank_fields.items())


def yank_
----------
Solution: 

    fields_with_names = []
    for attname, value in list(attrs.items()):
        field = get_field_as(value, _as)
        if not field:
            continue
        fields_with_names.append((attname, field))

    if sort:
        fields_with_names = sorted(fields_with_names, key=lambda f: f[1])
    return OrderedDict(fields_with_names)
--------------------
Prompt: 
def resolve_aliases(data_type):
    """
    Resolve all chained / nested aliases. This will recursively point
    nested aliases to their resolved data type (first non-alias in the chain).

    Note: This differs from unwrap_alias which simply identifies/returns
    the resolved data type.

    Args:
        data_type (DataType): The target DataType/Alias to resolve.
    Return:
        DataType: The resolved type.
    """
----------
Output: 

    if data_type.is_primitive():
        return DataType.Primitive
    elif data_type.is_primitive_array():
        return DataType.PrimitiveArray
    elif data_type.is_primitive_map():
        return DataType.PrimitiveMap
    elif data_type.is_primitive_map_array():
        return DataType.PrimitiveMapArray
    elif data_type.is_primitive_map_map():
        return DataType.PrimitiveMapMap
    elif data_type.is_primitive_map_map():
        return DataType.PrimitiveMapMap
    elif data_type.is_
----------
Solution: 

    if not is_alias(data_type):
        return data_type

    resolved = resolve_aliases(data_type.data_type)
    data_type.data_type = resolved

    return resolved
--------------------
Prompt: 
def epomeo_gpx(max_iters=200, optimize=True, plot=True):
    """
    Perform Gaussian process regression on the latitude and longitude data
    from the Mount Epomeo runs. Requires gpxpy to be installed on your system
    to load in the data.
    """
----------
Output: 

    import gpxpy
    from gpxpy.gpx import GPX
    from gpxpy.gpx.GPX import GPX_File
    from gpxpy.gpx.GPX import GPX_File_Type
    from gpxpy.gpx.GPX import GPX_File_Type_Options
    from gpxpy.gpx.GPX import GPX_File_Type_Options_No_File
    from gpxpy.gpx.GPX import GPX_File_Type_Options_No_File_Type
    from gpxpy.gpx
----------
Solution: 

    try:import pods
    except ImportError:
        print('pods unavailable, see https://github.com/sods/ods for example datasets')
        return
    data = pods.datasets.epomeo_gpx()
    num_data_list = []
    for Xpart in data['X']:
        num_data_list.append(Xpart.shape[0])

    num_data_array = np.array(num_data_list)
    num_data = num_data_array.sum()
    Y = np.zeros((num_data, 2))
    t = np.zeros((num_data, 2))
    start = 0
    for Xpart, index in zip(data['X'], range(len(data['X']))):
        end = start+Xpart.shape[0]
        t[start:end, :] = np.hstack((Xpart[:, 0:1],
                                    index*np.ones((Xpart.shape[0], 1))))
        Y[start:end, :] = Xpart[:, 1:3]

    num_inducing = 200
    Z = np.hstack((np.linspace(t[:,0].min(), t[:, 0].max(), num_inducing)[:, None],
                   np.random.randint(0, 4, num_inducing)[:, None]))

    k1 = GPy.kern.RBF(1)
    k2 = GPy.kern.Coregionalize(output_dim=5, rank=5)
    k = k1**k2

    m = GPy.models.SparseGPRegression(t, Y, kernel=k, Z=Z, normalize_Y=True)
    m.constrain_fixed('.*variance', 1.)
    m.inducing_inputs.constrain_fixed()
    m.Gaussian_noise.variance.constrain_bounded(1e-3, 1e-1)
    m.optimize(max_iters=max_iters,messages=True)

    return m
--------------------
Prompt: 
def style(self, value):
        """gets/sets the style"""
----------
Output: 

        return self._style

    @style.setter
    def style(self, value):
        """gets/sets the style"""
        self._style = value

    @property
    def style_type(self):
        """gets/sets the style type"""
        return self._style_type

    @style_type.setter
    def style_type(self, value):
        """gets/sets the style type"""
        self._style_type = value

    @property
    def style_name(self):
        """gets/sets the style name"""
        return self._style_name

    @style_name.setter
   
----------
Solution: 

        if self._style != value and \
           value in self._styles:
            self._style = value
--------------------
Prompt: 
def parameters_changed(self):
        """
        Parameters have now changed
        """
----------
Output: 

        self.logger.debug("Parameters changed")
        self.parameters_changed = True

    def get_parameters(self):
        """
        Get the current parameters
        """
        return self.parameters_list

    def get_parameter_by_name(self, name):
        """
        Get the parameter with the given name
        """
        return self.parameters_list[name]

    def get_parameter_by_index(self, index):
        """
        Get the parameter with the given index
        """
        return self.parameters_list[index]

    def get_parameter_by_name_and_index(
----------
Solution: 

        # Get the model matrices from the kernel
        (F,L,Qc,H,Pinf,dF,dQc,dPinf) = self.kern.sde()

        # Use the Kalman filter to evaluate the likelihood
        self._log_marginal_likelihood = self.kf_likelihood(F,L,Qc,H,self.sigma2,Pinf,self.X.T,self.Y.T)
        gradients  = self.compute_gradients()
        self.sigma2.gradient_full[:] = gradients[-1]
        self.kern.gradient_full[:] = gradients[:-1]
--------------------
Prompt: 
def _create_dd_event(self, events, image, c_tags, priority='Normal'):
        """Create the actual event to submit from a list of similar docker events"""
----------
Output: 

        return {
            'type': 'docker',
            'image': image,
            'command': 'docker run -d -p {0} {1}'.format(
                self.project.name, image),
            'environment': self.environment,
            'environment_vars': {'PULP_VERSION': self.project.version},
            'labels': {'airflow-version': 'v' + airflow_version.replace('.', '-').replace('+', '-')},
                     'airflow-version': airflow_version.replace('+', '-'),
                     'airflow-branch': self.branch,
                     'airflow-task-id':
----------
Solution: 

        if not events:
            return

        max_timestamp = 0
        status = defaultdict(int)
        status_change = []

        for ev, c_name in events:
            max_timestamp = max(max_timestamp, int(ev['time']))
            status[ev['status']] += 1
            status_change.append([c_name, ev['status']])

        status_text = ", ".join(["%d %s" % (count, st) for st, count in status.iteritems()])
        msg_title = "%s %s on %s" % (image, status_text, self.hostname)
        msg_body = (
            "%%%\n"
            "{image_name} {status} on {hostname}\n"
            "```\n{status_changes}\n```\n"
            "%%%"
        ).format(
            image_name=image,
            status=status_text,
            hostname=self.hostname,
            status_changes="\n".join(
                ["%s \t%s" % (change[1].upper(), change[0]) for change in status_change])
        )

        if any(error in status_text for error in ERROR_ALERT_TYPE):
            alert_type = "error"
        else:
            alert_type = None

        return {
            'timestamp': max_timestamp,
            'host': self.hostname,
            'event_type': EVENT_TYPE,
            'msg_title': msg_title,
            'msg_text': msg_body,
            'source_type_name': EVENT_TYPE,
            'event_object': 'docker:%s' % image,
            'tags': list(c_tags),
            'alert_type': alert_type,
            'priority': priority
        }
--------------------
Prompt: 
def prefix(self, name):
        """
        :param string name: the name of an attribute to look up.

        :return: the prefix component of the named attribute's name,
            or None.
        """
----------
Output: 

        return self.attribute_prefix_map.get(name, None)

    def __getitem__(self, name):
        """
        :param string name: the name of an attribute to look up.

        :return: the attribute value or None.
        """
        return self.attribute_value_map.get(name, None)

    def __setitem__(self, name, value):
        """
        :param string name: the name of an attribute to look up.

        :param string value: the attribute value.
        """
        self.attribute_value_map[name] = value

    def __delitem__(
----------
Solution: 

        a_node = self.adapter.get_node_attribute_node(self.impl_element, name)
        if a_node is None:
            return None
        return a_node.prefix
--------------------
Prompt: 
def create_review(self, commit=github.GithubObject.NotSet, body=None, event=github.GithubObject.NotSet, comments=github.GithubObject.NotSet):
        """
        :calls: `POST /repos/:owner/:repo/pulls/:number/reviews <https://developer.github.com/v3/pulls/reviews/>`_
        :param commit: github.Commit.Commit
        :param body: string
        :param event: string
        :param comments: list
        :rtype: :class:`github.PullRequestReview.PullRequestReview`
        """
----------
Output: 

        assert commit is github.GithubObject.NotSet or isinstance(commit, github.Commit.Commit), commit
        assert body is github.GithubObject.NotSet or isinstance(body, (str, unicode)), body
        assert event is github.GithubObject.NotSet or isinstance(event, (str, unicode)), event
        assert isinstance(comments, (list, tuple)), comments
        post_parameters = {
            "commit": commit._identity,
            "body": body,
            "event": event,
        }
        if body is not github.GithubObject.NotSet:
            post_parameters["body"] = body._identity
        if event is not
----------
Solution: 

        assert commit is github.GithubObject.NotSet or isinstance(commit, github.Commit.Commit), commit
        assert isinstance(body, str), body
        assert event is github.GithubObject.NotSet or isinstance(event, str), event
        assert comments is github.GithubObject.NotSet or isinstance(comments, list), comments
        post_parameters = dict()
        if commit is not github.GithubObject.NotSet:
            post_parameters['commit_id'] = commit.sha
        post_parameters['body'] = body
        post_parameters['event'] = 'COMMENT' if event == github.GithubObject.NotSet else event
        if comments is github.GithubObject.NotSet:
            post_parameters['comments'] = []
        else:
            post_parameters['comments'] = comments
        headers, data = self._requester.requestJsonAndCheck(
            "POST",
            self.url + "/reviews",
            input=post_parameters
        )
        self._useAttributes(data)
        return github.PullRequestReview.PullRequestReview(self._requester, headers, data, completed=True)
--------------------
Prompt: 
def draw(self):
        """Draws the image at the given location."""
----------
Output: 

        if self.image:
            self.image.draw(self.canvas, self.x, self.y, self.width, self.height)

    def __str__(self):
        """Returns a string representation of this image."""
        return 'Image: %s' % self.image.filename

    def __repr__(self):
        """Returns a string representation of this image."""
        return 'Image(%s)' % self.image.filename

class _DrawingMixin(object):
    """This mixin provides common functionality for all drawing commands."""
    def draw(self):
        """Draws the image
----------
Solution: 

        if not self.visible:
            return

        self.window.blit(self.image, self.loc)
--------------------
Prompt: 
def advance(self, length):
        """Advance the cursor in data buffer 'length' bytes."""
----------
Output: 

        self.data_buffer.extend(self.cursor)
        self.cursor = b''
        self.data_buffer_len = length

    def read(self, length):
        """Read 'length' bytes from the data buffer and advance cursor."""
        if not length:
            return b''
        if length > len(self.data_buffer):
            self.data_buffer_len = len(self.data_buffer)
        else:
            self.data_buffer_len = length
        return self.data_buffer_len

    def write(self, data):
        """Write 'data' bytes
----------
Solution: 

        new_position = self._position + length
        if new_position < 0 or new_position > len(self._data):
            raise Exception('Invalid advance amount (%s) for cursor.  '
                            'Position=%s' % (length, new_position))
        self._position = new_position
--------------------
Prompt: 
def thumb(self, size=BIGTHUMB):
        """Get a thumbnail as string or None if the file isnt an image

        size would be one of JFSFile.BIGTHUMB, .MEDIUMTHUMB, .SMALLTHUMB or .XLTHUMB"""
----------
Output: 

        if size == JFSFile.BIGTHUMB:
            return self.get_file_icon(JFSFile.MEDIUMTHUMB)
        elif size == JFSFile.XLTHUMB:
            return self.get_file_icon(JFSFile.SMALLTHUMB)
        else:
            return self.get_file_icon(JFSFile.UNKNOWN)

    def get_file_icon(self, filename):
        """Get the icon for a file

        filename is the filename of the file to get the icon for
        """
        if filename is None:
            return None
----------
Solution: 

        if not self.is_image():
            return None
        if not size in (self.BIGTHUMB, self.MEDIUMTHUMB, self.SMALLTHUMB, self.XLTHUMB):
            raise JFSError('Invalid thumbnail size: %s for image %s' % (size, self.path))
        #return self.jfs.raw('%s?mode=thumb&ts=%s' % (self.path, size))
        return self.jfs.raw(url=self.path,
                            params={'mode':'thumb', 'ts':size})
--------------------
Prompt: 
def sorted(self):
        """Utility function for sort_file_tabs_alphabetically()."""
----------
Output: 

        return self._sort_file_tabs_alphabetically

    def _sort_file_tabs_alphabetically(self, files, tab_size=8):
        """Sort the files in a tabular way, so that the tab bar is always
        the one with the largest values."""
        if len(files) == 0:
            return
        for i in range(len(files)):
            if len(files[i]) > tab_size:
                files[i] = sorted(files[i], key=lambda f: len(f[0]), reverse=True)

    def _get_
----------
Solution: 

        for i in range(0, self.tabs.tabBar().count() - 1):
            if (self.tabs.tabBar().tabText(i) >
                    self.tabs.tabBar().tabText(i + 1)):
                return False
        return True
--------------------
Prompt: 
def _make_request(self, url, headers, params=None):
        """
        Generic request handler for OpenStack API requests
        Raises specialized Exceptions for commonly encountered error codes
        """
----------
Output: 

        if params is None:
            params = {}
        if self.auth_version == 'v2.0':
            self.log.debug('Request: %s' % url)
        else:
            self.log.debug('Request: %s' % url +'(auth_version: %s)' % self.auth_version)
        if self.tenant_id:
            params['tenant_id'] = self.tenant_id
        if self.user_id:
            params['user_id'] = self.user_id
        if self.password:
            params['password'] = self
----------
Solution: 

        self.logger.debug("Request URL, Headers and Params: %s, %s, %s", url, headers, params)

        # Checking if request is in cache
        cache_key = "|".join([url, json.dumps(headers), json.dumps(params), str(self.timeout)])
        if cache_key in self.cache:
            self.logger.debug("Request found in cache. cache key %s", cache_key)
            return self.cache.get(cache_key)

        try:
            resp = requests.get(
                url, headers=headers, verify=self.ssl_verify, params=params, timeout=self.timeout, proxies=self.proxies
            )
            resp.raise_for_status()
        except requests.exceptions.HTTPError as e:
            self.logger.debug("Error contacting openstack endpoint: %s", e)
            if resp.status_code == 401:
                self.logger.info('Need to reauthenticate before next check')
                raise AuthenticationNeeded()
            elif resp.status_code == 409:
                raise InstancePowerOffFailure()
            else:
                raise e
        except Exception:
            self.logger.exception("Unexpected error contacting openstack endpoint {}".format(url))
            raise
        jresp = resp.json()
        self.logger.debug("url: %s || response: %s", url, jresp)

        # Adding response to the cache
        self.cache[cache_key] = jresp
        return jresp
--------------------
Prompt: 
def __skip_this(self, level):
        """
        Check whether this comparison should be skipped because one of the objects to compare meets exclusion criteria.
        :rtype: bool
        """
----------
Output: 

        if level == 0:
            return True
        elif level == 1:
            return self.__skip_this(1)
        else:
            raise ValueError("Level must be > 0")

    def __eq__(self, other):
        """
        Compare this comparison with another one.
        :rtype: bool
        """
        if self.__skip_this(0):
            return True
        elif other.__skip_this(0):
            return False
        else:
            return self.__compare_with_other(0, other)

    def __compare_with_other(self, other, other_type):
        """
        Compare
----------
Solution: 

        skip = False
        if self.exclude_paths and level.path() in self.exclude_paths:
            skip = True
        elif self.exclude_regex_paths and any(
                [exclude_regex_path.search(level.path()) for exclude_regex_path in self.exclude_regex_paths]):
            skip = True
        else:
            if self.exclude_types_tuple and (isinstance(level.t1, self.exclude_types_tuple) or
                                             isinstance(level.t2, self.exclude_types_tuple)):
                skip = True

        return skip
--------------------
Prompt: 
def _call_vagrant_command(self, args):
        """
        Run a vagrant command.  Return None.
        args: A sequence of arguments to a vagrant command line.

        """
----------
Output: 

        if not self.vagrant_command:
            return None
        if len(args) == 1:
            return self.vagrant_command[0]
        if len(args) == 2:
            return self.vagrant_command[1]
        if len(args) == 3:
            return self.vagrant_command[2]
        return self._call_vagrant_command(args)

    def _call_vagrant_command(self, args):
        """
        Run a vagrant command.  Return None.
        args: A sequence of arguments to
----------
Solution: 

        # Make subprocess command
        command = self._make_vagrant_command(args)
        with self.out_cm() as out_fh, self.err_cm() as err_fh:
            subprocess.check_call(command, cwd=self.root, stdout=out_fh,
                                  stderr=err_fh, env=self.env)
--------------------
Prompt: 
def _do_download(version, download_base, to_dir, download_delay):
    """Download Setuptools."""
----------
Output: 

    if not os.path.exists(to_dir):
        os.makedirs(to_dir)
    url = "https://github.com/pypa/setuptools"
    data = {"version": version, "download_base": download_base, "to_dir": to_dir,
            "download_delay": str(download_delay)}
    saveto = os.path.join(to_dir, "setup.py")
    if not os.path.exists(saveto):
        with open(saveto, "w", encoding="utf-8") as f:
            f
----------
Solution: 

    py_desig = 'py{sys.version_info[0]}.{sys.version_info[1]}'.format(sys=sys)
    tp = 'setuptools-{version}-{py_desig}.egg'
    egg = os.path.join(to_dir, tp.format(**locals()))
    if not os.path.exists(egg):
        archive = download_setuptools(version, download_base,
            to_dir, download_delay)
        _build_egg(egg, archive, to_dir)
    sys.path.insert(0, egg)

    # Remove previously-imported pkg_resources if present (see
    # https://bitbucket.org/pypa/setuptools/pull-request/7/ for details).
    if 'pkg_resources' in sys.modules:
        _unload_pkg_resources()

    import setuptools
    setuptools.bootstrap_install_from = egg
--------------------
Prompt: 
def _init_settings(self):
        """ Init setting """
----------
Output: 

        self.settings = QSettings()
        self.settings.beginGroup('qgis')
        self.settings.setValue('inasafe/analysis_extents_mode', 'HazardExposure')
        self.settings.setValue('inasafe/use_native_zonal_stats', True)
        self.settings.setValue('inasafe/analysis_extents_extent', extents_mode)
        self.settings.endGroup()
        self.settings.beginGroup('gdal')
        self.gdal_version = gdal.VersionInfo('VERSION_NUM')
        self.gdal_full_
----------
Solution: 

        self._show_whitespaces = False
        self._tab_length = 4
        self._use_spaces_instead_of_tabs = True
        self.setTabStopWidth(self._tab_length *
                             self.fontMetrics().width(" "))
        self._set_whitespaces_flags(self._show_whitespaces)
--------------------
Prompt: 
def endLoop(self, useDriverLoop):
        """
        Called by the engine to stop an event loop.
        """
----------
Output: 

        self.running = False
        self.loop = None
        self.loopContext = None
        self.loopContextStack = []
        self.loopContext = None
        self.loopContextStack.append(self)
        self.loop = None
        self.lastEvent = None
        self.lastEventStack = []
        self.lastEvent = None
        self.loopContext = None
        self.loopContextStack.pop()

    def get_loop_index(self):
        """
        Returns the index of the event loop.
        """
        return self.loopContextStack[-1]

    def get_loop_
----------
Solution: 

        self._queue = []
        self._driver.stop()
        if useDriverLoop:
            self._driver.endLoop()
        else:
            self._iterator = None
        self.setBusy(True)
--------------------
Prompt: 
def precision(self):
        """Calculates precision

        :return: Precision of matrix
        """
----------
Output: 

        return self.get_value(self.get_key(self.get_value(self.get_key(self.get_value(self.get_key(self.get_value(self.get_key(self.get_value(self.get_key(self.get_value(self.get_key(self.get_value(self.get_key(self.get_value(self.get_key(self.get_value(self.get_key(self.get_value(self.get_key(self.get_value(
----------
Solution: 

        true_pos = self.matrix[0][0]
        false_pos = self.matrix[1][0]
        return divide(1.0 * true_pos, true_pos + false_pos)
--------------------
Prompt: 
def percentile(self, percentile):
        """ Returns the nth percentile of the histogram. """
----------
Output: 

        return self.get_percentile(percentile)

    def get_percentile(self, percentile):
        """ Returns the percentile of the histogram. """
        return self.get_percentile_impl(percentile)

    def get_percentile_impl(self, percentile):
        """ Returns the percentile of the histogram. """
        return self.get_percentile_impl_impl(percentile)

    def get_percentile_impl_impl(self, percentile):
        """ Returns the percentile of the histogram's impl. """
        return self.get_percentile_impl_impl(percentile)

    def get_percentile_impl(self,
----------
Solution: 

        assert(percentile >= 0 and percentile <= 100)
        assert(self.kind in ["exponential", "linear", "enumerated", "boolean"])

        fraction = percentile / 100
        to_count = fraction * self.buckets.sum()
        percentile_bucket = 0

        for percentile_bucket in range(len(self.buckets)):
            freq = self.buckets.values[percentile_bucket]
            if to_count - freq <= 0:
                break
            to_count -= freq

        percentile_lower_boundary = self.buckets.index[percentile_bucket]
        percentile_frequency = self.buckets.values[percentile_bucket]

        if percentile_bucket == len(self.buckets) - 1 or percentile_frequency == 0:
            return percentile_lower_boundary

        width = self.buckets.index[percentile_bucket + 1] - self.buckets.index[percentile_bucket]
        return percentile_lower_boundary + width * to_count / percentile_frequency
--------------------
Prompt: 
def compute_diff(dir_base, dir_cmp):
    """ Compare `dir_base' and `dir_cmp' and returns a list with
    the following keys:
     - deleted files `deleted'
     - created files `created'
     - updated files `updated'
     - deleted directories `deleted_dirs'

    """
----------
Output: 

    return [
        os.path.join(dir_base, f)
        for f in os.listdir(dir_base)
        if os.path.isfile(os.path.join(dir_base, f)) and not dir_cmp.has_key(f)
    ]


def get_diff(dir_base, dir_cmp):
    """ Return the list of files that are changed in `dir_cmp'

    """
    return [
        os.path.join(dir_base, f)
        for f in os.listdir(dir_base)
        if not dir_
----------
Solution: 

    data = {}
    data['deleted'] = list(set(dir_cmp['files']) - set(dir_base['files']))
    data['created'] = list(set(dir_base['files']) - set(dir_cmp['files']))
    data['updated'] = []
    data['deleted_dirs'] = list(set(dir_cmp['subdirs']) - set(dir_base['subdirs']))

    for f in set(dir_cmp['files']).intersection(set(dir_base['files'])):
        if dir_base['index'][f] != dir_cmp['index'][f]:
            data['updated'].append(f)

    return data
--------------------
Prompt: 
def _get_reconciled_name_object(self, other):
        """
        If the result of a set operation will be self,
        return self, unless the name changes, in which
        case make a shallow copy of self.
        """
----------
Output: 

        if self.op!= other.op:
            return self.copy()

    def is_valid(self):
        """
        Check if the current state of the symbol is valid.
        """
        return self.op in self.valid_operations

    def is_final(self):
        """
        Check if the current state of the symbol is final.
        """
        return self.op in self.final_operations

    def is_defined(self):
        """
        Check if the current state of the symbol is defined.
        """
        return self.valid_definitions

    def is_final_definition(self, definition
----------
Solution: 

        name = get_op_result_name(self, other)
        if self.name != name:
            return self._shallow_copy(name=name)
        return self
--------------------
Prompt: 
def plotplanarPotentials(Pot,*args,**kwargs):
    """
    NAME:

       plotplanarPotentials

    PURPOSE:

       plot a planar potential

    INPUT:

       Rrange - range (can be Quantity)

       xrange, yrange - if relevant (can be Quantity)

       grid, gridx, gridy - number of points to plot

       savefilename - save to or restore from this savefile (pickle)

       ncontours - number of contours to plot (if applicable)

       +bovy_plot(*args,**kwargs) or bovy_dens2d(**kwargs)

    OUTPUT:

       plot to output device

    HISTORY:

       2010-07-13 - Written - Bovy (NYU)

    """
----------
Output: 
(*args,**kwargs)

    OUTPUT:

       plot a planar potential (if applicable)

    HISTORY:

       2010-07-12 - Written - Bovy (NYU)

    """
    if isinstance(Rrange,Quantity):
        Rrange = Rrange.to(units.Rsun)

    if isinstance(yrange,units.Quantity):
        yrange = yrange.to(units.Rsun)

    if isinstance(Pot,units.Quantity):
        Pot = Pot.to(units.Quantity)

    if isinstance(savefilename,str):
        if not os.path.
----------
Solution: 

    Pot= flatten(Pot)
    Rrange= kwargs.pop('Rrange',[0.01,5.])
    xrange= kwargs.pop('xrange',[-5.,5.])
    yrange= kwargs.pop('yrange',[-5.,5.])
    if _APY_LOADED:
        if hasattr(Pot,'_ro'):
            tro= Pot._ro
        else:
            tro= Pot[0]._ro
        if isinstance(Rrange[0],units.Quantity):
            Rrange[0]= Rrange[0].to(units.kpc).value/tro
        if isinstance(Rrange[1],units.Quantity):
            Rrange[1]= Rrange[1].to(units.kpc).value/tro
        if isinstance(xrange[0],units.Quantity):
            xrange[0]= xrange[0].to(units.kpc).value/tro
        if isinstance(xrange[1],units.Quantity):
            xrange[1]= xrange[1].to(units.kpc).value/tro
        if isinstance(yrange[0],units.Quantity):
            yrange[0]= yrange[0].to(units.kpc).value/tro
        if isinstance(yrange[1],units.Quantity):
            yrange[1]= yrange[1].to(units.kpc).value/tro
    grid= kwargs.pop('grid',100)
    gridx= kwargs.pop('gridx',100)
    gridy= kwargs.pop('gridy',gridx)
    savefilename= kwargs.pop('savefilename',None)
    isList= isinstance(Pot,list)
    nonAxi= ((isList and Pot[0].isNonAxi) or (not isList and Pot.isNonAxi))
    if not savefilename is None and os.path.exists(savefilename):
        print("Restoring savefile "+savefilename+" ...")
        savefile= open(savefilename,'rb')
        potR= pickle.load(savefile)
        if nonAxi:
            xs= pickle.load(savefile)
            ys= pickle.load(savefile)
        else:
            Rs= pickle.load(savefile)
        savefile.close()
    else:
        if nonAxi:
            xs= nu.linspace(xrange[0],xrange[1],gridx)
            ys= nu.linspace(yrange[0],yrange[1],gridy)
            potR= nu.zeros((gridx,gridy))
            for ii in range(gridx):
                for jj in range(gridy):
                    thisR= nu.sqrt(xs[ii]**2.+ys[jj]**2.)
                    if xs[ii] >= 0.:
                        thisphi= nu.arcsin(ys[jj]/thisR)
                    else:
                        thisphi= -nu.arcsin(ys[jj]/thisR)+nu.pi
                    potR[ii,jj]= evaluateplanarPotentials(Pot,thisR,
                                                          phi=thisphi,
                                                          use_physical=False)
        else:
            Rs= nu.linspace(Rrange[0],Rrange[1],grid)
            potR= nu.zeros(grid)
            for ii in range(grid):
                potR[ii]= evaluateplanarPotentials(Pot,Rs[ii],
                                                   use_physical=False)
        if not savefilename is None:
            print("Writing planar savefile "+savefilename+" ...")
            savefile= open(savefilename,'wb')
            pickle.dump(potR,savefile)
            if nonAxi:
                pickle.dump(xs,savefile)
                pickle.dump(ys,savefile)
            else:
                pickle.dump(Rs,savefile)
            savefile.close()
    if nonAxi:
        if not 'orogin' in kwargs:
            kwargs['origin']= 'lower'
        if not 'cmap' in kwargs:
            kwargs['cmap']= 'gist_yarg'
        if not 'contours' in kwargs:
            kwargs['contours']= True
        if not 'xlabel' in kwargs:
            kwargs['xlabel']= r"$x / R_0$"
        if not 'ylabel' in kwargs:
            kwargs['ylabel']= "$y / R_0$"
        if not 'aspect' in kwargs:
            kwargs['aspect']= 1.
        if not 'cntrls' in kwargs:
            kwargs['cntrls']= '-'
        ncontours= kwargs.pop('ncontours',10)
        if not 'levels' in kwargs:
            kwargs['levels']= nu.linspace(nu.nanmin(potR),nu.nanmax(potR),ncontours)
        return plot.bovy_dens2d(potR.T,
                                xrange=xrange,
                                yrange=yrange,**kwargs)
    else:
        kwargs['xlabel']=r"$R/R_0$"
        kwargs['ylabel']=r"$\Phi(R)$"
        kwargs['xrange']=Rrange
        return plot.bovy_plot(Rs,potR,*args,**kwargs)
--------------------
Prompt: 
def get_delta_TR(tail_check_max, y_int):
    """
    input: tail_check_max, y_intercept
    output: delta_TR
    """
----------
Output: 

    return np.array([tail_check_max - y_int, tail_check_max + y_int])


def get_delta_T(tail_check_min, y_int):
    """
    input: tail_check_min, y_int
    output: delta_T
    """
    return np.array([tail_check_min - y_int, tail_check_min + y_int])


def get_delta_T_error(y_int, tail_check_max, delta_max):
    """
    input: y_int, delta
----------
Solution: 

    if tail_check_max == 0 or numpy.isnan(tail_check_max):
        return float('nan')
    delta_TR = (old_div(tail_check_max, abs(y_int))) * 100.
    return delta_TR
--------------------
Prompt: 
def _calc(count, 
              last_count, 
              start_time, 
              max_count, 
              speed_calc_cycles, 
              q, 
              last_speed,
              lock):
        """do the pre calculations in order to get TET, speed, TTG
        
        :param count:               count 
        :param last_count:          count at the last call, allows to treat the case of no progress
            between sequential calls
        :param start_time:          the time when start was triggered
        :param max_count:           the maximal value count 
        :type max_count:
        :param speed_calc_cycles:
        :type speed_calc_cycles:
        :param q:
        :type q:
        :param last_speed:
        :type last_speed:
        :param lock:
        :type lock:
        """
----------
Output: 
:             int
        :type q:                      str
        :type q:                      str
        :type  last_q:                      str
        :type  start_time:                  int
        :type  max_count:                  int
        :type  speed_calc_cycles:            int
        :type  q:                      str
        :type  last_speed:                  int
        :type  start_time:                  int
        :type  max_count:                  int
        :type  speed_calc_cycles:            int
        :type  q:                      str
        :type  last_speed:                  int
       
----------
Solution: 

        count_value = count.value
        start_time_value = start_time.value
        current_time = time.time()
        
        if last_count.value != count_value:
            # some progress happened
        
            with lock:
                # save current state (count, time) to queue
                
                q.put((count_value, current_time))
    
                # get older state from queue (or initial state)
                # to to speed estimation                
                if q.qsize() > speed_calc_cycles:
                    old_count_value, old_time = q.get()
                else:
                    old_count_value, old_time = 0, start_time_value
            
            last_count.value = count_value
            #last_old_count.value = old_count_value
            #last_old_time.value = old_time
            
            speed = (count_value - old_count_value) / (current_time - old_time)
            last_speed.value = speed 
        else:
            # progress has not changed since last call
            # use also old (cached) data from the queue
            #old_count_value, old_time = last_old_count.value, last_old_time.value
            speed = last_speed.value  

        if (max_count is None):
            max_count_value = None
        else:
            max_count_value = max_count.value
            
        tet = (current_time - start_time_value)
        
        if (speed == 0) or (max_count_value is None) or (max_count_value == 0):
            ttg = None
        else:
            ttg = math.ceil((max_count_value - count_value) / speed)
            
        return count_value, max_count_value, speed, tet, ttg
--------------------
Prompt: 
def symbol_for_ticker(self, ticker, field, provider, providers):
        """Return an instance of *symboldata* containing
information about the data provider, the data provider ticker name
and the data provider field."""
----------
Output: 

        return symboldata.Symbol(ticker, field, self.context)

    def get_ticker_name(self, ticker):
        """Return the name of the data provider field for *ticker*."""
        return self.get_field_name(ticker, 'name')

    def get_field_name(self, ticker, field):
        """Return the instance of *fielddata* containing
information about the data provider, the field name and the instance
of *symboldata* containing information about the data provider,
the data provider ticker name and the instance of *symboldata*
containing
----------
Solution: 

        provider = provider or settings.default_provider
        if provider:
            provider = providers.get(provider, None)
        return self.symboldata(ticker, field, provider)
--------------------
Prompt: 
def copy_to(self, dest, buffering: int = -1):
        """
        copy the file to dest path.

        `dest` canbe `str`, `FileInfo` or `DirectoryInfo`.

        if `dest` is `DirectoryInfo`, that mean copy into the dir with same name.
        """
----------
Output: 

        if not isinstance(dest, str):
            raise TypeError("dest must be a str")
        if not isinstance(buffer, int):
            raise TypeError("buffer must be an int")
        if buffering < 0:
            raise ValueError("buffer must be non-negative")
        if dest.is_dir():
            dest = dest.parent
        if not dest.is_file():
            raise TypeError("dest must be a FileInfo or DirectoryInfo")
        if not dest.is_directory():
            raise TypeError("dest must be a directory")
        if not dest.is_file():
            raise TypeError("dest must be a
----------
Solution: 

        if isinstance(dest, str):
            dest_path = dest
        elif isinstance(dest, FileInfo):
            dest_path = dest.path
        elif isinstance(dest, DirectoryInfo):
            dest_path = dest.path / self.path.name
        else:
            raise TypeError('dest is not one of `str`, `FileInfo`, `DirectoryInfo`')

        with open(self._path, 'rb', buffering=buffering) as source:
            # use x mode to ensure dest does not exists.
            with open(dest_path, 'xb') as dest_file:
                for buffer in source:
                    dest_file.write(buffer)
--------------------
Prompt: 
def seperate_symbols(func):
    """
    Seperate the symbols in symbolic function func. Return them in alphabetical
    order.

    :param func: scipy symbolic function.
    :return: (vars, params), a tuple of all variables and parameters, each 
        sorted in alphabetical order.
    :raises TypeError: only symfit Variable and Parameter are allowed, not sympy
        Symbols.
    """
----------
Output: 

    if isinstance(func, sympy.Symbol):
        return func.variables, func.params
    elif isinstance(func, sympy.Symbol):
        return tuple(seperate_symbols(seperate_symbol) for seperate_symbol in seperate_symbols(func))
    elif isinstance(func, sympy.Symbol):
        return seperate_symbols(seperate_symbol)
    else:
        raise TypeError("Invalid function: %s" % str(func))

def seperate_symbols_list(seperate_symbols_list):
    """
    Seperate the
----------
Solution: 

    params = []
    vars = []
    for symbol in func.free_symbols:
        if not isidentifier(str(symbol)):
            continue  # E.g. Indexed objects might print to A[i, j]
        if isinstance(symbol, Parameter):
            params.append(symbol)
        elif isinstance(symbol, Idx):
            # Idx objects are not seen as parameters or vars.
            pass
        elif isinstance(symbol, (MatrixExpr, Expr)):
            vars.append(symbol)
        else:
            raise TypeError('model contains an unknown symbol type, {}'.format(type(symbol)))

    for der in func.atoms(sympy.Derivative):
        # Used by jacobians and hessians, where derivatives are treated as
        # Variables. This way of writing it is purposefully discriminatory
        # against derivatives wrt variables, since such derivatives should be
        # performed explicitly in the case of jacs/hess, and are treated
        # differently in the case of ODEModels.
        if der.expr in vars and all(isinstance(s, Parameter) for s in der.variables):
            vars.append(der)

    params.sort(key=lambda symbol: symbol.name)
    vars.sort(key=lambda symbol: symbol.name)
    return vars, params
--------------------
Prompt: 
def search(d, recursive=True, store_meta=True):
    """
    Search for DICOM files within a given directory and receive back a 
    dictionary of {StudyInstanceUID: {SeriesNumber: [files]}}
    
    Example usage::
        >>> import yaxil.dicom
        >>> yaxil.dicom.search("~/dicoms").keys()
        ['1.2.340.500067.8.9.10.11012.13000001401516017181900000200']
        
    :param d: Directory name
    :type d: str
    :param recursive: Search recursively
    :type recursive: bool
    :param store_meta: Read and store metadata for each file for fast lookups
    :type store_meta: bool
    :returns: Dictionary of {StudyInstanceUID: {SeriesNumber: [files]}}
    :rtype: dict
    """
----------
Output: 
 recursively for files
    :type recursive: bool
    :param store_meta: If True, the metadata will be stored in the dicom 
        directory.  Otherwise, the default database will be used.
    :type store_meta: bool
    :returns: A dictionary of {StudyInstanceUID: {SeriesNumber: [files]}}
    """
    if not os.path.isdir(d):
        raise ValueError("Directory %s does not exist" % d)
    return search_files(d, recursive, store_meta)

def search_files(d, recursive=True, store_meta=
----------
Solution: 

    # say this fast three times
    scans = col.defaultdict(lambda: col.defaultdict(lambda: col.defaultdict(list)))
    for dirpath,dirnames,filenames in os.walk(os.path.expanduser(d)):
        for f in filenames:
            fullfile = os.path.join(dirpath, f)
            try:
                d = pydicom.read_file(fullfile, stop_before_pixels=True)
            except pydicom.filereader.InvalidDicomError:
                continue
            meta = {k: getattr(d, k, None) for k in d.dir()} if store_meta else None
            scans[d.StudyInstanceUID][d.SeriesNumber][d.InstanceNumber].append(DicomFile(meta=meta, file=fullfile))
        if not recursive:
            del dirnames[:]
    return scans
--------------------
Prompt: 
def _wait_for_lock_to_disappear(handle, lock_file, lock_file_timeout_sec):
  """Waits for the lock file to disappear.

  The lock file was created by another process that is performing a download
  into its own temporary directory. The name of this temp directory is
  sha1(<module>).<uuid>.tmp where <uuid> comes from the lock file.

  Args:
    handle: The location from where a module is being download.
    lock_file: Lock file created by another process downloading this module.
    lock_file_timeout_sec: The amount of time to wait (in seconds) before we
                           can declare that the other downloaded has been
                           abandoned. The download is declared abandoned if
                           there is no file size change in the temporary
                           directory within the last 'lock_file_timeout_sec'.
  """
----------
Output: 
 timeout in seconds for the lock file.

  Returns:
    The path to the lock file.
  """
  handle = handle.lower()
  if not lock_file.is_locked():
    raise LockError('Lock file %s is not locked' % lock_file.name)
  if not lock_file_timeout_sec.is_locked():
    raise LockTimeoutError('Lock timeout in seconds for %s' % lock_file.name)
  if not os.path.exists(lock_file.lock_file):
    raise LockError('Lock file %s does not exist' % lock_
----------
Solution: 

  locked_tmp_dir_size = 0
  locked_tmp_dir_size_check_time = time.time()
  lock_file_content = None
  while tf_v1.gfile.Exists(lock_file):
    try:
      logging.log_every_n(
          logging.INFO,
          "Module '%s' already being downloaded by '%s'. Waiting.", 10,
          handle, tf_utils.read_file_to_string(lock_file))
      if (time.time() - locked_tmp_dir_size_check_time >
          lock_file_timeout_sec):
        # Check whether the holder of the current lock downloaded anything
        # in its temporary directory in the last 'lock_file_timeout_sec'.
        cur_locked_tmp_dir_size = _locked_tmp_dir_size(lock_file)
        cur_lock_file_content = tf_utils.read_file_to_string(lock_file)
        if (cur_locked_tmp_dir_size == locked_tmp_dir_size and
            cur_lock_file_content == lock_file_content):
          # There is was no data downloaded in the past
          # 'lock_file_timeout_sec'. Steal the lock and proceed with the
          # local download.
          logging.warning("Deleting lock file %s due to inactivity.",
                          lock_file)
          tf_v1.gfile.Remove(lock_file)
          break
        locked_tmp_dir_size = cur_locked_tmp_dir_size
        locked_tmp_dir_size_check_time = time.time()
        lock_file_content = cur_lock_file_content
    except tf.errors.NotFoundError:
      # Lock file or temp directory were deleted during check. Continue
      # to check whether download succeeded or we need to start our own
      # download.
      pass
    finally:
      time.sleep(5)
--------------------
Prompt: 
def contains_pts(self, pts):
        """Containment test on arrays."""
----------
Output: 

        return self.test_point_in_polygon(pts, self.polygon)

    def test_point_in_polygon(self, pt):
        """Test if a point is inside a polygon."""
        return self.contains_pts(self.polygon.exterior.coords)

    def test_point_in_polygon_with_holes(self):
        """Test if a point is inside a polygon with holes."""
        return self.contains_pts(self.polygon.exterior.coords)

    def test_point_in_polygon_with_points(self):
        """Test if a
----------
Solution: 

        obj1, obj2 = self.objects
        arg1 = obj2.contains_pts(pts)
        arg2 = np.logical_not(obj1.contains_pts(pts))
        return np.logical_and(arg1, arg2)
--------------------
Prompt: 
def is_holiday(now=None, holidays="/etc/acct/holidays"):
    """is_holiday({now}, {holidays="/etc/acct/holidays"}"""
----------
Output: 

    if now is None:
        now = datetime.now()
    if holidays is None:
        return now.weekday() in [0, 1]
    else:
        return now.weekday() in [1, 2]


def is_holiday_date(now=None, holidays="/etc/acct/holidays"):
    """is_holiday_date({now}, {holidays="/etc/acct/holidays"})"""
    if is_holiday(now=None, holidays="/etc/acct/holidays"):
        return False
    else:
        return True


def is_holiday_
----------
Solution: 

    now = _Time(now)
    # Now, parse holiday file.
    if not os.path.exists(holidays):
        raise Exception("There is no holidays file: %s" % holidays)

    f = open(holidays, "r")
    # First, read all leading comments.
    line = f.readline()
    while line[0] == '*': line = f.readline()

    # We just got the year line.
    (year, primestart, primeend) = str.split(line)
    # If not the right year, we have no idea for certain.  Skip.
    if not year == now.year: return 0

    # Now the dates.  Check each against now.
    while line != '':
        # Of course, ignore comments.
        if line[0] == '*':
            line = f.readline()
            continue

        try:
            # Format: "1/1	New Years Day"
            (month, day) = str.split(str.split(line)[0], "/")
            # The _Time class has leading-zero padded day numbers.
            if len(day) == 1: day = '0' + day
            # Get month number from index map (compensate for zero indexing).
            month = MONTH_MAP[int(month) - 1]

            # Check the date.
            #print month, now.month, day, now.day
            if month == now.month and day == now.day:
                return 1

            line = f.readline()
        except:
            # Skip malformed lines.
            line = f.readline()
            continue

    # If no match found, we must not be in a holiday.
    return 0
--------------------
Prompt: 
def runs(self, path="", filters={}, order="-created_at", per_page=None):
        """Return a set of runs from a project that match the filters provided.
        You can filter by config.*, summary.*, state, username, createdAt, etc.

        The filters use the same query language as MongoDB:

        https://docs.mongodb.com/manual/reference/operator/query

        Order can be created_at, heartbeat_at, config.*.value, or summary.*.  By default
        the order is descending, if you prepend order with a + order becomes ascending.
        """
----------
Output: 

        if per_page is None:
            per_page = 10
        return self._get_project_runs(path, filters, order, path)

    def _get_project_runs(self, path="", filters={}, path_params={}):
        """Return a set of runs from a project that match the filters provided.
        You can filter by config.*, summary.*, state, username, path, filters, path_params,
        and heartbeat.*.

        The filters use the same query language as MongoDB:

        https://docs.mongodb.com/manual/reference/operator/query

        The path params use
----------
Solution: 

        username, project, run = self._parse_path(path)
        if not self._runs.get(path):
            self._runs[path + str(filters) + str(order)] = Runs(self.client, username, project,
                                                                filters=filters, order=order, per_page=per_page)
        return self._runs[path + str(filters) + str(order)]
--------------------
Prompt: 
def to_utc(self, dt):
        """Convert any timestamp to UTC (with tzinfo)."""
----------
Output: 

        if dt.tzinfo is not None:
            return dt.astimezone(self.tzinfo)
        else:
            return dt.replace(tzinfo=pytz.utc)

    def to_local(self, dt):
        """Convert any timestamp to local time (with tzinfo)."""
        if dt.tzinfo is not None:
            return dt.astimezone(self.tzinfo)
        else:
            return pytz.utc.localize(dt)

    def to_utc_with_tzinfo(self, dt):
        """Convert any timestamp to UTC (with tzinfo)."""
        if dt.tzinfo is not None:
            return to_utc(
----------
Solution: 

        if dt.tzinfo is None:
            return dt.replace(tzinfo=self.utc)
        return dt.astimezone(self.utc)
--------------------
Prompt: 
def get_experiment(self, coll_name, exp_name):
        """
        Convenience method that gets experiment resource.
        
        Args:
            coll_name (str): Collection name
            exp_name (str): Experiment name

        Returns:
            (ExperimentResource)
        """
----------
Output: 

        return self.get_resource(exp_name)
    
    def get_collection(self, coll_name):
        """
        Returns a resource for the given experiment name.
        
        Args:
            coll_name (str): Experiment name

        Returns:
            (ExperimentResource)
        """
        return self.get_experiment(coll_name)
    
    def get_resource(self, coll_name):
        """
        Returns a resource for the given experiment name.
        
        Args:
            coll_name (str): Experiment name

        Returns:
            (ExperimentResource)
        """
        return self.get_resource_collection(coll_
----------
Solution: 

        exp = ExperimentResource(exp_name, coll_name)
        return self.get_project(exp)
--------------------
Prompt: 
def add_status_code(code):
    """
    Decorator used for adding exceptions to :class:`SanicException`.
    """
----------
Output: 

    def wrapper(func):
        @wraps(func)
        def wrapper_wrapper(self, *args, **kwargs):
            try:
                return func(self, *args, **kwargs)
            except SanicException as e:
                return SanicException(e.message, e.code, e.code_type, e.message_id, code)
        return wrapper_wrapper
    return wrapper


class SanicException(Exception):
    """
    Base class for all Sanic exceptions.
    """
    pass


class SanicExceptionWithMessage(SanicException):
    """
    Base class for
----------
Solution: 


    def class_decorator(cls):
        cls.status_code = code
        _sanic_exceptions[code] = cls
        return cls

    return class_decorator
--------------------
Prompt: 
def detect_mode(cls, **params):
        """Detect which listing mode of the given params.

        :params kwargs params: the params
        :return: one of the available modes
        :rtype: str
        :raises ValueError: if multiple modes are detected
        """
----------
Output: 

        return params.get('mode', cls.DEFAULT_MODE)

    @classmethod
    def get_all_modes(cls, **params):
        """Get all modes of the given params.

        :params kwargs params: the params
        :return: a list of the available modes
        :rtype: list of str
        :raises ValueError: if multiple modes are detected
        """
        return cls.get_listing_modes(**params)

    @classmethod
    def get_listing_modes(cls, **params):
        """Get a list of modes of the given params.

        :params kwargs params: the params
        :return
----------
Solution: 

        modes = []
        for mode in cls.modes:
            if params.get(mode) is not None:
                modes.append(mode)
        if len(modes) > 1:
            error_message = 'ambiguous mode, must be one of {}'
            modes_csv = ', '.join(list(cls.modes))
            raise ValueError(error_message.format(modes_csv))
        return modes[0] if modes else cls.default_mode
--------------------
Prompt: 
def markPartitionForEvent(self, db_name, tbl_name, part_vals, eventType):
    """
    Parameters:
     - db_name
     - tbl_name
     - part_vals
     - eventType
    """
----------
Output: 

    try:
      self.dbCatalog[db_name]['partitions'].update(part_vals)
      self.dbCatalog[db_name]['partitions'][tbl_name]['events'].update(
          part_vals)
    except KeyError:
      self.dbCatalog[db_name]['partitions'][tbl_name] = {
          'partitions': set(),
          'events': set()}

  def markPartitionForPartition(self, db_name, tbl_name, eventType):
    """
    Parameters:
     - db_name
     - tbl_name
     - part_vals
     - eventType
    """
    try
----------
Solution: 

    self.send_markPartitionForEvent(db_name, tbl_name, part_vals, eventType)
    self.recv_markPartitionForEvent()
--------------------
Prompt: 
def connect(self, host, port):
        """Connect to a host on a given port.
        If the hostname ends with a colon (`:') followed by a number, and
        there is no port specified, that suffix will be stripped off and the
        number interpreted as the port number to use.
        """
----------
Output: 

        if ':' in host:
            host, port = host.split(':')
        if not port and self.default_port:
            self.default_port = int(self.default_port)
        self.host = host
        self.port = int(port)
        self.pid = os.getpid()
        self.name = 'connection from %s:%s' % (self.host, self.port)

    def __enter__(self):
        self.open()
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        self.close()

    def open
----------
Solution: 

        if not port and (host.find(':') == host.rfind(':')):
            i = host.rfind(':')
            if i >= 0:
                host, port = host[:i], host[i+1:]
            try: port = int(port)
            except ValueError:
                raise socket.error, "nonnumeric port"
        #if self.verbose > 0:
        #    print 'connect:', (host, port)
        msg = "getaddrinfo returns an empty list"
        self.sock = None
        for res in socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM):
            af, socktype, proto, canonname, sa = res
            try:
                self.sock = socket.socket(af, socktype, proto)
                #if self.debuglevel > 0: print 'connect:', (host, port)
                self.sock.connect(sa)
            except socket.error, msg:
                #if self.debuglevel > 0: print 'connect fail:', (host, port)
                self.close()
                continue
            break
        if not self.sock:
            raise socket.error, msg
--------------------
Prompt: 
def _elements_to_dict(data, position, obj_end, opts, subdocument=None):
    """Decode a BSON document."""
----------
Output: 

    if subdocument is None:
        subdocument = data

    if subdocument is None:
        return data

    if subdocument is None:
        return data

    if subdocument is None:
        return data

    if subdocument is None:
        return data

    if subdocument is not None:
        return subdocument

    if opts.get('subdocument_as_id', None) is not None:
        return subdocument.as_dict()

    return data


def _parse_timestamp(timestamp):
    """Parse a BSON timestamp."""
    try:
        return datetime.datetime.strptime(timestamp, '%Y
----------
Solution: 

    if type(opts.document_class) == tuple:
        result = opts.document_class[0](**opts.document_class[1]) if not subdocument else dict()
    else:
        result = opts.document_class() if not subdocument else dict()
    end = obj_end - 1
    while position < end:
        (key, value, position) = _element_to_dict(data, position, obj_end, opts)
        result[key] = value
    return result
--------------------
Prompt: 
def from_dict(cls, d):
        """
        Convert a dictionary into an xarray.Dataset.

        Input dict can take several forms::

            d = {'t': {'dims': ('t'), 'data': t},
                 'a': {'dims': ('t'), 'data': x},
                 'b': {'dims': ('t'), 'data': y}}

            d = {'coords': {'t': {'dims': 't', 'data': t,
                                  'attrs': {'units':'s'}}},
                 'attrs': {'title': 'air temperature'},
                 'dims': 't',
                 'data_vars': {'a': {'dims': 't', 'data': x, },
                               'b': {'dims': 't', 'data': y}}}

        where 't' is the name of the dimesion, 'a' and 'b' are names of data
        variables and t, x, and y are lists, numpy.arrays or pandas objects.

        Parameters
        ----------
        d : dict, with a minimum structure of {'var_0': {'dims': [..], \
                                                         'data': [..]}, \
                                               ...}

        Returns
        -------
        obj : xarray.Dataset

        See also
        --------
        Dataset.to_dict
        DataArray.from_dict
        """
----------
Output: 
': {'t': {'dims': 't', 'data': t,
                                'attrs': {'units':'K'}}},
                 'dims': 't',
                 'data': {'t': {'dims': 't', 'data': t,
                                'attrs': {'units':'K'}}},
                 'attrs': {'title': 'air temperature'},
                 'title': 'air temperature'}

        """
        if isinstance(d, dict):
            return cls(d)
        else:
            return cls(np.asarray(d))

    def to_dict(self):
        """
        Convert a dictionary into an xarray.Dataset
----------
Solution: 


        if not set(['coords', 'data_vars']).issubset(set(d)):
            variables = d.items()
        else:
            import itertools
            variables = itertools.chain(d.get('coords', {}).items(),
                                        d.get('data_vars', {}).items())
        try:
            variable_dict = OrderedDict([(k, (v['dims'],
                                              v['data'],
                                              v.get('attrs'))) for
                                         k, v in variables])
        except KeyError as e:
            raise ValueError(
                "cannot convert dict without the key "
                "'{dims_data}'".format(dims_data=str(e.args[0])))
        obj = cls(variable_dict)

        # what if coords aren't dims?
        coords = set(d.get('coords', {})) - set(d.get('dims', {}))
        obj = obj.set_coords(coords)

        obj.attrs.update(d.get('attrs', {}))

        return obj
--------------------
Prompt: 
def evaluate(self, truth, predicted):
        """
        Evaluates the predicted outputs against the gold data.
        """
----------
Output: 

        if self.verbose:
            print("  %s: %s" % (self.name, predicted))
        if truth == predicted:
            print("  OK")
        else:
            print("  FAIL")
            print("  %s: %s" % (self.name, truth))
            print("  %s: %s" % (self.name, predicted))

        # Check if the gold data is valid
        if not self.validate(predicted):
            return False

        # Check if the gold data is correct
        if not self.compare(self.gold_data, predicted):
----------
Solution: 

        correct = 0.0
        total = 0.0
        for i in range(len(truth)):
            if(truth[i] == predicted[i]):
                correct += 1
            total += 1
        return 1.0*correct/total
--------------------
Prompt: 
def date_range(cls,start_time,end_time,freq):
        """
        Returns a new SArray that represents a fixed frequency datetime index.

        Parameters
        ----------
        start_time : datetime.datetime
          Left bound for generating dates.

        end_time : datetime.datetime
          Right bound for generating dates.

        freq : datetime.timedelta
          Fixed frequency between two consecutive data points.

        Returns
        -------
        out : SArray

        Examples
        --------
        >>> import datetime as dt
        >>> start = dt.datetime(2013, 5, 7, 10, 4, 10)
        >>> end = dt.datetime(2013, 5, 10, 10, 4, 10)
        >>> sa = tc.SArray.date_range(start,end,dt.timedelta(1))
        >>> print sa
        dtype: datetime
        Rows: 4
        [datetime.datetime(2013, 5, 7, 10, 4, 10),
         datetime.datetime(2013, 5, 8, 10, 4, 10),
         datetime.datetime(2013, 5, 9, 10, 4, 10),
         datetime.datetime(2013, 5, 10, 10, 4, 10)]
       """
----------
Output: 
, 5, 7, 10, 5, 10)
        >>> freq = dt.date_range(start, end,freq)
        >>> out = dt.scatter(dt.datetime(2013, 5, 7, 10, 4, 10), dt.datetime(2013, 5, 7, 10, 5, 10), dt.datetime(2013, 5, 7, 10, 4, 10))
        """
        return cls(start_time, end, freq)

    @classmethod
    def date_interval(cls,start_time,end_time):
        """
        Returns a new SArray that represents a fixed frequency
----------
Solution: 


        if not isinstance(start_time,datetime.datetime):
            raise TypeError("The ``start_time`` argument must be from type datetime.datetime.")

        if not isinstance(end_time,datetime.datetime):
            raise TypeError("The ``end_time`` argument must be from type datetime.datetime.")

        if not isinstance(freq,datetime.timedelta):
            raise TypeError("The ``freq`` argument must be from type datetime.timedelta.")

        from .. import extensions
        return extensions.date_range(start_time,end_time,freq.total_seconds())
--------------------
Prompt: 
def list(self, path=None, with_metadata=False, include_partitions=False):
        """get a list of all of the files in the repository"""
----------
Output: 

        if path is None:
            path = self.path

        if path is None:
            path = self.path / "**"

        if path.is_dir():
            return [path / f for f in self.ls(path.name, with_metadata=with_metadata, include_partitions=include_partitions)]
        else:
            return [path / f for f in self.ls(path.name, with_metadata=with_metadata, include_partitions=include_partitions)]

    def ls(self, path=None, with_metadata=False):
        """get a list of all
----------
Solution: 

        return self.upstream.list(
            path,
            with_metadata=with_metadata,
            include_partitions=include_partitions)
--------------------
Prompt: 
def _greater_or_close(a, value, **kwargs):
    r"""Compare values for greater or close to boolean masks.

    Returns a boolean mask for values greater than or equal to a target within a specified
    absolute or relative tolerance (as in :func:`numpy.isclose`).

    Parameters
    ----------
    a : array-like
        Array of values to be compared
    value : float
        Comparison value

    Returns
    -------
    array-like
        Boolean array where values are greater than or nearly equal to value.

    """
----------
Output: 

    return np.less_equal(a, value, **kwargs)


def _less_or_equal(a, b, **kwargs):
    r"""Compare values for less or equal to boolean masks.

    Returns a boolean mask for values less than or equal to a target within a specified
    absolute or relative tolerance (as in :func:`numpy.isclose`).

    Parameters
    ----------
    a : array-like
        Array of values to be compared
    b : array-like
        Array of boolean values

    """
    return np.equal(a, b, **kwargs)


def _less_or_
----------
Solution: 

    return (a > value) | np.isclose(a, value, **kwargs)
--------------------
Prompt: 
def psycopg2_wait_callback(conn):
    """A wait callback to allow greenlet to work with Psycopg.
    The caller must be from a greenlet other than the main one.

    :param conn: psycopg2 connection or file number

    This function must be invoked from a coroutine with parent, therefore
    invoking it from the main greenlet will raise an exception.
    """
----------
Output: 

    if not isinstance(conn, int):
        raise Exception("conn must be an int")
    while True:
        try:
            yield conn.wait()
        except psycopg2.OperationalError:
            if conn == 0:
                raise StopIteration
            else:
                raise
        except StopIteration:
            break

----------
Solution: 

    while True:
        state = conn.poll()
        if state == extensions.POLL_OK:
            # Done with waiting
            break
        elif state == extensions.POLL_READ:
            _wait_fd(conn)
        elif state == extensions.POLL_WRITE:
            _wait_fd(conn, read=False)
        else:  # pragma    nocover
            raise OperationalError("Bad result from poll: %r" % state)
--------------------
Prompt: 
def bbox(self):
        """
        Minimal bounding box that contains all the visible layers.

        Use :py:attr:`~psd_tools.api.psd_image.PSDImage.viewbox` to get
        viewport bounding box. When the psd is empty, bbox is equal to the
        canvas bounding box.

        :return: (left, top, right, bottom) `tuple`.
        """
----------
Output: 

        return (self.viewbox.bbox()[0], self.viewbox.bbox()[1],
                self.viewbox.bbox()[2], self.viewbox.bbox()[3])

    def get_image_data(self, image):
        """
        Get the data from a PIL Image object.

        :param image: (width, height, 3) `PIL Image` object.
        :return: (width, height, 3) `tuple`.
        """
        return (image.size[0], image.size[1], 3)

    def get_image_data_size(self,
----------
Solution: 

        bbox = super(PSDImage, self).bbox
        if bbox == (0, 0, 0, 0):
            bbox = self.viewbox
        return bbox
--------------------
Prompt: 
def main() -> int:
    """
    Utility to create and publish the Docker cache to Docker Hub
    :return:
    """
----------
Output: 

    parser = argparse.ArgumentParser(description='Cache a Docker image')
    parser.add_argument('--cache-dir', type=str, default=None, help='Directory to cache images')
    parser.add_argument('--docker-host', type=str, default=None, help='Hostname of the Docker Hub')
    parser.add_argument('--docker-port', type=int, default=None, help='Port of the Docker Hub')
    parser.add_argument('--docker-user', type=str, default=None, help='Username of the Docker Hub')
    parser.add
----------
Solution: 

    # We need to be in the same directory than the script so the commands in the dockerfiles work as
    # expected. But the script can be invoked from a different path
    base = os.path.split(os.path.realpath(__file__))[0]
    os.chdir(base)

    logging.getLogger().setLevel(logging.DEBUG)
    logging.getLogger('botocore').setLevel(logging.INFO)
    logging.getLogger('boto3').setLevel(logging.INFO)
    logging.getLogger('urllib3').setLevel(logging.INFO)
    logging.getLogger('s3transfer').setLevel(logging.INFO)

    def script_name() -> str:
        return os.path.split(sys.argv[0])[1]

    logging.basicConfig(format='{}: %(asctime)-15s %(message)s'.format(script_name()))

    parser = argparse.ArgumentParser(description="Utility for preserving and loading Docker cache", epilog="")
    parser.add_argument("--docker-registry",
                        help="Docker hub registry name",
                        type=str,
                        required=True)

    args = parser.parse_args()

    platforms = build_util.get_platforms()
    try:
        _login_dockerhub()
        return build_save_containers(platforms=platforms, registry=args.docker_registry, load_cache=True)
    finally:
        _logout_dockerhub()
--------------------
Prompt: 
def query_pager_by_user(userid, current_page_num=1):
        """
        Query pager
        """
----------
Output: 

        if not current_page_num:
            current_page_num = 1
        pager = Paginator(
            self.object_list,
            per_page=current_page_num,
            orphans=1,
            allow_empty_first_page=True
        )
        return pager.page(userid, current_page_num)

    def get_page(self, page_num):
        """
        Get page
        """
        return Paginator(
            self.object_list,
            page_num,
            orphans=1
        )

    def get_page_range(self,
----------
Solution: 

        return TabLog.select().where(TabLog.user_id == userid).order_by(
            TabLog.time_create.desc()
        ).paginate(
            current_page_num, CMS_CFG['list_num']
        )
--------------------
Prompt: 
def get_random_id(length):
    """Generate a random, alpha-numerical id."""
----------
Output: 

    return ''.join(random.choice(string.ascii_lowercase) for _ in range(length))


def get_random_string(length):
    """Generate a random, alpha-numerical string."""
    return ''.join(random.choice(string.ascii_lowercase) for _ in range(length))


def get_random_alpha(length):
    """Generate a random, alpha-numerical alpha."""
    return ''.join(random.choice(string.ascii_lowercase) for _ in range(length))


def get_random_alphanumeric(length):
    """Generate a random
----------
Solution: 

    alphabet = string.ascii_uppercase + string.ascii_lowercase + string.digits
    return ''.join(random.choice(alphabet) for _ in range(length))
--------------------
Prompt: 
def translate(self, package, into=None):
    """From a SourcePackage, translate to a binary distribution."""
----------
Output: 

    if into is None:
      into = self.distribution.name
    if package.version:
      version = package.version
    else:
      version = ''
    return '%s-%s-%s' % (package.name, version, package.version)

  def __init__(self, package, version, revision, patches, options):
    self.package = package
    self.version = version
    self.revision = revision
    self.patches = patches
    self.options = options
    self.deps_file = os.path.join(package.deps_file, package.name + '.deps')
   
----------
Solution: 

    if not isinstance(package, SourcePackage):
      return None
    if not package.local:
      raise ValueError('SourceTranslator cannot translate remote packages.')

    installer = None
    version = self._interpreter.version
    unpack_path = Archiver.unpack(package.local_path)
    into = into or safe_mkdtemp()

    try:
      if self._use_2to3 and version >= (3,):
        with TRACER.timed('Translating 2->3 %s' % package.name):
          self.run_2to3(unpack_path)
      installer = self._installer_impl(unpack_path, interpreter=self._interpreter)
      with TRACER.timed('Packaging %s' % package.name):
        try:
          dist_path = installer.bdist()
        except self._installer_impl.InstallFailure as e:
          TRACER.log('Failed to install package at %s: %s' % (unpack_path, e))
          return None
        target_path = os.path.join(into, os.path.basename(dist_path))
        safe_copy(dist_path, target_path)
        target_package = Package.from_href(target_path)
        if not target_package:
          TRACER.log('Target path %s does not look like a Package.' % target_path)
          return None
        if not target_package.compatible(self._supported_tags):
          TRACER.log('Target package %s is not compatible with %s' % (
              target_package, self._supported_tags))
          return None
        return DistributionHelper.distribution_from_path(target_path)
    except Exception as e:
      TRACER.log('Failed to translate %s' % package)
      TRACER.log(traceback.format_exc())
    finally:
      if installer:
        installer.cleanup()
      if unpack_path:
        safe_rmtree(unpack_path)
--------------------
Prompt: 
def find_user_file(self, option_name, filename_list):
        """! @brief Search the project directory for a file."""
----------
Output: 

        if not filename_list:
            return None
        if not os.path.isdir(filename_list):
            return None
        for filename in filename_list:
            if os.path.isfile(filename):
                return filename
        return None

    def find_project_file_by_name(self, option_name, filename_list):
        """! @brief Search the project directory for a file by name."""
        if not filename_list:
            return None
        if not os.path.isdir(filename_list):
            return None
        for filename in filename_list:
            if os.path.isfile
----------
Solution: 

        if option_name is not None:
            filePath = self._options.get(option_name, None)
        else:
            filePath = None
        
        # Look for default filenames if a path wasn't provided.
        if filePath is None:
            for filename in filename_list:
                thisPath = os.path.join(self.project_dir, filename)
                if os.path.isfile(thisPath):
                    filePath = thisPath
                    break
        # Use the path passed in options, which may be absolute, relative to the
        # home directory, or relative to the project directory.
        else:
            filePath = os.path.expanduser(filePath)
            if not os.path.isabs(filePath):
                filePath = os.path.join(self.project_dir, filePath)
        
        return filePath
--------------------
Prompt: 
def terminate(self):
        """Terminate running processes."""
----------
Output: 

        self.logger.info("Terminating processes")
        self.logger.debug("Terminating processes: %s", self.processes)
        self.processes = []
        for p in self.processes:
            p.terminate()
        self.logger.info("Terminating processes")
        self.logger.debug("Terminating processes: %s", self.processes)

    def _get_process_list(self):
        """Return a list of processes."""
        return self.processes

    def _get_process_list_lock(self):
        """Lock process list."""
        return self.processes_lock

   
----------
Solution: 

        if self._process.state() == QProcess.Running:
            try:
                self._process.terminate()
            except Exception:
                pass
        self._fired = True
--------------------
Prompt: 
def dock_json_has_plugin_conf(self, plugin_type, plugin_name):
        """
        Check whether a plugin is configured.
        """
----------
Output: 

        return self.get_plugin_conf(plugin_type).get(plugin_name)

    def get_plugin_conf(self, plugin_type, plugin_name):
        """
        Get the configuration for a given plugin.
        """
        if plugin_type not in self.plugins_by_type:
            return {}
        return self.plugins_by_type[plugin_type].get(plugin_name)

    def get_plugin_names(self):
        """
        Get the names of all the plugins.
        """
        return self.plugins.keys()

    def get_plugin_names_
----------
Solution: 


        try:
            self.dock_json_get_plugin_conf(plugin_type, plugin_name)
            return True
        except (KeyError, IndexError):
            return False
--------------------
Prompt: 
def dcounts(self):
        """
        :return: a data frame with names and distinct counts and fractions for all columns in the database
        """
----------
Output: 

        return pd.DataFrame(self.data, index=self.index, columns=self.fractions)

    def __repr__(self):
        return "Dcounts(data=%s, index=%s, fractions=%s)" % (self.data, self.index, self.fractions)


class Dcounts(object):
    """
    A class to store the data frame of a database.

    :param name: the name of the database
    :param db: the database
    :param name_type: the type of the database
    :param name_type_desc: the description of the
----------
Solution: 

        print("WARNING: Distinct value count for all tables can take a long time...", file=sys.stderr)
        sys.stderr.flush()

        data = []
        for t in self.tables():
            for c in t.columns():
                data.append([t.name(), c.name(), c.dcount(), t.size(), c.dcount() / float(t.size())])
        df = pd.DataFrame(data, columns=["table", "column", "distinct", "size", "fraction"])
        return df
--------------------
Prompt: 
def compare_recursive_mtime(path, cutoff, newest=True):
    """Compare the newest/oldest mtime for all files in a directory.
    Cutoff should be another mtime to be compared against. If an mtime that is
    newer/older than the cutoff is found it will return True.
    E.g. if newest=True, and a file in path is newer than the cutoff, it will
    return True.
    """
----------
Output: 

    if path in path_map:
        if newest:
            return path_map[path] < cutoff
        else:
            return path_map[path] > cutoff
    else:
        if newest:
            return path_map[path] < newest
        else:
            return path_map[path] < path_map[path_map.keys()[0]]


def get_file_mtime(path):
    """Return the newest/oldest mtime for a file in the given directory.
    """
    return get_file_map(path, file_mtime_map)


def get_file
----------
Solution: 

    if os.path.isfile(path):
        mt = mtime(path)
        if newest:
            if mt > cutoff:
                return True
        elif mt < cutoff:
            return True
    for dirname, _, filenames in os.walk(path, topdown=False):
        for filename in filenames:
            mt = mtime(os.path.join(dirname, filename))
            if newest:  # Put outside of loop?
                if mt > cutoff:
                    return True
            elif mt < cutoff:
                return True
    return False
--------------------
Prompt: 
def is_numeric_dtype(dtype):
    """Return ``True`` if ``dtype`` is a numeric type."""
----------
Output: 

    return _is_dtype_type(dtype) and not is_numeric_dtype(dtype).all()


def is_float_dtype(dtype):
    """Return ``True`` if ``dtype`` is a float type."""
    return _is_dtype_type(dtype) and not is_float_dtype(dtype).all()


def is_complex_dtype(dtype):
    """Return ``True`` if ``dtype`` is a complex type."""
    return _is_dtype_type(dtype) and not is_complex_dtype(dtype).all()


def is_complex_
----------
Solution: 

    dtype = np.dtype(dtype)
    return np.issubsctype(getattr(dtype, 'base', None), np.number)
--------------------
Prompt: 
def get_facility_status(self, facility):
        """Get the current status of a Guest Additions facility.

        in facility of type :class:`AdditionsFacilityType`
            Facility to check status for.

        out timestamp of type int
            Timestamp (in ms) of last status update seen by the host.

        return status of type :class:`AdditionsFacilityStatus`
            The current (latest) facility status.

        """
----------
Output: 

        return self.get_host_facility_status(facility).get_last_status()

    def get_host_facility_status(self, host):
        """Get the current (latest) status of a host's Additions.

        :param host: Host's Additions.

        out timestamp of type int
            Timestamp (in ms) of last status update seen by the host.

        return status of type :class:`AdditionsHostFacilityStatus`
            The current (latest) host's Additions status.

        """
        return self.get_host_facility_status(host).get_status()
----------
Solution: 

        if not isinstance(facility, AdditionsFacilityType):
            raise TypeError("facility can only be an instance of type AdditionsFacilityType")
        (status, timestamp) = self._call("getFacilityStatus",
                     in_p=[facility])
        status = AdditionsFacilityStatus(status)
        return (status, timestamp)
--------------------
Prompt: 
def disassemble_string(self, lpAddress, code):
        """
        Disassemble instructions from a block of binary code.

        @type  lpAddress: int
        @param lpAddress: Memory address where the code was read from.

        @type  code: str
        @param code: Binary code to disassemble.

        @rtype:  list of tuple( long, int, str, str )
        @return: List of tuples. Each tuple represents an assembly instruction
            and contains:
             - Memory address of instruction.
             - Size of instruction in bytes.
             - Disassembly line of instruction.
             - Hexadecimal dump of instruction.
        """
----------
Output: 
adecimal disassembly line of instruction.
             - Hexadecimal instruction line of instruction.
             - Hexadecimal instruction line of instruction.
             - Hexadecimal instruction line of instruction.
             - Address of instruction.
             - Disassembly line of instruction.
        """
        return self._disassemble_string(lpAddress, code, disassemble_string_length=16)

    def _disassemble_string(self, lpAddress, code, disassemble_length=16):
        """
        Disassemble instructions from a block of binary code.

        @type  lpAddress: int
        @param lpAddress: Memory address where the code was
----------
Solution: 

        aProcess = self.get_process()
        return aProcess.disassemble_string(lpAddress, code)
--------------------
Prompt: 
def create_database(self, name, owner=None):
        """
        Create a new MapD database

        Parameters
        ----------
        name : string
          Database name
        """
----------
Output: 

        if owner is None:
            owner = self.user
        if not self.exists:
            raise ValueError("Database %s does not exist" % name)
        if not self.is_open:
            raise ValueError("Database %s is already opened" % name)
        if not self.is_closed:
            raise ValueError("Database %s is closed" % name)
        if not self.is_open:
            raise ValueError("Database %s is already opened" % name)
        if not self.is_closed:
            raise ValueError("Database %s is already closed" % name)
        if self.
----------
Solution: 

        statement = ddl.CreateDatabase(name, owner=owner)
        self._execute(statement)
--------------------
Prompt: 
def get_buckets(self, bucket_type=None, timeout=None):
        """
        Serialize bucket listing request and deserialize response
        """
----------
Output: 

        if bucket_type is None:
            bucket_type = self.bucket_type
        if not isinstance(bucket_type, (list, tuple)):
            raise ValueError("bucket_type should be list or tuple")
        if len(bucket_type) == 0:
            bucket_type = ["all"]
        if not isinstance(timeout, (int, float)):
            raise ValueError("timeout should be int or float")
        if not isinstance(self.timeout, (int, float)):
            raise ValueError("timeout should be int or float")
        if not isinstance(self.wait_time, (int, float)):
----------
Solution: 

        msg_code = riak.pb.messages.MSG_CODE_LIST_BUCKETS_REQ
        codec = self._get_codec(msg_code)
        msg = codec.encode_get_buckets(bucket_type,
                                       timeout, streaming=False)
        resp_code, resp = self._request(msg, codec)
        return resp.buckets
--------------------
Prompt: 
def doDup(self, WHAT={}, **params):
		"""This function will perform the command -dup."""
----------
Output: 

		if self.is_admin:
			return self.admin_do( WHAT, params )
		else:
			return self.doDup_1( WHAT, **params )

	def doDup_1( self, WHAT={}, **params ):
		"""This function will perform the command -dup."""
		if self.is_admin:
			return self.admin_do( WHAT, params )
		else:
			return self.doDup_2( WHAT, **params )

	def doDup_2( self, WHAT={}, **params ):
----------
Solution: 


		if hasattr(WHAT, '_modified'):
			for key, value in WHAT._modified():
				if WHAT.__new2old__.has_key(key):
					self._addDBParam(WHAT.__new2old__[key].encode('utf-8'), value)
				else:
					self._addDBParam(key, value)
			self._addDBParam('RECORDID', WHAT.RECORDID)
			self._addDBParam('MODID', WHAT.MODID)
		elif type(WHAT) == dict:
			for key in WHAT:
				self._addDBParam(key, WHAT[key])
		else:
			raise FMError, 'Python Runtime: Object type (%s) given to function doDup as argument WHAT cannot be used.' % type(WHAT)

		if self._layout == '':
			raise FMError, 'No layout was selected'

		for key in params:
			self._addDBParam(key, params[key])

		if self._checkRecordID() == 0:
			raise FMError, 'RecordID is missing'

		return self._doAction('-dup')
--------------------
Prompt: 
def load_system_host_keys(self, filename=None):
        """
        Load host keys from a system (read-only) file.  Host keys read with
        this method will not be saved back by `save_host_keys`.

        This method can be called multiple times.  Each new set of host keys
        will be merged with the existing set (new replacing old if there are
        conflicts).

        If ``filename`` is left as ``None``, an attempt will be made to read
        keys from the user's local "known hosts" file, as used by OpenSSH,
        and no exception will be raised if the file can't be read.  This is
        probably only useful on posix.

        :param str filename: the filename to read, or ``None``

        :raises: ``IOError`` --
            if a filename was provided and the file could not be read
        """
----------
Output: 
 be raised if any of the host keys could not be
        read.
        """
        self._known_hosts = {}
        if filename is not None:
            self.load_host_keys(filename)

    def load_system_host_keys_file(self, filename):
        """
        Load host keys from a system (read-only) file.

        If the file exists and is readable, parsing will stop here.  Otherwise,
        parsing will try to read the keys from the user's local "known hosts"
        file, as a set of "known hosts" keys.  Host keys read with this method
----------
Solution: 

        if filename is None:
            # try the user's .ssh key file, and mask exceptions
            filename = os.path.expanduser("~/.ssh/known_hosts")
            try:
                self._system_host_keys.load(filename)
            except IOError:
                pass
            return
        self._system_host_keys.load(filename)
--------------------
Prompt: 
def next_url(request):
    """
    Returns URL to redirect to from the ``next`` param in the request.
    """
----------
Output: 

    next = request.GET.get('next', None)
    if not next:
        next = request.META.get('HTTP_REFERER', None)
    if not next:
        next = request.META.get('HTTP_REFERER', None)
    return next


def login_required(view_func):
    """
    Decorator for views that checks that the user is logged in, redirecting
    to the log-in page if necessary.
    """
    def _checklogin(request, *args, **kwargs):
        if request.user.is_authenticated():
            return view_func(
----------
Solution: 

    next = request.GET.get("next", request.POST.get("next", ""))
    host = request.get_host()
    return next if next and is_safe_url(next, host=host) else None
--------------------
Prompt: 
def _urlopen_as_json(self, url, headers=None):
        """Shorcut for return contents as json"""
----------
Output: 

        try:
            return json.loads(self.opener.open(url, headers=headers).read())
        except (IOError, ValueError):
            return None

    def _request(self, url, method, headers=None, data=None):
        """Shorcut for return contents as http request"""
        if not self.session:
            self.session = requests.Session()
        if not data:
            data = self._urlopen_as_json(url, headers)
        if not method:
            method = 'GET'
        if not data.get('data'):
            data['data'] = {}
       
----------
Solution: 

        req = Request(url, headers=headers)
        return json.loads(urlopen(req).read())
--------------------
Prompt: 
def to_json_format(conf):
    """Convert fields of a python dictionary to be dumped in json format"""
----------
Output: 

    return json.dumps(conf, indent=4, sort_keys=True)


def to_json(conf):
    """Convert fields of a python dictionary to be dumped in json format"""
    return to_json_format(conf)


def to_yaml(conf):
    """Convert fields of a YAML string to be dumped in yaml format"""
    return yaml.safe_dump(conf, indent=4, default_flow_style=False)


def to_yaml_file(conf):
    """Convert a python dictionary to a YAML file"""
    return to_yaml(conf)
----------
Solution: 

    if 'fmode' in conf:
        conf['fmode'] = oct(conf['fmode'])[-3:]
    if 'dmode' in conf:
        conf['dmode'] = oct(conf['dmode'])[-3:]
--------------------
Prompt: 
def load_file(self, file_obj, verbose):
        """
        The type of open file objects such as sys.stdout; alias of the built-in file.
        @TODO: when is this used? 
        """
----------
Output: 

        self.file_obj = file_obj
        self.verbose = verbose
        self.file_obj.seek(0)
        self.file_obj.truncate()
        self.file_obj.seek(0)
        self.file_obj.seek(0)
        self.file_obj.seek(0)
        self.file_obj.seek(0)
        self.file_obj.seek(0)
        self.file_obj.seek(0)
        self.file_obj.seek(0)
        self.file_obj.seek(0)
        self.file
----------
Solution: 

        if verbose: printDebug("----------")
        if verbose: printDebug("Reading: <%s> ...'" % file_obj.name)

        if type(file_obj) == file:
            self.rdfgraph = self.rdfgraph + file_obj
            self.sources_valid += [file_obj.NAME]
        else:
            self.loading_failed(self.rdf_format_opts)
            self.sources_invalid += [file_obj.NAME]
--------------------
Prompt: 
def metadata(self, filename):
        """
        Get some metadata for a given file.

        Can vary from a backend to another but some are always present:
        - `filename`: the base filename (without the path/prefix)
        - `url`: the file public URL
        - `checksum`: a checksum expressed in the form `algo:hash`
        - 'mime': the mime type
        - `modified`: the last modification date
        """
----------
Output: 

        return self._get_metadata(filename)

    def _get_metadata(self, filename):
        """
        Get some metadata for a given file.

        Can vary from a backend to another but some are always present:
        - `filename`: the base filename (without the path/prefix)
        - `url`: the file public URL
        - `checksum`: a checksum expressed in the form `algo:hash`
        -'mime': the mime type
        - `modified`: the last modification date
        """
        if filename.startswith(self.url):
            return filename[len(self.url):]
        else:
----------
Solution: 

        metadata = self.backend.metadata(filename)
        metadata['filename'] = os.path.basename(filename)
        metadata['url'] = self.url(filename, external=True)
        return metadata
--------------------
Prompt: 
def process_result_value(self, value, dialect):
        """Convert a JSON encoded string to a dictionary structure."""
----------
Output: 

        if value is None:
            return None
        return json.loads(value)

    def process_literal_param(self, value, dialect):
        """Convert a JSON encoded string to a Python datetime."""
        if value is None:
            return None
        return datetime.strptime(value, "%Y-%m-%dT%H:%M:%S.%fZ")

    def process_literal_date(self, value, dialect):
        """Convert a Python datetime to a Python date."""
        if value is None:
            return None
        return value.strftime("%Y-%m-%d")

    def process_literal_
----------
Solution: 

        if value is not None:
            with BytesIO(value) as stream:
                with GzipFile(fileobj=stream, mode="rb") as file_handle:
                    value = json.loads(file_handle.read().decode("utf-8"))
        return value
--------------------
Prompt: 
def get_full_name(self, role, name):
        """
        If ``name`` is already the full name of an object, return
        ``name``.  Otherwise, if ``name`` is a partial object name,
        look up the full name and return it.
        """
----------
Output: 

        if isinstance(role, ParameterizedThing):
            return role.name
        else:
            return self.get_name(role, name)

    def get_name(self, thing):
        """
        If ``thing`` is a string, return it as-is.  Otherwise,
        return ``thing``.
        """
        if isinstance(thing, str):
            return thing
        else:
            return self.get_full_name(thing, self.get_name_for_role(role))

    def get_name_for_role(self, role):
        """
        If ``role`` is
----------
Solution: 


        # An initial '.' indicates a partial name
        if name[0] == '.':
            # Find matches for the partial name in the string
            # containing all full names for this role
            ptrn = r'(?<= )[^,]*' + name + r'(?=,)'
            ml = re.findall(ptrn, self.rolnam[role])
            # Handle cases depending on the number of returned matches,
            # raising an error if exactly one match is not found
            if len(ml) == 0:
                raise KeyError('name matching %s not found' % name,
                               'name', len(ml))
            elif len(ml) > 1:
                raise KeyError('multiple names matching %s found' % name,
                               'name', len(ml))
            else:
                return ml[0]
        else:
            # The absence of an initial '.' indicates a full
            # name. Return the name if it is present in the inventory,
            # otherwise raise an error
            try:
                dom = IntersphinxInventory.roledomain[role]
            except KeyError:
                raise KeyError('role %s not found' % role, 'role', 0)
            if name in self.inv[dom]:
                return name
            else:
                raise KeyError('name %s not found' % name, 'name', 0)
--------------------
Prompt: 
def saveSession(self, filepath):
        """Save androguard session."""
----------
Output: 

        self.save(filepath)
        self.saveAs(filepath)

    def save(self, filepath):
        """Save session to file."""
        self.saveAs(filepath)
        self.saveAs(filepath)

    def saveAs(self, filepath):
        """Save session to file."""
        self.save(filepath)
        self.saveAsBinary(filepath)

    def saveAsBinary(self, filepath):
        """Save session to file."""
        self.save(filepath, binary=True)

    def saveAsBinaryBinary(self, filepath):
        """Save session to file."""
        self.save
----------
Solution: 

        try:
            session_module.Save(self.session, filepath)
        except RuntimeError as e:
            log.exception(e)
            os.remove(filepath)
            log.warning("Session not saved")
--------------------
Prompt: 
def ensure_list(value: Union[T, Sequence[T]]) -> Sequence[T]:
    """Wrap value in list if it is not one."""
----------
Output: 

    if isinstance(value, list):
        return value
    return [value]


def ensure_list_item(
    value: Union[T, Sequence[T]],
    *,
    index: int = 0,
    data: Any = None,
    description: str = "",
    options: Optional[Dict[str, Any]] = None,
) -> Union[T, Sequence[T]]:
    """Wrap value in list_item if it is not one."""
    if isinstance(value, list_item):
        return value
    return [value, index]


def ensure_list_dict(
    value
----------
Solution: 

    if value is None:
        return []
    return value if isinstance(value, list) else [value]
--------------------
Prompt: 
def get_parser(parser=None):
    """Get parser for mpu."""
----------
Output: 

    if not parser:
        parser = argparse.ArgumentParser(
            description='Test the MPU-based MXNet model on MXNet-100k data')
    parser.add_argument(
        '--use_fp16',
        action='store_true',
        help='Whether to use fp16 instead of fp32 for training.')
    parser.add_argument(
        '--use_fp32',
        action='store_false',
        help='Whether to use fp32 instead of fp16 for inference.')
    return parser


def main(_):
    """Test the MXNet model on MXNet-
----------
Solution: 

    from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
    if parser is None:
        parser = ArgumentParser(description=__doc__,
                                formatter_class=ArgumentDefaultsHelpFormatter)
    subparsers = parser.add_subparsers()
    pkg_init_parser = subparsers.add_parser('init')
    pkg_init_parser.add_argument("root",
                                 nargs='?',
                                 help="project root - should be empty")
    pkg_init_parser.set_defaults(func=run_init)
    return parser
--------------------
Prompt: 
def symlink(target, link, target_is_directory=False):
	"""
	An implementation of os.symlink for Windows (Vista and greater)
	"""
----------
Output: 

	if target_is_directory:
		target = os.path.join(os.path.dirname(target), link.lstrip(os.sep))
	else:
		target = os.path.join(os.path.dirname(target), link)
	if not os.path.islink(target):
		os.symlink(target, target_is_directory)
	else:
		os.symlink(target, target_is_directory)

def copy_win32_files(src, dst, symlinks=False):
	"""
	Copies the
----------
Solution: 

	target_is_directory = (
		target_is_directory or
		_is_target_a_directory(link, target)
	)
	# normalize the target (MS symlinks don't respect forward slashes)
	target = os.path.normpath(target)
	handle_nonzero_success(
		api.CreateSymbolicLink(link, target, target_is_directory))
--------------------
Prompt: 
def confirm_lock(lockfile):
    """
    Confirm that the given lockfile contains our pid.
    Should be entirely unecessary, but paranoia always served me well.
    """
----------
Output: 

    try:
        os.kill(lockfile, 0)
    except OSError:
        pass
    else:
        raise AssertionError("Expected to fail")


@attr(shard=2)
@ddt.ddt
class TestLock(TestCase):
    """
    Tests for locking and unlocking an item
    """
    def setUp(self):
        super(TestLock, self).setUp()
        self.course = CourseFactory.create()
        self.chapter = ItemFactory.create(category='chapter', parent_location=self.course.location)  # pylint: disable=no-member
        self.sequential = ItemFactory.create
----------
Solution: 

    pidfile = open(lockfile, "r")
    pidfile_pid = pidfile.readline().strip()
    pidfile.close()
    if int(pidfile_pid) != os.getpid():
        raise RuntimeError, ("pidfile %s contains pid %s; expected pid %s!" %
                             (lockfile, os.getpid(), pidfile_pid))
    return True
--------------------
Prompt: 
def calc_individual_chi_squares(residuals,
                                long_probabilities,
                                rows_to_obs):
    """
    Calculates individual chi-squared values for each choice situation in the
    dataset.

    Parameters
    ----------
    residuals : 1D ndarray.
        The choice vector minus the predicted probability of each alternative
        for each observation.
    long_probabilities : 1D ndarray.
        The probability of each alternative being chosen in each choice
        situation.
    rows_to_obs : 2D scipy sparse array.
        Should map each row of the long format dataferame to the unique
        observations in the dataset.

    Returns
    -------
    ind_chi_squareds : 1D ndarray.
        Will have as many elements as there are columns in `rows_to_obs`. Each
        element will contain the pearson chi-squared value for the given choice
        situation.
    """
----------
Output: 
 Returns
    -------
    chi_squares : 1D ndarray.
        The individual chi-squared values for each row of the long format
        dataferame.

    """
    # TODO: add a test for the following
    # TODO: add a test for the following
    # TODO: add a test for the following
    # TODO: add a test for the following
    # TODO: add a test for the following
    # TODO: add a test for the following
    # TODO: add a test for the following
    # TODO: add a test for the following
    # TODO: add a test for the following
    # TODO: add
----------
Solution: 

    chi_squared_terms = np.square(residuals) / long_probabilities
    return rows_to_obs.T.dot(chi_squared_terms)
--------------------
Prompt: 
def _resolve_user_group_names(opts):
    """
    Resolve user and group names in related opts
    """
----------
Output: 

    user_group_names = []
    group_names = []
    for opt in opts:
        if opt in [u'user', u'group']:
            if opt == u'user':
                user_group_names = user_group_names + user_group_names
            elif opt == u'group':
                group_names = group_names + group_names
        else:
            raise ValueError("Unknown option %s" % opt)
    for opt in [u'ipauserauthtype', u'ipauserauthtype_password', u'ipauserauthtype_user', u'ipauser
----------
Solution: 

    name_id_opts = {'uid': 'user.info',
                    'gid': 'group.info'}
    for ind, opt in enumerate(opts):
        if opt.split('=')[0] in name_id_opts:
            _givenid = opt.split('=')[1]
            _param = opt.split('=')[0]
            _id = _givenid
            if not re.match('[0-9]+$', _givenid):
                _info = __salt__[name_id_opts[_param]](_givenid)
                if _info and _param in _info:
                    _id = _info[_param]
            opts[ind] = _param + '=' + six.text_type(_id)
        opts[ind] = opts[ind].replace('\\040', '\\ ')
    return opts
--------------------
Prompt: 
def edge_has_annotation(edge_data: EdgeData, key: str) -> Optional[Any]:
    """Check if an edge has the given annotation.

    :param edge_data: The data dictionary from a BELGraph's edge
    :param key: An annotation key
    :return: If the annotation key is present in the current data dictionary

    For example, it might be useful to print all edges that are annotated with 'Subgraph':

    >>> from pybel.examples import sialic_acid_graph
    >>> for u, v, data in sialic_acid_graph.edges(data=True):
    >>>     if edge_has_annotation(data, 'Species')
    >>>         print(u, v, data)
    """
----------
Output: 
edge_has_annotation(
   ...     sialic_acid.SialicAcidGraph, 'Subgraph'):
   ...     print(u, v, data)
   ...
    """
    return edge_data.has_annotation(key)


def edge_has_annotation_with_key(
    edge_data: EdgeData, key: str, default: bool = True
) -> Optional[EdgeData]:
    """Check if an edge has the given annotation.

    :param edge_data: The data dictionary from a BELGraph's edge
    :param key: An annotation key
----------
Solution: 

    annotations = edge_data.get(ANNOTATIONS)

    if annotations is None:
        return None

    return annotations.get(key)
--------------------
Prompt: 
def init_db(self):
        """这个任务数据库只在程序开始时读入, 在程序关闭时导出.

        因为Gtk没有像在Qt中那么方便的使用SQLite, 而必须将所有数据读入一个
        liststore中才行.
        """
----------
Output: 

        self.db = sqlite3.connect(self.db_path)
        self.db.row_factory = sqlite3.Row
        self.db.text_factory = str
        self.db.executescript(self.db_path, "create table if not exists " + self.db_table + " (id int primary key autoincrement, " + self.db_id + ")")
        self.db.executescript(self.db_table, "create table if not exists " + self.db_if_not_exists + " (id int primary key autoincrement, " + self
----------
Solution: 

        cache_path = os.path.join(Config.CACHE_DIR,
                                  self.app.profile['username'])
        if not os.path.exists(cache_path):
            os.makedirs(cache_path, exist_ok=True)
        db = os.path.join(cache_path, TASK_FILE)
        self.conn = sqlite3.connect(db)
        self.cursor = self.conn.cursor()
        sql = 
--------------------
Prompt: 
def _set_system_mode(self, v, load=False):
    """
    Setter method for system_mode, mapped from YANG variable /rbridge_id/system_mode (container)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_system_mode is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_system_mode() directly.
    """
----------
Output: 

    def _set_system_mode_with_value(self, v, load=False, system_mode=None):
        """
        Setter method for system_mode, mapped from YANG variable /rbridge_id/system_mode (container)
        If this variable is read-only (config: false) in the
        source YANG file, then _set_system_mode is considered as a private
        method. Backends looking to populate this variable should
        do so via calling thisObj._set_system_mode() directly.
        """
        if system_mode is None:
            raise ValueError("Invalid
----------
Solution: 

    if hasattr(v, "_utype"):
      v = v._utype(v)
    try:
      t = YANGDynClass(v,base=system_mode.system_mode, is_container='container', presence=False, yang_name="system-mode", rest_name="system-mode", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'info': u'Maintenance mode on/off', u'callpoint': u'MaintenanceCallPoint', u'cli-incomplete-no': None}}, namespace='urn:brocade.com:mgmt:brocade-rbridge', defining_module='brocade-rbridge', yang_type='container', is_config=True)
    except (TypeError, ValueError):
      raise ValueError({
          'error-string': 
--------------------
Prompt: 
def connect(self):
        """ Simple connect """
----------
Output: 

        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.socket.connect(self.address)
        self.socket.setblocking(0)
        self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        self.socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)
        self.socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEPALIVE, 1)
        self.socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_KEEP
----------
Solution: 

        try:
            self.telnet = Telnet(self.host, self.port)
            time.sleep(1)
            self.get()
            self.get('login admin admin')
            self.update()
        except socket.gaierror:
            self.telnet = None
            LOGGER.error("Cannot connect to %s (%d)",
                         self.host, self.retries)
--------------------
Prompt: 
def _itertuples(df):
    """Custom implementation of ``DataFrame.itertuples`` that
    returns plain tuples instead of namedtuples. About 50% faster.
    """
----------
Output: 

    for i, row in df.itertuples(index=False):
        yield tuple(row)


def _itertuples_except(df):
    """Custom implementation of ``DataFrame.itertuples`` that
    raises a ValueError exception if more than one tuple in ``df`` fail.
    """
    for i, row in _iterrows(df):
        if len(row) > 1:
            raise ValueError("tuples are not unique: {}!= {}".format(
                i, len(row)
            ))


def _iterrows_except_ignore_index(df):
    """Custom implementation of ``DataFrame
----------
Solution: 

    cols = [df.iloc[:, k] for k in range(len(df.columns))]
    return zip(df.index, *cols)
--------------------
Prompt: 
def sample(self, nmr_samples, burnin=0, thinning=1):
        """Take additional samples from the given likelihood and prior, using this sampler.

        This method can be called multiple times in which the sample state is stored in between.

        Args:
            nmr_samples (int): the number of samples to return
            burnin (int): the number of samples to discard before returning samples
            thinning (int): how many sample we wait before storing a new one. This will draw extra samples such that
                    the total number of samples generated is ``nmr_samples * (thinning)`` and the number of samples
                    stored is ``nmr_samples``. If set to one or lower we store every sample after the burn in.

        Returns:
            SamplingOutput: the sample output object
        """
----------
Output: 
ning + 1)``.
        """
        if self.likelihood is None:
            raise ValueError("No likelihood provided")
        if self.prior is None:
            raise ValueError("No prior provided")
        if self.burnin is None:
            raise ValueError("No burnin provided")
        if self.thinning is None:
            raise ValueError("No thinning provided")
        if self.nmr_samples is None:
            raise ValueError("No nmr_samples provided")
        if self.nmr_prior is None:
            raise ValueError("No nmr_prior provided")
        if self.n_samples is None
----------
Solution: 

        if not thinning or thinning < 1:
            thinning = 1
        if not burnin or burnin < 0:
            burnin = 0

        max_samples_per_batch = max(1000 // thinning, 100)

        with self._logging(nmr_samples, burnin, thinning):
            if burnin > 0:
                for batch_start, batch_end in split_in_batches(burnin, max_samples_per_batch):
                    self._sample(batch_end - batch_start, return_output=False)
            if nmr_samples > 0:
                outputs = []
                for batch_start, batch_end in split_in_batches(nmr_samples, max_samples_per_batch):
                    outputs.append(self._sample(batch_end - batch_start, thinning=thinning))
                return SimpleSampleOutput(*[np.concatenate([o[ind] for o in outputs], axis=-1) for ind in range(3)])
--------------------
Prompt: 
def excludeSNPs(inPrefix, outPrefix, exclusionFileName):
    """Exclude some SNPs using Plink.

    :param inPrefix: the prefix of the input file.
    :param outPrefix: the prefix of the output file.
    :param exclusionFileName: the name of the file containing the markers to be
                              excluded.

    :type inPrefix: str
    :type outPrefix: str
    :type exclusionFileName: str

    Using Plink, exclude a list of markers from ``inPrefix``, and saves the
    results in ``outPrefix``. The list of markers are in ``exclusionFileName``.

    """
----------
Output: 
 """
    with open(exclusionFileName, 'w') as f:
        for marker in inPrefix.split(','):
            f.write(marker + '\n')

    return inPrefix, outPrefix, exclusionFileName


def main():
    """Main function."""
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument('-i', '--input',
                        dest='input',
                        required=True,
                        help='the input file')
    parser.add_argument('-o', '--output',
                        dest='output',
                        required=True,
                        help='the output file')
    parser.add_
----------
Solution: 

    plinkCommand = ["plink", "--noweb", "--bfile", inPrefix, "--exclude",
                    exclusionFileName, "--make-bed", "--out", outPrefix]
    runCommand(plinkCommand)
--------------------
Prompt: 
def get_interaction_energy(self, assign_ff=True, ff=None, mol2=False,
                               force_ff_assign=False):
        """Calculates the interaction energy of the AMPAL object.

        Parameters
        ----------
        assign_ff: bool, optional
            If true the force field will be updated if required.
        ff: BuffForceField, optional
            The force field to be used for scoring.
        mol2: bool, optional
            If true, mol2 style labels will also be used.
        force_ff_assign: bool, optional
            If true, the force field will be completely reassigned, 
            ignoring the cached parameters.

        Returns
        -------
        buff_score: buff.BUFFScore
            A BUFFScore object with information about each of the
            interactions and the `Atoms` involved.

        Raises
        ------
        AttributeError
            Raise if a component molecule does not have an `update_ff`
            method.
        """
----------
Output: 
 updated if required.

        Returns
        -------
        float
            The interaction energy of the AMPAL object.

        """
        if assign_ff_assign:
            self.ff_assign_to_energy(ff)
        if not mol2:
            return 0.0
        if force_ff_assign:
            return self.force_field_to_energy(force_ff)
        if not self.ff_assign_to_energy(ff):
            return 0.0
        return self.get_interaction_energy(assign_ff=assign_ff, ff=ff)

    def get_interaction_energy_
----------
Solution: 

        if not ff:
            ff = global_settings['buff']['force_field']
        if assign_ff:
            for molecule in self._molecules:
                if hasattr(molecule, 'update_ff'):
                    molecule.update_ff(
                        ff, mol2=mol2, force_ff_assign=force_ff_assign)
                else:
                    raise AttributeError(
                        'The following molecule does not have a update_ff'
                        'method:\n{}\nIf this is a custom molecule type it'
                        'should inherit from BaseAmpal:'.format(molecule))
        interactions = find_inter_ampal(self, ff.distance_cutoff)
        buff_score = score_interactions(interactions, ff)
        return buff_score
--------------------
Prompt: 
def _get_summary_struct(self):
        """
        Returns a structured description of the model, including (where
        relevant) the schema of the training data, description of the training
        data, training statistics, and model hyperparameters.

        Returns
        -------
        sections : list (of list of tuples)
            A list of summary sections.
              Each section is a list.
                Each item in a section list is a tuple of the form:
                  ('<label>','<field>')

        section_titles: list
            A list of section titles.
              The order matches that of the 'sections' object.
        """
----------
Output: 

        summary_struct = []
        for section in self.sections:
            if section.label == section_names[self.label]:
                for field in section.field:
                    if field.type =='string':
                        summary_struct.append(section.label)
                        field_title = field.string_value
                        break
        for item in self.schema:
            if item.type =='section':
                for label in item.label:
                    if label.startswith(section_names[self.label]):
                        for field in summary_struct:
                            if field.type =='section_title':
                                if
----------
Solution: 

        section = []
        section_titles = ['Attributes']
        for f in self._list_fields():
            section.append( ("%s" % f,"%s"% f) )

        return ([section], section_titles)
--------------------
Prompt: 
def degree_prune(graph, max_degree=20):
    """Prune the k-neighbors graph back so that nodes have a maximum
    degree of ``max_degree``.

    Parameters
    ----------
    graph: sparse matrix
        The adjacency matrix of the graph

    max_degree: int (optional, default 20)
        The maximum degree of any node in the pruned graph

    Returns
    -------
    result: sparse matrix
        The pruned graph.
    """
----------
Output: 

    # TODO: this is a bit of a hack, but it works
    # TODO: this is a bit of a hack, but it works
    # TODO: this is a bit of a hack, but it works
    # TODO: this is a bit of a hack, but it works
    # TODO: this is a bit of a hack, but it works
    # TODO: this is a bit of a hack, but it works
    # TODO: this is a bit of a hack, but it works
    # TODO: this is a bit of a hack, but it works
    # TODO: this is a bit
----------
Solution: 


    result = graph.tolil()
    for i, row_data in enumerate(result.data):
        if len(row_data) > max_degree:
            cut_value = np.argsort(row_data)[max_degree]
            row_data = [x if x <= cut_value else 0.0 for x in row_data]
            result.data[i] = row_data
    result = result.tocsr()
    result.eliminate_zeros()
    return result
--------------------
Prompt: 
def load_bytes(buf, num, pos):
    """Load sequence of bytes"""
----------
Output: 

    return _read_bytes(buf, num, pos)

def load_int(buf, num, pos):
    """Load integer"""
    return _read_int(buf, num, pos)

def load_float(buf, num, pos):
    """Load floating point"""
    return _read_float(buf, num, pos)

def load_int_from_str(buf, pos):
    """Load integer from string"""
    return _read_int(buf, pos)

def _read_int(buf, pos, size=None):
    """Read
----------
Solution: 

    end = pos + num
    if end > len(buf):
        raise BadRarFile('cannot load bytes')
    return buf[pos : end], end
--------------------
Prompt: 
def path(self, which=None):
        """Extend ``nailgun.entity_mixins.Entity.path``.

        The format of the returned path depends on the value of ``which``:

        facts
            /discovered_hosts/facts

        ``super`` is called otherwise.

        """
----------
Output: 

        if which in (None, '', 'local'):
            return super(DiscoveredHostCollection, self).path()
        return Path(self._server_url, self._server_url + '/v2/discovered_hosts')

    def list(self, params=None):
        """List all discovered hosts.

        :param params: a dict with additional parameters.

        :returns: a list of ``nailgun.entity_mixins.DiscoveredHost`` objects.

        """
        if params is None:
            params = {}

        if self._server_url is None:
            return self._discovered_hosts
----------
Solution: 

        if which == 'facts':
            return '{0}/{1}'.format(
                super(DiscoveredHost, self).path(which='base'),
                which
            )
        return super(DiscoveredHost, self).path(which)
--------------------
Prompt: 
def git_status(self, r):
        """return tuple of (status, adds, dels).
        status like !! ??
        adds and dels are lists of additions and deletions.
        """
----------
Output: 

        return r.git.status(self.path)

    def git_add(self, files):
        """add files to git repository.
        files is a list of files to add.
        """
        return r.git.add(self.path, files)

    def git_delete(self, filename):
        """delete file from git repository.
        filename is a path to a file.
        """
        return r.git.delete(self.path, filename)

    def git_diff(self, filename):
        """return tuple of (diff, adds, dels).
        diff is a list of
----------
Solution: 

        ret = self._cachedStatus.get(r.filename, None) if r else None
        return ret if ret else ["//", None, None]
--------------------
Prompt: 
def get_parameter(name, datatype, properties):
    """
    Build a Parameter object using its name, datatype and properties list
    :param name: name of the parameter
    :type name: basestring
    :param datatype: datatype of the parameter (must be a value of
    PARAMETER_CLASSES keys
    :type datatype: basestring
    :param properties: property values to be set in attributes or qualifiers
    :type properties: dict
    """
----------
Output: 

    return Parameter(name, datatype, properties)

----------
Solution: 

    return PARAMETER_CLASSES.get(datatype, Parameter)(name, datatype,
                                                      properties)
--------------------
Prompt: 
def _get_isolated(self, hostport):
        """Get a Peer for the given destination for a request.

        A new Peer is added and returned if one does not already exist for the
        given host-port. Otherwise, the existing Peer is returned.

        **NOTE** new peers will not be added to the peer heap.
        """
----------
Output: 

        if hostport in self.peers:
            return self.peers[hostport]
        else:
            new_peer = Peer(self.host, self.port, self.timeout)
            self.peers[hostport] = new_peer
            return new_peer

    def _get_peers(self):
        """Get a list of all peers.

        **NOTE** list of peers will not be returned.
        """
        return self.peers.values()

    def _get_timeout(self):
        """Get a list of all timeouts.

        **NOTE** list of timeouts will not be returned.
----------
Solution: 

        assert hostport, "hostport is required"
        if hostport not in self._peers:
            # Add a peer directly from a hostport, do NOT add it to the peer
            # heap
            peer = self.peer_class(
                tchannel=self.tchannel,
                hostport=hostport,
            )
            self._peers[peer.hostport] = peer

        return self._peers[hostport]
--------------------
Prompt: 
def remove_permission(self, queue, label):
        """
        Remove a permission from a queue.

        :type queue: :class:`boto.sqs.queue.Queue`
        :param queue: The queue object

        :type label: str or unicode
        :param label: The unique label associated with the permission
                      being removed.

        :rtype: bool
        :return: True if successful, False otherwise.
        """
----------
Output: 

        return self.get_boolean(
            "RemovePermission",
            params=dict(
                label=label,
                force=True,
            )
        )

    def get_all_permissions(self, label):
        """
        Get all permissions associated with a queue.

        :type label: str or unicode
        :param label: The unique label associated with the permission
                      being removed.

        :rtype: list of strings
        :return: The unique label associated with the permission
                      being removed.
        """
        params = dict(
            label=label,
            force=True,
        )
        return self.get_list("
----------
Solution: 

        params = {'Label': label}
        return self.get_status('RemovePermission', params, queue.id)
--------------------
Prompt: 
def get_environment(self, fUnicode = None):
        """
        Retrieves the environment with wich the program is running.

        @note: Duplicated keys are joined using null characters.
            To avoid this behavior, call L{get_environment_variables} instead
            and convert the results to a dictionary directly, like this:
            C{dict(process.get_environment_variables())}

        @see: L{win32.GuessStringType}

        @type  fUnicode: bool or None
        @param fUnicode: C{True} to return a list of Unicode strings, C{False}
            to return a list of ANSI strings, or C{None} to return whatever
            the default is for string types.

        @rtype:  dict(str S{->} str)
        @return: Dictionary of environment keys and values.

        @raise WindowsError: On error an exception is raised.
        """
----------
Output: 
 return a list of strings, and C{None} to return the default value.

        @rtype:  L{Environment}
        @return:  The environment object.

        @raise WindowsError: An error occured while trying to access the program
            or the environment is not supported.
        """
        if fUnicode is None:
            fUnicode = self.fUnicode
        if fUnicode:
            return self.get_unicode_environment(fUnicode)
        else:
            return Environment(0)

    def get_unicode_environment(self, fUnicode = None):
        """
        Retrieves the Unicode strings from the current environment.
----------
Solution: 


        # Get the environment variables.
        variables = self.get_environment_variables()

        # Convert the strings to ANSI if requested.
        if fUnicode is None:
            gst = win32.GuessStringType
            fUnicode = gst.t_default == gst.t_unicode
        if not fUnicode:
            variables = [ ( key.encode('cp1252'), value.encode('cp1252') ) \
                        for (key, value) in variables ]

        # Add the variables to a dictionary, concatenating duplicates.
        environment = dict()
        for key, value in variables:
            if key in environment:
                environment[key] = environment[key] + u'\0' + value
            else:
                environment[key] = value

        # Return the dictionary.
        return environment
--------------------
Prompt: 
def run_conf(self):
        """Run configuration (namedtuple)
        """
----------
Output: 

        self.conf = namedtuple('conf', ['name', 'value'])
        self.conf.name = self.name
        self.conf.value = self.value

    def __str__(self):
        """Return string representation of configuration
        """
        return self.conf.__str__() + '\n' + self.conf.name + '\n' + self.conf.value.__str__()

    def __repr__(self):
        """Return string representation of configuration
        """
        return self.conf.__repr__() + '\n' + self.conf.name + '\n' + self.conf.value.__repr
----------
Solution: 

        run_conf = namedtuple('run_conf', field_names=self._run_conf.keys())
        return run_conf(**self._run_conf)
--------------------
Prompt: 
def iscontainer(*items):
    """
    Checks whether all the provided items are containers (i.e of class list,
    dict, tuple, etc...)
    """
----------
Output: 

    return isinstance(items, list) and all(iscontainer_like(item) for item in items)


def iscontainer_like(obj):
    """
    Checks whether an object is an instance of a container (list,
    tuple, dict, etc...)
    """
    return isinstance(obj, list) and all(isinstance(item, Container) for item in obj)


def is_container_like(obj):
    """
    Checks whether an object is an instance of a container (list,
    tuple, dict, etc...)
    """
    return isinstance(obj, list) and all(isinstance
----------
Solution: 

    return all(isinstance(i, Iterable) and not isinstance(i, basestring)
               for i in items)
--------------------
Prompt: 
def pathFromIndex( self, index ):
        """
        Returns the joined path from the given model index.  This will
        join together the full path with periods.
        
        :param      index | <QModelIndex>
        
        :return     <str>
        """
----------
Output: 

        return self.model().path( index.row(), index.column(), self.name() )
    
    def name( self ):
        """
        Returns the name of the model.
        
        :return     <str>
        
        :return     <str>
        """
        return self.model().name()
    
    def rowCount( self ):
        """
        Returns the number of rows.
        
        :return     <int>
        
        :return     <int>
        """
        return self.rowCount
    
    def columnCount( self ):
        """
        Returns the number of columns.
        
        :return     <int>
        
        :return     <int
----------
Solution: 

        item = self._model.itemFromIndex(index)
        out  = []
        
        while ( item ):
            out.append(nativestring(item.text()))
            item = item.parent()
        
        return '.'.join(reversed(out))
--------------------
Prompt: 
def explicit(fixed_qubits: Iterable[raw_types.Qid],
                 fallback: Optional['QubitOrder']=None) -> 'QubitOrder':
        """A basis that contains exactly the given qubits in the given order.

        Args:
            fixed_qubits: The qubits in basis order.
            fallback: A fallback order to use for extra qubits not in the
                fixed_qubits list. Extra qubits will always come after the
                fixed_qubits, but will be ordered based on the fallback. If no
                fallback is specified, a ValueError is raised when extra qubits
                are specified.

        Returns:
            A Basis instance that forces the given qubits in the given order.
        """
----------
Output: 

                are specified.

        Returns:
            The explicit qubit list.
        """
        if fallback is None:
            if len(fixed_qubits) == 0:
                raise ValueError('No explicit qubits specified.')
            else:
                raise ValueError('No explicit qubits specified.')

        if len(fixed_qubits) == 1:
            return QubitOrder(fixed_qubits[0])

        if len(fixed_qubits) == 2:
            return QubitOrder(fallback[0] if fallback else None,
                             fixed_qubits[1] if fixed_qubits else None],
                            
----------
Solution: 

        result = tuple(fixed_qubits)
        if len(set(result)) < len(result):
            raise ValueError(
                'Qubits appear in fixed_order twice: {}.'.format(result))

        def func(qubits):
            remaining = set(qubits) - set(fixed_qubits)
            if not remaining:
                return result
            if not fallback:
                raise ValueError(
                    'Unexpected extra qubits: {}.'.format(remaining))
            return result + fallback.order_for(remaining)

        return QubitOrder(func)
--------------------
Prompt: 
def _sincedb_start_position(self):
        """Retrieves the starting position from the sincedb sql db
        for a given file
        """
----------
Output: 

        return self.db.execute('SELECT pos FROM sincedb WHERE file=?', (self.filename,)).fetchone()[0]

    def _sincedb_end_position(self):
        """Retrieves the ending position from the sincedb sql db
        for a given file
        """
        return self.db.execute('SELECT pos FROM sincedb WHERE file=?', (self.filename,)).fetchone()[0]

    def _get_file_position(self):
        """Retrieves the file position from the file name
        """
        return self.db.execute('SELECT pos FROM files WHERE name=?', (self.filename
----------
Solution: 

        if not self._sincedb_path:
            return None

        self._sincedb_init()
        self._log_debug('retrieving start_position from sincedb')
        conn = sqlite3.connect(self._sincedb_path, isolation_level=None)
        cursor = conn.cursor()
        cursor.execute('select position from sincedb where fid = :fid and filename = :filename', {
            'fid': self._fid,
            'filename': self._filename
        })

        start_position = None
        for row in cursor.fetchall():
            start_position, = row

        return start_position
--------------------
Prompt: 
def create_controller(self):
        """Create a controller to handle the request

        :returns: Controller, this Controller instance should be able to handle
            the request
        """
----------
Output: 

        raise NotImplementedError()

    def update_controller(self, context, id, controller):
        """Update a controller

        :param context: The request context
        :param id: The UUID of the controller
        :param controller: The new controller instance
        """
        raise NotImplementedError()

    def delete_controller(self, context, id):
        """Delete a controller

        :param context: The request context
        :param id: The UUID of the controller
        """
        raise NotImplementedError()

    def get_controller_id_by_uuid(self, context, uuid):
        """Get the ID of the specified controller

        :param context
----------
Solution: 

        body = None
        req = self.request
        res = self.response
        rou = self.router
        con = None

        controller_info = {}
        try:
            controller_info = rou.find(req, res)

        except IOError as e:
            logger.warning(str(e), exc_info=True)
            raise CallError(
                408,
                "The client went away before the request body was retrieved."
            )

        except (ImportError, AttributeError, TypeError) as e:
            exc_info = sys.exc_info()
            logger.warning(str(e), exc_info=exc_info)
            raise CallError(
                404,
                "{} not found because of {} \"{}\" on {}:{}".format(
                    req.path,
                    exc_info[0].__name__,
                    str(e),
                    os.path.basename(exc_info[2].tb_frame.f_code.co_filename),
                    exc_info[2].tb_lineno
                )
            )

        else:
            con = controller_info['class_instance']

        return con
--------------------
Prompt: 
def on(self, event, f=None):
        """Registers the function ``f`` to the event name ``event``.

        If ``f`` isn't provided, this method returns a function that
        takes ``f`` as a callback; in other words, you can use this method
        as a decorator, like so::

            @ee.on('data')
            def data_handler(data):
                print(data)

        In both the decorated and undecorated forms, the event handler is
        returned. The upshot of this is that you can call decorated handlers
        directly, as well as use them in remove_listener calls.
        """
----------
Output: 
url_rule.

       .. versionadded:: 1.0
        """
        if f is None:
            return self.on_form_pre(event, f)
        self.on_form_post(event, f)
        return self.remove_url_rule(event)

    def on_form_pre(self, event, f=None):
        """Registers a function ``f`` as a callback.

        If ``f`` isn't provided, this method returns a function that
        takes ``f`` as a callback; in other words, you can use this method
        as a decorator, like so::

           
----------
Solution: 


        def _on(f):
            self._add_event_handler(event, f, f)
            return f

        if f is None:
            return _on
        else:
            return _on(f)
--------------------
Prompt: 
def from_url(url, **options):
        """
        Downloads the contents of a given URL and loads it
        into a new TableFu instance
        """
----------
Output: 

        if options.get('format', None) == 'json':
            return TableFu(url, **options)
        elif options.get('format', None) == 'csv':
            return TableFu(url, **options)
        elif options.get('format', None) == 'tab':
            return TableFu(url, **options)
        elif options.get('format', None) == 'json':
            return TableFu(url, **options)
        else:
            raise ValueError("Unknown format: %s" % options['format'])

    def to_url(self, table):
        """
        Converts
----------
Solution: 

        resp = urllib2.urlopen(url)
        return TableFu(resp, **options)
--------------------
Prompt: 
def reversetext(contenttoreverse, reconvert=True):
    """
    Reverse any content

    :type contenttoreverse: string
    :param contenttoreverse: The content to be reversed

    :type reeval: boolean
    :param reeval: Wether or not to reconvert the object back into it's initial state. Default is "True".
    """
----------
Output: 

    if reeval:
        return contenttoreverse
    else:
        return contenttoreverse[::-1]

def get_content_range(start, end, step=1):
    """
    Get a list of the content between two ranges

    :type start: string
    :param start: The content range start

    :type end: string
    :param end: The content range end

    :type step: integer
    :param step: The content range step

    :rtype: list
    :return: A list of the content between two ranges.
    """
    if step > 0:
        return list(range(start
----------
Solution: 


    # If reconvert is specified
    if reconvert is True:
        # Return the evalated form
        return eval(
            str(type(contenttoreverse)).split("'")[1] + "('" +
            str(contenttoreverse)[::-1] + "')")

    # Return the raw version
    return contenttoreverse[::-1]
--------------------
Prompt: 
def __ensure_message_length_multiple(bit_data, bit_len: int, pauses, bit_sample_pos, divisor: int):
        """
        In case of ASK modulation, this method tries to use pauses after messages as zero bits so that
        the bit lengths of messages are divisible by divisor
        :param bit_data: List of bit arrays
        :param bit_len: Bit length that was used for demodulation
        :param pauses: List of pauses
        :param bit_sample_pos: List of Array of bit sample positions
        :param divisor: Divisor the messages should be divisible by
        """
----------
Output: 
visor: Demodulation factor
        :return:
        """
        # TODO: Check if this is necessary
        if bit_len == 0:
            return

        # TODO: Check if this is necessary
        if pauses == 0:
            return

        # TODO: Check if this is necessary
        if len(bit_data) < bit_len:
            self.__logger.warning("Not enough bits to demodulate message")
            return

        # TODO: Check if this is necessary
        if len(bit_data) < pauses:
            self.__logger.warning("Not enough pauses to demodulate message")
           
----------
Solution: 

        for i in range(len(bit_data)):
            missing_bits = (divisor - (len(bit_data[i]) % divisor)) % divisor
            if missing_bits > 0 and pauses[i] >= bit_len * missing_bits:
                bit_data[i].extend([0] * missing_bits)
                pauses[i] = pauses[i] - missing_bits * bit_len

                try:
                    bit_sample_pos[i][-1] = bit_sample_pos[i][-2] + bit_len
                except IndexError as e:
                    logger.warning("Error padding message " + str(e))
                    continue

                bit_sample_pos[i].extend([bit_sample_pos[i][-1] + (k + 1) * bit_len for k in range(missing_bits - 1)])
                bit_sample_pos[i].append(bit_sample_pos[i][-1] + pauses[i])
--------------------
Prompt: 
def show_item(h):
    """Show any RAR3/5 record.
    """
----------
Output: 

    if h.has_key('r'):
        r = h['r']
        del h['r']
        del r['id']
        del r['title']
        del r['authors']
        del r['tags']
        del r['comments']
        del r['isbn']
        del r['publisher']
        del r['series']
        del r['series_index']
        del r['series_index_2']
        del r['isbn']
        del r['publisher_sort']
        del r['publisher_sort_2']
        del r['publisher_sort_1']
        del r
----------
Solution: 

    if isinstance(h, rf.Rar3Info):
        show_item_v3(h)
    elif isinstance(h, rf.Rar5Info):
        show_item_v5(h)
    else:
        xprint('Unknown info record')
--------------------
Prompt: 
def get_varname_from_locals(val, locals_, default='varname-not-found',
                            strict=False, cmpfunc_=operator.is_):
    """ Finds the string name which has where locals_[name] is val

    Check the varname is in the parent namespace
    This will only work with objects not primatives

    Args:
        val (): some value
        locals_ (dict): local dictionary to search
        default (str):
        strict (bool):

    Returns:
        str: the varname which is Val (if it exists)

    """
----------
Output: 

    for key in locals_:
        if key in locals_ and locals_[key] == val:
            return key
    if strict:
        raise ValueError('Cannot find default value for %s' % default)
    return default


def get_varname_from_dict(dict_, default='varname-not-found',
                        strict=False, cmpfunc_=operator.is_):
    """ Finds the string name which has where dict_["name"] is val]

    Check the varname is in the parent namespace
    This will only work with objects not primatives

    Args:
        dict_ (dict): the dictionary to search
----------
Solution: 

    if val is None or isinstance(val, (int, float, bool)):
        # Cannot work on primative types
        return default
    try:
        for count, val_ in enumerate(six.itervalues(locals_)):
            if cmpfunc_(val, val_):
                index_ = count
        varname = six.text_type(list(locals_.keys())[index_])
    except NameError:
        varname = default
        if strict:
            raise
    return varname
--------------------
Prompt: 
def setup_ui(self, ):
        """Setup the general ui

        :returns: None
        :rtype: None
        :raises: None
        """
----------
Output: 

        self.ui = Ui(self)
        self.ui.setupUi(self)
        self.ui.buttonBox.button(QDialogButtonBox.Ok).setText(self.tr("OK"))
        self.ui.buttonBox.button(QDialogButtonBox.Cancel).setText(self.tr("Cancel"))
        self.ui.buttonBox.button(QDialogButtonBox.Ok).setDefault(True)
        self.ui.buttonBox.button(QDialogButtonBox.Ok).setFocus(Qt.OtherFocusReason)
        self.ui.buttonBox.button(QDialogButtonBox.Ok
----------
Solution: 

        plus_icon = get_icon('glyphicons_433_plus_bright.png', asicon=True)
        self.add_tb.setIcon(plus_icon)

        self.shot_browser = ListBrowser(4, parent=self, headers=["Project", "Sequence", "Shot", "Type"])
        self.asset_browser = ListBrowser(4, parent=self, headers=["Project", "Assettype", "Asset", "Type"])

        self.shotmodel = self.create_shot_model()
        self.assetmodel = self.create_asset_model()

        self.shot_browser.set_model(self.shotmodel)
        self.asset_browser.set_model(self.assetmodel)

        self.shot_vbox.addWidget(self.shot_browser)
        self.asset_vbox.addWidget(self.asset_browser)
--------------------
Prompt: 
def normalize(expr):
    """No elimination, but normalize arguments."""
----------
Output: 

    return expr

def is_number(expr):
    """Is x a number?"""
    return isinstance(expr, (int, long))

def is_sequence(expr):
    """Is x a sequence?"""
    return isinstance(expr, (list, tuple))

def is_set(expr):
    """Is x a set?"""
    return isinstance(expr, (set, frozenset))

def is_dict_like(expr):
    """Is x a dict-like object?"""
    try:
        _ = frozenset(expr)
        return True
    except TypeError:
        return False

def
----------
Solution: 

    args = [normalize(arg) for arg in expr.args]

    return type(expr)(expr.func, *args, start=expr.start, end=expr.end)
--------------------
Prompt: 
def add_types(graph, phenotypes):  # TODO missing expression phenotypes! also basket type somehow :(
    """ Add disjoint union classes so that it is possible to see the invariants
        associated with individual phenotypes """
----------
Output: 

    for phenotype in phenotypes:
        if phenotype not in graph:
            graph[phenotype] = {}
        if phenotype not in graph[phenotype]:
            graph[phenotype][
                'disjoint_union_class'] = {}
        if phenotype not in graph[phenotype][
                'disjoint_union_class']:
            graph[phenotype][
                'disjoint_union_class'][
                    phenotype] = {}
        if phenotype not in graph[phenotype][
                'individual_classes']:
            graph[phenotype][
                'individual_classes
----------
Solution: 


    collect = defaultdict(set)
    def recurse(id_, start, level=0):
        #print(level)
        for t in graph.g.triples((None, None, id_)):
            if level == 0:
                if t[1] != rdflib.term.URIRef('http://www.w3.org/2002/07/owl#someValuesFrom'):
                    continue
            if type_check(t, (rdflib.term.URIRef, rdflib.term.URIRef, rdflib.term.BNode)):
                #print(start, t[0])
                collect[start].add(t[0])
                return  #  we're done here, otherwise we hit instantiated subclasses
            if level > 1:
                if t[1] == rdflib.URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#first') or \
                   t[1] == rdflib.URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#rest'):
                    continue

            recurse(t[0], start, level + 1)


    for phenotype in phenotypes:
        recurse(phenotype, phenotype)

    return collect
--------------------
Prompt: 
def mongoengine_validate_wrapper(old_clean, new_clean):
    """
    A wrapper function to validate formdata against mongoengine-field
    validator and raise a proper django.forms ValidationError if there
    are any problems.
    """
----------
Output: 

    def _check_for_duplicates(form_data, field):
        """
        Check for duplicate values.
        """
        # Check for duplicate values.
        if form_data[field] in (None, ''):
            return True

        # Check for duplicate values.
        for key in form_data:
            if key not in ('id', 'name', 'email', 'password', 'confirm_password'):
                if form_data[key] == '':
                    msg = _('This field is required.')
                    self._errors['name'] = self.error_class([msg])
                if not form_data[key
----------
Solution: 


    def inner_validate(value):
        value = old_clean(value)
        try:
            new_clean(value)
            return value
        except ValidationError, e:
            raise forms.ValidationError(e)
    return inner_validate
--------------------
Prompt: 
def _float_check(self, attribute_array, value, irow, key):
        """Checks if value is valid float, appends to array if valid, appends
        nan if not"""
----------
Output: 

        try:
            float(value)
        except ValueError:
            return

        if not math.isfinite(value):
            return

        if not irow:
            return

        if not key:
            return

        if not isinstance(irow, list):
            return

        for i in irow:
            try:
                float(i)
            except ValueError:
                return

        return

    def _validate_float_range(self, attribute_array, value, irow, key):
        """Checks if value is valid float, appends to array if valid, appends
        nan if not"""
        try:
            float(value)
----------
Solution: 

        value = value.strip(' ')
        try:
            if value:
                attribute_array = np.hstack([attribute_array, float(value)])
            else:
                attribute_array = np.hstack([attribute_array, np.nan])
        except:
            print(irow, key)
            msg = 'Input file format error at line: %d' % (irow + 2)
            msg += ' key: %s' % (key)
            raise ValueError(msg)
        return attribute_array
--------------------
Prompt: 
def add_arguments(parser, doc_str, add_base_url=True):
    """Add standard arguments for DataONE utilities to a command line parser."""
----------
Output: 

    parser.add_argument(
        '--api-version',
        action='version',
        version='v1',
        help='API version to use.',
        version_string=__version__)
    parser.add_argument(
        '--api-url',
        action='store',
        dest='api_url',
        help='API URL to use.',
        default=None)
    parser.add_argument(
        '--api-key',
        action='store',
        dest='api_key',
        help='API key to use.',
        default=None)
    parser.add_argument(
        '--api-secret',
----------
Solution: 

    parser.description = doc_str
    parser.formatter_class = argparse.RawDescriptionHelpFormatter
    parser.add_argument("--debug", action="store_true", help="Debug level logging")
    parser.add_argument(
        "--cert-pub",
        dest="cert_pem_path",
        action="store",
        default=django.conf.settings.CLIENT_CERT_PATH,
        help="Path to PEM formatted public key of certificate",
    )
    parser.add_argument(
        "--cert-key",
        dest="cert_key_path",
        action="store",
        default=django.conf.settings.CLIENT_CERT_PRIVATE_KEY_PATH,
        help="Path to PEM formatted private key of certificate",
    )
    parser.add_argument(
        "--public", action="store_true", help="Do not use certificate even if available"
    )
    parser.add_argument(
        "--disable-server-cert-validation",
        action="store_true",
        help="Do not validate the TLS/SSL server side certificate of the source node (insecure)",
    )
    parser.add_argument(
        "--timeout",
        type=float,
        action="store",
        default=DEFAULT_TIMEOUT_SEC,
        help="Timeout for DataONE API calls to the source MN",
    )
    parser.add_argument(
        "--retries",
        type=int,
        action="store",
        default=DEFAULT_RETRY_COUNT,
        help="Retry DataONE API calls that raise HTTP level exceptions",
    )
    parser.add_argument(
        "--page-size",
        type=int,
        action="store",
        default=DEFAULT_PAGE_SIZE,
        help="Number of objects to retrieve in each list method API call to source MN",
    )
    parser.add_argument(
        "--major",
        type=int,
        action="store",
        help="Skip automatic detection of API major version and use the provided version",
    )
    parser.add_argument(
        "--max-concurrent",
        type=int,
        action="store",
        default=DEFAULT_MAX_CONCURRENT_TASK_COUNT,
        help="Max number of concurrent DataONE API",
    )
    if not add_base_url:
        parser.add_argument(
            "--baseurl",
            action="store",
            default=django.conf.settings.DATAONE_ROOT,
            help="Remote MN or CN BaseURL",
        )
    else:
        parser.add_argument("baseurl", help="Remote MN or CN BaseURL")
--------------------
Prompt: 
def __set_labels(self, labels):
        """
        Add a label on the document.
        """
----------
Output: 

        if self.labels is not None:
            self.labels.append(label)

    def __get_labels(self):
        """
        Return the list of labels on this document.
        """
        return self.labels

    def __set_title(self, title):
        """
        Add a title on the document.
        """
        if self.title is not None:
            self.title.append(title)

    def __get_title(self):
        """
        Return the list of titles on this document.
        """
        return self.title

    def __set_title_and_ids(self,
----------
Solution: 

        with self.fs.open(self.fs.join(self.path, self.LABEL_FILE), 'w') \
                as file_desc:
            for label in labels:
                file_desc.write("%s,%s\n" % (label.name,
                                             label.get_color_str()))
--------------------
Prompt: 
def answer(part, module='mlai2014.json'):
    """Returns the answers to the lab classes."""
----------
Output: 

    return json.loads(part.get('answers', '[]'))


def get_lab_class(module='mlai2014.json'):
    """Returns the lab class."""
    return json.loads(json.dumps(json.loads(get_lab_answers(module))))


def get_lab_answers(module='mlai2014.json'):
    """Returns the answers to the lab classes."""
    return json.loads(part.get('answers', '[]'))


def get_lab_class_by_id(id):
    """Returns the lab class by ID."""
   
----------
Solution: 

    marks = json.load(open(os.path.join(data_directory, module), 'rb'))
    return marks['Lab '  + str(part+1)]
--------------------
Prompt: 
def output_ip(gandi, ip, datacenters, vms, ifaces, output_keys, justify=11):
    """ Helper to output an ip information."""
----------
Output: 

    ifaces = [i.strip() for i in ifaces.split(',')]
    if len(ifaces) == 0:
        return
    ifaces = [i.strip() for i in ifaces.split(',')]
    if len(ifaces) == 1:
        return ifaces[0]
    ifaces = [i.strip() for i in ifaces.split(',')]
    if len(ifaces) == 2:
        return ifaces[0] + '.' + ifaces[1] + '.' + ifaces[2]
    if len(i
----------
Solution: 

    output_generic(gandi, ip, output_keys, justify)

    if 'type' in output_keys:
        iface = ifaces.get(ip['iface_id'])
        type_ = 'private' if iface.get('vlan') else 'public'
        output_line(gandi, 'type', type_, justify)
        if type_ == 'private':
            output_line(gandi, 'vlan', iface['vlan']['name'], justify)

    if 'vm' in output_keys:
        iface = ifaces.get(ip['iface_id'])
        vm_id = iface.get('vm_id')
        if vm_id:
            vm_name = vms.get(vm_id, {}).get('hostname')
            if vm_name:
                output_line(gandi, 'vm', vm_name, justify)

    if 'dc' in output_keys:
        for dc in datacenters:
            if dc['id'] == ip.get('datacenter_id',
                                  ip.get('datacenter', {}).get('id')):
                dc_name = dc.get('dc_code', dc.get('iso', ''))
                break

        output_line(gandi, 'datacenter', dc_name, justify)
--------------------
Prompt: 
def set_density_matrix(self, density_matrix_repr: Union[int, np.ndarray]):
        """Set the density matrix to a new density matrix.

        Args:
            density_matrix_repr: If this is an int, the density matrix is set to
            the computational basis state corresponding to this state. Otherwise
            if this is a np.ndarray it is the full state, either a pure state
            or the full density matrix.  If it is the pure state it must be the
            correct size, be normalized (an L2 norm of 1), and be safely
            castable to an appropriate dtype for the simulator.  If it is a
            mixed state it must be correctly sized and positive semidefinite
            with trace one.
        """
----------
Output: 
 type.
        """
        if isinstance(density_matrix_repr, int):
            density_matrix_repr = np.array([density_matrix_repr])
        if isinstance(density_matrix_repr, np.ndarray):
            density_matrix_repr = np.array([1 if x == 0 else 0 for x in density_matrix_repr])
        self.density_matrix = density_matrix_repr

    def set_density_matrix_repr(self, density_matrix_repr: Union[int, np.ndarray]):
        """Set the density matrix to a new density matrix.

        Args:
           
----------
Solution: 

        density_matrix = density_matrix_utils.to_valid_density_matrix(
            density_matrix_repr, len(self._qubit_map), self._dtype)
        density_matrix = np.reshape(density_matrix,
                                    self.simulator_state().density_matrix.shape)
        np.copyto(dst=self.simulator_state().density_matrix, src=density_matrix)
--------------------
Prompt: 
def _set_interface_priority(self, v, load=False):
    """
    Setter method for interface_priority, mapped from YANG variable /routing_system/interface/ve/intf_isis/interface_isis/interface_priority (container)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_interface_priority is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_interface_priority() directly.
    """
----------
Output: 

    def __set_interface_priority(self, v, load=False):
        """
        Setter method for interface_priority, mapped from YANG variable /routing_system/interface/ve/intf_isis/interface_isis/interface_priority (container)
        If this variable is read-only (config: false) in the
        source YANG file, then _set_interface_priority is considered as a private
        method. Backends looking to populate this variable should
        do so via calling thisObj._set_interface_priority() directly.
        """
        if hasattr(v, "_utype"):
            v
----------
Solution: 

    if hasattr(v, "_utype"):
      v = v._utype(v)
    try:
      t = YANGDynClass(v,base=interface_priority.interface_priority, is_container='container', presence=False, yang_name="interface-priority", rest_name="priority", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'info': u'Router priority for ISIS', u'alt-name': u'priority', u'cli-incomplete-no': None, u'cli-incomplete-command': None}}, namespace='urn:brocade.com:mgmt:brocade-isis', defining_module='brocade-isis', yang_type='container', is_config=True)
    except (TypeError, ValueError):
      raise ValueError({
          'error-string': 
--------------------
Prompt: 
def public_data_download_cli(source, username, directory, max_size, quiet,
                             debug):
    """
    Command line tools for downloading public data.
    """
----------
Output: 

    if not os.path.exists(directory):
        os.makedirs(directory)

    if not quiet:
        print('Downloading public data from %s' % source)

    if not os.path.exists(source):
        raise ValueError('Source directory does not exist: %s' % source)

    if not quiet:
        print('Downloading public data from %s' % directory)

    if not os.path.exists(source):
        raise ValueError('Source directory does not exist: %s' % source)

    if not quiet:
        print('Downloading public data from %s' % source)

    if
----------
Solution: 

    return public_download(source, username, directory, max_size, quiet, debug)
--------------------
Prompt: 
def start(self):
        """
        Start all the processes
        """
----------
Output: 

        self.process_list = []
        self.process_list.append(self.process_handler)
        self.process_list.append(self.process_handler)
        self.process_list.append(self.process_handler)
        self.process_handler.start()
        self.process_list.append(self.process_handler)

    def stop(self):
        """
        Stop all the processes
        """
        self.process_handler.stop()
        self.process_handler.join()
        for process in self.process_list:
            process.join()

    def join
----------
Solution: 

        Global.LOGGER.info("starting the flow manager")
        self._start_actions()
        self._start_message_fetcher()
        Global.LOGGER.debug("flow manager started")
--------------------
Prompt: 
def _start_zmq_monitor(self):
        """
        Starts ZMQ monitor for debugging purposes.
        :return:
        """
----------
Output: 

        self.log.info("Starting ZMQ Monitor")
        self.log.info("Using %s as ZMQ context" % self.config.get('zmq', 'debug'))
        self.zmq_context = zmq.Context()
        self.zmq_context.linger = 0
        self.zmq_context.socket(zmq.PUB)
        self.zmq_monitor = gevent.spawn(self.zmq_context.socket, 'inproc://zeromq.ipc', log=self.log)

    def _stop_zmq_context(self):
        """
        Stops ZMQ context
----------
Solution: 

        # Socket monitor shall be used the only for debug
        # purposes so using threading doesn't look too bad here

        if HAS_ZMQ_MONITOR and self.opts['zmq_monitor']:
            log.debug('Starting ZMQ monitor')
            import threading
            self._w_monitor = ZeroMQSocketMonitor(self._socket)
            threading.Thread(target=self._w_monitor.start_poll).start()
            log.debug('ZMQ monitor has been started started')
--------------------
Prompt: 
def get_policy(self):
        """
        Returns an instance of :attr:`~policy_class`.

        :return: An instance of the current policy class.
        :rtype: dockermap.map.policy.base.BasePolicy
        """
----------
Output: 

        return self._policy

    def get_docker_host(self):
        """
        Returns an instance of :attr:`~docker_host`.

        :return: An instance of :attr:`~docker_host`.
        :rtype: dockermap.map.host.DockerHost
        """
        return self._docker_host

    def get_docker_port(self):
        """
        Returns an instance of :attr:`~docker_port`.

        :return: An instance of :attr:`~docker_port`.
        :rtype: int
        """
        return self._docker_port

    def get_docker_host_config(self
----------
Solution: 

        if not self._policy:
            self._policy = self.policy_class(self._maps, self._clients)
        return self._policy
--------------------
Prompt: 
def get_posts(self, count=10, offset=0, recent=True,  tag=None,
                  user_id=None, include_draft=False):
        """
        Get posts given by filter criteria

        :param count: The number of posts to retrieve (default 10). If count
         is ``None``, all posts are returned.
        :type count: int
        :param offset: The number of posts to offset (default 0)
        :type offset: int
        :param recent: Order by recent posts or not
        :type recent: bool
        :param tag: Filter by a specific tag
        :type tag: str
        :param user_id: Filter by a specific user
        :type user_id: str
        :param include_draft: Whether to include posts marked as draft or not
        :type include_draft: bool

        :return: A list of posts, with each element a dict containing values
         for the following keys: (title, text, draft, post_date,
         last_modified_date). If count is ``None``, then all the posts are
         returned.
        """
----------
Output: 
 :type tag: str
        :param user_id: Filter by a specific user id
        :type user_id: str
        :returns: Generator of :class:`Post` objects.
        """
        if tag is None:
            for i in range(count):
                yield self.get_post(i, offset, recent,  user_id)
        else:
            for i in range(count):
                yield self.get_tag(i, tag)

    def get_post(self, post_id):
        """
        Get a single post

        :param post_id: The id of the post
----------
Solution: 

        raise NotImplementedError("This method needs to be implemented by the "
                                  "inheriting class")
--------------------
Prompt: 
def import_element(self, xml_element):
        """
        Imports the element from an lxml element and loads its content.

        :param lxml.etree._Element xml_element: XML element to import.
        """
----------
Output: 

        self.xml_root = xml_element
        self.xml_element = xml_element.text
        self.import_list = self.xml_root.xpath('//*[@id="import"]')
        self.import_element = self.import_list[0]
        self.import_list = self.xml_root.xpath('//*[@id="import"]')
        self.import_element.set('class', 'import-element')
        self.import_element.set('data-oe-id', xml_element.get('id'))
        self.import_list.set
----------
Solution: 


        self.xml_element = xml_element

        uri, tag = Element.get_namespace_and_tag(self.xml_element.tag)
        self.namespace = uri
        self.name = tag

        self.load_xml_attrs()

        if self.xml_element.text is None:
            self.text = ''
        else:
            self.text = self.xml_element.text
--------------------
Prompt: 
def print_table(lines, separate_head=True):
	"""Prints a formatted table given a 2 dimensional array"""
----------
Output: 

	for line in lines:
		print line

	if separate_head:
		print '\n'

	for i in range(len(lines)):
		for j in range(i):
			print '\t'.join(lines[i][j:i+1])

	if separate_head:
		print '\n'

	for i in range(len(lines)):
		for j in range(i):
			print '\t'.join(lines[i][j:i+1])

	if separate_head:
		print '\n'

	
----------
Solution: 

	#Count the column width
	widths = []
	for line in lines:
			for i,size in enumerate([len(x) for x in line]):
					while i >= len(widths):
							widths.append(0)
					if size > widths[i]:
							widths[i] = size
	   
	#Generate the format string to pad the columns
	print_string = ""
	for i,width in enumerate(widths):
			print_string += "{" + str(i) + ":" + str(width) + "} | "
	if (len(print_string) == 0):
			return
	print_string = print_string[:-3]
	   
	#Print the actual data
	for i,line in enumerate(lines):
			print(print_string.format(*line))
			if (i == 0 and separate_head):
					print("-"*(sum(widths)+3*(len(widths)-1)))
--------------------
Prompt: 
def _decompress(self, fp):
        """
        Internal function for decompressing a backup file with the DEFLATE algorithm

        :rtype: Proxy
        """
----------
Output: 

        raise NotImplementedError()

    def _open_backup(self, fp):
        """
        Internal function for opening a backup file

        :rtype: Proxy
        """
        raise NotImplementedError()

    def _close_backup(self, backup):
        """
        Internal function for closing a backup file

        :param backup: the backup file to close
        """
        raise NotImplementedError()

    def _get_backup_file_list(self, backup):
        """
        Internal function for getting a list of backups

        :rtype: list
        """
        raise NotImplementedError()

    def _get_backup_files(self, backup):
        """
        Internal
----------
Solution: 

        decompressor = zlib.decompressobj()
        if self.stream:
            return Proxy(decompressor.decompress, fp)
        else:
            out = io.BytesIO(decompressor.decompress(fp.read()))
            out.write(decompressor.flush())
            out.seek(0)
            return out
--------------------
Prompt: 
def change_event_params(self, handler, **kwargs):
        """
        This allows the client to change the parameters for an event, in the case that there is a desire for slightly different behavior, such as reasigning keys.

        handler - the handler object that the desired changes are made to.

        kwargs - the variable number of keyword arguments for the parameters that must match the properties of the corresponding event.
            """
----------
Output: 

        if handler.name == 'on_change':
            self.on_change(kwargs['value'])
        elif handler.name == 'on_change_event':
            self.on_change_event(kwargs['value'])
        elif handler.name == 'on_close':
            self.on_close(kwargs['event'])
        elif handler.name == 'on_close_event':
            self.on_close_event(kwargs['event'])
        elif handler.name == 'on_close_event':
            self.on_close_event(kwargs['event'])
        elif handler.name ==
----------
Solution: 

        if not isinstance(handler, Handler):
            raise TypeError("given object must be of type Handler.")
        if not self.remove_handler(handler):
            raise ValueError("You must pass in a valid handler that already exists.")

        self.add_handler(handler.type, handler.actions, **kwargs)
        self.event = handler.event
--------------------
Prompt: 
def vector_sum(vector1, vector2, coeff=1.0):
    """ Sums the vectors.

    This function computes the result of the vector operation :math:`\\overline{v}_{1} + c * \\overline{v}_{2}`, where
    :math:`\\overline{v}_{1}` is ``vector1``, :math:`\\overline{v}_{2}`  is ``vector2`` and :math:`c` is ``coeff``.

    :param vector1: vector 1
    :type vector1: list, tuple
    :param vector2: vector 2
    :type vector2: list, tuple
    :param coeff: multiplier for vector 2
    :type coeff: float
    :return: updated vector
    :rtype: list
    """
----------
Output: 
 :type vector2: list, tuple
    :return: sum of the vectors
    :rtype: float
    """
    if len(vector1)!= len(vector2):
        raise ValueError("vector_sum: length of vectors must be equal to length of vector1")
    if len(vector1) == 0:
        return 0.0
    if len(vector2) == 0:
        return 0.0
    if len(vector1) == 0:
        return 0.0
    if len(vector2) == 0:
        return 0.0
    if len(vector1) == 0:
        return
----------
Solution: 

    summed_vector = [v1 + (coeff * v2) for v1, v2 in zip(vector1, vector2)]
    return summed_vector
--------------------
Prompt: 
def create(self):
        """
        Create an instance of the Time Series Service with the typical
        starting settings.
        """
----------
Output: 

        if self.is_time_series_service_enabled():
            self.client.system.time_series_service.create(self.resource_group, self.name, self.location, self.sku, self.location_type, self.tags, self.sku_name, self.location_name, self.tags)
        else:
            raise Exception(f"Time Series Service {self.name} is not enabled.")

    def is_time_series_service_enabled(self):
        """
        Check if the Time Series service is enabled.
        """
        return self.client.
----------
Solution: 

        self.service.create()

        predix.config.set_env_value(self.use_class, 'ingest_uri',
                self.get_ingest_uri())
        predix.config.set_env_value(self.use_class, 'ingest_zone_id',
                self.get_ingest_zone_id())

        predix.config.set_env_value(self.use_class, 'query_uri',
                self.get_query_uri())
        predix.config.set_env_value(self.use_class, 'query_zone_id',
                self.get_query_zone_id())
--------------------
Prompt: 
def as_span(cls, lower_version=None, upper_version=None,
                lower_inclusive=True, upper_inclusive=True):
        """Create a range from lower_version..upper_version.

        Args:
            lower_version: Version object representing lower bound of the range.
            upper_version: Version object representing upper bound of the range.

        Returns:
            `VersionRange` object.
        """
----------
Output: 

        if lower_version is None:
            lower_version = cls.DEFAULT_VERSION
        if upper_version is None:
            upper_version = cls.DEFAULT_VERSION
        if lower_inclusive:
            lower_version = VersionRange(lower_version.lower_bound,
                lower_version.upper_bound, upper_version.lower_bound)
        if upper_inclusive:
            lower_version = VersionRange(upper_version.lower_bound,
                lower_version.upper_bound, lower_version.lower_bound)
        return cls(lower_version, upper_version)

    def
----------
Solution: 

        lower = (None if lower_version is None
                 else _LowerBound(lower_version, lower_inclusive))
        upper = (None if upper_version is None
                 else _UpperBound(upper_version, upper_inclusive))
        bound = _Bound(lower, upper)

        range = cls(None)
        range.bounds = [bound]
        return range
--------------------
Prompt: 
def get_local_file_dist(self):
        """
        Handle importing from a source archive; this also uses setup_requires
        but points easy_install directly to the source archive.
        """
----------
Output: 

        # Create a source distribution object until we have enough data to
        # install it (ie. we have one, even though we don't, and the data
        # directory itself, and one without.
        from rez.package_maker import get_distribution
        dist = get_distribution('rez')
        for dist_name in dist.get_entry_map('console_scripts').keys():
            dist_path = os.path.join(dist.location, dist_name)
            if not os.path.isdir(dist_path):
                os.makedirs(dist_path)
            dist_
----------
Solution: 


        if not os.path.isfile(self.path):
            return

        log.info('Attempting to unpack and import astropy_helpers from '
                 '{0!r}'.format(self.path))

        try:
            dist = self._do_download(find_links=[self.path])
        except Exception as e:
            if DEBUG:
                raise

            log.warn(
                'Failed to import {0} from the specified archive {1!r}: '
                '{2}'.format(PACKAGE_NAME, self.path, str(e)))
            dist = None

        if dist is not None and self.auto_upgrade:
            # A version of astropy-helpers was found on the available path, but
            # check to see if a bugfix release is available on PyPI
            upgrade = self._do_upgrade(dist)
            if upgrade is not None:
                dist = upgrade

        return dist
--------------------
Prompt: 
def registerSave(self, obj) :
		"""Each object can only be save donce during a session, returns False if the object has already been saved. True otherwise"""
----------
Output: 

		if self.save_lock :
			self.save_lock.acquire()
			try :
				if self.save_lock.locked() :
					return False
			finally :
				self.save_lock.release()
			return True
		else :
			return False

	def registerLoad(self, obj) :
		"""Each object can only be load donce during a session, returns False if the object has already been loaded. True otherwise"""
		if self.load_lock :
			self.load_lock.acquire()
			try :
			
----------
Solution: 

		if obj._runtimeId in self.savedObject :
			return False

		self.savedObject.add(obj._runtimeId)
		return True
--------------------
Prompt: 
def string_or_bool(value):
    """
    Ritorna True o False in caso venga passata la stringa 'true' o 'false'
    (o 't' o 'f') altrimenti ritorna una stringa.

    :param value: Stringa da analizzare.
    :type value: str
    """
----------
Output: 

    if isinstance(value, str):
        if value.lower() == "true":
            return True
        elif value.lower() == "false":
            return False
        else:
            raise ValueError("Falha debería ser un stringo: %s" % value)
    elif isinstance(value, bool):
        if value:
            return True
        else:
            raise ValueError("Falha debería ser un booleano: %s" % value)
    else:
        raise ValueError("Falha debería ser un stringo: %s" % value)


def string
----------
Solution: 

    if value.lower() in ['t', 'true']:
        value = True
    elif value.lower() in ['f', 'false']:
        value = False
    elif str.isdigit(str(value)):
        value = int(value) != 0
    else:
        value = str(value) # pylint: disable=redefined-variable-type
    return value
--------------------
Prompt: 
def apply(self, doc):
        """
        Generate MentionParagraphs from a Document by parsing all of its Paragraphs.

        :param doc: The ``Document`` to parse.
        :type doc: ``Document``
        :raises TypeError: If the input doc is not of type ``Document``.
        """
----------
Output: 

        if not isinstance(doc, Paragraph):
            raise TypeError('Input must be of type Paragraph.')

        for i, paragraph in enumerate(doc.paras):
            if not isinstance(paragraph, Paragraph):
                raise TypeError('Input must be of type Paragraph.')

            for j, field in enumerate(paragraph):
                if field.tag == 'p':
                    self.add_paragraph(field, i, j, paragraph.paras[j + 1])
                elif field.tag == 'r':
                    self.add_paragraph(field, i, j, paragraph.paras[j + 1
----------
Solution: 

        if not isinstance(doc, Document):
            raise TypeError(
                "Input Contexts to MentionParagraphs.apply() must be of type Document"
            )

        for paragraph in doc.paragraphs:
            yield TemporaryParagraphMention(paragraph)
--------------------
Prompt: 
def authorize_client_credentials(
        self, client_id, client_secret=None, scope="private_agent"
    ):
        """Authorize to platform with client credentials

        This should be used if you posses client_id/client_secret pair
        generated by platform.
        """
----------
Output: 

        self.client_id = client_id
        self.client_secret = client_secret
        self.scope = scope
        self.client_secret_expires = None
        self.client_secret_expires_in = None
        self.client_secret_expires_at = None
        if client_id and not client_secret:
            raise ValueError("You must provide client_secret_key to authorize.")
        elif client_id and client_secret:
            raise ValueError("You must provide client_id and client_secret_key to authorize.")
        elif client_id and not client_secret:
            raise
----------
Solution: 

        self.auth_data = {
            "grant_type": "client_credentials",
            "scope": [ scope ],
            "client_id": client_id,
            "client_secret": client_secret
        }

        self._do_authorize()
--------------------
Prompt: 
def BatchConvert(self, metadata_value_pairs, token=None):
    """Convert batch of FileStoreHashs."""
----------
Output: 

    return [self._ConvertHash(metadata_value_pair) for metadata_value_pair in metadata_value_pairs]

  def _ConvertHash(self, metadata_value_pair):
    """Converts a FileStoreHash entry to a list of FileInfo objects."""
    return [self._FileInfo(metadata_value) for metadata_value in metadata_value_pair.values]

  def _FileInfo(self, metadata_value):
    """Creates FileInfo proto from a FileStoreHash entry.

    Args:
      metadata_value: A FileInfo proto to be used for the new instance.

   
----------
Solution: 


    urns = [urn for metadata, urn in metadata_value_pairs]
    urns_dict = dict((urn, metadata) for metadata, urn in metadata_value_pairs)

    results = []
    for hash_urn, client_files in filestore.HashFileStore.GetClientsForHashes(
        urns, token=token):
      for hit in client_files:
        metadata = ExportedMetadata(urns_dict[hash_urn])
        metadata.client_urn = rdfvalue.RDFURN(hit).Split(2)[0]

        result = ExportedFileStoreHash(
            metadata=metadata,
            hash=hash_urn.hash_value,
            fingerprint_type=hash_urn.fingerprint_type,
            hash_type=hash_urn.hash_type,
            target_urn=hit)
        results.append(result)

    return results
--------------------
Prompt: 
def strace_configure(self, port_width):
        """Configures the trace port width for tracing.

        Note that configuration cannot occur while STRACE is running.

        Args:
          self (JLink): the ``JLink`` instance
          port_width (int): the trace port width to use.

        Returns:
          ``None``

        Raises:
          ValueError: if ``port_width`` is not ``1``, ``2``, or ``4``.
          JLinkException: on error.
        """
----------
Output: 

        if port_width == 1:
            raise ValueError('trace port 1 is not supported.')
        elif port_width == 2:
            raise ValueError('trace port 2 is not supported.')
        elif port_width == 4:
            raise ValueError('trace port 4 is not supported.')
        else:
            raise ValueError('trace port width must be 1 or 2 or 4.')

    def strace_attach(self, port_width):
        """Attaches a trace port to the current thread.

        Args:
          self (JLink): the ``JLink`` instance
          port_width (int): the trace port to attach.
----------
Solution: 

        if port_width not in [1, 2, 4]:
            raise ValueError('Invalid port width: %s' % str(port_width))

        config_string = 'PortWidth=%d' % port_width
        res = self._dll.JLINK_STRACE_Config(config_string.encode())
        if res < 0:
            raise errors.JLinkException('Failed to configure STRACE port')

        return None
--------------------
Prompt: 
def with_slots(cls):
    """
    Decorator for a class with _slots_. It automatically defines
    the methods __eq__, __ne__, assert_equal.
    """
----------
Output: 

    def __init__(self, *args, **kwargs):
        super(WithSlots, cls).__init__(*args, **kwargs)
        self.assertEqual = cls.assertEqual
        self.assertNotEqual = cls.assertNotEqual
        self.assertNotEquals = cls.assertNotEquals
        self.assertFalse = cls.assertFalse

    def __call__(self, f):
        @wraps(f)
        def wrapper(*args, **kwargs):
            with self.subTest(f.__name__):
                return f(*args, **kwargs)
        return wrapper


@override_settings(ROOT_URLCONF='test_client_regress.urls')
----------
Solution: 

    def _compare(self, other):
        for slot in self.__class__._slots_:
            attr = operator.attrgetter(slot)
            source = attr(self)
            target = attr(other)
            if isinstance(source, numpy.ndarray):
                eq = numpy.array_equal(source, target)
            elif hasattr(source, '_slots_'):
                source.assert_equal(target)
                eq = True
            else:
                eq = source == target
            yield slot, source, target, eq

    def __eq__(self, other):
        return all(eq for slot, source, target, eq in _compare(self, other))

    def __ne__(self, other):
        return not self.__eq__(other)

    def assert_equal(self, other, ignore=()):
        for slot, source, target, eq in _compare(self, other):
            if not eq and slot not in ignore:
                raise AssertionError('slot %s: %s is different from %s' %
                                     (slot, source, target))

    cls._slots_  # raise an AttributeError for missing slots
    cls.__eq__ = __eq__
    cls.__ne__ = __ne__
    cls.assert_equal = assert_equal
    return cls
--------------------
Prompt: 
def _get_simple(self, name):
        """
        Query the stack for a non-dotted name.

        """
----------
Output: 

        parts = name.split('.')
        if len(parts) > 1:
            raise ValueError("Can't use * as a wildcard for the stack")
        obj = self.get_object(parts[0])
        if not obj:
            raise ValueError("No object found for %s" % name)
        if len(parts) == 1:
            return self.get_object(obj[0])
        return obj

    def _get_dotted(self, name):
        """
        Query the stack for a dotted name.

        """
        parts = name.split('.')
        if len(parts) > 1:
           
----------
Solution: 

        for item in reversed(self._stack):
            result = _get_value(item, name)
            if result is not _NOT_FOUND:
                return result

        raise KeyNotFoundError(name, "part missing")
--------------------
Prompt: 
def get_class(mcs):
        """ Generates new class to gether logic of all available extensions
            ::

                mc = ExtensibleType._("MyClass")
                @six.add_metaclass(mc)
                class MyClassBase(object):
                    pass

                # get class with all extensions enabled
                MyClass = mc.get_class()

        """
----------
Output: 

        class_name = mcs.get_name()
        if class_name is None:
            return None
        return type(class_name, (object,), {
            '__doc__': self.docstring,
            'name': class_name,
            'docstring': self.docstring,
            'enabled': mcs.get_enabled(),
           'metadata': mcs.get_metadata(),
            'enabled_metadata': mcs.get_metadata(),
           'metadata_metadata': mcs.get_metadata_metadata_metadata(),
           'metadata_enabled': mcs.get_enabled_metadata_metadata_
----------
Solution: 

        if mcs._generated_class is None:
            mcs._generated_class = type(
                mcs._cls_name,
                tuple(mcs._base_classes),
                {'_generated': True})
        return mcs._generated_class
--------------------
Prompt: 
def get_doctypes(self, default_doctypes=None):
        """Returns the doctypes (or mapping type names) to use."""
----------
Output: 

        if default_doctypes is None:
            default_doctypes = self.doctypes
        return default_doctypes

    def get_doctype(self, dname):
        """Returns the doctypes for a document type.

        @param dname: the document type name
        @type dname: string
        @return: the doctype objects
        @rtype: list of doctypes.keys
        """
        if dname in self.doctypes:
            return self.doctypes[dname]
        return []

    def get_doctypes_from_path(self, path):
        """Returns the doctypes for
----------
Solution: 

        doctypes = self.type.get_mapping_type_name()
        if isinstance(doctypes, six.string_types):
            doctypes = [doctypes]
        return super(S, self).get_doctypes(default_doctypes=doctypes)
--------------------
Prompt: 
def _execute(self, endpoint, database, query, default_timeout, properties=None):
        """Executes given query against this client"""
----------
Output: 

        self.log.debug("Executing query: %s" % query)
        return self._client.execute(endpoint, database, query, properties)

    def _execute_with_retry(self, database, properties=None):
        """Executes given query against this client"""
        self.log.debug("Executing query: %s" % query)
        return self._execute(database, database, query, default_timeout, properties)

    def _execute_with_timeout(self, database, properties=None):
        """Executes given query against this client"""
        self.log.debug
----------
Solution: 


        request_payload = {"db": database, "csl": query}
        if properties:
            request_payload["properties"] = properties.to_json()

        request_headers = {
            "Accept": "application/json",
            "Accept-Encoding": "gzip,deflate",
            "Content-Type": "application/json; charset=utf-8",
            "x-ms-client-version": "Kusto.Python.Client:" + VERSION,
            "x-ms-client-request-id": "KPC.execute;" + str(uuid.uuid4()),
        }

        if self._auth_provider:
            request_headers["Authorization"] = self._auth_provider.acquire_authorization_header()

        timeout = self._get_timeout(properties, default_timeout)
        response = self._session.post(endpoint, headers=request_headers, json=request_payload, timeout=timeout.seconds)

        if response.status_code == 200:
            if endpoint.endswith("v2/rest/query"):
                return KustoResponseDataSetV2(response.json())
            return KustoResponseDataSetV1(response.json())

        raise KustoServiceError([response.json()], response)
--------------------
Prompt: 
def make_duplicate_request(request):
    """
    Since werkzeug request objects are immutable, this is needed to create an
    identical reuet object with immutable values so it can be retried after a
    POST failure.
    """
----------
Output: 

    return werkzeug.datastructures.ImmutableMultiDict(request.values)


def make_response(body, status=200, headers=None):
    """
    Similar to :func:`werkzeug.wrappers.Response.get_response` but the
    response object is not writable.
    """
    if not isinstance(body, werkzeug.datastructures.Response):
        body = werkzeug.datastructures.Response(body)

    if not headers:
        headers = []

    if isinstance(body, werkzeug.datastructures.FileStorage):
        response = werkzeug.wrappers.Response(body, status, headers)
    else:
        with tempfile.NamedTemporaryFile
----------
Solution: 

    class FakeRequest(object):
        method = 'GET'
        path = request.path
        headers = request.headers
        GET = request.GET
        POST = request.POST
        user = getattr(request, 'user', None)
        cookies = request.cookies
        is_xhr = request.is_xhr
    return FakeRequest()
--------------------
Prompt: 
def a_not_committed(ctx):
    """Provide the message that current software is not committed and reload is not possible."""
----------
Output: 

    return ctx.message.get("message", {}).get("reload", False)


def a_not_loaded(ctx):
    """Provide the message that current user is not loaded."""
    return ctx.message.get("message", {}).get("not_loaded", False)


def a_no_data(ctx):
    """Provide the message that current user has no data."""
    return ctx.message.get("message", {}).get("data", False)


def a_no_data_available(ctx):
    """Provide the message that current user has no data available."""
    return ctx
----------
Solution: 

    ctx.ctrl.sendline('n')
    ctx.msg = "Some active software packages are not yet committed. Reload may cause software rollback."
    ctx.device.chain.connection.emit_message(ctx.msg, log_level=logging.ERROR)
    ctx.failed = True
    return False
--------------------
Prompt: 
def _log_length_error(self, key, length):
        """ Helper function for logging a response length error. """
----------
Output: 

        log.error(u"Response length error: %s (length: %s)" % (key, length))

    def _parse_response_headers(self, response):
        """ Parses the response for errors and returns the data in a dict. """
        self.title = response['title']
        self.body = response['body']
        self.status_code = response['status_code']
        self.status_message = response['status_message']
        self.retry_after = response['retry_after']
        self.retry_after_count = response['retry_after_count']
----------
Solution: 

        extra = {
            "max_detail_length": settings.defaults["max_detail_length"],
            "len": length
        }
        if self.key_name:
            extra[self.key_name] = key
        msg = "Length of data in %s is too long." % self.__class__.__name__
        log.error(msg, extra=extra)
--------------------
Prompt: 
def _check_values(in_values):
        """ Check if values need to be converted before they get mogrify'd
        """
----------
Output: 

        if isinstance(in_values, dict):
            for key, val in in_values.items():
                if key == 'name':
                    if val == 'default':
                        return True
                    else:
                        return False
                elif key == 'id':
                    if val == 'default':
                        return True
                    else:
                        return False
                elif key == 'description':
                    if val == 'default':
                        return True
                    else:
                        return False
                elif key == 'type':
                    if val == 'default':
                        return True
                    else:
                        return False
            return True
        else:
            return False

    def _get_
----------
Solution: 

        out_values = []
        for value in in_values:
            # if isinstance(value, (dict, list)):
            #     out_values.append(json.dumps(value))
            # else:
            out_values.append(value)

        return tuple(out_values)
--------------------
Prompt: 
def are_you_sure(flag_changed, evt, parent=None, title="File has been changed",
                 msg="Are you sure you want to exit?"):
    """
    "Are you sure you want to exit" question dialog.

    If flag_changed, shows question dialog. If answer is not yes, calls evt.ignore()

    Arguments:
      flag_changed
      evt -- QCloseEvent instance
      parent=None -- parent form, used to centralize the question dialog at
      title -- title for question dialog
      msg -- text of question dialog

    Returns True or False. True means: "yes, I want to exit"
    """
----------
Output: 
 close"
    """
    if parent is None:
        parent = self.form
        self.setParent(parent)
    if not flag_changed:
        self.form.close()
        return False
    if not self.isVisible():
        self.hide()
        return False
    if not flag_changed:
        self.setFocus()
        return False
    if not self.is_file_loaded:
        self.form.load(self.model.filePath(self.model.fileName()))
    return self.ignore()


def show_question(parent, title="File has been loaded", msg="File
----------
Solution: 

    if flag_changed:
        r = QMessageBox.question(parent, title, msg,
             QMessageBox.Yes|QMessageBox.No, QMessageBox.Yes)
        if r != QMessageBox.Yes:
            evt.ignore()
--------------------
Prompt: 
def str_with_sizes(self, max_name, max_remote_id, max_size):
        """
        Create string for report based on internal properties using sizes to line up columns.
        :param max_name: int width of the name column
        :param max_remote_id: int width of the remote_id column
        :return: str info from this report item
        """
----------
Output: 

        return self.name + " " + str(max_name) + " " + str(max_size)

    def __str__(self):
        """
        String representation of this report.
        :return: str info from this report item
        """
        return self.name + " " + str(self.size)

    def __repr__(self):
        """
        String representation of this report.
        :return: str info from this report item
        """
        return self.name + " " + str(self.size)


class ReportItem(object):
    """
    A report item that is used
----------
Solution: 

        name_str = self.name.ljust(max_name)
        remote_id_str = self.remote_id.ljust(max_remote_id)
        size_str = self.size.ljust(max_size)
        return u'{}    {}    {}    {}'.format(name_str, remote_id_str, size_str, self.file_hash)
--------------------
Prompt: 
def _handle_timeout(self, msg_id, start_time):
        """Handle a timed-out callback request.

        Parameters
        ----------
        msg_id : uuid.UUID for message
            The name of the reply which was expected.

        """
----------
Output: 

        self.logger.info('Received request for %s: %s', msg_id, msg)
        self._last_msg_id = msg_id
        self._last_msg_name = msg.name
        self.logger.info('  - last_msg_name: %s', self._last_msg_name)
        self.logger.info('  - last_msg_id: %s', msg.id)
        self.logger.info('  - start_time: %s', start_time)
        self.logger.info('  - self.name: %
----------
Solution: 

        msg, reply_cb, inform_cb, user_data, timeout_handle = \
            self._pop_async_request(msg_id, None)
        # We may have been racing with the actual reply handler if the reply
        # arrived close to the timeout expiry,
        # which means the self._pop_async_request() call gave us None's.
        # In this case, just bail.
        #
        # NM 2014-09-17 Not sure if this is true after porting to tornado,
        # but I'm too afraid to remove this code :-/
        if timeout_handle is None:
            return

        reason = "Request {0.name} timed out after {1:f} seconds.".format(
            msg, self.ioloop.time() - start_time)
        self._do_fail_callback(
            reason, msg, reply_cb, inform_cb, user_data, timeout_handle)
--------------------
Prompt: 
def retryable(fn, protocol=None):
    """
    Wraps a client operation that can be retried according to the set
    :attr:`RiakClient.retries`. Used internally.
    """
----------
Output: 

    def wrapped(self, *args, **kwargs):
        if self.retries is not None:
            self.RiakClient.retries -= 1
            if self.RiakClient.retries == 0:
                raise RiakError("Riak client operation failed")
        return fn(self, *args, **kwargs)
    return wrapped


class RiakClient(object):
    """
    A :class:`RiakClient` object is a context manager that uses the
    :class:`RiakClient.get_context` method to get a context from the server.
    """

    def __init
----------
Solution: 

    def wrapper(self, *args, **kwargs):
        pool = self._choose_pool(protocol)

        def thunk(transport):
            return fn(self, transport, *args, **kwargs)

        return self._with_retries(pool, thunk)

    wrapper.__doc__ = fn.__doc__
    wrapper.__repr__ = fn.__repr__

    return wrapper
--------------------
Prompt: 
def _send(self):
        """ Send the webhook method """
----------
Output: 

        if self.token:
            self.send_webhook(self.token)
        else:
            self.send_webhook(self.url)
        return self.send_response()

    def send_response(self):
        """ Send the webhook response """
        if self.token:
            self.token = self._send()
        if self.token:
            self.send_webhook(self.url, response=self.response)
        else:
            self.send_webhook(self.url)
        return self.send_response_from_file()

    def send_response_from_file(
----------
Solution: 


        payload = self.payload
        sending_metadata = {'success': False}
        post_attributes = {'timeout': self.timeout}

        if self.custom_headers:
            post_attributes['headers'] = self.custom_headers
        if not post_attributes.get('headers', None):
            post_attributes['headers'] = {}
        post_attributes['headers']['Content-Type'] = self.encoding

        post_attributes['data'] = self.format_payload()

        if self.signing_secret:
            post_attributes['headers']['x-hub-signature'] = self.create_signature(post_attributes['data'], \
                                                                                  self.signing_secret)

        for i, wait in enumerate(range(len(self.attempts) - 1)):

            self.attempt = i + 1
            sending_metadata['attempt'] = self.attempt

            try:
                print(self.url)
                self.response = requests.post(self.url, **post_attributes)

                if sys.version > '3':
                    # Converts bytes object to str object in Python 3+
                    self.response_content = self.response.content.decode('utf-8')
                else:
                    self.response_content = self.response.content

                sending_metadata['status_code'] = self.response.status_code

                # anything with a 200 status code  is a success
                if self.response.status_code >= 200 and self.response.status_code < 300:
                    # Exit the sender method.  Here we provide the payload as a result.
                    #   This is useful for reporting.
                    self.notify("Attempt {}: Successfully sent webhook {}".format(
                        self.attempt, self.hash_value)
                    )
                    sending_metadata['response'] = self.response_content
                    sending_metadata['success'] = True
                    break
                else:
                    self.error = "Status code (%d). Message: %s" % (self.response.status_code, self.response.text)


            except Exception as ex:
                err_formatted = str(ex).replace('"',"'")
                sending_metadata['response'] = '{"status_code": 500, "status":"failure","error":"'+err_formatted+'"}'
                self.error = err_formatted

            self.notify("Attempt {}: Could not send webhook {}".format(
                    self.attempt, self.hash_value)
            )
            self.notify_debug("Webhook {}. Body: {}".format(
                    self.hash_value, self.payload)
            )

            # If last attempt
            if self.attempt == (len(self.attempts) - 1):
                self.notify_error("Failed to send webhook {}. Body: {}".format(
                    self.hash_value, self.payload)
                )
            else:
                # Wait a bit before the next attempt
                sleep(wait)

        sending_metadata['error'] = None if sending_metadata['success'] or not self.error else self.error
        sending_metadata['post_attributes'] = post_attributes
        merged_dict = sending_metadata.copy()
        if isinstance(payload, string_types):
            payload = {'payload': payload}

        # Add the hash value if there is one.
        if self.hash_value is not None and len(self.hash_value) > 0:
            payload['hash'] = self.hash_value

        merged_dict.update(payload)
        return merged_dict
--------------------
Prompt: 
def attention(query: torch.Tensor,
              key: torch.Tensor,
              value: torch.Tensor,
              mask: torch.Tensor = None,
              dropout: Callable = None) -> Tuple[torch.Tensor, torch.Tensor]:
    """Compute 'Scaled Dot Product Attention'"""
----------
Output: 

    if mask is not None:
        query = query.masked_fill(mask)
    if dropout is not None:
        query = dropout(query)
    return query, value, mask, mask, mask


def attention_layer(query: torch.Tensor,
                    key: torch.Tensor,
                    value: torch.Tensor,
                    mask: torch.Tensor = None,
                    dropout: Callable = None) -> Tuple[torch.Tensor, torch.Tensor]:
    """Compute 'Dot Product Attention Layer'"""
    if mask is not None:
        query = query.masked_fill(mask)
    if dropout
----------
Solution: 

    d_k = query.size(-1)
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    p_attn = F.softmax(scores, dim=-1)
    if dropout is not None:
        p_attn = dropout(p_attn)
    return torch.matmul(p_attn, value), p_attn
--------------------
Prompt: 
def ToCsv(self, columns_order=None, order_by=(), separator=","):
    """Writes the data table as a CSV string.

    Output is encoded in UTF-8 because the Python "csv" module can't handle
    Unicode properly according to its documentation.

    Args:
      columns_order: Optional. Specifies the order of columns in the
                     output table. Specify a list of all column IDs in the order
                     in which you want the table created.
                     Note that you must list all column IDs in this parameter,
                     if you use it.
      order_by: Optional. Specifies the name of the column(s) to sort by.
                Passed as is to _PreparedData.
      separator: Optional. The separator to use between the values.

    Returns:
      A CSV string representing the table.
      Example result:
       'a','b','c'
       1,'z',2
       3,'w',''

    Raises:
      DataTableException: The data does not match the type.
    """
----------
Output: 
 the column(s) to sort by.
                If not provided, the default sort order is used.
      separator: Optional. Specifies the separator to use between columns.
    """
    if columns_order is None:
      columns_order = self.GetColumnOrder()
    if order_by is None:
      order_by = self.GetColumnName()

    if columns_order:
      self.WriteCSVLine(separator)

    if order_by:
      self.WriteCSVLine(separator)

    if separator:
      self.WriteCSVLine(separator)

    if order_by:
      self.WriteCSV
----------
Solution: 


    csv_buffer = six.StringIO()
    writer = csv.writer(csv_buffer, delimiter=separator)

    if columns_order is None:
      columns_order = [col["id"] for col in self.__columns]
    col_dict = dict([(col["id"], col) for col in self.__columns])

    def ensure_str(s):
      "Compatibility function. Ensures using of str rather than unicode."
      if isinstance(s, str):
        return s
      return s.encode("utf-8")

    writer.writerow([ensure_str(col_dict[col]["label"])
                     for col in columns_order])

    # We now go over the data and add each row
    for row, unused_cp in self._PreparedData(order_by):
      cells_list = []
      # We add all the elements of this row by their order
      for col in columns_order:
        value = ""
        if col in row and row[col] is not None:
          value = self.CoerceValue(row[col], col_dict[col]["type"])
        if isinstance(value, tuple):
          # We have a formatted value. Using it only for date/time types.
          if col_dict[col]["type"] in ["date", "datetime", "timeofday"]:
            cells_list.append(ensure_str(self.ToString(value[1])))
          else:
            cells_list.append(ensure_str(self.ToString(value[0])))
        else:
          cells_list.append(ensure_str(self.ToString(value)))
      writer.writerow(cells_list)
    return csv_buffer.getvalue()
--------------------
Prompt: 
def catalog(self):
        """Primary registered catalog for the wrapped portal type
        """
----------
Output: 

        return self._catalog

    @property
    def name(self):
        """The name of the wrapped portal type
        """
        return self._name

    @property
    def description(self):
        """Description of the wrapped portal type
        """
        return self._description

    @property
    def version(self):
        """Version of the wrapped portal type
        """
        return self._version

    @property
    def is_active(self):
        """Whether the portal is currently active
        """
        return self._is_active

    @property
    def is_enabled(self):
        """Whether the portal is enabled
        """
        return
----------
Solution: 

        if self._catalog is None:
            logger.debug("SuperModel::catalog: *Fetch catalog*")
            self._catalog = self.get_catalog_for(self.brain)
        return self._catalog
--------------------
Prompt: 
def get_ip_prefixes_from_bird(filename):
    """Build a list of IP prefixes found in Bird configuration.

    Arguments:
        filename (str): The absolute path of the Bird configuration file.

    Notes:
        It can only parse a file with the following format

            define ACAST_PS_ADVERTISE =
                [
                    10.189.200.155/32,
                    10.189.200.255/32
                ];

    Returns:
        A list of IP prefixes.

    """
----------
Output: 

    ip_prefixes = []
    with open(filename, 'r') as f:
        for line in f:
            if line.startswith('define'):
                if line.startswith('type'):
                    ip_prefixes.append(line.split()[1])
                elif line.startswith('value'):
                    ip_prefixes.append(line.split()[1])
    return ip_prefixes


def get_ip_prefixes_from_ip_address(ip_address):
    """Build a list of IP prefixes found in the given IP address.

    Arguments:
        ip_address (str): The IP address of the
----------
Solution: 

    prefixes = []
    with open(filename, 'r') as bird_conf:
        lines = bird_conf.read()

    for line in lines.splitlines():
        line = line.strip(', ')
        if valid_ip_prefix(line):
            prefixes.append(line)

    return prefixes
--------------------
Prompt: 
def cd(dir_name):
    """
    do something in other directory and return back after block ended
    """
----------
Output: 

    if not os.path.isdir(dir_name):
        raise Exception("Directory %s does not exist" % dir_name)

    for root, dirs, files in os.walk(dir_name):
        for dir_name in dirs:
            if not cd(root + dir_name):
                return False
        for file in files:
            if not cd(root + file):
                return False
    return True


def get_all_files(dir_name):
    """
    get all the files in a directory
    """
    return [os.path.join(root, file) for file
----------
Solution: 

    old_path = os.path.abspath('.')
    os.chdir(dir_name)
    try:
        yield
        os.chdir(old_path)
    except Exception:
        os.chdir(old_path)
        raise
--------------------
Prompt: 
def queryByPortSensor(portiaConfig, edgeId, port, sensor, last=False, params={ 'from': None, 'to': None, 'order': None, 'precision': 'ms', 'limit': None }):
    """Returns a pandas data frame with the portia select resultset"""
----------
Output: 

    if not last:
        last = True
    if not edgeId:
        edgeId = port
    if last:
        return _query_by_port(portiaConfig, sensor, edgeId, params)
    else:
        return _query_by_port(portiaConfig, sensor, edgeId, params)

def _query_by_port(portiaConfig, sensor, edgeId, params):
    """Returns pandas data frame with the portia select resultset"""
    if not last:
        edgeId = sensor.getEdgeId()
    if not edgeId:
        return _query_
----------
Solution: 


    header = {'Accept': 'text/csv'}

    if last == False:
        endpoint = '/select/device/{0}/port/{1}/sensor/{2}{3}'.format( edgeId, port, sensor, utils.buildGetParams(params) )
    else:
        endpoint = '/select/device/{0}/port/{1}/sensor/{2}/last{3}'.format( edgeId, port, sensor, utils.buildGetParams(params) )

    response = utils.httpGetRequest(portiaConfig, endpoint, header)

    if response.status_code == 200:
        try:

            dimensionSeries = pandas.read_csv( StringIO(response.text), sep=';' )
            if portiaConfig['debug']:
                print( '[portia-debug]: {0} rows'.format( len(dimensionSeries.index) ) )

            return dimensionSeries

        except:
            raise Exception('couldn\'t create pandas data frame')
    else:
        raise Exception('couldn\'t retrieve data')
--------------------
Prompt: 
def related_archives(self):
        """
        The pathnames of the source distribution(s) for this requirement (a list of strings).

        .. note:: This property is very new in pip-accel and its logic may need
                  some time to mature. For now any misbehavior by this property
                  shouldn't be too much of a problem because the pathnames
                  reported by this property are only used for cache
                  invalidation (see the :attr:`last_modified` and
                  :attr:`checksum` properties).
        """
----------
Output: 

        return self._related_archives

    @property
    def has_hash(self):
        """
        Whether this requirement has a hash (a string) across
        all its dependencies.

       .. versionadded:: 1.1
        """
        return self._hash is not None

    @property
    def has_build_tag(self):
        """
        Whether this requirement has a build tag (a string) across all its
        dependencies.

       .. versionadded:: 1.1
        """
        return self._build_tag is not None

    @property
    def has_ext_modules(self):
        """
        Whether this requirement is an extension
----------
Solution: 

        # Escape the requirement's name for use in a regular expression.
        name_pattern = escape_name(self.name)
        # Escape the requirement's version for in a regular expression.
        version_pattern = re.escape(self.version)
        # Create a regular expression that matches any of the known source
        # distribution archive extensions.
        extension_pattern = '|'.join(re.escape(ext) for ext in ARCHIVE_EXTENSIONS if ext != '.whl')
        # Compose the regular expression pattern to match filenames of source
        # distribution archives in the local source index directory.
        pattern = '^%s-%s(%s)$' % (name_pattern, version_pattern, extension_pattern)
        # Compile the regular expression for case insensitive matching.
        compiled_pattern = re.compile(pattern, re.IGNORECASE)
        # Find the matching source distribution archives.
        return [os.path.join(self.config.source_index, fn)
                for fn in os.listdir(self.config.source_index)
                if compiled_pattern.match(fn)]
--------------------
Prompt: 
def find_phase_transformation(Ne, Nl, r, Lij, verbose=0,
                              return_equations=False, **kwds):
    """This function returns a phase transformation specified as a list of
    lenght Ne whose elements correspond to each theta_i. Each element is a
    list of length Nl which specifies the coefficients multiplying each
    optical frequency omega^l. So for instance [[1,1],[1,0],[0,0]] means

    theta_1=omega^1+omega^2
    theta_2=omega^1
    theta_3=0.
    """
----------
Output: 
 """
    # Find the number of frequencies
    Nf = len(Ne)
    # Find the coefficients
    if verbose:
        print('find_phase_transformation')
    # Find the optical frequency
    if len(r) == Nf:
        if len(kwds) == Nf:
                return_optical_frequency = kwds[Nf]
            else:
                return_optical_frequency = [0.0 for i in range(len(Ne[0]))]
        else:
                return_optical_frequency = [0.0 for i in range(len(Ne[0]))]
----------
Solution: 

    # We first define the needed variables
    _omega_laser = [Symbol('omega_laser'+str(l+1)) for l in range(Nl)]
    _theta = [Symbol('theta'+str(i+1)) for i in range(Ne)]

    # We find all the equations that the specified problem has to fulfil.
    eqs = []
    for i in range(Ne):
        for j in range(i+1, Ne):
            if type(r[0]) == list:
                if (r[0][i][j] != 0) or (r[1][i][j] != 0) or (r[2][i][j] != 0):
                    for l in range(Nl):
                        if l+1 in Lij[i][j]:
                            eqs += [_omega_laser[l] + _theta[j] - _theta[i]]
            else:
                if (r[0][i, j] != 0) or (r[1][i, j] != 0) or (r[2][i, j] != 0):
                    for l in range(Nl):
                        if l+1 in Lij[i][j]:
                            eqs += [_omega_laser[l] + _theta[j] - _theta[i]]

    if return_equations:
        return eqs

    sol = solve(eqs, _theta, dict=True)[0]
    for i in range(Ne):
        if _theta[i] not in sol.keys():
            sol.update({_theta[i]: _theta[i]})

    sol_simple = {_theta[i]: sol[_theta[i]]-sol[_theta[-1]] for i in range(Ne)}

    sol = []
    for i in range(Ne):
        soli = []
        for l in range(Nl):
            soli += [diff(sol_simple[_theta[i]], _omega_laser[l])]
        sol += [soli]

    return sol
--------------------
Prompt: 
def profile(self):
        """
        Buffered result of :meth:`build_profile`
        """
----------
Output: 

        return self.build_profile(self.profile_name, self.profile_version)

    def build_profile(self, profile_name, profile_version):
        """
        Builds a :class:`Profile` object
        """
        return Profile(self, profile_name, profile_version, self.profile_path)

    def get_profile_path(self):
        """
        Returns the path to the profile file
        """
        return self.profile_path

    def get_version(self):
        """
        Returns the version of the current profile
        """
        return self.profile_version

    def
----------
Solution: 

        if self._profile is None:
            self._profile = self.build_profile()
        return self._profile
--------------------
Prompt: 
def symlink_create(self, symlink, target, type_p):
        """Creates a symbolic link in the guest.

        in symlink of type str
            Path to the symbolic link that should be created.  Guest path
            style.

        in target of type str
            The path to the symbolic link target.  If not an absolute, this will
            be relative to the @a symlink location at access time.  Guest path
            style.

        in type_p of type :class:`SymlinkType`
            The symbolic link type (mainly for Windows). See :py:class:`SymlinkType` 
            for more information.

        raises :class:`OleErrorNotimpl`
            The method is not implemented yet.
        
        """
----------
Output: 
 for more details.

       .. versionadded:: 0.3.0

        """
        if isinstance(target, SymlinkTarget):
            if not self.exists():
                raise RuntimeError("File %s does not exist" % self.path)
            if not os.path.isabs(target):
                target = os.path.join(self.path, target)
            if not os.path.exists(os.path.dirname(symlink)):
                os.makedirs(os.path.dirname(symlink))
            if not os.path.exists(os.path.join(self.path, 'bin', 'win
----------
Solution: 

        if not isinstance(symlink, basestring):
            raise TypeError("symlink can only be an instance of type basestring")
        if not isinstance(target, basestring):
            raise TypeError("target can only be an instance of type basestring")
        if not isinstance(type_p, SymlinkType):
            raise TypeError("type_p can only be an instance of type SymlinkType")
        self._call("symlinkCreate",
                     in_p=[symlink, target, type_p])
--------------------
Prompt: 
def open_fastq(in_file):
    """ open a fastq file, using gzip if it is gzipped
    from bcbio package
    """
----------
Output: 

    if in_file.endswith(".gz"):
        return gzip.open(in_file, "rt")
    else:
        return open(in_file, "rt")


def _is_fastq_valid_file(in_file):
    """ check if a fastq file is valid
    """
    if not _is_valid_fastq(in_file):
        return False
    return True


def _is_fastq_valid_file_or_url(in_file):
    """ check if a fastq file is valid
    """
    if not _is_valid_file(in
----------
Solution: 

    _, ext = os.path.splitext(in_file)
    if ext == ".gz":
        return gzip.open(in_file, 'rb')
    if ext in [".fastq", ".fq", ".fasta", ".fa"]:
        return open(in_file, 'r')
    return ValueError("File needs to be fastq|fasta|fq|fa [.gz]")
--------------------
Prompt: 
def IOR(classical_reg1, classical_reg2):
    """
    Produce an inclusive OR instruction.

    :param classical_reg1: The first classical register, which gets modified.
    :param classical_reg2: The second classical register or immediate value.
    :return: A ClassicalOr instance.
    """
----------
Output: 

    return IORInstruction(classical_reg1, classical_reg2)


def IORInstruction(classical_reg1, classical_reg2):
    """
    Produce an instruction.

    :param classical_reg1: The first classical register, which gets modified.
    :param classical_reg2: The second classical register or immediate value.
    :return: A ClassicalOr instance.
    """
    return IORInstruction(classical_reg1, classical_reg2)


class IORInstructionFactory(object):
    """
    Factory for
----------
Solution: 

    left, right = unpack_reg_val_pair(classical_reg1, classical_reg2)
    return ClassicalInclusiveOr(left, right)
--------------------
Prompt: 
def resolver(self, vocab_data, attribute):
        """Pull the requested attribute based on the given vocabulary
        and content.
        """
----------
Output: 

        if attribute in vocab_data:
            return vocab_data[attribute]
        return None

    def _get_vocab_data(self, attribute):
        """Get the requested attribute based on the given attribute.
        """
        if attribute in self.vocab_data:
            return self.vocab_data[attribute]
        return None

    def _get_attribute_from_dict(self, dict_name, attribute):
        """Get the requested attribute from a dictionary.
        """
        if dict_name in dict_to_attr:
            return dict_to_attr[dict_name][attribute]
        return
----------
Solution: 

        term_list = vocab_data.get(self.content_vocab, [])
        # Loop through the terms from the vocabulary.
        for term_dict in term_list:
            # Match the name to the current content.
            if term_dict['name'] == self.content:
                return term_dict[attribute]
        return self.content
--------------------
Prompt: 
def mappedPolygon(self, polygon, path=None, percent=0.5):
        """
        Maps the inputed polygon to the inputed path \
        used when drawing items along the path.  If no \
        specific path is supplied, then this object's own \
        path will be used.  It will rotate and move the \
        polygon according to the inputed percentage.
        
        :param      polygon     <QPolygonF>
        :param      path        <QPainterPath>
        :param      percent     <float>
        
        :return     <QPolygonF> mapped_poly
        """
----------
Output: 

        if path is None:
            path = self.path()
        if path.isEmpty():
            return QPainterPath()
        if path.isClipPath():
            path = clip_to_rect(path)
        if path.isNull():
            return QPainterPath()
        if path.contains(path.arc(polygon.center().x(), polygon.center().y(), percent*2)) or \
           path.contains(path.line(polygon.center().x(), polygon.center().y(), percent*2)) or \
           path.contains(path.arc(polygon.center().x(),
----------
Solution: 

        translatePerc = percent
        anglePerc = percent

        # we don't want to allow the angle percentage greater than 0.85
        # or less than 0.05 or we won't get a good rotation angle
        if 0.95 <= anglePerc:
            anglePerc = 0.98
        elif anglePerc <= 0.05:
            anglePerc = 0.05

        if not path:
            path = self.path()
        if not (path and path.length()):
            return QPolygonF()

        # transform the polygon to the path
        point = path.pointAtPercent(translatePerc)
        angle = path.angleAtPercent(anglePerc)

        # rotate about the 0 axis
        transform = QTransform().rotate(-angle)
        polygon = transform.map(polygon)

        # move to the translation point
        transform = QTransform().translate(point.x(), point.y())

        # create the rotated polygon
        mapped_poly = transform.map(polygon)
        self._polygons.append(mapped_poly)

        return mapped_poly
--------------------
Prompt: 
def p_with_statement(self, p):
        """with_statement : WITH LPAREN expr RPAREN statement"""
----------
Output: 

        p[0] = self.handleChildren(p[2], p[4])

    def p_with_statement_error(self, p):
        """with_statement : WITH LBRACE expr RBRACE statement_error"""
        p[0] = self.handleChildren(p[2], p[4], p[6])

    def p_with_statement_error_in_block(self, p):
        """with_statement : WITH LPAREN expr RPAREN statement_error"""
        p[0] = self.handleChildren(p[2], p[4], p[6
----------
Solution: 

        p[0] = self.asttypes.With(expr=p[3], statement=p[5])
        p[0].setpos(p)
--------------------
Prompt: 
def hmac_hex_key(self, hmac_hex_key):
        """
        Sets the hmac_hex_key of this CfsslAuthCredentials.
        The key that is used to compute the HMAC of the request using the HMAC-SHA-256 algorithm. Must contain an even number of hexadecimal characters. 

        :param hmac_hex_key: The hmac_hex_key of this CfsslAuthCredentials.
        :type: str
        """
----------
Output: 

        self._hmac_sha256 = hmac_sha256
        return self
    
    def set_hmac_sha256(self, hmac_sha256):
        """
        Sets the hmac_sha256 of this CfsslAuthCredentials.
        The key that is used to compute the HMAC of the request using the HMAC-SHA-256 algorithm. Must contain an even number of hexadecimal characters. 

        :param hmac_sha256: The hmac_sha256 of this CfsslAuthCredentials.
        :type: str
        """
        self._hmac_sha256 = hmac_sha256
        return self
    
    def get_hmac_
----------
Solution: 

        if hmac_hex_key is None:
            raise ValueError("Invalid value for `hmac_hex_key`, must not be `None`")
        if hmac_hex_key is not None and len(hmac_hex_key) > 64:
            raise ValueError("Invalid value for `hmac_hex_key`, length must be less than or equal to `64`")
        if hmac_hex_key is not None and not re.search('^([a-fA-F0-9][a-fA-F0-9]){1,32}$', hmac_hex_key):
            raise ValueError("Invalid value for `hmac_hex_key`, must be a follow pattern or equal to `/^([a-fA-F0-9][a-fA-F0-9]){1,32}$/`")

        self._hmac_hex_key = hmac_hex_key
--------------------
Prompt: 
def sorted_feed_cols(df):
    """
    takes a dataframe's columns that would be of the form:
    ['feed003', 'failsafe_feed999', 'override_feed000', 'feed001', 'feed002']
    and returns:
    ['override_feed000', 'feed001', 'feed002', 'feed003', 'failsafe_feed999']
    """
----------
Output: 

    return df.sort_values('feed_id', ascending=False)


def get_feed_ids(df):
    """
    takes a dataframe's columns that would be of the form:
    ['feed003', 'failsafe_feed999', 'override_feed000', 'feed001', 'feed002']
    and returns:
    ['failsafe_feed000', 'failsafe_feed999', 'override_feed000', 'failsafe_feed001', 'failsafe_feed002']
    """
    return sorted_feed_cols(df)


def get_feed_ids_
----------
Solution: 

    cols = df.columns
    ind = [int(c.split("feed")[1]) for c in cols]
    cols = zip(ind,cols)
    cols.sort()
    cols = [c[1] for c in cols]
    return cols
--------------------
Prompt: 
def extract_domain(host):
    """
    Domain name extractor. Turns host names into domain names, ported
    from pwdhash javascript code"""
----------
Output: 

    return re.sub(r'[^a-zA-Z0-9]', '', host)


def extract_id(host):
    """
    Extracts the id from the javascript code and returns it as a string
    """
    return re.sub(r'id([0-9]+)', '', host)


def extract_name(host):
    """
    Extracts the name from the javascript code and returns it as a string
    """
    return re.sub(r'[^a-zA-Z0-9]', '', host)


def extract_value(value):
    """
    Extracts the
----------
Solution: 

    host = re.sub('https?://', '', host)
    host = re.match('([^/]+)', host).groups()[0]
    domain = '.'.join(host.split('.')[-2:])
    if domain in _domains:
        domain = '.'.join(host.split('.')[-3:])
    return domain
--------------------
Prompt: 
def process_table(fname):
    """Return processor by processing a given sheet of a spreadsheet file.

    Parameters
    ----------
    fname : str
        The name of the Excel file (typically .xlsx extension) to process

    Returns
    -------
    sp : indra.sources.sofia.processor.SofiaProcessor
        A SofiaProcessor object which has a list of extracted INDRA
        Statements as its statements attribute.
    """
----------
Output: 

    with open(fname, 'r') as f:
        reader = csv.reader(f, delimiter='\t')
        for row in reader:
            if row[0] =='state':
                yield row[1:]

    return


def process_sheet(fname):
    """Return processor by processing a given sheet of a spreadsheet file.

    Parameters
    ----------
    fname : str
        The name of the spreadsheet file (typically.xlsx extension) to process

    Returns
    -------
    sp : indra.sources.sofia.processor.SofiaProcessor
        A SofiaProcessor object which has a
----------
Solution: 

    book = openpyxl.load_workbook(fname, read_only=True)
    try:
        rel_sheet = book['Relations']
    except Exception as e:
        rel_sheet = book['Causal']
    event_sheet = book['Events']
    entities_sheet = book['Entities']
    sp = SofiaExcelProcessor(rel_sheet.rows, event_sheet.rows,
                             entities_sheet.rows)
    return sp
--------------------
Prompt: 
def demean(in_file, in_mask, only_mask=False, newpath=None):
    """Demean ``in_file`` within the mask defined by ``in_mask``"""
----------
Output: 

    if newpath is None:
        newpath = in_file
    if only_mask:
        out_file = in_file + ".demean"
    else:
        out_file = in_file + ".demean.txt"
    with open(out_file, 'w') as out_f:
        for line in open(in_file):
            line = line.strip()
            if line == "" or line[0] == "#":
                continue
            line = line.split()
            if newpath is not None:
                out_f.write(newpath + "\n")
            else:
               
----------
Solution: 

    import os
    import numpy as np
    import nibabel as nb
    from nipype.utils.filemanip import fname_presuffix

    out_file = fname_presuffix(in_file, suffix='_demeaned',
                               newpath=os.getcwd())
    nii = nb.load(in_file)
    msk = nb.load(in_mask).get_data()
    data = nii.get_data()
    if only_mask:
        data[msk > 0] -= np.median(data[msk > 0])
    else:
        data -= np.median(data[msk > 0])
    nb.Nifti1Image(data, nii.affine, nii.header).to_filename(
        out_file)
    return out_file
--------------------
Prompt: 
def filter_permissions(self, search):
        """Filter given query based on permissions of the user in the request.

        :param search: ElasticSearch query object

        """
----------
Output: 

        if not search.query_key:
            return
        if not search.query_key.startswith('search.'):
            return
        if not search.query_key.startswith('search.'):
            return
        if search.query_key.startswith('search.'):
            search.query_key ='search.' + search.query_key[len('search.'):]
        if search.query_key.startswith('search.'):
            search.query_key ='search' + search.query_key[len('search.'):]
        if search.query_key in self.request.session:
            del self.request
----------
Solution: 

        user = self.request.user
        if user.is_superuser:
            return search
        if user.is_anonymous:
            user = get_anonymous_user()

        filters = [Q('match', users_with_permissions=user.pk)]
        filters.extend([
            Q('match', groups_with_permissions=group.pk) for group in user.groups.all()
        ])
        filters.append(Q('match', public_permission=True))

        # `minimum_should_match` is set to 1 by default
        return search.query('bool', should=filters)
--------------------
Prompt: 
def colors_to_needle_positions(rows):
    """Convert rows to needle positions.

    :return:
    :rtype: list
    """
----------
Output: 

    return [
        (
            row["name"],
            row["color"],
            row["position"]) for row in rows]


def get_needle_positions(db_session):
    """Get the list of needle positions.

    :param db_session:
    :type db_session: sqlalchemy.orm.session.Session
    :return:
    :rtype: list
    """
    needle_positions = db_session.query(
        func.count(func.distinct(NeedlePosition.name))
    ).filter(
        NeedlePosition.color == 'blue',
        NeedlePosition.position
----------
Solution: 

    needles = []
    for row in rows:
        colors = set(row)
        if len(colors) == 1:
            needles.append([NeedlePositions(row, tuple(colors), False)])
        elif len(colors) == 2:
            color1, color2 = colors
            if color1 != row[0]:
                color1, color2 = color2, color1
            needles_ = _row_color(row, color1)
            needles.append([NeedlePositions(needles_, (color1, color2), True)])
        else:
            colors = []
            for color in row:
                if color not in colors:
                    colors.append(color)
            needles_ = []
            for color in colors:
                needles_.append(NeedlePositions(_row_color(row, color),
                                                (color,), False))
            needles.append(needles_)
    return needles
--------------------
Prompt: 
def _decode_datetime(obj):
    """Decode a msgpack'ed datetime."""
----------
Output: 

    if isinstance(obj, datetime):
        return obj.isoformat()
    elif isinstance(obj, datetime_tz):
        return obj.isoformat() + "Z"
    elif isinstance(obj, timedelta):
        return str(obj)
    elif isinstance(obj, Decimal):
        return str(obj)
    elif isinstance(obj, uuid.UUID):
        return str(obj)
    else:
        raise TypeError("Unsupported type: %s" % type(obj))


def _decode_timedelta(obj):
    """Decode a msgpack'ed timedelta."""
    if isinstance(obj, timedelta):
        return obj.total
----------
Solution: 


    if '__datetime__' in obj:
        obj = datetime.datetime.strptime(obj['as_str'].decode(), "%Y%m%dT%H:%M:%S.%f")
    return obj
--------------------
Prompt: 
def delete_plat_operator(operator, auth, url):
    """
    Function to set the password of an existing operator
    :param operator: str Name of the operator account

    :param auth: requests auth object #usually auth.creds from auth pyhpeimc.auth.class

    :param url: base url of IMC RS interface #usually auth.url from pyhpeimc.auth.authclass

    :return: int of 204 if successfull

    :rtype: int

    >>> from pyhpeimc.auth import *

    >>> from pyhpeimc.plat.operator import *

    >>> auth = IMCAuth("http://", "10.101.0.203", "8080", "admin", "admin")

    >>> success_delete_operator = delete_plat_operator('testadmin', auth.creds, auth.url)

    >>> assert type(success_delete_operator) is int

    >>> assert success_delete_operator == 204

    >>> fail_delete_operator = delete_plat_operator('testadmin', auth.creds, auth.url)

    >>> assert type(fail_delete_operator) is int

    >>> assert fail_delete_operator == 409

    """
----------
Output: 
 import *

    >>> delete_plat_operator('get_password', auth, 'http://localhost:8080/api/v1')
    200
    """
    if not isinstance(operator, str):
        raise TypeError('The parameter "operator" must be a string.')

    auth.creds = auth.creds or auth.url

    return delete_request(auth, url, operator, 'DELETE')


def delete_request(auth, url, method, data=None, action=None):
    """
    Function to delete an IMC RS request
    :param auth: requests auth object #usually auth.url
----------
Solution: 

    oper_id = None
    plat_oper_list = get_plat_operator(auth, url)
    for i in plat_oper_list:
        if operator == i['name']:
            oper_id = i['id']
        else:
            oper_id = None
    if oper_id is None:
        # print ("User does not exist")
        return 409
    f_url = url + "/imcrs/plat/operator/" + str(oper_id)
    response = requests.delete(f_url, auth=auth, headers=HEADERS)
    try:
        if response.status_code == 204:
            # print("Operator: " + operator +
            #  " was successfully deleted")
            return response.status_code
    except requests.exceptions.RequestException as error:
        return "Error:\n" + str(error) + ' delete_plat_operator: An Error has occured'
--------------------
Prompt: 
def bait(self):
        """
        Use bbduk to bait FASTQ reads from input files using the primer file as the target
        """
----------
Output: 

        return self.run_command('bbduk', self.primer_file)

    def bait_f(self, f):
        """
        Use bbduk to bait FASTQ reads from the primer file
        """
        return self.run_command('bbduk', f)

    def bait_g(self, g):
        """
        Use bbduk to bait GFF3 reads from the primer file
        """
        return self.run_command('bbduk', self.gff3_file)

    def bait_g_f(self,
----------
Solution: 

        with progressbar(self.metadata) as bar:
            for sample in bar:
                if sample.general.bestassemblyfile != 'NA':
                    # Only need to perform baiting on FASTQ files
                    if sample[self.analysistype].filetype == 'fastq':
                        # Make the system call - allow for single- or paired-end reads
                        if len(sample.general.fastqfiles) == 2:
                            # Create the command to run the baiting - ref: primer file, k: shortest primer length
                            # in1, in2: paired inputs, hdist: number of mismatches, interleaved: use interleaved output
                            # outm: single, zipped output file of reads that match the target file
                            sample[self.analysistype].bbdukcmd = \
                                'bbduk.sh ref={primerfile} k={klength} in1={forward} in2={reverse} ' \
                                'hdist={mismatches} threads={threads} interleaved=t outm={outfile}' \
                                .format(primerfile=self.formattedprimers,
                                        klength=self.klength,
                                        forward=sample.general.trimmedcorrectedfastqfiles[0],
                                        reverse=sample.general.trimmedcorrectedfastqfiles[1],
                                        mismatches=self.mismatches,
                                        threads=str(self.cpus),
                                        outfile=sample[self.analysistype].baitedfastq)
                        else:
                            sample[self.analysistype].bbdukcmd = \
                                'bbduk.sh ref={primerfile} k={klength} in={fastq} hdist={mismatches} ' \
                                'threads={threads} interleaved=t outm={outfile}' \
                                .format(primerfile=self.formattedprimers,
                                        klength=self.klength,
                                        fastq=sample.general.trimmedcorrectedfastqfiles[0],
                                        mismatches=self.mismatches,
                                        threads=str(self.cpus),
                                        outfile=sample[self.analysistype].baitedfastq)
                        # Run the system call (if necessary)
                        if not os.path.isfile(sample[self.analysistype].baitedfastq):
                            run_subprocess(sample[self.analysistype].bbdukcmd)
--------------------
Prompt: 
def profile_stats(adapter, threshold = 0.9):

    """
        Compares the pairwise hamming distances for all the sample profiles in
        the database. Returns a table of the number of distances within given
        ranges.

        Args:
            adapter (MongoAdapter): Adapter to mongodb
            threshold (float): If any distance is found above this threshold
                a warning will be given, stating the two matching samples.

        Returns:
            distance_dict (dict): dictionary with ranges as keys, and the number
                of distances that are within these ranges as values.

    """
----------
Output: 


    # Get the number of samples
    n = adapter.find_one(
       'stats',
        {
            'type': 'table',
            'database':'stats',
            'collection':'stats',
            'threshold': threshold
        }
    )

    # Get the number of matches
    n_matches = adapter.find(
       'stats',
        {
            'type': 'table',
            'database':'matches',
            'collection':'matches',
            'n':'stats_n'
        }
    )

    # Get the number of matches
    n_matches = adapter.find(
       'stats',
----------
Solution: 

    profiles = []
    samples = []

    #Instatiate the distance dictionary with a count 0 for all the ranges
    distance_dict = {key: 0 for key in HAMMING_RANGES.keys()}

    for case in adapter.cases():

        for individual in case['individuals']:

            if individual.get('profile'):
                #Make sample name <case_id>.<sample_id>
                sample_id = f"{case['case_id']}.{individual['ind_id']}"
                ind_profile = individual['profile']

                #Numpy array to hold all the distances for this samples profile
                distance_array = np.array([], dtype=np.float)

                for sample, profile in zip(samples, profiles):

                    #Get distance and append to distance array
                    distance = compare_profiles(ind_profile, profile)
                    distance_array = np.append(distance_array, distance)

                    #Issue warning if above threshold
                    if distance >= threshold:
                        LOG.warning(f"{sample_id} is {distance} similar to {sample}")

                #Check number of distances in each range and add to distance_dict
                for key,range in HAMMING_RANGES.items():

                    #Calculate the number of hamming distances found within the
                    #range for current individual
                    distance_dict[key] += np.sum(
                        (distance_array >= range[0]) & (distance_array < range[1])
                    )

                #Append profile and sample_id for this sample for the next
                #iteration
                profiles.append(ind_profile)
                samples.append(sample_id)

    return distance_dict
--------------------
Prompt: 
def _settings_part(self):
        """
        A |SettingsPart| object providing access to the document-level
        settings for this document. Creates a default settings part if one is
        not present.
        """
----------
Output: 

        if not self.settings:
            self.settings = Settings()
        return self.settings

    def _get_settings(self):
        """
        A |Settings| object providing access to the document-level
        settings for this document.
        """
        if not self.settings:
            self.settings = Settings()
        return self.settings

    def _get_text(self):
        """
        A |Text| object providing access to the document-level
        text for this document.
        """
        if not self.text:
            self.text = Text()
        return self.text

    def _get_text_
----------
Solution: 

        try:
            return self.part_related_by(RT.SETTINGS)
        except KeyError:
            settings_part = SettingsPart.default(self.package)
            self.relate_to(settings_part, RT.SETTINGS)
            return settings_part
--------------------
Prompt: 
def _self_time(self):
    """Returns the time spent in this workunit outside of any children."""
----------
Output: 

    return self._self_time_in_seconds

  def _set_time(self, t):
    """Sets the time spent in this workunit."""
    self._self_time_in_seconds = t

  time = property(_get_time, _set_time, doc="""\
      Retrieves the time in seconds for the given task.

    :type: int
    """)

  def _get_time_in_seconds(self):
    """Returns the time in seconds for the given task."""
    return self._self_time_in_seconds

  def _set_time_in_seconds(
----------
Solution: 

    return self.duration() - sum([child.duration() for child in self.children])
--------------------
Prompt: 
def from_tibiadata(cls, content):
        """Parses the content of the World Overview section from TibiaData.com into an object of this class.

        Notes
        -----
        Due to TibiaData limitations, :py:attr:`record_count` and :py:attr:`record_date` are unavailable
        object.

        Additionally, the listed worlds in :py:attr:`worlds` lack some information when obtained from TibiaData.
        The following attributes are unavailable:

        - :py:attr:`ListedWorld.status` is always ``Online``.
        - :py:attr:`ListedWorld.battleye_protected` is always ``False``
        - :py:attr:`ListedWorld.battleye_date` is always ``None``.


        Parameters
        ----------
        content: :class:`str`
            The JSON response of the worlds section in TibiaData.com

        Returns
        -------
        :class:`WorldOverview`
            An instance of this class containing only the available worlds.

        Raises
        ------
        InvalidContent
            If the provided content is the json content of the world section in TibiaData.com
        """
----------
Output: 
 - :py:attr:`ListedWorld.date` is ``None``.
        """
        status = content.split()[0]
        if status == 'Online':
            return cls(content=content)
        elif status == 'Offline':
            return cls(content=content)
        else:
            raise ValueError('Unknown Overview status: %s' % status)

    def to_tibiadata(self):
        """Returns the :py:class:`World` object as a :py:class:`TibiaData` object.

        Notes
        -----
        Due to TibiaData limitations, :py:
----------
Solution: 

        json_data = parse_json(content)
        try:
            worlds_json = json_data["worlds"]["allworlds"]
            world_overview = cls()
            for world_json in worlds_json:
                world = ListedWorld(world_json["name"], world_json["location"], world_json["worldtype"])
                world._parse_additional_info(world_json["additional"])
                world.online_count = world_json["online"]
                world_overview.worlds.append(world)
            return world_overview
        except KeyError:
            raise InvalidContent("content is not a worlds json response from TibiaData.com.")
--------------------
Prompt: 
def from_name(cls, name):
        """ Retrieve a disk id associated to a name. """
----------
Output: 

        if name == cls.DISK_ID:
            return cls.DISK_ID
        if name == cls.DISK_SIZE:
            return cls.disk_size
        if name == cls.DISK_FREE:
            return cls.disk_free
        if name == cls.DISK_TYPE:
            return cls.disk_type
        if name == cls.DISK_FREE_SPACE:
            return cls.disk_free_space
        if name == cls.DISK_TYPE_SPARSE:
            return cls.disk_type_sparse
        if name == cls.DISK_TYPE_SPARSE_SPARSE:
           
----------
Solution: 

        disks = cls.list({'name': name})
        if len(disks) == 1:
            return disks[0]['id']
        elif not disks:
            return

        raise DuplicateResults('disk name %s is ambiguous.' % name)
--------------------
Prompt: 
def add_variables_from_file(self, file_path):
        """
        Adds all OpenFisca variables contained in a given file to the tax and benefit system.
        """
----------
Output: 

        self.tax_benefit_system.add_variables_from_file(file_path)
        self.benefit_system.add_variables_from_file(file_path)

    def add_tax_benefit_system(self, tax_id, benefit_system):
        """
        Adds all OpenFisca tax-related variables contained in a given OpenFisca ID.
        """
        self.tax_benefit_system.add_variables_from_taxid(tax_id, benefit_system)
        self.benefit_
----------
Solution: 

        try:
            file_name = path.splitext(path.basename(file_path))[0]

            #  As Python remembers loaded modules by name, in order to prevent collisions, we need to make sure that:
            #  - Files with the same name, but located in different directories, have a different module names. Hence the file path hash in the module name.
            #  - The same file, loaded by different tax and benefit systems, has distinct module names. Hence the `id(self)` in the module name.
            module_name = '{}_{}_{}'.format(id(self), hash(path.abspath(file_path)), file_name)

            module_directory = path.dirname(file_path)
            try:
                module = load_module(module_name, *find_module(file_name, [module_directory]))
            except NameError as e:
                logging.error(str(e) + ": if this code used to work, this error might be due to a major change in OpenFisca-Core. Checkout the changelog to learn more: <https://github.com/openfisca/openfisca-core/blob/master/CHANGELOG.md>")
                raise
            potential_variables = [getattr(module, item) for item in dir(module) if not item.startswith('__')]
            for pot_variable in potential_variables:
                # We only want to get the module classes defined in this module (not imported)
                if isclass(pot_variable) and issubclass(pot_variable, Variable) and pot_variable.__module__ == module_name:
                    self.add_variable(pot_variable)
        except Exception:
            log.error('Unable to load OpenFisca variables from file "{}"'.format(file_path))
            raise
--------------------
Prompt: 
def _validate_page(self):
        """
        Run PageValidator on the PAGE-XML documents referenced in the METS.
        """
----------
Output: 

        # TODO: We should probably validate the entire document here and just return the XML?
        #       We can't use etree.iterparse because we need to support
        #       Python 2.6
        #       And we need to support 2.7
        #       For Python 2.6, we need to parse the XML using etree.
        #       That will require a lot of testing.
        # TODO: Test the whole document.
        pass

    def _check_element(self, xml_str, tag_name, text):
        """
        Check that the XML matches the given tag.

        Arguments:
            xml
----------
Solution: 

        for ocrd_file in self.mets.find_files(mimetype=MIMETYPE_PAGE, local_only=True):
            self.workspace.download_file(ocrd_file)
            page_report = PageValidator.validate(ocrd_file=ocrd_file, strictness=self.page_strictness)
            self.report.merge_report(page_report)
--------------------
Prompt: 
def getreference(self, validate=True):
        """Return and validate the Phonetic Content's reference. Raises UnresolvableTextContent when invalid"""
----------
Output: 

        if validate:
            self.validate()
        return self.get_text_record()

    def get_reference_list(self):
        """Return a list of all references in the Phonetic Content."""
        return self.getreference()

    def validate(self):
        """Check the validity of the content."""
        errors = []
        if self.is_valid_record():
            errors.append(self.get_text_record())
        return errors

    def get_text_record(self):
        """Return a list of all text records in the Phonetic Content."""
        return self.text_records

   
----------
Solution: 


        if self.offset is None: return None #nothing to test
        if self.ref:
            ref = self.doc[self.ref]
        else:
            ref = self.finddefaultreference()

        if not ref:
            raise UnresolvableTextContent("Default reference for phonetic content not found!")
        elif not ref.hasphon(self.cls):
            raise UnresolvableTextContent("Reference has no such phonetic content (class=" + self.cls+")")
        elif validate and self.phon() != ref.textcontent(self.cls).phon()[self.offset:self.offset+len(self.data[0])]:
            raise UnresolvableTextContent("Reference (class=" + self.cls+") found but no phonetic match at specified offset ("+str(self.offset)+")! Expected '" + self.text() + "', got '" + ref.textcontent(self.cls).text()[self.offset:self.offset+len(self.data[0])] +"'")
        else:
            #finally, we made it!
            return ref
--------------------
Prompt: 
def detect(self):
        """Detect all currently known devices. Returns the root device."""
----------
Output: 

        return self._devices

    def get_device_name(self, device):
        """Return the name of the given device or None if we don't know."""
        return self._devices[device]["name"]

    def get_extra_attributes(self, device):
        """Return the device specific attributes."""
        return self._devices[device]["attributes"]

    def get_extra_state_attributes(self, device):
        """Return device specific state attributes."""
        return self._devices[device]["attributes"]

    def get_extra_attributes_by_name(self, device, name):
        """Return device
----------
Solution: 

        root = self._actions.detect()
        prune_empty_node(root, set())
        return root
--------------------
Prompt: 
def _derive_stereographic():
    """Compute the formulae to cut-and-paste into the routine below."""
----------
Output: 

    # The following two lines are to be removed:
    #
    # - The following two lines are to be replaced by the above
    # - The following two lines are to be removed by the above
    #
    # - The following two lines are to be replaced by the above
    # - The following two lines are to be removed by the above
    #
    # - The following two lines are to be removed by the above
    # - The following two lines are to be removed by the above
    #
    # - The following two lines are to be removed by the above
    # - The following two lines are to be removed
----------
Solution: 

    from sympy import symbols, atan2, acos, rot_axis1, rot_axis3, Matrix
    x_c, y_c, z_c, x, y, z = symbols('x_c y_c z_c x y z')

    # The angles we'll need to rotate through.
    around_z = atan2(x_c, y_c)
    around_x = acos(-z_c)

    # Apply rotations to produce an "o" = output vector.
    v = Matrix([x, y, z])
    xo, yo, zo = rot_axis1(around_x) * rot_axis3(-around_z) * v

    # Which we then use the stereographic projection to produce the
    # final "p" = plotting coordinates.
    xp = xo / (1 - zo)
    yp = yo / (1 - zo)

    return xp, yp
--------------------
Prompt: 
def _ixs(self, i, axis=0):
        """
        Parameters
        ----------
        i : int, slice, or sequence of integers
        axis : int
        """
----------
Output: 

        if isinstance(i, slice):
            return self.__getitem__(i)
        elif isinstance(i, int):
            return self.reindex(self.index[axis])
        else:
            raise TypeError('invalid index type: %s' % type(i))

    def _get_slice_axis(self, slice_obj, axis=0):
        """
        Get the axis of concatenation (appending) along axis.

        Parameters
        ----------
        slice_obj : sequence of slice, int, or sequence of integers
        axis : int
        """
        if isinstance(slice_obj, int):
            return slice_
----------
Solution: 


        ax = self._get_axis(axis)
        key = ax[i]

        # xs cannot handle a non-scalar key, so just reindex here
        # if we have a multi-index and a single tuple, then its a reduction
        # (GH 7516)
        if not (isinstance(ax, MultiIndex) and isinstance(key, tuple)):
            if is_list_like(key):
                indexer = {self._get_axis_name(axis): key}
                return self.reindex(**indexer)

        # a reduction
        if axis == 0:
            values = self._data.iget(i)
            return self._box_item_values(key, values)

        # xs by position
        self._consolidate_inplace()
        new_data = self._data.xs(i, axis=axis, copy=True, takeable=True)
        return self._construct_return_type(new_data)
--------------------
Prompt: 
def find_dependencies(self, dataset_keys, **dfilter):
        """Create the dependency tree.

        Args:
            dataset_keys (iterable): Strings or DatasetIDs to find dependencies for
            **dfilter (dict): Additional filter parameters. See
                              `satpy.readers.get_key` for more details.

        Returns:
            (Node, set): Root node of the dependency tree and a set of unknown datasets

        """
----------
Output: 

        # TODO: This is a temporary solution, but it is not used by the
        #       `readers.get_key` method.
        dfilter = copy.deepcopy(dfilter)
        dfilter.update(dfilter)
        nodes = set()
        for key in dataset_keys:
            node = self.add_node(key)
            nodes.add(node)
        return (nodes, set())

    def get_node(self, key):
        """Get the node of the dependency tree.

        Args:
            key (str): String of the key to find the node of.

        Returns:
----------
Solution: 

        unknown_datasets = set()
        for key in dataset_keys.copy():
            n, unknowns = self._find_dependencies(key, **dfilter)

            dataset_keys.discard(key)  # remove old non-DatasetID
            if n is not None:
                dataset_keys.add(n.name)  # add equivalent DatasetID
            if unknowns:
                unknown_datasets.update(unknowns)
                continue

            self.add_child(self, n)

        return unknown_datasets
--------------------
Prompt: 
def _element_to_node(self, node, name, value):
        """ Insert the parsed element (``name``, ``value`` pair) into the node.

        You should always use the returned node and forget the one
        that was given in parameter.

        :param node: the node where the is added to
        :returns: the node. Note that this may be a new node instance.
        """
----------
Output: 

        if isinstance(node, Node):
            node.name = name
            node.value = value
            return node
        elif isinstance(node, list):
            return NodeList(node)
        elif isinstance(node, dict):
            return NodeDict(node)
        else:
            raise TypeError("Unknown type of node: %s" % type(node))

    def _parse_node_list(self, node_list):
        """ Parse a list of ``Node`` objects.

        :param node_list: a list of ``Node`` objects
        :returns: a list of ``Node`` objects
        """
        return
----------
Solution: 


        # is the target node a list?
        try:
            node.append(value)
        except AttributeError:
            pass
        else:
            return node

        # target node is a dict
        if name in node:
            # there's already an element with same name -> convert the node into list
            node = node.values() + [value]
        else:
            # just add the value into the node
            node[name] = value
        return node
--------------------
Prompt: 
def _add_pos_args(self, *args):
        """
        Set values for *positional* placeholders (``$1,$2,...``)

        :param args: Values to be used
        """
----------
Output: 

        self._pos_args = args
        self._pos_kwargs = args

    def _get_pos_kwargs(self):
        """
        Return values for *pos_args* and *pos_kwargs* as a dictionary.
        """
        return self._pos_kwargs

    def _set_pos_args(self, pos_args, pos_kwargs):
        """
        Set values for *positional* placeholders (``$1,$2,...``)

        :param pos_args: Values to be used
        :param pos_kwargs: Values for *pos_args* and *pos_kwargs*
        """
       
----------
Solution: 

        arg_array = self._body.setdefault('args', [])
        arg_array.extend(args)
--------------------
Prompt: 
def add_read_groups(job, job_vars):
    """
    This function adds read groups to the headers

    job_vars: tuple     Tuple of dictionaries: input_args and ids
    """
----------
Output: 

    if job.id not in job_vars:
        job_vars[job.id] = {}
    for arg in job_vars[job.id]:
        if arg['id'] not in job.input_args:
            job.input_args[arg['id']] = []
        if arg['input_id'] not in job.ids:
            job.ids[arg['id']] = []
        if arg['id'] not in job.ids[arg_id]:
            job.ids[arg_id][arg['input_id']] = []
        if arg['input_id'] not
----------
Solution: 

    input_args, ids = job_vars
    work_dir = job.fileStore.getLocalTempDir()
    sudo = input_args['sudo']
    # I/O
    alignments = return_input_paths(job, work_dir, ids, 'alignments.bam')
    output = os.path.join(work_dir, 'rg_alignments.bam')
    # Command and callg
    parameter = ['AddOrReplaceReadGroups',
                 'INPUT={}'.format(docker_path(alignments)),
                 'OUTPUT={}'.format(docker_path(output)),
                 'RGSM={}'.format(input_args['uuid']),
                 'RGID={}'.format(input_args['uuid']),
                 'RGLB=TruSeq',
                 'RGPL=illumina',
                 'RGPU=barcode',
                 'VALIDATION_STRINGENCY=SILENT']
    docker_call(tool='quay.io/ucsc_cgl/picardtools:1.95--dd5ac549b95eb3e5d166a5e310417ef13651994e',
                tool_parameters=parameter, work_dir=work_dir, sudo=sudo)
    # Write to FileStore
    ids['rg_alignments.bam'] = job.fileStore.writeGlobalFile(output)
    # Run child job
    return job.addChildJobFn(bamsort_and_index, job_vars, disk='30 G').rv()
--------------------
Prompt: 
def format_registryfield_nodes(field_name, field, field_id, state, lineno):
    """Create a section node that documents a RegistryField config field.

    Parameters
    ----------
    field_name : `str`
        Name of the configuration field (the attribute name of on the config
        class).
    field : ``lsst.pex.config.RegistryField``
        A configuration field.
    field_id : `str`
        Unique identifier for this field. This is used as the id and name of
        the section node. with a -section suffix
    state : ``docutils.statemachine.State``
        Usually the directive's ``state`` attribute.
    lineno (`int`)
        Usually the directive's ``lineno`` attribute.

    Returns
    -------
    ``docutils.nodes.section``
        Section containing documentation nodes for the RegistryField.
    """
----------
Output: 
``
        The current state.
    lineno : int
        The line number in the source file where the config class is defined.

    Returns
    -------
    docutils.nodes.section
        The section node.
    """
    node = docutils.nodes.section()
    node.document = self.state.document
    node.settings = self.state.settings
    node.settings.env.note_dependency(line=lineno, node=self.env)
    return node

  def format_node_list(self, node_list):
    """Create a list of section nodes.

    Parameters
    ----------
    node_list : `
----------
Solution: 

    from lsst.pex.config.registry import ConfigurableWrapper

    # Create a definition list for the choices
    # This iteration is over field.registry.items(), not field.items(), so
    # that the directive shows the configurables, not their ConfigClasses.
    choice_dl = nodes.definition_list()
    for choice_value, choice_class in field.registry.items():
        # Introspect the class name from item in the registry. This is harder
        # than it should be. Most registry items seem to fall in the first
        # category. Some are ConfigurableWrapper types that expose the
        # underlying task class through the _target attribute.
        if hasattr(choice_class, '__module__') \
                and hasattr(choice_class, '__name__'):
            name = '.'.join((choice_class.__module__, choice_class.__name__))
        elif isinstance(choice_class, ConfigurableWrapper):
            name = '.'.join((choice_class._target.__class__.__module__,
                             choice_class._target.__class__.__name__))
        else:
            name = '.'.join((choice_class.__class__.__module__,
                             choice_class.__class__.__name__))

        item = nodes.definition_list_item()
        item_term = nodes.term()
        item_term += nodes.literal(text=repr(choice_value))
        item += item_term
        item_definition = nodes.definition()
        def_para = nodes.paragraph()
        def_para += pending_task_xref(rawsource=name)
        item_definition += def_para
        item += item_definition
        choice_dl.append(item)

    choices_node = nodes.definition_list_item()
    choices_node.append(nodes.term(text='Choices'))
    choices_definition = nodes.definition()
    choices_definition.append(choice_dl)
    choices_node.append(choices_definition)

    # Field type
    field_type_item = nodes.definition_list_item()
    field_type_item.append(nodes.term(text="Field type"))
    field_type_item_content = nodes.definition()
    field_type_item_content_p = nodes.paragraph()
    if field.multi:
        multi_text = "Multi-selection "
    else:
        multi_text = "Single-selection "
    field_type_item_content_p += nodes.Text(multi_text, multi_text)
    field_type_item_content_p += make_python_xref_nodes_for_type(
        type(field),
        state,
        hide_namespace=True)[0].children[0]
    if field.optional:
        field_type_item_content_p += nodes.Text(' (optional)', ' (optional)')
    field_type_item_content += field_type_item_content_p
    field_type_item += field_type_item_content

    dl = nodes.definition_list()
    dl += create_default_item_node(field, state)
    dl += field_type_item
    dl += choices_node

    # Doc for this field, parsed as rst
    desc_node = create_description_node(field, state)

    # Title for configuration field
    title = create_title_node(field_name, field, field_id, state, lineno)

    return [title, dl, desc_node]
--------------------
Prompt: 
def plot(data, output_dir_path='.', width=10, height=8):
    """Create two plots: 1) loss 2) accuracy.
        Args:
            data: Panda dataframe in *the* format.
    """
----------
Output: 

    plt.figure(figsize=(width, height))
    plt.plot(data.index, data.loss, 'ro', markersize=10)
    plt.plot(data.index, data.accuracy, 'b', markersize=10)
    plt.xlabel('Time (s)')
    plt.ylabel('Accuracy (m)')
    plt.title('loss vs accuracy')
    plt.legend(loc='best')
    plt.savefig(output_dir_path + 'plots.png')


def main():
    """Create two plots: 1) loss 3) accuracy.
        Args:
           
----------
Solution: 

    if not isinstance(data, pd.DataFrame):
        data = pd.DataFrame(data)
    plot_accuracy(data, output_dir_path=output_dir_path,
                  width=width, height=height)
    plot_loss(data, output_dir_path, width=width, height=height)
--------------------
Prompt: 
def delete(self, obj, force=False):
        """Deletes all of the fields at the specified locations.

        args:

            ``obj=``\ *OBJECT*
                the object to remove the fields from

            ``force=``\ *BOOL*
                if True, missing attributes do not raise errors.  Otherwise,
                the first failure raises an exception without making any
                changes to ``obj``.
        """
----------
Output: 

        if force:
            self.force_delete(obj)
        else:
            self.delete_all(obj)

    def force_delete(self, obj):
        """Forcefully deletes the specified object.

        args:

            ``obj=``\ *OBJECT*
                the object to remove the fields from

            ``force=``\ *BOOL*
                if True, missing attributes do not raise errors.  Otherwise,
                the first failure raises an exception without making any
                changes to ``obj``.
        """
        if self.db_type =='mysql':
            self.db_type ='mysql'
            self.db
----------
Solution: 

        # TODO: this could be a whole lot more efficient!
        if not force:
            for fs in self:
                try:
                    fs.get(obj)
                except FieldSelectorException:
                    raise

        for fs in self:
            try:
                fs.delete(obj)
            except FieldSelectorException:
                pass
--------------------
Prompt: 
def actuator_control_target_send(self, time_usec, group_mlx, controls, force_mavlink1=False):
                """
                Set the vehicle attitude and body angular rates.

                time_usec                 : Timestamp (micros since boot or Unix epoch) (uint64_t)
                group_mlx                 : Actuator group. The "_mlx" indicates this is a multi-instance message and a MAVLink parser should use this field to difference between instances. (uint8_t)
                controls                  : Actuator controls. Normed to -1..+1 where 0 is neutral position. Throttle for single rotation direction motors is 0..1, negative range for reverse direction. Standard mapping for attitude controls (group 0): (index 0-7): roll, pitch, yaw, throttle, flaps, spoilers, airbrakes, landing gear. Load a pass-through mixer to repurpose them as generic outputs. (float)

                """
----------
Output: 
 is normal, 1 is linear, 2 is quadratic.
                force_mavlink1                 : Force the MAVLink version to be 1.0 or 0.0 (bool)
                """
                self.attitude.attitude = time_usec
                self.body.body.angular_velocity = -1.0 if not force_mavlink1 else 1.0 if not force_mavlink1 else 0.0
                self.attitude.body.timestamp = self.body.body.attitude
                self.attitude.body.timestamp_type = time_usec.header.timestamp_type
                self.
----------
Solution: 

                return self.send(self.actuator_control_target_encode(time_usec, group_mlx, controls), force_mavlink1=force_mavlink1)
--------------------
Prompt: 
def _restore_output(self, statement: Statement, saved_state: utils.RedirectionSavedState) -> None:
        """Handles restoring state after output redirection as well as
        the actual pipe operation if present.

        :param statement: Statement object which contains the parsed input from the user
        :param saved_state: contains information needed to restore state data
        """
----------
Output: 

        if not statement.is_valid():
            raise utils.InvalidStatement(
                "Invalid output statement: {}".format(statement.get_description())
            )

        if saved_state.is_error:
            raise utils.ErrorStatement(
                "Error while restoring state: {}".format(saved_state.get_description())
            )

        saved_state.restore()

    def _get_output_stream(self, output_stream_name: str) -> Optional[Optional[Stream]]:
        """Returns the stream to write the output to.

        :param output_stream_name: Name of the output
----------
Solution: 

        if saved_state.redirecting:
            # If we redirected output to the clipboard
            if statement.output and not statement.output_to:
                self.stdout.seek(0)
                write_to_paste_buffer(self.stdout.read())

            try:
                # Close the file or pipe that stdout was redirected to
                self.stdout.close()
            except BrokenPipeError:
                pass

            # Restore the stdout values
            self.stdout = saved_state.saved_self_stdout
            sys.stdout = saved_state.saved_sys_stdout

            # Check if we need to wait for the process being piped to
            if self.cur_pipe_proc_reader is not None:
                self.cur_pipe_proc_reader.wait()

        # Restore cur_pipe_proc_reader. This always is done, regardless of whether this command redirected.
        self.cur_pipe_proc_reader = saved_state.saved_pipe_proc_reader
--------------------
Prompt: 
def logical_or(f1, f2):  # function factory
    """Logical or from functions.

    Parameters
    ----------
    f1, f2 : function
        Function that takes array and returns true or false for each item in array.

    Returns
    -------
    Function.
    """
----------
Output: 

    return lambda x, y: x or y
    # end function_or

# end class _SpectralShape

class _SpectralShape(Shape):
    """The base class for spectral shapes.

    This class is used to represent a shape that is a spectral shape.
    """
    def __init__(self, **kwargs):
        super(_SpectralShape, self).__init__(**kwargs)
    # end function_or

# end class _SpectralShape


class _Spectral(Shape):
    """The base class for spectral objects.

    This class is used to represent a spectral object.
----------
Solution: 

    def f(value):
        return np.logical_or(f1(value), f2(value))
    f.__name__ = "(" + f1.__name__ + "_or_" + f2.__name__ + ")"
    return f
--------------------
Prompt: 
def star(self, **args):
		"""
		star any gist by providing gistID or gistname(for authenticated user)
		"""
----------
Output: 

		if len(args) == 0:
			raise ArgumentError("Missing argument")
		if len(args) == 1:
			gistID = args[0]
		else:
			gistID = args[1]
		if not args:
			raise ArgumentError("Missing argument")
		if len(args) == 2:
			gistName = args[0]
		else:
			gistName = args[2]
		if not args:
			raise ArgumentError("Missing argument")
		if len(args) == 3:
		
----------
Solution: 

		if 'name' in args:
			self.gist_name = args['name']
			self.gist_id = self.getMyID(self.gist_name)
		elif 'id' in args:
			self.gist_id = args['id']
		else:
			raise Exception('Either provide authenticated user\'s Unambigious Gistname or any unique Gistid to be starred')

		r = requests.put(
			'%s'%BASE_URL+'/gists/%s/star' % self.gist_id,
			headers=self.gist.header
			)
		if (r.status_code == 204):
			response = {
				'id': self.gist_id
			}
			return response

		raise Exception('Gist can\'t be starred')
--------------------
Prompt: 
def create_experiment(self, workflow_type, microscope_type, plate_format,
            plate_acquisition_mode):
        """Creates the experiment.

        Parameters
        ----------
        workflow_type: str
            workflow type
        microscope_type: str
            microscope type
        plate_format: int
            well-plate format, i.e. total number of wells per plate
        plate_acquisition_mode: str
            mode of image acquisition that determines whether acquisitions will
            be interpreted as time points as part of a time series experiment
            or as multiplexing cycles as part of a serial multiplexing
            experiment

        Returns
        -------
        dict
            experiment resource representation

        See also
        --------
        :func:`tmserver.api.experiment.create_experiment`
        :class:`tmlib.models.experiment.ExperimentReference`
        :class:`tmlib.models.experiment.Experiment`
        """
----------
Output: 
.

        Returns
        -------
        image_acquisition : :class:`ImageAcquisition`
            image acquisition
        """
        if workflow_type == 'image':
            return ImageAcquisition(self.image_data, self.image_metadata,
                self.image_data_type, self.image_data_metadata_type)
        elif workflow_type == 'plate':
            return PlateAcquisition(self.plate_data, self.plate_metadata,
                self.plate_data_type, self.plate_data_metadata_type)
        elif workflow_type == 'acquisition':
           
----------
Solution: 

        logger.info('create experiment "%s"', self.experiment_name)
        content = {
            'name': self.experiment_name,
            'workflow_type': workflow_type,
            'microscope_type': microscope_type,
            'plate_format': plate_format,
            'plate_acquisition_mode': plate_acquisition_mode
        }
        url = self._build_api_url('/experiments')
        res = self._session.post(url, json=content)
        res.raise_for_status()
        data = res.json()['data']
        self._experiment_id = data['id']
        return data
--------------------
Prompt: 
def dot(self, other, dims=None):
        """Perform dot product of two DataArrays along their shared dims.

        Equivalent to taking taking tensordot over all shared dims.

        Parameters
        ----------
        other : DataArray
            The other array with which the dot product is performed.
        dims: list of strings, optional
            Along which dimensions to be summed over. Default all the common
            dimensions are summed over.

        Returns
        -------
        result : DataArray
            Array resulting from the dot product over all shared dimensions.

        See also
        --------
        dot
        numpy.tensordot

        Examples
        --------

        >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
        >>> da = DataArray(da_vals, dims=['x', 'y', 'z'])
        >>> dm_vals = np.arange(4)
        >>> dm = DataArray(dm_vals, dims=['z'])

        >>> dm.dims
        ('z')
        >>> da.dims
        ('x', 'y', 'z')

        >>> dot_result = da.dot(dm)
        >>> dot_result.dims
        ('x', 'y')
        """
----------
Output: 
 = DataArray(np.arange(6).reshape(2, 3), dims=['x', 'y'])
        >>> da.dot(da)
        <xarray.DataArray (x: 6)>
        array([[ 0.,  1.,  2.],
               [ 1.,  3.,  4.],
               [ 2.,  4.,  6.],
               [ 3.,  6.,  9.]])
        Coordinates:
        * x        - x coordinates
        * y        - y coordinates

        >>> da.dims
        ['x', 'y']
        >>> da.dims = ['x', 'y']
        >>>
----------
Solution: 

        if isinstance(other, Dataset):
            raise NotImplementedError('dot products are not yet supported '
                                      'with Dataset objects.')
        if not isinstance(other, DataArray):
            raise TypeError('dot only operates on DataArrays.')

        return computation.dot(self, other, dims=dims)
--------------------
Prompt: 
def And(*xs, simplify=True):
    """Expression conjunction (product, AND) operator

    If *simplify* is ``True``, return a simplified expression.
    """
----------
Output: 

    if not isinstance(xs, list):
        return [xs]
    if len(xs) == 1:
        return xs[0]
    return reduce(operator.and_, xs)


def Or(*xs):
    """Expression conjunction (product, OR) operator

    If *xs* is a list of ``Expression`` objects, return a single ``Expression``
    object.
    """
    if not isinstance(xs, list):
        return [xs]
    if len(xs) == 1:
        return xs[0]
    return reduce(operator.or_, xs)


def Not(xs):
   
----------
Solution: 

    xs = [Expression.box(x).node for x in xs]
    y = exprnode.and_(*xs)
    if simplify:
        y = y.simplify()
    return _expr(y)
--------------------
Prompt: 
def load_from_path(self, filepath, container, **opts):
        """
        :param filepath: XML file path
        :param container: callble to make a container object
        :param opts: optional keyword parameters to be sanitized

        :return: Dict-like object holding config parameters
        """
----------
Output: 

        self.log.debug("Loading config file %s", filepath)
        try:
            self.config = container.find('configuration')
            self.log.debug("Found configuration %s", self.config.text)
            return self.config
        except Exception as e:
            self.log.debug("Failed to find configuration %s: %s", self.config.text, e)
            return {}

    def save_to_path(self, filepath, xml):
        """
        :param filepath: XML file path
        :param xml: The config object to save
        """
        self.log.debug
----------
Solution: 

        root = ET.parse(filepath).getroot()
        nspaces = _namespaces_from_file(filepath)
        return root_to_container(root, container=container,
                                 nspaces=nspaces, **opts)
--------------------
Prompt: 
def login():
    """Log in a registered user by adding the user id to the session."""
----------
Output: 

    session['user_id'] = session.get('user_id', None) + 1
    return redirect(url_for('login'))


@app.route('/logout')
@login_required
def logout():
    """Log out of a registered user by removing the user id from the session."""
    session.pop('user_id', None)
    return redirect(url_for('login'))


@app.route('/login', methods=['GET', 'POST'])
def login_post():
    """
    This function is used to log in a user by sending the login form to the
   
----------
Solution: 

    if request.method == "POST":
        username = request.form["username"]
        password = request.form["password"]
        error = None
        user = User.query.filter_by(username=username).first()

        if user is None:
            error = "Incorrect username."
        elif not user.check_password(password):
            error = "Incorrect password."

        if error is None:
            # store the user id in a new session and return to the index
            session.clear()
            session["user_id"] = user.id
            return redirect(url_for("index"))

        flash(error)

    return render_template("auth/login.html")
--------------------
Prompt: 
def create_issue(self, request, group, form_data, **kwargs):
        """
        Creates the issue on the remote service and returns an issue ID.
        """
----------
Output: 

        try:
            return self.remote_service.create_issue(
                group_id=group.id,
                title=form_data.get('title', ''),
                body=form_data.get('body', ''),
                assignee_id=request.user.id,
                milestone_id=request.user.milestone.id,
                description=form_data.get('description', ''),
                due_on=self.parse_date(form_data.get('due_on', '2013-12-31')),
                due_on_str=form_data.get('
----------
Solution: 

        instance = self.get_option('instance', group.project)
        project = (
            form_data.get('project') or
            self.get_option('default_project', group.project)
        )

        client = self.get_client(request.user)

        title = form_data['title']
        description = form_data['description']
        link = absolute_uri(group.get_absolute_url(params={'referrer': 'vsts_plugin'}))
        try:
            created_item = client.create_work_item(
                instance=instance,
                project=project,
                title=title,
                comment=markdown(description),
                link=link,
            )
        except Exception as e:
            self.raise_error(e, identity=client.auth)

        return {
            'id': created_item['id'],
            'url': created_item['_links']['html']['href'],
            'title': title,
        }
--------------------
Prompt: 
def handle_unsubscribe(self, request, path):
        # type: (Subscribe, List[str]) -> CallbackResponses
        """Remove from the notifier list and send a return

        Args:
            request (Subscribe): The original subscribe request
            path (list): The relative path from ourself

        Returns:
            list: [(callback, Response)] that need to be called
        """
----------
Output: 

        # We don't want to call the notifier if the path is empty
        if path == []:
            return

        # We don't want to call the notifier if the path is not empty
        if path == []:
            return

        # We don't want to call the notifier if the path is empty
        if path == []:
            return

        # We don't want to call the notifier if the path is empty
        if path == []:
            return

        # We don't want to call the notifier if the path is empty
        if path == []:
            return

        # We don't want to call the notifier if the path is empty
        if
----------
Solution: 

        ret = []
        if path:
            # Recurse down
            name = path[0]
            child = self.children[name]
            ret += child.handle_unsubscribe(request, path[1:])
            if not child.children and not child.update_requests \
                    and not child.delta_requests:
                del self.children[name]
        else:
            # This is for us
            if request in self.update_requests:
                self.update_requests.remove(request)
            else:
                self.delta_requests.remove(request)
            ret.append(request.return_response())
        return ret
--------------------
Prompt: 
def rbinomial(n, p, size=None):
    """
    Random binomial variates.
    """
----------
Output: 

    if size is None:
        size = n
    if size > 1:
        return [p * (1 - p) + n for n in range(size)]
    else:
        return [p * (1 - p) + p * (1 - p) * (1 - p) + p * (1 - p)]

def rbinomial_n(n, p, size=None):
    """
    Random binomial variates.
    """
    if size is None:
        size = n
    if size > 1:
        return [n * (1 - n) + p * (1
----------
Solution: 

    if not size:
        size = None
    return np.random.binomial(np.ravel(n), np.ravel(p), size)
--------------------
Prompt: 
def build_statusbar(self):
        """construct and return statusbar widget"""
----------
Output: 

        self.statusbar = gtk.Statusbar()
        self.statusbar.set_has_resize_grip(False)
        self.statusbar.set_has_resize_grip(False)
        self.statusbar.set_has_resize_grip(False)
        self.statusbar.set_size_request(self.statusbar_width, -1)
        self.statusbar.set_has_resize_grip(False)
        self.statusbar.set_no_show_all(True)
        self.statusbar.set_no_show
----------
Solution: 

        info = {}
        cb = self.current_buffer
        btype = None

        if cb is not None:
            info = cb.get_info()
            btype = cb.modename
            info['buffer_no'] = self.buffers.index(cb)
            info['buffer_type'] = btype
        info['total_messages'] = self.dbman.count_messages('*')
        info['pending_writes'] = len(self.dbman.writequeue)
        info['input_queue'] = ' '.join(self.input_queue)

        lefttxt = righttxt = u''
        if cb is not None:
            lefttxt, righttxt = settings.get(btype + '_statusbar', (u'', u''))
            lefttxt = string_decode(lefttxt, 'UTF-8')
            lefttxt = lefttxt.format(**info)
            righttxt = string_decode(righttxt, 'UTF-8')
            righttxt = righttxt.format(**info)

        footerleft = urwid.Text(lefttxt, align='left')
        pending_writes = len(self.dbman.writequeue)
        if pending_writes > 0:
            righttxt = ('|' * pending_writes) + ' ' + righttxt
        footerright = urwid.Text(righttxt, align='right')
        columns = urwid.Columns([
            footerleft,
            ('pack', footerright)])
        footer_att = settings.get_theming_attribute('global', 'footer')
        return urwid.AttrMap(columns, footer_att)
--------------------
Prompt: 
def finalize_file(self):
        # type: (Descriptor) -> None
        """Finalize file for download
        :param Descriptor self: this
        """
----------
Output: 

        self.log.info('Downloading %s', self.filename)
        if self.filename.endswith('.zip'):
            self.log.info('Decompressing %s', self.filename)
            zip_file = zipfile.ZipFile(self.filename, 'r')
            for member in zip_file.namelist():
                if member.endswith('/'):
                    continue
                member_path = os.path.join(self.path, member)
                if not member_path.endswith('/'):
                    os.makedirs(os.path.dirname(member_path))
                with open(member_path, 'wb')
----------
Solution: 

        # delete bad file if integrity failed
        if self._integrity_failed:
            self.final_path.unlink()
        else:
            self._restore_file_attributes()
            self._restore_file_lmt()
        # update resume file
        self._update_resume_for_completed()
        with self._meta_lock:
            self._finalized = True
--------------------
Prompt: 
def to_api_repr(self):
        """Construct JSON API representation for the parameter.

        :rtype: dict
        :returns: JSON mapping
        """
----------
Output: 

        return {
            'name': self.name,
            'description': self.description,
           'required': self.required,
            'default': self.default,
            'type': self.type,
            'default_from': self.default_from,
            'default_to': self.default_to,
           'required_from': self.required_from,
           'required_to': self.required_to,
            'default_from': self.default_from,
           'required_to': self.required_to,
            'default_to': self.default_to,
           
----------
Solution: 

        value = self.value
        converter = _SCALAR_VALUE_TO_JSON_PARAM.get(self.type_)
        if converter is not None:
            value = converter(value)
        resource = {
            "parameterType": {"type": self.type_},
            "parameterValue": {"value": value},
        }
        if self.name is not None:
            resource["name"] = self.name
        return resource
--------------------
Prompt: 
def _filenames_from_arg(filename):
    """Utility function to deal with polymorphic filenames argument."""
----------
Output: 

    if os.path.isdir(filename):
        filenames = []
        for filename in sorted(os.listdir(filename)):
            if filename.endswith(".py") and not filename.startswith("."):
                module_name = filename.split(".")[0]
                module = importlib.import_module(module_name)
                filenames.append(filename)
        return filenames
    else:
        raise ValueError("Unknown filename '%s'" % filename)


def _get_filename(filename):
    """Return the absolute path of a filename."""
    return os.path.join(os.path.dirname(os.path
----------
Solution: 

    if isinstance(filename, string_types):
        filenames = [filename]
    elif isinstance(filename, (list, tuple)):
        filenames = filename
    else:
        raise Exception('filename argument must be string, list or tuple')
    for fn in filenames:
        if not os.path.exists(fn):
            raise ValueError('file not found: %s' % fn)
        if not os.path.isfile(fn):
            raise ValueError('not a file: %s' % fn)
    return filenames
--------------------
Prompt: 
def listen_on_tcp_port():
    """listen_on_tcp_port

    Run a simple server for processing messages over ``TCP``.

    ``LISTEN_ON_HOST`` - listen on this host ip address

    ``LISTEN_ON_PORT`` - listen on this ``TCP`` port

    ``LISTEN_SIZE`` - listen on to packets of this size

    ``LISTEN_SLEEP`` - sleep this number of seconds per loop

    ``LISTEN_SHUTDOWN_HOOK`` - shutdown if file is found on disk

    """
----------
Output: 

    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.bind(('', 0))
    sock.listen(1)
    return sock


def listen_on_udp_port():
    """listen_on_udp_port

    Run a simple server for processing messages over UDP.

    ``LISTEN_ON_HOST`` - listen on this host ip address

    ``LISTEN_ON_PORT`` - listen on this ``UDP`` port

    ``LISTEN_SLEEP`` - sleep this number of seconds per loop

    ``LISTEN_SHUTDOWN_HOOK``
----------
Solution: 


    host = os.getenv(
        "LISTEN_ON_HOST",
        "127.0.0.1").strip().lstrip()
    port = int(os.getenv(
        "LISTEN_ON_PORT",
        "80").strip().lstrip())
    backlog = int(os.getenv(
        "LISTEN_BACKLOG",
        "5").strip().lstrip())
    size = int(os.getenv(
        "LISTEN_SIZE",
        "1024").strip().lstrip())
    sleep_in_seconds = float(os.getenv(
        "LISTEN_SLEEP",
        "0.5").strip().lstrip())
    shutdown_hook = os.getenv(
        "LISTEN_SHUTDOWN_HOOK",
        "/tmp/shutdown-listen-server-{}-{}".format(
            host,
            port)).strip().lstrip()

    if os.path.exists(shutdown_hook):
        print(("Please remove the shutdown hook file: "
               "\nrm -f {}")
              .format(
                shutdown_hook))
        sys.exit(1)

    now = datetime.datetime.now().isoformat()
    print(("{} - Starting Server address={}:{} "
           "backlog={} size={} sleep={} shutdown={}")
          .format(
            now,
            host,
            port,
            backlog,
            size,
            sleep_in_seconds,
            shutdown_hook))

    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.bind((host, port))
    s.listen(backlog)

    msg = 0
    while 1:
        client, address = s.accept()
        send_data = False
        data = None
        while not data:
            data = client.recv(size)
            if data:
                now = datetime.datetime.now().isoformat()
                print(("{} received msg={} "
                       "data={} replying")
                      .format(
                        now,
                        msg,
                        data))
                msg += 1
                if msg > 1000000:
                    msg = 0

                send_data = True
            else:
                time.sleep(sleep_in_seconds)

        if send_data:
            client.send(data)
        if os.path.exists(shutdown_hook):
            now = datetime.datetime.now().isoformat()
            print(("{} detected shutdown "
                   "file={}")
                  .format(
                    now,
                    shutdown_hook))

        client.close()
--------------------
Prompt: 
def _as_graph_element(self):
    """Returns the underlying graph element if possible."""
----------
Output: 

    if self._graph is not None:
      return self._graph.as_graph_element(self.handle)
    return self._handle

  def _AsTensor(self):
    """Returns a Tensor after converting it to a Tensor."""
    self.stack.append([])
    self.tensor.op._set_attr("_class", "Const")  # pylint: disable=protected-access
    if self._shape_val is not None:
      self.tensor.shape.CopyFrom(self._shape_val.name)
    if self._handle_data is not None:
      self.tensor.handle.
----------
Solution: 

    if self.is_sequence():
      raise TypeError('A Pretty Tensor that holds a sequence cannot be '
                      'represented as a graph element.')
    else:
      # Self might be holding something else that isn't a true tensor, so
      # if the 'tensor' can behave like a graph element, look for its
      # _AsGraphElement method and call it. Graph elements themselves may not
      # have or need this method, so just return other items directly.
      obj = self.tensor
      conv_fn = getattr(obj, '_as_graph_element', None)
      if conv_fn and isinstance(conv_fn, collections.Callable):
        obj = conv_fn()
      return obj
--------------------
Prompt: 
def initialize(
    plugins,
    exclude_files_regex=None,
    exclude_lines_regex=None,
    path='.',
    scan_all_files=False,
):
    """Scans the entire codebase for secrets, and returns a
    SecretsCollection object.

    :type plugins: tuple of detect_secrets.plugins.base.BasePlugin
    :param plugins: rules to initialize the SecretsCollection with.

    :type exclude_files_regex: str|None
    :type exclude_lines_regex: str|None
    :type path: str
    :type scan_all_files: bool

    :rtype: SecretsCollection
    """
----------
Output: 
_files: bool
    :type path: str
    :type scan_all_lines: bool

    :rtype: SecretsCollection
    :return: a SecretsCollection object.

    :raises SecretsCollectionError: if the rules are not met.
    """
    if exclude_files_regex is None:
        exclude_files_regex = tuple()
    if exclude_lines_regex is None:
        exclude_lines_regex = tuple()

    if exclude_files_regex is not None:
        for rule in plugins:
            if isinstance(rule, BaseRule):
                for exclude_line in exclude
----------
Solution: 

    output = SecretsCollection(
        plugins,
        exclude_files=exclude_files_regex,
        exclude_lines=exclude_lines_regex,
    )

    if os.path.isfile(path):
        # This option allows for much easier adhoc usage.
        files_to_scan = [path]
    elif scan_all_files:
        files_to_scan = _get_files_recursively(path)
    else:
        files_to_scan = _get_git_tracked_files(path)

    if not files_to_scan:
        return output

    if exclude_files_regex:
        exclude_files_regex = re.compile(exclude_files_regex, re.IGNORECASE)
        files_to_scan = filter(
            lambda file: (
                not exclude_files_regex.search(file)
            ),
            files_to_scan,
        )

    for file in files_to_scan:
        output.scan_file(file)

    return output
--------------------
Prompt: 
def getDate():
    """Returns a formatted string with the current date."""
----------
Output: 

    return time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())

def getTime():
    """Returns a formatted string with the current time."""
    return time.strftime("%H:%M:%S", time.localtime())

def getDate():
    """Returns a formatted string with the current date."""
    return time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())

def getTime():
    """Returns a formatted string with the current time."""
    return time.strftime("%H:%M:%S", time.localtime())

----------
Solution: 


    _ltime = _time.localtime(_time.time())
    date_str = _time.strftime('%Y-%m-%dT%H:%M:%S',_ltime)

    return date_str
--------------------
Prompt: 
def _update_classifier(self, data, labels, w, classes):
        """Update the classifier parameters theta and bias

        Parameters
        ----------

        data : list of 2D arrays, element i has shape=[voxels_i, samples_i]
            Each element in the list contains the fMRI data of one subject for
            the classification task.

        labels : list of arrays of int, element i has shape=[samples_i]
            Each element in the list contains the labels for the data samples
            in data_sup.

        w : list of 2D array, element i has shape=[voxels_i, features]
            The orthogonal transforms (mappings) :math:`W_i` for each subject.

        classes : int
            The number of classes in the classifier.


        Returns
        -------

        theta : array, shape=[features, classes]
            The MLR parameter for the class planes.

        bias : array shape=[classes,]
            The MLR parameter for class biases.
        """
----------
Output: 
_i, n_components]
            Each element in the list contains the weight of each subject
            component.

        classes : list of int, element i has shape=[voxels_i]
            Each element in the list contains the number of subjects for the
            classification task.
        """
        if self.n_components is None:
            raise ValueError('n_components is required for online learning')
        if self.theta is None:
            raise ValueError('theta is required for online learning')
        if self.bias is None:
            raise ValueError('bias is required for online learning')
        if self.theta is not None
----------
Solution: 


        # Stack the data and labels for training the classifier
        data_stacked, labels_stacked, weights = \
            SSSRM._stack_list(data, labels, w)

        features = w[0].shape[1]
        total_samples = weights.size

        data_th = S.shared(data_stacked.astype(theano.config.floatX))
        val_ = S.shared(labels_stacked)
        total_samples_S = S.shared(total_samples)
        theta_th = T.matrix(name='theta', dtype=theano.config.floatX)
        bias_th = T.col(name='bias', dtype=theano.config.floatX)
        constf2 = S.shared(self.alpha / self.gamma, allow_downcast=True)
        weights_th = S.shared(weights)

        log_p_y_given_x = \
            T.log(T.nnet.softmax((theta_th.T.dot(data_th.T)).T + bias_th.T))
        f = -constf2 * T.sum((log_p_y_given_x[T.arange(total_samples_S), val_])
                             / weights_th) + 0.5 * T.sum(theta_th ** 2)

        manifold = Product((Euclidean(features, classes),
                            Euclidean(classes, 1)))
        problem = Problem(manifold=manifold, cost=f, arg=[theta_th, bias_th],
                          verbosity=0)
        solver = ConjugateGradient(mingradnorm=1e-6)
        solution = solver.solve(problem)
        theta = solution[0]
        bias = solution[1]

        del constf2
        del theta_th
        del bias_th
        del data_th
        del val_
        del solver
        del solution

        return theta, bias
--------------------
Prompt: 
def parse(cls, key):
        """
        Parse a key represented by a string and return its character code.
        """
----------
Output: 

        if key == cls.ESCAPE:
            return cls.ESCAPE
        elif key == cls.SPACE:
            return cls.SPACE
        elif key == cls.END:
            return cls.END
        elif key == cls.BACKSPACE:
            return cls.BACKSPACE
        elif key == cls.DELETE:
            return cls.DELETE
        elif key == cls.INSERT:
            return cls.INSERT
        elif key == cls.DELETE_ALL:
            return cls.DELETE_ALL
        elif key == cls.INSERT_ALL:
            return cls.INSERT_ALL
        elif key == cls.DELETE_BACKSPACE:
            return cls
----------
Solution: 


        try:
            if isinstance(key, int):
                return key
            elif re.match('[<]KEY_.*[>]', key):
                # Curses control character
                return getattr(curses, key[1:-1])
            elif re.match('[<].*[>]', key):
                # Ascii control character
                return getattr(curses.ascii, key[1:-1])
            elif key.startswith('0x'):
                # Ascii hex code
                return int(key, 16)
            elif len(key) == 2:
                # Double presses
                return tuple(cls.parse(k) for k in key)
            else:
                # Ascii character
                code = ord(key)
                if 0 <= code <= 255:
                    return code
                # Python 3.3 has a curses.get_wch() function that we can use
                # for unicode keys, but Python 2.7 is limited to ascii.
                raise exceptions.ConfigError('Invalid configuration! `%s` is '
                                             'not in the ascii range' % key)

        except (AttributeError, ValueError, TypeError):
            raise exceptions.ConfigError('Invalid configuration! "%s" is not a '
                                         'valid key' % key)
--------------------
Prompt: 
def get_git_repositories_activity_metrics(self, project, from_date, aggregation_type, skip, top):
        """GetGitRepositoriesActivityMetrics.
        [Preview API] Retrieves git activity metrics for repositories matching a specified criteria.
        :param str project: Project ID or project name
        :param datetime from_date: Date from which, the trends are to be fetched.
        :param str aggregation_type: Bucket size on which, trends are to be aggregated.
        :param int skip: The number of repositories to ignore.
        :param int top: The number of repositories for which activity metrics are to be retrieved.
        :rtype: [RepositoryActivityMetrics]
        """
----------
Output: 
 to return.
        :return: list[dict]
        """
        params = {}
        if project is not None:
            params['ProjectId'] = project
        if aggregation_type is not None:
            params['BucketSize'] = {
                'Total': bucket_size_to_total(from_date),
                'Filtered': bucket_size_to_filtered(skip),
                'TotalFiltered': bucket_size_to_total(skip),
            }
        if skip is not None:
            params['Skip'] = {
                'Total': skip,
                'Filtered': skip,
                'Total
----------
Solution: 

        route_values = {}
        if project is not None:
            route_values['project'] = self._serialize.url('project', project, 'str')
        query_parameters = {}
        if from_date is not None:
            query_parameters['fromDate'] = self._serialize.query('from_date', from_date, 'iso-8601')
        if aggregation_type is not None:
            query_parameters['aggregationType'] = self._serialize.query('aggregation_type', aggregation_type, 'str')
        if skip is not None:
            query_parameters['$skip'] = self._serialize.query('skip', skip, 'int')
        if top is not None:
            query_parameters['$top'] = self._serialize.query('top', top, 'int')
        response = self._send(http_method='GET',
                              location_id='df7fbbca-630a-40e3-8aa3-7a3faf66947e',
                              version='5.0-preview.1',
                              route_values=route_values,
                              query_parameters=query_parameters)
        return self._deserialize('[RepositoryActivityMetrics]', self._unwrap_collection(response))
--------------------
Prompt: 
def pretty_print_graphql(query, use_four_spaces=True):
    """Take a GraphQL query, pretty print it, and return it."""
----------
Output: 

    return query.replace(' ', '\n').replace('\t','')


def get_graphql_schema(query):
    """Return the schema for a GraphQL query."""
    return 'graphql'


def get_graphql_query(query):
    """Return a GraphQL query."""
    return query.replace(' ', '\n').replace('\t','')


def get_graphql_query_from_file(filename):
    """Return a GraphQL query from a file."""
    with open(filename, 'r') as f:
        return get_graphql_query
----------
Solution: 

    # Use our custom visitor, which fixes directive argument order
    # to get the canonical representation
    output = visit(parse(query), CustomPrintingVisitor())

    # Using four spaces for indentation makes it easier to edit in
    # Python source files.
    if use_four_spaces:
        return fix_indentation_depth(output)
    return output
--------------------
Prompt: 
def hdf5_read_out(self):
        """Read out an hdf5 file.

        Takes the output of :class:`gwsnrcalc.genconutils.genprocess.GenProcess`
        and reads it out to an HDF5 file.

        """
----------
Output: 

        self.logger.debug("Reading out file %s", self.filename)
        self.h5file = tables.openFile(self.filename, mode="w", title="out")
        self.h5file.createTable(self.h5file.root, 'gwsnrcalc_out',
                                 self.gwsnrcalc_out,
                                 'gwsnrcalc_out',
                                 'gwsnrcalc_out',
                                 'gwsnrcalc_out',
                                 'gwsnrcalc_out',
                                 'gwsnrcalc_out',
                                 'gwsnrcalc
----------
Solution: 

        with h5py.File(self.WORKING_DIRECTORY + '/' + self.output_file_name, 'w') as f:

            header = f.create_group('header')
            header.attrs['Title'] = 'Generated SNR Out'
            header.attrs['Author'] = 'Generator by: Michael Katz'
            header.attrs['Date/Time'] = str(datetime.datetime.now())

            for which in ['x', 'y']:
                header.attrs[which + 'val_name'] = getattr(self, which + 'val_name')
                header.attrs['num_' + which + '_pts'] = getattr(self, 'num_' + which)

            ecc = 'eccentricity' in self.__dict__
            if ecc:
                name_list = ['observation_time', 'start_frequency', 'start_separation'
                             'eccentricity']
            else:
                name_list = ['spin_1', 'spin_2', 'spin', 'end_time']

            name_list += ['total_mass', 'mass_ratio', 'start_time', 'luminosity_distance',
                          'comoving_distance', 'redshift']

            for name in name_list:
                if name != self.xval_name and name != self.yval_name:
                    try:
                        getattr(self, name)
                        header.attrs[name] = getattr(self, name)
                    except AttributeError:
                        pass

            if self.added_note != '':
                header.attrs['Added note'] = self.added_note

            data = f.create_group('data')

            # read out x,y values in compressed data set
            dset = data.create_dataset(self.x_col_name, data=self.xvals,
                                       dtype='float64', chunks=True,
                                       compression='gzip', compression_opts=9)

            dset = data.create_dataset(self.y_col_name, data=self.yvals,
                                       dtype='float64', chunks=True,
                                       compression='gzip', compression_opts=9)

            # read out all datasets
            for key in self.output_dict.keys():
                dset = data.create_dataset(key, data=self.output_dict[key],
                                           dtype='float64', chunks=True,
                                           compression='gzip', compression_opts=9)
--------------------
Prompt: 
def load(self):
        """Load all available DRPs in 'entry_point'."""
----------
Output: 

        self.load_drp_entries()
        self.load_drp_entries_from_file()
        self.load_drp_entries_from_file()
        self.load_drp_entries_from_file()

    def load_drp_entries_from_file(self):
        """Load DRP entries from a file."""
        self.drp_entries = []
        for line in self.file:
            line = line.strip()
            if line:
                entry = self.parse_entry(line)
                self.drp_entries.append(entry)
----------
Solution: 


        for drpins in self.iload(self.entry):
            self.drps[drpins.name] = drpins

        return self
--------------------
Prompt: 
def prev_moment_operating_on(
            self,
            qubits: Sequence[ops.Qid],
            end_moment_index: Optional[int] = None,
            max_distance: Optional[int] = None) -> Optional[int]:
        """Finds the index of the next moment that touches the given qubits.

        Args:
            qubits: We're looking for operations affecting any of these qubits.
            end_moment_index: The moment index just after the starting point of
                the reverse search. Defaults to the length of the list of
                moments.
            max_distance: The number of moments (starting just before from the
                end index and moving backward) to check. Defaults to no limit.

        Returns:
            None if there is no matching moment, otherwise the index of the
            latest matching moment.

        Raises:
            ValueError: negative max_distance.
        """
----------
Output: 
: The maximum distance to search. If not None,
                then the search will be limited to this number of operations.
                Defaults to the number of operations.

        Returns:
            The index of the next moment that touches the given qubits.
        """
        if end_moment_index is None:
            end_moment_index = len(qubits)
        if max_distance is None:
            max_distance = len(self.qubits)
        if max_distance < 0:
            return None
        if end_moment_index < 0:
            return end_distance
        if max_distance > 0:
----------
Solution: 

        if end_moment_index is None:
            end_moment_index = len(self._moments)

        if max_distance is None:
            max_distance = len(self._moments)
        elif max_distance < 0:
            raise ValueError('Negative max_distance: {}'.format(max_distance))
        else:
            max_distance = min(end_moment_index, max_distance)

        # Don't bother searching indices past the end of the list.
        if end_moment_index > len(self._moments):
            d = end_moment_index - len(self._moments)
            end_moment_index -= d
            max_distance -= d
        if max_distance <= 0:
            return None

        return self._first_moment_operating_on(qubits,
                                               (end_moment_index - k - 1
                                                for k in range(max_distance)))
--------------------
Prompt: 
def format_value(column_dict, value, key=None):
    """
    Format a value coming from the database (for example converts datetimes to
    strings)

    :param column_dict: The column datas collected during inspection
    :param value: A value coming from the database
    :param key: The exportation key
    """
----------
Output: 

    if key:
        return column_dict[key]
    return value


class Exportation(object):
    """
    A class for exportation of course data.

    :param course_id: The id of the course
    :param exportation_type: The type of exportation (e.g. "verified", "honor", "audit", etc)
    """
    def __init__(self, course_id, exportation_type):
        self.course_id = course_id
        self.exportation_type = exportation_type
        self.start_date = None
        self.end
----------
Solution: 

    formatter = column_dict.get('formatter')
    prop = column_dict['__col__']

    res = value

    if value in ('', None,):
        res = ''

    elif formatter is not None:
        res = formatter(value)

    else:
        if hasattr(prop, "columns"):
            sqla_column = prop.columns[0]
            column_type = getattr(sqla_column.type, 'impl', sqla_column.type)

            formatter = FORMATTERS_REGISTRY.get_formatter(column_type, key)
            if formatter is not None:
                res = formatter(value)
    return res
--------------------
Prompt: 
def modified_created(instance):
    """`modified` property must be later or equal to `created` property
    """
----------
Output: 

    if instance.modified:
        return True
    return False


def get_default_ip_protocol():
    """`ip_protocol` property must be the IP protocol used by the
    server.
    """
    return IP_PROTOCOL_MAP.get(DEFAULT_PROTOCOL, DEFAULT_PROTOCOL)


def get_default_ip_protocol_by_name(name):
    """`ip_protocol` property must be the IP protocol used by the
    server.
    """
    return IP_PROTOCOL_MAP.get(DEFAULT_PROTOCOL, DEFAULT_IP_PROTOCOL_MAP.get(name, DEFAULT_
----------
Solution: 

    if 'modified' in instance and 'created' in instance and \
            instance['modified'] < instance['created']:
        msg = "'modified' (%s) must be later or equal to 'created' (%s)"
        return JSONError(msg % (instance['modified'], instance['created']),
                         instance['id'])
--------------------
Prompt: 
def _state_command(self, container_id=None, command='start', sudo=None):

    """ A generic state command to wrap pause, resume, kill, etc., where the
        only difference is the command. This function will be unwrapped if the
        child functions get more complicated (with additional arguments).

        Equivalent command line example:      
           singularity oci <command> <container_ID>
           
        Parameters
        ==========
        container_id: the id to start.
        command: one of start, resume, pause, kill, defaults to start.
        sudo: Add sudo to the command. If the container was created by root,
              you need sudo to interact and get its state.

        Returns
        =======
        return_code: the return code to indicate if the container was started.
    """
----------
Output: 

    """

    def _state_command(self, sudo=None):

        """
        This function will wrap pause, resume, kill, etc., where the
        only difference is the command. This function will be unwrapped if the
        child functions get more complicated (with additional arguments).

        Equivalent command line example:      
           pause oci <command> <container_ID>
           
        Parameters
        ==========
        sudo: Add sudo to the command.
        """

        if not self.args.pause:
            self.args.pause = True

        if not self.args.resume:
            self.args.resume = True

       
----------
Solution: 

    sudo = self._get_sudo(sudo)
    container_id = self.get_container_id(container_id)

    # singularity oci state
    cmd = self._init_command(command)

    # Finally, add the container_id
    cmd.append(container_id)

    # Run the command, return return code
    return self._run_and_return(cmd, sudo)
--------------------
Prompt: 
def compile_truncate(self, query):
        """
        Compile a truncate statement into SQL

        :param query: A QueryBuilder instance
        :type query: QueryBuilder

        :return: The compiled truncate statement
        :rtype: str
        """
----------
Output: 

        return query.tokens[0]

    def compile_update(self, query):
        """
        Compile a update statement into SQL

        :param query: A QueryBuilder instance
        :type query: QueryBuilder

        :return: The compiled update statement
        :rtype: str
        """
        return query.tokens[1]

    def compile_delete(self, query):
        """
        Compile a delete statement into SQL

        :param query: A QueryBuilder instance
        :type query: QueryBuilder

        :return: The compiled delete statement
        :rtype: str
        """
        return query.tokens[2]

    def compile_
----------
Solution: 

        sql = {
            "DELETE FROM sqlite_sequence WHERE name = %s"
            % self.get_marker(): [query.from__]
        }

        sql["DELETE FROM %s" % self.wrap_table(query.from__)] = []

        return sql
--------------------
Prompt: 
def get_queue_for_job(self, job):
        """
        Returns a queue to put job into.
        """
----------
Output: 

        return self.queues.get(job, None)

    def get_job_for_job_id(self, job_id):
        """
        Returns a job to get from the queue with the given job id.
        """
        return self.jobs.get(job_id, None)

    def get_job_status(self, job_id):
        """
        Returns a job status.
        """
        return self.jobs.get(job_id, None)

    def get_job_status_id(self, job_id):
        """
        Returns a job status id.
        """
----------
Solution: 

        if self._queue is not None:
            return self._queue
        key = '{0}{1}'.format(self.queue_class.redis_queue_namespace_prefix,
                              job.origin)
        return self.queue_class.from_queue_key(
                key, connection=self.connection, job_class=self.job_class)
--------------------
Prompt: 
def _add_transcripts(self, variant_obj, info_dict):
        """Return all transcripts sound in the vcf file"""
----------
Output: 

        for transcript in self.transcripts:
            if transcript.name == variant_obj.name:
                transcript.add_transcript(variant_obj)
                return transcript
        return None

    def _add_exons(self, exon_obj, info_dict):
        """Return all exons in the vcf file"""
        for exon in self.exons:
            if exon.name == exon_obj.name:
                exon.add_exon(exon_obj)
                return exon
        return None

    def _add_introns(self, intron_obj
----------
Solution: 

        vep_string = info_dict.get('CSQ')

        #Check if snpeff annotation:
        snpeff_string = info_dict.get('ANN')
        
        # We check one of these.
        # VEP has presedence over snpeff
        if vep_string:
            #Get the vep annotations
            vep_info = get_vep_info(
                vep_string = vep_string,
                vep_header = self.vep_header
                )
            for transcript_info in vep_info:
                transcript = self._get_vep_transcript(transcript_info)
                variant_obj.add_transcript(transcript)

        elif snpeff_string:
            #Get the vep annotations
            snpeff_info = get_snpeff_info(
                snpeff_string = snpeff_string,
                snpeff_header = self.snpeff_header
                )
            for transcript_info in snpeff_info:
                transcript = self._get_snpeff_transcript(transcript_info)
                variant_obj.add_transcript(transcript)
--------------------
Prompt: 
def background(self):
        """Only a getter on purpose. See the tests."""
----------
Output: 

        return self._background

    @background.setter
    def background(self, value):
        """Setter for background property."""
        self._background = value

    @property
    def state(self):
        """Getter for state property."""
        return self._state

    @state.setter
    def state(self, value):
        """Setter for state property."""
        self._state = value

    @property
    def supported_features(self):
        """Getter for supported features property."""
        return self._supported_features

    @supported_features.setter
    def supported_features(self, value):
        """Setter for supported features property
----------
Solution: 

        if self._background is None:
            self._background = GSBackgroundLayer()
            self._background._foreground = self
        return self._background
--------------------
Prompt: 
def kem(request):
	"""
	due to the base directory settings of django, the model_path needs to be different when
	testing with this section.
	"""
----------
Output: 

	return request.path

def get_test_path(request):
	"""
	get_test_path is a method of the model_path to test the file system.
	"""
	return request.model_path

def get_test_path_from_path(path):
	"""
	get_test_path_from_path is a method of the model_path to test the file system.
	"""
	return path.split(path_delimiter)[0]

def get_test_path_from_file(path):
	
----------
Solution: 

	keyword = request.GET['keyword']
	lang = request.GET['lang']
	ontology = 'ontology' if 'ontology' in request.GET and bool(json.loads(request.GET['ontology'].lower())) else 'origin'
	result = multilanguage_model[lang][ontology].most_similar(keyword, int(request.GET['num']) if 'num' in request.GET else 10)
	return JsonResponse(result, safe=False)
--------------------
Prompt: 
def _CaptureRequestLogId(self):
    """Captures the request log id if possible.

    The request log id is stored inside the breakpoint labels.
    """
----------
Output: 

    if self._request_log_id:
      return self._request_log_id

    if self._process_id:
      return self._process_id

    if self._process_name:
      return self._process_name

    return None

  def _GetProcessName(self):
    """Returns the name of the process that owns the current thread.

    Returns:
      The name of the process that owns the current thread or None if the
      current thread has not been determined.
    """
    if self._process_id:
      return self._process_id.split(".")[-1]

    return
----------
Solution: 

    # pylint: disable=not-callable
    if callable(request_log_id_collector):
      request_log_id = request_log_id_collector()
      if request_log_id:
        # We have a request_log_id, save it into the breakpoint labels
        self.breakpoint['labels'][
            labels.Breakpoint.REQUEST_LOG_ID] = request_log_id
--------------------
Prompt: 
def resize(self, package):
        """
        ::
        
            POST /:login/machines/:id?action=resize
        
        Initiate resizing of the remote machine to a new package.
        """
----------
Output: 

        return self._post("/machines/%s/action" % package.id,
                         data=dict(id=package.id, action=self._get_action_info(package)))

    def _get_action_info(self, action):
        """
        ::
        
            _get_action_info(action)
        
        Retrieve the info about an action.
        """
        return self._get("/machines/%s/action/%s/info" % (self.id, action.id),
                         data={"action": action.to_dict()})

    def _post(self, url, data=None):
       
----------
Solution: 

        if isinstance(package, dict):
            package = package['name']
        action = {'action': 'resize',
                  'package': package}
        j, r = self.datacenter.request('POST', self.path, params=action)
        r.raise_for_status()
--------------------
Prompt: 
def _parse_raid_config_data(self, raid_config):
        """It will parse raid config data based on raid controllers

        :param raid_config: A dictionary containing target raid configuration
                            data. This data stucture should be as follows:
                            raid_config = {'logical_disks': [{'raid_level': 1,
                            'size_gb': 100, 'controller':
                            'HPE Smart Array P408i-a SR Gen10'},
                            <info-for-logical-disk-2>]}
        :returns: A dictionary of controllers, each containing list of
                  their respected logical drives.
        """
----------
Output: 
 of disks and
                  a list of dictionaries with keys:
                      'logical_disks': [{'raid_level': 1,
                           'size_gb': 100, 'controller':
                            'HPE Smart Array P408i-a'},
                            <info-for-logical-disk-2>]}
        """
        raid_controllers = {}
        for controller in raid_config['logical_disks']:
            if controller['raid_level'] not in raid_controllers:
                raid_controllers[controller['raid_level']] = {}
            if controller['size_gb'] not in raid_controllers[controller['
----------
Solution: 

        default = (
            self.smart_storage.array_controllers.get_default_controller.model)
        controllers = {default: []}
        for ld in raid_config['logical_disks']:
            if 'controller' not in ld.keys():
                controllers[default].append(ld)
            else:
                ctrl = ld['controller']
                if ctrl not in controllers:
                    controllers[ctrl] = []
                controllers[ctrl].append(ld)
        return controllers
--------------------
Prompt: 
def on_switch_page(self, notebook, page_pointer, page_num, user_param1=None):
        """Update state selection when the active tab was changed
        """
----------
Output: 

        self.notebook = notebook
        self.page_pointer = page_pointer
        self.page_num = page_num
        self.user_param1 = user_param1
        self.user_param2 = user_param2

    def on_switch_to_next_button(self, notebook, page_num, user_param1=None):
        """Update state selection when the active tab was changed
        """
        self.notebook.set_current_page(self.page_pointer + 1)
        self.notebook.set_next_page(self.page_pointer + 2)
----------
Solution: 

        page = notebook.get_nth_page(page_num)

        # find state of selected tab
        for tab_info in list(self.tabs.values()):
            if tab_info['page'] is page:
                state_m = tab_info['state_m']
                sm_id = state_m.state.get_state_machine().state_machine_id
                selected_state_m = self.current_state_machine_m.selection.get_selected_state()

                # If the state of the selected tab is not in the selection, set it there
                if selected_state_m is not state_m and sm_id in self.model.state_machine_manager.state_machines:
                    self.model.selected_state_machine_id = sm_id
                    self.current_state_machine_m.selection.set(state_m)
                return
--------------------
Prompt: 
def radio_status_encode(self, rssi, remrssi, txbuf, noise, remnoise, rxerrors, fixed):
                """
                Status generated by radio and injected into MAVLink stream.

                rssi                      : Local signal strength (uint8_t)
                remrssi                   : Remote signal strength (uint8_t)
                txbuf                     : Remaining free buffer space in percent. (uint8_t)
                noise                     : Background noise level (uint8_t)
                remnoise                  : Remote background noise level (uint8_t)
                rxerrors                  : Receive errors (uint16_t)
                fixed                     : Count of error corrected packets (uint16_t)

                """
----------
Output: 
 errors (uint8_t)
                fixed                   : Fixed signal strength (uint8_t)
                """
                if not fixed:
                        return rssi
                if not rxerrors:
                        return remrssi
                if not rssi:
                        return rxbuf
                if noise > 0:
                        return self.encode_float(txbuf, noise)
                return rssi

        def radio_status_decode(self, rssi, remrssi, txbuf, noise, remnoise, rxerrors, fixed):
                """
                Status generated by radio and injected into MAVLink stream.

                rssi                      : Local signal strength
----------
Solution: 

                return MAVLink_radio_status_message(rssi, remrssi, txbuf, noise, remnoise, rxerrors, fixed)
--------------------
Prompt: 
def _residual(self, x, in_filter, out_filter, stride,
                activate_before_residual=False):
    """Residual unit with 2 sub layers."""
----------
Output: 

    with tf.variable_scope('residual'):
      x = self._conv_block(x, in_filter, out_filter, stride)
      x = self._conv_block(x, in_filter, out_filter, out_filter)
      x = self._conv_block(x, out_filter, out_filter, activate_before_residual)
      x = self._conv_block(x, out_filter, out_filter, stride)
      x = self._conv_block(x, out_filter, out_filter, False)
      x = self._conv_
----------
Solution: 

    if activate_before_residual:
      with tf.variable_scope('shared_activation'):
        x = self._layer_norm('init_bn', x)
        x = self._relu(x, self.hps.relu_leakiness)
        orig_x = x
    else:
      with tf.variable_scope('residual_only_activation'):
        orig_x = x
        x = self._layer_norm('init_bn', x)
        x = self._relu(x, self.hps.relu_leakiness)

    with tf.variable_scope('sub1'):
      x = self._conv('conv1', x, 3, in_filter, out_filter, stride)

    with tf.variable_scope('sub2'):
      x = self._layer_norm('bn2', x)
      x = self._relu(x, self.hps.relu_leakiness)
      x = self._conv('conv2', x, 3, out_filter, out_filter, [1, 1, 1, 1])

    with tf.variable_scope('sub_add'):
      if in_filter != out_filter:
        orig_x = tf.nn.avg_pool(orig_x, stride, stride, 'VALID')
        orig_x = tf.pad(
            orig_x, [[0, 0], [0, 0], [0, 0],
                     [(out_filter - in_filter) // 2,
                      (out_filter - in_filter) // 2]])
      x += orig_x

    return x
--------------------
Prompt: 
def build_re_pattern_from_intervals(intervals: IntervalListType) -> BuiltInReType:
    """
    Convert intervals to regular expression pattern.

    :param intervals: Unicode codepoint intervals.
    """
----------
Output: 

    if not intervals:
        return None
    if len(intervals) == 1:
        return IntervalPattern(intervals[0])
    if len(intervals) == 2:
        return BuiltInReType(intervals[0], intervals[1])
    if len(intervals) == 3:
        return IntervalType(intervals[0], intervals[1], intervals[2])
    if len(intervals) == 4:
        return IntervalType(intervals[0], intervals[1], intervals[2], intervals[3], intervals[4])
    if len(intervals) == 5:
        return IntervalPattern(intervals[0
----------
Solution: 


    inner = [f'{chr(lb)}-{chr(ub)}' for lb, ub in intervals]
    joined_inner = ''.join(inner)
    pattern = f'[{joined_inner}]+'

    return re.compile(pattern, re.UNICODE)
--------------------
Prompt: 
def thirds(reference_labels, estimated_labels):
    """Compare chords along root & third relationships.

    Examples
    --------
    >>> (ref_intervals,
    ...  ref_labels) = mir_eval.io.load_labeled_intervals('ref.lab')
    >>> (est_intervals,
    ...  est_labels) = mir_eval.io.load_labeled_intervals('est.lab')
    >>> est_intervals, est_labels = mir_eval.util.adjust_intervals(
    ...     est_intervals, est_labels, ref_intervals.min(),
    ...     ref_intervals.max(), mir_eval.chord.NO_CHORD,
    ...     mir_eval.chord.NO_CHORD)
    >>> (intervals,
    ...  ref_labels,
    ...  est_labels) = mir_eval.util.merge_labeled_intervals(
    ...      ref_intervals, ref_labels, est_intervals, est_labels)
    >>> durations = mir_eval.util.intervals_to_durations(intervals)
    >>> comparisons = mir_eval.chord.thirds(ref_labels, est_labels)
    >>> score = mir_eval.chord.weighted_accuracy(comparisons, durations)

    Parameters
    ----------
    reference_labels : list, len=n
        Reference chord labels to score against.
    estimated_labels : list, len=n
        Estimated chord labels to score against.

    Returns
    -------
    comparison_scores : np.ndarray, shape=(n,), dtype=float
        Comparison scores, in [0.0, 1.0]

    """
----------
Output: 
min(), est_intervals.max())
    >>> (ref_labels,
   ...  ref_intervals) = mir_eval.util.thirds(reference_labels, ref_labels)
    >>> (est_labels,
   ...  est_labels) = mir_eval.util.thirds(reference_labels, ref_labels)
    >>> (ref_labels,
   ...  ref_intervals) = mir_eval.util.ref_intervals(
   ...  ref_labels, ref_intervals)
    (0, 0)

    """
    # Check that the reference labels
----------
Solution: 

    validate(reference_labels, estimated_labels)
    ref_roots, ref_semitones = encode_many(reference_labels, False)[:2]
    est_roots, est_semitones = encode_many(estimated_labels, False)[:2]

    eq_roots = ref_roots == est_roots
    eq_thirds = ref_semitones[:, 3] == est_semitones[:, 3]
    comparison_scores = (eq_roots * eq_thirds).astype(np.float)

    # Ignore 'X' chords
    comparison_scores[np.any(ref_semitones < 0, axis=1)] = -1.0
    return comparison_scores
--------------------
Prompt: 
def get_sections(self, section_name):
        """
        Return the list of sections stored in self.timers() given `section_name`
        A fake section is returned if the timer does not have section_name.
        """
----------
Output: 

        return []

    def get_timers(self):
        """
        Return a list of fake timers stored in self.timers()
        """
        return []

    def get_timers_count(self):
        """
        Return the number of fake timers stored in self.timers()
        """
        return len(self.timers_list)

    def get_timer_by_name(self, name):
        """
        Return a fake timer object by name
        """
        return self.timers_list[self.get_timer_by_name_key(name)]

    def get_timer_by_name_
----------
Solution: 

        sections = []
        for timer in self.timers():
            for sect in timer.sections:
                if sect.name == section_name:
                    sections.append(sect)
                    break
            else:
                sections.append(AbinitTimerSection.fake())

        return sections
--------------------
Prompt: 
def draw_MM0(self):
        """
        Draws the M/M0 plot in the GUI on canvas3
        """
----------
Output: 

        self.plot_M0.set_xlabel('Time (s)')
        self.plot_M0.set_ylabel('M/M0 (dB)')
        self.plot_M0.set_title('M/M0 (dB)')
        self.plot_M0.set_ylim(0, self.plot_M0.get_ylim()[1])
        self.plot_M0.set_ylim(0, self.plot_M0.get_ylim()[2])
        self.plot_M0.set_xlim(0, self.plot_M0
----------
Solution: 

        self.fig3.clf()
        self.fig3.text(0.02, 0.96, 'M/M0', {'family': self.font_type, 'fontsize': 10 *
                                            self.GUI_RESOLUTION, 'style': 'normal', 'va': 'center', 'ha': 'left'})
        self.mplot = self.fig3.add_axes(
            [0.2, 0.15, 0.7, 0.7], frameon=True, facecolor='None')

        thermal_x, thermal_y = [], []
        thermal_x_bad, thermal_y_bad = [], []
        af_x, af_y = [], []
        af_x_bad, af_y_bad = [], []
        for i in range(len(self.Data[self.s]['zijdblock'])):
            step = self.Data[self.s]['zijdblock_steps'][i]
            # bad point
            if self.Data[self.s]['measurement_flag'][i] == 'b':
                if step == "0":
                    thermal_x_bad.append(self.Data[self.s]['zijdblock'][i][0])
                    af_x_bad.append(self.Data[self.s]['zijdblock'][i][0])
                    thermal_y_bad.append(
                        self.Data[self.s]['zijdblock'][i][3]/self.Data[self.s]['zijdblock'][0][3])
                    af_y_bad.append(
                        self.Data[self.s]['zijdblock'][i][3]/self.Data[self.s]['zijdblock'][0][3])
                elif "C" in step:
                    thermal_x_bad.append(self.Data[self.s]['zijdblock'][i][0])
                    thermal_y_bad.append(
                        self.Data[self.s]['zijdblock'][i][3]/self.Data[self.s]['zijdblock'][0][3])
                elif "T" in step:
                    af_x_bad.append(self.Data[self.s]['zijdblock'][i][0])
                    af_y_bad.append(
                        self.Data[self.s]['zijdblock'][i][3]/self.Data[self.s]['zijdblock'][0][3])
                else:
                    continue

            else:
                if step == "0":
                    thermal_x.append(self.Data[self.s]['zijdblock'][i][0])
                    af_x.append(self.Data[self.s]['zijdblock'][i][0])
                    thermal_y.append(
                        self.Data[self.s]['zijdblock'][i][3]/self.Data[self.s]['zijdblock'][0][3])
                    af_y.append(self.Data[self.s]['zijdblock'][i]
                                [3]/self.Data[self.s]['zijdblock'][0][3])
                elif "C" in step:
                    thermal_x.append(self.Data[self.s]['zijdblock'][i][0])
                    thermal_y.append(
                        self.Data[self.s]['zijdblock'][i][3]/self.Data[self.s]['zijdblock'][0][3])
                elif "T" in step:
                    af_x.append(self.Data[self.s]['zijdblock'][i][0])
                    af_y.append(self.Data[self.s]['zijdblock'][i]
                                [3]/self.Data[self.s]['zijdblock'][0][3])
                else:
                    continue

        if len(thermal_x)+len(thermal_x_bad) > self.Data[self.s]['zijdblock_steps'].count('0'):
            self.mplot.plot(thermal_x, thermal_y, 'ro-',
                            markersize=self.MS, lw=1, clip_on=False, zorder=1)
            for i in range(len(thermal_x_bad)):
                self.mplot.plot([thermal_x_bad[i]], [thermal_y_bad[i]], 'o',
                                mfc='None', mec='k', markersize=self.MS, clip_on=False, zorder=1)

        self.mplot.set_xlabel('Thermal (C)', color='r')
        for tl in self.mplot.get_xticklabels():
            tl.set_color('r')

        self.mplot_af = self.mplot.twiny()
        if len(af_x)+len(af_x_bad) > self.Data[self.s]['zijdblock_steps'].count('0'):
            self.mplot_af.plot(
                af_x, af_y, 'bo-', markersize=self.MS, lw=1, clip_on=False, zorder=1)
            for i in range(len(af_x_bad)):
                self.mplot_af.plot([af_x_bad[i]], [
                                   af_y_bad[i]], 'o', mfc='None', mec='k', markersize=self.MS, clip_on=False, zorder=1)

        self.mplot_af.set_xlabel('AF (mT)', color='b')
        for tl in self.mplot_af.get_xticklabels():
            tl.set_color('b')

        self.mplot.tick_params(axis='both', which='major', labelsize=7)
        self.mplot_af.tick_params(axis='both', which='major', labelsize=7)
        self.mplot.spines["right"].set_visible(False)
        self.mplot_af.spines["right"].set_visible(False)
        self.mplot.get_xaxis().tick_bottom()
        self.mplot.get_yaxis().tick_left()
        self.mplot.set_ylabel("M / NRM0", fontsize=8*self.GUI_RESOLUTION)

        self.canvas3.draw()
--------------------
Prompt: 
def show_driver(devname):
    """
    Queries the specified network device for associated driver information

    CLI Example:

    .. code-block:: bash

        salt '*' ethtool.show_driver <devname>
    """
----------
Output: 

    cmd = [
        'ethtool',
       'show',
        'network',
       'show-interface',
        'driver',
       'show-ip',
       'show-address',
       'show-carrier',
       'show-carrier-port',
       'show-carrier-state',
       'show-carrier-type',
       'show-carrier-ip',
       'show-carrier-address',
       'show-carrier-state',
       'show-carrier-type',
       'show-carrier-ip',
       'show-carrier-state',
       'show-carrier-type',
       'show
----------
Solution: 


    try:
        module = ethtool.get_module(devname)
    except IOError:
        log.error('Driver information not implemented on %s', devname)
        return 'Not implemented'

    try:
        businfo = ethtool.get_businfo(devname)
    except IOError:
        log.error('Bus information no available on %s', devname)
        return 'Not available'

    ret = {
        'driver': module,
        'bus_info': businfo,
    }

    return ret
--------------------
Prompt: 
def receive_information_confirmation(self, message):
        """A InformationConfirmation is received.

        If :meth:`the api version is supported
        <AYABInterface.communication.Communication.api_version_is_supported>`,
        the communication object transitions into a
        :class:`InitializingMachine`, if unsupported, into a
        :class:`UnsupportedApiVersion`
        """
----------
Output: 

        raise NotImplementedError

    def api_version_is_supported(self, version):
        """Check if the API version is supported.

        :param version: The API version to check.
        :type version: :class:`ayab.Interface.APIVersion`
        :returns: True if the API version is supported, False otherwise.
        :rtype: bool
        """
        raise NotImplementedError

    def get_information_confirmation(self, message):
        """Get information confirmation.

        :param message: The information confirmation message.
        :type message: :class:`information_confirmation.InformationConfirmationMessage`
        :returns:
----------
Solution: 

        if message.api_version_is_supported():
            self._next(InitializingMachine)
        else:
            self._next(UnsupportedApiVersion)

        self._communication.controller = message
--------------------
Prompt: 
def _check_worktree_support(failhard=True):
    """
    Ensure that we don't try to operate on worktrees in git < 2.5.0.
    """
----------
Output: 

    if not failhard:
        run_cmd("git", "checkout", "-b", "test-branch", "master")
        run_cmd("git", "add", "test-branch")
        run_cmd("git", "commit", "-m", "test commit")
        run_cmd("git", "rm", "test-branch")
    else:
        run_cmd("git", "checkout", "-b", "master")
        run_cmd("git", "add", "master")
        run_cmd("git", "commit", "-m", "test commit")


def _check
----------
Solution: 

    git_version = version(versioninfo=False)
    if _LooseVersion(git_version) < _LooseVersion('2.5.0'):
        if failhard:
            raise CommandExecutionError(
                'Worktrees are only supported in git 2.5.0 and newer '
                '(detected git version: ' + git_version + ')'
            )
        return False
    return True
--------------------
Prompt: 
def enumeration_to_list(enm):
    """
    Turns the java.util.Enumeration into a list.

    :param enm: the enumeration to convert
    :type enm: JB_Object
    :return: the list
    :rtype: list
    """
----------
Output: 

    return javabridge.call(javabridge.get_env(), "java.util.EnumerationToList", "()V")


def get_class_name(class_name):
    """
    Returns the name of the Java class.

    :param class_name: the name of the class
    :type class_name: str
    :return: the class name
    :rtype: str
    """
    return javabridge.call(
        env(), "java.lang.String", class_name, "()Lweka/core/String;")


def get_field_
----------
Solution: 

    result = []
    while javabridge.call(enm, "hasMoreElements", "()Z"):
        result.append(javabridge.call(enm, "nextElement", "()Ljava/lang/Object;"))
    return result
--------------------
Prompt: 
def get_unique_name(self, name):
        """ returns an available unique name

        Parameters
        ----------
        name : str
            name to be made unique

        Returns
        -------
        unique_name : str
            new unique name

        """
----------
Output: 

        return self.unique_name

    def get_name(self, name):
        """ returns an available name

        Parameters
        ----------
        name : str
            name to be made unique

        Returns
        -------
        name : str
            new name

        """
        return self.name

    def get_name_and_type(self, name, dtype):
        """ returns an available name and type

        Parameters
        ----------
        name : str
            name to be made unique
        dtype : str
            type to be made unique

        Returns
        -------
        name : str
            name and type

        """
        return self.name, self.dtype, self
----------
Solution: 


        if name not in self._db:
            self._db[name] = 0
            return name
        else:
            index = self._db[name]
            self._db[name] = index + 1
            return f"{name}_{index}"
--------------------
Prompt: 
def update_compaction(model):
    """Updates the compaction options for the given model if necessary.

    :param model: The model to update.

    :return: `True`, if the compaction options were modified in Cassandra,
        `False` otherwise.
    :rtype: bool
    """
----------
Output: 

    if model.compaction_options is None:
        return False

    if not model.compaction_options.get('compaction_strategy', None):
        strategy = model.compaction_options.get('compaction_strategy', None)
        if strategy is None:
            return False
        strategy = strategy.upper()
        if strategy == 'ONE':
            model.compaction_options['compaction_strategy'] = 'ONE_TO_ONE'
        elif strategy == 'TWO':
            model.compaction_options['compaction_strategy'] = 'TWO_TO_TWO'
       
----------
Solution: 

    logger.debug("Checking %s for compaction differences", model)
    table = get_table_settings(model)

    existing_options = table.options.copy()

    existing_compaction_strategy = existing_options['compaction_strategy_class']

    existing_options = json.loads(existing_options['compaction_strategy_options'])

    desired_options = get_compaction_options(model)

    desired_compact_strategy = desired_options.get('class', SizeTieredCompactionStrategy)

    desired_options.pop('class', None)

    do_update = False

    if desired_compact_strategy not in existing_compaction_strategy:
        do_update = True

    for k, v in desired_options.items():
        val = existing_options.pop(k, None)
        if val != v:
            do_update = True

    # check compaction_strategy_options
    if do_update:
        options = get_compaction_options(model)
        # jsonify
        options = json.dumps(options).replace('"', "'")
        cf_name = model.column_family_name()
        query = "ALTER TABLE {} with compaction = {}".format(cf_name, options)
        logger.debug(query)
        execute(query)
        return True

    return False
--------------------
Prompt: 
def _copyDPToClipboard(self):
        """Callback for item menu."""
----------
Output: 

        self.on_dmenu_copy_activate(None)

    def on_dmenu_copy_activate(self, widget):
        """Callback for item menu."""
        if widget.get_active():
            self.dmenu.copy_clipboard(widget.get_active())
        else:
            self.dmenu.copy_clipboard(widget.get_active(), False)

    def on_dmenu_paste_activate(self, widget):
        """Callback for item menu."""
        if widget.get_active():
            self.dmenu.paste_clipboard(widget.get_
----------
Solution: 

        dp = self._dp_menu_on
        if dp and dp.archived:
            path = dp.fullpath.replace(" ", "\\ ")
            QApplication.clipboard().setText(path, QClipboard.Clipboard)
            QApplication.clipboard().setText(path, QClipboard.Selection)
--------------------
Prompt: 
def _buildElementTree(self,):
        """Turns object into a Element Tree
        """
----------
Output: 

        element = ElementTree.Element('test')
        element.text = 'test'
        element.tail = '\n'
        return element

    def test_parse_xml_file(self):
        """XML file parsing
        """
        file = open(os.path.join(self.test_dir, 'test.xml'), 'r')
        xml_file = file.read()
        file.close()
        self.assertEqual(xml_file, TEST_XML_FILE)
        self.assertEqual(self._buildElementTree(), TEST_XML_FILE)
        self.assertEqual(self._buildElementTree(file
----------
Solution: 

        t_binder = ctree.Element(self.name)

        for k,v in self.__dict__.items():
            if k not in ('name', 'urls', 'inputs', 'paging') and v :
                t_binder.set(k,v)

        self.etree = t_binder
        return t_binder
--------------------
Prompt: 
def _get_vectors(self, tree, precomputed_info):
        """
        Populate the vectors m and M.
        """
----------
Output: 

        self.m = {}
        self.M = {}
        for i in range(len(tree)):
            self.m[i] = {}
            self.M[i]['m'] = tree[i]
            self.M[i]['m']['m'] = self.m[i]['m']['m']
            self.M[i]['m']['m']['m'] = self.M[i]['m']['m']
            self.M[i]['m']['m']['m'] = self.M[i]['m']['m']
            self.M[i]['m']['m']['m']
----------
Solution: 

        little_m = []
        big_m = []

        leaf_nodes = sorted(tree.leaf_nodes(), key=lambda x: x.taxon.label)
        # inner nodes, sorted order
        for leaf_a, leaf_b in combinations(leaf_nodes, 2):
            mrca = tree.mrca(taxa=[leaf_a.taxon, leaf_b.taxon])
            little_m.append(precomputed_info[mrca].edges_from_root)
            big_m.append(precomputed_info[mrca].dist_from_root)
        
        # leaf nodes, sorted order
        for leaf in leaf_nodes:
            little_m.append(1)
            big_m.append(leaf.edge_length)

        return np.array(little_m), np.array(big_m)
--------------------
Prompt: 
def get_commit_command(self, message, author=None):
        """Get the command to commit changes to tracked files in the working tree."""
----------
Output: 

        if author is None:
            author = self.author
        if author is None:
            return 'git commit'
        return 'git commit -m "%s" %s' % (author, get_commit_message(message, author))

    def get_commit_message(self, message, author=None):
        """Get the commit message for a file in the working tree."""
        if author is None:
            author = self.author
        if author is None:
            return 'unknown'
        return '%s <%s>' % (author, get_commit_message(message, author))

    def get
----------
Solution: 

        command = ['git']
        if author:
            command.extend(('-c', 'user.name=%s' % author.name))
            command.extend(('-c', 'user.email=%s' % author.email))
        command.append('commit')
        command.append('--all')
        command.append('--message')
        command.append(message)
        return command
--------------------
Prompt: 
def from_file_url(url):
    """ Convert from file:// url to file path
    """
----------
Output: 

    return urlparse.urlparse(url).path


def to_file_url(file_url):
    """ Convert to file:// url
    """
    return urlparse.urljoin(file_url, to_file_path(file_url))


def to_file_path(file_url):
    """ Convert to file:// file_url
    """
    return to_file_url(file_url)


def get_file_content(file_url):
    """ Get content of file
    """
    return urllib2.urlopen(file_url).read()


def get_file_
----------
Solution: 

    if url.startswith('file://'):
        url = url[len('file://'):].replace('/', os.path.sep)

    return url
--------------------
Prompt: 
def _binary_stdio():
    """Construct binary stdio streams (not text mode).

    This seems to be different for Window/Unix Python2/3, so going by:
        https://stackoverflow.com/questions/2850893/reading-binary-data-from-stdin
    """
----------
Output: 

    # TODO: use a more robust way to read binary data from stdin
    #       (not text mode)
    #       (not binary mode)
    #       (not text mode)
    #       (not binary mode)
    #       (not text mode)
    #       (not text mode)
    #       (not text mode)
    #       (not text mode)
    #       (not text mode)
    #       (not text mode)
    #       (not text mode)
    #       (not text mode)
    #       (not text mode)
    #       (not text mode)
   
----------
Solution: 

    PY3K = sys.version_info >= (3, 0)

    if PY3K:
        # pylint: disable=no-member
        stdin, stdout = sys.stdin.buffer, sys.stdout.buffer
    else:
        # Python 2 on Windows opens sys.stdin in text mode, and
        # binary data that read from it becomes corrupted on \r\n
        if sys.platform == "win32":
            # set sys.stdin to binary mode
            # pylint: disable=no-member,import-error
            import os
            import msvcrt
            msvcrt.setmode(sys.stdin.fileno(), os.O_BINARY)
            msvcrt.setmode(sys.stdout.fileno(), os.O_BINARY)
        stdin, stdout = sys.stdin, sys.stdout

    return stdin, stdout
--------------------
Prompt: 
def populate_values(self, rows, field):
        """
        Populates the filter values of this filter using list of rows.

        :param list[dict[str,T]] rows: The row set.
        :param str field: The field name.
        """
----------
Output: 

        for row in rows:
            self.values.append(row)

    def __iter__(self):
        """
        Iterates over the filters.
        """
        for filter in self.filters:
            yield filter

    def __len__(self):
        """
        Returns the number of rows in this filter.
        """
        return len(self.values)

    def __getitem__(self, index):
        """
        Returns the row at the given index.

        :param int index: The row index.
        """
        return self.values[index]

    def __str__(self):
        """
        Returns the filter as a
----------
Solution: 

        self._values.clear()
        for row in rows:
            condition = SimpleConditionFactory.create_condition(self._field, row[field])
            if condition.scheme == 'plain':
                self._values.append(condition.expression)
            else:
                self._conditions.append(condition)
--------------------
Prompt: 
def _saferound(value, decimal_places):
    """
    Rounds a float value off to the desired precision
    """
----------
Output: 

    return round(value, decimal_places)


def _get_date_range(start_date, end_date):
    """
    Returns a two-tuple of (start_date, end_date)
    """
    start_date = start_date.replace(tzinfo=pytz.UTC)
    end_date = end_date.replace(tzinfo=pytz.UTC)
    return start_date, end_date


def _get_date_range_by_name(start_date, end_date, name):
    """
    Returns a two-tuple of (start_date
----------
Solution: 

    try:
        f = float(value)
    except ValueError:
        return ''
    format = '%%.%df' % decimal_places
    return format % f
--------------------
Prompt: 
def _process_change(self, payload, user, repo, repo_url, project, event,
                        properties):
        """
        Consumes the JSON as a python object and actually starts the build.

        :arguments:
            payload
                Python Object that represents the JSON sent by GitHub Service
                Hook.
        """
----------
Output: 

        self.update_json(payload, 'change')
        self.start_change_thread()

    def _process_comment(self, payload, comment):
        """
        Consumes the JSON as a python object and actually starts the build.

        :arguments:
            payload
                Python Object that represents the JSON sent by GitHub Service
                Hook.
        """
        self.update_json(payload, 'comment')
        self.start_change_thread()

    def _get_comments(self, repo, project, number):
        """
        Retrieves the number of comments for a given project.

        :arguments:
           
----------
Solution: 

        changes = []
        refname = payload['ref']

        # We only care about regular heads or tags
        match = re.match(r"^refs/(heads|tags)/(.+)$", refname)
        if not match:
            log.msg("Ignoring refname `{}': Not a branch".format(refname))
            return changes
        category = None  # None is the legacy category for when hook only supported push
        if match.group(1) == "tags":
            category = "tag"

        branch = match.group(2)
        if payload.get('deleted'):
            log.msg("Branch `{}' deleted, ignoring".format(branch))
            return changes

        # check skip pattern in commit message. e.g.: [ci skip] and [skip ci]
        head_msg = payload['head_commit'].get('message', '')
        if self._has_skip(head_msg):
            return changes
        commits = payload['commits']
        if payload.get('created'):
            commits = [payload['head_commit']]
        for commit in commits:
            files = []
            for kind in ('added', 'modified', 'removed'):
                files.extend(commit.get(kind, []))

            when_timestamp = dateparse(commit['timestamp'])

            log.msg("New revision: {}".format(commit['id'][:8]))

            change = {
                'author': '{} <{}>'.format(commit['author']['name'],
                                           commit['author']['email']),
                'files': files,
                'comments': commit['message'],
                'revision': commit['id'],
                'when_timestamp': when_timestamp,
                'branch': branch,
                'revlink': commit['url'],
                'repository': repo_url,
                'project': project,
                'properties': {
                    'github_distinct': commit.get('distinct', True),
                    'event': event,
                },
                'category': category
            }
            # Update with any white-listed github event properties
            change['properties'].update(properties)

            if callable(self._codebase):
                change['codebase'] = self._codebase(payload)
            elif self._codebase is not None:
                change['codebase'] = self._codebase

            changes.append(change)

        return changes
--------------------
Prompt: 
def redirect(self, url: str, permanent: bool = False, status: int = None) -> None:
        """Sends a redirect to the given (optionally relative) URL.

        If the ``status`` argument is specified, that value is used as the
        HTTP status code; otherwise either 301 (permanent) or 302
        (temporary) is chosen based on the ``permanent`` argument.
        The default is 302 (temporary).
        """
----------
Output: 

        if self.closed:
            raise ValueError("Request closed on a closed session.")
        if status is None:
            status = 303 if permanent else 302
        self.redirect_cache[url] = (status, permanent)

    def http_error_302(self, url: str, permanent: bool = False, status: int = None) -> None:
        """Sends a HTTP error 302 to the given (optionally relative) URL.

        If the ``status`` argument is specified, that value is used as the
        HTTP status code; otherwise either 301 (permanent) or 302
        (temporary) is chosen based
----------
Solution: 

        if self._headers_written:
            raise Exception("Cannot redirect after headers have been written")
        if status is None:
            status = 301 if permanent else 302
        else:
            assert isinstance(status, int) and 300 <= status <= 399
        self.set_status(status)
        self.set_header("Location", utf8(url))
        self.finish()
--------------------
Prompt: 
def insert_or_replace_entity(self, entity):
        """
        Adds an insert or replace entity operation to the batch. See 
        :func:`~azure.storage.table.tableservice.TableService.insert_or_replace_entity` for more 
        information on insert or replace operations.

        The operation will not be executed until the batch is committed.

        :param entity:
            The entity to insert or replace. Could be a dict or an entity object. 
            Must contain a PartitionKey and a RowKey.
        :type entity: a dict or :class:`azure.storage.table.models.Entity`
       """
----------
Output: 

        :keyword int timeout:
            The timeout parameter is expressed in seconds.
        :keyword bool skip_token:
            If true, the first token in a page will be skipped.
        :keyword bool continuation_token:
            If true, the continuation token will be appended to each page 
            before it is returned in the result.
        :keyword bool is_last_page:
            If true, the operation will return only the last 
            page of data. If false, the operation will 
            return only the data that you have in the last page.
        :keyword bool is_empty:
            If true, the operation will return
----------
Solution: 

        request = _insert_or_replace_entity(entity)
        self._add_to_batch(entity['PartitionKey'], entity['RowKey'], request)
--------------------
Prompt: 
def _log_config_on_step(self, trial_state, new_state, trial,
                            trial_to_clone, new_config):
        """Logs transition during exploit/exploit step.

        For each step, logs: [target trial tag, clone trial tag, target trial
        iteration, clone trial iteration, old config, new config].
        """
----------
Output: 

        for tag in trial.tags:
            if tag.name == tag_name:
                trial_state.log_tags.append(tag)
                new_trial_tag = tag
                new_config = trial_config.copy()
                _log_tag_to_clone(new_config, trial_tag=trial_tag,
                                iteration=trial.iteration)
                trial_state.log_iteration_tag = trial_tag
                trial_state.log_target_iteration = iteration_tag = None
                trial_iteration_tag = None
                iteration_tag = None
                target_config =
----------
Solution: 

        trial_name, trial_to_clone_name = (trial_state.orig_tag,
                                           new_state.orig_tag)
        trial_id = "".join(itertools.takewhile(str.isdigit, trial_name))
        trial_to_clone_id = "".join(
            itertools.takewhile(str.isdigit, trial_to_clone_name))
        trial_path = os.path.join(trial.local_dir,
                                  "pbt_policy_" + trial_id + ".txt")
        trial_to_clone_path = os.path.join(
            trial_to_clone.local_dir,
            "pbt_policy_" + trial_to_clone_id + ".txt")
        policy = [
            trial_name, trial_to_clone_name,
            trial.last_result[TRAINING_ITERATION],
            trial_to_clone.last_result[TRAINING_ITERATION],
            trial_to_clone.config, new_config
        ]
        # Log to global file.
        with open(os.path.join(trial.local_dir, "pbt_global.txt"), "a+") as f:
            f.write(json.dumps(policy) + "\n")
        # Overwrite state in target trial from trial_to_clone.
        if os.path.exists(trial_to_clone_path):
            shutil.copyfile(trial_to_clone_path, trial_path)
        # Log new exploit in target trial log.
        with open(trial_path, "a+") as f:
            f.write(json.dumps(policy) + "\n")
--------------------
Prompt: 
def only_trashed(cls):
        """
        Get a new query builder that only includes soft deletes

        :type cls: orator.orm.model.Model

        :rtype: orator.orm.builder.Builder
        """
----------
Output: 

        return Orator.objects.filter(
            Q(trashed=True) |
            Q(trashed=False)
        ).order_by('pk').first()

    @classmethod
    def get_by_id(cls, orator_id):
        """
        Get a query builder by its id

        :type orator_id: int

        :rtype: QueryBuilder
        """
        return cls.objects.get(pk=int(orator_id))

    @classmethod
    def get_by_name(cls, name):
        """
        Get a query builder by its name

        :type name:
----------
Solution: 

        instance = cls()

        column = instance.get_qualified_deleted_at_column()

        return instance.new_query_without_scope(SoftDeletingScope()).where_not_null(
            column
        )
--------------------
Prompt: 
def assignee(self, assignee_id, action='ADD'):
        """
        Adds a assignee to the task

        Args:
            assignee_id: The id of the assignee to be added
            action:

        """
----------
Output: 

        if action == 'ADD':
            self.add_assignee(assignee_id)
        elif action == 'REMOVE':
            self.remove_assignee(assignee_id)
        else:
            raise ValueError('Invalid action')

    def add_assignee(self, assignee_id, assignee_type='Assignee', action='ADD'):
        """
        Adds a new assignee to the task

        Args:
            assignee_id: The id of the assignee to be added
            assignee_type: The type of the assignee
----------
Solution: 

        if not self.can_update():
            self._tcex.handle_error(910, [self.type])

        return self.tc_requests.assignee(
            self.api_type, self.api_sub_type, self.unique_id, assignee_id, action=action
        )
--------------------
Prompt: 
def unique_addresses(data=None, file_path=None):
    """
    The function to search an input string and/or file, extracting and
    counting IPv4/IPv6 addresses/networks. Summarizes ports with sub-counts.
    If both a string and file_path are provided, it will process them both.

    Args:
        data (:obj:`str`): The data to process.
        file_path (:obj:`str`): An optional file path to process.

    Returns:
        dict: The addresses/networks mapped to ports and counts:

        ::

            {
                '1.2.3.4' (dict) - Each address or network found is a
                    dictionary:
                    {
                        'count' (int) - Total number of times seen.
                        'ports' (dict) - Mapping of port numbers as keys and
                            the number of times seen for this ip as values.
                    }
            }

    Raises:
        ValueError: Arguments provided are invalid.
    """
----------
Output: 
.4': {
                    'address': '1.2.3.4',
                    'count': '1.2.3.4'
                }
            }

        Otherwise, it will return None.
    """
    if data is None:
        data = get_unique_input_addresses()

    if file_path is not None:
        return _parse_ports_file(file_path, data)
    else:
        return _parse_ports(data, None)


def get_unique_input_addresses():
    """
    Returns:
        list: The list of IPv4/IPv6 addresses/
----------
Solution: 


    if not data and not file_path:

        raise ValueError('No data or file path provided.')

    ret = {}
    base = {
        'count': 0,
        'ports': {}
    }

    file_data = None
    if file_path:

        log.debug('Opening file for unique address analysis: {0}'.format(
                str(file_path)))

        f = open(str(file_path), 'r')

        # Read the file.
        file_data = f.read()

    pattern = re.compile(
        str(IP_REGEX),
        re.DOTALL
    )

    # Check if there is data.
    log.debug('Analyzing input/file data'.format(
                str(file_path)))
    for input_data in [data, file_data]:

        if input_data:

            # Search for IPs.
            for match in pattern.finditer(input_data):

                is_net = False
                port = None
                try:

                    found = match.group('ip')

                    if '.' in found and ':' in found:

                        split = found.split(':')
                        ip_or_net = split[0]
                        port = split[1]

                    elif '[' in found:

                        split = found.split(']:')
                        ip_or_net = split[0][1:]
                        port = split[1]

                    elif '/' in found:

                        is_net = True
                        ip_or_net = found

                    else:

                        ip_or_net = found

                    if is_net:

                        ip_obj = ip_network(ip_or_net)

                    else:
                        ip_obj = ip_address(ip_or_net)

                    obj_str = ip_obj.__str__()

                    if obj_str not in ret.keys():

                        ret[obj_str] = copy.deepcopy(base)

                    ret[obj_str]['count'] += 1

                    if port:

                        try:

                            ret[obj_str]['ports'][str(port)] += 1

                        except KeyError:

                            ret[obj_str]['ports'][str(port)] = 1

                except (KeyError, ValueError):

                    continue

    return ret
--------------------
Prompt: 
def _get_cached_path(
            self,
            field_name,
            path_or_url,
            download_if_missing=False,
            overwrite=False):
        """
        Get the local path for a possibly remote file, invoking either
        a download or install error message if it's missing.
        """
----------
Output: 

        if not path_or_url:
            raise ValueError(
                "Either a path to a file or a URL must be provided.")

        if not os.path.isfile(path_or_url):
            raise ValueError(
                "The file '%s' does not exist." % path_or_url)

        if os.path.isdir(path_or_url):
            if not overwrite:
                raise ValueError(
                    "The file '%s' exists, overwrite it?" % path_or_url)
            else:
                return path_or_url

        if os.path.isfile(path_
----------
Solution: 

        if len(field_name) == 0:
            raise ValueError("Expected non-empty field name")
        if len(path_or_url) == 0:
            raise ValueError("Expected non-empty path_or_url")
        return self.download_cache.local_path_or_install_error(
            field_name=field_name,
            path_or_url=path_or_url,
            download_if_missing=download_if_missing,
            overwrite=overwrite)
--------------------
Prompt: 
def generate_help_text(self):
        """ generates the help text based on commands typed """
----------
Output: 

        text = ""
        if self.commands:
            text += "commands:\n"
            for command in self.commands:
                text += generate_help_text(command)
        return text

    def __str__(self):
        """ return a string representation of the command """
        text = ""
        if self.name:
            text += "name: %s\n" % self.name
        if self.description:
            text += "description: %s\n" % self.description
        if self.enabled:
            text += "enabled: %s\n" % self.enabled
        if self.
----------
Solution: 

        param_descrip = example = ""
        self.description_docs = u''

        rows, _ = get_window_dim()
        rows = int(rows)

        param_args = self.completer.leftover_args
        last_word = self.completer.unfinished_word
        command = self.completer.current_command
        new_command = ' '.join([command, last_word]).strip()

        if not self.completer.complete_command and new_command in self.completer.command_description:
            command = new_command

        # get command/group help
        if self.completer and command in self.completer.command_description:
            self.description_docs = u'{}'.format(self.completer.command_description[command])

        # get parameter help if full command
        if self.completer and command in self.completer.command_param_info:
            param = param_args[-1] if param_args else ''
            param = last_word if last_word.startswith('-') else param

            if param in self.completer.command_param_info[command] and self.completer.has_description(
                    command + " " + param):
                param_descrip = ''.join([
                    param, ":", '\n', self.completer.param_description.get(command + " " + param, '')])

            if command in self.completer.command_examples:
                string_example = []
                for example in self.completer.command_examples[command]:
                    for part in example:
                        string_example.append(part)
                ''.join(string_example)
                example = self._space_examples(
                    self.completer.command_examples[command], rows, self.example_page)

        return param_descrip, example
--------------------
Prompt: 
def read_header(self):
        """Read the header of a MPQ archive."""
----------
Output: 

        with open(self.filename, 'rb') as f:
            return read_mm_header(f)

    def read_frames(self, nframes):
        """Read the frames of a MPQ archive."""
        with open(self.filename, 'rb') as f:
            return read_mm_frames(f, nframes)

    def read_frames_old(self, nframes):
        """Read the frames of a MPQ archive with a new version."""
        with open(self.filename, 'rb') as f:
            return read_mm_frames(f, nframes)
----------
Solution: 


        def read_mpq_header(offset=None):
            if offset:
                self.file.seek(offset)
            data = self.file.read(32)
            header = MPQFileHeader._make(
                struct.unpack(MPQFileHeader.struct_format, data))
            header = header._asdict()
            if header['format_version'] == 1:
                data = self.file.read(12)
                extended_header = MPQFileHeaderExt._make(
                    struct.unpack(MPQFileHeaderExt.struct_format, data))
                header.update(extended_header._asdict())
            return header

        def read_mpq_user_data_header():
            data = self.file.read(16)
            header = MPQUserDataHeader._make(
                struct.unpack(MPQUserDataHeader.struct_format, data))
            header = header._asdict()
            header['content'] = self.file.read(header['user_data_header_size'])
            return header

        magic = self.file.read(4)
        self.file.seek(0)

        if magic == b'MPQ\x1a':
            header = read_mpq_header()
            header['offset'] = 0
        elif magic == b'MPQ\x1b':
            user_data_header = read_mpq_user_data_header()
            header = read_mpq_header(user_data_header['mpq_header_offset'])
            header['offset'] = user_data_header['mpq_header_offset']
            header['user_data_header'] = user_data_header
        else:
            raise ValueError("Invalid file header.")

        return header
--------------------
Prompt: 
def construct_request(self, request_args=None, **kwargs):
        """
        The method where everything is setup for sending the request.
        The request information is gathered and the where and how of sending the
        request is decided.

        :param request_args: Initial request arguments
        :param kwargs: Extra keyword arguments
        :return: A dictionary with the keys 'url' and possibly 'body', 'kwargs',
            'request' and 'ht_args'.
        """
----------
Output: 

        if request_args is None:
            request_args = {}
        if request_args.get('url', None):
            request_args['url'] = self.url_for(request_args)
        if request_args.get('body', None):
            request_args['body'] = self.body_for(request_args)
        if request_args.get('kwargs', None):
            request_args['kwargs'] = self.kwargs_for(request_args)
        if request_args.get('ht_args', None):
            request_args['ht_args'] =
----------
Solution: 

        if request_args is None:
            request_args = {}

        # remove arguments that should not be included in the request
        # _args = dict(
        #    [(k, v) for k, v in kwargs.items() if v and k not in SPECIAL_ARGS])

        return self.construct(request_args, **kwargs)
--------------------
Prompt: 
def get_details(self, obj):
        """ return detail url """
----------
Output: 

        return obj.get_absolute_url()

    def get_context_data(self, **kwargs):
        """
        Return context data for rendering the view
        """
        context = super(PageAdmin, self).get_context_data(**kwargs)
        context['title'] = self.title
        context['page_type'] = self.page_type
        context['page_id'] = self.id
        context['page_name'] = self.name
        context['page_url'] = self.get_details(self.get_object())
        return context

    def get_urls(self):
----------
Solution: 

        return reverse('api_user_social_links_detail',
                       args=[obj.user.username, obj.pk],
                       request=self.context.get('request'),
                       format=self.context.get('format'))
--------------------
Prompt: 
def __get_package_manager(self):
        """
        Installs and verifies package manager
        """
----------
Output: 

        if not self.is_installed():
            raise RuntimeError("Package manager is not installed")
        if not self.is_installed():
            raise RuntimeError("Package manager is not installed")
        if not self.is_installed():
            raise RuntimeError("Package manager is not installed")
        if not self.is_installed():
            raise RuntimeError("Package manager is not installed")

    def is_installed(self):
        """
        Checks if the package manager is installed
        """
        if not self.is_installed_path():
            return False
        return True

    def is_installed_path(self):
        """
        Checks if
----------
Solution: 

        package_manager = ""
        args = ""
        sudo_required = True
        if system.is_osx():
            package_manager = "brew"
            sudo_required = False
            args = " install"
        elif system.is_debian():
            package_manager = "apt-get"
            args = " -y install"
        elif system.is_fedora():
            package_manager = "yum"
            args = " install"
        elif system.is_arch():
            package_manager = "pacman"
            args = " --noconfirm -S"
        if lib.which(package_manager) is None:
            self.logger.warn("Package manager %s not installed! Packages will not be installed."
                             % package_manager)
            self.package_manager = None
        self.package_manager = package_manager
        self.sudo_required = sudo_required
        self.args = args
--------------------
Prompt: 
def use(wcspkg, raise_err=True):
    """Choose WCS package."""
----------
Output: 

    if not raise_err:
        return WCSPackage(wcspkg, 'wcs', '1.0.0', wcspkg.wcs.naxispath)
    else:
        return WCSPackage(wcspkg, 'wcs', '1.0.0', wcspkg.wcs.naxispath, raise_err=False)


def use_wcs_time(wcspkg, raise_err=True):
    """Choose WCS time."""
    if not raise_err:
        return WCSTimePackage(wcspkg, 'wcs', '1.0.
----------
Solution: 

    global coord_types, wcs_configured, WCS

    if wcspkg not in common.custom_wcs:
        # Try to dynamically load WCS
        modname = 'wcs_%s' % (wcspkg)
        path = os.path.join(wcs_home, '%s.py' % (modname))
        try:
            my_import(modname, path)
        except ImportError:
            return False

    if wcspkg in common.custom_wcs:
        bnch = common.custom_wcs[wcspkg]
        WCS = bnch.wrapper_class
        coord_types = bnch.coord_types
        wcs_configured = True
        return True

    return False
--------------------
Prompt: 
def addRandomEffect(self,K=None,covar_type='freeform',is_noise=False,normalize=True,Ks=None,offset=1e-4,rank=1,covar_K0=None):
        """
        Add random effect Term
        depending on self.P=1 or >1 add single trait or multi trait random effect term
        """
----------
Output: 

        if self.P==1:
            if covar_type=='freeform':
                if is_noise:
                    self.P = 1
                else:
                    self.P = random.randint(1,self.P)
            else:
                if is_noise:
                    self.P = 1
                else:
                    self.P = random.randint(1,self.P)
            if Ks==None:
                Ks = self.Ks
            if offset==None:
                offset = self.offset
            if rank==None:
                rank = self.rank
        else:
            if covar_
----------
Solution: 

        if self.P==1:	self.addSingleTraitTerm(K=K,is_noise=is_noise,normalize=normalize,Ks=Ks)
        else:			self.addMultiTraitTerm(K=K,covar_type=covar_type,is_noise=is_noise,normalize=normalize,Ks=Ks,offset=offset,rank=rank,covar_K0=covar_K0)
--------------------
Prompt: 
def _get_pool(name=None, session=None):
    """
    Get XEN resource pool object reference
    """
----------
Output: 

    if name is None:
        name = "default"
    if session is None:
        session = get_session()
    return session.xenapi.pool.get_all()[name]


def _get_record(record_type, ref):
    """
    Get record by ref
    """
    if record_type == "VM":
        return _VMRecord(ref)
    elif record_type == "VDI":
        return _VDIRecord(ref)
    elif record_type == "VBD":
        return _VBDRecord(ref)
    elif record_type == "VBD_snapshot":
       
----------
Solution: 

    if session is None:
        session = _get_session()
    pools = session.xenapi.pool.get_all()
    for pool in pools:
        pool_record = session.xenapi.pool.get_record(pool)
        if name in pool_record.get('name_label'):
            return pool
    return None
--------------------
Prompt: 
def _getBasicOrbit(self, orbit=None):
        """Load a particular orbit into .data for loaded day.

        Parameters
        ----------
        orbit : int
            orbit number, 1 indexed, negative indexes allowed, -1 last orbit

        Note
        ----
        A day of data must be loaded before this routine functions properly.
        If the last orbit of the day is requested, it will NOT automatically be
        padded with data from the next day.
        """
----------
Output: 

        if orbit is None:
            orbit = -1
        if orbit < 0:
            orbit = len(self.data) + orbit
        if orbit > len(self.data):
            raise ValueError("Invalid orbit number")
        return self.data[orbit:orbit+1]

    def _getDate(self, date):
        """Get the date from the date string.

        Parameters
        ----------
        date : str
            date string

        Returns
        -------
        datetime.date
            date object
        """
        if date is None:
            return None
        return datetime.datetime.strptime(
----------
Solution: 

        # ensure data exists
        if not self.sat.empty:
            # ensure proper orbit metadata present
            self._calcOrbits()

            # ensure user is requesting a particular orbit
            if orbit is not None:
                # pull out requested orbit
                if orbit == -1:
                    # load orbit data into data
                    self.sat.data = self._fullDayData[self._orbit_breaks[self.num + orbit]:]
                    self._current = self.num + orbit + 1
                elif ((orbit < 0) & (orbit >= -self.num)):
                    # load orbit data into data
                    self.sat.data = self._fullDayData[
                                    self._orbit_breaks[self.num + orbit]:self._orbit_breaks[self.num + orbit + 1]]
                    self._current = self.num + orbit + 1
                elif (orbit < self.num) & (orbit != 0):
                    # load orbit data into data
                    self.sat.data = self._fullDayData[self._orbit_breaks[orbit - 1]:self._orbit_breaks[orbit]]
                    self._current = orbit
                elif orbit == self.num:
                    self.sat.data = self._fullDayData[self._orbit_breaks[orbit - 1]:]
                    # recent addition, wondering why it wasn't there before,
                    # could just be a bug that is now fixed.
                    self._current = orbit
                elif orbit == 0:
                    raise ValueError('Orbits internally indexed by 1, 0 not ' +
                                     'allowed')
                else:
                    # gone too far
                    self.sat.data = []
                    raise ValueError('Requested an orbit past total orbits ' +
                                     'for day')
            else:
                raise ValueError('Must set an orbit')
--------------------
Prompt: 
def multi_raw(query, params, models, model_to_fields):
    """Scoop multiple model instances out of the DB at once, given a query that
    returns all fields of each.

    Return an iterable of sequences of model instances parallel to the
    ``models`` sequence of classes. For example::

        [(<User such-and-such>, <Watch such-and-such>), ...]

    """
----------
Output: 

    if isinstance(models, basestring):
        models = [models]
    if not isinstance(models, (list, tuple)):
        models = [models]

    for model in models:
        if isinstance(model, basestring):
            yield model
        elif isinstance(model, (tuple, list)):
            for item in multi_raw(item, params, models, model_to_fields):
                yield item
        else:
            raise ValueError("Invalid model: %s" % model)


def model_to_dict(model, fields):
    """Convert a model to a dictionary.

    Return an iterable of ``{
----------
Solution: 

    cursor = connections[router.db_for_read(models[0])].cursor()
    cursor.execute(query, params)
    rows = cursor.fetchall()

    for row in rows:
        row_iter = iter(row)
        yield [model_class(**dict((a, next(row_iter))
                           for a in model_to_fields[model_class]))
               for model_class in models]
--------------------
Prompt: 
def shift_christmas_boxing_days(self, year):
        """ When Christmas and/or Boxing Day falls on a weekend, it is rolled
            forward to the next weekday.
        """
----------
Output: 

        if year < 1900:
            return self.get_weekend_date(year + 1)
        else:
            return self.get_weekend_date(year + 2)

    def get_weekend_date(self, year):
        """ Returns the date of the next weekday.
        """
        if year < 1900:
            return datetime.date(year, 1, 1)
        else:
            return datetime.date(year, 2, 1)

    def get_next_weekday_date(self, year):
        """ Returns the date of the next weekday.
        """
        if year
----------
Solution: 

        christmas = date(year, 12, 25)
        boxing_day = date(year, 12, 26)
        boxing_day_label = "{} Shift".format(self.boxing_day_label)
        results = []
        if christmas.weekday() in self.get_weekend_days():
            shift = self.find_following_working_day(christmas)
            results.append((shift, "Christmas Shift"))
            results.append((shift + timedelta(days=1), boxing_day_label))
        elif boxing_day.weekday() in self.get_weekend_days():
            shift = self.find_following_working_day(boxing_day)
            results.append((shift, boxing_day_label))
        return results
--------------------
Prompt: 
def _readBlock(self):
        """Read a block of data from the remote reader."""
----------
Output: 

        if self._block:
            return self._block

        # Read the data from the remote reader
        self._block = self.remoteReader.read(self.remoteReader.CHUNK_SIZE)
        self._block = self._readBlock()

        # If we have a partial message, we need to check the message size
        # and if it's not we'll raise an exception.
        if self._block:
            self._block = self._readBlock()
            if self._block:
                self._block = _bytes2NativeString(self._block)
                if self._block:
                    self._block =
----------
Solution: 


        if self.interrupted or self.fp is None:
            if self.debug:
                log.msg('WorkerFileDownloadCommand._readBlock(): end')
            return True

        length = self.blocksize
        if self.bytes_remaining is not None and length > self.bytes_remaining:
            length = self.bytes_remaining

        if length <= 0:
            if self.stderr is None:
                self.stderr = "Maximum filesize reached, truncating file '{0}'".format(
                    self.path)
                self.rc = 1
            return True
        else:
            d = self.reader.callRemote('read', length)
            d.addCallback(self._writeData)
            return d
--------------------
Prompt: 
def is_nash(self, action_profile, tol=None):
        """
        Return True if `action_profile` is a Nash equilibrium.

        Parameters
        ----------
        action_profile : array_like(int or array_like(float))
            An array of N objects, where each object must be an integer
            (pure action) or an array of floats (mixed action).

        tol : scalar(float)
            Tolerance level used in determining best responses. If None,
            default to each player's `tol` attribute value.

        Returns
        -------
        bool
            True if `action_profile` is a Nash equilibrium; False
            otherwise.

        """
----------
Output: 
action_profile` is a Nash equilibrium.

        """
        if tol is None:
            if self.player_equilibrium is not None:
                tol = self.player_equilibrium.get_response_tolerance(self.player_equilibrium.get_player_id(self.player_id))
            else:
                tol = self.default_tolerance
        else:
            if self.player_equilibrium is not None:
                tol = self.player_equilibrium.get_response_tolerance(self.
----------
Solution: 

        if self.N == 2:
            for i, player in enumerate(self.players):
                own_action, opponent_action = \
                    action_profile[i], action_profile[1-i]
                if not player.is_best_response(own_action, opponent_action,
                                               tol):
                    return False

        elif self.N >= 3:
            for i, player in enumerate(self.players):
                own_action = action_profile[i]
                opponents_actions = \
                    tuple(action_profile[i+1:]) + tuple(action_profile[:i])

                if not player.is_best_response(own_action, opponents_actions,
                                               tol):
                    return False

        else:  # Trivial case with self.N == 1
            if not self.players[0].is_best_response(action_profile[0], None,
                                                    tol):
                return False

        return True
--------------------
Prompt: 
def parse_plugin_metadata(content):
  """Parse summary metadata to a Python object.

  Arguments:
    content: The `content` field of a `SummaryMetadata` proto
      corresponding to the pr_curves plugin.

  Returns:
    A `PrCurvesPlugin` protobuf object.
  """
----------
Output: 

  return ProtobufPlugin(content)


def parse_plugin_metadata_from_proto(content):
  """Parse summary metadata from a Python object.

  Arguments:
    content: The `content` field of a `SummaryMetadata` proto
      corresponding to the pr_curves plugin.

  Returns:
    A `SummaryMetadata` protobuf object.
  """
  return ProtobufPlugin.from_summary_metadata(content)


def parse_plugin_metadata_from_summary(summary_metadata):
  """Parse Python object from a `SummaryMetadata` proto.

  Arguments:
    summary_metadata:
----------
Solution: 

  if not isinstance(content, bytes):
    raise TypeError('Content type must be bytes')
  result = plugin_data_pb2.PrCurvePluginData.FromString(content)
  if result.version == 0:
    return result
  else:
    logger.warn(
        'Unknown metadata version: %s. The latest version known to '
        'this build of TensorBoard is %s; perhaps a newer build is '
        'available?', result.version, PROTO_VERSION)
    return result
--------------------
Prompt: 
def refresh_items(self):
        """ Refresh the items of the pattern.
        This method destroys the old items and creates and initializes
        the new items.

        It is overridden to NOT insert the children to the parent. The Fragment
        adapter handles this.

        """
----------
Output: 

        pass

    def _get_pattern_text(self):
        """ Return the text of the pattern.
        This method returns the text of the pattern.

        """
        return _("Refreshing %(count)d of %(total)d") % {
            'count': self.pattern_text.format(count=self.count, total=self.total),
            'total': self.pattern_text.format(total=self.pattern_text.format(count=self.total))}

    def _set_pattern_text(self, text):
        """ Set the text of the pattern.

        """
----------
Solution: 

        items = []
        if self.condition:
            for nodes, key, f_locals in self.pattern_nodes:
                with new_scope(key, f_locals):
                    for node in nodes:
                        child = node(None)
                        if isinstance(child, list):
                            items.extend(child)
                        else:
                            items.append(child)

        for old in self.items:
            if not old.is_destroyed:
                old.destroy()

        #: Insert items into THIS node, NOT the PARENT
        #if len(items) > 0:
        #    self.parent.insert_children(self, items)
        self.items = items
--------------------
Prompt: 
def request(self, method, url, data=None, files=None, query=None, headers=None, timeout=60):
        """
        Mechanism for issuing an API call
        """
----------
Output: 

        if data is None:
            data = {}
        if files is None:
            files = {}
        if query is None:
            query = {}
        if timeout is None:
            timeout = self.timeout
        if self.token:
            headers['Authorization'] = self.token
        if self.timeout:
            headers['Timeout'] = self.timeout
        if self.timeout_read:
            headers['Timeout'] = self.timeout_read
        if self.timeout_write:
            headers['Timeout'] = self.timeout_write
        if self.timeout_read:
            headers['Timeout'] = self.timeout_
----------
Solution: 

        headers = headers or {}

        if self.token:
            my_token = self.token
        else:
            from figure import token
            my_token = token

        if my_token:
            self.__set_authorization(headers, my_token)

        METHODS = {
            'get': self.__get,
            'post': self.__post,
            'put': self.__put,
            'head': self.__head,
            'patch': self.__patch
        }
        request_method = METHODS[method.lower()]

        abs_url = urlparse.urljoin(self.api_base, url)

        encoded_query = urllib.urlencode(query or {})

        abs_url = _build_api_url(abs_url, encoded_query)

        try:
            response = request_method(abs_url, data=data, files=files, headers=headers, timeout=timeout)
            response.encoding = 'utf-8'
        except RequestException:
            raise error.APIConnectionError()

        return self._interpret_response(response.text, response.status_code)
--------------------
Prompt: 
def _createbound(obj):
    """Create a new BoundNode representing a given object."""
----------
Output: 

    if obj.parent is None:
        return BoundNode(parent=None, parent_context=None)
    else:
        return BoundNode(parent=obj.parent, parent_context=obj)


class _ScopedExpr(Expr):
    """
    ScopedExpr is a compiled expression that can be used as a scope.
    """
    subexprs = ['target', 'value']

    def as_const(self, eval_ctx=None):
        # This is needed because the parser could be called with "as_call"
        # as well, which would otherwise conflict with Python's scope
       
----------
Solution: 

    # Start by allowing objects to define custom unbound reference hooks
    try:
        kls = obj._unboundreference_()
    except AttributeError:
        kls = type(obj)
    
    unbound = _createunbound(kls)
    def valueget():
        return obj
    for t in (BoundBitfieldNode, BoundStructureNode, BoundArrayNode):
        if isinstance(unbound, t._unboundtype):
            kls = t
            break
    else:
        kls = BoundSimpleNode
    
    child = kls(unbound, valueget)
    return child
--------------------
Prompt: 
def do_exit(self, line):
        """exit Exit from the CLI."""
----------
Output: 

        self.exit_code = self.EXIT_OK
        self.exit_message = ""
        self.exit_code_str = ""
        self.exit_message_str = ""
        self.exit_message_str = ""
        self.exit_message = line
        return self

    def do_quit(self, line):
        """quit."""
        self.exit_code = self.EXIT_OK
        self.exit_message = ""
        return self

    def do_EOF(self, line):
        """EOF."""
        self.exit_code = self.EXIT_OK
        self.exit
----------
Solution: 

        n_remaining_operations = len(self._command_processor.get_operation_queue())
        if n_remaining_operations:
            d1_cli.impl.util.print_warn(
                
--------------------
Prompt: 
def addprojecthook(self, project_id, url, push=False, issues=False, merge_requests=False, tag_push=False):
        """
        add a hook to a project

        :param project_id: project id
        :param url: url of the hook
        :return: True if success
        """
----------
Output: 

        if not self.projects:
            self.projects = self.get_projects()
        if not self.projects:
            return False
        if not self.push:
            self.push_status = 'pending'
            if issues:
                self.push_status = 'open'
        else:
            self.push_status = 'open'
        if push:
            self.push_status = 'push'
        if issues:
            self.push_status = 'issues'
        if merge_requests:
            self.merge_status ='merge_requests'
        if tag_push:
            self.tag
----------
Solution: 

        data = {
            'id': project_id,
            'url': url,
            'push_events': int(bool(push)),
            'issues_events': int(bool(issues)),
            'merge_requests_events': int(bool(merge_requests)),
            'tag_push_events': int(bool(tag_push)),
        }

        request = requests.post(
            '{0}/{1}/hooks'.format(self.projects_url, project_id),
            headers=self.headers, data=data, verify=self.verify_ssl, auth=self.auth, timeout=self.timeout)

        if request.status_code == 201:
            return request.json()
        else:
            return False
--------------------
Prompt: 
def clipTo(self, bsp):
        """ 
        Remove all polygons in this BSP tree that are inside the other BSP tree
        `bsp`.
        """
----------
Output: 

        if self.isClosed():
            raise ValueError("Cannot close a closed BSP")
        self.removeAll()

    def removeAll(self):
        """ 
        Remove all nodes and subpolygons from this BSP.
        """
        self.nodes = []
        self.subpolygons = []

    def getNode(self, id):
        """ 
        Return a node from this BSP by ID.
        """
        return self.nodes[id]

    def getSubpolygons(self, id):
        """ 
        Return a subpolygon from this BSP.
        """
        return self.subpolygons[id
----------
Solution: 

        self.polygons = bsp.clipPolygons(self.polygons)
        if self.front: 
            self.front.clipTo(bsp)
        if self.back: 
            self.back.clipTo(bsp)
--------------------
Prompt: 
def set_time_items(glob):
    """
    This function prepares the benchmark items for inclusion in main script's
     global scope.
    
    :param glob: main script's global scope dictionary reference
    """
----------
Output: 

    global time_items
    time_items = {}
    for key in glob.keys():
        if key.startswith("time_"):
            time_items[key[7:]] = glob[key]
    return time_items

def get_time_items(glob):
    """
    This function prepares the benchmark items for inclusion in main script's
     global scope.
    
    :param glob: main script's global scope dictionary reference
    """
    global time_items
    if glob.has_key("time_"):
        return glob["time_"]
    else:
        return time_items

def get_
----------
Solution: 

    a = glob['args']
    l = glob['logger']

    class __TimeManager(object):
        def __init__(self):
            c = a._collisions
            self._stats = getattr(a, c.get("stats") or "stats", False)
            self._timings = getattr(a, c.get("timings") or "timings", False)
            self.enabled = self._stats or self._timings
            self.last = self.start = time.time()
            self.times = []
        
        def stats(self):
            end = time.time()
            b = ""
            for d, s, e in self.times:
                b += "\n{}\n> {} seconds".format(d, e - s)
            l.time("Total time: {} seconds{}".format(end - self.start, b))
    
    glob['time_manager'] = manager = __TimeManager()

    def _take_time(start=None, descr=None):
        t = manager.last = time.time()
        if start is not None and descr is not None:
            manager.times.append((descr, float(start), float(t)))
        return t - (start or 0)

    class Timer(object):
        class TimeoutError(Exception):
            pass  # TimeoutError is not handled in Python 2
    
        def __init__(self, description=None, message=TO_MSG, timeout=None,
                     fail_on_timeout=False):
            self.fail = fail_on_timeout
            self.id = len(manager.times)
            self.descr = "#" + str(self.id) + \
                         (": " + (description or "")).rstrip(": ")
            self.message = message
            self.start = _take_time()
            self.timeout = timeout

        def __enter__(self):
            if manager.enabled:
                if self.timeout is not None:
                    signal.signal(signal.SIGALRM, self._handler)
                    signal.alarm(self.timeout)
                if manager._timings and self.descr:
                    l.time(self.descr)
                return self
        
        def __exit__(self, exc_type, exc_value, exc_traceback):
            if manager.enabled:
                d = _take_time(self.start, self.descr)
                if manager._timings:
                    l.time("> Time elapsed: {} seconds".format(d))
                if self.timeout is not None:
                    if self.fail and exc_type is Timer.TimeoutError:
                        return True

        def _handler(self, signum, frame):
            raise Timer.TimeoutError(self.message)
    
    glob['Timer'] = Timer
    
    def get_time(message=None, start=manager.start):
        if manager._timings:
            l.time("> {}: {} seconds".format(message or "Time elapsed since "
                                          "execution start", _take_time(start)))
    
    glob['get_time'] = get_time
    
    def get_time_since_last(message=None):
        get_time(message or "Time elapsed since last measure", manager.last)
    
    glob['get_time_since_last'] = get_time_since_last
--------------------
Prompt: 
def setup_environment():
    """Ensures that the environmental variable RAFCON_LIB_PATH is existent
    """
----------
Output: 

    if not os.path.exists(RAFCON_LIB_PATH):
        os.mkdir(RAFCON_LIB_PATH)
    if not os.path.exists(RAFCON_LIB_PATH + "/lib"):
        os.mkdir(RAFCON_LIB_PATH + "/lib")
    if not os.path.exists(RAFCON_LIB_PATH + "/lib/libboost_program_options.jar"):
        print("ERROR: Could not find libboost_program_options.jar")
        sys.exit(1)
    if not os.path.exists(
----------
Solution: 

    try:
        from gi.repository import GLib
        user_data_folder = GLib.get_user_data_dir()
    except ImportError:
        user_data_folder = join(os.path.expanduser("~"), ".local", "share")
    rafcon_root_path = dirname(realpath(rafcon.__file__))
    user_library_folder = join(user_data_folder, "rafcon", "libraries")

    # The RAFCON_LIB_PATH points to a path with common RAFCON libraries
    # If the env variable is not set, we have to determine it. In the future, this should always be
    # ~/.local/share/rafcon/libraries, but for backward compatibility, also a relative RAFCON path is supported
    if not os.environ.get('RAFCON_LIB_PATH', None):
        if exists(user_library_folder):
            os.environ['RAFCON_LIB_PATH'] = user_library_folder
        else:
            os.environ['RAFCON_LIB_PATH'] = join(dirname(dirname(rafcon_root_path)), 'share', 'libraries')

    # Install dummy _ builtin function in case i18.setup_l10n() is not called
    if sys.version_info >= (3,):
        import builtins as builtins23
    else:
        import __builtin__ as builtins23
    if "_" not in builtins23.__dict__:
        builtins23.__dict__["_"] = lambda s: s
--------------------
Prompt: 
def sharp(self) -> 'Channel':
        r"""Return the 'sharp' transpose of the superoperator.

        The transpose :math:`S^\#` switches the two covariant (bra)
        indices of the superoperator. (Which in our representation
        are the 2nd and 3rd super-indices)

        If :math:`S^\#` is Hermitian, then :math:`S` is a Hermitian-map
        (i.e. transforms Hermitian operators to hJrmitian operators)

        Flattening the :math:`S^\#` superoperator to a matrix gives
        the Choi matrix representation. (See channel.choi())
        """
----------
Output: 
 :math:`S^\#` transpose of the matrix.

        Parameters
        ----------
        x : `Tensor`
            The superoperator to be sharpened.

        Returns
        -------
        x_sharpened : `Tensor`
            The'sharpened' transpose of the superoperator.

        Examples
        --------
        >>> x = tf.constant([[1, 2, 3], [4, 5, 6]])
        >>> sharp = tf.sharp()
        >>> sharp.eval(x)
        [1, 4, 5, 6]

        """
        return self._sharp_transpose(self
----------
Solution: 


        N = self.qubit_nb

        tensor = self.tensor
        tensor = bk.reshape(tensor, [2**N] * 4)
        tensor = bk.transpose(tensor, (0, 2, 1, 3))
        tensor = bk.reshape(tensor, [2] * 4 * N)
        return Channel(tensor, self.qubits)
--------------------
Prompt: 
def find_widening_points(function_addr, function_endpoints, graph):  # pylint: disable=unused-argument
        """
        Given a local transition graph of a function, find all widening points inside.

        Correctly choosing widening points is very important in order to not lose too much information during static
        analysis. We mainly consider merge points that has at least one loop back edges coming in as widening points.

        :param int function_addr: Address of the function.
        :param list function_endpoints: Endpoints of the function, typically coming from Function.endpoints.
        :param networkx.DiGraph graph: A local transition graph of a function, normally Function.graph.
        :return: A list of addresses of widening points.
        :rtype: list
        """
----------
Output: 

        :param int graph: The transition graph.
        :return: A list of the number of widening points inside the transition graph.
        :rtype: int
        """
        return [graph.get_edge_data(function_addr, i) for i in range(len(function_endpoints))]

    @staticmethod
    def find_loops(graph, addr):  # pylint: disable=unused-argument
        """
        Given a transition graph of a function, find all loops in the transition graph.

        :param int graph: The transition graph.
        :param str addr: Address of the function.
        :
----------
Solution: 


        sccs = networkx.strongly_connected_components(graph)

        widening_addrs = set()

        for scc in sccs:
            if len(scc) == 1:
                node = next(iter(scc))
                if graph.has_edge(node, node):
                    # self loop
                    widening_addrs.add(node.addr)
            else:
                for n in scc:
                    predecessors = graph.predecessors(n)
                    if any([ p not in scc for p in predecessors]):
                        widening_addrs.add(n.addr)
                        break

        return list(widening_addrs)
--------------------
Prompt: 
def build(
    c,
    clean=False,
    browse=False,
    nitpick=False,
    opts=None,
    source=None,
    target=None,
):
    """
    Build the project's Sphinx docs.
    """
----------
Output: 

    if opts is None:
        opts = {}
    if source is None:
        source = []
    if target is None:
        target = []
    if not opts.get("no_toc", False):
        if not nitpick:
            c.generate_toc(opts=opts)
        else:
            c.generate_toctree(opts=opts)
    if not opts.get("no_toc_depth", False):
        if not browse:
            c.generate_toctree(opts=opts)
        else:
            c.generate_toc(opts=opts, depth=1)
    if
----------
Solution: 

    if clean:
        _clean(c)
    if opts is None:
        opts = ""
    if nitpick:
        opts += " -n -W -T"
    cmd = "sphinx-build{0} {1} {2}".format(
        (" " + opts) if opts else "",
        source or c.sphinx.source,
        target or c.sphinx.target,
    )
    c.run(cmd, pty=True)
    if browse:
        _browse(c)
--------------------
Prompt: 
def note_update(self, note_id, coor_x=None, coor_y=None, width=None,
                    height=None, body=None):
        """Function to update a note (Requires login) (UNTESTED).

        Parameters:
            note_id (int): Where note_id is the note id.
            coor_x (int): The x coordinates of the note in pixels,
                          with respect to the top-left corner of the image.
            coor_y (int): The y coordinates of the note in pixels,
                          with respect to the top-left corner of the image.
            width (int): The width of the note in pixels.
            height (int): The height of the note in pixels.
            body (str): The body of the note.
        """
----------
Output: 
 corner of the image.
            width (int): The width of the image.
            height (int): The height of the image.
            body (str): The body of the note.
        """
        self.note_id = note_id
        self.width = width
        self.height = height
        self.coor_x = coor_x
        self.coor_y = coor_y
        self.body = body
        self.coor_x = coor_x
        self.coor_y = coor_y

    def get_note_data(self, coor
----------
Solution: 

        params = {
            'note[x]': coor_x,
            'note[y]': coor_y,
            'note[width]': width,
            'note[height]': height,
            'note[body]': body
            }
        return self._get('notes/{0}.jso'.format(note_id), params, method='PUT',
                         auth=True)
--------------------
Prompt: 
def translate_detector(self, vector):
        """Translate the detector by a given vector"""
----------
Output: 

        return self.detector.translate(vector)

    def get_detector_name(self, detector):
        """Get the name of the detector"""
        return self.detector.get_name(detector)

    def get_detector_type(self, detector):
        """Get the type of the detector"""
        return self.detector.get_type(detector)

    def get_detector_parameters(self, detector):
        """Get the parameters of the detector"""
        return self.detector.get_parameters(detector)

    def get_detector_data(self, detector):
        """Get the data of the
----------
Solution: 

        vector = np.array(vector, dtype=float)
        self.pmts.pos_x += vector[0]
        self.pmts.pos_y += vector[1]
        self.pmts.pos_z += vector[2]
        self.reset_caches()
--------------------
Prompt: 
def get(self, uri):
        """Get request
        
        Args:
            uri (str): URI
            
        Returns:
            Json: API response
            
        Raises:
            Exception: Network issue
        """
----------
Output: 

        return self._request(uri, method='GET')

    def post(self, uri, data):
        """Post request
        
        Args:
            uri (str): URI
            
        Returns:
            Json: API response
            
        Raises:
            Exception: Network issue
        """
        return self._request(uri, method='POST', data=data)

    def put(self, uri, data):
        """Put request
        
        Args:
            uri (str): URI
            
        Returns:
            Json: API response
            
        Raises:
            Exception: Network issue
        """
        return self._request(uri, method='PUT', data=data)

    def
----------
Solution: 

        r = None
        
        try:
            r = requests.get(uri,
                             allow_redirects=True,
                             timeout=Settings.requests_timeout,
                             headers={},
                             auth=HTTPDigestAuth(self.user, self.password))
            return self.answer(r.status_code, r.json())
        except:
            raise
        finally:
            if r:
                r.connection.close()
--------------------
Prompt: 
def AVP(avpId, **fields):
    """ Craft an AVP based on its id and optional parameter fields"""
----------
Output: 

    return _AVP(avpId, 'a', **fields)

def AVP_uint32(avpId, **fields):
    """ Craft an AVP based on its id and optional parameter fields"""
    return _AVP(avpId, 'a', **fields)

def AVP_uint32_array(avpId, **fields):
    """ Craft an AVP based on its id and optional parameter fields"""
    return _AVP(avpId, 'a', **fields)

def _AVP(avpId, type, **
----------
Solution: 

    val = None
    classType = AVP_Unknown
    if isinstance(avpId, str):
        try:
            for vnd in AvpDefDict:
                for code in AvpDefDict[vnd]:
                    val = AvpDefDict[vnd][code]
                    if val[0][:len(
                            avpId)] == avpId:  # A prefix of the full name is considered valid  # noqa: E501
                        raise
            found = False
        except BaseException:
            found = True
    else:
        if isinstance(avpId, list):
            code = avpId[0]
            vnd = avpId[1]
        else:  # Assume this is an int
            code = avpId
            vnd = 0
        try:
            val = AvpDefDict[vnd][code]
            found = True
        except BaseException:
            found = False
    if not found:
        warning('The AVP identifier %s has not been found.' % str(avpId))
        if isinstance(avpId, str):  # The string input is not valid
            return None
    # At this point code, vnd are provisionned val may be set (if found is True)  # noqa: E501
    # Set/override AVP code
    fields['avpCode'] = code
    # Set vendor if not already defined and relevant
    if 'avpVnd' not in fields and vnd:
        fields['avpVnd'] = vnd
    # Set flags if not already defined and possible ...
    if 'avpFlags' not in fields:
        if val:
            fields['avpFlags'] = val[2]
        else:
            fields['avpFlags'] = vnd and 128 or 0
    # Finally, set the name and class if possible
    if val:
        classType = val[1]
    _ret = classType(**fields)
    if val:
        _ret.name = 'AVP ' + val[0]
    return _ret
--------------------
Prompt: 
def find_best_candidate(self, elev_source_files=None):
        """
        Heuristically determines which tile should be recalculated based on
        updated edge information. Presently does not check if that tile is
        locked, which could lead to a parallel thread closing while one thread
        continues to process tiles.
        """
----------
Output: 

        if elev_source_files is None:
            elev_source_files = self.get_source_files()
        if len(elev_source_files) == 0:
            return None
        if elev_source_files[0] in elev_source_files:
            return 'locked'
        if elev_source_files[1] in elev_source_files:
            return'recalculated'
        if elev_source_files[0] in self.edge_list:
            return'recalculated'
        return None

    def get_source_files
----------
Solution: 

        self.fill_percent_done()
        i_b = np.argmax(self.percent_done.values())
        if self.percent_done.values()[i_b] <= 0:
            return None

        # check for ties
        I = np.array(self.percent_done.values()) == \
            self.percent_done.values()[i_b]
        if I.sum() == 1:
            pass  # no ties
        else:
            I2 = np.argmax(np.array(self.max_elev.values())[I])
            i_b = I.nonzero()[0][I2]

            # Make sure the apples are still apples
            assert(np.array(self.max_elev.keys())[I][I2]
                   == np.array(self.percent_done.keys())[I][I2])

        if elev_source_files is not None:
            fn = self.percent_done.keys()[i_b]
            lckfn = _get_lockfile_name(fn)
            if os.path.exists(lckfn):  # another process is working on it
                # Find a different Candidate
                i_alt = np.argsort(self.percent_done.values())[::-1]
                for i in i_alt:
                    fn = self.percent_done.keys()[i]
                    lckfn = _get_lockfile_name(fn)
                    if not os.path.exists(lckfn):
                        break
            # Get and return the index
            i_b = elev_source_files.index(fn)

        return i_b
--------------------
Prompt: 
def RgbToGreyscale(r, g, b):
    """Convert the color from RGB to its greyscale equivalent

    Parameters:
      :r:
        The Red component value [0...1]
      :g:
        The Green component value [0...1]
      :b:
        The Blue component value [0...1]

    Returns:
      The color as an (r, g, b) tuple in the range:
      the range:
      r[0...1],
      g[0...1],
      b[0...1]

    >>> '(%g, %g, %g)' % Color.RgbToGreyscale(1, 0.8, 0)
    '(0.6, 0.6, 0.6)'

    """
----------
Output: 
 RgbToGreyscale((0,0,0), (0,0,255,0))
    (255, 0, 0, 255)

    >>> '(%g, %g, %g)' % RgbToGreyscale((0,0,0), (0,255,0,0))
    (0, 0, 0, 0)

    >>> '(%g, %g, %g)' % RgbToGreyscale((0,0,0), (0,0,0,255))
    (0, 0, 0, 0)
    """
    return (0, 0
----------
Solution: 

    v = (r + g + b) / 3.0
    return (v, v, v)
--------------------
Prompt: 
def install_dap(name, version='', update=False, update_allpaths=False, first=True,
                force=False, nodeps=False, reinstall=False, __ui__=''):
    """Install a dap from dapi
    If update is True, it will remove previously installed daps of the same name"""
----------
Output: 

    if not nodeps:
        dap_cmd = 'dap -n -i'
    else:
        dap_cmd = 'dap -n -r'
    if not reinstall:
        dap_cmd +='-y'
    cmd = dap_cmd +'install'
    if version:
        cmd +='-v %s' % version
    if first:
        cmd +='-a'
    if force:
        cmd +='-f'
    else:
        cmd +='-y'
    if update_allpaths:
        cmd +='-a'
    if __
----------
Solution: 

    m, d = _get_metadap_dap(name, version)
    if update:
        available = d['version']
        current = get_installed_version_of(name)
        if not current:
            raise DapiLocalError('Cannot update not yet installed DAP.')
        if dapver.compare(available, current) <= 0:
            return []
    path, remove_dir = download_dap(name, d=d)

    ret = install_dap_from_path(path, update=update, update_allpaths=update_allpaths, first=first,
                                force=force, nodeps=nodeps, reinstall=reinstall, __ui__=__ui__)

    try:
        if remove_dir:
            shutil.rmtree(os.dirname(path))
        else:
            os.remove(path)
    except:
        pass

    return ret
--------------------
Prompt: 
def rk5_stepper(x, h, y , deriv, errors=False):
    """ Compute y(x+h) from y and dy/dx=deriv(x,y).

    Uses a one-step 5th-order Runge-Kutta algorithm.

    Returns x+h, y(x+h) if errors is False; otherwise
    returns x+h, y(x+h), yerr where yerr is an error
    estimate.

    Adapted from Numerical Recipes.
    """
----------
Output: 

    if errors:
        return (x+h, y, yerr)
    else:
        return (x+h, y+h)

def rk5_stepper_scalar(x, h, y, deriv, errors=False):
    """ Compute y(x+h) from y and dy/dx=deriv(x,y,x).

    Uses a scalar algorithm.

    Returns x+h, y(x+h) if errors is False; otherwise
    returns x+h, y(x+h), yerr where yerr is an error
    estimate.

    Adapted
----------
Solution: 

    k1 = h * deriv(x,y)
    k2 = h * deriv(x+0.2*h, y+0.2*k1)
    k3 = h * deriv(x+0.3*h, y+0.075*k1+0.225*k2)
    k4 = h * deriv(x+0.6*h, y+0.3*k1-0.9*k2+1.2*k3)
    k5 = h * deriv(x+h, y-.2037037037037037037037037*k1
             +2.5*k2-2.592592592592592592592593*k3
             +1.296296296296296296296296*k4)
    k6 = h * deriv(x+0.875*h, y+.2949580439814814814814815e-1*k1
             +.341796875*k2+.4159432870370370370370370e-1*k3
             +.4003454137731481481481481*k4+.61767578125e-1*k5)
    yn = y + (.9788359788359788359788361e-1*k1
             +.4025764895330112721417070*k3
             +.2104377104377104377104378*k4
             +.2891022021456804065499718*k6)
    xn = x+h
    if errors:
        yerr = (-.429377480158730158730159e-2*k1
                +.186685860938578329882678e-1*k3
                -.341550268308080808080807e-1*k4
                -.1932198660714285714285714e-1*k5
                +.391022021456804065499718e-1*k6)
        return xn,yn,yerr
    else:
        return xn,yn
--------------------
Prompt: 
def write_pkg_file(self, file):
    """Write the PKG-INFO format data to a file object.
    """
----------
Output: 

    if not file:
      return
    if self.version:
      file.write("%s\n" % self.version)
    if self.release:
      file.write("%s\n" % self.release)
    if self.epoch:
      file.write("%s\n" % self.epoch)
    if self.release_epoch:
      file.write("%s\n" % self.release_epoch)
    if self.release_version:
      file.write("%s\n" % self.release_version)
    if self.build_time:
      file.write("%
----------
Solution: 

    version = self.get_metadata_version()

    if six.PY2:
        def write_field(key, value):
            file.write("%s: %s\n" % (key, self._encode_field(value)))
    else:
        def write_field(key, value):
            file.write("%s: %s\n" % (key, value))

    write_field('Metadata-Version', str(version))
    write_field('Name', self.get_name())
    write_field('Version', self.get_version())
    write_field('Summary', self.get_description())
    write_field('Home-page', self.get_url())

    if version < StrictVersion('1.2'):
        write_field('Author', self.get_contact())
        write_field('Author-email', self.get_contact_email())
    else:
        optional_fields = (
            ('Author', 'author'),
            ('Author-email', 'author_email'),
            ('Maintainer', 'maintainer'),
            ('Maintainer-email', 'maintainer_email'),
        )

        for field, attr in optional_fields:
            attr_val = getattr(self, attr)

            if attr_val is not None:
                write_field(field, attr_val)

    write_field('License', self.get_license())
    if self.download_url:
        write_field('Download-URL', self.download_url)
    for project_url in self.project_urls.items():
        write_field('Project-URL',  '%s, %s' % project_url)

    long_desc = rfc822_escape(self.get_long_description())
    write_field('Description', long_desc)

    keywords = ','.join(self.get_keywords())
    if keywords:
        write_field('Keywords', keywords)

    if version >= StrictVersion('1.2'):
        for platform in self.get_platforms():
            write_field('Platform', platform)
    else:
        self._write_list(file, 'Platform', self.get_platforms())

    self._write_list(file, 'Classifier', self.get_classifiers())

    # PEP 314
    self._write_list(file, 'Requires', self.get_requires())
    self._write_list(file, 'Provides', self.get_provides())
    self._write_list(file, 'Obsoletes', self.get_obsoletes())

    # Setuptools specific for PEP 345
    if hasattr(self, 'python_requires'):
        write_field('Requires-Python', self.python_requires)

    # PEP 566
    if self.long_description_content_type:
        write_field(
            'Description-Content-Type',
            self.long_description_content_type
        )
    if self.provides_extras:
        for extra in self.provides_extras:
            write_field('Provides-Extra', extra)
--------------------
Prompt: 
def get_uservars(self, user=None):
        """Get all variables about a user (or all users).

        :param optional str user: The user ID to retrieve all variables for.
            If not passed, this function will return all data for all users.

        :return dict: All the user variables.

            * If a ``user`` was passed, this is a ``dict`` of key/value pairs
              of that user's variables. If the user doesn't exist in memory,
              this returns ``None``.
            * Otherwise, this returns a ``dict`` of key/value pairs that map
              user IDs to their variables (a ``dict`` of ``dict``).
        """
----------
Output: 
 variables to their values.
        """
        if user is None:
            return None
        return self._get_uservars(user)

    def _get_uservars(self, user=None):
        """Get all variables about a user (or all users).

        :param optional str user: The user ID to retrieve all variables for.
            If not passed, this function will return all data for all users.

        :return dict: All the user variables.

            * If a ``user`` was passed, this is a ``dict`` of key/value pairs
              of that user's variables. If the user doesn't
----------
Solution: 


        if user is None:
            # All the users!
            return self._session.get_all()
        else:
            # Just this one!
            return self._session.get_any(user)
--------------------
Prompt: 
def scan(self, seqs, nreport=100, scan_rc=True, normalize=False):
        """
        scan a set of regions / sequences
        """
----------
Output: 

        for seq in seqs:
            self.add_region(seq)
        if scan_rc:
            self.scan_rc()
        if normalize:
            self.normalize()

    def add_region(self, seq):
        """
        add a region
        """
        self.regions.append(seq)

    def scan_rc(self):
        """
        scan the region
        """
        for i in range(len(self.regions)):
            if self.regions[i] is not None:
                if self.report_rc:
                        print(self.regions[i], self.report_rc)
----------
Solution: 


        if not self.threshold:
            sys.stderr.write(
                "Using default threshold of 0.95. "
                "This is likely not optimal!\n"
                )
            self.set_threshold(threshold=0.95)

        seqs = as_fasta(seqs, genome=self.genome)
           
        it = self._scan_sequences(seqs.seqs, 
                    nreport, scan_rc)
       
        if normalize:
            if len(self.meanstd) == 0:
                self.set_meanstd()
            mean_std = [self.meanstd.get(m_id) for m_id in self.motif_ids]
            means = [x[0] for x in  mean_std]
            stds = [x[1] for x in  mean_std]


        for result in it:
            if normalize:
                zresult = [] 
                for i,mrow in enumerate(result):
                    mrow = [((x[0] - means[i]) / stds[i], x[1], x[2]) for x in mrow]
                    zresult.append(mrow)
                yield zresult
            else:
                yield result
--------------------
Prompt: 
def get_invalidation_bits(self, group_index, channel, fragment):
        """ get invalidation indexes for the channel

        Parameters
        ----------
        group_index : int
            group index
        channel : Channel
            channel object
        fragment : (bytes, int)
            (fragment bytes, fragment offset)

        Returns
        -------
        invalidation_bits : iterable
            iterable of valid channel indexes; if all are valid `None` is
            returned

        """
----------
Output: 

        if not self.valid_group_index(group_index):
            return None

        invalidation_bits = []
        for i in range(group_index.num_channels):
            if not self.channel_valid_index(channel, i):
                invalidation_bits.append(None)
            else:
                break
        else:
            invalidation_bits = tuple(invalidation_bits)

        return invalidation_bits

    def channel_valid_index(self, channel, i):
        """ Check if the channel index `i` is valid

        Parameters
        ----------
        channel : Channel
            channel object
----------
Solution: 

        group = self.groups[group_index]
        dtypes = group.types

        data_bytes, offset, _count = fragment
        try:
            invalidation = self._invalidation_cache[(group_index, offset, _count)]
        except KeyError:
            record = group.record
            if record is None:
                dtypes = group.types
                if dtypes.itemsize:
                    record = fromstring(data_bytes, dtype=dtypes)
                else:
                    record = None

            invalidation = record["invalidation_bytes"].copy()
            self._invalidation_cache[(group_index, offset, _count)] = invalidation

        ch_invalidation_pos = channel.pos_invalidation_bit
        pos_byte, pos_offset = divmod(ch_invalidation_pos, 8)

        mask = 1 << pos_offset

        invalidation_bits = invalidation[:, pos_byte] & mask
        invalidation_bits = invalidation_bits.astype(bool)

        return invalidation_bits
--------------------
Prompt: 
def GenerateCalendarDatesFieldValuesTuples(self):
    """Generates tuples of calendar_dates.txt values. Yield zero tuples if
    this ServicePeriod should not be in calendar_dates.txt ."""
----------
Output: 

    if not self.IsInfinite():
      return

    for tuple in self.GetTuples():
      yield tuple[0], tuple[1], tuple[2], tuple[3], tuple[4], tuple[5], tuple[6], tuple[7], tuple[8], tuple[9], tuple[10], tuple[11]

  def GetTuples(self):
    """Returns a list of calendar_tuples.txt values.

    The list is sorted by the start date.  Each tuple is a tuple of the
    start date and the number of days since the start date.  The tuple is
   
----------
Solution: 

    for date, (exception_type, _) in self.date_exceptions.items():
      yield (self.service_id, date, unicode(exception_type))
--------------------
Prompt: 
def remove_datasets_from_ckan(portal_url, apikey, filter_in=None,
                              filter_out=None, only_time_series=False,
                              organization=None):
    """Borra un dataset en el portal pasado por parámetro.

            Args:
                portal_url (str): La URL del portal CKAN de destino.
                apikey (str): La apikey de un usuario con los permisos que le
                    permitan borrar el dataset.
                filter_in(dict): Diccionario de filtrado positivo, similar al
                    de search.get_datasets.
                filter_out(dict): Diccionario de filtrado negativo, similar al
                    de search.get_datasets.
                only_time_series(bool): Filtrar solo los datasets que tengan
                    recursos con series de tiempo.
                organization(str): Filtrar solo los datasets que pertenezcan a
                    cierta organizacion.
            Returns:
                None
    """
----------
Output: 
sibles.
                filter_out(dict): Filtrado posibles que no se encuentra el dataset.
                only_time_series(bool): Si se encuentra el dataset, no se
                        encuentra el permiso.
                organization (str): La parámetro de la organización.
    """
    # Borrar un usuario
    if not apikey:
        return portal_url

    # Cargar datos
    if filter_in:
        for key, value in filter_in.items():
            if value:
                apikey
----------
Solution: 

    ckan_portal = RemoteCKAN(portal_url, apikey=apikey)
    identifiers = []
    datajson_filters = filter_in or filter_out or only_time_series
    if datajson_filters:
        identifiers += get_datasets(
            portal_url + '/data.json',
            filter_in=filter_in, filter_out=filter_out,
            only_time_series=only_time_series, meta_field='identifier'
        )
    if organization:
        query = 'organization:"' + organization + '"'
        search_result = ckan_portal.call_action('package_search', data_dict={
            'q': query, 'rows': 500, 'start': 0})
        org_identifiers = [dataset['id']
                           for dataset in search_result['results']]
        start = 500
        while search_result['count'] > start:
            search_result = ckan_portal.call_action(
                'package_search', data_dict={
                    'q': query, 'rows': 500, 'start': start})
            org_identifiers += [dataset['id']
                                for dataset in search_result['results']]
            start += 500

        if datajson_filters:
            identifiers = set(identifiers).intersection(set(org_identifiers))
        else:
            identifiers = org_identifiers

    for identifier in identifiers:
        ckan_portal.call_action('dataset_purge', data_dict={'id': identifier})
--------------------
Prompt: 
def set(self, value):
        """This parameter method attempts to set a specific value for this parameter. The value will be validated first, and if it can not be set. An error message will be set in the error property of this parameter"""
----------
Output: 

        if value is None:
            return
        if value is None:
            self.error = None
        else:
            self.error = None
        self.value = value

    def get(self):
        """This parameter method returns the value of this parameter as a string"""
        return self.value

    def set_error(self, error):
        """This method attempts to set a specific value for this parameter. The error will be validated first, and if it can not be set. An error message will be set in the error property of this parameter"""
        self.value = None
        self.error = error

    def
----------
Solution: 

        if self.validate(value):
            #print "Parameter " + self.id + " successfully set to " + repr(value)
            self.hasvalue = True
            if isinstance(value, float):
                self.value = round(value)
            else:
                self.value = int(value)
            return True
        else:
            #print "Parameter " + self.id + " COULD NOT BE set to " + repr(value)
            return False
--------------------
Prompt: 
def send_zipfile(request, fileList):
    """                                                                         
    Create a ZIP file on disk and transmit it in chunks of 8KB,                 
    without loading the whole file into memory. A similar approach can          
    be used for large dynamic PDF files.                                        
    """
----------
Output: 

    # Create a new temporary directory
    tempdir = tempfile.mkdtemp()

    # Create a temporary file
    f = tempfile.NamedTemporaryFile(dir=tempdir)

    # Write the files
    for file in fileList:
        f.write(file + '\n')
    f.flush()

    # Load the file
    try:
        # Load the file
        f.write(f.name)
        f.close()
    except:
        raise Http404

    # Generate the PDF
    try:
        # Create the PDF
        generate_pdf(f, fileList, file)
    except:
        raise Http404

    # Load the
----------
Solution: 

    temp = tempfile.TemporaryFile()
    archive = zipfile.ZipFile(temp, 'w', zipfile.ZIP_DEFLATED)
    for artist,files in fileList.iteritems():
        for f in files:
            archive.write(f[0], '%s/%s' % (artist, f[1]))
    archive.close()
    wrapper = FixedFileWrapper(temp)
    response = HttpResponse(wrapper, content_type='application/zip')
    response['Content-Disposition'] = 'attachment; filename=FrogSources.zip'
    response['Content-Length'] = temp.tell()
    temp.seek(0)
    return response
--------------------
Prompt: 
def create_emergency_codes(self, code=None):
        """Generate emergency codes

        :param code: SMS code
        :type code: str
        :raises: :class:`SteamAuthenticatorError`
        :return: list of codes
        :rtype: list

        .. note::
            A confirmation code is required to generate emergency codes and this method needs
            to be called twice as shown below.

        .. code:: python

            sa.create_emergency_codes()              # request a SMS code
            sa.create_emergency_codes(code='12345')  # creates emergency codes
        """
----------
Output: 
 request a SMS code
            sa.create_emergency_codes(code='12345')  # request a SMS code
            sa.create_emergency_codes(code='12345')  # request a SMS code
            sa.create_emergency_codes(code='12345')  # request a SMS code
            sa.create_emergency_codes(code='12345')  # request a SMS code
            sa.create_emergency_codes(code='12345')  # request a SMS code
            sa.create_emergency_codes(code='12345')  # request a
----------
Solution: 

        if code:
            return self._send_request('createemergencycodes', {'code': code}).get('codes', [])
        else:
            self._send_request('createemergencycodes', {})
            return None
--------------------
Prompt: 
def run_as(self, identifiers):
        """
        :type identifiers:  subject_abcs.IdentifierCollection
        """
----------
Output: 

        for identifier in identifiers:
            if identifier.identifier_type =='subject':
                self.subject_as_dict[identifier.subject_id] = subject_abcs.Subject(identifier)
            elif identifier.identifier_type == 'identifier':
                self.subject_as_dict[identifier.subject_id] = identifier
            elif identifier.identifier_type =='subject_id':
                self.subject_as_dict[identifier.subject_id] = identifier
            elif identifier.identifier_type == 'identifier_type':
                self.subject_as_dict[identifier.subject_id]
----------
Solution: 

        if (not self.has_identifiers):
            msg = ("This subject does not yet have an identity.  Assuming the "
                   "identity of another Subject is only allowed for Subjects "
                   "with an existing identity.  Try logging this subject in "
                   "first, or using the DelegatingSubject.Builder "
                   "to build ad hoc Subject instances with identities as "
                   "necessary.")
            raise ValueError(msg)
        self.push_identity(identifiers)
--------------------
Prompt: 
def set(self, type, offset, value):
        """
        Set the specified bit field and returns its old value.
        """
----------
Output: 

        self.write_byte(offset)
        self.write_byte(value)

    def set_from_string(self, string):
        """
        Set the specified bit field from a string.
        """
        self.write_byte(string.upper())
        self.write_byte(string.lower())

    def set_from_file(self, filename):
        """
        Set the specified bit field from a file.
        """
        self.write_byte(filename.upper())
        self.write_byte(self.read_byte())
        self.write_byte(self.read_byte
----------
Solution: 

        self._command_stack.extend(['SET', type, offset, value])
        return self
--------------------
Prompt: 
def get_item(track_url, client_id=CLIENT_ID):
    """
    Fetches metadata for a track or playlist
    """
----------
Output: 

    if track_url.startswith('http://'):
        return get_http_client(track_url)
    else:
        return get_json_response(client_id)


def get_http_client(url):
    """
    Returns an HTTP client for the given URL
    """
    return requests.get(url, auth=(url, ''), timeout=5)


def get_json_response(url):
    """
    Returns a JSON response for the given URL
    """
    return requests.get(url, auth=(url, ''), timeout=15)


def get_json_response
----------
Solution: 

    try:
        item_url = url['resolve'].format(track_url)

        r = requests.get(item_url, params={'client_id': client_id})
        logger.debug(r.url)
        if r.status_code == 403:
            return get_item(track_url, ALT_CLIENT_ID)

        item = r.json()
        no_tracks = item['kind'] == 'playlist' and not item['tracks']
        if no_tracks and client_id != ALT_CLIENT_ID:
            return get_item(track_url, ALT_CLIENT_ID)
    except Exception:
        if client_id == ALT_CLIENT_ID:
            logger.error('Failed to get item...')
            return
        logger.error('Error resolving url, retrying...')
        time.sleep(5)
        try:
            return get_item(track_url, ALT_CLIENT_ID)
        except Exception as e:
            logger.error('Could not resolve url {0}'.format(track_url))
            logger.exception(e)
            sys.exit(0)
    return item
--------------------
Prompt: 
def computeUniquePointsSensed(nCols, nPoints, s):
  """
  If a network with nCols columns senses an object s times, how many
  unique points are actually sensed?  The number is generally <= nCols * s
  because features may be repeated across sensations.
  """
----------
Output: 

  if nPoints == 0:
    return 0
  else:
    return computeUniquePoints(nCols, nPoints, s)

def computeUniquePoints(nCols, nPoints, s):
  """
  If a network with nCols columns are repeated, how many unique points
  are actually sensed?  The number is generally <= nCols * s
  because features may be repeated across sensations.
  """
  if nPoints == 0:
    return 0
  else:
    return computeUniquePoints(nCols, nPoints, s)

def computeUniquePointsFromSess(nCols,
----------
Solution: 

  if nCols == 1:
    return min(s, nPoints)
  elif nCols < nPoints:
    q = float(nCols) / nPoints
    unique = min(int(round(( 1.0 - math.pow(1.0 - q, s)) * nPoints)),
                 nPoints)
    return unique
  else:
    return nPoints
--------------------
Prompt: 
def submit_jobs(root_dir, jobs, sgeargs=None):
    """ Submit each of the passed jobs to the SGE server, using the passed
    directory as root for SGE output.

    - root_dir       Path to output directory
    - jobs           List of Job objects
    """
----------
Output: 

    for job in jobs:
        sge_server.submit_job(job)

    # Wait for the jobs to complete
    sge_server.wait_for_jobs(jobs)

    # Check if the job has been submitted
    submitted = False
    for job in jobs:
        if job.status =='submitted':
            submitted = True
            break

    # Check if the job has been submitted
    if not submitted:
        sge_server.wait_for_jobs(jobs)

    # Check if the job has been submitted
    if not submitted:
        sge_server.wait_for_jobs(
----------
Solution: 

    waiting = list(jobs)                 # List of jobs still to be done
    # Loop over the list of pending jobs, while there still are any
    while len(waiting) > 0:
        # extract submittable jobs
        submittable = extract_submittable_jobs(waiting)
        # run those jobs
        submit_safe_jobs(root_dir, submittable, sgeargs)
        # remove those from the waiting list
        for job in submittable:
            waiting.remove(job)
--------------------
Prompt: 
def set_config(self, config):
        """See :py:meth:`pyctools.core.config.ConfigMixin.set_config`."""
----------
Output: 

        self.config = config
        self.config.update(self.config_defaults)
        self.config.update(config)
        self.config.update(self.config_defaults)

    def get_config(self):
        """See :py:meth:`pyctools.core.config.ConfigMixin.get_config`."""
        return self.config

    def set_config_defaults(self, config_defaults):
        """See :py:meth:`pyctools.core.config.ConfigMixin.set_config`."""
        self.config_defaults = config_defaults
        self.config
----------
Solution: 

        self.config.update(config)
        if self.config_map:
            for child_name in self.config_map:
                child_config = self._compound_children[child_name].get_config()
                for parent_item, child_item in self.config_map[child_name]:
                    child_config[child_item] = self.config[parent_item]
                self._compound_children[child_name].set_config(child_config)
        else:
            for name, child in self._compound_children.items():
                child.set_config(self.config[name])
--------------------
Prompt: 
def mass_loss_loon05(L,Teff):
    """ 
    mass loss rate van Loon etal (2005).

    Parameters
    ----------
    L : float
        L in L_sun.
    Teff : float
        Teff in K.
        
    Returns
    -------
    Mdot
        Mdot in Msun/yr
    
    Notes
    -----
    ref: van Loon etal 2005, A&A 438, 273
    
    """
----------
Output: 

    # calculate the mass loss rate
    T = Teff/L
    # calculate the reference temperature
    Tref = T/T
    # calculate the reference pressure
    P = pressure(Tref)
    # calculate the reference pressure derivative
    dT = (P-Tref)/Tref
    # calculate the mass loss rate
    Mdot = 0.5*(dT-T)
    # calculate the mass loss rate derivative
    dM = (P-T)/Tref
    # calculate the mass loss rate derivative
    dP = pressure(P)
    # calculate the mass loss rate derivative
    dP = pressure
----------
Solution: 

    
    Mdot = -5.65 + np.log10(old_div(L,10.**4)) -6.3*np.log10(old_div(Teff,3500.))
    return Mdot
--------------------
Prompt: 
def disconnect_from(callback, signals):
    """
    Removes a callback from specified signal registries and prevents it from responding
    to any emitted signal.

    :param callback: A callable registered with smokesignal
    :param signals: A single signal or list/tuple of signals
    """
----------
Output: 

    for signal in signals:
        if isinstance(signal, tuple):
            for signal_tuple in signal:
                if signal_tuple[0] == signal_tuple[1]:
                    if callback in signal_tuple[2]:
                        del signal_tuple[2][callback]
                    if callback in signal_tuple[3]:
                        del signal_tuple[3][callback]
                    if callback in signal_tuple[4]:
                        del signal_tuple[4][callback]
                    if callback in signal_tuple[5]:
                        del signal_tuple[5][callback]
                    if callback in signal_tuple[6]:
                       
----------
Solution: 

    # Support for lists of signals
    if not isinstance(signals, (list, tuple)):
        signals = [signals]

    # Remove callback from receiver list if it responds to the signal
    for signal in signals:
        if responds_to(callback, signal):
            receivers[signal].remove(callback)
--------------------
Prompt: 
def dM(self, k, t, param, Mkt, tips=None, gaps=None):
        """See docs for `DistributionModel` abstract base class."""
----------
Output: 

        pass

    def __str__(self):
        """See docs for `DistributionModel` abstract base class."""
        return self.__class__.__name__

    def __repr__(self):
        """See docs for `DistributionModel` abstract base class."""
        return self.__class__.__name__ + '()'

    def _initialize_parameters(self):
        """See docs for `DistributionModel` abstract base class."""
        self._param_names = []
        self._param_values = []
        self._param_bounds = []
        self._param_frozen = []
        self._param_frozen_parameters = []
        self._param
----------
Solution: 

        assert 0 <= k < self.ncats
        assert ((param in self.freeparams) or (param == 't') or (
                param == self.distributedparam))
        assert param not in self.distributionparams
        return self._models[k].dM(t, param, Mkt, tips=tips, gaps=gaps)
--------------------
Prompt: 
def _process_fields_convert_map(self, parameters, download=False):
        """
        process fields_convert_map, ListView doesn't support list type but dict

        fields_convert_map should be define as list or dict
        for list, it can be:
            [name, name, ...]
            [(name, func), (name, func), ...] if func is str, it'll be the property name of class
        for dict, it can be:
            {'name':func, ...}
        :param model: model object
        :param parameters:
        :param prefix: it'll used to combine prefix+_convert_xxx to get convert function
            from class
        :return:
        """
----------
Output: 
 the prefix of the fields
        :param suffix: it'll used to combine the suffix of the fields
        :param download: it'll use the download method
        :return:
        """
        if isinstance(parameters, dict):
            for field in parameters:
                if isinstance(field, str):
                    field = getattr(self, field)
                if isinstance(field, list):
                    field = _process_fields_convert(field, self.fields_convert_map)
                    field = _process_fields_convert(field, self.fields_convert_map)
                if isinstance(field, str):
                    field = getattr
----------
Solution: 

        if 'fields_convert_map' in parameters:
            _f = parameters.get('fields_convert_map') or []
            parameters['fields_convert_map'] = self._get_fields_convert_map(_f, download)
--------------------
Prompt: 
def _graphic(self):
        """
        Adds the correct graphic options depending of the OS
        """
----------
Output: 

        self.add_option(
            'graphic_width',
            int(self.get_option('graphic_width', 0)),
            doc='Width of the graphic in pixels',
            default=0,
        )
        self.add_option(
            'graphic_height',
            int(self.get_option('graphic_height', 0)),
            doc='Height of the graphic in pixels',
            default=0,
        )
        self.add_option(
            'graphic_margin',
            int(self.get_option('graphic_margin', 0)),
            doc
----------
Solution: 


        if sys.platform.startswith("win"):
            return []
        if len(os.environ.get("DISPLAY", "")) > 0:
            return []
        if "-nographic" not in self._options:
            return ["-nographic"]
        return []
--------------------
Prompt: 
def do_async_send(self):
        """
        Send any queued data. This function should only be called after a write
        event on a file descriptor.
        """
----------
Output: 

        self.send_queue.put(self.file_descriptor)
        self.send_queue.put(self.file_descriptor)

    def send_file(self, path, callback=None):
        """
        Send a file to the server.

        :param path: The path to the file to send.
        :type path: str
        :param callback: The callback to call when the file is sent.
            This function should only be called after a write event on a file
            descriptor.
        :type callback: function
        :return: The number of bytes written.
        :rtype: int
        """
----------
Solution: 

        assert len(self.sendbuf)

        nwritten = self.sock.send(self.sendbuf)
        nframes = 0

        for entry in self.sendbuf_frames:
            frame, offset, callback = entry

            if offset <= nwritten:
                nframes += 1

                if callback:
                    callback()
            else:
                entry[1] -= nwritten

        self.sendbuf = self.sendbuf[nwritten:]
        self.sendbuf_frames = self.sendbuf_frames[nframes:]
--------------------
Prompt: 
def get_pt(self, viewer, points, pt, canvas_radius=None):
        """Takes an array of points `points` and a target point `pt`.
        Returns the first index of the point that is within the
        radius of the target point.  If none of the points are within
        the radius, returns None.
        """
----------
Output: 

        if canvas_radius is None:
            canvas_radius = self.canvas_radius
        for i in range(len(points)):
            if points[i] <= pt[i] <= target_point[i]:
                return i
        return None

    def get_index_of_target_point(self, target_point):
        """Takes an index of a target point `index` and returns the
        index of the target point.  If none of the points are within
        the radius, returns None.
        """
        return self.get_index_of_pt(target_point, self.target
----------
Solution: 

        if canvas_radius is None:
            canvas_radius = self.cap_radius

        if hasattr(self, 'rot_deg'):
            # rotate point back to cartesian alignment for test
            ctr_pt = self.get_center_pt()
            pt = trcalc.rotate_coord(pt, [-self.rot_deg], ctr_pt)

        res = self.within_radius(viewer, points, pt, canvas_radius)
        return np.flatnonzero(res)
--------------------
Prompt: 
def ctrl_request_update(_, nl_sock_h):
    """https://github.com/thom311/libnl/blob/libnl3_2_25/lib/genl/ctrl.c#L37.

    Positional arguments:
    nl_sock_h -- nl_sock class instance.

    Returns:
    Integer, genl_send_simple() output.
    """
----------
Output: 

    return _ctrl_request_update(nl_sock_h, _nl_socket_h.ctrl_request_update)

def ctrl_request_get_name(_, nl_sock_h):
    """https://github.com/thom311/libnl/blob/libnl3_2_25/lib/genl/ctrl.c#L38.

    Positional arguments:
    nl_sock_h -- nl_sock class instance.

    Returns:
    String, ctrl_send_name() output.
    """
    return _ctrl_request_get_name(
----------
Solution: 

    return int(genl_send_simple(nl_sock_h, GENL_ID_CTRL, CTRL_CMD_GETFAMILY, CTRL_VERSION, NLM_F_DUMP))
--------------------
Prompt: 
def prep_seq2c_bed(data):
    """Selecting the bed file, cleaning, and properly annotating for Seq2C
    """
----------
Output: 

    # get the chromosomes
    chroms = []
    for i in range(0, len(data), 3):
        chrom = data[i:i+3]
        if chrom in chroms:
            chroms.remove(chrom)
        else:
            chroms.append(chrom)
    # get the contig names
    contig_names = []
    for i in range(0, len(data), 4):
        contig = data[i:i+4]
        if contig in contig_names:
            contig_names.remove(contig)
        else:
            con
----------
Solution: 

    if dd.get_background_cnv_reference(data, "seq2c"):
        bed_file = _background_to_bed(dd.get_background_cnv_reference(data, "seq2c"), data)
    else:
        bed_file = regions.get_sv_bed(data)
    if bed_file:
        bed_file = bedutils.clean_file(bed_file, data, prefix="svregions-")
    else:
        bed_file = bedutils.clean_file(dd.get_variant_regions(data), data)
    if not bed_file:
        return None

    col_num = bt.BedTool(bed_file).field_count()
    if col_num < 4:
        annotated_file = annotate.add_genes(bed_file, data, max_distance=0)
        if annotated_file == bed_file:
            raise ValueError("BED file for Seq2C must be annotated with gene names, "
                             "however the input BED is 3-columns and we have no transcript "
                             "data to annotate with " + bed_file)
        annotated_file = annotate.gene_one_per_line(annotated_file, data)
    else:
        annotated_file = bed_file

    ready_file = "%s-seq2cclean.bed" % (utils.splitext_plus(annotated_file)[0])
    if not utils.file_uptodate(ready_file, annotated_file):
        bed = bt.BedTool(annotated_file)
        if col_num > 4 and col_num != 8:
            bed = bed.cut(range(4))
        bed = bed.filter(lambda x: x.name not in ["", ".", "-"])
        with file_transaction(data, ready_file) as tx_out_file:
            bed.saveas(tx_out_file)
        logger.debug("Saved Seq2C clean annotated ready input BED into " + ready_file)

    return ready_file
--------------------
Prompt: 
def check_node_parent(
        cls, resource_id, new_parent_id, db_session=None, *args, **kwargs
    ):
        """
        Checks if parent destination is valid for node

        :param resource_id:
        :param new_parent_id:
        :param db_session:
        :return:
        """
----------
Output: 

        if db_session is None:
            db_session = db.session
        if new_parent_id is None:
            return False
        if resource_id is None:
            return False
        if not cls.query.filter_by(id=resource_id).first():
            return False
        if not cls.query.filter_by(parent_id=new_parent_id).first():
            return False
        if not cls.query.filter_by(id=new_parent_id).first():
            return False
        if not cls.query.filter_by(parent_id=new
----------
Solution: 

        db_session = get_db_session(db_session)
        new_parent = ResourceService.lock_resource_for_update(
            resource_id=new_parent_id, db_session=db_session
        )
        # we are not moving to "root" so parent should be found
        if not new_parent and new_parent_id is not None:
            raise ZigguratResourceTreeMissingException("New parent node not found")
        else:
            result = cls.path_upper(new_parent_id, db_session=db_session)
            path_ids = [r.resource_id for r in result]
            if resource_id in path_ids:
                raise ZigguratResourceTreePathException(
                    "Trying to insert node into itself"
                )
--------------------
Prompt: 
def draw(self):
        """
        Draw the elbow curve for the specified scores and values of K.
        """
----------
Output: 

        if self.k_fold:
            self.draw_kfold()
        if self.k_value:
            self.draw_kvalue()
        if self.k_value_std:
            self.draw_kfold()
        if self.k_value_mean:
            self.draw_kfold()
        if self.k_value:
            self.draw_kvalue(std=self.k_value_std)
        if self.k_value:
            self.draw_kfold(mean=self.k_value)
        if self.k_value:
            self
----------
Solution: 

        # Plot the silhouette score against k
        self.ax.plot(self.k_values_, self.k_scores_, marker="D")
        if self.locate_elbow and self.elbow_value_!=None:
            elbow_label = "$elbow\ at\ k={}, score={:0.3f}$".format(self.elbow_value_, self.elbow_score_)
            self.ax.axvline(self.elbow_value_, c=LINE_COLOR, linestyle="--", label=elbow_label)
            
        # If we're going to plot the timings, create a twinx axis
        if self.timings:
            self.axes = [self.ax, self.ax.twinx()]
            self.axes[1].plot(
                self.k_values_, self.k_timers_, label="fit time",
                c='g', marker="o", linestyle="--", alpha=0.75,
            )


        return self.ax
--------------------
Prompt: 
def transform(curve, tf):
    """Transforms the curve by the homogeneous transformation matrix tf"""
----------
Output: 

    return tf.dot(tf, np.transpose(tf))


def get_curve_from_file(filename):
    """Loads a homogeneous transformation matrix from a file"""
    with open(filename, 'r') as f:
        matrix = np.array(f.read(), dtype=np.float64)
    return np.array(matrix, dtype=np.float64)


def get_matrix_from_file(filename):
    """Loads a homogeneous transformation matrix from a file"""
    with open(filename, 'r') as f:
        matrix = np.array(
----------
Solution: 

    def to_point(p):
        return np.array([[p.real], [p.imag], [1.0]])

    def to_vector(z):
        return np.array([[z.real], [z.imag], [0.0]])

    def to_complex(v):
        return v.item(0) + 1j * v.item(1)

    if isinstance(curve, Path):
        return Path(*[transform(segment, tf) for segment in curve])
    elif is_bezier_segment(curve):
        return bpoints2bezier([to_complex(tf.dot(to_point(p)))
                               for p in curve.bpoints()])
    elif isinstance(curve, Arc):
        new_start = to_complex(tf.dot(to_point(curve.start)))
        new_end = to_complex(tf.dot(to_point(curve.end)))
        new_radius = to_complex(tf.dot(to_vector(curve.radius)))
        return Arc(new_start, radius=new_radius, rotation=curve.rotation,
                   large_arc=curve.large_arc, sweep=curve.sweep, end=new_end)
    else:
        raise TypeError("Input `curve` should be a Path, Line, "
                        "QuadraticBezier, CubicBezier, or Arc object.")
--------------------
Prompt: 
def is_mouse_over(self, event):
        """
        Check whether a MouseEvent is over thus scroll bar.

        :param event: The MouseEvent to check.

        :returns: True if the mouse event is over the scroll bar.
        """
----------
Output: 

        if event.type() == MouseEvent.MOUSE_OVER:
            return self.is_mouse_over(event)
        elif event.type() == MouseEvent.MIDDLE_DOWN:
            return self.is_middle_down(event)
        elif event.type() == MiddleDownEvent.MIDDLE_UP:
            return self.is_middle_up(event)
        elif event.type() == MiddleUpEvent.MIDDLE_UP:
            return self.is_middle_up(event)
        elif event.type() == MouseOverEvent.MIDDLE_
----------
Solution: 

        return event.x == self._x and self._y <= event.y < self._y + self._height
--------------------
Prompt: 
def to_dict(self):  # cls, self):
        """Convert Docpie into a JSONlizable dict.

        Use it in this way:
        pie = Docpie(__doc__)
        json.dumps(pie.convert_2_dict())

        Note the `extra` info will be lost if you costomize that,
        because a function is not JSONlizable.
        You can use `set_config(extra={...})` to set it back.
        """
----------
Output: 

        return self.convert_2_dict()

    def set_config(self, config):
        """Set the config.

        Use it in this way:
        pie.set_config(config)
        """
        self.config = config

    def to_json(self):
        """Convert Docpie into a JSON string.

        Use it in this way:
        json.dumps(pie.to_dict())
        """
        return self.__doc__

    def to_dict_with_list(self):
        """Convert a list of Docpie objects into a JSON string.

        Returns a list of Docpie objects
----------
Solution: 

        config = {
            'stdopt': self.stdopt,
            'attachopt': self.attachopt,
            'attachvalue': self.attachvalue,
            'auto2dashes': self.auto2dashes,
            'case_sensitive': self.case_sensitive,
            'namedoptions': self.namedoptions,
            'appearedonly': self.appeared_only,
            'optionsfirst': self.options_first,
            'option_name': self.option_name,
            'usage_name': self.usage_name,
            'name': self.name,
            'help': self.help,
            'version': self.version
        }

        text = {
            'doc': self.doc,
            'usage_text': self.usage_text,
            'option_sections': self.option_sections,
        }

        # option = [convert_2_dict(x) for x in self.options]

        option = {}
        for title, options in self.options.items():
            option[title] = [convert_2_dict(x) for x in options]

        usage = [convert_2_dict(x) for x in self.usages]

        return {
            '__version__': self._version,
            '__class__': 'Docpie',
            '__config__': config,
            '__text__': text,
            'option': option,
            'usage': usage,
            'option_names': [list(x) for x in self.opt_names],
            'opt_names_required_max_args': self.opt_names_required_max_args
        }
--------------------
Prompt: 
def set_collection(self, collection):
        """Reset sort state, set collection and emit resetModel signal"""
----------
Output: 

        self.model.set_sort_column_id(collection.get_sort_column_id())
        self.emit(SIGNAL("dataChanged(const QModelIndex&)"), self.model.index_changed)

    def set_sort_order(self, order):
        """Set sort order"""
        self.model.set_sort_order(order)
        self.emit(SIGNAL("dataChanged(const QSortFilter&)"), self.model.sort_changed)
        self.sort(0, Qt.AscendingOrder)

    def sort_by_name(self, column, order
----------
Solution: 

        self.beginResetModel()
        self.collection = collection
        self.sort_state = (-1, False)
        self.endResetModel()
--------------------
Prompt: 
def to_capabilities(self):
        """
        Creates a capabilities with all the options that have been set

        :Returns: A dictionary with everything
        """
----------
Output: 

        capabilities = {}
        options = self.options
        for option in options:
            capabilities[option] = {}
            for option_value in options[option]:
                capabilities[option][option_value] = {}
                for key, value in self.options[option_value].items():
                    capabilities[option][option_value][key] = self.options[option_value][option_value].get(key, "")
            for key, value in self.options[option_value].items():
                capabilities[option_value][option_value]["default"] = self.options[option_value]["default
----------
Solution: 

        caps = self._caps
        chrome_options = self.experimental_options.copy()
        chrome_options["extensions"] = self.extensions
        if self.binary_location:
            chrome_options["binary"] = self.binary_location
        chrome_options["args"] = self.arguments
        if self.debugger_address:
            chrome_options["debuggerAddress"] = self.debugger_address

        caps[self.KEY] = chrome_options

        return caps
--------------------
Prompt: 
def exec_command(self, cmd, tmp_path, sudo_user, sudoable=False, executable='/bin/sh'):
        """ run a command on the remote host """
----------
Output: 

        super(Connection, self).exec_command(cmd, tmp_path, sudo_user, sudoable)

        self._display.vvv("EXEC %s" % cmd, host=self._play_context.remote_addr)

        if executable:
            cmd = executable +'-c'+ cmd

        self._display.vvv("EXEC %s" % cmd, host=self._play_context.remote_addr)
        rc, stdout, stderr = self._exec_command(cmd, tmp_path)

        # save result in resultfile
        if self._play_context.become and self
----------
Solution: 


        vvv("EXEC COMMAND %s" % cmd)

        if self.runner.sudo and sudoable:
            raise errors.AnsibleError("fireball does not use sudo, but runs as whoever it was initiated as.  (That itself is where to use sudo).")

        data = dict(
            mode='command',
            cmd=cmd,
            tmp_path=tmp_path,
            executable=executable,
        )
        data = utils.jsonify(data)
        data = utils.encrypt(self.key, data)
        self.socket.send(data)
        
        response = self.socket.recv()
        response = utils.decrypt(self.key, response)
        response = utils.parse_json(response)

        return (response.get('rc',None), '', response.get('stdout',''), response.get('stderr',''))
--------------------
Prompt: 
def _tidy(self, html, smart_tidy):
        """ Tidy HTML if we have a tidy method.

        This fixes problems with some sites which would otherwise trouble
        DOMDocument's HTML parsing.

        Although sometimes it makes the problem worse, which is why we can
        override it in site config files.
        """
----------
Output: 

        if smart_tidy:
            return smart_tidy.tidy(html)
        return html

    def _tidy_error(self, msg):
        """ Return a string error message if the HTML string is not well formed.

        This fixes problems with some sites which would otherwise trouble
        DOMDocument's HTML parsing.
        """
        if not msg:
            return ''
        return''.join(msg.split())

    def _get_element_by_id(self, id):
        """ Return the element with the given id.

        This is a helper method for the DOM Level 3 spec.
        """
----------
Solution: 


        if self.config.tidy and tidylib and smart_tidy:

            try:
                document, errors = tidylib.tidy_document(html, self.tidy_config)

            except UnicodeDecodeError:
                # For some reason, pytidylib fails to decode, whereas the
                # original html content converts perfectly manually.
                document, errors = tidylib.tidy_document(html.encode('utf-8'),
                                                         self.tidy_config)
                document = document.decode('utf-8')
            # if errors:
            #     LOGGER.debug(u'Ignored errors returned by tidylib: %s',
            #                  errors)

            self.tidied = True
            self.html = document

            LOGGER.info(u'Tidied document.')

        else:
            self.html = html
--------------------
Prompt: 
def get_place_tags(index_page, domain):  #: TODO geoip to docstring
    """
    Return list of `place` tags parsed from `meta` and `whois`.

    Args:
        index_page (str): HTML content of the page you wisht to analyze.
        domain (str): Domain of the web, without ``http://`` or other parts.

    Returns:
        list: List of :class:`.SourceString` objects.
    """
----------
Output: 

    return [
        source_string.strip()
        for source_string in index_page.xpath('//*[@id="meta"]/@id | //*[@id="whois"]/@id', namespaces=ns)
    ]


def get_place_links(index_page, domain):  #: TODO geoip to docstring
    """
    Return list of `place` links parsed from `meta` and `whois`.

    Args:
        index_page (str): HTML content of the page you wisht to analyze.
        domain (str): Domain of the web, without ``http://
----------
Solution: 

    ip_address = get_ip_address(domain)
    dom = dhtmlparser.parseString(index_page)

    place_tags = [
        get_html_geo_place_tags(dom),
        get_whois_tags(ip_address),
        # [_get_geo_place_tag(ip_address)],  # TODO: implement geoip
    ]

    return sum(place_tags, [])
--------------------
Prompt: 
def _clean_spaces_backtick_quoted_names(tok):
    """Clean up a column name if surrounded by backticks.

    Backtick quoted string are indicated by a certain tokval value. If a string
    is a backtick quoted token it will processed by
    :func:`_remove_spaces_column_name` so that the parser can find this
    string when the query is executed.
    See also :meth:`NDFrame._get_space_character_free_column_resolver`.

    Parameters
    ----------
    tok : tuple of int, str
        ints correspond to the all caps constants in the tokenize module

    Returns
    -------
    t : tuple of int, str
        Either the input or token or the replacement values
    """
----------
Output: 
 module.

    Returns
    -------
    tuple of int, str
        The tuple of tokens that were removed.
    """
    if tok[0] == tokenize.OP and tok[1] == tokenize.OP:
        return (tokenize.NAME, 'OP')
    elif tok[0] == tokenize.NAME and tok[1] == tokenize.NAME:
        return (tokenize.OP, 'NAME')
    elif tok[0] == tokenize.NAME and tok[1] == tokenize.NAME:
        return (tokenize.OP, 'OP')
    elif tok[0] == tokenize.OP and tok[1]
----------
Solution: 

    toknum, tokval = tok
    if toknum == _BACKTICK_QUOTED_STRING:
        return tokenize.NAME, _remove_spaces_column_name(tokval)
    return toknum, tokval
--------------------
Prompt: 
def make_imshow_plot(grid, name):
    """
    Takes a grid of RGB or RGBA values and a filename to save the figure into.
    Generates a figure by coloring all grid cells appropriately.
    """
----------
Output: 

    if type(grid) == np.ndarray:
        grid = np.array(grid)
    if type(grid) == np.ndarray:
        grid = np.array(grid)
    if type(grid) == np.ndarray:
        grid = np.array(grid)
    if type(grid) == str:
        plt.savefig(name)
    else:
        plt.savefig(name, bbox_inches='tight')


def make_imshow_grid(grid, name):
    """
    Takes a list of RGB values and a filename to save the figure into.
    Generates a grid
----------
Solution: 

    plt.tick_params(labelbottom="off", labeltop="off", labelleft="off",
                    labelright="off", bottom="off", top="off", left="off",
                    right="off")
    plt.imshow(grid, interpolation="nearest", aspect=1, zorder=1)
    plt.tight_layout()
    plt.savefig(name, dpi=1000, bbox_inches="tight")
--------------------
Prompt: 
def _torque_queue_nodes(queue):
    """Retrieve the nodes available for a queue.

    Parses out nodes from `acl_hosts` in qstat -Qf and extracts the
    initial names of nodes used in pbsnodes.
    """
----------
Output: 

    # The first line is always the header
    line = queue[0].split()
    # The second line is always the header
    header = line[0]
    # The third line is always the header
    if len(line) > 1:
        header = line[1]
    # The fourth line is always the header
    if len(line) > 3:
        header = line[3]
    # The fifth line is always the header
    if len(line) > 4:
        header = line[4]
    # The sixth line is always the header
    if len(line) >
----------
Solution: 

    qstat_out = subprocess.check_output(["qstat", "-Qf", queue]).decode()
    hosts = []
    in_hosts = False
    for line in qstat_out.split("\n"):
        if line.strip().startswith("acl_hosts = "):
            hosts.extend(line.replace("acl_hosts = ", "").strip().split(","))
            in_hosts = True
        elif in_hosts:
            if line.find(" = ") > 0:
                break
            else:
                hosts.extend(line.strip().split(","))
    return tuple([h.split(".")[0].strip() for h in hosts if h.strip()])
--------------------
Prompt: 
def get_matches(expr_lst, ts):
    """
    Get a list of TimeSeries objects that match the given expression.
    :param list expr_lst: Expression
    :param list ts: TimeSeries
    :return list new_ts: Matched time series objects
    :return list idxs: Indices of matched objects
    """
----------
Output: 

    return [TimeSeries(idx) for idx in expr_lst if ts.contains(idx)]


def get_matches_from_list(expr_lst, ts):
    """
    Get a list of TimeSeries objects that match the given expression.
    :param list expr_lst: Expression
    :param list ts: TimeSeries
    :return list new_ts: Matched time series objects
    :return list idxs: Indices of matched objects
    """
    return [TimeSeries(idx) for idx in expr_lst if ts.contains(idx) and ts.contains(ts.get_
----------
Solution: 

    logger_ts.info("enter get_matches")
    new_ts = []
    idxs = []
    match = False
    try:
        for idx, ts_data in enumerate(ts):
            for expr in expr_lst:
                try:
                    val = ts_data[expr[0]]
                    # Check what comparison operator is being used
                    if expr[1] == 'in':
                        # "IN" operator can't be used in get_truth. Handle first.
                        if expr[2] in val:
                            match = True
                    elif match_operators(val, expr[1], expr[2]):
                        # If it's a typical operator, check with the truth test.
                        match = True
                    else:
                        # If one comparison is false, then it can't possibly be a match
                        match = False
                        break
                except KeyError as e:
                    logger_ts.warn("get_matches: KeyError: getting value from TimeSeries object, {}, {}".format(expr, e))
                    match = False
                except IndexError as e:
                    logger_ts.warn("get_matches: IndexError: getting value from TimeSeries object, {}, {}".format(expr, e))
                    match = False
            if match:
                idxs.append(idx)
                new_ts.append(ts_data)
    except AttributeError as e:
        logger_ts.debug("get_matches: AttributeError: unable to get expression matches, {},  {}".format(type(ts), e))
        print("Error: Timeseries is an invalid data type")
    if not new_ts:
        print("No matches found for that expression")
    else:
        print("Found {} matches from {} columns".format(len(new_ts), len(ts)))
    logger_ts.info("exit get_matches")
    return new_ts, idxs
--------------------
Prompt: 
def defverb(self, s1, p1, s2, p2, s3, p3):
        """
        Set the verb plurals for s1, s2 and s3 to p1, p2 and p3 respectively.

        Where 1, 2 and 3 represent the 1st, 2nd and 3rd person forms of the verb.

        """
----------
Output: 

        self.defaultid = defaultid
        self.defbultid = defbultid
        self.defaultid_s1 = defaultid_s1
        self.defaultid_s2 = defaultid_s2
        self.defaultid_s3 = defaultid_s3
        self.defbultid_p1 = defbultid_p1
        self.defbultid_p2 = defbultid_p2
        self.defaultid_p3 = defaulti
----------
Solution: 

        self.checkpat(s1)
        self.checkpat(s2)
        self.checkpat(s3)
        self.checkpatplural(p1)
        self.checkpatplural(p2)
        self.checkpatplural(p3)
        self.pl_v_user_defined.extend((s1, p1, s2, p2, s3, p3))
        return 1
--------------------
Prompt: 
def unixtimestamp(datetime):
    """Get unix time stamp from that given datetime. If datetime
    is not tzaware then it's assumed that it is UTC
    """
----------
Output: 

    if not datetime.tzinfo:
        return datetime.replace(tzinfo=utc)
    return datetime.astimezone(utc).replace(tzinfo=None)


def unixtimestamp_to_datetime(unixtimestamp):
    """Convert a unix timestamp to a datetime object
    """
    if not unixtimestamp:
        return None
    return datetime.utcfromtimestamp(unixtimestamp)


def get_date_range(start_date, end_date):
    """Get the start and end date range from the given start and end date
    """
    if not start_date:
        return None
    if not end_date:
       
----------
Solution: 

    epoch = UTC.localize(datetime.utcfromtimestamp(0))
    if not datetime.tzinfo:
        dt = UTC.localize(datetime)
    else:
        dt = UTC.normalize(datetime)
    delta = dt - epoch
    return total_seconds(delta)
--------------------
Prompt: 
def insert(self, idx, val):
        """
        Insert the element *val* into the list at *idx*. Raises a ValueError if
        the *val* at *idx* would violate the sort order.
        """
----------
Output: 

        if idx < 0:
            raise ValueError("Negative indexing is not allowed in Insert")
        if idx >= len(self.data):
            raise ValueError("Index value is out of range")
        self.data.insert(idx, val)
        self.sort_order[idx] = idx
        self.sort_order[self.sort_order.index(val)] = self.sort_order.index(idx)

    def remove(self, val):
        """
        Remove the element *val* from the list at *idx*. Raises a ValueError if
        the *val* at *idx* would vi
----------
Solution: 

        _maxes, _lists, _len = self._maxes, self._lists, self._len

        if idx < 0:
            idx += _len
        if idx < 0:
            idx = 0
        if idx > _len:
            idx = _len

        if not _maxes:
            # The idx must be zero by the inequalities above.
            _maxes.append(val)
            _lists.append([val])
            self._len = 1
            return

        if not idx:
            if val > _lists[0][0]:
                msg = '{0} not in sort order at index {1}'.format(repr(val), 0)
                raise ValueError(msg)
            else:
                _lists[0].insert(0, val)
                self._expand(0)
                self._len += 1
                return

        if idx == _len:
            pos = len(_lists) - 1
            if _lists[pos][-1] > val:
                msg = '{0} not in sort order at index {1}'.format(repr(val), _len)
                raise ValueError(msg)
            else:
                _lists[pos].append(val)
                _maxes[pos] = _lists[pos][-1]
                self._expand(pos)
                self._len += 1
                return

        pos, idx = self._pos(idx)
        idx_before = idx - 1
        if idx_before < 0:
            pos_before = pos - 1
            idx_before = len(_lists[pos_before]) - 1
        else:
            pos_before = pos

        before = _lists[pos_before][idx_before]
        if before <= val <= _lists[pos][idx]:
            _lists[pos].insert(idx, val)
            self._expand(pos)
            self._len += 1
        else:
            msg = '{0} not in sort order at index {1}'.format(repr(val), idx)
            raise ValueError(msg)
--------------------
Prompt: 
def restart(self):
        """
        Performs a soft reload of the HAProxy process.
        """
----------
Output: 

        self.logger.info("Restarting HAProxy process")
        self.process.send_signal(signal.SIGHUP)
        self.process.wait()

    def run(self):
        """
        Runs the HAProxy process.
        """
        self.logger.info("Starting HAProxy process")
        self.process = subprocess.Popen(self.command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        self.logger.info("Restarting HAProxy process")
        self.restart()
        self.logger.info("HAProxy process is running")

    def stop(
----------
Solution: 

        version = self.get_version()

        command = [
            "haproxy",
            "-f", self.config_file_path, "-p", self.pid_file_path
        ]
        if version and version >= (1, 5, 0):
            command.extend(["-L", self.peer.name])
        if os.path.exists(self.pid_file_path):
            with open(self.pid_file_path) as fd:
                command.extend(["-sf", fd.read().replace("\n", "")])

        try:
            output = subprocess.check_output(command)
        except subprocess.CalledProcessError as e:
            logger.error("Failed to restart HAProxy: %s", str(e))
            return

        if output:
            logging.error("haproxy says: %s", output)

        logger.info("Gracefully restarted HAProxy.")
--------------------
Prompt: 
def _score(estimator, X_test, y_test, scorer):
    """Compute the score of an estimator on a given test set."""
----------
Output: 

    if hasattr(estimator,'score'):
        # For multi-metric evaluation, the score method of _score expects
        # scores to be a float between 0.0 and 1.0
        return estimator.score(X_test, y_test)
    else:
        # For multi-class evaluation, the score method is proportional to the
        # number of labels.
        return 1.0


def _permutation_test_score(estimator, X, y, n_permutations, n_labels):
    """Auxiliary function for permutation_test_score"""
    if hasattr(estimator, 'permutation'):
----------
Solution: 

    if y_test is None:
        score = scorer(estimator, X_test)
    else:
        score = scorer(estimator, X_test, y_test)
    if not isinstance(score, numbers.Number):
        raise ValueError("scoring must return a number, got %s (%s) instead."
                         % (str(score), type(score)))
    return score
--------------------
Prompt: 
def connect(self):
        """
        Create internal connection to AMQP service.
        """
----------
Output: 

        self.connection = self.connection_class(self.connection_params)
        self.connection.on_open(self.on_open)
        self.connection.on_close(self.on_close)
        self.connection.on_open(self.on_open)
        self.connection.on_close(self.on_close)
        self.connection.on_open(self.on_close)
        self.connection.on_open(self.on_close)
        self.channel.exchange_declare(exchange=self.exchange, type='direct')
        self
----------
Solution: 

        logging.info("Connecting to {} with user {}.".format(self.host, self.username))
        credentials = pika.PlainCredentials(self.username, self.password)
        connection_params = pika.ConnectionParameters(host=self.host,
                                                      credentials=credentials,
                                                      heartbeat_interval=self.heartbeat_interval)
        self.connection = pika.BlockingConnection(connection_params)
--------------------
Prompt: 
def list(database=None, directory=None, verbose=None):
    """List migrations."""
----------
Output: 

    if database is None:
        database = get_database()
    if directory is None:
        directory = get_directory()
    if verbose is None:
        verbose = conf.get('verbosity', '0')
    if database not in databases:
        raise Exception('Unknown database: %s' % database)
    if directory not in directory_migrations:
        raise Exception('Unknown directory: %s' % directory)
    return directory_migrations[database]


def get_database():
    """Return the name of the current database."""
    return conf.get('db_name', 'invenio')


def get
----------
Solution: 

    router = get_router(directory, database, verbose)
    click.echo('Migrations are done:')
    click.echo('\n'.join(router.done))
    click.echo('')
    click.echo('Migrations are undone:')
    click.echo('\n'.join(router.diff))
--------------------
Prompt: 
def _extract_meta_value(self, tag):
        # type: (str, List[str]) -> str
        """Find a target value by `tag` from given meta data.

        :param tag: str
        :param meta_data: list
        :return: str
        """
----------
Output: 

        target_value = None
        for meta_data in meta_data_list:
            if tag in meta_data:
                target_value = meta_data[tag]
                break
        return target_value

    def _get_target_value(self, tag, target_value):
        # type: (str, str, Any) -> str
        """Find a target value by `tag` from given target value.

        :param tag: str
        :param target_value: str
        :return: str
        """
        for target_item in target_value:
            if tag in target_item:
----------
Solution: 

        try:
            return [l[len(tag):] for l in self.meta_data if l.startswith(tag)][0]
        except IndexError:
            return '* Not Found *'
--------------------
Prompt: 
def from_dict(input_dict):
        """
        Instantiate an object of a derived class using the information
        in input_dict (built by the to_dict method of the derived class).
        More specifically, after reading the derived class from input_dict,
        it calls the method _build_from_input_dict of the derived class.
        Note: This method should not be overrided in the derived class. In case
        it is needed, please override _build_from_input_dict instate.

        :param dict input_dict: Dictionary with all the information needed to
           instantiate the object.
        """
----------
Output: 

        :return:
        """
        self._input_dict = input_dict
        self._instate = instate
        self._build_from_state()

    def _build_from_state(self):
        """
        Build the object from the given state.
        """
        self._instate.add_state(self.state_id, self.state_name, self.state_id)
        self.instate_id = self._instate.id
        self.state_name = self._instate.name
        self.state_name = self.state_name.replace(" ",
----------
Solution: 


        import copy
        input_dict = copy.deepcopy(input_dict)
        normalizer_class = input_dict.pop('class')
        import GPy
        normalizer_class = eval(normalizer_class)
        return normalizer_class._build_from_input_dict(normalizer_class, input_dict)
--------------------
Prompt: 
def _structure_tuple(self, obj, tup):
        """Deal with converting to a tuple."""
----------
Output: 

        return (obj, self._get_type(obj))

    def _get_type(self, obj):
        """Get the type of an object."""
        return self._types[self._get_type_name(obj)]

    def _get_type_name(self, obj):
        """Get the type name of an object."""
        return self._types_names[self._get_type_name_name(obj)]

    def _get_type_name_name(self, obj):
        """Get the name of an object."""
        return self._types_names_names[obj]
----------
Solution: 

        tup_params = tup.__args__
        has_ellipsis = tup_params and tup_params[-1] is Ellipsis
        if tup_params is None or (has_ellipsis and tup_params[0] is Any):
            # Just a Tuple. (No generic information.)
            return tuple(obj)
        if has_ellipsis:
            # We're dealing with a homogenous tuple, Tuple[int, ...]
            tup_type = tup_params[0]
            conv = self._structure_func.dispatch(tup_type)
            return tuple(conv(e, tup_type) for e in obj)
        else:
            # We're dealing with a heterogenous tuple.
            return tuple(
                self._structure_func.dispatch(t)(e, t)
                for t, e in zip(tup_params, obj)
            )
--------------------
Prompt: 
def extract_ar (archive, compression, cmd, verbosity, interactive, outdir):
    """Extract a AR archive."""
----------
Output: 

    cmd = cmd.replace('ar', 'ar_').replace('t', 't_').replace('o', 'o_').replace('a', 'a_').replace('t', 't_').replace('s','s_').replace('t', 't_').replace('o', 'o_').replace('a', 'a_').replace('t', 't_').replace('s','s_').replace('t', 't_').replace('o', 'o_').replace('a', 'a_').replace('t', 't_').replace('s
----------
Solution: 

    opts = 'x'
    if verbosity > 1:
        opts += 'v'
    cmdlist = [cmd, opts, os.path.abspath(archive)]
    return (cmdlist, {'cwd': outdir})
--------------------
Prompt: 
def save_book(self, f_buf=None):
        """
        Return a file buffer containing the resulting xls

        :param obj f_buf: A file buffer supporting the write and seek
        methods
        """
----------
Output: 

        if f_buf is None:
            f_buf = BytesIO()
        if self.book_format =='mobi':
            self.book_xliff = f_buf.write
        elif self.book_format == 'pdf':
            self.book_xliff = f_buf.write
        else:
            raise ValueError("Unknown format: %s" % self.book_format)
        return self.book_xliff

    def save_book_as(self, book_as, f_buf=None):
        """
        Return a file buffer containing the resulting xls

       
----------
Solution: 

        if f_buf is None:
            f_buf = StringIO.StringIO()
        f_buf.write(openpyxl.writer.excel.save_virtual_workbook(self.book))
        f_buf.seek(0)
        return f_buf
--------------------
Prompt: 
def from_axis_angle(axis, angle):
    """
    Returns a #Quaternion that represents the right-handed
    rotation of *angle* radians about the givne *axis*.

    :param axis: The unit vector representing the axis of rotation.
    :param angle: The angle of rotation, in radians.
    """
----------
Output: 

    return Quaternion(
        axis[0],
        axis[1],
        axis[2],
        angle[0],
        angle[1],
        angle[2],
        axis[3],
        axis[4],
        angle[3],
        axis[5],
        axis[6],
        axis[7],
        axis[8],
        axis[9],
        axis[10],
        axis[11],
        axis[12],
        axis[13],
        axis[14],
        axis[15],
        axis[16],
        axis[17],
        axis[18],
        angle[1],
       
----------
Solution: 


    sincomp = math.sin(angle / 2.0)
    return Quaternion(
      axis.x * sincomp, axis.y * sincomp,
      axis.z * sincomp, math.cos(angle / 2.0))
--------------------
Prompt: 
def get_arguments(options):
   """ This function handles and validates the wrapper arguments. """
----------
Output: 

   # Check if the wrapper arguments are valid
   if options.wrapper_args is None:
      raise ValueError("Must specify a wrapper arguments.")

   # Check if the wrapper arguments are valid
   if options.wrapper_args.find(" ") == -1:
      raise ValueError("Must specify a wrapper argument.")

   # Check if the wrapper is a valid file
   if options.wrapper_args.find(" ") == -1:
      raise ValueError("Must specify a wrapper file.")

   # Check if the wrapper is a valid directory
   if options.wrapper_args.find(" ") == -1:
      raise ValueError("Must
----------
Solution: 

   # These the next couple of lines defines the header of the Help output
   parser = ArgumentParser(
      formatter_class=RawDescriptionHelpFormatter,
      usage=(
--------------------
Prompt: 
def describe_keyspaces(self, ):
    """
    list the defined keyspaces in this cluster
    """
----------
Output: 

    print "Available Keyspaces:"
    keyspaces = []
    for node in self.nodes:
      keyspaces.append(node.keypaces)
    print "  " + str(len(keyspaces)) + " defined in this cluster:"
    for keyspace in keyspaces:
      print "    " + keyspace.name
    print "  " + str(len(self.keyspaces)) + " defined in this cluster:"
    for node in self.keyspaces:
      print "    " + node.name
    return node

  def get_all_keyspaces(self):
    """
    return the
----------
Solution: 

    self._seqid += 1
    d = self._reqs[self._seqid] = defer.Deferred()
    self.send_describe_keyspaces()
    return d
--------------------
Prompt: 
def process_raw_data(cls, raw_data):
        """Create a new model using raw API response."""
----------
Output: 

        return cls(
            id=raw_data.get('id'),
            name=raw_data.get('name'),
            description=raw_data.get('description'),
            type=raw_data.get('type'),
            unit_of_measure=raw_data.get('unit_of_measure'),
            icon=raw_data.get('icon'),
            device_state=raw_data.get('device_state'),
            unit_of_measurement=raw_data.get('unit_of_measurement'),
            unit_of_measure=raw_data.get('unit_of
----------
Solution: 

        properties = raw_data.get("properties", {})

        load_balancing_rules = []
        for raw_content in properties.get("loadBalancingRules", []):
            resource = Resource.from_raw_data(raw_content)
            load_balancing_rules.append(resource)
        properties["loadBalancingRules"] = load_balancing_rules

        return super(Probes, cls).process_raw_data(raw_data)
--------------------
Prompt: 
def handle(self, *args, **options):
        """Command handle."""
----------
Output: 

        if len(args)!= 1:
            raise CommandError("Please provide a single argument to %s" % self.get_app_label())

        self.stdout.write("Command finished. Press Ctrl-C to exit.")

----------
Solution: 

        verbosity = int(options['verbosity'])

        if self.has_filter(options):
            self.filter_indices(options, verbosity)
        else:
            # Process all indices.
            index_builder.build()
--------------------
Prompt: 
def disconnect(self):
        """Disconnect from the socket."""
----------
Output: 

        self.sock.close()

    def send(self, data):
        """Send data to the socket."""
        self.sock.sendall(data)

    def recv(self, bufsize):
        """Receive data from the socket and return the result."""
        data = ""
        while True:
            try:
                data += self.sock.recv(bufsize)
            except socket.error:
                break
        return data

    def __init__(self, host, port, username=None, password=None,
                 pkey=None, cert=None, timeout=5, use_ssl=False, ca_cert=None
----------
Solution: 

        if self.pipeline:
            self._send(*self.pipeline)
        self.pipeline = None
--------------------
Prompt: 
def base_type(self, type):
        """Return the base type of `type`."""
----------
Output: 

        if type.is_pyobject:
            return py_object_type
        elif type.is_ptr:
            return "((PyObject *)%s)" % type.base_type
        else:
            return type

    def analyse_types(self, env):
        """
        Analyse the types of the result of an expression for caching and
        examining its subexpressions.
        """
        self.arg_free_func = \
            self.arg_free_dict = weakref.ref(env.declare_tuple_type)
        self.arg_free_dict_val = weakref.ref(env.declare
----------
Solution: 

        while 1:
            if type.arg == "leafref":
                node = type.i_type_spec.i_target_node
            elif type.i_typedef is None:
                break
            else:
                node = type.i_typedef
            type = node.search_one("type")
        if type.arg == "decimal64":
            return [type.arg, int(type.search_one("fraction-digits").arg)]
        elif type.arg == "union":
            return [type.arg, [self.base_type(x) for x in type.i_type_spec.types]]
        else:
            return type.arg
--------------------
Prompt: 
def iterate_similarity_datasets(args):
    """Generator over all similarity evaluation datasets.

    Iterates over dataset names, keyword arguments for their creation and the
    created dataset.

    """
----------
Output: 

    # Load the data.
    data_path = args.data_path
    if not os.path.exists(data_path):
        print("No data found at: %s" % data_path)
        return

    # Load the datasets.
    for name in args.dataset_names:
        if not os.path.exists(name):
            print("No data found at: %s" % name)
            continue

        # Load the keyword arguments.
        if name == "n_components":
            n_components = args.n_components
        elif name == "n_features":
            n_
----------
Solution: 

    for dataset_name in args.similarity_datasets:
        parameters = nlp.data.list_datasets(dataset_name)
        for key_values in itertools.product(*parameters.values()):
            kwargs = dict(zip(parameters.keys(), key_values))
            yield dataset_name, kwargs, nlp.data.create(dataset_name, **kwargs)
--------------------
Prompt: 
def send_response(self, code, message=None, size='-'):
        """
        Send the response header and log the response code.

        Also send two standard headers with the server software
        version and the current date.
        """
----------
Output: 

        if message is None:
            message = self.response_header
        if size == '-1':
            self.response_size = None
        else:
            self.response_size = int(size)
        if code == '200':
            self.response_code = 200
        elif code == '400':
            self.response_code = 400
        elif code == '404':
            self.response_code = 404
        elif code == '500':
            self.response_code = 500
        elif code == '503':
            self.response_code = 503
        elif code == '504':
            self.
----------
Solution: 

        # pylint: disable-msg=W0221
        if self._to_log or LOG.isEnabledFor(logging.DEBUG):
            self.log_request(code, size)

        if message is None:
            if code in self.responses:
                message = self.responses[code][0]
            else:
                message = ''

        if self.request_version != 'HTTP/0.9':
            self.wfile.write("%s %d %s\r\n" %
                             ("HTTP/1.1", code, message))
        self.send_header('Server', self.version_string())
        self.send_header('Date', self.date_time_string())
--------------------
Prompt: 
def precision(precision, id_, hwid, type_):
    """Change the precision for the sensor and persist it in the sensor's EEPROM"""
----------
Output: 

    return _lib.TCOD_plot_get_precision(c_int(id_), precision, byref(hwid))

def set_fps(fps):
    """Set the frame rate for the display of the given FPS"""
    return _lib.TCOD_plot_set_fps(c_int(fps), c_int(id_))

def set_name(name):
    """Set the name for the plot"""
    return _lib.TCOD_plot_set_name(c_int(name))

def set_color(red, green, blue):
----------
Solution: 

    if id_ and (hwid or type_):
        raise click.BadOptionUsage(
            "If --id is given --hwid and --type are not allowed."
        )

    if id_:
        try:
            sensor = W1ThermSensor.get_available_sensors()[id_ - 1]
        except IndexError:
            raise click.BadOptionUsage(
                "No sensor with id {0} available. "
                "Use the ls command to show all available sensors.".format(id_)
            )
    else:
        sensor = W1ThermSensor(type_, hwid)

    sensor.set_precision(precision, persist=True)
--------------------
Prompt: 
def trips(self, val):
        """
        Update ``self._trips_i`` if ``self.trips`` changes.
        """
----------
Output: 

        if val is None:
            self._trips_i = None
        else:
            self._trips_i = val

    def _get_trips_i(self):
        """
        Return the current trips value.
        """
        return self._trips_i

    def _set_trips_i(self, val):
        """
        Set the current trips value.
        """
        if val is None:
            self._trips_i = None
        else:
            self._trips_i = val

    trips_i = property(_get_trips_i, _
----------
Solution: 

        self._trips = val
        if val is not None and not val.empty:
            self._trips_i = self._trips.set_index("trip_id")
        else:
            self._trips_i = None
--------------------
Prompt: 
def read(self):
        try:
            bytes = self.sock.recv(self.max_size)
        except:
            self.torrent.kill_peer(self)
            return
        """
        Chain of events:
            - process_input
            - check save_state and read length, id, and message accordingly
                - if we have a piece (really a block), we piece.save it out
                  inside call to ppiece
                    - If we've completed a piece we:
                        - Tell the switchboard to write it out
                        - init a new piece
        """
----------
Output: 

        self.save_state = self.sock.recv(self.save_size)
        self.save_length = self.sock.recv(self.save_size)
        self.read_state = self.sock.recv(self.read_size)
        self.id = self.sock.recv(self.id_size)
        self.message = self.sock.recv(self.message_size)
        """
        Chain of events:
            - process_input
            - check save_state and read length, id, and message accordingly
                - if we have a piece (really a
----------
Solution: 

        if len(bytes) == 0:
            print 'Got 0 bytes from fileno {}.'.format(self.fileno())
            self.torrent.kill_peer(self)
        self.process_input(bytes)
--------------------
Prompt: 
def get(self, timeout=None):
        """
        Waits for a job to finish (max of given timeout seconds) and return job results. When a job exits get() will
        keep returning the same result until zero-os doesn't remember the job anymore (self.exists == False)

        :notes: the timeout here is a client side timeout, it's different than the timeout given to the job on start
        (like in system method) witch will cause the job to be killed if it exceeded this timeout.

        :param timeout: max time to wait for the job to finish in seconds
        :return: Return object
        """
----------
Output: 
: the job results
        """
        if timeout is None:
            timeout = self.timeout
        if timeout > 0:
            end_time = time.time() + timeout
        else:
            end_time = None

        if self.exists:
            return self.get_result()
        else:
            return self.get_result() + end_time


class Job(object):
    """
    Represents a single job.
    """

    def __init__(self, command, name, timeout=None):
        """
        :param command: the command to run
        :param name: the name of the job
        :
----------
Solution: 

        if timeout is None:
            timeout = self._client.timeout
        r = self._client._redis
        start = time.time()
        maxwait = timeout
        while maxwait > 0:
            if not self.exists:
                raise JobNotFoundError(self.id)
            v = r.brpoplpush(self._queue, self._queue, min(maxwait, 10))
            if v is not None:
                payload = json.loads(v.decode())
                r = Return(payload)
                logger.debug('%s << %s, stdout="%s", stderr="%s", data="%s"',
                             self._id, r.state, r.stdout, r.stderr, r.data[:1000])
                return r
            logger.debug('%s still waiting (%ss)', self._id, int(time.time() - start))
            maxwait -= 10
        raise TimeoutError()
--------------------
Prompt: 
def _control_resp(self, data, state):
        """ Handle a control response.

        :param data: Payload.
        :param state: Requested state.
        :returns: Acknowledged state.
        """
----------
Output: 

        if data.get('status') == 'ok':
            self.log.info('Received control request: %s', data)
            state = self.get_acknowledgement(data)
            if state is None:
                return False
            else:
                self.log.debug('Got state: %s', state)
                return state
        else:
            self.log.error('Received control response: %s', data)
            return False

    def _get_state(self, data):
        """ Handle a state request.

        :param data: Payload.
        :returns: State.
        """
       
----------
Solution: 

        if _is_control_response(data):
            ack_state = bytes([data[22]])
            if state == ack_state:
                _LOGGER.debug("Received state ack from %s, state: %s",
                              self.host, ord(ack_state))
                return ack_state
--------------------
Prompt: 
def is_path_sibling_creatable(pathname: str) -> bool:
    """Checks whether current user can create siblings of the given path.

    Returns
    -------
    `True` if the current user has sufficient permissions to create siblings
    (i.e., arbitrary files in the parent directory) of the passed pathname;
    `False` otherwise.
    """
----------
Output: 

    try:
        st = os.stat(pathname)
    except OSError:
        return False
    if st.st_mode & 0o777!= 0o700:
        return False
    if st.st_mode & 0o777!= st.st_mode & 0o777:
        return False
    return True


def is_path_writable(path: str) -> bool:
    """Checks whether the given path can be written to.

    Parameters
    ----------
    path : str
        The path to check.

    Returns
    -------
    `True` if the given path can be written to, `False`
----------
Solution: 

    # Parent directory of the passed path. If empty, we substitute the current
    # working directory (CWD) instead.
    dirname = os.path.dirname(pathname) or os.getcwd()

    try:
        # For safety, explicitly close and hence delete this temporary file
        # immediately after creating it in the passed path's parent directory.
        with tempfile.TemporaryFile(dir=dirname):
            pass
        return True
    # While the exact type of exception raised by the above function depends on
    # the current version of the Python interpreter, all such types subclass
    # the following exception superclass.
    except EnvironmentError:
        return False
--------------------
Prompt: 
def unicode_value(self, string):
        """
        String argument must be in unicode format.
        """
----------
Output: 

        raise NotImplementedError()

    def get_prep_value(self, value):
        """
        Argument must be an object that is a valid, possibly Unicode string.
        """
        raise NotImplementedError()

    def formfield(self, **kwargs):
        """
        Set up the widget for rendering the widget.
        """
        defaults = {'widget': unicode_widget}
        defaults.update(kwargs)
        return super(MultiWidget, self).formfield(**defaults)

    def get_db_prep_lookup(self, lookup_type, value):
        """
        Returns a lookup type that can be used to look up a widget for
       
----------
Solution: 

        result = 0
        # don't accept strings that contain numbers
        if self.regex_has_numbers.search(string):
            raise AbnumException(error_msg % string)
        else:
            num_str = self.regex_values.sub(lambda x: '%s ' % self.values[x.group()],
                                            string)
            # don't accept strings, that contains letters which haven't been be converted to numbers
            try:
                result = sum([int(i) for i in num_str.split()])
            except Exception as e:
                raise AbnumException(error_msg % string)
        return result
--------------------
Prompt: 
def teardown_handler(teardown_fixtures_fn, teardown_fn):
        """Returns a function that adds fixtures handling to the teardown method.

        Calls the given teardown method first before calling the fixtures teardown.
        """
----------
Output: 

        def teardown_fixture(self, fixture):
            """A fixture which will be cleaned up automatically."""
            self.addCleanup(fixture.close)
            self.addCleanup(fixture.close)
            self.fixture = fixture

        def cleanup_fixtures_fn(self, fixture):
            """A fixture which will be cleaned up automatically."""
            self.addCleanup(fixture.close)
            self.addCleanup(fixture.close)
            self.fixture = fixture

        def _inner_teardown(self):
            """A fixture which will be cleaned up automatically."""
            self.addCleanup(fixture.close)
            self.addCleanup(fixture.
----------
Solution: 

        def handler(obj):
            teardown_fn(obj)
            teardown_fixtures_fn(obj)
        return handler
--------------------
Prompt: 
def iterscrapers(self, method, mode = None):
		""" Iterates over all available scrapers. """
----------
Output: 

		if mode is None:
			mode = self.scraper_mode
		for scraper in self.scrapers:
			if scraper.method == method:
				yield scraper
			elif mode == scraper_mode:
				yield scraper_func(scraper)

	def get_scraper_by_id(self, id):
		""" Returns a scraper object for the given scraper id. """
		for scraper in self.iterscrapers(id):
			if scraper.id == id:
				return scraper
		return None

	def get_scraper_by
----------
Solution: 


		global discovered
		if discovered.has_key(self.language) and discovered[self.language].has_key(method):
			for Scraper in discovered[self.language][method]:
				yield Scraper
--------------------
Prompt: 
def signal_list_names(type_):
    """Returns a list of signal names for the given type

    :param type\\_:
    :type type\\_: :obj:`GObject.GType`
    :returns: A list of signal names
    :rtype: :obj:`list`
    """
----------
Output: 

    return [GObject.SignalFlags.RUN_LAST,
            GObject.SignalFlags.RUN_LAST |
            GObject.SignalFlags.ACTION_RUN_LAST]


def signal_list_ids(type_):
    """Returns a list of signal ids for the given type

    :param type\\_:
    :type type\\_: :obj:`GObject.GType`
    :returns: A list of signal ids
    :rtype: :obj:`list`
    """
    return [GObject.SignalFlags.RUN_LAST |
            GObject.SignalFlags.RUN_LAST |
            GObject
----------
Solution: 


    ids = signal_list_ids(type_)
    return tuple(GObjectModule.signal_name(i) for i in ids)
--------------------
Prompt: 
def select(dataspec, testsuite, mode='list', cast=True):
    """
    Select data from [incr tsdb()] profiles.

    Args:
        query (str): TSQL select query (e.g., `'i-id i-input mrs'` or
            `'* from item where readings > 0'`)
        testsuite (str, TestSuite): testsuite or path to testsuite
            containing data to select
        mode (str): see :func:`delphin.itsdb.select_rows` for a
            description of the *mode* parameter (default: `list`)
        cast (bool): if `True`, cast column values to their datatype
            according to the relations file (default: `True`)
    Returns:
        a generator that yields selected data
    """
----------
Output: 

        data (int): number of rows to select.
            *default* is ``0``
        dataspec (str): path to the *data* database.
            *default* is ``/home/user/test_data.db``
        testsuite (str, str): name of the test suite.
            *default* is ``None``
        test_id (str): id of the test to select.
            *default* is ``None``
        mrs (str): path to the *MRS* database.
            *default* is ``None``

    Returns:
        int: error code (0:
----------
Solution: 

    if isinstance(testsuite, itsdb.ItsdbProfile):
        testsuite = itsdb.TestSuite(testsuite.root)
    elif not isinstance(testsuite, itsdb.TestSuite):
        testsuite = itsdb.TestSuite(testsuite)
    return tsql.select(dataspec, testsuite, mode=mode, cast=cast)
--------------------
Prompt: 
def suites(self, request, pk=None):
        """
        List of test suite names available in this project
        """
----------
Output: 

        if pk:
            return self.get_suite_names(request, pk)
        else:
            return self.get_suite_names(request)

    def get_suite_names(self, request):
        """
        List of test suite names available in this project
        """
        return [
            self.get_test_suite_name(request, test_suite)
            for test_suite in self.suites(request)
        ]

    def get_test_suite_name(self, request, test_suite):
        """
        Given a test suite name, return the test suite name
        """
       
----------
Solution: 

        suites_names = self.get_object().suites.values_list('slug')
        suites_metadata = SuiteMetadata.objects.filter(kind='suite', suite__in=suites_names)
        page = self.paginate_queryset(suites_metadata)
        serializer = SuiteMetadataSerializer(page, many=True, context={'request': request})
        return self.get_paginated_response(serializer.data)
--------------------
Prompt: 
def nation(self, nation_name, password=None, autologin=None):
        """Setup access to the Nation API with the Nation object

            :param nation_name: Name of the nation
            :param password: (Optional) password for this nation
            :param autologin (Optional) autologin for this nation
            :type nation_name: str
            :type password: str
            :type autologin: str
            :returns: Nation Object based off nation_name
            :rtype: Nation
        """
----------
Output: 

        return Nation(self, nation_name, autologin, password)

    def get_nation_by_name(self, name):
        """Get the Nation object by name

            :param name: (String) name of the nation
            :type name: str
            :returns: Nation Object
            :rtype: Nation
        """
        return self.nation(name, autologin=True)

    def get_nation_by_email(self, email):
        """Get the Nation object by email

            :param email: (String) email of the nation
            :
----------
Solution: 

        return Nation(nation_name, self, password=password, autologin=autologin)
--------------------
Prompt: 
def listFieldsFromSource(self, template_source):
        """List all the attributes to be rendered directly from template
        source

        :param template_source: the template source (usually represents the
            template content in string format)
        :return: a :class:`set` contains all the needed attributes
        :rtype: set
        """
----------
Output: 

        return set(self.attr_dict.keys())

    def get_field_names(self, template_source):
        """Return the list of attributes to be rendered directly from template
        source

        :param template_source: the template source (usually represents the
            template content in string format)
        :return: a list of attribute names
        :rtype: list
        """
        return list(self.attr_dict.keys())

    def get_field_value(self, template_name, field_name):
        """Return the value of a given attribute of a given type

        :param template_name:
----------
Solution: 


        ast = self.environment.parse(template_source)
        return jinja2.meta.find_undeclared_variables(ast)
--------------------
Prompt: 
def run(self, args=None):
        """ The name IS required here. """
----------
Output: 

        if args is None:
            args = []
        if not self.args:
            self.args = args
        if not self.kwargs:
            self.kwargs = {}
        if not self.vars:
            self.vars = {}
        if not self.file:
            self.file = None
        if not self.args:
            self.args = []
        if not self.kwargs:
            self.kwargs = {}
        if not self.vars:
            self.vars = {}
        if not self.file:
            self.file = self.args[0]
        if not self.file:
            self
----------
Solution: 

        print(Colors.BLUE+"[INFO] Unlinking {}...".format(
              args.names)+Colors.RESET)
        for name in args.names:
            self.unlink(Link.package_dir, name)
--------------------
Prompt: 
def empirical_SVD(stream_list, linear=True):
    """
    Depreciated. Use empirical_svd.
    """
----------
Output: 

    if linear:
        return empirical_SVD_linear(stream_list)
    else:
        return empirical_SVD_svd(stream_list)


def empirical_SVD_linear(stream_list, n_components=2):
    """
    Linear function for the empirical SVD.
    """
    n_components = n_components + 1
    if len(stream_list) < n_components:
        return np.array([])
    if len(stream_list) == 1:
        return np.array([1.])
    if len(stream_
----------
Solution: 

    warnings.warn('Depreciated, use empirical_svd instead.')
    return empirical_svd(stream_list=stream_list, linear=linear)
--------------------
Prompt: 
def _sanitize(cls, message):
        """
        Sanitize the given message,
        dealing with multiple arguments
        and/or string formatting.

        :param message: the log message to be sanitized
        :type  message: string or list of strings
        :rtype: string
        """
----------
Output: 

        if isinstance(message, list):
            message = " ".join(message)
        return message

    @classmethod
    def _get_log_level(cls, level):
        """
        Get the log level.

        :param level: the log level to be used
        :type  level: integer
        :rtype: integer
        """
        if level in (0, 1):
            return logging.ERROR
        elif level in (2, 3):
            return logging.WARNING
        elif level in (3, 4):
            return logging.INFO
        elif level in (5, 6):
            return logging.DEBUG
        elif level
----------
Solution: 

        if isinstance(message, list):
            if len(message) == 0:
                sanitized = u"Empty log message"
            elif len(message) == 1:
                sanitized = message[0]
            else:
                sanitized = message[0] % tuple(message[1:])
        else:
            sanitized = message
        if not gf.is_unicode(sanitized):
            raise TypeError("The given log message is not a Unicode string")
        return sanitized
--------------------
Prompt: 
def read_texture(filename, attrs=None):
    """Loads a ``vtkTexture`` from an image file."""
----------
Output: 

    if attrs is None:
        attrs = {}
    if filename.endswith('.obj'):
        obj = read_obj(filename)
    elif filename.endswith('.obj.type'):
        obj = read_obj(filename)
    elif filename.endswith('.obj.name'):
        obj = read_texture(filename, attrs)
    elif filename.endswith('.obj.name.ext'):
        obj = read_texture(filename, attrs)
    elif filename.endswith('.obj.name.ext'):
        obj = read_texture(filename, attrs)
    elif filename.endswith('.obj.name.ext'):
        obj =
----------
Solution: 

    filename = os.path.abspath(os.path.expanduser(filename))
    try:
        # intitialize the reader using the extnesion to find it
        reader = get_reader(filename)
        image = standard_reader_routine(reader, filename, attrs=attrs)
        return vtki.image_to_texture(image)
    except KeyError:
        # Otherwise, use the imageio reader
        pass
    return vtki.numpy_to_texture(imageio.imread(filename))
--------------------
Prompt: 
def run(self):
        """Start the FSM.

        Returns:
            boolean: True if FSM reaches the last state or false if the exception or error message was raised

        """
----------
Output: 

        try:
            self.fsm.poll()
        except KeyboardInterrupt:
            self.fsm.stop()
            return False
        except Exception as e:
            self.fsm.error(e)
            return False

        self.fsm.poll()
        return True

    def get_fsm_state(self):
        """Get the current FSM state.

        Returns:
            FSMState: Current FSM state

        """
        try:
            return self.fsm.get_fsm_state()
        except Exception as e:
            self.fsm.error(e)
            return FSMState.UNKNOWN

    def get_fsm_
----------
Solution: 

        ctx = FSM.Context(self.name, self.device)
        transition_counter = 0
        timeout = self.timeout
        self.log("{} Start".format(self.name))
        while transition_counter < self.max_transitions:
            transition_counter += 1
            try:
                start_time = time()
                if self.init_pattern is None:
                    ctx.event = self.ctrl.expect(self.events, searchwindowsize=self.searchwindowsize, timeout=timeout)
                else:
                    self.log("INIT_PATTERN={}".format(pattern_to_str(self.init_pattern)))
                    try:
                        ctx.event = self.events.index(self.init_pattern)
                    except ValueError:
                        self.log("INIT_PATTERN unknown.")
                        continue
                    finally:
                        self.init_pattern = None

                finish_time = time() - start_time
                key = (ctx.event, ctx.state)
                ctx.pattern = self.events[ctx.event]

                if key in self.transition_table:
                    transition = self.transition_table[key]
                    next_state, action_instance, next_timeout = transition
                    self.log("E={},S={},T={},RT={:.2f}".format(ctx.event, ctx.state, timeout, finish_time))
                    if callable(action_instance) and not isclass(action_instance):
                        if not action_instance(ctx):
                            self.log("Error: {}".format(ctx.msg))
                            return False
                    elif isinstance(action_instance, Exception):
                        self.log("A=Exception {}".format(action_instance))
                        raise action_instance
                    elif action_instance is None:
                        self.log("A=None")
                    else:
                        self.log("FSM Action is not callable: {}".format(str(action_instance)))
                        raise RuntimeWarning("FSM Action is not callable")

                    if next_timeout != 0:  # no change if set to 0
                        timeout = next_timeout
                    ctx.state = next_state
                    self.log("NS={},NT={}".format(next_state, timeout))

                else:
                    self.log("Unknown transition: EVENT={},STATE={}".format(ctx.event, ctx.state))
                    continue

            except EOF:
                raise ConnectionError("Session closed unexpectedly", self.ctrl.hostname)

            if ctx.finished or next_state == -1:
                self.log("{} Stop at E={},S={}".format(self.name, ctx.event, ctx.state))
                return True

        # check while else if even exists
        self.log("FSM looped. Exiting")
        return False
--------------------
Prompt: 
def _calldata_fields(fields, exclude_fields, format_ids):
    """Utility function to determine which calldata (i.e., FORMAT) fields to
    extract."""
----------
Output: 

    calldata = {}
    for field in fields:
        if field[0] in calldata:
            calldata[field[0]].append(field[1])
        else:
            calldata[field[0]] = [field[1]]
    return calldata


def _get_calldata_fields(calldata, format_ids):
    """Utility function to determine which calldata (i.e., FORMAT) fields to
    extract."""
    fields = []
    for call in calldata[call]:
        if call[0] in format_ids:
            fields.append((call[1
----------
Solution: 

    if fields is None:
        # no fields specified by user
        # default to all standard fields plus all FORMAT fields in VCF header
        fields = config.STANDARD_CALLDATA_FIELDS + format_ids
    else:
        # fields specified by user
        for f in fields:
            # check if field is standard or defined in VCF header
            if (f not in config.STANDARD_CALLDATA_FIELDS and
                    f not in format_ids):
                # support extracting FORMAT even if not declared in header,
                # but warn...
                print('WARNING: no definition found for field %s' % f,
                      file=sys.stderr)
    # process exclusions
    if exclude_fields is not None:
        fields = [f for f in fields if f not in exclude_fields]
    return tuple(fields)
--------------------
Prompt: 
def parent(self, resource):
        """Set parent resource

        :param resource: parent resource
        :type resource: Resource

        :raises ResourceNotFound: resource not found on the API
        """
----------
Output: 

        self._set_parent(resource, self.parent_id)

    def _set_parent(self, resource, parent_id):
        """Set parent resource

        :param resource: parent resource
        :type resource: Resource

        :raises ResourceNotFound: resource not found on the API
        """
        self.parent_id = parent_id
        self.resource_id = resource.id

    def get_parent(self):
        """Get parent resource

        :returns: parent resource
        :rtype: Resource

        :raises ResourceNotFoundError: resource not found on the API
        """
        try:
            return self.parent_
----------
Solution: 

        resource.check()
        self['parent_type'] = resource.type
        self['parent_uuid'] = resource.uuid
--------------------
Prompt: 
def make_command_table(entry_points):
    """
    Return a nicely formatted table of all the PIP commands installed on this 
    system to incorporate into the help text.  The table will have two columns.  
    The first will list the commands that comprise the main pipeline and the 
    second will list all the other miscellaneous helper functions.
    """
----------
Output: 

    # The following is a list of all the functions that are called by the pipeline.
    # These are called by the pipeline when the user clicks on the pipeline.
    # They are called by the helper functions when the pipeline is running.
    # They are called by the helper functions when the helper functions are called.
    #
    # The second will list the functions that are called by the miscellaneous pipeline.
    # These are called by the pipeline when the user clicks on the pipeline.
    #
    # The third will list the functions that are called by the pipeline.
    # These are called by the pipeline when
----------
Solution: 

    import itertools

    # Split every command installed on the system into two categories: those 
    # that are part of the main pipeline and those that are just utilities or 
    # helpers.  Pipeline scripts start with numbers, helper scripts don't.

    pipeline_commands = []
    helper_commands = []

    for command in sorted(entry_points):
        if re.match('\d+_', command):
            pipeline_commands.append(command)
        else:
            helper_commands.append(command)

    # Figure out what the longest pipeline command is, so we know how much to 
    # indent the helper commands.

    col_width = 0
    for command in pipeline_commands:
        col_width = max(len(command), col_width)

    # Make the table.

    rows = []
    columns = itertools.izip_longest(
            pipeline_commands, helper_commands, fillvalue='')

    for commands in columns:
        row = '        {0[0]:{1}}   {0[1]}'.format(commands, col_width)
        rows.append(row)

    return '\n'.join(rows)
--------------------
Prompt: 
def shell(self):
        """
        Spawn a debug shell.
        """
----------
Output: 

        self.debug_shell = DebugShell(self.config)
        self.debug_shell.start()

    def debug_shell_thread(self):
        """
        Spawn a debug thread.
        """
        self.debug_thread = DebugThread(self.config)
        self.debug_thread.start()

    def debug_shell_thread_thread(self):
        """
        Spawn a debug thread thread.
        """
        self.debug_thread_thread = DebugThreadThreadThread(self.config)
        self.debug_thread_thread.start()

    def debug_shell_thread_
----------
Solution: 

        click.echo(click.style("NOTICE!", fg="yellow", bold=True) + " This is a " + click.style("local", fg="green", bold=True) + " shell, inside a " + click.style("Zappa", bold=True) + " object!")
        self.zappa.shell()
        return
--------------------
Prompt: 
def overall_f_measure(self):
        """Overall f-measure metrics (f_measure, precision, and recall)

        Returns
        -------
        dict
            results in a dictionary format

        """
----------
Output: 

        if self.f_measure is None:
            return {}
        return self.f_measure

    def __repr__(self):
        return self.name

    def __str__(self):
        return self.name

    def __init__(self, name, score, description, score_mode,
                 score_weighting, n_features, n_classes, n_clusters,
                 init, verbose):
        self.name = name
        self.score = score
        self.description = description
        self.score_mode = score_mode
        self.score_weighting = score_weighting
        self.init
----------
Solution: 

        if self.overall['Nsys'] == 0 and self.empty_system_output_handling == 'zero_score':
            precision = 0

        else:
            precision = metric.precision(
                Ntp=self.overall['Ntp'],
                Nsys=self.overall['Nsys']
            )

        recall = metric.recall(
            Ntp=self.overall['Ntp'],
            Nref=self.overall['Nref']
        )

        f_measure = metric.f_measure(
            precision=precision,
            recall=recall
        )

        return {
            'f_measure': f_measure,
            'precision': precision,
            'recall': recall
        }
--------------------
Prompt: 
def readehf(filename):
    """Read EDF header (ESRF data format, as of beamline ID01 and ID02)

    Input
    -----
    filename: string
        the file name to load

    Output
    ------
    the EDF header structure in a dictionary
    """
----------
Output: 

    with open(filename, 'rb') as f:
        return pickle.load(f)


def readelf(filename):
    """Read EDF footer (LFH data format, as of beamline ID01 and ID02)

    Input
    -----
    filename: string
        the file name to load

    Output
    ------
    the dictionary in a dictionary
    """
    with open(filename, 'rb') as f:
        return pickle.load(f)


def readehf(filename):
    """Read EDF header (LFH data format, as of beamline ID01 and ID02)
----------
Solution: 

    f = open(filename, 'r')
    edf = {}
    if not f.readline().strip().startswith('{'):
        raise ValueError('Invalid file format.')
    for l in f:
        l = l.strip()
        if not l:
            continue
        if l.endswith('}'):
            break  # last line of header
        try:
            left, right = l.split('=', 1)
        except ValueError:
            raise ValueError('Invalid line: ' + l)
        left = left.strip()
        right = right.strip()
        if not right.endswith(';'):
            raise ValueError(
                'Invalid line (does not end with a semicolon): ' + l)
        right = right[:-1].strip()
        m = re.match('^(?P<left>.*)~(?P<continuation>\d+)$', left)
        if m is not None:
            edf[m.group('left')] = edf[m.group('left')] + right
        else:
            edf[left] = _readedf_extractline(left, right)
    f.close()
    edf['FileName'] = filename
    edf['__Origin__'] = 'EDF ID02'
    edf['__particle__'] = 'photon'
    return edf
--------------------
Prompt: 
def unregister_namespace(self, namespace):
        """
        Unregister a namespace.

        :param namespace: Namespace tag.
        :type namespace: str
        """
----------
Output: 

        self.xml.remove(namespace)

    def get_namespace_name(self):
        """
        Get the name of the namespace.

        :return: Name of the namespace.
        :rtype: str
        """
        return self.xml.tag

    def get_namespace_uri(self):
        """
        Get the URI of the namespace.

        :return: URI of the namespace.
        :rtype: str
        """
        return self.xml.get('uri')

    def get_namespace_version(self):
        """
        Get the version of the namespace.

        :return: Version of the namespace.
       
----------
Solution: 

        if namespace not in self._namespaces:
            raise NoRegisteredError("Namespace '{0}' is not registered on loader.".format(namespace))

        del self._namespaces[namespace]
--------------------
Prompt: 
def import_field(self, field, obj, data, is_m2m=False):
        """
        Calls :meth:`import_export.fields.Field.save` if ``Field.attribute``
        and ``Field.column_name`` are found in ``data``.
        """
----------
Output: 

        return self.field_export(obj, field, data, is_m2m)

    def field_export(self, obj, field, data, is_m2m=False):
        """
        Exports a single field's data to a separate table.
        """
        raise NotImplementedError('subclasses of BaseExportBackend must provide a _field_export() method')

    def get_field_names(self, model):
        """
        Returns a list of all field names in the given model, including
        the model name and the serialization format, if any.
        """
        raise NotImplementedError('subclasses of BaseExport
----------
Solution: 

        if field.attribute and field.column_name in data:
            field.save(obj, data, is_m2m)
--------------------
Prompt: 
def child_moved(self, child):
        """ Handle the child moved event from the declaration.

        """
----------
Output: 

        if self.is_move_event(child):
            self.is_move_event(child)
            self.emit("move-event", child)
        else:
            self.emit("move-event", child)

    def child_removed(self, child):
        """ Handle the child removed event from the declaration.

        """
        if self.is_move_event(child):
            self.is_move_event(child)
            self.emit("move-event", child)
        else:
            self.emit("move-event", child)

    def get_child_cursor(self
----------
Solution: 

        super(AndroidViewGroup, self).child_moved(child)
        #: Remove and re-add in correct spot
        self.child_removed(child)
        self.child_added(child)
--------------------
Prompt: 
def base36(value):
    """
    Encode int to base 36.
    """
----------
Output: 

    return value if value is not None and value!= "" else "0"


def base36_to_int(value):
    """
    Encode base 36 to int.
    """
    return int(value) if value is not None and int(value)!= "" else 0


def base36_to_int_list(value):
    """
    Converts a list of integers into a list of integers.
    """
    return [int(x) for x in value.split()]


def base36_to_int_dict(value):
    """
    Converts a dict into a list of integers
----------
Solution: 

    result = ''
    while value:
        value, i = divmod(value, 36)
        result = BASE36_ALPHABET[i] + result
    return result
--------------------
Prompt: 
def translate(self, addr):
        """
        Reverse DNS the public broadcast_address, then lookup that hostname to get the AWS-resolved IP, which
        will point to the private IP address within the same datacenter.
        """
----------
Output: 

        if not self.public_subnet_ids:
            return None
        addr = addr.split("/")[0]
        for subnet_id in self.public_subnet_ids:
            if subnet_id == addr:
                return subnet_id
        return None

    def _get_subnet_ids(self):
        """
        Return the list of subnets.
        """
        return self.public_subnets.keys()

    def _get_subnet_name(self, subnet_id):
        """
        Return the name of the subnet.
        """
        return self.public_subnets[subnet_id].name

    def
----------
Solution: 

        # get family of this address so we translate to the same
        family = socket.getaddrinfo(addr, 0, socket.AF_UNSPEC, socket.SOCK_STREAM)[0][0]
        host = socket.getfqdn(addr)
        for a in socket.getaddrinfo(host, 0, family, socket.SOCK_STREAM):
            try:
                return a[4][0]
            except Exception:
                pass
        return addr
--------------------
Prompt: 
def native_obj(self):
        """Native storage object."""
----------
Output: 

        return self.get_object(self.name)

    def get_object(self, name):
        """Get a StorageObject instance."""
        if name == self.name:
            return self.native_obj()
        if name == 'all':
            return self.get_all_storage_objects()
        if name == 'all_files':
            return self.get_all_files_storage_objects()
        if name == 'all_folders':
            return self.get_all_folders_storage_objects()
        if name == 'all_subfolders':
            return self.get_all
----------
Solution: 

        if self.__native is None:
            self.__native = self._get_object()

        return self.__native
--------------------
Prompt: 
def _get_description(string):
    """
    Parse annotation to get nice description
    """
----------
Output: 

    if string.startswith('http://'):
        return string
    else:
        return 'Unknown'


def _get_annotation_url(url):
    """
    Return the URL of the annotation
    """
    return url.replace('http://', 'https://')


def _get_annotation_id(url):
    """
    Return the ID of the annotation
    """
    return int(re.search(r'annotation_id=(\d+)', url).group(1))


def _get_annotation_type(string):
    """
    Parse annotation to get type
    """
    if string.
----------
Solution: 

    ann = set()
    if not string:
        return "This cluster is inter-genic."
    for item in string:
        for db in item:
            ann = ann.union(set(item[db]))
    return "annotated as: %s ..." % ",".join(list(ann)[:3])
--------------------
Prompt: 
def _choose_unit(value, unit=None, asciimode=None):
    """
    Finds a good unit to print seconds in

    Args:
        value (float): measured value in seconds
        unit (str): if specified, overrides heuristic decision
        asciimode (bool): if True, forces ascii for microseconds

    Returns:
        tuple[(str, float)]: suffix, mag:
            string suffix and conversion factor

    Example:
        >>> assert _choose_unit(1.1, unit=None)[0] == 's'
        >>> assert _choose_unit(1e-2, unit=None)[0] == 'ms'
        >>> assert _choose_unit(1e-4, unit=None, asciimode=True)[0] == 'us'
        >>> assert _choose_unit(1.1, unit='ns')[0] == 'ns'
    """
----------
Output: 
 =='s'
        >>> assert _choose_unit(1e-3, unit=None)[0] =='s'
        >>> assert _choose_unit(1e-6, unit=None)[0] =='s'
        >>> assert _choose_unit(1e-9, unit=None)[0] =='s'
        >>> assert _choose_unit(1e-12, unit=None)[0] =='s'
        >>> assert _choose_unit(1e-15, unit=None)[0] =='s'
        >>> assert _choose_unit(1
----------
Solution: 

    from collections import OrderedDict
    micro = _trychar('µs', 'us', asciimode)
    units = OrderedDict([
        ('s', ('s', 1e0)),
        ('ms', ('ms', 1e-3)),
        ('us', (micro, 1e-6)),
        ('ns', ('ns', 1e-9)),
    ])
    if unit is None:
        for suffix, mag in units.values():  # pragma: nobranch
            if value > mag:
                break
    else:
        suffix, mag = units[unit]
    return suffix, mag
--------------------
Prompt: 
def MakeDeployableBinary(self, template_path, output_path):
    """This will add the config to the client template and create a .deb."""
----------
Output: 

    with open(template_path, 'r') as template_file:
      template = template_file.read()
    self.client_template.append(template)
    self.client_template.append(
        'deb %s %s' % (self.options.build_dir, output_path))

  def MakeDeployableSources(self, options):
    """This will add the config to the client template and create a.cpio."""
    sources = []
    for source in options.sources:
      if source.endswith('.cpio'):
        sources.append(source)
    self.Make
----------
Solution: 

    buildpackage_binary = "/usr/bin/dpkg-buildpackage"
    if not os.path.exists(buildpackage_binary):
      logging.error("dpkg-buildpackage not found, unable to repack client.")
      return

    with utils.TempDirectory() as tmp_dir:
      template_dir = os.path.join(tmp_dir, "dist")
      utils.EnsureDirExists(template_dir)

      zf = zipfile.ZipFile(template_path)
      for name in zf.namelist():
        dirname = os.path.dirname(name)
        utils.EnsureDirExists(os.path.join(template_dir, dirname))
        with open(os.path.join(template_dir, name), "wb") as fd:
          fd.write(zf.read(name))

      # Generate the dpkg files.
      self.GenerateDPKGFiles(tmp_dir)

      # Create a client config.
      client_context = ["Client Context"] + self.context
      client_config_content = self.GetClientConfig(client_context)

      # We need to strip leading /'s or .join will ignore everything that comes
      # before it.
      target_dir = config.CONFIG.Get(
          "ClientBuilder.target_dir", context=self.context).lstrip("/")
      agent_dir = os.path.join(
          template_dir, "debian",
          config.CONFIG.Get("ClientBuilder.package_name", context=self.context),
          target_dir)

      with open(
          os.path.join(
              agent_dir,
              config.CONFIG.Get(
                  "ClientBuilder.config_filename", context=self.context)),
          "wb") as fd:
        fd.write(client_config_content)

      # Set the daemon to executable.
      os.chmod(
          os.path.join(
              agent_dir,
              config.CONFIG.Get("Client.binary_name", context=self.context)),
          0o755)

      arch = config.CONFIG.Get("Template.arch", context=self.context)

      try:
        old_working_dir = os.getcwd()
      except OSError:
        old_working_dir = os.environ.get("HOME", "/tmp")

      try:
        os.chdir(template_dir)
        command = [buildpackage_binary, "-uc", "-d", "-b", "-a%s" % arch]

        try:
          subprocess.check_output(command, stderr=subprocess.STDOUT)
        except subprocess.CalledProcessError as e:
          if "Failed to sign" not in e.output:
            logging.error("Error calling %s.", command)
            logging.error(e.output)
            raise

        filename_base = config.CONFIG.Get(
            "ClientBuilder.debian_package_base", context=self.context)
        output_base = config.CONFIG.Get(
            "ClientRepacker.output_basename", context=self.context)
      finally:
        try:
          os.chdir(old_working_dir)
        except OSError:
          pass

      utils.EnsureDirExists(os.path.dirname(output_path))

      for extension in [
          ".changes",
          config.CONFIG.Get(
              "ClientBuilder.output_extension", context=self.context)
      ]:
        input_name = "%s%s" % (filename_base, extension)
        output_name = "%s%s" % (output_base, extension)

        # TODO(user):pytype: incorrect move() definition in typeshed.
        # pytype: disable=wrong-arg-types
        shutil.move(
            os.path.join(tmp_dir, input_name),
            os.path.join(os.path.dirname(output_path), output_name))
        # pytype: enable=wrong-arg-types

      logging.info("Created package %s", output_path)
      return output_path
--------------------
Prompt: 
def set_pixel(framebuf, x, y, color):
        """Set a given pixel to a color."""
----------
Output: 

        framebuf.set_pixel(x, y, color)
        return self

    def get_pixel(self, x, y):
        """Get a given pixel."""
        return self.framebuffer.get_pixel(x, y)

    def set_pixel_color(self, color):
        """Set a given pixel to a given color."""
        self.framebuffer.set_pixel(x, y, color)
        return self

    def get_pixel_color(self, frame, x, y):
        """Get a given pixel."""
        return frame.get_pixel(x, y)
----------
Solution: 

        index = (y >> 3) * framebuf.stride + x
        offset = y & 0x07
        framebuf.buf[index] = (framebuf.buf[index] & ~(0x01 << offset)) | ((color != 0) << offset)
--------------------
Prompt: 
def bin_data(self, fmt=None):
        """
        Returns the unscaled raw values in the `value<N>` attributes as raw byte
        array. Use `bin_data_format`, `num_values` and the individual sensor
        documentation to determine how to interpret the data.

        Use `fmt` to unpack the raw bytes into a struct.

        Example::

            >>> from ev3dev2.sensor.lego import InfraredSensor
            >>> ir = InfraredSensor()
            >>> ir.value()
            28
            >>> ir.bin_data('<b')
            (28,)
        """
----------
Output: 

        if fmt is None:
            return self.raw_values

        if fmt == 'bin':
            return self.bin_data_format(num_values=self.num_values)

        if fmt == 'data':
            return self.data_format(num_values=self.num_values)

        return self.raw_values

    def __str__(self):
        """
        Returns the string representation of the sensor.

        Example::

            >>> ir = InfraredSensor()
            >>> print(bin_data(ir.value()))
            b'0'
            b'1'
            b'2
----------
Solution: 


        if self._bin_data_size == None:
            self._bin_data_size = {
                    "u8":     1,
                    "s8":     1,
                    "u16":    2,
                    "s16":    2,
                    "s16_be": 2,
                    "s32":    4,
                    "float":  4
                }.get(self.bin_data_format, 1) * self.num_values

        if None == self._bin_data:
            self._bin_data = self._attribute_file_open( 'bin_data' )

        self._bin_data.seek(0)
        raw = bytearray(self._bin_data.read(self._bin_data_size))

        if fmt is None: return raw

        return unpack(fmt, raw)
--------------------
Prompt: 
def get_simple_dirs(self, simple_dir: Path) -> List[Path]:
        """Return a list of simple index directories that should be searched
        for package indexes when compiling the main index page."""
----------
Output: 

        return [
            Path(
                os.path.join(simple_dir, "packages", "lib", "python2.7", "site-packages"),
                [
                    Path(
                        os.path.join(
                            simple_dir,
                            "lib",
                            "python2.7",
                            "site-packages",
                            "simple_{}".format(simple_dir.name),
                            "index.html",
                        ),
                        Path(
                            os.path.join(
                                simple_dir,
                                "packages",
                                "lib",
                                "python2.7",
                                "index_{}".format(simple_
----------
Solution: 

        if self.hash_index:
            # We are using index page directory hashing, so the directory
            # format is /simple/f/foo/.  We want to return a list of dirs
            # like "simple/f".
            subdirs = [simple_dir / x for x in simple_dir.iterdir() if x.is_dir()]
        else:
            # This is the traditional layout of /simple/foo/.  We should
            # return a single directory, "simple".
            subdirs = [simple_dir]
        return subdirs
--------------------
Prompt: 
def nuclear_norm(data):
    r"""Nuclear norm

    This method computes the nuclear (or trace) norm of the input data.

    Parameters
    ----------
    data : np.ndarray
        Input data array

    Returns
    -------
    float nuclear norm value

    Examples
    --------
    >>> from modopt.math.matrix import nuclear_norm
    >>> a = np.arange(9).reshape(3, 3)
    >>> nuclear_norm(a)
    15.49193338482967

    Notes
    -----
    Implements the following equation:

    .. math::
        \|\mathbf{A}\|_* = \sum_{i=1}^{\min\{m,n\}} \sigma_i (\mathbf{A})

    """
----------
Output: 
 \frac{1}{2} \sum_{i=1}^{n} \frac{1}{2} \sum_{j=1}^{n} \frac{1}{2} \sum_{k=1}^{n} \frac{1}{2} \sum_{l=1}^{n} \frac{1}{2} \sum_{m=1}^{n}^{n} \frac{1}{2} \sum_{n=1}^{n} \frac{1}{2} \sum_{n=1}^{n}^{n} \frac{
----------
Solution: 


    # Get SVD of the data.
    u, s, v = np.linalg.svd(data)

    # Return nuclear norm.
    return np.sum(s)
--------------------
Prompt: 
def admin_tools_render_dashboard(context, location='index', dashboard=None):
    """
    Template tag that renders the dashboard, it takes two optional arguments:

    ``location``
        The location of the dashboard, it can be 'index' (for the admin index
        dashboard) or 'app_index' (for the app index dashboard), the default
        value is 'index'.

    ``dashboard``
        An instance of ``Dashboard``, if not given, the dashboard is retrieved
        with the ``get_index_dashboard`` or ``get_app_index_dashboard``
        functions, depending on the ``location`` argument.
    """
----------
Output: 
location`` parameter.

    """
    if dashboard is None:
        dashboard = get_default_dashboard(context)
    if location is None:
        location = get_default_location(context)
    if not dashboard.location:
        raise ValueError('The dashboard location is required.')
    if not location:
        raise ValueError('The dashboard location is required.')
    if not dashboard.dashboard:
        raise ValueError('The dashboard is retrieved with the default location.')
    if not location:
        raise ValueError('The dashboard location is required.')
    if not dashboard.location:
        raise ValueError('The dashboard is retrieved with the default location.')
   
----------
Solution: 

    if dashboard is None:
        dashboard = get_dashboard(context, location)

    dashboard.init_with_context(context)
    dashboard._prepare_children()

    try:
        preferences = DashboardPreferences.objects.get(
            user=context['request'].user,
            dashboard_id=dashboard.get_id()
        ).data
    except DashboardPreferences.DoesNotExist:
        preferences = '{}'
        try:
            DashboardPreferences(
                user=context['request'].user,
                dashboard_id=dashboard.get_id(),
                data=preferences
            ).save()
        except IntegrityError:
            # dashboard already was saved for that (user, dashboard)
            pass

    context.update({
        'template': dashboard.template,
        'dashboard': dashboard,
        'dashboard_preferences': preferences,
        'split_at': math.ceil(
            float(len(dashboard.children)) / float(dashboard.columns)
        ),
        'has_disabled_modules': len(
            [m for m in dashboard.children if not m.enabled]
        ) > 0,
        'admin_url': reverse('%s:index' % get_admin_site_name(context)),
    })
    return context
--------------------
Prompt: 
def makeFigFromFile(filename,*args,**kwargs):
    """
    Renders an image in a matplotlib figure, so it can be added to reports 
    args and kwargs are passed to plt.subplots
    """
----------
Output: 

    if not filename.endswith('.png'):
        filename += '.png'
    return plt.savefig(filename, *args,**kwargs)

def makeFig(image,filename,*args,**kwargs):
    """
    Renders an image in a matplotlib figure, so it can be added to reports 
    args and kwargs are passed to plt.subplots
    """
    if not image.endswith('.png'):
        image += '.png'
    return plt.savefig(filename, *args,**kwargs)

def makeFigFromArray(array,filename,*args,**kwargs):
    """
    Renders an image in a
----------
Solution: 

    import matplotlib.pyplot as plt
    img = plt.imread(filename)
    fig,ax = plt.subplots(*args,**kwargs)
    ax.axis('off')
    ax.imshow(img)
    return fig
--------------------
Prompt: 
def get_conf_files(filename):
    """
    Return :class:`list` of the all configuration files. *filename* is a
    path of the main configuration file.

    ::

        >>> get_conf_files('exampleapp.conf')
        ['exampleapp.conf', 'exampleapp.conf.d/10-database.conf']
    """
----------
Output: 

    if not os.path.isfile(filename):
        return []
    else:
        return [filename] + sorted(os.listdir(os.path.join(
            os.path.dirname(os.path.abspath(__file__)),
            filename))


def get_db_files(filename):
    """
    Return all configuration files for a given database. *filename* is a
    path of the main database file.

    ::

        >>> get_db_files('exampleapp.db')
        ['exampleapp.db']
    """
    if not os.path.isfile(filename):
        return []
   
----------
Solution: 

    if not os.path.isfile(filename):
        raise ValueError("'%s' is not a file" % filename)
    conf_d_path = "%s.d" % filename
    if not os.path.exists(conf_d_path):
        return [filename]
    return [filename] + get_conf_d_files(conf_d_path)
--------------------
Prompt: 
def get_with_range(self, name, start, end):
        r"""Get a color table from the registry with a corresponding norm.

        Builds a `matplotlib.colors.BoundaryNorm` using `start`, `end`, and
        the number of colors, based on the color table obtained from `name`.

        Parameters
        ----------
        name : str
            The name under which the color table will be stored
        start : float
            The starting boundary
        end : float
            The ending boundary

        Returns
        -------
        `matplotlib.colors.BoundaryNorm`, `matplotlib.colors.ListedColormap`
            The boundary norm based on `start` and `end` with the number of colors
            from the number of entries matching the color table, and the color table itself.

        """
----------
Output: 
 and the number of colors.

        Examples
        --------
        >>> from matplotlib.colors import LogNorm
        >>> log_norm = LogNorm()
        >>> log_norm.set_under(0.5)
        >>> log_norm.set_over(1)
        >>> log_norm.set_under(0.5)
        >>> log_norm.get_with_range('LogNorm', 0.5, log_norm.vmin, 0.5)
        (0.5, 0.5)
        """
        return self.get_color_table(name, start, end)

    def get
----------
Solution: 

        from numpy import linspace

        # Need one more boundary than color
        num_steps = len(self[name]) + 1
        boundaries = linspace(start, end, num_steps)
        return self.get_with_boundaries(name, boundaries)
--------------------
Prompt: 
def delete(self, *, auto_commit=False):
        """Removes a resource from the database

        Args:
            auto_commit (bool): Automatically commit the transaction. Default: `False`

        Returns:
            `None`
        """
----------
Output: 

        self.db_session.delete(self.resource_id)
        if auto_commit:
            self.db_session.commit()

    def get_resource_by_id(self, resource_id):
        """Gets a resource from the database

        Args:
            resource_id (str): Resource id

        Returns:
            `Resource`
        """
        return self.db_session.query(Resource).get(resource_id)

    def get_resource_by_name(self, name):
        """Gets a resource from the database

        Args:
            name (str): Resource name

        Returns:
----------
Solution: 

        try:
            db.session.delete(self.resource)
            if auto_commit:
                db.session.commit()
        except SQLAlchemyError:
            self.log.exception('Failed deleting resource: {}'.format(self.id))
            db.session.rollback()
--------------------
Prompt: 
def deploy_file(file_path, bucket):
    """ Uploads a file to an S3 bucket, as a public file. """
----------
Output: 

    try:
        import boto
        from boto.s3.connection import S3Connection
        from boto.s3.key import Key
        from boto.s3.bucket import Bucket
        from boto.s3.connection import OrdinaryCallingFormat
        from boto.s3.connection import S3Connection
    except ImportError:
        raise AnsibleError("A non-Ansible inventory script or set of tools/metrics must be installed")

    # Set defaults
    inventory_options = dict(type='memory',
                           name=None,
                           ram=None,
                           disk=None,
                           cores=None,
                           ram=None,
----------
Solution: 


    # Paths look like:
    #  index.html
    #  css/bootstrap.min.css

    logger.info("Deploying {0}".format(file_path))

    # Upload the actual file to file_path
    k = Key(bucket)
    k.key = file_path
    try:
        k.set_contents_from_filename(file_path)
        k.set_acl('public-read')
    except socket.error:
        logger.warning("Caught socket.error while trying to upload {0}".format(
            file_path))
        msg = "Please file an issue with alotofeffort if you see this,"
        logger.warning(msg)
        logger.warning("providing as much info as you can.")
--------------------
Prompt: 
def examples(manager: Manager):
    """Load examples to the database."""
----------
Output: 

    return [
        Example(
            name=name,
            description=description,
            created_at=created_at,
            updated_at=updated_at,
            created_by_id=creator_id,
            updated_by_id=creator_id,
            created_at=created_at,
            updated_at=updated_at,
            updated_by_id=creator_id,
            updated_by_name=creator_name,
            updated_by_name=creator_name,
            updated_by_id=creator_id,
            updated_at=created_at,
           
----------
Solution: 

    for graph in (sialic_acid_graph, statin_graph, homology_graph, braf_graph, egf_graph):
        if manager.has_name_version(graph.name, graph.version):
            click.echo('already inserted {}'.format(graph))
            continue
        click.echo('inserting {}'.format(graph))
        manager.insert_graph(graph, use_tqdm=True)
--------------------
Prompt: 
def InsertFloatArg(self, string='', **unused_kwargs):
    """Inserts a Float argument."""
----------
Output: 

    self.AddString(string)

  def InsertIntArg(self, string='', **unused_kwargs):
    """Inserts an Integer argument."""
    self.AddString(string)

  def InsertStringArg(self, string='', **unused_kwargs):
    """Inserts a string argument."""
    self.AddString(string)

  def InsertFloat(self, string='', **unused_kwargs):
    """Inserts a float argument."""
    self.AddFloatArg(string, **unused_kwargs)

  def InsertBoolArg(self, string='', **unused_kwargs):
    """Inserts a Boolean argument
----------
Solution: 

    try:
      float_value = float(string)
    except (TypeError, ValueError):
      raise errors.ParseError('{0:s} is not a valid float.'.format(string))
    return self.InsertArg(float_value)
--------------------
Prompt: 
async def get_trans_flags(self) -> 'Flags':
        """
        Gives a chance to middlewares to make the translation flags
        """
----------
Output: 

        return await self.bot.db.get_guild_settings(self.guild_id).get_trans_flags()

    async def get_server_settings(self) -> 'ServerSettings':
        """
        Gives a chance to middlewares to make the server settings
        """
        return await self.bot.db.get_guild_settings(self.guild_id).get_server_settings()

    async def get_settings(self) -> 'Settings':
        """
        Gives a chance to get the server settings
        """
        return await self.bot.db.get_guild_
----------
Solution: 


        from bernard.middleware import MiddlewareManager

        async def make_flags(request: Request) -> 'Flags':
            return {}

        mf = MiddlewareManager.instance().get('make_trans_flags', make_flags)
        return await mf(self)
--------------------
Prompt: 
def _fusion_range_to_dsl(tokens) -> FusionRangeBase:
    """Convert a PyParsing data dictionary into a PyBEL.

    :type tokens: ParseResult
    """
----------
Output: 

    if not tokens:
        return None

    if isinstance(tokens, ParseResult):
        return FusionRangeBase(tokens.tokens, tokens.start, tokens.end)

    if isinstance(tokens, FusionRangeBase):
        return tokens

    raise ValueError("Expected a dictionary of tokens, got %s" % type(tokens))


def _fusion_range_to_dsl_with_tokens(tokens) -> FusionRangeBase:
    """Convert a PyParsing data dictionary into a PyBEL.

    :type tokens: FusionRangeResult
    """
    if not tokens:
        return None
----------
Solution: 

    if FUSION_MISSING in tokens:
        return missing_fusion_range()

    return fusion_range(
        reference=tokens[FUSION_REFERENCE],
        start=tokens[FUSION_START],
        stop=tokens[FUSION_STOP]
    )
--------------------
Prompt: 
def log_start(self, start):
        """log all the headers and stuff at the start of the request"""
----------
Output: 

        if self.debug:
            print("start: %s" % start)
        self.request.log_start(start)
        self.request.log_continue()

    def log_continue(self):
        """log the request headers and stuff at the continue point"""
        if self.debug:
            print("continue: %s" % self.request.log_continue())
        self.request.log_continue()

    def log_stop(self):
        """log the request headers and stuff at the stop of the request"""
        if self.debug:
            print("stop: %s" %
----------
Solution: 

        if not logger.isEnabledFor(logging.INFO): return

        try:
            req = self.request

            logger.info("REQUEST {} {}?{}".format(req.method, req.path, req.query))
            logger.info(datetime.datetime.strftime(datetime.datetime.utcnow(), "DATE %Y-%m-%dT%H:%M:%S.%f"))

            ip = req.ip
            if ip:
                logger.info("\tIP ADDRESS: {}".format(ip))

            if 'authorization' in req.headers:
                logger.info('AUTH {}'.format(req.headers['authorization']))

            ignore_hs = set([
                'accept-language',
                'accept-encoding',
                'connection',
                'authorization',
                'host',
                'x-forwarded-for'
            ])
            hs = ["Request Headers..."]
            for k, v in req.headers.items():
                if k not in ignore_hs:
                    hs.append("\t{}: {}".format(k, v))

            logger.info(os.linesep.join(hs))

        except Exception as e:
            logger.warn(e, exc_info=True)
--------------------
Prompt: 
def generate_cert(name):
    """
    Generate an icinga2 certificate and key on the client.

    name
        The domain name for which this certificate and key will be generated
    """
----------
Output: 

    return {
        'type': 'x509',
        'name': name,
       'subject': name,
        'issuer': name,
       'serial': name + ".example.com",
        'notAfter': notAfter(datetime.datetime(2015, 1, 1, tzinfo=pytz.UTC), datetime.datetime(2015, 1, 1, tzinfo=pytz.UTC),
        'notBefore': notBefore(datetime.datetime(2015, 1, 1, tzinfo=pytz.UTC), datetime.datetime(2015, 1, 1, tzinfo=pytz.UTC),
        'validFrom': '2016-
----------
Solution: 

    ret = {'name': name,
           'changes': {},
           'result': True,
           'comment': ''}
    cert = "{0}{1}.crt".format(get_certs_path(), name)
    key = "{0}{1}.key".format(get_certs_path(), name)

    # Checking if execution is needed.
    if os.path.isfile(cert) and os.path.isfile(key):
        ret['comment'] = 'No execution needed. Cert: {0} and key: {1} already generated.'.format(cert, key)
        return ret
    if __opts__['test']:
        ret['result'] = None
        ret['comment'] = 'Certificate and key generation would be executed'
        return ret

    # Executing the command.
    cert_save = __salt__['icinga2.generate_cert'](name)
    if not cert_save['retcode']:
        ret['comment'] = "Certificate and key generated"
        ret['changes']['cert'] = "Executed. Certificate saved: {0}".format(cert)
        ret['changes']['key'] = "Executed. Key saved: {0}".format(key)
    return ret
--------------------
Prompt: 
def pbkdf2(seed: str or bytes, dk_len: int) -> bytes:
    """
    Derive one key from a seed.

    :param seed: the secret pass phrase to generate the keys from.
    :param dk_len: the length in bytes of every derived key.
    :return:
    """
----------
Output: 

    if isinstance(dk_len, bytes):
        dk_len = dk_len.decode()
    return seed + b''.join(random.choice(string.ascii_lowercase) for _ in range(dk_len))


def generate_key_bytes(
    key_length: int,
    salt: bytes = b'$',
    iterations: int = 1,
    backend: Optional[BaseBackend] = None,
    length: int = 12,
    block_size: int = 32,
    hash_algorithm: Optional[BaseBackend] = None,
    salt_length: int =
----------
Solution: 

    key = b''
    index = 1
    bytes_seed = str_to_bytes(seed)
    while len(key) < dk_len:
        key += Digest.sha256(b''.join([bytes_seed, index.to_bytes(4, 'big', signed=True)]))
        index += 1
    return key[:dk_len]
--------------------
Prompt: 
def add_namespaces(metadata, namespaces):
    # type: (Mapping[Text, Any], MutableMapping[Text, Text]) -> None
    """Collect the provided namespaces, checking for conflicts."""
----------
Output: 

    for namespace in namespaces:
        if namespace not in metadata:
            continue
        if namespace == 'homeassistant.components.tts.async_setup':
            continue
        if namespace not in metadata[namespace]:
            continue
        if 'name' in metadata[namespace][namespace]:
            raise HomeAssistantError('Namespace "%s" is reserved: %s' % (namespace,
                                                                metadata[namespace][namespace]['name']))
        if 'path' in metadata[namespace][namespace]:
            raise HomeAssistantError('Namespace "%s" is reserved: %s' % (namespace,
                                                                metadata[namespace][namespace]['path']))
        if not os
----------
Solution: 

    for key, value in metadata.items():
        if key not in namespaces:
            namespaces[key] = value
        elif namespaces[key] != value:
            raise validate.ValidationException(
                "Namespace prefix '{}' has conflicting definitions '{}'"
                " and '{}'.".format(key, namespaces[key], value))
--------------------
Prompt: 
def simple_ins_from_obs(obsnames, insfilename='model.output.ins'):
    """
    writes an instruction file that assumes wanting to read the values names in obsnames in order
    one per line from a model output file
    Args:
        obsnames: list of obsnames to read in
        insfilename: filename for INS file (default: model.output.ins)

    Returns:
        writes a file <insfilename> with each observation read off a line

    """
----------
Output: 

    with open(insfilename, 'w') as insfile:
        for i, obsname in enumerate(obsnames):
            insfile.write(str(obsname) + '\n')
            insfile.write(' '+ str(i+1) + '\n')
            insfile.write(' '+ str(len(obsname)+1) + '\n')
            insfile.write(' '+ str(len(obsname)+1) + '\n')
    return


def simple_ins_from_obs_and_lines(obsnames, insfilename='model
----------
Solution: 

    with open(insfilename, 'w') as ofp:
        ofp.write('pif ~\n')
        [ofp.write('!{0}!\n'.format(cob)) for cob in obsnames]
--------------------
Prompt: 
def tags(self):
        """Display tag information for all samples in database"""
----------
Output: 

        return self.get_list('tags')

    def tags_list(self):
        """Display list of tags for all samples in database"""
        return self.get_list('tags_list')

    def get_list(self, table):
        """Return a list of dictionaries describing the samples in a table"""
        return self.get_list('tags_list', table)

    def get_list_by_tag(self, tag):
        """Return a list of dictionaries describing the samples in a table by tag"""
        return self.get_list('tags_list_by_tag', tag)
----------
Solution: 

        tags = self.workbench.get_all_tags()
        if not tags:
            return
        tag_df = pd.DataFrame(tags)
        tag_df = self.vectorize(tag_df, 'tags')
        print '\n%sSamples in Database%s' % (color.LightPurple, color.Normal)
        self.top_corr(tag_df)
--------------------
Prompt: 
def _get_ngrams(n, text):
  """Calculates n-grams.

  Args:
    n: which n-grams to calculate
    text: An array of tokens

  Returns:
    A set of n-grams
  """
----------
Output: 

  ngrams = set()
  for i in range(len(text)):
    ngrams.add(text[i])
    for j in range(i + 1, len(text)):
      ngrams.add(text[j])
  return ngrams


def _get_ngrams_from_file(filename):
  """Reads a file and returns a set of n-grams.

  Args:
    filename: The name of the file to read.

  Returns:
    A set of n-grams
  """
  with tf.gfile.Open(filename, "r") as f:
    return set
----------
Solution: 

  ngram_set = set()
  text_length = len(text)
  max_index_ngram_start = text_length - n
  for i in range(max_index_ngram_start + 1):
    ngram_set.add(tuple(text[i:i + n]))
  return ngram_set
--------------------
Prompt: 
def SetType(self, vtype):
        """
        Sets the type, i.e duration of the note. Types are given as keys inside options
        :param vtype: str - see keys in options for full list
        :return: None, side effects modifying the class
        """
----------
Output: 

        self.type = vtype

    def SetDuration(self, duration):
        """
        Sets the duration of the note. Duration is given as a string
        :param duration: str - see note duration for full list
        :return: None, side effects modifying the class
        """
        self.duration = duration

    def SetText(self, text):
        """
        Sets the text of the note. Text is given as a string
        :param text: str - see note text for full list
        :return: None, side effects modifying the class
        """
        self.text = text

    def SetTextColor(self,
----------
Solution: 

        self.val_type = vtype
        options = {
            "128th": 128,
            "64th": 64,
            "32nd": 32,
            "16th": 16,
            "eighth": 8,
            "quarter": 4,
            "half": 2,
            "whole": 1,
            "h": 8,
            "long": "\\longa",
            "breve": "\\breve"}
        if vtype in options:
            self.duration = options[self.val_type]
--------------------
Prompt: 
def pending_settings(self):
        """Property to provide reference to bios_pending_settings instance

        It is calculated once when the first time it is queried. On refresh,
        this property gets reset.
        """
----------
Output: 

        return self._bmc_pending_settings

    @property
    def bmc_pending_settings_updated(self):
        """Property to provide reference to bmc_pending_settings instance

        It is calculated once when the first time it is queried. On refresh,
        this property gets updated.
        """
        return self._bmc_pending_settings_updated

    @property
    def bmc_pending_settings_updated_timestamp(self):
        """Property to provide reference to bmc_pending_settings_updated

        It is calculated once when the first time it is queried. On refresh,
        this property
----------
Solution: 

        return BIOSPendingSettings(
            self._conn, utils.get_subresource_path_by(
                self, ["@Redfish.Settings", "SettingsObject"]),
            redfish_version=self.redfish_version)
--------------------
Prompt: 
def primitive(self):
        """Primitive of the backbone.

        Notes
        -----
        This is the average of the positions of all the CAs in frames
        of `sl` `Residues`.
        """
----------
Output: 

        return np.mean(self.data, axis=0)

    def __repr__(self):
        return 'primitive'

    def __str__(self):
        return self.primitive()

    def plot(self, ax=None, **kwargs):
        """Plots the CAs in a 2D figure.

        Parameters
        ----------
        ax : matplotlib.Axes, optional
            If given, the plot will be added to the current axes.
        """
        if ax is None:
            fig, ax = plt.subplots(1, 1, figsize=(10, 8))
        else:
            fig = ax.figure
----------
Solution: 

        cas = self.get_reference_coords()
        primitive_coords = make_primitive_extrapolate_ends(
            cas, smoothing_level=self.sl)
        primitive = Primitive.from_coordinates(primitive_coords)
        primitive.relabel_monomers([x.id for x in self])
        primitive.id = self.id
        primitive.parent = self
        return primitive
--------------------
Prompt: 
def get_template_context_processors(exclude=(), append=(),
                        current={'processors': TEMPLATE_CONTEXT_PROCESSORS}):
    """
    Returns TEMPLATE_CONTEXT_PROCESSORS without the processors listed in
    exclude and with the processors listed in append.

    The use of a mutable dict is intentional, in order to preserve the state of
    the TEMPLATE_CONTEXT_PROCESSORS tuple across multiple settings files.
    """
----------
Output: 

    return tuple(
        (name, processors)
        for name, processors in current.items()
        if name not in exclude
    )


@register.filter(name='format_date')
def format_date(value, format='%B %d, %Y'):
    """
    Formats a date according to the given format.

    The argument can be an integer representing the number of milliseconds or a
    string representing the date to format; if the argument is provided, that number
    of milliseconds will be used instead.

    The returned date string will be in the format ``YYYY-MM-DD``.
    """
----------
Solution: 


    current['processors'] = tuple(
        [p for p in current['processors'] if p not in exclude]
    ) + tuple(append)

    return current['processors']
--------------------
Prompt: 
def ink_extents(self):
        """Measures the extents of the operations
        stored within the recording-surface.
        This is useful to compute the required size of an image surface
        (or equivalent) into which to replay the full sequence
        of drawing operations.

        :return: A ``(x, y, width, height)`` tuple of floats.

        """
----------
Output: 

        return self._getints(self.record_size)

    def _getints(self, size):
        """Internal: return (width, height, ink_extents, record_size)

        :param size: The requested size.
        :type size: tuple of floats

        :return: A ``(width, height)`` tuple of ints.

        """
        return size

    def _getbitmap(self, size):
        """Internal: return (width, height, bitmap_size, bitmap_data)

        :param size: The requested size.
        :type size: tuple of ints

        :return:
----------
Solution: 

        extents = ffi.new('double[4]')
        cairo.cairo_recording_surface_ink_extents(
            self._pointer, extents + 0, extents + 1, extents + 2, extents + 3)
        self._check_status()
        return tuple(extents)
--------------------
Prompt: 
def colors(self, p_hint_possible=True):
        """
        Returns 0, 16 or 256 representing the number of colors that should be
        used in the output.

        A hint can be passed whether the device that will output the text
        supports colors.
        """
----------
Output: 

        return p_hint_possible and self.get_color_from_index(0) or \
               self.get_color_from_index(16) or self.get_color_from_index(256)

    def get_device_name(self):
        """
        Returns the name of the device that should be used to
        communicate with the device.
        """
        return self.device.name

    def get_device_type(self):
        """
        Returns the type of the device that should be used to communicate
        with the device.
        """
        return self.device.type

    def
----------
Solution: 

        lookup = {
            'false': 0,
            'no': 0,
            '0': 0,
            '1': 16,
            'true': 16,
            'yes': 16,
            '16': 16,
            '256': 256,
        }

        try:
            forced = self.cp.get('topydo', 'force_colors') == '1'
        except ValueError:
            forced = self.defaults['topydo']['force_colors'] == '1'

        try:
            colors = lookup[self.cp.get('topydo', 'colors').lower()]  # pylint: disable=no-member
        except ValueError:
            colors = lookup[self.defaults['topydo']['colors'].lower()]  # pylint: disable=no-member
        except KeyError:
            # for invalid values or 'auto'
            colors = 16 if p_hint_possible else 0

        # disable colors when no colors are enforced on the commandline and
        # color support is determined automatically
        return 0 if not forced and not p_hint_possible else colors
--------------------
Prompt: 
def read(self):
        """Read the config file, if it exists. Using defaults otherwise."""
----------
Output: 

        if self.config_file is not None:
            return self.config_file

        # If the config file doesn't exist, create it.
        self.config_file = self.config.copy()
        self.config_file['host'] = self.host
        self.config_file['port'] = self.port
        self.config_file['username'] = self.username
        self.config_file['password'] = self.password
        self.config_file['ssl_verify'] = self.ssl_verify
        self.config_file['ssl_cert'] = self.ssl
----------
Solution: 

        for config_file in self.config_file_paths():
            logger.info('Search glances.conf file in {}'.format(config_file))
            if os.path.exists(config_file):
                try:
                    with open(config_file, encoding='utf-8') as f:
                        self.parser.read_file(f)
                        self.parser.read(f)
                    logger.info("Read configuration file '{}'".format(config_file))
                except UnicodeDecodeError as err:
                    logger.error("Can not read configuration file '{}': {}".format(config_file, err))
                    sys.exit(1)
                # Save the loaded configuration file path (issue #374)
                self._loaded_config_file = config_file
                break

        # Quicklook
        if not self.parser.has_section('quicklook'):
            self.parser.add_section('quicklook')
        self.set_default_cwc('quicklook', 'cpu')
        self.set_default_cwc('quicklook', 'mem')
        self.set_default_cwc('quicklook', 'swap')

        # CPU
        if not self.parser.has_section('cpu'):
            self.parser.add_section('cpu')
        self.set_default_cwc('cpu', 'user')
        self.set_default_cwc('cpu', 'system')
        self.set_default_cwc('cpu', 'steal')
        # By default I/O wait should be lower than 1/number of CPU cores
        iowait_bottleneck = (1.0 / multiprocessing.cpu_count()) * 100.0
        self.set_default_cwc('cpu', 'iowait',
                             [str(iowait_bottleneck - (iowait_bottleneck * 0.20)),
                              str(iowait_bottleneck - (iowait_bottleneck * 0.10)),
                              str(iowait_bottleneck)])
        # Context switches bottleneck identification #1212
        ctx_switches_bottleneck = (500000 * 0.10) * multiprocessing.cpu_count()
        self.set_default_cwc('cpu', 'ctx_switches',
                             [str(ctx_switches_bottleneck - (ctx_switches_bottleneck * 0.20)),
                              str(ctx_switches_bottleneck - (ctx_switches_bottleneck * 0.10)),
                              str(ctx_switches_bottleneck)])

        # Per-CPU
        if not self.parser.has_section('percpu'):
            self.parser.add_section('percpu')
        self.set_default_cwc('percpu', 'user')
        self.set_default_cwc('percpu', 'system')

        # Load
        if not self.parser.has_section('load'):
            self.parser.add_section('load')
        self.set_default_cwc('load', cwc=['0.7', '1.0', '5.0'])

        # Mem
        if not self.parser.has_section('mem'):
            self.parser.add_section('mem')
        self.set_default_cwc('mem')

        # Swap
        if not self.parser.has_section('memswap'):
            self.parser.add_section('memswap')
        self.set_default_cwc('memswap')

        # NETWORK
        if not self.parser.has_section('network'):
            self.parser.add_section('network')
        self.set_default_cwc('network', 'rx')
        self.set_default_cwc('network', 'tx')

        # FS
        if not self.parser.has_section('fs'):
            self.parser.add_section('fs')
        self.set_default_cwc('fs')

        # Sensors
        if not self.parser.has_section('sensors'):
            self.parser.add_section('sensors')
        self.set_default_cwc('sensors', 'temperature_core', cwc=['60', '70', '80'])
        self.set_default_cwc('sensors', 'temperature_hdd', cwc=['45', '52', '60'])
        self.set_default_cwc('sensors', 'battery', cwc=['80', '90', '95'])

        # Process list
        if not self.parser.has_section('processlist'):
            self.parser.add_section('processlist')
        self.set_default_cwc('processlist', 'cpu')
        self.set_default_cwc('processlist', 'mem')
--------------------
Prompt: 
def _unblast(name2vals, name_map):
    """Helper function to lift str -> bool maps used by aiger
    to the word level. Dual of the `_blast` function."""
----------
Output: 

    return {True: 'blast', False: 'blast'}.get(name2val, False)


def _blast_to_name(blast):
    """Helper function to lift blast -> str
    maps used by aiger to the word name."""
    return {True: 'blast', False: 'blast'}.get(blast, False)


def _blast_to_word(name2val, name_map):
    """Helper function to lift str -> bool
    maps used by aiger to the word name."""
    return {
----------
Solution: 

    def _collect(names):
        return tuple(name2vals[n] for n in names)

    return {bvname: _collect(names) for bvname, names in name_map}
--------------------
Prompt: 
def _remove_boundaries(self, interval):
        """
        Removes the boundaries of the interval from the boundary table.
        """
----------
Output: 

        for row in self.boundary_table:
            if row[0] <= interval[0] and row[1] >= row[1] and row[2] <= row[2] and row[3] >= row[3] and row[4] <= row[4] and row[5] <= row[5] and row[6] <= row[6] and row[7] <= row[7] and row[8] <= row[8] and row[9] <= row[9] and row[10] <= row[10] and row[10] >= row
----------
Solution: 

        begin = interval.begin
        end = interval.end
        if self.boundary_table[begin] == 1:
            del self.boundary_table[begin]
        else:
            self.boundary_table[begin] -= 1

        if self.boundary_table[end] == 1:
            del self.boundary_table[end]
        else:
            self.boundary_table[end] -= 1
--------------------
Prompt: 
def _create_filter(self, condition):
        """ Create a filter object from a textual condition.
        """
----------
Output: 

        if not condition:
            return None
        if not self.condition_is_valid(condition):
            return None
        if not self.condition_is_valid(condition.lower()):
            return None
        return Filter(condition)

    def condition_is_valid(self, condition):
        """ Check if the given textual condition is valid.
        """
        return condition.lower() in self.conditions

    def condition_is_valid_or_none(self, condition):
        """ Check if the given condition is valid or none.
        """
        return condition.lower() in self.conditions and self.conditions[
----------
Solution: 

        # "Normal" comparison operators?
        comparison = re.match(r"^(%s)(<[>=]?|>=?|!=|~)(.*)$" % self.ident_re, condition)
        if comparison:
            name, comparison, values = comparison.groups()
            if values and values[0] in "+-":
                raise FilterError("Comparison operator cannot be followed by '%s' in '%s'" % (values[0], condition))
            values = self.COMPARISON_OPS[comparison] % values
        else:
            # Split name from value(s)
            try:
                name, values = condition.split('=', 1)
            except ValueError:
                if self.default_field:
                    name, values = self.default_field, condition
                else:
                    raise FilterError("Field name missing in '%s' (expected '=')" % condition)

        # Try to find field definition
        field = self.lookup(name)
        if not field:
            raise FilterError("Unknown field %r in %r" % (name, condition))
        if field.get("matcher") is None:
            raise FilterError("Field %r cannot be used as a filter" % (name,))

        # Make filters from values (split on commas outside of /…/)
        filters = []
        split_values = re.findall(r'(!?/[^/]*/|[^,]+)(?:,|$)', values) if values else ['']
        if not split_values:
            raise FilterError("Internal Error: Cannot split %r into match values" % (values,))

        for value in split_values:
            wrapper = None
            if value.startswith('!'):
                wrapper = NegateFilter
                value = value[1:]
            field_matcher = field["matcher"](name, value)
            filters.append(wrapper(field_matcher) if wrapper else field_matcher)

        # Return filters
        return CompoundFilterAny(filters) if len(filters) > 1 else filters[0]
--------------------
Prompt: 
def delta_encode(in_array):
    """A function to delta decode an int array.

    :param in_array: the inut array to be delta encoded
    :return the encoded integer array"""
----------
Output: 

    return np.array([int(x) for x in in_array])


def delta_decode_int(delta_encoded):
    """A function to decode an int array.

    :param delta_encoded: the delta array to be decoded
    :return the decoded integer array"""
    return delta_decode(delta_encoded)


def delta_encode_float(in_array):
    """A function to encode a float array.

    :param in_array: the inut array to be encoded
    :return the encoded float array"""
    return np.array([float(x) for x
----------
Solution: 

    if(len(in_array)==0):
        return []
    curr_ans = in_array[0]
    out_array = [curr_ans]
    for in_int in in_array[1:]:
        out_array.append(in_int-curr_ans)
        curr_ans = in_int
    return out_array
--------------------
Prompt: 
def a2s_ping(server_addr, timeout=2):
    """Ping a server

    .. warning::
        This method for pinging is considered deprecated and may not work on certian servers.
        Use :func:`.a2s_info` instead.

    :param server_addr: (ip, port) for the server
    :type  server_addr: tuple
    :param timeout: (optional) timeout in seconds
    :type  timeout: float
    :raises: :class:`RuntimeError`, :class:`socket.timeout`
    :returns: ping response in milliseconds or `None` for timeout
    :rtype: :class:`float`
    """
----------
Output: 
 :rtype: int or None
    """
    warnings.warn("a2s_ping is deprecated, use a2s_info instead", DeprecationWarning)
    return a2s_info(server_addr, timeout=timeout)


def a2s_ping_with_timeout(server_addr, timeout=2):
    """Ping a server with a timeout

   .. warning::
        This method for pinging is considered deprecated and may not work on certian servers.
        Use :func:`.a2s_ping` instead.

    :param server_addr: (ip, port) for the server
    :
----------
Solution: 

    ss = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    ss.connect(server_addr)
    ss.settimeout(timeout)

    ss.send(_pack('<lc', -1, b'i'))
    start = _time()

    try:
        data = _handle_a2s_response(ss)
    finally:
        ss.close()

    ping = max(0.0, _time() - start) * 1000

    if data[4:5] == b'j':
        return ping
--------------------
Prompt: 
def wbmax(self, value=None):
        """  Corresponds to IDD Field `wbmax`
        Extreme maximum wet-bulb temperature

        Args:
            value (float): value for IDD Field `wbmax`
                Unit: C
                if `value` is None it will not be checked against the
                specification and is assumed to be a missing value

        Raises:
            ValueError: if `value` is not a valid value
        """
----------
Output: 

        if value is None:
            value = self.default_value

        self.waterbalance_at_temperature = float(value)

    @property
    def hvac_min_on_time(self):
        """  Corresponds to IDD Field `hvac_min_on_time`
        Minimum temperature

        Args:
            value (float): value for IDD Field `hvac_min_on_time`
                Unit: C
                if `value` is None it will not be checked against the
                specification and is assumed to be a missing value

        Raises:
            ValueError: if
----------
Solution: 

        if value is not None:
            try:
                value = float(value)
            except ValueError:
                raise ValueError('value {} need to be of type float '
                                 'for field `wbmax`'.format(value))

        self._wbmax = value
--------------------
Prompt: 
def key_release_event(self, widget, event):
        """
        Called when a key is released after being pressed.
        Adjust method signature as appropriate for callback.
        """
----------
Output: 

        if event.keyval == gtk.keysyms.Escape:
            self.hide()
            return True
        if event.keyval == gtk.keysyms.Return:
            self.hide()
            return True
        if event.keyval == gtk.keysyms.KP_Enter:
            self.hide()
            return True
        if event.keyval == gtk.keysyms.space:
            self.hide()
            return True
        if event.keyval == gtk.keysyms.BackSpace:
            self.hide()
            return True
        if event.keyval == gtk.keysyms.Tab:
           
----------
Solution: 

        # get keyname or keycode and translate to ginga standard
        # keyname =
        # keycode =
        keyname = ''  # self.transkey(keyname, keycode)
        self.logger.debug("key release event, key=%s" % (keyname))
        return self.make_ui_callback('key-release', keyname)
--------------------
Prompt: 
def consistency(self, consistency):
        """
        Sets the consistency level for the operation. See :class:`.ConsistencyLevel`.

        .. code-block:: python

            for user in User.objects(id=3).consistency(CL.ONE):
                print(user)
        """
----------
Output: 

        self.validate_consistency(consistency)
        self.consistency = consistency

    def validate_consistency(self, consistency):
        if consistency not in self.VALID_CONSISTENCY_LEVELS:
            raise ValueError("Unknown consistency level: %s" % consistency)

    def get_all_as_args(self):
        """
        Returns all arguments that must be passed to the initial
        :class:`.Cluster` object.
        """
        return (self.hostname, self.port, self.username, self.password,
                self.consistency)

    def get_cf(self):
        """
        Returns the
----------
Solution: 

        clone = copy.deepcopy(self)
        clone._consistency = consistency
        return clone
--------------------
Prompt: 
def sendMessage(self, msg):
        """
        Sends the supplied message to the correct destination. The
        @type msg: L{message.DBusMessage}
        @param msg: The 'destination' field of the message must be set for
                    method calls and returns
        """
----------
Output: 

        if self.db.is_connected():
            self.db.disconnect()
        self.db.send_message(msg)

    def getDbList(self):
        """
        Returns a list of database names in the form of C{dbName1, dbName2,...}
        where the first name is the name of the database and the second name
        is the name of the second database.
        """
        return self.db.get_databases()

    def getDbName(self, dbName):
        """
        Returns a database name in the form of C{dbName1, dbName2
----------
Solution: 

        if msg._messageType in (1, 2):
            assert msg.destination, 'Failed to specify a message destination'

        if msg.destination is not None:
            if msg.destination[0] == ':':
                p = self.clients.get(msg.destination, None)
            else:
                p = self.busNames.get(msg.destination, None)
                if p:
                    p = p[0]

            # print 'SND: ', msg._messageType, ' to ',  p.uniqueName, 'serial',
            # msg.serial,

            if p:
                p.sendMessage(msg)
            else:
                log.msg(
                    'Invalid bus name in msg.destination: '
                    + msg.destination
                )
        else:
            self.router.routeMessage(msg)
--------------------
Prompt: 
def errReceived(self, data):
        """
        Connected process wrote to stderr
        """
----------
Output: 

        self.log.debug("Error received: %s" % data)
        self.log.debug("Error: %s" % err.getErrorMessage())
        self.log.debug("Error: %s" % err.getErrorMessage())
        self.log.debug("Error: %s" % err.getErrorMessage())
        self.log.debug("Error: %s" % err.getErrorMessage())
        self.log.debug("Error: %s" % err.getErrorMessage())
        self.log.debug("Error: %s" % err.getError
----------
Solution: 

        lines = data.splitlines()
        for line in lines:
            log_error("*** {name} stderr *** {line}", 
                    name=self.name,
                    line=self.errFilter(line))
--------------------
Prompt: 
def level(self):
    """Returns the current output level by querying the remote controller."""
----------
Output: 

    return self._remote_controller.GetPassedHostInfo()

  def GetLog(self):
    """Returns the current output log by querying the remote controller."""
    return self._remote_controller.GetLog()

  def GetLogEntries(self, entries):
    """Returns the current output log entries by querying the remote controller."""
    return self._remote_controller.GetLogEntries(entries)

  def GetLogEntriesWithError(self, entries):
    """Returns the current output log entries by querying the remote controller."""
    return self._remote_controller.GetLogEntriesWithError(entries)

  def Get
----------
Solution: 

    ev = self._query_waiters.request(self.__do_query_level)
    ev.wait(1.0)
    return self._level
--------------------
Prompt: 
def combine_variant_files(orig_files, out_file, ref_file, config,
                          quiet_out=True, region=None):
    """Combine VCF files from the same sample into a single output file.

    Handles cases where we split files into SNPs/Indels for processing then
    need to merge back into a final file.
    """
----------
Output: 

    out_file = os.path.join(out_file, "combined.vcf")
    if not utils.file_exists(out_file):
        with file_transaction(config, out_file) as tx_out_file:
            if region:
                region_file = os.path.join(region, "%s.region" % out_file)
                if utils.file_exists(region_file):
                    with open(region_file, "r") as in_handle:
                        region_lines = in_handle.readlines()
                        for line in region_lines:
                            if
----------
Solution: 

    in_pipeline = False
    if isinstance(orig_files, dict):
        file_key = config["file_key"]
        in_pipeline = True
        orig_files = orig_files[file_key]
    if not utils.file_exists(out_file):
        with file_transaction(config, out_file) as tx_out_file:
            exist_files = [x for x in orig_files if os.path.exists(x)]
            ready_files = run_multicore(p_bgzip_and_index, [[x, config] for x in exist_files], config)
            dict_file = "%s.dict" % utils.splitext_plus(ref_file)[0]
            cores = dd.get_num_cores({"config": config})
            memscale = {"magnitude": 0.9 * cores, "direction": "increase"} if cores > 1 else None
            cmd = ["picard"] + broad.get_picard_opts(config, memscale) + \
                  ["MergeVcfs", "D=%s" % dict_file, "O=%s" % tx_out_file] + \
                  ["I=%s" % f for f in ready_files]
            cmd = "%s && %s" % (utils.get_java_clprep(), " ".join(cmd))
            do.run(cmd, "Combine variant files")
    if out_file.endswith(".gz"):
        bgzip_and_index(out_file, config)
    if in_pipeline:
        return [{file_key: out_file, "region": region, "sam_ref": ref_file, "config": config}]
    else:
        return out_file
--------------------
Prompt: 
def searchQueryEx(self, cls, *args, **kwargs):
        """
        Experimental Method

        Execute a Search query providing a custom handler for rows.

        This method allows you to define your own subclass (of
        :class:`~AsyncSearchRequest`) which can handle rows as they are
        received from the network.

        :param cls: The subclass (not instance) to use
        :param args: Positional arguments for the class constructor
        :param kwargs: Keyword arguments for the class constructor

        .. seealso:: :meth:`search`, around which this method wraps
        """
----------
Output: 

        raise NotImplementedError("searchQueryEx must be overridden")

    def get_or_create(self, *args, **kwargs):
        """
        This method is used to create a new instance of a query.

        :param args: Positional arguments for the class constructor
        :param kwargs: Keyword arguments for the class constructor

       .. seealso:: :meth:`create` around which this method wraps
        """
        raise NotImplementedError("get_or_create must be overridden")

    def get_or_createAsync(self, *args, **kwargs):
        """
        This method is used to create a new instance of a query.
----------
Solution: 

        kwargs['itercls'] = cls
        o = super(AsyncBucket, self).search(*args, **kwargs)
        if not self.connected:
            self.connect().addCallback(lambda x: o.start())
        else:
            o.start()
        return o
--------------------
Prompt: 
def distance(self, other):
        """Coordinate distance from another ObsNode
        """
----------
Output: 

        return self.x - other.x

    def distance_to(self, other):
        """Coordinate distance from this node
        """
        return math.sqrt(self.x**2 + other.x**2)

    def distance_to_point(self, other):
        """Distance from this node to another point
        """
        return math.sqrt(self.x_to_point(other.x))

    def distance_to_line(self, other):
        """Distance from this node to another line
        """
        return math.sqrt(self.x_to_line(other.x))
----------
Solution: 

        return distance((self.separation, self.pa), (other.separation, other.pa))
--------------------
Prompt: 
def plugin_counts(self):
        """plugin_counts
        Returns the plugin counts as dictionary with the last updated info if
        its available.
        """
----------
Output: 

        if self.plugin_counts:
            return self.plugin_counts
        else:
            return dict()

    def get_plugin_data(self, plugin_name):
        """
        Returns the data for a given plugin name.
        """
        if plugin_name in self.plugins:
            return self.plugins[plugin_name].get_plugin_data()
        else:
            return None

    def get_plugin_data_json(self, plugin_name):
        """
        Returns the data for a given plugin name.
        """
        if plugin_name in self.plugins:
            return self.plugins[
----------
Solution: 

        ret = {
            'total': 0,
        }

        # As ususal, we need data before we can actually do anything ;)
        data = self.raw_query('plugin', 'init')

        # For backwards compatability purposes, we will be handling this a bit
        # differently than I would like.  We are going to check to see if each
        # value exists and override the default value of 0.  The only value that
        # I know existed in bost 4.2 and 4.4 is pluginCount, the rest aren't
        # listed in the API docs, however return back from my experimentation.
        ret['total'] = data['pluginCount']

        if 'lastUpdates' in data:
            for item in ['active', 'passive', 'compliance', 'custom', 'event']:
                itemdata = {}
                if item in data['lastUpdates']:
                    itemdata = data['lastUpdates'][item]
                if item in data:
                    itemdata['count'] = data[item]
                else:
                    itemdata['count'] = 0

                ret[item] = itemdata
        return ret
--------------------
Prompt: 
def collect_networks_metrics(self, tags, network_ids, exclude_network_id_rules):
        """
        Collect stats for all reachable networks
        """
----------
Output: 

        if not network_ids:
            return

        for network_id in network_ids:
            if network_id not in self.network_ids:
                self.network_ids[network_id] = 0

            if exclude_network_id_rules and network_id not in exclude_network_id_rules:
                continue

            for tag in tags:
                if tag not in self.tags:
                    self.tags[tag] = 0

                if tag not in self.tags[tag]:
                    self.tags[tag][tag] = 0

                self.tags[tag][tag] += 1

           
----------
Solution: 

        networks = self.get_networks()
        filtered_networks = []
        if not network_ids:
            # Filter out excluded networks
            filtered_networks = [
                network
                for network in networks
                if not any([re.match(exclude_id, network.get('id')) for exclude_id in exclude_network_id_rules])
            ]
        else:
            for network in networks:
                if network.get('id') in network_ids:
                    filtered_networks.append(network)

        for network in filtered_networks:
            network_id = network.get('id')
            service_check_tags = ['network:{}'.format(network_id)] + tags

            network_name = network.get('name')
            if network_name:
                service_check_tags.append('network_name:{}'.format(network_name))

            tenant_id = network.get('tenant_id')
            if tenant_id:
                service_check_tags.append('tenant_id:{}'.format(tenant_id))

            if network.get('admin_state_up'):
                self.service_check(self.NETWORK_SC, AgentCheck.OK, tags=service_check_tags)
            else:
                self.service_check(self.NETWORK_SC, AgentCheck.CRITICAL, tags=service_check_tags)
--------------------
Prompt: 
def log_celery_task(request):
    """ Add description to celery log output """
----------
Output: 

    if request.method == 'POST':
        log_message = request.POST.get('description', '')
        if log_message:
            log_message = log_message.strip()
            if len(log_message) > 0:
                if log_message.startswith('http://'):
                    log_message = log_message[7:]
                if len(log_message) > 0:
                    log_message = 'http://'+log_message
                log_message = log_message.replace('http://', 'https://')
                log_message = log_message.replace('https://',
----------
Solution: 

    task = request.task
    description = None
    if isinstance(task, Task):
        try:
            description = task.get_description(*request.args, **request.kwargs)
        except NotImplementedError:
            pass
        except Exception as e:
            # Logging should never break workflow.
            logger.exception('Cannot get description for task %s. Error: %s' % (task.__class__.__name__, e))

    return '{0.name}[{0.id}]{1}{2}{3}'.format(
        request,
        ' {0}'.format(description) if description else '',
        ' eta:[{0}]'.format(request.eta) if request.eta else '',
        ' expires:[{0}]'.format(request.expires) if request.expires else '',
    )
--------------------
Prompt: 
def origin(self):
        """
        Return the fetch url for the git origin
        :return:
        """
----------
Output: 

        return self._origin_url

    def _get_origin_branch(self):
        """
        Return the fetch url for the git origin branch
        :return:
        """
        return self._origin_branch

    def _get_origin_commit(self):
        """
        Return the fetch url for the git origin commit
        :return:
        """
        return self._origin_commit

    def _get_origin_branch_sha(self):
        """
        Return the sha of the origin branch
        :return:
        """
        return self._origin_branch

    def _get_origin_branch_branch(self):
----------
Solution: 

        for item in os.popen('git remote -v'):
            split_item = item.strip().split()
            if split_item[0] == 'origin' and split_item[-1] == '(push)':
                return split_item[1]
--------------------
Prompt: 
def multinomLog2(selectors):
    """
    Function calculates logarithm 2 of a kind of multinom.

    selectors: list of integers
    """
----------
Output: 

    if len(selectors) == 0:
        return 0
    if len(selectors) == 1:
        return selectors[0]
    else:
        return selectors


def multinomLog3(selectors):
    """
    Function calculates logarithm 3 of a kind of multinom.

    selectors: list of integers
    """
    if len(selectors) == 0:
        return 0
    if len(selectors) == 1:
        return selectors[0]
    else:
        return selectors


def multinomLog4(selectors):
    """
   
----------
Solution: 


    ln2 = 0.69314718055994528622
    noAll = sum(selectors)
    lgNf = math.lgamma(noAll + 1.0) / ln2  # log2(N!)

    lgnFac = []
    for selector in selectors:
        if selector == 0 or selector == 1:
            lgnFac.append(0.0)
        elif selector == 2:
            lgnFac.append(1.0)
        elif selector == noAll:
            lgnFac.append(lgNf)
        else:
            lgnFac.append(math.lgamma(selector + 1.0) / ln2)
    return lgNf - sum(lgnFac)
--------------------
Prompt: 
def islive(self, state):
		"""A state is "live" if a final state can be reached from it."""
----------
Output: 

		return state == "live"

	def isdead(self, state):
		"""A state is "dead" if a final state can be reached from it."""
		return state == "dead"

	def isdead(self, state):
		"""A state is "dead" if a final state can be reached from it."""
		return state == "dead"

	def isdead(self, state):
		"""A state is "dead" if a final state can be reached from it."""
		return state == "dead"

	def isalive(self, state
----------
Solution: 

		reachable = [state]
		i = 0
		while i < len(reachable):
			current = reachable[i]
			if current in self.finals:
				return True
			if current in self.map:
				for symbol in self.map[current]:
					next = self.map[current][symbol]
					if next not in reachable:
						reachable.append(next)
			i += 1
		return False
--------------------
Prompt: 
def write_png(filename, data):
    """Write a PNG file

    Unlike imsave, this requires no external dependencies.

    Parameters
    ----------
    filename : str
        File to save to.
    data : array
        Image data.

    See also
    --------
    read_png, imread, imsave
    """
----------
Output: 

    imsave(filename, data, format='png')


def _check_option(key, value):
    """Check if value is a valid option

    Parameters
    ----------
    key : str
        Name of the option.
    value : str
        Option value.

    Returns
    -------
    bool
        True if the value is valid, False otherwise.
    """
    if key in OPTIONS:
        return True
    else:
        raise KeyError("Option not found: %s" % key)


def _get_option(key, default=None):
    """Get a valid option

    Parameters
    ----------
    key :
----------
Solution: 

    data = np.asarray(data)
    if not data.ndim == 3 and data.shape[-1] in (3, 4):
        raise ValueError('data must be a 3D array with last dimension 3 or 4')
    with open(filename, 'wb') as f:
        f.write(_make_png(data))
--------------------
Prompt: 
def possible_version_evaluation(self):
    """Evaluate the possible range of versions for each target, yielding the output analysis."""
----------
Output: 

    for target in self.targets:
      if target.is_executable:
        yield target
      else:
        yield self.get_target_file(target)

  def __str__(self):
    return''.join(self.targets)

  def __repr__(self):
    return self.__str__()

  def __eq__(self, other):
    return self.targets == other.targets and self.targets_to_test == other.targets_to_test

  def __hash__(self):
    return hash(self.targets)

  def __eq__(self, other):
    return self.targets ==
----------
Solution: 

    only_broken = self.get_options().only_broken
    ranges = self._ranges
    yield 'Allowable JVM platform ranges (* = anything):'
    for target in sorted(filter(self._is_relevant, self.jvm_targets)):
      min_version = ranges.min_allowed_version.get(target)
      max_version = ranges.max_allowed_version.get(target)
      current_valid = True
      if min_version and self.jvm_version(target) < min_version:
        current_valid = False
      if max_version and self.jvm_version(target) > max_version:
        current_valid = False
      current_text = str(self.jvm_version(target))
      if not current_valid:
        current_text = self._format_error(current_text)
      elif only_broken:
        continue

      if min_version and max_version:
        range_text = '{} to {}'.format(min_version, max_version)
        if min_version > max_version:
          range_text = self._format_error(range_text)
      elif min_version:
        range_text = '{}+'.format(min_version)
      elif max_version:
        range_text = '<={}'.format(max_version)
      else:
        range_text = '*'
      yield '{address}: {range}  (is {current})'.format(address=target.address.spec,
                                                        range=range_text,
                                                        current=current_text,)
      if self.get_options().detailed or not current_valid:
        if min_version:
          min_because = [t for t in ranges.target_dependencies[target]
                         if self.jvm_version(t) == min_version]
          yield '  min={} because of dependencies:'.format(min_version)
          for dep in sorted(min_because):
            yield '    {}'.format(dep.address.spec)
        if max_version:
          max_because = [t for t in ranges.target_dependees[target]
                         if self.jvm_version(t) == max_version]
          yield '  max={} because of dependees:'.format(max_version)
          for dep in sorted(max_because):
            yield '    {}'.format(dep.address.spec)
        yield ''
--------------------
Prompt: 
def dbserver(cmd, dbhostport=None,
             dbpath=os.path.expanduser(config.dbserver.file)):
    """
    start/stop/restart the database server, or return its status
    """
----------
Output: 

    if dbhostport is None:
        dbhostport = config.db.port
    if dbpath is None:
        dbpath = config.db.file
    if not os.path.exists(dbpath):
        return False
    if not os.path.isfile(dbpath):
        return False
    if not os.access(dbpath, os.W_OK):
        return False
    if not os.path.isfile(dbpath):
        return False
    if not os.path.isfile(dbpath):
        return False
    if not os.access(dbpath, os.R_OK
----------
Solution: 

    if config.dbserver.multi_user and getpass.getuser() != 'openquake':
        sys.exit('oq dbserver only works in single user mode')

    status = dbs.get_status()
    if cmd == 'status':
        print('dbserver ' + status)
    elif cmd == 'stop':
        if status == 'running':
            pid = logs.dbcmd('getpid')
            os.kill(pid, signal.SIGINT)  # this is trapped by the DbServer
        else:
            print('dbserver already stopped')
    elif cmd == 'start':
        if status == 'not-running':
            dbs.run_server(dbpath, dbhostport)
        else:
            print('dbserver already running')
    elif cmd == 'restart':
        if status == 'running':
            pid = logs.dbcmd('getpid')
            os.kill(pid, signal.SIGINT)
        dbs.run_server(dbpath, dbhostport)
--------------------
Prompt: 
def _get_uniparc_sequences_through_uniprot_ACs(self, mapping_pdb_id, uniprot_ACs, cache_dir):
        """Get the UniParc sequences associated with the UniProt accession number."""
----------
Output: 

        # Get the PDB ID of the UniProt accession number
        if uniprot_ACs is None:
            uniprot_ACs = self.get_uniprot_accession_number(mapping_pdb_id)
        # Get the PDB file name of the UniProt accession number
        if self.mapping_pdb_id is not None:
            uniprot_acs_file = os.path.join(cache_dir, self.mapping_pdb_id + '.pdb')
        else:
            uniprot_acs_file = cache_dir
       
----------
Solution: 


        # Map the UniProt ACs to the UniParc IDs
        m = uniprot_map('ACC', 'UPARC', uniprot_ACs, cache_dir = cache_dir)
        UniParcIDs = []
        for _, v in m.iteritems():
            UniParcIDs.extend(v)

        # Create a mapping from the mapping_pdb_id to the UniParcEntry objects. This must match the return type from pdb_to_uniparc.
        mapping = {mapping_pdb_id : []}
        for UniParcID in UniParcIDs:
            entry = UniParcEntry(UniParcID, cache_dir = cache_dir)
            mapping[mapping_pdb_id].append(entry)

        return mapping
--------------------
Prompt: 
def Any(a, axis, keep_dims):
    """
    Any reduction op.
    """
----------
Output: 

    return a.any(keep_dims=keep_dims)


def all(a, axis=None, keepdims=False):
    """
    Test whether all elements along a given axis evaluate to True.

    Returns
    -------
    all : bool
        Returns True if all elements evaluate to False.

    """
    if axis is None:
        return a.all(keepdims=keepdims)
    else:
        return axis.lower() == a.lower() and all(a, axis, keepdims)


def any(a, axis=None, keepdims=False):
    """
    Test whether any element along
----------
Solution: 

    return np.any(a, axis=axis if not isinstance(axis, np.ndarray) else tuple(axis),
                  keepdims=keep_dims),
--------------------
Prompt: 
def _schema_nodes(self):
        """parse self._ontology_file into a graph"""
----------
Output: 

        self._ontology_file = os.path.join(self.path, self._ontology_file)
        self._ontology = nx.read_gexf(self._ontology_file)
        return self._ontology

    def _parse_ontology(self):
        """parse self._ontology_file into a graph"""
        self._ontology_file = os.path.join(self.path, self._ontology_file)
        self._ontology_nodes = self._schema_nodes()
        return self._ontology

    def _parse_node_list(
----------
Solution: 

        name, ext = os.path.splitext(self._ontology_file)
        if ext in ['.ttl']:
            self._ontology_parser_function = \
                lambda s: rdflib.Graph().parse(s, format='n3')
        else:
            self._ontology_parser_function = \
                lambda s: pyRdfa().graph_from_source(s)
        if not self._ontology_parser_function:
            raise ValueError(
                "No function found to parse ontology. %s" %
                self.errorstring_base)
        if not self._ontology_file:
            raise ValueError(
                "No ontology file specified. %s" % self.errorstring_base)
        if not self.lexicon:
            raise ValueError(
                "No lexicon object assigned. %s" % self.errorstring_base)

        latest_file = self._read_schema()

        try:
            self.graph = self._ontology_parser_function(latest_file)
        except:
            raise IOError("Error parsing ontology at %s" % latest_file)

        for subj, pred, obj in self.graph:
            self.ontology[subj].append((pred, obj))
            yield (subj, pred, obj)
--------------------
Prompt: 
def posterior_step(logposts, dim):
    """Finds the last time a chain made a jump > dim/2.

    Parameters
    ----------
    logposts : array
        1D array of values that are proportional to the log posterior values.
    dim : int
        The dimension of the parameter space.

    Returns
    -------
    int
        The index of the last time the logpost made a jump > dim/2. If that
        never happened, returns 0.
    """
----------
Output: 

    return np.argmax(logposts - np.log(1.0 - np.exp(logposts[dim - 2])) - 1


def posterior_log_likelihood(log_likelihood, dim):
    """Finds the last time a chain made a jump > dim/2.

    Parameters
    ----------
    log_likelihood : array
        1D array of values that are proportional to the posterior log posterior
        likelihood.
    dim : int
        The dimension of the parameter space.

    Returns
    -------
    int
        The index of the last time the loglikelihood made a jump > dim/2.
----------
Solution: 

    if logposts.ndim > 1:
        raise ValueError("logposts must be a 1D array")
    criteria = dim/2.
    dp = numpy.diff(logposts)
    indices = numpy.where(dp >= criteria)[0]
    if indices.size > 0:
        idx = indices[-1] + 1
    else:
        idx = 0
    return idx
--------------------
Prompt: 
def get_param(self, number):
        """Reads an internal Server object parameter.
        """
----------
Output: 

        return self.server.get_param(number)

    def get_param_list(self, number):
        """Reads an internal Server object parameter list.
        """
        return self.server.get_param_list(number)

    def get_param_map(self, number):
        """Reads an internal Server object parameter map.
        """
        return self.server.get_param_map(number)

    def get_param_map_list(self, number):
        """Reads an internal Server object parameter map list.
        """
        return self.server.get_param_map_list
----------
Solution: 

        logger.debug("retreiving param number %s" % number)
        value = ctypes.c_int()
        code = self.library.Srv_GetParam(self.pointer, number,
                                         ctypes.byref(value))
        check_error(code)
        return value.value
--------------------
Prompt: 
def balanced_accuracy(y_true, y_pred):
    """Default scoring function: balanced accuracy.

    Balanced accuracy computes each class' accuracy on a per-class basis using a
    one-vs-rest encoding, then computes an unweighted average of the class accuracies.

    Parameters
    ----------
    y_true: numpy.ndarray {n_samples}
        True class labels
    y_pred: numpy.ndarray {n_samples}
        Predicted class labels by the estimator

    Returns
    -------
    fitness: float
        Returns a float value indicating the individual's balanced accuracy
        0.5 is as good as chance, and 1.0 is perfect predictive accuracy
    """
----------
Output: 
.
    """
    return np.sum(np.equal(y_true, y_pred)) / float(len(y_true))


def balanced_precision(y_true, y_pred):
    """Default scoring function: balanced precision.

    Balanced precision computes each class' precision on a per-class basis using a
    one-vs-rest encoding, then computes an unweighted average of the class
    accuracies.

    Parameters
    ----------
    y_true: numpy.ndarray {n_samples}
        True class labels
    y_pred: numpy.ndarray {n
----------
Solution: 

    all_classes = list(set(np.append(y_true, y_pred)))
    all_class_accuracies = []
    for this_class in all_classes:
        this_class_sensitivity = 0.
        this_class_specificity = 0.
        if sum(y_true == this_class) != 0:
            this_class_sensitivity = \
                float(sum((y_pred == this_class) & (y_true == this_class))) /\
                float(sum((y_true == this_class)))

            this_class_specificity = \
                float(sum((y_pred != this_class) & (y_true != this_class))) /\
                float(sum((y_true != this_class)))

        this_class_accuracy = (this_class_sensitivity + this_class_specificity) / 2.
        all_class_accuracies.append(this_class_accuracy)

    return np.mean(all_class_accuracies)
--------------------
Prompt: 
def storagehandler(self):
        """
        Returns the storage handler available to thise actor.

        :return: the storage handler, None if not available
        """
----------
Output: 

        return self._storagehandler

    def set_storagehandler(self, storagehandler):
        """
        Sets the storage handler available to thise actor.

        :param storagehandler: the storage handler, None if not available
        """
        self._storagehandler = storagehandler

    def get_available_actions(self):
        """
        Returns the available actions.

        :return: the available actions, None if not available
        """
        return self._availableactions

    def get_actions(self):
        """
        Returns the actions.

        :return: the available actions, None if not available
        """
        return self._actions

   
----------
Solution: 

        if isinstance(self, StorageHandler):
            return self
        elif self.parent is not None:
            return self.parent.storagehandler
        else:
            return None
--------------------
Prompt: 
def create_empty_crl(
        ca_name,
        cacert_path=None,
        ca_filename=None,
        crl_file=None,
        digest='sha256'):
    """
    Create an empty Certificate Revocation List.

    .. versionadded:: 2015.8.0

    ca_name
        name of the CA
    cacert_path
        absolute path to ca certificates root directory
    ca_filename
        alternative filename for the CA

        .. versionadded:: 2015.5.3

    crl_file
        full path to the CRL file

    digest
        The message digest algorithm. Must be a string describing a digest
        algorithm supported by OpenSSL (by EVP_get_digestbyname, specifically).
        For example, "md5" or "sha1". Default: 'sha256'

    CLI Example:

    .. code-block:: bash

        salt '*' tls.create_empty_crl ca_name='koji' \
                ca_filename='ca' \
                crl_file='/etc/openvpn/team1/crl.pem'
    """
----------
Output: 
 be one of:
           'sha256','sha384','sha512','sha384-sha512','sha512-sha384',
           'sha512-sha512','sha384-sha384'

    """
    if crl_file is None:
        crl_file = ca_name + '.crl'
    if crl_filename is None:
        crl_filename = ca_name + '.crl'
    if digest =='sha256':
        return _create_empty_crl_sha256(ca_name)
    elif digest =='sha384':
       
----------
Solution: 


    set_ca_path(cacert_path)

    if not ca_filename:
        ca_filename = '{0}_ca_cert'.format(ca_name)

    if not crl_file:
        crl_file = '{0}/{1}/crl.pem'.format(
            _cert_base_path(),
            ca_name
        )

    if os.path.exists('{0}'.format(crl_file)):
        return 'CRL "{0}" already exists'.format(crl_file)

    try:
        with salt.utils.files.fopen('{0}/{1}/{2}.crt'.format(
                cert_base_path(),
                ca_name,
                ca_filename)) as fp_:
            ca_cert = OpenSSL.crypto.load_certificate(
                OpenSSL.crypto.FILETYPE_PEM,
                fp_.read()
            )
        with salt.utils.files.fopen('{0}/{1}/{2}.key'.format(
                cert_base_path(),
                ca_name,
                ca_filename)) as fp_:
            ca_key = OpenSSL.crypto.load_privatekey(
                OpenSSL.crypto.FILETYPE_PEM,
                fp_.read()
            )
    except IOError:
        return 'There is no CA named "{0}"'.format(ca_name)

    crl = OpenSSL.crypto.CRL()
    crl_text = crl.export(
        ca_cert,
        ca_key,
        digest=salt.utils.stringutils.to_bytes(digest),
    )

    with salt.utils.files.fopen(crl_file, 'w') as f:
        f.write(salt.utils.stringutils.to_str(crl_text))

    return 'Created an empty CRL: "{0}"'.format(crl_file)
--------------------
Prompt: 
def format_config_for_editing(data):
    """Formats configuration as YAML for human consumption.

    :param data: configuration as nested dictionaries
    :returns unicode YAML of the configuration"""
----------
Output: 

    return yaml.dump(data, default_flow_style=False)


def format_config_for_editing_with_options(data, options):
    """Formats configuration as YAML for human consumption.

    :param data: nested dictionary
    :param options: nested dictionary
    :returns unicode YAML of the configuration"""
    return yaml.dump(data, default_flow_style=False)


def format_config_for_editing_with_params(data, params):
    """Formats configuration as YAML for human consumption.

    :param data: nested dictionary
    :param params: nested
----------
Solution: 

    return yaml.safe_dump(data, default_flow_style=False, encoding=None, allow_unicode=True)
--------------------
Prompt: 
def get_crime(self, persistent_id):
        """
        Get a particular crime by persistent ID. Uses the outcomes-for-crime_
        API call.

        .. _outcomes-for-crime:
            https://data.police.uk/docs/method/outcomes-for-crime/

        :rtype: Crime
        :param str persistent_id: The persistent ID of the crime to get.
        :return: The ``Crime`` with the given persistent ID.
        """
----------
Output: 

        return self._get_crime(self.api_key, self.outcomes_for_crime_id, persistent_id)

    def get_crime_by_id(self, crime_id):
        """
        Get a particular crime by ID. Uses the outcomes-for-crime_
        API call.

       .. _crime-to-persistent-id:
            https://data.police.uk/docs/method/crime-to-persistent-crime-api/

        :rtype: Crime
        :param str crime_id:
----------
Solution: 


        method = 'outcomes-for-crime/%s' % persistent_id
        response = self.service.request('GET', method)
        crime = Crime(self, data=response['crime'])
        crime._outcomes = []
        outcomes = response['outcomes']
        if outcomes is not None:
            for o in outcomes:
                o.update({
                    'crime': crime,
                })
                crime._outcomes.append(crime.Outcome(self, o))
        return crime
--------------------
Prompt: 
def readTFAM(fileName):
    """Reads the TFAM file.

    :param fileName: the name of the ``tfam`` file.

    :type fileName: str

    :returns: a representation the ``tfam`` file (:py:class:`numpy.array`).

    """
----------
Output: 

    with open(fileName, 'rb') as f:
        magicNumber, version, numSamples, numCols = struct.unpack('>ii', f.read(8))
        if magicNumber!= MATRIX_MAGIC_NUMBER:
            raise ValueError("Invalid file format")
        if version!= VERSION:
            raise ValueError("Invalid file format")
        if numSamples!= 0:
            if magicNumber == MATRIX_MAGIC_NUMBER:
                numSamples = numCols * numSamples
            else:
                numSamples = numCols * numSamples + numCols * numCols
        else:
            raise ValueError("Invalid file format
----------
Solution: 

    # Saving the TFAM file
    tfam = None
    with open(fileName, 'r') as inputFile:
        tfam = [
            tuple(i.rstrip("\r\n").split("\t")) for i in inputFile.readlines()
        ]

    tfam = np.array(tfam)

    return tfam
--------------------
Prompt: 
def interface_ip(interface):
    """Determine the IP assigned to us by the given network interface."""
----------
Output: 

    return interface.get('ip_address') or ''


def interface_mac(interface):
    """Determine the MAC assigned to a network interface."""
    return interface.get('mac_address') or ''


def interface_list(interface):
    """Return a list of network interfaces."""
    return interface.get('networks', {}).keys()


def interface_info(interface):
    """Return a dict with detailed information about a network interface."""
    return {'id': interface_id(interface),
            'name': interface_name(interface)}


def interface_list_filtered(interface_list
----------
Solution: 

    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    return socket.inet_ntoa(
        fcntl.ioctl(
            sock.fileno(), 0x8915, struct.pack('256s', interface[:15])
        )[20:24]
    )
--------------------
Prompt: 
def coordination_geometry_symmetry_measures_sepplane_optim(self, coordination_geometry,
                                                               points_perfect=None,
                                                               nb_set=None, optimization=None):
        """
        Returns the symmetry measures of a given coordination_geometry for a set of permutations depending on
        the permutation setup. Depending on the parameters of the LocalGeometryFinder and on the coordination
         geometry, different methods are called.
        :param coordination_geometry: Coordination geometry for which the symmetry measures are looked for
        :return: the symmetry measures of a given coordination_geometry for a set of permutations
        :raise: NotImplementedError if the permutation_setup does not exists
        """
----------
Output: 
_geometry for a set of permutations
        """
        if points_perfect is None:
            points_perfect = self.points_perfect
        if nb_set is None:
            nb_set = self.nb_set
        if optimization is None:
            optimization = self.optimization
        if points_perfect is None:
            points_perfect = self.points_perfect
        if nb_set is None:
            nb_set = self.nb_set
        if points_perfect.is_empty():
            points_perfect = self.points_perfect
        if nb_set >
----------
Solution: 

        csms = []
        permutations = []
        algos = []
        local2perfect_maps = []
        perfect2local_maps = []
        for algo in coordination_geometry.algorithms:
            if algo.algorithm_type == SEPARATION_PLANE:
                cgsm = self.coordination_geometry_symmetry_measures_separation_plane_optim(
                    coordination_geometry,
                    algo,
                    points_perfect=points_perfect,
                    nb_set=nb_set,
                    optimization=optimization)
                csm, perm, algo, local2perfect_map, perfect2local_map = cgsm

                csms.extend(csm)
                permutations.extend(perm)
                algos.extend(algo)
                local2perfect_maps.extend(local2perfect_map)
                perfect2local_maps.extend(perfect2local_map)
        return csms, permutations, algos, local2perfect_maps, perfect2local_maps
--------------------
Prompt: 
def _export_table(table, fname):
    """Export DataFrame to .csv"""
----------
Output: 

    if not isinstance(table, pd.DataFrame):
        raise ValueError("table must be a pandas DataFrame")
    if fname.endswith(".csv"):
        return table.to_csv(fname, index=False)
    else:
        raise ValueError("fname must end with.csv")


def _export_dataframe(df, fname):
    """Export DataFrame to.csv"""
    if not isinstance(df, pd.DataFrame):
        raise ValueError("df must be a pandas DataFrame")
    if fname.endswith(".csv"):
        return df.to_csv(fname, index=False)
    else:
        raise ValueError
----------
Solution: 

    import os.path as op
    extension = op.splitext(fname.lower())[1]
    if extension == '':
        fname = fname + '.csv'
    table.to_csv(fname, index=None, sep=',', encoding='utf-8',
                 float_format='%.4f', decimal='.')
--------------------
Prompt: 
def get_holidays(self, division=None, year=None):
        """
        Gets a list of all known bank holidays, optionally filtered by division and/or year
        :param division: see division constants; defaults to common holidays
        :param year: defaults to all available years
        :return: list of dicts with titles, dates, etc
        """
----------
Output: 

        if division is None:
            division = self.division
        if year is None:
            year = self.year
        if division == 'all':
            return [
                {
                    'title': 'Banking',
                    'date': date,
                    'holiday_type': 'holiday',
                    'type': 'title',
                    'title_fr': title_fr,
                    'title_en': title_en,
                    'type_fr': 'type',
                    'type_en': 'type',
                    'type_en_fr': 'type',
                    'holiday_type_fr': 'type
----------
Solution: 

        if division:
            holidays = self.data[division]
        else:
            holidays = self.data[self.ENGLAND_AND_WALES]
            dates_in_common = six.moves.reduce(
                set.intersection,
                (set(map(lambda holiday: holiday['date'], division_holidays))
                 for division, division_holidays in six.iteritems(self.data))
            )
            holidays = filter(lambda holiday: holiday['date'] in dates_in_common, holidays)
        if year:
            holidays = filter(lambda holiday: holiday['date'].year == year, holidays)
        return list(holidays)
--------------------
Prompt: 
def add_all(self, items):
        """
        Adds the elements in the specified collection to this queue.

        :param items: (Collection), collection which includes the items to be added.
        :return: (bool), ``true`` if this queue is changed after call, ``false`` otherwise.
        """
----------
Output: 

        return self._add_all(True, items)

    def remove(self, items):
        """
        Removes the elements in the specified collection from this queue.

        :param items: (Collection), collection which includes the items to be removed.
        :return: (bool), ``true`` if this queue is changed to the empty collection, ``false`` otherwise.
        """
        return self._remove(True, items)

    def size(self):
        """
        Returns the number of elements in the queue.

        :return: (int), number of elements in the queue.
        """
        return self._size(
----------
Solution: 

        check_not_none(items, "Value can't be None")
        data_items = []
        for item in items:
            check_not_none(item, "Value can't be None")
            data_items.append(self._to_data(item))
        return self._encode_invoke(queue_add_all_codec, data_list=data_items)
--------------------
Prompt: 
def generate_slug(name):
        """Generate a slug for the knowledge.

        :param name: text to slugify
        :return: slugified text
        """
----------
Output: 

        slug = slugify(name)
        if not slug:
            return name
        # We want to return the slugified name with the number of characters in it
        slug = re.sub(r"[^\w\s-]", "", slug)
        # We want to return the slugified name with the number of characters in it
        slug = re.sub(r"[-\s]+", "", slug)
        # We want to return the slugified name with the number of characters in it
        slug = re.sub(r"[-\s]+", "", slug)
        # We want to return the slugified name
----------
Solution: 

        slug = slugify(name)

        i = KnwKB.query.filter(db.or_(
            KnwKB.slug.like(slug),
            KnwKB.slug.like(slug + '-%'),
        )).count()

        return slug + ('-{0}'.format(i) if i > 0 else '')
--------------------
Prompt: 
def execute(self, eopatch):
        """ Execute function which adds new vector layer to the EOPatch

        :param eopatch: input EOPatch
        :type eopatch: EOPatch
        :return: New EOPatch with added vector layer
        :rtype: EOPatch
        """
----------
Output: 

        if self.is_vector:
            self.add_vector_layer()
        else:
            raise NotImplementedError()

    def add_vector_layer(self):
        """ Add new vector layer to the layer list

        :return: New EOPatch with added vector layer
        :rtype: EOPatch
        """
        self.vector_layers.append(self.vector_layer)
        self.vector_layers_layer_index = len(self.vector_layers) - 1

    def add_vector_layer_from_file(self, filename):
        """ Add new vector layer from a file

        :
----------
Solution: 

        bbox_map = self._get_submap(eopatch)
        height, width = self._get_shape(eopatch)
        data_transform = rasterio.transform.from_bounds(*eopatch.bbox, width=width, height=height)

        if self.feature_name in eopatch[self.feature_type]:
            raster = eopatch[self.feature_type][self.feature_name].squeeze()
        else:
            raster = np.ones((height, width), dtype=self.raster_dtype) * self.no_data_value

        if not bbox_map.empty:
            rasterio.features.rasterize([(bbox_map.cascaded_union.buffer(0), self.raster_value)], out=raster,
                                        transform=data_transform, dtype=self.raster_dtype)

        eopatch[self.feature_type][self.feature_name] = raster[..., np.newaxis]

        return eopatch
--------------------
Prompt: 
def run_step_group(pipeline_definition, step_group_name, context):
    """Get the specified step group from the pipeline and run its steps."""
----------
Output: 

    step_group = pipeline_domain.StepGroup.from_yaml(step_group_yaml)
    return run_one_off_job(context, step_group)


def run_one_off_job(context, step_group):
    """Run the one-off MapReduce job."""
    job_id = model.Mapreduce.get_new_job_id()
    logging.info('Running MapReduce job %s.', job_id)
    job_class = model.MapreduceWorkItem

    # Create a new job.
    job_id = job_class(
       
----------
Solution: 

    logger.debug(f"starting {step_group_name}")
    assert step_group_name

    steps = get_pipeline_steps(pipeline=pipeline_definition,
                               steps_group=step_group_name)

    run_pipeline_steps(steps=steps, context=context)

    logger.debug(f"done {step_group_name}")
--------------------
Prompt: 
def catch_gzip_errors(f):
    """
    A decorator to handle gzip encoding errors which have been known to
    happen during hydration.
    """
----------
Output: 

    def wrapper(request, *args, **kwargs):
        try:
            return f(request, *args, **kwargs)
        except Exception, e:
            return HttpResponseServerError(e)
        except Exception, e:
            return HttpResponseServerError()
    return wrapper


def catch_errors_gzip(f):
    """
    A decorator to handle errors which have been known to
    happen during hydration.
    """
    def wrapper(request, *args, **kwargs):
        try:
            return f(request, *args, **kwargs)
        except Exception, e:
            return HttpResponseServerError(e)
       
----------
Solution: 

    def new_f(self, *args, **kwargs):
        try:
            return f(self, *args, **kwargs)
        except requests.exceptions.ContentDecodingError as e:
            log.warning("caught gzip error: %s", e)
            self.connect()
            return f(self, *args, **kwargs)
    return new_f
--------------------
Prompt: 
def forms(self, req, tag):
        """
        Make and return some forms, using L{self.parameter.getInitialLiveForms}.

        @return: some subforms.
        @rtype: C{list} of L{LiveForm}
        """
----------
Output: 

        return self.parameter.getInitialLiveForms(tag)



class IAuthenticator(Interface):
    """
    An authentication plugin for HTTP Digest authentication.

    @since: 10.2
    """
    def requestAvatar(self, avatarId, mind, *interfaces):
        """
        Request a C{avatarId} avatar.

        @param avatarId: the C{avatarId} to request the avatar with.
        @type avatarId: C{str}

        @param mind: the authentication mechanism to use.
        @type mind: L{ICredentials} provider

        @param interfaces: a list
----------
Solution: 

        liveForms = self.parameter.getInitialLiveForms()
        for liveForm in liveForms:
            liveForm.setFragmentParent(self)
        return liveForms
--------------------
Prompt: 
def get_insider_roster(self):
        """Insider Roster

        Returns the top 10 insiders, with the most recent information.

        Reference: https://iexcloud.io/docs/api/#insider-roster

        Data Weighting: ``5000`` per symbol

        Returns
        -------
        list or pandas.DataFrame
            Stocks Insider Roster Endpoint data
        """
----------
Output: 

        params = {'q': '1', 'type': 'roster', 'page': '1'}
        return self.get_data(params)

    def get_insider_time(self):
        """Insider Time

        Returns the top 10 insiders, with the most recent information.

        Reference: https://iexcloud.io/docs/api/#insider-time

        Data Weighting: ``5000`` per symbol

        Returns
        -------
        list or pandas.DataFrame
            Stocks Insider Time data
        """
        params = {'type':'symbol', 'page': '1'}
       
----------
Solution: 

        def fmt_p(out):
            out = {(symbol, owner["entityName"]): owner
                   for symbol in out
                   for owner in out[symbol]}
            return pd.DataFrame(out)

        return self._get_endpoint("insider-roster", fmt_p=fmt_p)
--------------------
Prompt: 
def create_intent(project_id, display_name, training_phrases_parts,
                  message_texts):
    """Create an intent of the given intent type."""
----------
Output: 

    intent_type = Intent.get_by_id(project_id, intent_type_id)
    if not intent_type:
        intent_type = Intent(
            id=project_id,
            display_name=display_name,
            training_phrases_parts=training_phrases_parts,
            message_phrases=message_phrases)
        db.session.add(project_intent)
        db.session.commit()
    return intent_type.to_dict()


def create_question(question_text, author_id, content):
    """
----------
Solution: 

    import dialogflow_v2 as dialogflow
    intents_client = dialogflow.IntentsClient()

    parent = intents_client.project_agent_path(project_id)
    training_phrases = []
    for training_phrases_part in training_phrases_parts:
        part = dialogflow.types.Intent.TrainingPhrase.Part(
            text=training_phrases_part)
        # Here we create a new training phrase for each provided part.
        training_phrase = dialogflow.types.Intent.TrainingPhrase(parts=[part])
        training_phrases.append(training_phrase)

    text = dialogflow.types.Intent.Message.Text(text=message_texts)
    message = dialogflow.types.Intent.Message(text=text)

    intent = dialogflow.types.Intent(
        display_name=display_name,
        training_phrases=training_phrases,
        messages=[message])

    response = intents_client.create_intent(parent, intent)

    print('Intent created: {}'.format(response))
--------------------
Prompt: 
def parse_type_str(expected_base=None, with_arrlist=False):
    """
    Used by BaseCoder subclasses as a convenience for implementing the
    ``from_type_str`` method required by ``ABIRegistry``.  Useful if normalizing
    then parsing a type string with an (optional) expected base is required in
    that method.
    """
----------
Output: 

    if expected_base is None:
        expected_base = expected_module + "." + options.module

    # If the test is run from the command line, then return the parsed module.
    if with_arrlist:
        return [options.module] + _parse_arrlist(options.arrlist)
    else:
        return options.module

def _parse_arrlist(arrlist):
    """
    Given a list of strings, parse out the elements of the array
    """
    if with_arrlist:
        return [x.strip() for x in arrlist.split(",")]
   
----------
Solution: 

    def decorator(old_from_type_str):
        @functools.wraps(old_from_type_str)
        def new_from_type_str(cls, type_str, registry):
            normalized_type_str = normalize(type_str)
            abi_type = parse(normalized_type_str)

            type_str_repr = repr(type_str)
            if type_str != normalized_type_str:
                type_str_repr = '{} (normalized to {})'.format(
                    type_str_repr,
                    repr(normalized_type_str),
                )

            if expected_base is not None:
                if not isinstance(abi_type, BasicType):
                    raise ValueError(
                        'Cannot create {} for non-basic type {}'.format(
                            cls.__name__,
                            type_str_repr,
                        )
                    )
                if abi_type.base != expected_base:
                    raise ValueError(
                        'Cannot create {} for type {}: expected type with '
                        "base '{}'".format(
                            cls.__name__,
                            type_str_repr,
                            expected_base,
                        )
                    )

            if not with_arrlist and abi_type.arrlist is not None:
                raise ValueError(
                    'Cannot create {} for type {}: expected type with '
                    'no array dimension list'.format(
                        cls.__name__,
                        type_str_repr,
                    )
                )
            if with_arrlist and abi_type.arrlist is None:
                raise ValueError(
                    'Cannot create {} for type {}: expected type with '
                    'array dimension list'.format(
                        cls.__name__,
                        type_str_repr,
                    )
                )

            # Perform general validation of default solidity types
            abi_type.validate()

            return old_from_type_str(cls, abi_type, registry)

        return classmethod(new_from_type_str)

    return decorator
--------------------
Prompt: 
def read_calibration(
        detx=None, det_id=None, from_file=False, det_id_table=None
):
    """Retrive calibration from file, the DB."""
----------
Output: 

    if detx is None:
        detx = get_detx_from_file(from_file)
    if det_id is None:
        det_id = get_detx_from_file(from_file)
    if det_id_table is None:
        detx_id = detx.get_detx_id_table()
    if detx_id is None:
        raise ValueError("No detx id table found")
    if det_id is None:
        raise ValueError("No detector id table found")
    if detx.is_detector_id:
       
----------
Solution: 

    from km3pipe.calib import Calibration    # noqa

    if not (detx or det_id or from_file):
        return None
    if detx is not None:
        return Calibration(filename=detx)
    if from_file:
        det_ids = np.unique(det_id_table)
        if len(det_ids) > 1:
            log.critical("Multiple detector IDs found in events.")
        det_id = det_ids[0]
    if det_id is not None:
        if det_id < 0:
            log.warning(
                "Negative detector ID found ({0}). This is a MC "
                "detector and cannot be retrieved from the DB.".format(det_id)
            )
            return None
        return Calibration(det_id=det_id)
    return None
--------------------
Prompt: 
def add_ip_range(self, id_environment, id_ip_config):
        """Makes relationship of environment with ip config and returns your id.

        :param id_environment: Environment ID.
        :param id_ip_config: IP Configuration ID.

        :return: Following dictionary:

        {'config_do_ambiente': {'id_config_do_ambiente': < id_config_do_ambiente >}}

        :raise InvalidParameterError: Some parameter was invalid.
        :raise ConfigEnvironmentDuplicateError: Error saving duplicate Environment Configuration.
        :raise DataBaseError: Networkapi failed to access the database.
        :raise XMLError: Networkapi failed to generate the XML response.
        """
----------
Output: 
Error: DataBase error.
        """
        self._logger.debug("add_ip_range: id_environment=%s, id_ip_config=%s", id_environment, id_ip_config)
        self.id_environment[id_environment] = id_ip_config
        return self.id_environment

    def get_ip_range(self, id_ip_do):
        """Gets the relationship of the environment with ip do.

        :param id_ip_do: Id of the relationship.

        :return: Following dictionary:

        {'do_id': {'id_
----------
Solution: 


        environment_map = dict()
        environment_map['id_environment'] = id_environment
        environment_map['id_ip_config'] = id_ip_config

        code, xml = self.submit(
            {'ambiente': environment_map}, 'POST', 'ipconfig/')

        return self.response(code, xml)
--------------------
Prompt: 
def _get_grad_method(self, data):
        r"""Get the gradient

        This method calculates the gradient step from the input data

        Parameters
        ----------
        data : np.ndarray
            Input data array

        Notes
        -----
        Implements the following equation:

        .. math::
            \nabla F(x) = \mathbf{H}^T(\mathbf{H}\mathbf{x} - \mathbf{y})

        """
----------
Output: 

        if self.grad_method is None:
            self.grad_method = None
            return None
        else:
            return self.grad_method

    def _get_grad_method_grad(self, data):
        r"""Calculate the gradient step

        This method calculates the gradient step from the input data

        Parameters
        ----------
        data : np.ndarray
            Gradient data array

        Notes
        -----
        Implements the following equation:

       .. math::
            \nabla F(x) = \mathbf{H}^T(\mathbf{H}\mathbf{x} - \mathbf{y})

        """
       
----------
Solution: 


        self.grad = self.trans_op(self.op(data) - self.obs_data)
--------------------
Prompt: 
def iri(uri_string):
        """converts a string to an IRI or returns an IRI if already formated

        Args:
            uri_string: uri in string format

        Returns:
            formated uri with <>
        """
----------
Output: 

        if not uri_string:
            return None
        return iri_from_uri(uri_string, self.iri_format)

    def iri_format(self, uri):
        """converts a URI to a URI if already formated

        Args:
            uri: URI in string format

        Returns:
            formated URI with <>
        """
        if not uri:
            return None
        return iri_from_uri(uri, self.iri_format)

    def get_uri_path(self, uri):
        """returns a path for a URI if already formated

        Args:
            uri: URI in string
----------
Solution: 

        uri_string = str(uri_string)
        if uri_string[:1] == "?":
            return uri_string
        if uri_string[:1] == "[":
            return uri_string
        if uri_string[:1] != "<":
            uri_string = "<{}".format(uri_string.strip())
        if uri_string[len(uri_string)-1:] != ">":
            uri_string = "{}>".format(uri_string.strip())
        return uri_string
--------------------
Prompt: 
def write_record(self, event_str):
        """Writes a serialized event to file."""
----------
Output: 

        self.writer.write(self.encode_event(event_str))

    def encode_event(self, event):
        """Encodes an event as a JSON string."""
        return json.dumps(event)

    def decode_event(self, event_str):
        """Decodes an event as a string."""
        return json.loads(event_str)

    def write_record_batch(self, batch):
        """Writes a batch of events to file."""
        self.writer.write(self.encode_event_batch(batch))

    def encode_event_batch(self, batch
----------
Solution: 

        header = struct.pack('Q', len(event_str))
        header += struct.pack('I', masked_crc32c(header))
        footer = struct.pack('I', masked_crc32c(event_str))
        self._writer.write(header + event_str + footer)
--------------------
Prompt: 
def status_count(self, project):
        """
        return a dict
        """
----------
Output: 

        return {
            'projects': {
                'total': 0,
                'failed': 0,
                'passed': 0,
               'skipped': 0,
                'failed_ratio': 0,
                'passed_ratio': 0,
                'failed_ratio': 0,
               'skipped_ratio': 0,
                'failed_ratio': 0,
                'failed_ratio_ratio': 0,
                'failed_ratio_ratio': 0,
            },
            'total': len(self.projects),
            'failed': len(self.failed_projects),
            'passed': len(self.passed_projects),
----------
Solution: 

        pipe = self.redis.pipeline(transaction=False)
        for status in range(1, 5):
            pipe.scard(self._gen_status_key(project, status))
        ret = pipe.execute()

        result = {}
        for status, count in enumerate(ret):
            if count > 0:
                result[status + 1] = count
        return result
--------------------
Prompt: 
def gen_ascii_docs(src='fastai'):
    """Generate documentation for fastai library in HTML (asciidoctor required)
    :param str src: The absolute/relative path of source file/dir
    """
----------
Output: 

    if src == 'fastai':
        return """
            <html>
                <head>
                    <title>
                        <h1>fastai</h1>
                    </title>
                </head>
            </html>
        """
    else:
        return """
            <html>
                <head>
                    <title>
                        <h1>fastai</h1>
                    </title>
                </head>
            </html>
        """

----------
Solution: 

    os.chdir(Path(__file__).absolute().parent)
    with working_directory('..'):
        path = Path(src)
        if path.is_dir():
            file_paths = list(path.glob('**/*.py'))
        else:
            file_paths = [path]

    pat = re.compile('^(?!__init__).*.py\Z')
    for file_path in file_paths:
        if pat.match(file_path.name):
            file_path.parent.mkdir(parents=True, exist_ok=True)
            with working_directory('..'):
                tmpl_str = parse_module(file_path)

            (file_path.parent/(file_path.name.rsplit('.',1)[0] + '.adoc.tmpl')).write_text(tmpl_str)
            (file_path.parent/(file_path.name.rsplit('.',1)[0] + '.adoc')).write_text(re.sub(r"{{(.*?)}}", parse_tmpl, tmpl_str, flags=re.DOTALL))
    if path.is_dir():
        subprocess.call(['asciidoctor', str(path) + '/**/*.adoc'])
    else:
        subprocess.call(['asciidoctor', str(path).rsplit('.',1)[0] + '.adoc'])
--------------------
Prompt: 
def remove_users_from_user_group(self, id, **kwargs):  # noqa: E501
        """Remove multiple users from a specific user group  # noqa: E501

          # noqa: E501
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.remove_users_from_user_group(id, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str id: (required)
        :param list[str] body: List of users that should be removed from user group
        :return: ResponseContainerUserGroup
                 If the method is called asynchronously,
                 returns the request thread.
        """
----------
Output: 
: (required)
        :return: None
                 If the method is called asynchronously,
                 returns the request thread.
        """
        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.delete_users_with_http_info(id, **kwargs)  # noqa: E501
        else:
            (data) = self.delete_users_with_http_info.metadata['url_map']['deleteUsersFromUserGroupRequest']  # noqa: E501
            return_data = self._deserialize('str', body,'str')
----------
Solution: 

        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.remove_users_from_user_group_with_http_info(id, **kwargs)  # noqa: E501
        else:
            (data) = self.remove_users_from_user_group_with_http_info(id, **kwargs)  # noqa: E501
            return data
--------------------
Prompt: 
def get_asset_content_form_for_update(self, asset_content_id=None):
        """Gets the asset form for updating content for an existing asset.

        A new asset content form should be requested for each update
        transaction.

        :param asset_content_id: the ``Id`` of the ``AssetContent``
        :type asset_content_id: ``osid.id.Id``
        :return: the asset content form
        :rtype: ``osid.repository.AssetContentForm``
        :raise: ``NotFound`` -- ``asset_content_id`` is not found
        :raise: ``NullArgument`` -- ``asset_content_id`` is ``null``
        :raise: ``OperationFailed`` -- unable to complete request

        *compliance: mandatory -- This method must be implemented.*

        """
----------
Output: 

        :raise: ``NullArgument`` -- ``asset_content_id`` is ``null``
        :raise: ``OperationFailed`` -- unable to complete request
        :raise: ``PermissionDenied`` -- authorization failure

        *compliance: mandatory -- This method must be implemented.*

        """
        return AssetContentForm

    def can_delete_assets(self):
        """Tests if this user can delete ``Assets``.

        A return of true does not guarantee successful authorization. A
        return of false indicates that it is known deleting an ``Asset``
        will result in a ``PermissionDenied``. This is intended as a
        hint
----------
Solution: 

        if asset_content_id is None:
            raise NullArgument()
        asset = None
        for a in AssetLookupSession(self._repository_id,
                                    proxy=self._proxy,
                                    runtime=self._runtime).get_assets():
            # might want to set plenary view
            # to assure ordering?
            for ac in a.get_asset_contents():
                if ac.get_id() == asset_content_id:
                    asset = a
                    asset_content = ac
        if asset is None:
            raise NotFound()
        asset_content_form = objects.AssetContentForm(asset_content._my_map, asset_id=asset.get_id())
        self._forms[asset_content_form.get_id().get_identifier()] = not UPDATED
        return asset_content_form
--------------------
Prompt: 
def start(self, argv):
        """
        Start BenchExec.
        @param argv: command-line options for BenchExec
        """
----------
Output: 

        self.parser.add_option('-c', '--config', action='store', type='string', dest='config',
                               default=None,
                               help='config file to use')
        self.parser.add_option('-d', '--debug', action='store_true', dest='debug',
                               default=False,
                               help='debug mode')
        self.parser.add_option('-e', '--exit', action='store', type='string', dest='exit',
                               default=None,
                               help='exit code')
        self.parser.add_option('-f', '--force', action='store
----------
Solution: 

        parser = self.create_argument_parser()
        self.config = parser.parse_args(argv[1:])

        for arg in self.config.files:
            if not os.path.exists(arg) or not os.path.isfile(arg):
                parser.error("File {0} does not exist.".format(repr(arg)))

        if os.path.isdir(self.config.output_path):
            self.config.output_path = os.path.normpath(self.config.output_path) + os.sep

        self.setup_logging()

        self.executor = self.load_executor()

        returnCode = 0
        for arg in self.config.files:
            if self.stopped_by_interrupt:
                break
            logging.debug("Benchmark %r is started.", arg)
            rc = self.execute_benchmark(arg)
            returnCode = returnCode or rc
            logging.debug("Benchmark %r is done.", arg)

        logging.debug("I think my job is done. Have a nice day!")
        return returnCode
--------------------
Prompt: 
def new_module(name):
    """
    Do all of the gruntwork associated with creating a new module.

    """
----------
Output: 

    if name in sys.modules:
        return sys.modules[name]
    else:
        mod = types.ModuleType(name)
        mod.__loader__ = None
        mod.__package__ = None
        mod.__loader__ = None
        mod.__path__ = []
        return mod

def _run_code(code, mod, ns, ns_map, module_name):
    """
    Run code in a namespace.

    """
    # This function is called from _pytest.main()
    if code.co_flags & inspect.CO_VARARGS:
        return _pytest.runtest_inner
----------
Solution: 

    parent = None
    if '.' in name:
        parent_name = name.rsplit('.', 1)[0]
        parent = __import__(parent_name, fromlist=[''])

    module = imp.new_module(name)
    sys.modules[name] = module
    if parent:
        setattr(parent, name.rsplit('.', 1)[1], module)
    return module
--------------------
Prompt: 
def _getPowerupInterfaces(self):
        """
        Collect powerup interfaces this object declares that it can be
        installed on.
        """
----------
Output: 

        return self._powerupInterfaces

    def getPowerupInterfaces(self):
        """
        Return a list of the powerup interfaces installed on this system.
        """
        return self._powerupInterfaces.values()

    def getPowerupTime(self):
        """
        Return the time the powerup was installed.
        """
        return self._powerupTime

    def getPowerupTime(self):
        """
        Return the time the powerup was started.
        """
        return self._powerupTimeStart

    def getPowerupTime(self):
        """
        Return the time the powerup was started.
        """
----------
Solution: 

        powerupInterfaces = getattr(self.__class__, "powerupInterfaces", ())
        pifs = []
        for x in powerupInterfaces:
            if isinstance(x, type(Interface)):
                #just an interface
                pifs.append((x, 0))
            else:
                #an interface and a priority
                pifs.append(x)

        m = getattr(self, "__getPowerupInterfaces__", None)
        if m is not None:
            pifs = m(pifs)
            try:
                pifs = [(i, p) for (i, p) in pifs]
            except ValueError:
                raise ValueError("return value from %r.__getPowerupInterfaces__"
                                 " not an iterable of 2-tuples" % (self,))
        return pifs
--------------------
Prompt: 
def create_non_data_file(self, params, file_data):
        """
        Creates a new file-based dataset with the name provided in the files
        tuple.  A valid file input would be:
        files = (
            {'file': ("gtfs2", open('myfile.zip', 'rb'))}
        )
        """
----------
Output: 

        if not isinstance(params, dict):
            raise ValueError("params must be a dict")
        if not isinstance(file_data, (Dataset, list)):
            raise ValueError("file_data must be a list")
        if len(files) == 0:
            raise ValueError("no files to create")
        if len(file_data) == 1:
            file_data = file_data[0]
        else:
            raise ValueError("multiple files to create")
        return Dataset(file_data, params)

    def create_invalid_file(self, file_data):
        """
        Creates a new file
----------
Solution: 

        api_prefix = '/api/imports2/'

        if not params.get('method', None):
            params['method'] = 'blob'

        return self._perform_request("post", api_prefix, params=params, files=file_data)
--------------------
Prompt: 
def active(self):
        """ returns a dictionary of connections set as active.
        """
----------
Output: 

        return self._connections

    def get_connection(self, key):
        """ returns a connection set for the given key.
        """
        return self._connections[key]

    def get_connections(self, key):
        """ returns a list of connections set as active.
        """
        return self._connections.get(key, [])

    def get_connection_by_id(self, key):
        """ returns a connection with a given id.
        """
        return self._connections[key]

    def get_connection_by_id_and_name(self, key):
        """ returns a connection with
----------
Solution: 

        return {key: value for key, value in self.conns.items()
                if value.active}
--------------------
Prompt: 
def normalize_package_path(cls, package_path):
    """Returns a normalized version of the given package path.

    The root package might by denoted by '' or '.' and is normalized to ''.

    All other packages are of the form 'path' or 'path/subpath', etc.

    If the given path is either absolute or relative (includes the parent dir path signifier, '..'),
    then an error is raised since the path cannot be noramlized.

    :param string package_path: The Go package path to normalize.
    :raises: `ValueError` if the package path cannot be normalized.
    """
----------
Output: 
 normalized to a valid
        version.
    """
    if not package_path:
        raise ValueError('Invalid Go package path: empty string')

    if not os.path.isabs(package_path):
        raise ValueError('Invalid Go package path: invalid path')

    if not os.path.normpath(package_path).startswith(os.path.sep):
        raise ValueError('Invalid Go package path: invalid path')

    if not os.path.isdir(package_path):
        raise ValueError('Invalid Go package path: invalid path')

    if os.path.basename(package_path) == '':
        raise ValueError
----------
Solution: 

    if package_path.startswith(os.pardir + os.sep):
      raise ValueError('Relative package paths are not allowed. Given: {!r}'.format(package_path))
    if os.path.isabs(package_path):
      raise ValueError('Absolute package paths are not allowed. Given: {!r}'.format(package_path))
    return '' if not package_path or package_path == os.curdir else package_path.lstrip('/')
--------------------
Prompt: 
def _set_pwm(self, raw_values):
        """
        Set pwm values on the controlled pins.

        :param raw_values: Raw values to set (0-4095).
        """
----------
Output: 

        if raw_values == 0:
            self.write_register(0x00)
        elif raw_values == 4095:
            self.write_register(0x01)
        else:
            raise ValueError("Invalid pwm value: %d" % raw_values)

    def _set_pwm_mode(self, mode):
        """
        Set the PWM mode.

        :param mode: PWM mode (0-4095).
        """
        if mode == 0x00:
            self.write_register(0x01)
        elif mode == 0x01:
            self.write
----------
Solution: 

        for i in range(len(self._pins)):
            self._device.set_pwm(self._pins[i], 0, raw_values[i])
--------------------
Prompt: 
def get(self, **kwargs):
        """Find correct filterchain based on generic variables
        """
----------
Output: 

        return self.filterchain

    def _get_filter_chain(self):
        """Find the filter chain for the given filter
        """
        return self.filterchain

    def _get_filter_chain_id(self):
        """Find the filter chain ID for the given filter
        """
        return self.filterchain_id

    def _get_filter_chain_name(self):
        """Find the filter chain name for the given filter
        """
        return self.filterchain_name

    def _get_filter_chain_name_id(self):
        """Find the filter chain ID for the given
----------
Solution: 

        for chain in self.chains:
            for key in kwargs:
                getter_name = "get_"+key
                # scan all possible getters
                if (hasattr(chain, getter_name)):
                    getter = getattr(chain, getter_name) # e.g. "get_address"
                    if (getter() == kwargs[key]):
                        return chain
        return None
--------------------
Prompt: 
def select_servers(self,
                       selector,
                       server_selection_timeout=None,
                       address=None):
        """Return a list of Servers matching selector, or time out.

        :Parameters:
          - `selector`: function that takes a list of Servers and returns
            a subset of them.
          - `server_selection_timeout` (optional): maximum seconds to wait.
            If not provided, the default value common.SERVER_SELECTION_TIMEOUT
            is used.
          - `address`: optional server address to select.

        Calls self.open() if needed.

        Raises exc:`ServerSelectionTimeoutError` after
        `server_selection_timeout` if no matching servers are found.
        """
----------
Output: 
` on timeout.
        """
        if server_selection_timeout is None:
            server_selection_timeout = self.default_timeout
        if address is None:
            address = self.address
        return [x for x in self.open(selector, server_selection_timeout)
                if selector in x[0] and x[1] == x[2]]

    def _select_servers(self,
                       servers,
                       server_selection_timeout=None,
                       address=None):
        """Return a list of Servers matching selector, or time out.

        :Parameters:
          - `servers`: list
----------
Solution: 

        if server_selection_timeout is None:
            server_timeout = self._settings.server_selection_timeout
        else:
            server_timeout = server_selection_timeout

        with self._lock:
            server_descriptions = self._select_servers_loop(
                selector, server_timeout, address)

            return [self.get_server_by_address(sd.address)
                    for sd in server_descriptions]
--------------------
Prompt: 
def convert2(self, imtls, sids):
        """
        Convert a probability map into a composite array of shape (N,)
        and dtype `imtls.dt`.

        :param imtls:
            DictArray instance
        :param sids:
            the IDs of the sites we are interested in
        :returns:
            an array of curves of shape (N,)
        """
----------
Output: 

        if not self.is_empty:
            sids = {sid: sid.dt for sid in sids}
        return imtls, sids

    def convert_to_iid(self, iid, sids):
        """
        Convert a probability map into a composite array of shape (N,)
        and dtype `imtls.iid.dtype`.

        :param iid:
            DictArray instance
        :param sids:
            the IDs of the sites we are interested in
        :returns:
            an array of iid of dtype dtype (N,)
        """
        if not self.is
----------
Solution: 

        assert self.shape_z == 1, self.shape_z
        curves = numpy.zeros(len(sids), imtls.dt)
        for imt in curves.dtype.names:
            curves_by_imt = curves[imt]
            for i, sid in numpy.ndenumerate(sids):
                try:
                    pcurve = self[sid]
                except KeyError:
                    pass  # the poes will be zeros
                else:
                    curves_by_imt[i] = pcurve.array[imtls(imt), 0]
        return curves
--------------------
Prompt: 
def release(self, shortname):
        """
        Get a specific release by its shortname.

        :param shortname: str, eg. "ceph-3-0"
        :returns: deferred that when fired returns a Release (Munch, dict-like)
                  object representing this release.
        :raises: ReleaseNotFoundException if this release does not exist.
        """
----------
Output: 

        return self._release(shortname)

    def _release(self, shortname):
        """
        Get a specific release by its shortname.

        :param shortname: str, eg. "ceph-3-0"
        :returns: deferred that when fired returns a Release (Munch, dict-like)
                  object representing this release.
        :raises: ReleaseNotFoundException if this release does not exist.
        """
        return defer.succeed(None)

    def _get_release_by_id(self, release_id):
        """
        Get a specific release by its ID.

        :param release
----------
Solution: 

        url = 'api/v6/releases/?shortname=%s' % shortname
        releases = yield self._get(url)
        # Note, even if this shortname does not exist, _get() will not errback
        # for this url. It simply returns an empty list. So check that here:
        if not releases:
            raise ReleaseNotFoundException('no release %s' % shortname)
        release = Release.fromDict(releases[0])
        release.connection = self
        defer.returnValue(release)
--------------------
Prompt: 
def WriteEventBody(self, event):
    """Writes the body of an event object to the output.

    Args:
      event (EventObject): event.
    """
----------
Output: 

    if not self.IsValidEvent(event):
      return

    self.WriteStruct(event.struct)

    if self.IsValidEvent(event_data_stream.EventDataStream):
      self.WriteDataStreamStartCode(event_data_stream.EventDataStream)

    if self.IsValidEvent(event_data_stream.EventDataStream):
      self.WriteDataStreamEndCode(event_data_stream.EventDataStream)

    if self.IsValidEvent(event_data_stream.EventDataStream):
      self.WriteDataStreamStartCode(event_data_
----------
Solution: 

    inode = getattr(event, 'inode', None)
    if inode is None:
      event.inode = 0

    try:
      message, _ = self._output_mediator.GetFormattedMessages(event)
    except errors.WrongFormatter:
      message = None

    if message:
      event.message = message

    json_dict = self._JSON_SERIALIZER.WriteSerializedDict(event)
    json_string = json.dumps(json_dict, sort_keys=True)
    # dumps() returns an ascii-encoded byte string in Python 2.
    if py2to3.PY_2:
      json_string = codecs.decode(json_string, 'ascii')
    self._output_writer.Write(json_string)
    self._output_writer.Write('\n')
--------------------
Prompt: 
def to_naf(self):
        """
        Converts the object to NAF
        """
----------
Output: 

        return NAF(self.name, self.value)

    def __str__(self):
        return self.to_naf()

    def __repr__(self):
        return self.to_naf()

class NAF(object):
    """
    Represents a single NAF (NAF) object
    """
    def __init__(self, name, value):
        self.name = name
        self.value = value
        self.naf_type = None
        self.naf_value = None

    def __str__(self):
        return self.to_naf()

    def __
----------
Solution: 

        if self.type == 'KAF':
            self.type = 'NAF'
            for node in self.__get_wf_nodes():
                node.set('id',node.get('wid'))
                del node.attrib['wid']
--------------------
Prompt: 
def main():
    """Return 0 on success."""
----------
Output: 

    return 0

if __name__ == '__main__':
    sys.exit(main())

----------
Solution: 

    args = parse_args()

    if not args.files:
        return 0

    with enable_sphinx_if_possible():
        status = 0
        pool = multiprocessing.Pool(multiprocessing.cpu_count())
        try:
            if len(args.files) > 1:
                results = pool.map(
                    _check_file,
                    [(name, args) for name in args.files])
            else:
                # This is for the case where we read from standard in.
                results = [_check_file((args.files[0], args))]

            for (filename, errors) in results:
                for error in errors:
                    line_number = error[0]
                    message = error[1]

                    if not re.match(r'\([A-Z]+/[0-9]+\)', message):
                        message = '(ERROR/3) ' + message

                    output_message('{}:{}: {}'.format(filename,
                                                      line_number,
                                                      message))

                    status = 1
        except (IOError, UnicodeError) as exception:
            output_message(exception)
            status = 1

        return status
--------------------
Prompt: 
def resolve_placeholders(path, placeholder_dict):
    """
    **Purpose**: Substitute placeholders in staging attributes of a Task with actual paths to the corresponding tasks.

    :arguments:
        :path: string describing the staging paths, possibly containing a placeholder
        :placeholder_dict: dictionary holding the values for placeholders

    """
----------
Output: 

    if path is None:
        return None
    if path.startswith('http'):
        return 'http://{}{}'.format(settings.SERVER_HOST, settings.SERVER_PORT, path)
    if path.startswith('file'):
        return 'file://{}{}'.format(settings.MEDIA_ROOT, path)
    if path.startswith('http'):
        return 'http://{}{}'.format(settings.SERVER_HOST, path)
    if path.startswith('file'):
        return 'file://{}{}'.format(settings.MEDIA_ROOT, path)
    if path.startswith('file'):
        return 'file://
----------
Solution: 


    try:

        if isinstance(path, unicode):
            path = str(path)

        if not isinstance(path, str):
            raise TypeError(expected_type=str, actual_type=type(path))

        if '$' not in path:
            return path

        # Extract placeholder from path
        if len(path.split('>')) == 1:
            placeholder = path.split('/')[0]
        else:
            if path.split('>')[0].strip().startswith('$'):
                placeholder = path.split('>')[0].strip().split('/')[0]
            else:
                placeholder = path.split('>')[1].strip().split('/')[0]

        # SHARED
        if placeholder == "$SHARED":
            return path.replace(placeholder, 'pilot://')

        # Expected placeholder format:
        # $Pipeline_{pipeline.uid}_Stage_{stage.uid}_Task_{task.uid}

        broken_placeholder = placeholder.split('/')[0].split('_')

        if not len(broken_placeholder) == 6:
            raise ValueError(
                obj='placeholder',
                attribute='task',
                expected_value='$Pipeline_(pipeline_name)_Stage_(stage_name)_Task_(task_name) or $SHARED',
                actual_value=broken_placeholder)

        pipeline_name = broken_placeholder[1]
        stage_name = broken_placeholder[3]
        task_name = broken_placeholder[5]
        resolved_placeholder = None

        if pipeline_name in placeholder_dict.keys():
            if stage_name in placeholder_dict[pipeline_name].keys():
                if task_name in placeholder_dict[pipeline_name][stage_name].keys():
                    resolved_placeholder = path.replace(placeholder, placeholder_dict[
                                                        pipeline_name][stage_name][task_name]['path'])
                else:
                    logger.warning('%s not assigned to any task in Stage %s Pipeline %s' %
                                   (task_name, stage_name, pipeline_name))
            else:
                logger.warning('%s not assigned to any Stage in Pipeline %s' % (
                    stage_name, pipeline_name))
        else:
            logger.warning('%s not assigned to any Pipeline' % (pipeline_name))

        if not resolved_placeholder:
            logger.warning('No placeholder could be found for task name %s \
                        stage name %s and pipeline name %s. Please be sure to \
                        use object names and not uids in your references,i.e, \
                        $Pipeline_(pipeline_name)_Stage_(stage_name)_Task_(task_name)')
            raise ValueError(
                obj='placeholder',
                attribute='task',
                expected_value='$Pipeline_(pipeline_name)_Stage_(stage_name)_Task_(task_name) or $SHARED',
                actual_value=broken_placeholder)

        return resolved_placeholder

    except Exception, ex:

        logger.exception('Failed to resolve placeholder %s, error: %s' %(path, ex))
        raise
--------------------
Prompt: 
def unregister(self, entry_point):
        """Unregister a provider

        :param str entry_point: provider to unregister (entry point syntax).

        """
----------
Output: 

        self.log.debug("Unregistering %s", entry_point)
        self.entries.remove(entry_point)
        self.log.debug("Entry point %s removed", entry_point)

    def get_entry_point(self, entry_point):
        """Get a :class:`EntryPoint` for the given entry point

        :param str entry_point: entry point syntax.

        """
        return entry_point.copy()

    def get_entry_map(self, entry_point):
        """Get a :class:`EntryPointMap` for the given entry point

        :param str entry
----------
Solution: 

        if entry_point not in self.registered_extensions:
            raise ValueError('Extension not registered')

        ep = EntryPoint.parse(entry_point)
        self.registered_extensions.remove(entry_point)
        if self._extensions_by_name is not None:
            del self._extensions_by_name[ep.name]
        for i, ext in enumerate(self.extensions):
            if ext.name == ep.name:
                del self.extensions[i]
                break
--------------------
Prompt: 
def as_dict(self):
        """
        Return a Listing object as Dictionary
        :return: dict
        """
----------
Output: 

        return {
            'id': self.id,
            'name': self.name,
            'description': self.description,
            'created_at': self.created_at,
            'updated_at': self.updated_at,
            'created_by': self.created_by.name,
            'project_id': self.project.id,
           'status': self.status,
            'type': self.type,
            'user_id': self.user.id,
            'project_id': self.project.id,
            'domain_id': self.domain.id
----------
Solution: 

        return {
            'search_type': self.search_type,
            'agent_id': self.agent_id,
            'id': self.id,
            'price': self.price,
            'price_change': self.price_change,
            'viewings': self.upcoming_viewings,
            'facilities': self.facilities,
            'overviews': self.overviews,
            'formalised_address': self.formalised_address,
            'address_line_1': self.address_line_1,
            'county': self.county,
            'listing_image': self.images,
            'listing_hires_image': self.hires_images,
            'agent': self.agent,
            'agent_url': self.agent_url,
            'contact_number': self.contact_number,
            'daft_link': self.daft_link,
            'shortcode': self.shortcode,
            'date_insert_update': self.date_insert_update,
            'views': self.views,
            'description': self.description,
            'dwelling_type': self.dwelling_type,
            'posted_since': self.posted_since,
            'num_bedrooms': self.bedrooms,
            'num_bathrooms': self.bathrooms,
            'city_center_distance': self.city_center_distance,
            'transport_routes': self.transport_routes,
            'latitude': self.latitude,
            'longitude': self.longitude,
            'ber_code': self.ber_code,
            'commercial_area_size': self.commercial_area_size
        }
--------------------
Prompt: 
def setOverlayAutoCurveDistanceRangeInMeters(self, ulOverlayHandle, fMinDistanceInMeters, fMaxDistanceInMeters):
        """
        For high-quality curved overlays only, sets the distance range in meters from the overlay used to automatically curve
        the surface around the viewer.  Min is distance is when the surface will be most curved.  Max is when least curved.
        """
----------
Output: 

        self.ulOverlayHandle = ulOverlayHandle
        self.fMinDistanceInMeters = fMinDistanceInMeters
        self.fMaxDistanceInMeters = fMaxDistanceInMeters

    def setOverlayAutoCurveRadius(self, radius):
        self.radius = radius

    def setOverlayAutoCurveColor(self, color):
        self.color = color

    def setOverlayAutoCurveOpacity(self, opacity):
        self.opacity = opacity

    def setOverlayAutoCurveOpacityRange(self, fMinRange, fMaxRange):
        """
        For high-quality curved overlays
----------
Solution: 


        fn = self.function_table.setOverlayAutoCurveDistanceRangeInMeters
        result = fn(ulOverlayHandle, fMinDistanceInMeters, fMaxDistanceInMeters)
        return result
--------------------
Prompt: 
def delete_run():
    """
    Delete the selected run from the database.
    :return:
    """
----------
Output: 

    run_id = request.args.get('run_id')
    if run_id is not None:
        try:
            run = Run.query.get(run_id)
            if run is not None:
                run.delete()
                flash('Run deleted')
            else:
                flash('No run found')
        except:
            flash('No run found')
    return redirect(url_for('run.index'))


@blueprint.route('/run/<int:run_id>/delete', methods=['POST'])
@login_required
def delete_run_by_id(run_
----------
Solution: 

    assert request.method == "POST", "POST request expected received {}".format(request.method)
    if request.method == "POST":
        try:
            selections = json.loads(request.form["selections"])
            utils.drop_run(selections["project"], selections["run"])
            return jsonify({"response": "deleted {}".format(selections["run"])})
        except Exception as e:
            logging.error(e)
    return jsonify({"0": "__EMPTY"})
--------------------
Prompt: 
def create_block_storage(kwargs=None, call=None):
    """
    Create a block storage
    """
----------
Output: 

    if not kwargs:
        kwargs = {}
    if not call:
        call ='swift.obj.reconstructor.SsyncReconstructor'
    return SsyncReconstructor(conf=None, **kwargs)


def get_db_connection(conf):
    """
    Get a swift.common.db.Connection object
    """
    return Connection(conf)


def get_container_memcache_key(conf, account, container):
    """
    Get the memcache key for container
    """
    return 'container/memcache/%s/%s' % (account, container)


def get_db_
----------
Solution: 

    if call == 'action':
        raise SaltCloudSystemExit(
            'The create_block_storage function must be called with '
            '-f or --function'
        )

    conn = get_conn()

    # Assemble the composite block storage object.
    block_storage = _get_block_storage(kwargs)

    data = conn.create_block_storage(block_storage=block_storage)

    return {'BlockStorage': data}
--------------------
Prompt: 
def _correct_qualimap_genome_results(samples):
    """ fixing java.lang.Double.parseDouble error on entries like "6,082.49"
    """
----------
Output: 

    if not isinstance(samples, list):
        samples = [samples]
    for sample in samples:
        if sample.get("name") == "qualimap_genome":
            sample["name"] = "Double"
        if sample.get("name") == "genome":
            sample["name"] = "Double"
        if sample.get("name") == "genome_index":
            sample["name"] = "Double"
        if sample.get("name") == "genome_name":
            sample["name"] = "Double"
        if sample.get("name") == "genome_name_index":
----------
Solution: 

    for s in samples:
        if verify_file(s.qualimap_genome_results_fpath):
            correction_is_needed = False
            with open(s.qualimap_genome_results_fpath, 'r') as f:
                content = f.readlines()
                metrics_started = False
                for line in content:
                    if ">> Reference" in line:
                        metrics_started = True
                    if metrics_started:
                        if line.find(',') != -1:
                            correction_is_needed = True
                            break
            if correction_is_needed:
                with open(s.qualimap_genome_results_fpath, 'w') as f:
                    metrics_started = False
                    for line in content:
                        if ">> Reference" in line:
                            metrics_started = True
                        if metrics_started:
                            if line.find(',') != -1:
                                line = line.replace(',', '')
                        f.write(line)
--------------------
Prompt: 
def _find_lib_path():
    """Find mxnet library."""
----------
Output: 

    if mxnet_home:
        return mxnet_home
    else:
        return os.path.dirname(os.path.abspath(__file__))


def _find_data_path(data_dir=None):
    """Find data path."""
    if data_dir:
        return os.path.join(data_dir, 'cifar-10-batches-py')
    else:
        return os.path.dirname(os.path.abspath(__file__))


def _get_data_path(data_dir=None):
    """Find data path."""
    if data_dir
----------
Solution: 

    curr_path = os.path.dirname(os.path.abspath(os.path.expanduser(__file__)))
    amalgamation_lib_path = os.path.join(curr_path, '../../lib/libmxnet_predict.so')
    if os.path.exists(amalgamation_lib_path) and os.path.isfile(amalgamation_lib_path):
        lib_path = [amalgamation_lib_path]
        return lib_path
    else:
        logging.info('Cannot find libmxnet_predict.so. Will search for MXNet library using libinfo.py then.')
        try:
            from mxnet.libinfo import find_lib_path
            lib_path = find_lib_path()
            return lib_path
        except ImportError:
            libinfo_path = os.path.join(curr_path, '../../python/mxnet/libinfo.py')
            if os.path.exists(libinfo_path) and os.path.isfile(libinfo_path):
                libinfo = {'__file__': libinfo_path}
                exec(compile(open(libinfo_path, "rb").read(), libinfo_path, 'exec'), libinfo, libinfo)
                lib_path = libinfo['find_lib_path']()
                return lib_path
            else:
                raise RuntimeError('Cannot find libinfo.py at %s.' % libinfo_path)
--------------------
Prompt: 
def simxStart(connectionAddress, connectionPort, waitUntilConnected, doNotReconnectOnceDisconnected, timeOutInMs, commThreadCycleInMs):
    """
    Please have a look at the function description/documentation in the V-REP user manual
    """
----------
Output: 

    return lib.rep_simx_start(connectionAddress, connectionPort, waitUntilConnected, doNotReconnectOnceDisconnected, timeOutInMs, commThreadCycleInMs)

def simxStop(connectionAddress, waitUntilConnected, doNotReconnectOnceStopped, commThreadStopped):
    """
    Please have a look at the function description/documentation in the V-REP user manual
    """
    return lib.rep_simx_stop(connectionAddress, connectionPort, waitUntilConnected, doNotReconnectOnceStopped, timeOutInMs, commThreadStopped)

def sim
----------
Solution: 


    if (sys.version_info[0] == 3) and (type(connectionAddress) is str):
        connectionAddress=connectionAddress.encode('utf-8')
    return c_Start(connectionAddress, connectionPort, waitUntilConnected, doNotReconnectOnceDisconnected, timeOutInMs, commThreadCycleInMs)
--------------------
Prompt: 
def _escape(value):
        """Escape a string (key or value) for InfluxDB's line protocol.

        :param str|int|float|bool value: The value to be escaped
        :rtype: str

        """
----------
Output: 

        if isinstance(value, bool):
            return str(value)
        else:
            return value.replace('\\', '\\\\').replace('"', '\\"')

    def _escape_string(value):
        """Escape a string (key or value) for InfluxDB's line protocol.

        :param str|int|float|bool value: The value to be escaped
        :rtype: str

        """
        if isinstance(value, bool):
            return value.replace('\\', '\\\\').replace('"', '\\"')
        else:
            return value.replace('"', '\\"')

    def _escape_string_list(value):
        """Escape a string list
----------
Solution: 

        value = str(value)
        for char, escaped in {' ': '\ ', ',': '\,', '"': '\"'}.items():
            value = value.replace(char, escaped)
        return value
--------------------
Prompt: 
def assert_not_in(obj, seq, message=None, extra=None):
    """Raises an AssertionError if obj is in iter."""
----------
Output: 

    try:
        iter(obj)
    except AssertionError:
        if message is not None:
            raise AssertionError(message)
        return
    if message is None:
        raise AssertionError("The truth value of an iterable is ambiguous.")
    if not seq:
        raise AssertionError("The truth value of an iterable is empty sequence.")
    if not extra:
        raise AssertionError("The truth value of an iterable is extra.")
    if not message:
        raise AssertionError("The truth value of an iterable is empty message.")
    if not isinstance(seq, (str, bytes)):
        raise AssertionError("The truth value of an iterable is not a string
----------
Solution: 

    # for very long strings, provide a truncated error
    if isinstance(seq, six.string_types) and obj in seq and len(seq) > 200:
        index = seq.find(obj)
        start_index = index - 50
        if start_index > 0:
            truncated = "(truncated) ..."
        else:
            truncated = ""
            start_index = 0
        end_index = index + len(obj) + 50
        truncated += seq[start_index:end_index]
        if end_index < len(seq):
            truncated += "... (truncated)"
        assert False, _assert_fail_message(message, obj, truncated, "is in", extra)
    assert obj not in seq, _assert_fail_message(message, obj, seq, "is in", extra)
--------------------
Prompt: 
def group_by_key_impl(sequence):
    """
    Implementation for group_by_key_t
    :param sequence: sequence to group
    :return: grouped sequence
    """
----------
Output: 

    return group_by_key(sequence, lambda x: x[0])


def group_by_key_impl_with_index(sequence, index):
    """
    Implementation for group_by_key_t
    :param sequence: sequence to group
    :param index: index of the sequence
    :return: grouped sequence with the index
    """
    return group_by_key(sequence, lambda x: x[0][index])


def group_by_key_impl_with_index_and_count(sequence, count):
    """
    Implementation for group_by_key
----------
Solution: 

    result = {}
    for element in sequence:
        if result.get(element[0]):
            result.get(element[0]).append(element[1])
        else:
            result[element[0]] = [element[1]]
    return six.viewitems(result)
--------------------
Prompt: 
def lowerlim(x, y, z, a, b, c):
    """Returns the real positive root of
      x/(a+t) + y/(b+t) + z/(c+t) = 1
    when x/a + y/b + z/c > 1 else zero
    """
----------
Output: 

    return np.where(np.abs(x) < 1e-10, 1, 0)


def upperlim(x, y, z, a, b, c):
    """Returns the real negative root of
      x/(a+t) + y/(b+t) + z/(c+t) = 1
    when x/a + y/b + z/c > 1 else zero
    """
    return np.where(np.abs(x) < 1e-10, 1, 0)


def log_likelihood(x, y, z, a, b
----------
Solution: 

    if x/a + y/b + z/c > 1:
        B = a + b + c - x - y - z
        C = a*b + a*c + b*c - a*y - a*z - b*x - b*z - c*x - c*y
        D = a*b*c - a*b*z - a*c*y - b*c*x
        r = np.roots([1,B,C,D])
        ll = r[~np.iscomplex(r) & (r>0.)]
        return ll[0].real
    else:
        return 0.
--------------------
Prompt: 
def amod(a, b):
    """Modulus function which returns numerator if modulus is zero"""
----------
Output: 

    return a * b == 0


def mod(a, b):
    """Modulus function which returns denominator if modulus is zero"""
    return a * b == 0


def pow(a, b):
    """Raise the power of two"""
    if a == 0:
        return 0
    if b == 0:
        return 1
    if mod(a, b) == 0:
        return 0
    return a * pow(b, a)


def pow_mod(a, b):
    """Raise the power of two"""
    if a == 0:
        return 0
    if b ==
----------
Solution: 

    modded = int(a % b)
    return b if modded is 0 else modded
--------------------
Prompt: 
def from_where(cls, where):
        """ Factory method for creating the top-level expression """
----------
Output: 

        if isinstance(where, basestring):
            where = cls.get_where_expression(where)
        return cls(where)

    @classmethod
    def get_where_expression(cls, where):
        """ Factory method for creating the top-level expression """
        if isinstance(where, cls.Expression):
            return where.where
        return None

    @classmethod
    def get_where_filters(cls, filters):
        """ Factory method for creating the top-level filters """
        if isinstance(filters, basestring):
            filters = cls.get_where_expression(filters)
        return cls(filters)

    @
----------
Solution: 

        if where.conjunction:
            return Conjunction.from_clause(where)
        else:
            return cls.from_clause(where[0])
--------------------
Prompt: 
def clean_text(value, topic=False):
    """
    Replaces "profane" words with more suitable ones.
    Uses bleach to strip all but whitelisted html.
    Converts bbcode to Markdown
    """
----------
Output: 

    return bleach.clean(value, tags=[], strip=True, attributes=None,
                         styles=['markdown.extensions.extra','markdown.extensions.codehilite'])

----------
Solution: 

    for x in PROFANITY_REPLACEMENTS:
        value = value.replace(x[0], x[1])

    for bbset in BBCODE_REPLACEMENTS:
        p = re.compile(bbset[0], re.DOTALL)
        value = p.sub(bbset[1], value)

    bleached = bleach.clean(value, tags=ALLOWED_TAGS, attributes=ALLOWED_ATTRIBUTES, strip=True)

    # We want to retain markdown quotes and we'll be running bleach again in format_post.
    bleached = bleached.replace('&gt;', '>').replace('&amp;', '&')
    return bleached
--------------------
Prompt: 
def give_repr(cls):  # pragma: no cover
    r"""Patch a class to give it a generic __repr__ method
    that works by inspecting the instance dictionary.

    Parameters
    ----------
    cls: type
        The class to add a generic __repr__ to.

    Returns
    -------
    cls: type
        The passed class is returned
    """
----------
Output: 

    return cls.__repr__


def _check_option_forward(option, names):
    """Check whether forward (i.e. forward) option is valid."""
    if option not in names:
        raise ValueError(f"option {option} is not a valid option")


def _check_option_reverse(option, names):
    """Check whether reverse (i.e. reverse) option is valid."""
    if option not in names:
        raise ValueError(f"option {option} is not a valid option")


def _check_option_forward_reverse(option, names):
----------
Solution: 

    def reprer(self):
        attribs = ', '.join(["%s=%r" % (k, v) for k, v in self.__dict__.items() if not k.startswith("_")])
        wrap = "{self.__class__.__name__}({attribs})".format(self=self, attribs=attribs)
        return wrap
    cls.__repr__ = reprer
    return cls
--------------------
Prompt: 
def bodvrd(bodynm, item, maxn):
    """
    Fetch from the kernel pool the double precision values
    of an item associated with a body.

    http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/bodvrd_c.html

    :param bodynm: Body name.
    :type bodynm: str
    :param item:
                Item for which values are desired,
                ("RADII", "NUT_PREC_ANGLES", etc.)
    :type item: str
    :param maxn: Maximum number of values that may be returned.
    :type maxn: int
    :return: tuple of (dim, values)
    :rtype: tuple
    """
----------
Output: 
 :param maxn:
                Maximum number of double precision values to return.
    :type maxn: int
    :return:
                Body name.
    :rtype: str
    """
    return item[1:maxn]


def bodvrd_c(bodynm, item, maxn):
    """
    Fetch from the kernel pool the double precision values
    of an item associated with a bodynm.

    http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/bodvrd_c.html

    :param b
----------
Solution: 

    bodynm = stypes.stringToCharP(bodynm)
    item = stypes.stringToCharP(item)
    dim = ctypes.c_int()
    values = stypes.emptyDoubleVector(maxn)
    maxn = ctypes.c_int(maxn)
    libspice.bodvrd_c(bodynm, item, maxn, ctypes.byref(dim), values)
    return dim.value, stypes.cVectorToPython(values)
--------------------
Prompt: 
def compile_command(context, backend, config):
    """
    Compile Sass project sources to CSS
    """
----------
Output: 

    if not config.debug:
        return

    if not context.config.get('css_path'):
        return

    if not context.config.get('css_path').endswith('.scss'):
        context.config['css_path'] += '.scss'

    context.log.info('Compiling %s', context.config['css_path'])

    context.log.info('Writing %s', context.config['css_path'])
    with open(context.config['css_path'], 'w') as f:
        f.write(SassProjectConfiguration.get_template('css
----------
Solution: 

    logger = logging.getLogger("boussole")
    logger.info(u"Building project")

    # Discover settings file
    try:
        discovering = Discover(backends=[SettingsBackendJson,
                                         SettingsBackendYaml])
        config_filepath, config_engine = discovering.search(
            filepath=config,
            basedir=os.getcwd(),
            kind=backend
        )

        project = ProjectBase(backend_name=config_engine._kind_name)
        settings = project.backend_engine.load(filepath=config_filepath)
    except BoussoleBaseException as e:
        logger.critical(six.text_type(e))
        raise click.Abort()

    logger.debug(u"Settings file: {} ({})".format(
                 config_filepath, config_engine._kind_name))
    logger.debug(u"Project sources directory: {}".format(
                 settings.SOURCES_PATH))
    logger.debug(u"Project destination directory: {}".format(
                 settings.TARGET_PATH))
    logger.debug(u"Exclude patterns: {}".format(
                 settings.EXCLUDES))

    # Find all sources with their destination path
    try:
        compilable_files = ScssFinder().mirror_sources(
            settings.SOURCES_PATH,
            targetdir=settings.TARGET_PATH,
            excludes=settings.EXCLUDES
        )
    except BoussoleBaseException as e:
        logger.error(six.text_type(e))
        raise click.Abort()

    # Build all compilable stylesheets
    compiler = SassCompileHelper()
    errors = 0
    for src, dst in compilable_files:
        logger.debug(u"Compile: {}".format(src))

        output_opts = {}
        success, message = compiler.safe_compile(settings, src, dst)

        if success:
            logger.info(u"Output: {}".format(message), **output_opts)
        else:
            errors += 1
            logger.error(message)

    # Ensure correct exit code if error has occured
    if errors:
        raise click.Abort()
--------------------
Prompt: 
def initialize_parse_state(self, build_file):
    """Creates a fresh parse state for the given build file.

    :param build_file: The BUILD file to set up a new ParseState for.
    :type build_file: :class:`pants.base.build_file.BuildFile`
    :returns: A fresh ParseState for parsing the given `build_file` with.
    :rtype: :class:`BuildConfiguration.ParseState`
    """
----------
Output: 

    return BuildConfiguration.ParseState(build_file.address.spec_path,
                                      build_file.address.spec_path,
                                      self.build_configuration_path,
                                      self.address.spec_path,
                                      self.build_file_parser_path,
                                      self.address.spec_path,
                                      self.build_file_parser_parser_path,
                                      self.address.spec_path,
                                      self.build_file_parser_cls_path,
                                      self.address.spec_path,
                                      self.build_file_parser_cls_cls_path,
                                      self
----------
Solution: 

    # TODO(John Sirois): Introduce a factory method to seal the BuildConfiguration and add a check
    # there that all anonymous types are covered by context aware object factories that are
    # Macro instances.  Without this, we could have non-Macro context aware object factories being
    # asked to be a BuildFileTargetFactory when they are not (in SourceRoot registration context).
    # See: https://github.com/pantsbuild/pants/issues/2125
    type_aliases = self._exposed_object_by_alias.copy()
    parse_context = ParseContext(rel_path=build_file.spec_path, type_aliases=type_aliases)

    def create_call_proxy(tgt_type, tgt_alias=None):
      def registration_callback(address, addressable):
        parse_context._storage.add(addressable, name=address.target_name)
      addressable_factory = self._get_addressable_factory(tgt_type, tgt_alias)
      return AddressableCallProxy(addressable_factory=addressable_factory,
                                  build_file=build_file,
                                  registration_callback=registration_callback)

    # Expose all aliased Target types.
    for alias, target_type in self._target_by_alias.items():
      proxy = create_call_proxy(target_type, alias)
      type_aliases[alias] = proxy

    # Expose aliases for exposed objects and targets in the BUILD file.
    parse_globals = type_aliases.copy()

    # Now its safe to add mappings from both the directly exposed and macro-created target types to
    # their call proxies for context awares and macros to use to manufacture targets by type
    # instead of by alias.
    for alias, target_type in self._target_by_alias.items():
      proxy = type_aliases[alias]
      type_aliases[target_type] = proxy

    for target_macro_factory in self._target_macro_factory_by_alias.values():
      for target_type in target_macro_factory.target_types:
        proxy = create_call_proxy(target_type)
        type_aliases[target_type] = proxy

    for alias, object_factory in self._exposed_context_aware_object_factory_by_alias.items():
      parse_globals[alias] = object_factory(parse_context)

    for alias, target_macro_factory in self._target_macro_factory_by_alias.items():
      parse_globals[alias] = target_macro_factory.target_macro(parse_context)

    return self.ParseState(parse_context, parse_globals)
--------------------
Prompt: 
def full_like(a, **kwargs):
    """Create a filled array like `a`."""
----------
Output: 

    kwargs.setdefault('subok', True)
    return np.full_like(a, fill_value=a.dtype, **kwargs)


def full_like_function(a, **kwargs):
    """Create a filled function that accepts a variable number of arguments."""
    kwargs.setdefault('subok', True)
    return np.full_like(a, fill_value=a.dtype, **kwargs)


def zeros_like(a, **kwargs):
    """Create a filled array of zeros with the same shape and type as a given array."""
    kwargs.setdefault('subok
----------
Solution: 

    _like_args(a, kwargs)
    if isinstance(a, Array):
        kwargs.setdefault('fill_value', a.fill_value)
    return full(**kwargs)
--------------------
Prompt: 
def get_payment_method(self, payment_method_id, **params):
        """https://developers.coinbase.com/api/v2#show-a-payment-method"""
----------
Output: 

        return self.api.get_payment_method(payment_method_id, **params)

    def get_payment_method_by_id(self, payment_method_id):
        """https://developers.coinbase.com/api/v2#show-a-payment-method"""
        return self.api.get_payment_method_by_id(payment_method_id)

    def get_payment_method_history(self, payment_method_id, **params):
        """https://developers.coinbase.com/api/v2#show-a-payment-
----------
Solution: 

        response = self._get('v2', 'payment-methods', payment_method_id, params=params)
        return self._make_api_object(response, PaymentMethod)
--------------------
Prompt: 
def setup_endpoints(provider):
    """Setup the OpenID Connect Provider endpoints."""
----------
Output: 

    def _get_endpoint(self):
        """Return the endpoint for the service provider."""
        return self.server.config.get('OPENID_SSO_ENDPOINT', self.server.config.get('OPENID_ENDPOINT', None))

    def _set_endpoint(self, value):
        """Set the endpoint for the service provider."""
        self.server.config['OPENID_SSO_ENDPOINT'] = value

    return OpenIDConnectProvider(_get_endpoint, _set_endpoint)

----------
Solution: 

    app_routing = {}
    endpoints = [
        AuthorizationEndpoint(
            pyoidcMiddleware(provider.authorization_endpoint)),
        TokenEndpoint(
            pyoidcMiddleware(provider.token_endpoint)),
        UserinfoEndpoint(
            pyoidcMiddleware(provider.userinfo_endpoint)),
        RegistrationEndpoint(
            pyoidcMiddleware(provider.registration_endpoint)),
        EndSessionEndpoint(
            pyoidcMiddleware(provider.endsession_endpoint))
    ]

    for ep in endpoints:
        app_routing["/{}".format(ep.etype)] = ep

    return app_routing
--------------------
Prompt: 
def step(self, disable_interrupts=True, start=0, end=0):
        """
        perform an instruction level step.  This function preserves the previous
        interrupt mask state
        """
----------
Output: 

        self.write_register(self.cpu, self.register, self.cpu.read_int(start), self.cpu.read_int(end))

    def execute(self):
        """
        Execute the instruction.
        """
        self.write_register(self.cpu, self.register, self.cpu.read_int(self.start), self.cpu.read_int(self.end))

    def write_register(self, cpu, register, start, end):
        """
        Write the register to the cpu
        """
        cpu.write_int(self.register, start
----------
Solution: 

        # Was 'if self.get_state() != TARGET_HALTED:'
        # but now value of dhcsr is saved
        dhcsr = self.read_memory(CortexM.DHCSR)
        if not (dhcsr & (CortexM.C_STEP | CortexM.C_HALT)):
            logging.error('cannot step: target not halted')
            return

        self.notify(Notification(event=Target.EVENT_PRE_RUN, source=self, data=Target.RUN_TYPE_STEP))

        self.clear_debug_cause_bits()

        # Save previous interrupt mask state
        interrupts_masked = (CortexM.C_MASKINTS & dhcsr) != 0

        # Mask interrupts - C_HALT must be set when changing to C_MASKINTS
        if not interrupts_masked and disable_interrupts:
            self.write_memory(CortexM.DHCSR, CortexM.DBGKEY | CortexM.C_DEBUGEN | CortexM.C_HALT | CortexM.C_MASKINTS)

        # Single step using current C_MASKINTS setting
        while True:
            if disable_interrupts or interrupts_masked:
                self.write_memory(CortexM.DHCSR, CortexM.DBGKEY | CortexM.C_DEBUGEN | CortexM.C_MASKINTS | CortexM.C_STEP)
            else:
                self.write_memory(CortexM.DHCSR, CortexM.DBGKEY | CortexM.C_DEBUGEN | CortexM.C_STEP)

            # Wait for halt to auto set (This should be done before the first read)
            while not self.read_memory(CortexM.DHCSR) & CortexM.C_HALT:
                pass

            # Range is empty, 'range step' will degenerate to 'step'
            if start == end:
                break

            # Read program counter and compare to [start, end)
            program_counter = self.read_core_register(CORE_REGISTER['pc'])
            if program_counter < start or end <= program_counter:
                break

            # Check other stop reasons
            if self.read_memory(CortexM.DFSR) & (CortexM.DFSR_DWTTRAP | CortexM.DFSR_BKPT):
                break
	
        # Restore interrupt mask state
        if not interrupts_masked and disable_interrupts:
            # Unmask interrupts - C_HALT must be set when changing to C_MASKINTS
            self.write_memory(CortexM.DHCSR, CortexM.DBGKEY | CortexM.C_DEBUGEN | CortexM.C_HALT)

        self.flush()

        self._run_token += 1

        self.notify(Notification(event=Target.EVENT_POST_RUN, source=self, data=Target.RUN_TYPE_STEP))
--------------------
Prompt: 
def startproject(project_name):
    """
    build a full status project
    """
----------
Output: 

    # get the project
    project = get_project(project_name)

    # get the project
    project_status = get_project_status(project)

    # get the project status
    if project_status == 'active':
        project_status = 'inactive'
    elif project_status == 'inactive':
        project_status = 'active'

    # get the project
    project_name = project.name

    # get the project
    project_status = get_project_status(project)

    # get the project name
    if project_name == 'None':
        print("No project name given,
----------
Solution: 

    # the destination path
    dst_path = os.path.join(os.getcwd(), project_name)
    start_init_info(dst_path)

    # create dst path
    _mkdir_p(dst_path)

    # create project tree
    os.chdir(dst_path)
    # create files
    init_code('manage.py', _manage_admin_code)
    init_code('requirement.txt', _requirement_admin_code)
    init_code('config.py', _config_sql_code)

    # create app/
    app_path = os.path.join(dst_path, 'app')
    _mkdir_p(app_path)

    # create files
    os.chdir(app_path)
    init_code('models.py', _models_admin_code)
    init_code('__init__.py', _init_admin_code)

    # create templates and static
    css_path, templates_path = create_templates_static_files(app_path)
    # create css files
    os.chdir(css_path)
    init_code('sign.css', _auth_login_css_code)

    # create main blueprint
    create_blueprint(
        app_path,
        'main',
        _views_blueprint_code % ('main', 'main'),
        _forms_basic_code,
        templates_path
    )

    # create auth blueprint
    auth_templates_path = create_blueprint(
        app_path,
        'auth',
        _auth_views_code,
        _auth_forms_code,
        templates_path
    )
    # create auth templates files
    os.chdir(auth_templates_path)
    init_code('login.html', _auth_login_html_code)

    # create admin site
    admin_path = os.path.join(app_path, 'admin')
    _mkdir_p(admin_path)

    # create admin files
    os.chdir(admin_path)
    init_code('__init__.py', '')
    init_code('views.py', _admin_views_code)

    # create admin templates
    os.chdir(templates_path)
    admin_templates_path = os.path.join(templates_path, 'admin')
    _mkdir_p(admin_templates_path)

    # create admin templates files
    os.chdir(admin_templates_path)
    init_code('index.html', _admin_index_html_code)
    init_code('logout.html', _admin_logout_html_code)

    init_done_info()
--------------------
Prompt: 
def signed_raw(self) -> str:
        """
        Return Revocation signed raw document string

        :return:
        """
----------
Output: 

        return self._signed_raw

    @property
    def signature(self) -> str:
        """
        Return Signature document

        :return:
        """
        return self._signature

    @property
    def signature_type(self) -> str:
        """
        Return Signature type document

        :return:
        """
        return self._signature_type

    @property
    def signature_id(self) -> str:
        """
        Return Signature ID document

        :return:
        """
        return self._signature_id

    @property
    def signature_version(self) -> str:
        """
        Return Signature version document

        :return
----------
Solution: 

        if not isinstance(self.identity, Identity):
            raise MalformedDocumentError("Can not return full revocation document created from inline")

        raw = self.raw()
        signed = "\n".join(self.signatures)
        signed_raw = raw + signed + "\n"
        return signed_raw
--------------------
Prompt: 
def load_de_novos(path, exclude_indels=True):
    """ load mutations into dict indexed by HGNC ID.
    
    Args:
        path: path to file containing de novo data. This should have five tab-
            separated columns e.g.
            hgnc  chr  pos      consequence         var_type
            CTC1  17   8139190  missense_variant    snv
            CTC1  17   8139191  frameshift_variant  indel
        exclude_indels: True/False for whether we want to exclude indels. If we
            are testing clustering of de novos, we can only use SNVs, but if we
            are determining mutation rates for a gene, then we include indels,
            in order to identify the transcripts that the de novos lie within.
    
    Returns:
        dictionary of missense and lof counts for each gene, indexed by HGNC symbols
    """
----------
Output: 
ls.
    """
    with open(path, 'r') as f:
        for line in f:
            if line.startswith('#'):
                continue
            if line.startswith('##'):
                continue
            if line.startswith('##contig'):
                continue
            if line.startswith('#var_type'):
                continue
            if line.startswith('#var'):
                continue
            if line.startswith('#var1'):
                continue
            if line.startswith('#var2'):
                continue
            if line.startswith('#var1'):
                continue
            if line.startswith('#var2'):
                continue
            if line.startswith('#var1'):
               
----------
Solution: 

    
    genes = {}
    with open(path, "r") as handle:
        header = handle.readline().strip().split("\t")
        for line in handle:
            
            line = line.rstrip().split("\t")
            gene = line[0]
            position = int(line[2]) - 1
            consequence = line[3]
            var_type = line[4]
            
            # ignore indels (some splice_acceptor_variants (in the
            # functional_consequences) are indels
            if exclude_indels and "indel" in var_type.lower():
                continue
            
            # trim out variants that are missing data
            if gene == "" or gene == "." or position == "NA":
                continue
            
            if gene not in genes:
                genes[gene] = {"missense": [], "nonsense": []}
            
            if consequence in missense:
                genes[gene]["missense"].append(position)
            elif consequence in lof:
                genes[gene]["nonsense"].append(position)
        
    return genes
--------------------
Prompt: 
def get_widget_css_class(self, attrs):
        """Gets the class for the widget."""
----------
Output: 

        widget = super(BaseWidget, self).get_widget_class()
        return widget.render_with_errors(attrs)

    def get_widget_attrs(self, widget):
        """Gets the attributes for the widget."""
        attrs = super(BaseWidget, self).get_widget_attrs(widget)
        attrs['class'] = self.get_css_class(attrs)
        return attrs

    def get_css_class(self, attrs):
        """Gets the class for the widget."""
        return self.get_widget_css_classes(attrs)

    def get_css_classes
----------
Solution: 

        size_class = 'size-{0}'.format(self.num_inputs)

        if 'class' in attrs:
            attrs['class'] += ' {0}'.format(size_class)
        else:
            attrs['class'] = size_class
--------------------
Prompt: 
def get_cpds(self):
        """
        Adds tables to BIF

        Returns
        -------
        dict: dict of type {variable: array}

        Example
        -------
        >>> from pgmpy.readwrite import BIFReader, BIFWriter
        >>> model = BIFReader('dog-problem.bif').get_model()
        >>> writer = BIFWriter(model)
        >>> writer.get_cpds()
        {'bowel-problem': array([ 0.01,  0.99]),
         'dog-out': array([ 0.99,  0.97,  0.9 ,  0.3 ,  0.01,  0.03,  0.1 ,  0.7 ]),
         'family-out': array([ 0.15,  0.85]),
         'hear-bark': array([ 0.7 ,  0.01,  0.3 ,  0.99]),
         'light-on': array([ 0.6 ,  0.05,  0.4 ,  0.95])}
        """
----------
Output: 
  0.8,  0.7,  0.6,  0.5,  0.4,  0.3,  0.2,  0.1,  0.3]),
         'pronoun': array([ 0.01,  0.99]),
         'pronouns': array([ 0.97,  0.9,  0.8,  0.7,  0.6,  0.5,  0.4,  0.3,  0.2,  0.1,  0.3]),
        
----------
Solution: 

        cpds = self.model.get_cpds()
        tables = {}
        for cpd in cpds:
            tables[cpd.variable] = cpd.values.ravel()
        return tables
--------------------
Prompt: 
def instruction_SBC(self, opcode, m, register):
        """
        Subtracts the contents of memory location M and the borrow (in the C
        (carry) bit) from the contents of the designated 8-bit register, and
        places the result in that register. The C bit represents a borrow and is
        set to the inverse of the resulting binary carry.

        source code forms: SBCA P; SBCB P

        CC bits "HNZVC": uaaaa
        """
----------
Output: 

        if opcode == 0x00:
            m = (m & ~0x01)
        elif opcode == 0x01:
            m = (m & ~0x02)
        elif opcode == 0x02:
            m = (m & ~0x04) | (m & ~0x08)
        elif opcode == 0x04:
            m = (m & ~0x08) | (m & ~0x10)
        elif opcode == 0x08:
            m = (m & ~0x10) | (m & ~0x20)
        elif opcode == 0x
----------
Solution: 

        a = register.value
        r = a - m - self.C
        register.set(r)
#        log.debug("$%x %02x SBC %s: %i - %i - %i = %i (=$%x)" % (
#            self.program_counter, opcode, register.name,
#            a, m, self.C, r, r
#        ))
        self.clear_NZVC()
        self.update_NZVC_8(a, m, r)
--------------------
Prompt: 
def policy(self):
        """Returns policy which contains this ioclass."""
----------
Output: 

        return self._policy

    @property
    def name(self):
        """Returns the name of this IOClass."""
        return self._name

    @property
    def type(self):
        """Returns the IOClass type."""
        return self._type

    @property
    def description(self):
        """Returns the description of this IOClass."""
        return self._description

    @property
    def is_open(self):
        """Returns True if the IOClass is open."""
        return self._policy == 'policy'

    @property
    def policy_id(self):
        """Returns the policy ID."""
        return self._
----------
Solution: 

        policies = VNXIOPolicy.get(cli=self._cli)
        ret = None
        for policy in policies:
            contained = policy.ioclasses.name
            if self._get_name() in contained:
                ret = VNXIOPolicy.get(name=policy.name, cli=self._cli)
                break
        return ret
--------------------
Prompt: 
def classify_single_recording(raw_data_json, model_folder, verbose=False):
    """
    Get the classification as a list of tuples. The first value is the LaTeX
    code, the second value is the probability.
    """
----------
Output: 

    if verbose:
        print("Classify single recording")
    return [(model_folder, 'latex'), (model_folder, 'probability')]


def classify_single_model(raw_data_json, model_folder, verbose=False):
    """
    Get the classification as a list of tuples. The first value is the
    LaTeX code, the second value is the probability.
    """
    if verbose:
        print("Classify single model")
    return [(model_folder,'model'), (model_folder, 'probability')]


def classify_single_recording(raw_data_json,
----------
Solution: 

    evaluation_file = evaluate_model(raw_data_json, model_folder, verbose)
    with open(os.path.join(model_folder, "info.yml")) as ymlfile:
        model_description = yaml.load(ymlfile)

    index2latex = get_index2latex(model_description)

    # Map line to probabilites for LaTeX commands
    with open(evaluation_file) as f:
        probabilities = f.read()
    probabilities = map(float, probabilities.split(" "))
    results = []
    for index, probability in enumerate(probabilities):
        results.append((index2latex[index], probability))
    results = sorted(results, key=lambda n: n[1], reverse=True)
    return results
--------------------
Prompt: 
def perimeter(self):
        """
        The total perimeter of the source segment, approximated lines
        through the centers of the border pixels using a 4-connectivity.

        If any masked pixels make holes within the source segment, then
        the perimeter around the inner hole (e.g. an annulus) will also
        contribute to the total perimeter.
        """
----------
Output: 

        return self.source.perimeter() + self.border.perimeter()

    def line_strings(self):
        """
        Return the list of lines that make up the source and border pixels
        for a line.
        """
        return self.source.line_strings() + [self.border.line_strings()]

    def line_strings_with_holes(self):
        """
        Return the list of lines, with holes for masked lines, if any
        masked pixels make holes within the source and return as a single string.
        """
        return self.line_strings() + self.line
----------
Solution: 


        if self._is_completely_masked:
            return np.nan * u.pix  # unit for table
        else:
            from skimage.measure import perimeter
            return perimeter(~self._total_mask, neighbourhood=4) * u.pix
--------------------
Prompt: 
def unesc(line, language):
    """Uncomment once a commented line"""
----------
Output: 

    if line.startswith('#'):
        return line[1:]
    else:
        return line


def parse_comment(line):
    """Parse a comment line"""
    if line.startswith('#'):
        return line[1:]
    else:
        return line


def parse_comment_line(line):
    """Parse a comment line"""
    if line.startswith('#'):
        return line[1:]
    else:
        return line


def parse_comment_line_with_comments(line):
    """Parse a comment line with comments"""
    if line.startswith('#'):
        return line[1:]
----------
Solution: 

    comment = _COMMENT[language]
    if line.startswith(comment + ' '):
        return line[len(comment) + 1:]
    if line.startswith(comment):
        return line[len(comment):]
    return line
--------------------
Prompt: 
def base_url(klass, space_id, parent_resource_id, resource_url='entries', resource_id=None, environment_id=None):
        """
        Returns the URI for the snapshot.
        """
----------
Output: 

        if space_id is None:
            space_id = ''
        return '/%s/%s/%s/%s/%s' % (
            parent_resource_url,
            environment_id,
            space_id,
            space_id,
            parent_resource_id,
            resource_url
        )

    @classmethod
    def _get_snapshot_uri(cls, space_id, parent_resource_id, resource_url='entries'):
        """
        Returns the URI for the snapshot.
        """
        if space_id is None:
            space_id = ''
        return '/%s/%s
----------
Solution: 


        return "spaces/{0}{1}/{2}/{3}/snapshots/{4}".format(
            space_id,
            '/environments/{0}'.format(environment_id) if environment_id is not None else '',
            resource_url,
            parent_resource_id,
            resource_id if resource_id is not None else ''
        )
--------------------
Prompt: 
def centroid(self):
        """
        The ``(y, x)`` coordinate of the centroid within the source
        segment.
        """
----------
Output: 

        return self._centroid

    @centroid.setter
    def centroid(self, value):
        self._centroid = value

    @property
    def z(self):
        """
        The ``(z, y)`` coordinate of the centroid within the
        source segment.
        """
        return self._centroid.z

    @centroid.setter
    def z(self, value):
        self._centroid.z = value

    @property
    def y(self):
        """
        The ``(x, z)`` coordinate of the centroid within the
        source segment.
        """
        return self._centroid.y

    @centroid.setter
    def y(
----------
Solution: 


        ycen, xcen = self.cutout_centroid.value
        return (ycen + self._slice[0].start,
                xcen + self._slice[1].start) * u.pix
--------------------
Prompt: 
def transpose_axes(image, axes, asaxes=None):
    """Return image with its axes permuted to match specified axes.

    A view is returned if possible.

    >>> transpose_axes(numpy.zeros((2, 3, 4, 5)), 'TYXC', asaxes='CTZYX').shape
    (5, 2, 1, 3, 4)

    """
----------
Output: 

    image = numpy.asarray(image)
    if asaxes is None:
        asaxes = image.ndim
    if asaxes not in _AXES2TUPLE and asaxes!= image.ndim:
        raise ValueError('invalid transpose axes: %s' % (asaxes))
    m = numpy.asarray(axes)
    if m.ndim == 0:
        m = numpy.expand_dims(image, axis=0)
    else:
        m = numpy.expand_dims(image, axis=1)
    asaxes = asaxes or m[0]
    if asaxes == 0:
       
----------
Solution: 

    for ax in axes:
        if ax not in asaxes:
            raise ValueError('unknown axis %s' % ax)
    # add missing axes to image
    if asaxes is None:
        asaxes = 'CTZYX'
    shape = image.shape
    for ax in reversed(asaxes):
        if ax not in axes:
            axes = ax + axes
            shape = (1,) + shape
    image = image.reshape(shape)
    # transpose axes
    image = image.transpose([axes.index(ax) for ax in asaxes])
    return image
--------------------
Prompt: 
def generate_entities_doc(ctx, out_path, package):
    """Create entities documentation from Canari python classes file."""
----------
Output: 

    # TODO: add a test for this
    package_path = os.path.join(out_path, package + '.py')
    package_name = package.replace('.', '/')
    ctx.env.temp_data['package'] = package_name
    ctx.env.temp_data['package_path'] = package_path
    ctx.env.temp_data['package_name'] = package_name
    ctx.env.temp_data['package'] = package_name
    ctx.env.temp_data['package_path'] = package_path
    ctx.env.temp_data
----------
Solution: 

    from canari.commands.generate_entities_doc import generate_entities_doc
    generate_entities_doc(ctx.project, out_path, package)
--------------------
Prompt: 
def add_path_part(url, regex=PATH_PART):
    """
    replace the variables in a url template with regex named groups
    :param url: string of a url template
    :param regex: regex of the named group
    :returns: regex
    """
----------
Output: 

    if not url:
        return regex

    if not isinstance(url, str):
        return regex

    if not regex.match(url):
        return regex

    groups = re.findall(regex.pattern, url)
    if groups:
        for group in groups:
            if group[0] == "(?P<group_name>(?:" + group[1] + ")*)":
                group_name = group[1] + " " + group_name
                group_name = group_name.replace("(?P<group_name>(?:" + group[1] + ")*)","
----------
Solution: 

    formatter = string.Formatter()
    url_var_template = "(?P<{var_name}>{regex})"

    for part in formatter.parse(url):
        string_part, var_name, _, _ = part
        if string_part:
            yield string_part
        if var_name:
            yield url_var_template.format(var_name=var_name, regex=regex)
--------------------
Prompt: 
def _fix_review_dates(self, item):
        """Convert dates so ES detect them"""
----------
Output: 

        if not item.get('date'):
            return item

        if isinstance(item['date'], datetime.datetime):
            return item['date'].strftime('%Y-%m-%d')

        return item

    def _get_date_range(self, item):
        """Convert dates to be in the format YYYY-MM-DD"""
        if not item.get('date'):
            return item

        if isinstance(item['date'], datetime.datetime):
            return item['date'].strftime('%Y-%m-%d')

        return item

    def _get_date_range_end(self, item):
        """Convert
----------
Solution: 


        for date_field in ['timestamp', 'createdOn', 'lastUpdated']:
            if date_field in item.keys():
                date_ts = item[date_field]
                item[date_field] = unixtime_to_datetime(date_ts).isoformat()

        if 'patchSets' in item.keys():
            for patch in item['patchSets']:
                pdate_ts = patch['createdOn']
                patch['createdOn'] = unixtime_to_datetime(pdate_ts).isoformat()

                if 'approvals' in patch:
                    for approval in patch['approvals']:
                        adate_ts = approval['grantedOn']
                        approval['grantedOn'] = unixtime_to_datetime(adate_ts).isoformat()

        if 'comments' in item.keys():
            for comment in item['comments']:
                cdate_ts = comment['timestamp']
                comment['timestamp'] = unixtime_to_datetime(cdate_ts).isoformat()
--------------------
Prompt: 
def _query_label(self, loc):
        """Query the label for the item with index `loc`. Preferentially
        queries the label from the cache, but if not yet cached, queries the
        oracle.

        Returns
        -------
        int
            the true label "0" or "1".
        """
----------
Output: 

        if loc not in self._cache:
            self._cache[loc] = self._query_label_cache(loc)
        return self._cache[loc].get_label()

    def _query_label_cache(self, loc):
        """Return the label for the item with index `loc`.

        Parameters
        ----------
        loc : str
            the label for the cache.

        Returns
        -------
        int
            the true label "0" or "1".
        """
        if loc not in self._label_cache:
            self._label_cache[loc] = self._query_label_cache(
----------
Solution: 

        # Try to get label from cache
        ell = self.cached_labels_[loc]

        if np.isnan(ell):
            # Label has not been cached. Need to query oracle
            oracle_arg = self.identifiers[loc]
            ell = self.oracle(oracle_arg)
            if ell not in [0, 1]:
                raise Exception("Oracle provided an invalid label.")
            #TODO Gracefully handle errors from oracle?
            self._queried_oracle[self.t_] = True
            self.cached_labels_[loc] = ell

        return ell
--------------------
Prompt: 
def update_events(self, events_data: List[Tuple[str, int]]) -> None:
        """Given a list of identifier/data event tuples update them in the DB"""
----------
Output: 

        for data in events_data:
            if data[0] not in self.events:
                self.events[data[0]] = []
            self.events[data[0]].append(data[1])

    def get_event_by_id(self, event_id: int) -> Tuple[str, int]:
        """Given a list of identifier/data tuples get the event name and data"""
        return self.events[event_id]

    def get_event_by_name(self, event_name: str) -> Tuple[str, int]:
        """Given a list
----------
Solution: 

        cursor = self.conn.cursor()
        cursor.executemany(
            'UPDATE state_events SET data=? WHERE identifier=?',
            events_data,
        )
        self.maybe_commit()
--------------------
Prompt: 
def _handle_info(self, *args, **kwargs):
        """
        Handles info messages and executed corresponding code
        """
----------
Output: 

        if self.code == 'info':
            self.code = 'info'
            self.msg = 'info'
            self.msg_type = 'info'
        elif self.code == 'execute':
            self.code = 'execute'
            self.msg = 'execute'
            self.msg_type = 'execute'
        elif self.code == 'exit':
            self.code = 'exit'
            self.msg = 'exit'
            self.msg_type = 'exit'
        else:
            self.msg = 'Unknown code: %s' % self.code
            self.msg
----------
Solution: 

        if 'version' in kwargs:
            # set api version number and exit
            self.api_version = kwargs['version']
            print("Initialized API with version %s" % self.api_version)
            return
        try:
            info_code = str(kwargs['code'])
        except KeyError:
            raise FaultyPayloadError("_handle_info: %s" % kwargs)

        if not info_code.startswith('2'):
            raise ValueError("Info Code must start with 2! %s", kwargs)

        output_msg = "_handle_info(): %s" % kwargs
        log.info(output_msg)

        try:
            self._code_handlers[info_code]()
        except KeyError:
            raise UnknownWSSInfo(output_msg)
--------------------
Prompt: 
def _getWorkerCommandList(self):
        """Generate the workerCommand as list"""
----------
Output: 

        return [self.workerCommand, self.workerCommand]

    def _getWorkerCommand(self, command):
        """Return the command to run the worker"""
        return self.workerCommands[command]

    def _getWorkerCommandList(self):
        """Generate the workerCommand as list"""
        return [self.workerCommands['getWorkerCommand'](i) for i in self.workerCommands]

    def _getWorkerCommand(self, command):
        """Return the command to run the worker"""
        return self.workerCommands[command]

    def _getWorkerCommandList(self):
        """Generate the
----------
Solution: 

        c = []
        c.extend(self._WorkerCommand_environment())
        c.extend(self._WorkerCommand_launcher())
        c.extend(self._WorkerCommand_options())
        c.extend(self._WorkerCommand_executable())

        return c
--------------------
Prompt: 
def _GetFileNames(self):
    """Returns a list of file names in the feed."""
----------
Output: 

    return self._feed_names

  def _GetFeedNames(self):
    """Returns a list of feed names in the feed."""
    return self._feed_names


class _FetchMapper(object):
  """Fetch mapper for fetching values from a database.

  Fetches values from a database such as the entity id, and all properties in the
  database.  This is used to fetch the entity's properties from the database.
  """

  def __init__(self, db, map_func):
    """Constructor.

    Args:
      db: A database name (string) for the datastore.
      map_
----------
Solution: 

    if self._zip:
      return self._zip.namelist()
    else:
      return os.listdir(self._path)
--------------------
Prompt: 
def git_status(self, r):
        """return tuple of (status, adds, dels).
        status like !! ??
        adds and dels are lists of additions and deletions.
        """
----------
Output: 

        if r.status_code == 200:
            return r.json(), r.headers
        else:
            return r.json(), r.headers

    def get_user_info(self, username):
        """return dict of user info.
        username like!!??
        """
        return {
            'username': username,
            'email': username + '@example.com',
            'id': username,
            'name': username,
            'avatar': self.avatar,
            'avatar_url': self.avatar_url,
            'url': 'https://api.github.com/users/%s
----------
Solution: 

        ret = self._cachedStatus.get(r.filename, None) if r else None
        return ret if ret else ["//", None, None]
--------------------
Prompt: 
def attach_vpn_gateway(self, vpn_gateway_id, vpc_id):
        """
        Attaches a VPN gateway to a VPC.

        :type vpn_gateway_id: str
        :param vpn_gateway_id: The ID of the vpn_gateway to attach

        :type vpc_id: str
        :param vpc_id: The ID of the VPC you want to attach the gateway to.

        :rtype: An attachment
        :return: a :class:`boto.vpc.vpngateway.Attachment`
        """
----------
Output: 

        params = {'Action': 'AttachVpnGateway',
                  'VpnGatewayId': vpn_gateway_id,
                  'VpcId': vpc_id,
                  'Type': 'VPNC',
                  'VpnGatewayAttachmentId': self.id}

        response = self.connection.make_request('AttachVpnGateway', params)
        attachment = response.get('Attachment', [])

        for el in attachment:
            if el.get('IsAttachable') == 'true':
                return el
        return None

    def detach_vpn_gateway(self):
        """
        Detaches a VPN connection from a VPC.
----------
Solution: 

        params = {'VpnGatewayId': vpn_gateway_id,
                  'VpcId' : vpc_id}
        return self.get_object('AttachVpnGateway', params, Attachment)
--------------------
Prompt: 
def get_check_threads(self):
        """Return iterator of checker threads."""
----------
Output: 

        for checker in self.checkers:
            if checker.is_enabled():
                yield checker

    def get_checkers(self):
        """Return iterator of registered checkers."""
        return self.checkers

    def get_checkers_for_file(self, filename):
        """Return a list of registered checkers for the given filename."""
        if self.is_enabled(filename):
            yield self.get_checkers_for_enabled(filename)
        if self.is_disabled(filename):
            yield self.get_checkers_for_disabled(filename)
        if self.is_enabled(filename):
           
----------
Solution: 

        for t in self.threads:
            name = t.getName()
            if name.startswith("CheckThread-"):
                yield name
--------------------
Prompt: 
def solve(self):
        """
        Solve the cross.
        """
----------
Output: 

        self.set_parameters(self.parameters)
        self.set_parameters(self.parameters)
        self.set_parameters(self.parameters)
        self.set_parameters(self.parameters)
        self.set_parameters(self.parameters)
        self.set_parameters(self.parameters)
        self.set_parameters(self.parameters)
        self.set_parameters(self.parameters)
        self.set_parameters(self.parameters)
        self.set_parameters(self.parameters)
        self.set_parameters(self.parameters)
        self.set_parameters(
----------
Solution: 

        result = Formula(path_actions(a_star_search(
            ({f: self.cube[f] for f in "LUFDRB"}, 
             self.cube.select_type("edge") & self.cube.has_colour(self.cube["D"].colour)), 
            self.cross_successors, 
            self.cross_state_value, 
            self.cross_goal, 
            )))
        self.cube(result)
        return result
--------------------
Prompt: 
def parse_optimize(self):
        """Parse the ouput resulted of a geometry optimization. Or a
        saddle point.

        """
----------
Output: 

        self.parse_geometry_opt()
        self.parse_saddle_point_opt()

    def parse_geometry_opt(self):
        """Parse the ouput geometry optimization.

        """
        if self.options.geometry_opt:
            self.options.geometry_opt = self.options.geometry_opt.strip()
        else:
            self.options.geometry_opt = None
        self.options.geometry_opt = None
        self.options.geometry_opt = None
        self.options.geometry_opt = None
        self.options.geometry_opt = None
        if self
----------
Solution: 

        match = re.search("EQUILIBRIUM GEOMETRY LOCATED", self.text)
        spmatch = "SADDLE POINT LOCATED" in self.text
        located = True if match or spmatch else False

        points = grep_split(" BEGINNING GEOMETRY SEARCH POINT NSERCH=",
                            self.text)
        if self.tddft == "excite":
            points = [self.parse_energy(point) for point in points[1:]]
        else:
            regex = re.compile(r'NSERCH:\s+\d+\s+E=\s+([+-]?\d+\.\d+)')
            points = [Energy(states=[State(0,None,float(m.group(1)), 0.0, 0.0)]) for m in regex.finditer(self.text)]
        
        # Error handling
        if "FAILURE TO LOCATE STATIONARY POINT, TOO MANY STEPS TAKEN" in self.text:
            self.errcode = GEOM_NOT_LOCATED
            self.errmsg = "too many steps taken: %i"%len(points)
        
        if located:
            self.errcode = OK
        
        return Optimize(points=points)
--------------------
Prompt: 
def solve_kkt(Q_LU, d, G, A, S_LU, rx, rs, rz, ry):
    """ Solve KKT equations for the affine step"""
----------
Output: 

    # TODO: Implement the KKT equations for the affine step

    # TODO: Implement the KKT equations for the step size
    # TODO: Implement the KKT equations for the Jacobian
    # TODO: Implement the KKT equations for the Hessian
    # TODO: Implement the KKT equations for the Hessian
    # TODO: Implement the KKT equations for the Hessian
    # TODO: Implement the KKT equations for the Hessian
    # TODO: Implement the KKT equations for the Hessian
    # TODO: Implement the KKT equations for the Hessian
    # TODO: Implement the KKT equations for the H
----------
Solution: 

    nineq, nz, neq, nBatch = get_sizes(G, A)

    invQ_rx = rx.btrisolve(*Q_LU)
    if neq > 0:
        h = torch.cat((invQ_rx.unsqueeze(1).bmm(A.transpose(1, 2)).squeeze(1) - ry,
                       invQ_rx.unsqueeze(1).bmm(G.transpose(1, 2)).squeeze(1) + rs / d - rz), 1)
    else:
        h = invQ_rx.unsqueeze(1).bmm(G.transpose(1, 2)).squeeze(1) + rs / d - rz

    w = -(h.btrisolve(*S_LU))

    g1 = -rx - w[:, neq:].unsqueeze(1).bmm(G).squeeze(1)
    if neq > 0:
        g1 -= w[:, :neq].unsqueeze(1).bmm(A).squeeze(1)
    g2 = -rs - w[:, neq:]

    dx = g1.btrisolve(*Q_LU)
    ds = g2 / d
    dz = w[:, neq:]
    dy = w[:, :neq] if neq > 0 else None

    return dx, ds, dz, dy
--------------------
Prompt: 
def script(container, script_path, fail_nonzero=False, upload_dir=False, **kwargs):
    """
    Runs a script inside a container, which is created with all its dependencies. The container is removed after it
    has been run, whereas the dependencies are not destroyed. The output is printed to the console.

    :param container: Container configuration name.
    :param script_path: Local path to the script file.
    :param fail_nonzero: Fail if the script returns with a nonzero exit code.
    :param upload_dir: Upload the entire parent directory of the script file to the remote.
    :param kwargs: Additional keyword arguments to the run_script action.
    """
----------
Output: 

    :param kwargs: Keyword arguments to pass to the script.
    :return: Remote command output.
    """
    kwargs['fail_nonzero'] = fail_nonzero
    if not upload_dir:
        upload_dir = True
    return run_command(
        [
            'bash',
            '-c',
            script_path,
            '-e',
            'bash -c "{0}"'.format(script_path),
            '-e',
            'bash -c "{0}"'.format(script_path),
            '-e',
            'bash -c "{0}"'.format(script_path),
            '-e
----------
Solution: 

    full_script_path = os.path.abspath(script_path)
    prefix, name = os.path.split(full_script_path)
    with temp_dir() as remote_tmp:
        if upload_dir:
            prefix_path, prefix_name = os.path.split(prefix)
            remote_script = posixpath.join(remote_tmp, prefix_name, name)
            put(prefix, remote_tmp, mirror_local_mode=True)
        else:
            remote_script = posixpath.join(remote_tmp, name)
            put(script_path, remote_script, mirror_local_mode=True)
        results = [output.result
                   for output in container_fabric().run_script(container, script_path=remote_script, **kwargs)
                   if o.action_type == ContainerUtilAction.SCRIPT]
    for res in results:
        puts("Exit code: {0}".format(res['exit_code']))
        if res['exit_code'] == 0 or not fail_nonzero:
            puts(res['log'])
        else:
            error(res['log'])
--------------------
Prompt: 
def destroy(self, request, pk=None):
        """For DELETE actions, actually deactivate the user, don't delete."""
----------
Output: 

        try:
            self.user = User.objects.get(pk=pk)
        except User.DoesNotExist:
            return Response(status=status.HTTP_404_NOT_FOUND)
        if not self.user.is_active:
            self.user.is_active = True
            self.user.save()
        return Response(status=status.HTTP_204_NO_CONTENT)

    def get_context_data(self, **kwargs):
        """Add the current user to the context."""
        context = super(UserViewSet, self).get_context_data(**kwargs)
        context['
----------
Solution: 

        user = self.get_object()
        user.is_active = False
        user.save()
        return Response(status=status.HTTP_204_NO_CONTENT)
--------------------
Prompt: 
def output_filename(output_dir, key_handle, public_id):
    """
    Return an output filename for a generated AEAD. Creates a hashed directory structure
    using the last three bytes of the public id to get equal usage.
    """
----------
Output: 

    return os.path.join(output_dir, 'aead_%s_%s.txt' % (key_handle, public_id))


def generate_aead_key(key_length=32):
    """
    Generate a unique AEAD key.
    """
    return os.path.join(key_dir(), 'aead_key_%s_%s' % (key_length, 'rsa' * key_length))


def generate_rsa_key(key_length=32):
    """
    Generate a unique RSA key.
    """
    return os.path.
----------
Solution: 

    parts = [output_dir, key_handle] + pyhsm.util.group(public_id, 2)
    path = os.path.join(*parts)

    if not os.path.isdir(path):
        os.makedirs(path)

    return os.path.join(path, public_id)
--------------------
Prompt: 
def _set_mpls_traffic_bypasses(self, v, load=False):
    """
    Setter method for mpls_traffic_bypasses, mapped from YANG variable /telemetry/profile/mpls_traffic_bypass/mpls_traffic_bypasses (list)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_mpls_traffic_bypasses is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_mpls_traffic_bypasses() directly.
    """
----------
Output: 
.

    YANG Description: This list describes the MPLS traffic bypasses between
the source and the telemetry client
"""
    if v is None:
      v = []
    elif hasattr(v, '__iter__'):
      v = list(v)
    else:
      v = [v]

    if load:
      self.mpls_traffic_bypass = v
    else:
      self.mpls_traffic_bypass = [ ]

    return

  def mmpls_traffic_bypass(self, mmpls_client_id):
    """
    Returns the specified MMPLS client ID, or raises an exception
   
----------
Solution: 

    if hasattr(v, "_utype"):
      v = v._utype(v)
    try:
      t = YANGDynClass(v,base=YANGListType("mpls_traffic_bypass_name",mpls_traffic_bypasses.mpls_traffic_bypasses, yang_name="mpls-traffic-bypasses", rest_name="bypass-lsp", parent=self, is_container='list', user_ordered=False, path_helper=self._path_helper, yang_keys='mpls-traffic-bypass-name', extensions={u'tailf-common': {u'callpoint': u'Mplstrafficbypass', u'cli-suppress-mode': None, u'alt-name': u'bypass-lsp', u'info': u'MPLS Stats profile by Bypass LSP name', u'cli-suppress-list-no': None}}), is_container='list', yang_name="mpls-traffic-bypasses", rest_name="bypass-lsp", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'callpoint': u'Mplstrafficbypass', u'cli-suppress-mode': None, u'alt-name': u'bypass-lsp', u'info': u'MPLS Stats profile by Bypass LSP name', u'cli-suppress-list-no': None}}, namespace='urn:brocade.com:mgmt:brocade-telemetry', defining_module='brocade-telemetry', yang_type='list', is_config=True)
    except (TypeError, ValueError):
      raise ValueError({
          'error-string': 
--------------------
Prompt: 
def pc_anova(self, covariates, num_pc=5):
        """ 
        Calculate one-way ANOVA between the first num_pc prinicipal components
        and known covariates. The size and index of covariates determines
        whether u or v is used.
    
        Parameters
        ----------
        covariates : pandas.DataFrame
            Dataframe of covariates whose index corresponds to the index of
            either u or v. 
    
        num_pc : int
            Number of principal components to correlate with.
    
        Returns
        -------
        anova : pandas.Panel
            Panel with F-values and p-values.
    
        """
----------
Output: 
 the ANOVA between the first num_pc prinicipal components
            and known covariates. The size and index of panels determines
            whether u or v is used.
    
        """
        # Get the panels
        panels = self.get_panels(covariates)

        # Get the size and index of panels
        size_index = panels.index.size
        index_size = size_index[size_index > 1]
        index_index = index_size[index_size > 1]
        index_u = index_index[u > 1]
        index_v
----------
Solution: 

        from scipy.stats import f_oneway
        if (covariates.shape[0] == self.u.shape[0] and 
            len(set(covariates.index) & set(self.u.index)) == self.u.shape[0]):
            mat = self.u
        elif (covariates.shape[0] == self.v.shape[0] and 
            len(set(covariates.index) & set(self.v.index)) == self.v.shape[0]):
            mat = self.v
        anova = pd.Panel(items=['fvalue', 'pvalue'],
                         major_axis=covariates.columns,
                         minor_axis=mat.columns[0:num_pc])
        for i in anova.major_axis:
            for j in anova.minor_axis:
                t = [mat[j][covariates[i] == x] for x in set(covariates[i])]
                f, p = f_oneway(*t)
                anova.ix['fvalue', i, j] = f 
                anova.ix['pvalue', i, j] = p 
        return anova
--------------------
Prompt: 
def reset(self, indices=None):
    """Reset the batch of environments.

    Args:
      indices: The batch indices of the environments to reset; defaults to all.

    Returns:
      Batch tensor of the new observations.
    """
----------
Output: 

    if indices is None:
      indices = tf.range(self.batch_size)
    if self.state_is_tuple:
      return tf.concat([self.state_tuple_state, self.state_tuple_action], axis=1)
    else:
      return tf.concat([self.state_is_tuple_action, self.action_tuple_state, self.state_is_tuple], axis=1)

  def _get_observation_from_action(self, action):
    """Get the observation tensor from the action.

    Args:
      action: The action to
----------
Solution: 

    if indices is None:
      indices = tf.range(len(self._batch_env))
    observ_dtype = self._parse_dtype(self._batch_env.observation_space)
    observ = tf.py_func(
        self._batch_env.reset, [indices], observ_dtype, name='reset')
    observ = tf.check_numerics(observ, 'observ')
    reward = tf.zeros_like(indices, tf.float32)
    done = tf.zeros_like(indices, tf.bool)
    with tf.control_dependencies([
        tf.scatter_update(self._observ, indices, observ),
        tf.scatter_update(self._reward, indices, reward),
        tf.scatter_update(self._done, indices, done)]):
      return tf.identity(observ)
--------------------
Prompt: 
def getDataset(self, id_):
        """
        Returns a dataset with the specified ID, or raises a
        DatasetNotFoundException if it does not exist.
        """
----------
Output: 

        return self._get_client().getDataset(id_)

    def getDatasetByID(self, id_):
        """
        Returns a dataset with the specified ID, or raises a
        DatasetNotFoundException if it does not exist.
        """
        return self._get_client().getDatasetByID(id_)

    def getDatasetByIDAndUrl(self, id_, url_):
        """
        Returns a dataset with the specified ID, or raises a
        DatasetNotFoundException if it does not exist.
        """
        return self._get_client().getDatasetByIDAndUrl(id_, url_)

    def getDatasetBy
----------
Solution: 

        if id_ not in self._datasetIdMap:
            raise exceptions.DatasetNotFoundException(id_)
        return self._datasetIdMap[id_]
--------------------
Prompt: 
def p_compound_list(p):
    """compound_list : list
                     | newline_list list1"""
----------
Output: 

    p[0] = p[1]
    p[0].AddChildren(p[2])

def p_compound_list1(p):
    """compound_list : list1
                     | newline_list list2
                     | newline_list list3
                     | newline_list list4
                     | newline_list list5
                     | newline_list list6
                     | newline_list list7
                     | newline_list list8"""
    if len(p) == 2:
        p[0] = p[1]
    elif len(p) == 3:
        p[0] = p[
----------
Solution: 

    if len(p) == 2:
        p[0] = p[1]
    else:
        parts = p[2]
        if len(parts) > 1:
            p[0] = ast.node(kind='list', parts=parts, pos=_partsspan(parts))
        else:
            p[0] = parts[0]
--------------------
Prompt: 
def pre_save(self, model_instance, add):
        """
        Resizes, commits image to storage, and returns field's value just before saving.
        """
----------
Output: 

        self.model_instance = model_instance
        self.add = add
        return self.get_field_value(self.model_instance,'size')

    def pre_delete(self, model_instance, delete):
        """
        Revokes the delete method on the given model instance.
        """
        self.model_instance = model_instance
        self.delete = delete
        return self.get_field_value(self.model_instance, 'delete')

    def pre_save_with_user(self, user, add):
        """
        Revokes the save method on the
----------
Solution: 

        file = getattr(model_instance, self.attname)
        if file and not file._committed:
            file.name = self._clean_file_name(model_instance, file.name)
            file.file = self._resize_image(model_instance, file)
            file.save(file.name, file, save=False)
        return file
--------------------
Prompt: 
def get_signing_key(self, key_type):
        """GetSigningKey.
        [Preview API]
        :param str key_type:
        :rtype: str
        """
----------
Output: 

        return self._signing_key

    def get_signing_key_value(self, key_type):
        """GetSigningKeyValue.
        [Preview API]
        :param str key_type:
        :rtype: str
        """
        return self._signing_key_value

    def set_signing_key_value(self, key_type, value):
        """SetSigningKeyValue.
        [Preview API]
        :param str key_type:
        :param str value:
        :rtype: str
        """
        self._signing_key_value = key_type, value
        return
----------
Solution: 

        route_values = {}
        if key_type is not None:
            route_values['keyType'] = self._serialize.url('key_type', key_type, 'str')
        response = self._send(http_method='GET',
                              location_id='92ed5cf4-c38b-465a-9059-2f2fb7c624b5',
                              version='5.0-preview.1',
                              route_values=route_values)
        return self._deserialize('str', response)
--------------------
Prompt: 
def merge_networks(network, donor=[]):
    r"""
    Combine multiple networks into one without doing any topological
    manipulations (such as stiching nearby pores to each other).

    Parameters
    ----------
    network : OpenPNM Network Object
        The network to which all the other networks should be added.

    donor : OpenPNM Network Object or list of Objects
        The network object(s) to add to the given network

    Notes
    -----
    This methods does *not* attempt to stitch the networks topologically.

    See Also
    --------
    extend
    trim
    stitch

    """
----------
Output: 

    if isinstance(donor, (list, tuple)):
        donor = tuple(map(lambda x: x.to_xml(), donor))
    if isinstance(network, (OpenPNMNetwork, OpenPNMNetworkList)):
        if isinstance(network_list, (list, tuple)):
            network = open(network_list[0], 'r').read()
        else:
            network = open(OpenPNM.Network, 'r').read()
        for donor_item in donor:
            network = network.replace(donor_item, donor_item.to_xml
----------
Solution: 

    if type(donor) == list:
        donors = donor
    else:
        donors = [donor]

    for donor in donors:
        network['pore.coords'] = sp.vstack((network['pore.coords'],
                                            donor['pore.coords']))
        network['throat.conns'] = sp.vstack((network['throat.conns'],
                                             donor['throat.conns'] +
                                             network.Np))
        p_all = sp.ones((sp.shape(network['pore.coords'])[0],), dtype=bool)
        t_all = sp.ones((sp.shape(network['throat.conns'])[0],), dtype=bool)
        network.update({'pore.all': p_all})
        network.update({'throat.all': t_all})
        for key in set(network.keys()).union(set(donor.keys())):
            if key.split('.')[1] not in ['conns', 'coords', '_id', 'all']:
                if key in network.keys():
                    pop_flag = False
                    if key not in donor.keys():
                        logger.debug('Adding ' + key + ' to donor')
                        # If key not on donor add it first
                        if network[key].dtype == bool:
                            donor[key] = False
                        else:
                            donor[key] = sp.nan
                        pop_flag = True
                    # Then merge it with existing array on network
                    try:
                        temp = sp.hstack((network[key], donor[key]))
                    except ValueError:
                        temp = sp.vstack((network[key], donor[key]))
                    network[key] = temp
                    if pop_flag:
                        donor.pop(key, None)
                else:
                    # If key not on network add it first
                    logger.debug('Adding ' + key + ' to network')
                    if donor[key].dtype == bool:
                        network[key] = False
                    else:
                        network[key] = sp.nan
                    # Then append donor values to network
                    s = sp.shape(donor[key])[0]
                    network[key][-s:] = donor[key]

    # Clear adjacency and incidence matrices which will be out of date now
    network._am.clear()
    network._im.clear()
--------------------
Prompt: 
def _process_interactions(self, row):
        """
        Process row of CTD data from CTD_chemicals_diseases.tsv.gz
        and generate triples. Only create associations based on direct evidence
        (not using the inferred-via-gene), and unambiguous relationships.
        (Ambiguous ones will be processed in the sister method using the
        disambiguated file). There are no OMIM ids for diseases in these cases,
        so we associate with only the mesh disease ids.
        Args:
            :param row (list): row of CTD data
        Returns:
            :return None
        """
----------
Output: 
 data.
        """
        interactions = []
        for i, row_interactions in enumerate(row):
            if row_interactions[0] == 'O':
                interactions.append(row_interactions[1])
            else:
                interactions.append(row_interactions[1])

        if len(row) > 1:
            interactions.append(row[0])

        if len(row) > 2:
            interactions.append(row[1])

        if len(row) > 3:
            interactions.append(row[2])

        if len(row) >
----------
Solution: 

        model = Model(self.graph)
        self._check_list_len(row, 10)
        (chem_name, chem_id, cas_rn, disease_name, disease_id, direct_evidence,
         inferred_gene_symbol, inference_score, omim_ids, pubmed_ids) = row

        if direct_evidence == '':
            return

        evidence_pattern = re.compile(r'^therapeutic|marker\/mechanism$')
        # dual_evidence = re.compile(r'^marker\/mechanism\|therapeutic$')

        # filter on those diseases that are mapped to omim ids in the test set
        intersect = list(
            set(['OMIM:' + str(i) for i in omim_ids.split('|')] +
                [disease_id]) & set(self.test_diseaseids))
        if self.test_mode and len(intersect) < 1:
            return
        chem_id = 'MESH:' + chem_id
        reference_list = self._process_pubmed_ids(pubmed_ids)
        if re.match(evidence_pattern, direct_evidence):
            rel_id = self.resolve(direct_evidence)
            model.addClassToGraph(chem_id, chem_name)
            model.addClassToGraph(disease_id, None)
            self._make_association(chem_id, disease_id, rel_id, reference_list)
        else:
            # there's dual evidence, but haven't mapped the pubs
            pass
            # LOG.debug(
            #   "Dual evidence for %s (%s) and %s (%s)",
            #   chem_name, chem_id, disease_name, disease_id)

        return
--------------------
Prompt: 
def unpack_text_io_wrapper(fp, encoding):
  """
  If *fp* is a #io.TextIOWrapper object, this function returns the underlying
  binary stream and the encoding of the IO-wrapper object. If *encoding* is not
  None and does not match with the encoding specified in the IO-wrapper, a
  #RuntimeError is raised.
  """
----------
Output: 

  return codecs.getreader(encoding)(fp)

def read_text_file(filename):
  """
  Read a file in text mode.

  :param filename: path of the file to read
  :type filename: str
  :return: text file content
  :rtype: str
  """
  with open(filename, 'rb') as fp:
    return fp.read()

def write_text_file(filename, content):
  """
  Write a file in text mode.

  :param filename: path of the file to write
  :type filename: str
  :param content: text file
----------
Solution: 


  if isinstance(fp, io.TextIOWrapper):
    if fp.writable() and encoding is not None and fp.encoding != encoding:
      msg = 'TextIOWrapper.encoding({0!r}) != {1!r}'
      raise RuntimeError(msg.format(fp.encoding, encoding))
    if encoding is None:
      encoding = fp.encoding
    fp = fp.buffer

  return fp, encoding
--------------------
Prompt: 
def make_optimised_chunks(self, min_length, max_length, pad_data=0):
    """
    Splits ScienceSegments up into chunks, of a given maximum length.
    The length of the last two chunks are chosen so that the data
    utilisation is optimised.
    @param min_length: minimum chunk length.
    @param max_length: maximum chunk length.
    @param pad_data: exclude the first and last pad_data seconds of the
    segment when generating chunks
    """
----------
Output: 

    if self.is_optimised:
      return
    min_length = min_length or self.min_length
    max_length = max_length or self.max_length
    if pad_data:
      self.last_data = pad_data
    self.last_length = max_length
    self.last_data_length = len(pad_data)
    self.last_length_min = min_length
    self.last_length_max = max_length
    self.last_data = self.last_data[:min_length]
    self.last_data_
----------
Solution: 

    for seg in self.__sci_segs:
      # pad data if requested
      seg_start = seg.start() + pad_data
      seg_end = seg.end() - pad_data

      if seg.unused() > max_length:
        # get number of max_length chunks
        N = (seg_end - seg_start)/max_length

        # split into chunks of max_length
        for i in range(N-1):
          start = seg_start + (i * max_length)
          stop = start + max_length
          seg.add_chunk(start, stop)

        # optimise data usage for last 2 chunks
        start = seg_start + ((N-1) * max_length)
        middle = (start + seg_end)/2
        seg.add_chunk(start, middle)
        seg.add_chunk(middle, seg_end)
        seg.set_unused(0)
      elif seg.unused() > min_length:
        # utilise as single chunk
        seg.add_chunk(seg_start, seg_end)
      else:
        # no chunk of usable length
        seg.set_unused(0)
--------------------
Prompt: 
def extract_tar(filename, dest_path, **kwargs):
    """
    Extracts a TAR archive. All element names starting with ``/`` (indicating an absolute path) or that contain ``..``
    as references to a parent directory are not extracted.

    :param filename: Path to the tar file.
    :type filename: unicode
    :param dest_path: Destination path to extract the contents to.
    :type dest_path: unicode
    :param kwargs: Additional kwargs for opening the TAR file (:func:`tarfile.open`).
    """
----------
Output: 

    if not os.path.isabs(filename):
        filename = os.path.join(dest_path, filename)
    with tarfile.open(filename, **kwargs) as tar:
        for member in tar.getmembers():
            tar.extract(member, dest_path)
    return tar


def extract_zip(filename, dest_path, **kwargs):
    """
    Extracts a ZIP archive. All elements names starting with ``/`` (indicating an absolute path) or that contain
    ``..`` as references to a parent directory are not extracted.

    :param filename: Path to the zip
----------
Solution: 

    with tarfile.open(filename, 'r', **kwargs) as tf:
        safe_members = [name for name in tf.getmembers() if _safe_name(name)]
        if safe_members:
            tf.extractall(dest_path, safe_members)
--------------------
Prompt: 
def _head_object(s3_conn, bucket, key):
    """Retrieve information about an object in S3 if it exists.

    Args:
        s3_conn (botocore.client.S3): S3 connection to use for operations.
        bucket (str): name of the bucket containing the key.
        key (str): name of the key to lookup.

    Returns:
        dict: S3 object information, or None if the object does not exist.
            See the AWS documentation for explanation of the contents.

    Raises:
        botocore.exceptions.ClientError: any error from boto3 other than key
            not found is passed through.
    """
----------
Output: 

            verification failed.
    """
    try:
        s3_conn.head_object(Bucket=bucket, Key=key)
    except botocore.exceptions.ClientError as e:
        if e.response['Error']['Code'] == '404':
            return None
        raise

    return s3_conn.get_object(Bucket=bucket, Key=key)


def _get_bucket_key_names(s3_conn):
    """Retrieve the names of the buckets in the S3 object.

    Args:
        s3_conn (botocore.client.S3): S3 connection to use
----------
Solution: 

    try:
        return s3_conn.head_object(Bucket=bucket, Key=key)
    except botocore.exceptions.ClientError as e:
        if e.response['Error']['Code'] == '404':
            return None
        else:
            raise
--------------------
Prompt: 
def truncate_table(self, table):
        """ Responsys.truncateTable call

        Accepts:
            InteractObject table

        Returns True on success
        """
----------
Output: 

        if table == None:
            return False
        if table.type == Table.Type_Feature:
            return True
        return False

    def get_table_id(self, table):
        """ Responsys.getTableId call

        Accepts:
            InteractObject table

        Returns the id of the table
        """
        if table == None:
            return None
        if table.id == None:
            return None
        return table.id

    def get_table_name(self, table):
        """ Responsys.getTableName call

        Accepts:
            InteractObject table

        Returns the name of
----------
Solution: 

        table = table.get_soap_object(self.client)
        return self.call('truncateTable', table)
--------------------
Prompt: 
def parse_packets(self, binary_packets):
        """
        Parses binary packets and return a list of parsed packets.

        DOES NOT CLOSE tshark. It must be closed manually by calling close() when you're done
        working with it.
        """
----------
Output: 

        return [parse_packet(packet) for packet in binary_packets]

    def close(self):
        """
        Closes the connection.
        """
        if self.sock is not None:
            self.sock.close()
            self.sock = None
        self.sock = None

    def __enter__(self):
        """
        Entering a new connection is a no-op if no address is given.
        """
        if self.sock is None:
            self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            self.sock.connect(self.host,
----------
Solution: 

        if not binary_packets:
            raise ValueError("Must supply at least one packet")
        parsed_packets = []

        if not self._current_tshark:
            self.eventloop.run_until_complete(self._get_tshark_process())
        for binary_packet in binary_packets:
            self._write_packet(binary_packet)

        def callback(pkt):
            parsed_packets.append(pkt)
            if len(parsed_packets) == len(binary_packets):
                raise StopCapture()

        self.eventloop.run_until_complete(self._get_parsed_packet_from_tshark(callback))
        return parsed_packets
--------------------
Prompt: 
def _refresh_state(self):
    """ Get the state of a job. Must be overridden by derived Job classes
        for Jobs that don't use a Future.
    """
----------
Output: 

    return self._state

  def _check_cancelled(self):
    """ Check if the job has been cancelled.
    """
    return self._cancelled

  def _check_cancelled_exception(self):
    """ Check if the job has been cancelled.
    """
    return self._cancelled_exception

  def _get_task_id(self):
    """ Get the task_id of a task.
    """
    return self._task_id

  def _set_task_id(self, task_id):
    """ Set the task_id of a task.
    """
    self._task_id = task
----------
Solution: 

    if self._is_complete:
      return

    if not self._future:
      raise Exception('Please implement this in the derived class')

    if self._future.done():
      self._is_complete = True
      self._end_time = datetime.datetime.utcnow()
      try:
        self._result = self._future.result()
      except Exception as e:
        message = str(e)
        self._fatal_error = JobError(location=traceback.format_exc(), message=message,
                                     reason=str(type(e)))
--------------------
Prompt: 
def _codec_can_decode_with_surrogatepass(codec, _cache={}):
    """Returns if a codec supports the surrogatepass error handler when
    decoding. Some codecs were broken in Python <3.4
    """
----------
Output: 

    try:
        codecs.lookup(encoding.lower())
    except LookupError:
        return False
    else:
        return True


def _codec_can_encode(codec, encoding):
    """Returns if a codec can encode the given encoding.
    """
    try:
        return codecs.lookup(encoding.lower())
    except LookupError:
        return False


def _codec_can_decode(codec, encoding):
    """Returns if a codec supports the surrogatepass error handler when
    decoding. Some codecs were broken in Python <3.4
    """
    try:
        codecs.lookup(encoding.lower())
----------
Solution: 


    try:
        return _cache[codec]
    except KeyError:
        try:
            u"\ud83d".encode(
                codec, _surrogatepass).decode(codec, _surrogatepass)
        except UnicodeDecodeError:
            _cache[codec] = False
        else:
            _cache[codec] = True
        return _cache[codec]
--------------------
Prompt: 
def R_package_resource(package, resource):
    """
    return a path to an R package resource, if it is available
    """
----------
Output: 

    if package.endswith('.rpm'):
        return resource
    return os.path.join(R_HOME, 'rpm', resource)


def R_package_path(package):
    """
    return the path to a R package
    """
    return R_HOME + '/lib/rpm/' + package


def R_package_path_glob(package):
    """
    return the path to a R package
    """
    return R_HOME + '/lib/rpm/rpm/' + R_package_resource(package, 'rpm') + '/lib/rpm/rpm'


def R_package
----------
Solution: 

    package_path = R_package_path(package)
    if not package_path:
        return None
    package_resource = os.path.join(package_path, resource)
    if not file_exists(package_resource):
        return None
    else:
        return package_resource
--------------------
Prompt: 
def week_schedule(index, on_time=None, off_time=None, off_days=None):
    """ Return boolean time series following given week schedule.

    Parameters
    ----------
    index : pandas.DatetimeIndex
        Datetime index
    on_time : str or datetime.time
        Daily opening time. Default: '09:00'
    off_time : str or datetime.time
        Daily closing time. Default: '17:00'
    off_days : list of str
        List of weekdays. Default: ['Sunday', 'Monday']

    Returns
    -------
    pandas.Series of bool
        True when on, False otherwise for given datetime index

    Examples
    --------
    >>> import pandas as pd
    >>> from opengrid.library.utils import week_schedule
    >>> index = pd.date_range('20170701', '20170710', freq='H')
    >>> week_schedule(index)
    """
----------
Output: 

        Daily schedule.

    """
    if on_time is None:
        on_time = '09:00'
    if off_time is None:
        off_time = '17:00'
    if off_days is None:
        off_days = ['Sunday', 'Monday']
    if isinstance(index, pandas.DatetimeIndex):
        index = index.to_pydatetime()
    if isinstance(index, str):
        index = pd.DatetimeIndex(index)
    if isinstance(index, pd.Series):
        return index
    else:
        return pd.Series(index, dtype
----------
Solution: 

    if on_time is None:
        on_time = '9:00'
    if off_time is None:
        off_time = '17:00'
    if off_days is None:
        off_days = ['Sunday', 'Monday']
    if not isinstance(on_time, datetime.time):
        on_time = pd.to_datetime(on_time, format='%H:%M').time()
    if not isinstance(off_time, datetime.time):
        off_time = pd.to_datetime(off_time, format='%H:%M').time()
    times = (index.time >= on_time) & (index.time < off_time) & (~index.weekday_name.isin(off_days))
    return pd.Series(times, index=index)
--------------------
Prompt: 
def _compute_mean_on_rock(self, C, mag, rrup, F, HW):
        """
        Compute mean value on rock (that is eq.1, page 105 with S = 0)
        """
----------
Output: 

        return C['R_R_mean'][0][mag] + C['R_R_mean'][0][rrup]

    def _compute_mean_on_rock_error(self, C, rrup, F, HW):
        """
        Compute mean value on rock (that is eq.2, page 105 with S = 0)
        """
        return C['R_R_mean'][0][mag] + C['R_R_mean'][0][rrup]

    def _compute_mean_on_rock_rate(self, C, rrup, F,
----------
Solution: 

        f1 = self._compute_f1(C, mag, rrup)
        f3 = self._compute_f3(C, mag)
        f4 = self._compute_f4(C, mag, rrup)

        return f1 + F * f3 + HW * f4
--------------------
Prompt: 
def thumbprint(self, hashalg=hashes.SHA256()):
        """Returns the key thumbprint as specified by RFC 7638.

        :param hashalg: A hash function (defaults to SHA256)
        """
----------
Output: 

        return self._thumbprint(hashalg=hashalg)

    def _thumbprint(self, hashalg=hashes.SHA256()):
        """Returns the key thumbprint as specified by RFC 7638.

        :param hashalg: A hash function (defaults to SHA256)
        """
        if hashalg == hashes.SHA256:
            return self._key_thumbprint_sha256
        else:
            return self._key_thumbprint_sha512

    def _key_thumbprint_sha512(self, hashalg=hashes.SHA256()):
        """Returns the key thumbprint as specified by RFC 76
----------
Solution: 


        t = {'kty': self._params['kty']}
        for name, val in iteritems(JWKValuesRegistry[t['kty']]):
            if val.required:
                t[name] = self._key[name]
        digest = hashes.Hash(hashalg, backend=default_backend())
        digest.update(bytes(json_encode(t).encode('utf8')))
        return base64url_encode(digest.finalize())
--------------------
Prompt: 
def _compile_constant_expression(self,
                                     expr: Expression,
                                     scope: Dict[str, TensorFluent],
                                     batch_size: Optional[int] = None,
                                     noise: Optional[List[tf.Tensor]] = None) -> TensorFluent:
        """Compile a constant expression `expr` into a TensorFluent
        in the given `scope` with optional batch size.

        Args:
            expr (:obj:`rddl2tf.expr.Expression`): A RDDL constant expression.
            scope (Dict[str, :obj:`rddl2tf.fluent.TensorFluent`]): A fluent scope.
            batch_size (Optional[size]): The batch size.

        Returns:
            :obj:`rddl2tf.fluent.TensorFluent`: The compiled expression as a TensorFluent.
        """
----------
Output: 
Fluent`]): A dictionary of parameters
                that will be passed to the :obj:`rddl2tf.constant_op` method.
            batch_size (Optional[int]): The number of elements in the batch.
            noise (Optional[List[tf.Tensor]]): A list of Tensor
            """
        if batch_size is not None:
            if not isinstance(batch_size, list):
                batch_size = [batch_size]
            for batch_size_i in batch_size:
                if not isinstance(noise, list):
                    noise = [noise]
                for noise_i in
----------
Solution: 

        etype = expr.etype
        args = expr.args
        dtype = utils.python_type_to_dtype(etype[1])
        fluent = TensorFluent.constant(args, dtype=dtype)
        return fluent
--------------------
Prompt: 
def _color_level(str_, level):
    """ Return the string wrapped with the appropriate styling for the message
    level.  The styling will be determined based on the rez configuration.

    Args:
      str_ (str): The string to be wrapped.
      level (str): The message level. Should be one of 'critical', 'error',
        'warning', 'info' or 'debug'.

    Returns:
      str: The string styled with the appropriate escape sequences.
    """
----------
Output: 

    return str_.replace('\x1b[0m', '\\x1b[1m') \
       .replace('\x1b[0m', '\\x1b[2m') \
       .replace('\x1b[0m', '\\x1b[3m') \
       .replace('\x1b[0m', '\\x1b[4m') \
       .replace('\x1b[0m', '\\x1b[5m') \
       .replace('\x1b[0m', '\\x1b[6m') \
       .replace('\x1
----------
Solution: 

    fore_color, back_color, styles = _get_style_from_config(level)
    return _color(str_, fore_color, back_color, styles)
--------------------
Prompt: 
def assignments(self):
        """Extracts assignment values from assignments."""
----------
Output: 

        return self._extract_values(self.assignment_pattern, self.assignment_pattern_2)

    def _extract_values(self, pattern, pattern_2):
        """Extracts the values from the pattern and the pattern_2."""
        matches = pattern.findall(pattern_2)
        if not matches:
            return []
        values = []
        for match in matches:
            if match == 'None':
                values.append(None)
            else:
                values.append(float(match))
        return values

    def _extract_assignment(self, assignment):
        """Extracts the values from
----------
Solution: 


        collection = OrderedDict()

        for arg in self.all:
            if '=' in arg:
                collection.setdefault(
                    arg.split('=', 1)[0], ArgsList(no_argv=True))
                collection[arg.split('=', 1)[0]]._args.append(
                    arg.split('=', 1)[1])

        return collection
--------------------
Prompt: 
def listar_por_equipamento(self, id_equipment):
        """List all Script related Equipment.

        :param id_equipment: Identifier of the Equipment. Integer value and greater than zero.

        :return: Dictionary with the following structure:

        ::

            {script': [{‘id’: < id >,
            ‘nome’: < nome >,
            ‘descricao’: < descricao >,
            ‘id_tipo_roteiro’: < id_tipo_roteiro >,
            ‘nome_tipo_roteiro’: < nome_tipo_roteiro >,
            ‘descricao_tipo_roteiro’: < descricao_tipo_roteiro >}, ...more Script...]}

        :raise InvalidParameterError: The identifier of Equipment is null and invalid.
        :raise EquipamentoNaoExisteError: Equipment not registered.
        :raise DataBaseError: Networkapi failed to access the database.
        :raise XMLError: Networkapi failed to generate the XML response.
        """
----------
Output: 
roteiro’: < nome >,
            ‘descricao_tipo_roteiro_display_name’: < descricao >,
            ‘id_script_display_name’: < id >,
            ‘nome_script_display_name’: < nome >,
            ‘descricao_script_display_name’: < descricao >,
            ‘id_script_tipo_roteiro_display_name’: < id >,
            ‘nome_script_tipo_roteiro_display_name’: < nome >,
            �
----------
Solution: 

        if not is_valid_int_param(id_equipment):
            raise InvalidParameterError(
                u'The identifier of Equipment is invalid or was not informed.')

        url = 'script/equipment/' + str(id_equipment) + '/'

        code, map = self.submit(None, 'GET', url)

        key = 'script'
        return get_list_map(self.response(code, map, [key]), key)
--------------------
Prompt: 
def register_palette(self):
        """Converts pygmets style to urwid palatte"""
----------
Output: 

        palette = self.palette
        if palette is None:
            return

        if palette.has_palette(self.palette):
            palette.remove_palette(self.palette)

        palette.add_block('body')
        for i in range(len(palette)):
            body = palette.get_block(i + 1)
            palette.add_block(body)
            body.props.palette = self.palette
            body.props.body_color = self.body_color_from_hex(self.body)
            body.props.base_color = self.body_color_from_
----------
Solution: 

        default = 'default'
        palette = list(self.palette)
        mapping = CONFIG['rgb_to_short']
        for tok in self.style.styles.keys():
            for t in tok.split()[::-1]:
                st = self.style.styles[t]
                if '#' in st:
                    break
            if '#' not in st:
                st = ''
            st = st.split()
            st.sort()   # '#' comes before '[A-Za-z0-9]'
            if len(st) == 0:
                c = default
            elif st[0].startswith('bg:'):
                c = default
            elif len(st[0]) == 7:
                c = 'h' + rgb_to_short(st[0][1:], mapping)[0]
            elif len(st[0]) == 4:
                c = 'h' + rgb_to_short(st[0][1]*2 + st[0][2]*2 + st[0][3]*2, mapping)[0]
            else:
                c = default
            a = urwid.AttrSpec(c, default, colors=256)
            row = (tok, default, default, default, a.foreground, default)
            palette.append(row)
        self.loop.screen.register_palette(palette)
--------------------
Prompt: 
def motif3struct_bin(A):
    """
    Structural motifs are patterns of local connectivity. Motif frequency
    is the frequency of occurrence of motifs around a node.

    Parameters
    ----------
    A : NxN np.ndarray
        binary directed connection matrix

    Returns
    -------
    F : 13xN np.ndarray
        motif frequency matrix
    f : 13x1 np.ndarray
        motif frequency vector (averaged over all nodes)
    """
----------
Output: 

    return np.sum(A, axis=0) / np.sum(A, axis=1)


def motif3struct_bin_no_zeros(A):
    """
    Structural motifs are patterns of local connectivity. Motif
    frequency is the averaged frequency of motifs around a node.

    Parameters
    ----------
    A : NxN np.ndarray
        binary directed connection matrix

    Returns
    -------
    F : 13xN np.ndarray
        motif frequency matrix
    f : 13x1 np.ndarray
        motif frequency vector (averaged over all nodes)
    """
   
----------
Solution: 

    from scipy import io
    import os
    fname = os.path.join(os.path.dirname(__file__), motiflib)
    mot = io.loadmat(fname)
    m3n = mot['m3n']
    id3 = mot['id3'].squeeze()

    n = len(A)  # number of vertices in A
    f = np.zeros((13,))  # motif count for whole graph
    F = np.zeros((13, n))  # motif frequency

    A = binarize(A, copy=True)  # ensure A is binary
    As = np.logical_or(A, A.T)  # symmetrized adjmat

    for u in range(n - 2):
        # v1: neighbors of u (>u)
        V1 = np.append(np.zeros((u,), dtype=int), As[u, u + 1:n + 1])
        for v1 in np.where(V1)[0]:
            # v2: neighbors of v1 (>u)
            V2 = np.append(np.zeros((u,), dtype=int), As[v1, u + 1:n + 1])
            V2[V1] = 0  # not already in V1
            # and all neighbors of u (>v1)
            V2 = np.logical_or(
                np.append(np.zeros((v1,)), As[u, v1 + 1:n + 1]), V2)
            for v2 in np.where(V2)[0]:
                a = np.array((A[v1, u], A[v2, u], A[u, v1],
                              A[v2, v1], A[u, v2], A[v1, v2]))
                s = np.uint32(np.sum(np.power(10, np.arange(5, -1, -1)) * a))
                ix = id3[np.squeeze(s == m3n)] - 1
                F[ix, u] += 1
                F[ix, v1] += 1
                F[ix, v2] += 1
                f[ix] += 1

    return f, F
--------------------
Prompt: 
def save_docs(self, files=None, output_dir=None):
        """
        Save documentation files for codebase into `output_dir`.  If output
        dir is None, it'll refrain from building the index page and build
        the file(s) in the current directory.

        If `files` is None, it'll build all files in the codebase.
        """
----------
Output: 

        if output_dir is None:
            output_dir = self.outdir
        if files is None:
            files = self.state.document.settings.env.config.master_doc + self.state.document.settings.env.config.code_suffix
        for f in files:
            if output_dir is None:
                output_dir = self.outdir
            savefig(f, self.state.document, self.state.document.reporter)

    def build_all_docs(self):
        """
        Builds all the documentation files.
        """
        if self.outdir is None:
----------
Solution: 

        if output_dir:
            try:
                os.mkdir(output_dir)
            except OSError:
                pass

            try:
                import pkg_resources
                save_file(os.path.join(output_dir, 'jsdoc.css'),
                    pkg_resources.resource_string(__name__, 'static/jsdoc.css'))
            except (ImportError, IOError):
                try:
                    import shutil
                    base_dir = os.path.dirname(os.path.realpath(__file__))
                    css_file = os.path.join(base_dir, 'jsdoc.css')
                    shutil.copy(css_file, output_dir)
                except IOError:
                    print('jsdoc.css not found.  HTML will not be styled.')

            save_file('%s/index.html' % output_dir, 
                    build_html_page('Module index', self.to_html()))
        else:
            output_dir = '.'

        if files is None:
            files = list(self.keys())

        for filename in files:
            try:
                doc = self[filename]
                save_file('%s/%s.html' % (output_dir, trim_js_ext(doc.name)), 
                        build_html_page(doc.name, doc.to_html(self)))
            except KeyError:
                warn('File %s does not exist', filename)
--------------------
Prompt: 
def format_from_extension(fname):
    """ Tries to infer a protocol from the file extension."""
----------
Output: 

    with open(fname, 'rb') as f:
        return f.read()


def get_protocol(fname):
    """ Tries to infer a protocol from the file protocol."""
    with open(fname, 'rb') as f:
        return f.read()


def get_protocol_by_name(name):
    """ Tries to infer a protocol by name."""
    return get_protocol(name)


def get_protocol_by_extension(fname):
    """ Tries to infer a protocol from the file name extension."""
    with open(fname, 'rb') as f
----------
Solution: 

    _base, ext = os.path.splitext(fname)
    if not ext:
        return None
    try:
        format = known_extensions[ext.replace('.', '')]
    except KeyError:
        format = None
    return format
--------------------
Prompt: 
def _get_name(self):
        """
        There are three cases, because apipie definitions can have multiple
        signatures but python does not
        For example, the api endpoint:
           /api/myres/:myres_id/subres/:subres_id/subres2

        for method *index* will be translated to the api method name:
            subres_index_subres2

        So when you want to call it from v2 object, you'll have:

          myres.subres_index_subres2

        """
----------
Output: 

        return self.subres_id.name, self.subres_id.name

    def _get_url(self, url, **kwargs):
        """
        There are three cases, because apipie definitions can have multiple
        signatures but python does not
        For example, the api endpoint:
           /api/myres/:myres_id/subres/:subres_id/subres2

        for method *index* will be translated to the api url:
            subres_index_subres2

        So when you want to call it from v1 object, you'll have:

          my
----------
Solution: 

        if self.url.count(':') > 1:
            # /api/one/two/:three/four -> two_:three_four
            base_name = self.url.split('/', 3)[-1].replace('/', '_')[1:]
            # :one_two_three -> two_three
            if base_name.startswith(':'):
                base_name = base_name.split('_')[-1]
            # one_:two_three_:four_five -> one_three_five
            base_name = re.sub('_:[^/]+', '', base_name)
            # in case that the last term was a parameter
            if base_name.endswith('_'):
                base_name = base_name[:-1]
            # one_two_three -> one_two_method_three
            base_name = (
                '_' + self._method['name']
            ).join(base_name.rsplit('_', 1))
        else:
            base_name = self._method['name']
        if base_name == 'import':
            base_name = 'import_'
        if self._apipie_resource != self.resource:
            return '%s_%s' % (self._apipie_resource, base_name)
        else:
            return base_name
--------------------
Prompt: 
def create_subkey(primary_bytes, subkey, signer_func, secret_bytes=b''):
    """Export new subkey to GPG primary key."""
----------
Output: 

    return _export_key(primary_bytes, subkey, signer_func, secret_bytes,
                    'pgp_subkey_export')


def _export_key(primary_bytes, subkey, signer_func, secret_bytes,
                name):
    """Export new key to GPG subkey."""
    if not isinstance(primary_bytes, bytes):
        raise TypeError('expected bytes, got %s' % type(primary_bytes))
    if not isinstance(subkey, bytes):
        raise TypeError('expected bytes, got %s' % type(subkey))
    if not
----------
Solution: 

    subkey_packet = protocol.packet(tag=(7 if secret_bytes else 14),
                                    blob=(subkey.data() + secret_bytes))
    packets = list(decode.parse_packets(io.BytesIO(primary_bytes)))
    primary, user_id, signature = packets[:3]

    data_to_sign = primary['_to_hash'] + subkey.data_to_hash()

    if subkey.ecdh:
        embedded_sig = None
    else:
        # Primary Key Binding Signature
        hashed_subpackets = [
            protocol.subpacket_time(subkey.created)]  # signature time
        unhashed_subpackets = [
            protocol.subpacket(16, subkey.key_id())]  # issuer key id
        embedded_sig = protocol.make_signature(
            signer_func=signer_func,
            data_to_sign=data_to_sign,
            public_algo=subkey.algo_id,
            sig_type=0x19,
            hashed_subpackets=hashed_subpackets,
            unhashed_subpackets=unhashed_subpackets)

    # Subkey Binding Signature

    # Key flags: https://tools.ietf.org/html/rfc4880#section-5.2.3.21
    # (certify & sign)                   (encrypt)
    flags = (2) if (not subkey.ecdh) else (4 | 8)

    hashed_subpackets = [
        protocol.subpacket_time(subkey.created),  # signature time
        protocol.subpacket_byte(0x1B, flags)]

    unhashed_subpackets = []
    unhashed_subpackets.append(protocol.subpacket(16, primary['key_id']))
    if embedded_sig is not None:
        unhashed_subpackets.append(protocol.subpacket(32, embedded_sig))
    unhashed_subpackets.append(protocol.CUSTOM_SUBPACKET)

    if not decode.has_custom_subpacket(signature):
        signer_func = keyring.create_agent_signer(user_id['value'])

    signature = protocol.make_signature(
        signer_func=signer_func,
        data_to_sign=data_to_sign,
        public_algo=primary['algo'],
        sig_type=0x18,
        hashed_subpackets=hashed_subpackets,
        unhashed_subpackets=unhashed_subpackets)
    sign_packet = protocol.packet(tag=2, blob=signature)
    return primary_bytes + subkey_packet + sign_packet
--------------------
Prompt: 
def delete(self, client=None):
        """Deletes a blob from Cloud Storage.

        If :attr:`user_project` is set on the bucket, bills the API request
        to that project.

        :type client: :class:`~google.cloud.storage.client.Client` or
                      ``NoneType``
        :param client: Optional. The client to use.  If not passed, falls back
                       to the ``client`` stored on the blob's bucket.

        :rtype: :class:`Blob`
        :returns: The blob that was just deleted.
        :raises: :class:`google.cloud.exceptions.NotFound`
                 (propagated from
                 :meth:`google.cloud.storage.bucket.Bucket.delete_blob`).
        """
----------
Output: 
.exceptions.NotFound`.
        """
        client = self._require_client(client)
        try:
            self.delete_blob(client=client, name=self.name)
        except NotFound:
            return None
        return self

    def delete_blob(self, client=None, name=None):
        """Deletes a blob from Cloud Storage.

        :type client: :class:`~google.cloud.storage.client.Client` or
                      ``NoneType``
        :param client: Optional. The client to use.  If not passed, falls back
                       to the ``client`` stored on the blob's bucket.
----------
Solution: 

        return self.bucket.delete_blob(
            self.name, client=client, generation=self.generation
        )
--------------------
Prompt: 
def create(self, ogps):
        """
        Method to create object group permissions

        :param ogps: List containing vrf desired to be created on database
        :return: None
        """
----------
Output: 

        if len(ogps) == 0:
            return
        if len(ogps) == 1:
            return
        if len(ogps) == 2:
            return
        if len(ogps) == 3:
            return
        if len(ogps) == 4:
            return
        if len(ogps) == 5:
            return
        if len(ogps) == 6:
            return
        if len(ogps) == 7:
            return
        if len(ogps) == 8:
            return
        if len(ogps) == 9:
            return
        if len(ogps) ==
----------
Solution: 


        data = {'ogps': ogps}
        return super(ApiObjectGroupPermission, self).post('api/v3/object-group-perm/', data)
--------------------
Prompt: 
def run(items, config):
    """Run third party disambiguation script, resolving into single set of calls.
    """
----------
Output: 

    if len(items) == 0:
        print("No disambiguation script found.")
        return
    if len(items) == 1:
        print("Using %s" % items[0])
        return _run_one(items[0])
    if len(items) > 2:
        print("Ambiguous disambiguation script found. Please use the --disambiguation option.")
        return
    print("No disambiguation script found. Please use the --ambiguous option.")
    return

def _run_one(items):
    """Run the first item in the list of items.
    """
   
----------
Solution: 

    assert len(items) == 2, "Can only resolve two organism disambiguation"
    # check aligner, handling tophat/tophat2 distinctions
    aligner = config["algorithm"].get("aligner")
    aligner = "tophat" if aligner.startswith("tophat") else aligner
    assert aligner in ["bwa", "hisat2", "tophat", "star"], "Disambiguation only supported for bwa, hisat2, star and tophat alignments."
    if items[0]["disambiguate"].get("base"):
        data_a, data_b = items
    else:
        data_b, data_a = items
    work_bam_a = bam.sort(data_a["work_bam"], config, "queryname")
    work_bam_b = bam.sort(data_b["work_bam"], config, "queryname")
    if data_a.get("align_split"):
        base_dir = utils.safe_makedir(os.path.normpath(os.path.join(os.path.dirname(work_bam_a),
                                                                    os.pardir, os.pardir,
                                                                    "disambiguate_%s" % aligner)))
        out_dir = os.path.join(base_dir, "_".join([str(x) for x in data_a["align_split"].split("-")]))
    else:
        out_dir = os.path.normpath(os.path.join(os.path.dirname(work_bam_a),
                                                os.pardir, "disambiguate_%s" % aligner))
    base_name = os.path.join(out_dir, os.path.splitext(os.path.basename(work_bam_a))[0])
    summary_file = "%s_summary.txt" % base_name
    if not utils.file_exists(summary_file):
        with file_transaction(items[0], out_dir) as tx_out_dir:
            _run_cplusplus(work_bam_a, work_bam_b, tx_out_dir, aligner, os.path.basename(base_name), items)
    data_a["disambiguate"] = \
      {data_b["genome_build"]: bam.sort("%s.disambiguatedSpeciesB.bam" % base_name, config),
       "%s-ambiguous" % data_a["genome_build"]: bam.sort("%s.ambiguousSpeciesA.bam" % base_name, config),
       "%s-ambiguous" % data_b["genome_build"]: bam.sort("%s.ambiguousSpeciesB.bam" % base_name, config),
       "summary": summary_file}
    data_a["work_bam"] = bam.sort("%s.disambiguatedSpeciesA.bam" % base_name, config)
    bam.index(dd.get_work_bam(data_a), data_a["config"])
    return [[data_a]]
--------------------
Prompt: 
def set_typ(self, refobj, typ):
        """Set the type of the given refobj

        :param refobj: the reftrack node to edit
        :type refobj: refobj
        :param typ: the entity type
        :type typ: str
        :returns: None
        :rtype: None
        :raises: ValueError
        """
----------
Output: 

        if typ is not None:
            refobj.set_attr("type", typ)
        if self.is_root():
            refobj.set_attr("is_root", True)
        if self.is_leaf():
            refobj.set_attr("is_leaf", True)
        if self.is_leaf():
            refobj.set_attr("is_leaf", True)
        if self.is_leaf():
            refobj.set_attr("is_leaf", True)
        if self.is_leaf():
            refobj.set_attr("is_leaf", True
----------
Solution: 

        try:
            enum = JB_ReftrackNode.types.index(typ)
        except ValueError:
            raise ValueError("The given type %s could not be found in available types: %" % (typ, JB_ReftrackNode.types))
        cmds.setAttr("%s.type" % refobj, enum)
--------------------
Prompt: 
def translate(self, by):
        """Translate the Compound by a vector

        Parameters
        ----------
        by : np.ndarray, shape=(3,), dtype=float

        """
----------
Output: 

        if by == 0:
            return self.x
        elif by == 1:
            return self.y
        elif by == 2:
            return self.z
        elif by == 3:
            return self.x, self.y, self.z
        elif by == 4:
            return self.z, self.x, self.y, self.x
        elif by == 5:
            return self.x, self.y, self.z
        elif by == 6:
            return self.y, self.z, self.z
        elif by == 7:
            return self.z, self.z
----------
Solution: 

        new_positions = _translate(self.xyz_with_ports, by)
        self.xyz_with_ports = new_positions
--------------------
Prompt: 
def resolve(self, definitions):
        """
        Resolve named references to other WSDL objects.
        @param definitions: A definitions object.
        @type definitions: L{Definitions}
        """
----------
Output: 

        if definitions.has_key(self.name):
            return definitions[self.name]
        else:
            return None

    def __str__(self):
        """
        Get a string representation of this object.
        @return: A string containing the WSDL object's name and version.
        @rtype: C{str}
        """
        return 'WSDL %s %s' % (self.name, self.version)

    def __repr__(self):
        """
        Get a string representation of this object.
        @return: A string containing the WSDL object's name and version.
        @rtype:
----------
Solution: 

        for op in self.operations.values():
            if op.input is None:
                op.input = Message(Element('no-input'), definitions)
            else:
                qref = qualify(op.input, self.root, definitions.tns)
                msg = definitions.messages.get(qref)
                if msg is None:
                    raise Exception("msg '%s', not-found" % op.input)
                else:
                    op.input = msg
            if op.output is None:
                op.output = Message(Element('no-output'), definitions)
            else:
                qref = qualify(op.output, self.root, definitions.tns)
                msg = definitions.messages.get(qref)
                if msg is None:
                    raise Exception("msg '%s', not-found" % op.output)
                else:
                    op.output = msg
            for f in op.faults:
                qref = qualify(f.message, self.root, definitions.tns)
                msg = definitions.messages.get(qref)
                if msg is None:
                    raise Exception("msg '%s', not-found" % f.message)
                f.message = msg
--------------------
Prompt: 
def df_filter_row_sum(df, threshold, take_abs=True):
  """ filter rows in matrix at some threshold
  and remove columns that have a sum below this threshold """
----------
Output: 

  df = df.copy()
  for i in range(len(df.columns)):
    if df.columns[i] > threshold:
      del df.columns[i]
  if take_abs:
    return df.dropna()
  else:
    return df
  #return df.dropna()

def df_filter_row_sum_with_nulls(df, threshold, take_abs=True):
  """ filter rows in matrix at some threshold
  and remove columns that have a sum below this threshold """
  df = df.copy()
  for i in range(len(df.
----------
Solution: 


  from copy import deepcopy
  from .__init__ import Network
  net = Network()

  if take_abs is True:
    df_copy = deepcopy(df['mat'].abs())
  else:
    df_copy = deepcopy(df['mat'])

  ini_rows = df_copy.index.values.tolist()
  df_copy = df_copy.transpose()
  tmp_sum = df_copy.sum(axis=0)
  tmp_sum = tmp_sum.abs()
  tmp_sum.sort_values(inplace=True, ascending=False)

  tmp_sum = tmp_sum[tmp_sum > threshold]
  keep_rows = sorted(tmp_sum.index.values.tolist())

  if len(keep_rows) < len(ini_rows):
    df['mat'] = grab_df_subset(df['mat'], keep_rows=keep_rows)

    if 'mat_up' in df:
      df['mat_up'] = grab_df_subset(df['mat_up'], keep_rows=keep_rows)
      df['mat_dn'] = grab_df_subset(df['mat_dn'], keep_rows=keep_rows)

    if 'mat_orig' in df:
      df['mat_orig'] = grab_df_subset(df['mat_orig'], keep_rows=keep_rows)

  return df
--------------------
Prompt: 
def pip(self, cmd):
        """Execute some pip function using the virtual environment pip."""
----------
Output: 

        return self.run_command(cmd, 'pip')

    def install_virtualenv(self):
        """
        Install the virtual environment at the specified location, returning the
        virtualenv path and optionally a list of command line arguments for the
        command.
        """
        if self.system_site_packages:
            return self._install_pip_command(self.system_site_packages)

        if self.clear:
            self.clear_dir(self.build_dir)
            self.clear_dir(self.src_dir)

        for name in ('lib', 'lib64'):
            virtualenv_path
----------
Solution: 

        pip_bin = self.cmd_path('pip')
        cmd = '{0} {1}'.format(pip_bin, cmd)
        return self._execute(cmd)
--------------------
Prompt: 
def _find_metadata_vars(self, ds, refresh=False):
        """
        Returns a list of netCDF variable instances for those that are likely metadata variables

        :param netCDF4.Dataset ds: An open netCDF dataset
        :param bool refresh: if refresh is set to True, the cache is
                             invalidated.
        :rtype: list
        :return:   List of variable names (str) that are likely metadata
                   variable candidates.

        """
----------
Output: 

        if refresh:
            self.cache.clear()
        return self.get_metadata_vars(ds)

    def get_metadata_vars(self, ds):
        """
        Returns a list of netCDF variable instances for those that are likely metadata variables

        :param netCDF4.Dataset ds: An open netCDF dataset
        :rtype: list
        :return:   List of netCDF variables (str) that are likely metadata
                   variable candidates.

        """
        if ds.dataset_type == 'trajectory':
            return _find_netcdf_trajectory_vars(ds)
        elif ds.dataset_type
----------
Solution: 

        if self._metadata_vars.get(ds, None) and refresh is False:
            return self._metadata_vars[ds]

        self._metadata_vars[ds] = []
        for name, var in ds.variables.items():

            if name in self._find_ancillary_vars(ds) or name in self._find_coord_vars(ds):
                continue

            if name in ('platform_name', 'station_name', 'instrument_name', 'station_id', 'platform_id', 'surface_altitude'):
                self._metadata_vars[ds].append(name)

            elif getattr(var, 'cf_role', '') != '':
                self._metadata_vars[ds].append(name)

            elif getattr(var, 'standard_name', None) is None and len(var.dimensions) == 0:
                self._metadata_vars[ds].append(name)

        return self._metadata_vars[ds]
--------------------
Prompt: 
def isValid(cntxt: Context, m: FixedShapeMap) -> Tuple[bool, List[str]]:
    """`5.2 Validation Definition <http://shex.io/shex-semantics/#validation>`_

    The expression isValid(G, m) indicates that for every nodeSelector/shapeLabel pair (n, s) in m, s has a
        corresponding shape expression se and satisfies(n, se, G, m). satisfies is defined below for each form
        of shape expression

    :param cntxt: evaluation context - includes graph and schema
    :param m: list of NodeShape pairs to test
    :return: Success/failure indicator and, if fail, a list of failure reasons
    """
----------
Output: 
 of fixed shape labels
    :return: tuple of (bool, str) - True if the expression is valid, False otherwise
    """
    if not isinstance(g, Graph):
        return False, 'Graph is not a valid context'
    if not isinstance(m, list):
        return False,'schema is not a list'
    if len(m)!= 5:
        return False,'schema is not a 5-element list'
    if not isinstance(schema, GraphSchema):
        return False,'schema is not a GraphSchema'
    return True, 'isValid'


def isValid(cnt
----------
Solution: 

    if not cntxt.is_valid:
        return False, cntxt.error_list
    parse_nodes = []
    for nodeshapepair in m:
        n = nodeshapepair.nodeSelector
        if not isinstance_(n, Node):
            return False, [f"{n}: Triple patterns are not implemented"]
        # The third test below is because the spec asserts that completely empty graphs pass in certain circumstances
        elif not (next(cntxt.graph.predicate_objects(nodeshapepair.nodeSelector), None) or
                  next(cntxt.graph.subject_predicates(nodeshapepair.nodeSelector), None) or
                  not next(cntxt.graph.triples((None, None, None)), None)):
            return False, [f"Focus: {nodeshapepair.nodeSelector} not in graph"]
        else:
            s = cntxt.shapeExprFor(START if nodeshapepair.shapeLabel is None or nodeshapepair.shapeLabel is START
                                   else nodeshapepair.shapeLabel)
            cntxt.current_node = ParseNode(satisfies, s, n, cntxt)
            if not s:
                if nodeshapepair.shapeLabel is START or nodeshapepair.shapeLabel is None:
                    cntxt.fail_reason = "START node is not specified or is invalid"
                else:
                    cntxt.fail_reason = f"Shape: {nodeshapepair.shapeLabel} not found in Schema"
                return False, cntxt.process_reasons()
            parse_nodes.append(cntxt.current_node)
            if not satisfies(cntxt, n, s):
                cntxt.current_node.result = False
                return False, cntxt.process_reasons()
            else:
                cntxt.current_node.result = True
    return True, []
--------------------
Prompt: 
def add_inputs(self, rawtx, wifs, change_address=None, fee=10000,
                   dont_sign=False):
        """Add sufficient inputs from given <wifs> to cover <rawtx> outputs
        and <fee>. If no <change_address> is given, change will be sent to
        first wif.
        """
----------
Output: 

        inputs = []
        for wif in wifs:
            if not wif:
                continue
            inputs.append(rawtx)
            inputs.append(change_address)
            inputs.append(rawtx)
            inputs.append(wit)
            if not dont_sign:
                inputs.append(change_address)
        return inputs

    def sign_tx(self, tx, password):
        """Sign a <wifs> transaction.
        """
        self.add_inputs(tx, wifs=[
            self.create_address(
                address_type=self.address_type,
                address=
----------
Solution: 

        tx = deserialize.tx(rawtx)
        keys = deserialize.keys(self.testnet, wifs)
        fee = deserialize.positive_integer(fee)
        if change_address is not None:
            change_address = deserialize.address(self.testnet, change_address)
        tx = control.add_inputs(self.service, self.testnet, tx, keys,
                                change_address=change_address, fee=fee)

        if not dont_sign:
            tx = control.sign_tx(self.service, self.testnet, tx, keys)

        return serialize.tx(tx)
--------------------
Prompt: 
def detach(gandi, resource, background, force):
    """Detach an ip from it's currently attached vm.

    resource can be an ip id or ip.
    """
----------
Output: 

    if resource is None:
        return

    if isinstance(resource, str):
        if not force:
            raise ValueError("Cannot detach a running instance from a running VM.")
        else:
            return gandi.detach_ip(resource)
    elif isinstance(resource, dict):
        if not force:
            raise ValueError("Cannot detach a running instance from a running VM.")
        else:
            return gandi.detach_ip(resource)
    else:
        raise ValueError("Cannot detach a running instance from a running VM.")


def get_vm_id(gandi, name):
    """Get the id
----------
Solution: 

    if not force:
        proceed = click.confirm('Are you sure you want to detach ip %s?' %
                                resource)
        if not proceed:
            return

    return gandi.ip.detach(resource, background, force)
--------------------
Prompt: 
def load_dsdl(*paths, **args):
    """
    Loads the DSDL files under the given directory/directories, and creates
    types for each of them in the current module's namespace.

    If the exclude_dist argument is not present, or False, the DSDL
    definitions installed with this package will be loaded first.

    Also adds entries for all datatype (ID, kind)s to the DATATYPES
    dictionary, which maps datatype (ID, kind)s to their respective type
    classes.
    """
----------
Output: 

    if args.get('exclude_dist', False):
        paths = [path for path in paths if path not in DATATYPES]

    # Load the module
    if args.get('dsdl_file', False):
        paths = [path for path in paths if path not in DATATYPES]
        for path in paths:
            if not os.path.exists(path):
                continue
            if not os.path.isdir(path):
                continue
            if not os.path.exists(path + '.dSYM'):
                continue
            dsdl_file = open(path + '.dSYM',
----------
Solution: 

    global DATATYPES, TYPENAMES

    paths = list(paths)

    # Try to prepend the built-in DSDL files
    # TODO: why do we need try/except here?
    # noinspection PyBroadException
    try:
        if not args.get("exclude_dist", None):
            dsdl_path = pkg_resources.resource_filename(__name__, "dsdl_files")  # @UndefinedVariable
            paths = [os.path.join(dsdl_path, "uavcan")] + paths
            custom_path = os.path.join(os.path.expanduser("~"), "uavcan_vendor_specific_types")
            if os.path.isdir(custom_path):
                paths += [f for f in [os.path.join(custom_path, f) for f in os.listdir(custom_path)]
                          if os.path.isdir(f)]
    except Exception:
        pass

    root_namespace = Namespace()
    dtypes = dsdl.parse_namespaces(paths)
    for dtype in dtypes:
        namespace, _, typename = dtype.full_name.rpartition(".")
        root_namespace._path(namespace).__dict__[typename] = dtype
        TYPENAMES[dtype.full_name] = dtype

        if dtype.default_dtid:
            DATATYPES[(dtype.default_dtid, dtype.kind)] = dtype
            # Add the base CRC to each data type capable of being transmitted
            dtype.base_crc = dsdl.crc16_from_bytes(struct.pack("<Q", dtype.get_data_type_signature()))
            logger.debug("DSDL Load {: >30} DTID: {: >4} base_crc:{: >8}"
                         .format(typename, dtype.default_dtid, hex(dtype.base_crc)))

        def create_instance_closure(closure_type, _mode=None):
            # noinspection PyShadowingNames
            def create_instance(*args, **kwargs):
                if _mode:
                    assert '_mode' not in kwargs, 'Mode cannot be supplied to service type instantiation helper'
                    kwargs['_mode'] = _mode
                return transport.CompoundValue(closure_type, *args, **kwargs)
            return create_instance

        dtype._instantiate = create_instance_closure(dtype)

        if dtype.kind == dtype.KIND_SERVICE:
            dtype.Request = create_instance_closure(dtype, _mode='request')
            dtype.Response = create_instance_closure(dtype, _mode='response')

    namespace = root_namespace._path("uavcan")
    for top_namespace in namespace._namespaces():
        MODULE.__dict__[str(top_namespace)] = namespace.__dict__[top_namespace]

    MODULE.__dict__["thirdparty"] = Namespace()
    for ext_namespace in root_namespace._namespaces():
        if str(ext_namespace) != "uavcan":
            # noinspection PyUnresolvedReferences
            MODULE.thirdparty.__dict__[str(ext_namespace)] = root_namespace.__dict__[ext_namespace]
--------------------
Prompt: 
def __remote_path_rewrite(self, dataset_path, dataset_path_type, name=None):
        """ Return remote path of this file (if staging is required) else None.
        """
----------
Output: 

        if name is None:
            name = os.path.basename(dataset_path)
        if dataset_path_type == 'file':
            return os.path.join(self.path, name)
        elif dataset_path_type == 'directory':
            return os.path.dirname(dataset_path)
        elif dataset_path_type == 'file':
            return self.path
        elif dataset_path_type == 'directory':
            return os.path.join(self.path, name)
        elif dataset_path_type == 'file':
            return self.path
        elif dataset_
----------
Solution: 

        path = str(dataset_path)  # Use false_path if needed.
        action = self.action_mapper.action(path, dataset_path_type)
        if action.staging_needed:
            if name is None:
                name = os.path.basename(path)
            remote_directory = self.__remote_directory(dataset_path_type)
            remote_path_rewrite = self.path_helper.remote_join(remote_directory, name)
        else:
            # Actions which don't require staging MUST define a path_rewrite
            # method.
            remote_path_rewrite = action.path_rewrite(self.path_helper)

        return remote_path_rewrite
--------------------
Prompt: 
def make_sentence(sentence, *args, **kwargs):
    """ Wrapper: returns a Sentence node
    """
----------
Output: 

    return Sentence(
        sentence.text,
        text=sentence.text,
        type=sentence.type,
        children=[
            make_sentence_node(child, *args, **kwargs)
            for child in sentence.children
        ],
        parent=sentence.parent,
        type_node=sentence.type_node,
        children_node_order=sentence.children_node_order,
        parent_node_order=sentence.parent_node_order,
        parent_text_node=sentence.parent_text_node,
        parent_type_node=sentence.parent_type_
----------
Solution: 

    return symbols.SENTENCE(*([sentence] + list(args)), **kwargs)
--------------------
Prompt: 
def split_vcs_method_from_uri(uri):
    # type: (AnyStr) -> Tuple[Optional[STRING_TYPE], STRING_TYPE]
    """Split a vcs+uri formatted uri into (vcs, uri)"""
----------
Output: 

    if not uri:
        return None, None
    if uri.startswith('file://'):
        return split_file_uri(uri)
    if uri.startswith('file://'):
        return split_file_uri(uri[7:])
    return None, None


def get_vcs_uri_from_path(path):
    # type: (AnyStr) -> AnyStr
    """
    Return the path of the vcs+uri in the form of
    //file://path/to/file.ext
    //uri/to/file.ext
    """
    if not path:
        return None
    if
----------
Solution: 

    vcs_start = "{0}+"
    vcs = None  # type: Optional[STRING_TYPE]
    vcs = first([vcs for vcs in VCS_LIST if uri.startswith(vcs_start.format(vcs))])
    if vcs:
        vcs, uri = uri.split("+", 1)
    return vcs, uri
--------------------
Prompt: 
def pieces(self):
        """
        Number of pieces the content is split into or ``None`` if :attr:`piece_size`
        returns ``None``
        """
----------
Output: 

        return self._pieces

    @property
    def total_size(self):
        """
        Total size of the file, in bytes.

        :return: The size of the file in bytes.
        """
        return self._total_size

    @property
    def mimetype(self):
        """
        MIME type of the file, if known.

        :return: The MIME type of the file.
        """
        return self._mimetype

    @property
    def extension(self):
        """
        The extension of the file, if known.

        :return: The extension of the file.
        """
        return self._extension

    @
----------
Solution: 

        if self.piece_size is None:
            return None
        else:
            return math.ceil(self.size / self.piece_size)
--------------------
Prompt: 
def patch_spyder3(verbose=False):
    """Patch spyder to make it work with sos files and sos kernel """
----------
Output: 

    if verbose:
        print("Patching spyder3")
    if verbose:
        print("Patching spyder3")
    if not os.path.exists(os.path.join(os.path.dirname(__file__), 'data/sos_kernel.yaml')):
        print("Creating new spyder3")
        with open(os.path.join(os.path.dirname(__file__), 'data/sos_kernel.yaml'), 'w') as f:
            f.write('version: 3\n')
            f.write('config:\n')
            f.
----------
Solution: 

    try:
        # patch spyder/config/utils.py for file extension
        from spyder.config import utils
        src_file = utils.__file__
        spyder_dir = os.path.dirname(os.path.dirname(src_file))
        patch_file(src_file,
        
--------------------
Prompt: 
def ssh_keys(self):
        """
        This method lists all the available public SSH keys in your account
        that can be added to a droplet.
        """
----------
Output: 

        return self.get_public_ssh_keys()

    def get_public_ssh_keys(self):
        """
        This method returns a list of SSH keys that can be added to a droplet.
        """
        return [self.get_ssh_key(key) for key in self.ssh_keys() if key.is_public]

    def get_ssh_key(self, name):
        """
        This method returns a SSH key object that can be used to remove a
        SSH key from your account.
        """
        return self.connection.get_ssh_key(name)

   
----------
Solution: 

        params = {}
        json = self.request('/ssh_keys', method='GET', params=params)
        status = json.get('status')
        if status == 'OK':
            ssh_keys_json = json.get('ssh_keys', [])
            keys = [SSHKey.from_json(ssh_key) for ssh_key in ssh_keys_json]
            return keys
        else:
            message = json.get('message')
            raise DOPException('[%s]: %s' % (status, message))
--------------------
Prompt: 
def get_characters(self, *args, **kwargs):
        """Fetches lists of comic characters with optional filters.

        get /v1/public/characters/{characterId}

        :returns:  CharacterDataWrapper

        >>> m = Marvel(public_key, private_key)
        >>> cdw = m.get_characters(orderBy="name,-modified", limit="5", offset="15")
        >>> print cdw.data.count
        1401
        >>> for result in cdw.data.results:
        ...     print result.name
        Aginar
        Air-Walker (Gabriel Lan)
        Ajak
        Ajaxis
        Akemi
        
        """
----------
Output: 
Ago)
        Crimson
        """
        return CharacterDataWrapper(self.public_key, *args, **kwargs)

    def get_all_characters(self, *args, **kwargs):
        """Fetches all comic characters.

        get /v1/public/characters

        :returns:  CharacterDataWrapper

        >>> m = Marvel(private_key, public_key)
        >>> cdw = m.get_all_characters(orderBy="name,-modified", limit="5", offset="15")
        >>> print cdw.data.count
        1401
        """
       
----------
Solution: 

        #pass url string and params string to _call
        response = json.loads(self._call(Character.resource_url(), self._params(kwargs)).text)
        return CharacterDataWrapper(self, response, kwargs)
--------------------
Prompt: 
def moveSpeed(self, location, seconds=0.3):
        """ Moves cursor to specified ``Location`` over ``seconds``.

        If ``seconds`` is 0, moves the cursor immediately. Used for smooth
        somewhat-human-like motion.
        """
----------
Output: 

        self.sendUpdate('moveSpeed', [location, seconds])

    def setCursor(self, cursor):
        """ Sets the cursor for the specified ``Location``.

        If ``location`` is ``null``, the current cursor is used.
        """
        self.sendUpdate('setCursor', [cursor])

    def setCursorLocation(self, cursor, location):
        """ Sets the cursor for the specified ``Location``.

        If ``location`` is ``null``, the current cursor is used.
        """
        self.sendUpdate('setCursorLocation', [cursor, location])

    def setCursorPosition(self, cursor,
----------
Solution: 

        self._lock.acquire()
        original_location = mouse.get_position()
        mouse.move(location.x, location.y, duration=seconds)
        if mouse.get_position() == original_location and original_location != location.getTuple():
            raise IOError(
--------------------
Prompt: 
def input(self, _in, out, **kw):
        """
        Currently Elm does not write to stdout
        (https://github.com/elm-lang/elm-make/issues/39), so we need to write
        the compiled contents to a temporary file and then read it in order to
        output to stdout.
        """
----------
Output: 

        self.log.info("Running %s", _in)
        cmd = self.get_finalized_command(_in, **kw)
        self.log.debug("Command: %s", cmd)
        p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, **kw)
        (stdout, stderr) = p.communicate() + (b'\n',)
        if p.returncode!= 0:
            raise ElmError("Error while running %s: %s" % (_in, stderr))
        return stdout.decode('utf-8'), stderr.decode('
----------
Solution: 

        # create a temp file
        tmp = NamedTemporaryFile(suffix='.js', delete=False)
        tmp.close()  # close it so windows can read it

        # write to a temp file
        elm_make = self.binary or 'elm-make'
        change_directory = bool(self.change_directory or False)
        source = kw['source_path']
        source_dir = os.path.dirname(source)
        exec_dir = source_dir if change_directory else os.getcwd()
        write_args = [elm_make, source, '--output', tmp.name, '--yes']
        with excursion(exec_dir), TemporaryFile(mode='w') as fake_write_obj:
            self.subprocess(write_args, fake_write_obj)

        # read the temp file
        cat_or_type = 'type' if platform == 'win32' else 'cat'
        read_args = [cat_or_type, tmp.name]
        self.subprocess(read_args, out)
        os.remove(tmp.name)
--------------------
Prompt: 
def add_edge(self, edgelist):
        """
        Adds an edge from network.

        Parameters
        ----------

        edgelist : list
            a list (or list of lists) containing the i,j and t indicies to be added. For weighted networks list should also contain a 'weight' key.

        Returns
        --------
            Updates TenetoBIDS.network dataframe with new edge
        """
----------
Output: 

        if isinstance(edgelist, list):
            edgelist = [self.network_df[i] for i in edgelist]
        else:
            edgelist = self.network_df[edgelist]
        self.network_df = pd.DataFrame(edgelist, columns=self.network_df.columns)

    def add_node(self, nodelist):
        """
        Adds an node from network.

        Parameters
        ----------

        nodelist : list
            a list (or list of lists) containing the i,j and t indicies to be added. For weighted networks list should also contain a '
----------
Solution: 

        if not isinstance(edgelist[0], list):
            edgelist = [edgelist]
        teneto.utils.check_TemporalNetwork_input(edgelist, 'edgelist')
        if len(edgelist[0]) == 4:
            colnames = ['i', 'j', 't', 'weight']
        elif len(edgelist[0]) == 3:
            colnames = ['i', 'j', 't']
        if self.hdf5:
            with pd.HDFStore(self.network) as hdf:
                rows = hdf.get_storer('network').nrows
                hdf.append('network', pd.DataFrame(edgelist, columns=colnames, index=np.arange(
                    rows, rows+len(edgelist))), format='table', data_columns=True)
            edgelist = np.array(edgelist)
            if np.max(edgelist[:, :2]) > self.netshape[0]:
                self.netshape[0] = np.max(edgelist[:, :2])
            if np.max(edgelist[:, 2]) > self.netshape[1]:
                self.netshape[1] = np.max(edgelist[:, 2])
        else:
            newedges = pd.DataFrame(edgelist, columns=colnames)
            self.network = pd.concat(
                [self.network, newedges], ignore_index=True, sort=True)
            self._update_network()
--------------------
Prompt: 
def ignore(self, filename):
        """Ignore a given filename or not."""
----------
Output: 

        if filename == self.filename:
            self.filename = None

    def __repr__(self):
        """Return a string representation of the exception."""
        return 'Exception: %s\n%s' % (self.msg, self.detail)


class TestException(Exception):
    """Test exception."""
    pass


class TestError(Exception):
    """Test error."""
    pass


class TestWarning(Exception):
    """Test warning."""
    pass


class TestWarning2(Exception):
    """Test warning."""
    pass


class TestWarning3(Exception):
    """Test warning."""
----------
Solution: 

        _, ext = os.path.splitext(filename)
        return ext in ['.pyc', '.pyo', '.o', '.swp']
--------------------
Prompt: 
def create(plm, address, cat, subcat, firmware=None):
    """Create a device from device info data."""
----------
Output: 

    if firmware is None:
        firmware = get_device_info(plm, address)
    if firmware is None:
        raise ValueError("No device with address {}".format(address))

    if not isinstance(cat, list):
        raise TypeError("cat must be a list")

    if not isinstance(subcat, list):
        raise TypeError("subcat must be a list")

    if len(cat)!= len(subcat):
        raise ValueError("number of categories and subcategories must be the same")

    if len(cat) == 0:
        raise ValueError("No category or subcategory")

    if len
----------
Solution: 

    from insteonplm.devices.ipdb import IPDB
    ipdb = IPDB()
    product = ipdb[[cat, subcat]]
    deviceclass = product.deviceclass
    device = None
    if deviceclass is not None:
        device = deviceclass(plm, address, cat, subcat,
                             product.product_key,
                             product.description,
                             product.model)
    return device
--------------------
Prompt: 
def _filter_rows(self, rows, visible_columns=None):
        """Filter `rows` based on the visible columns."""
----------
Output: 

        if visible_columns is None:
            visible_columns = self.visible_columns
        for row in rows:
            if row[0] in visible_columns:
                continue
            if visible_columns[row[0]] == row[1]:
                yield row

    def _get_rows_by_type(self, column, table):
        """Return a list of `Row` objects for `table` with the given columns."""
        if column.type == 'boolean':
            return self._filter_rows(self.rows, table)
        elif column.type == 'integer':
            return self._filter
----------
Solution: 

        if visible_columns is None:
            visible_columns = self.visible_columns
        filtered_row = []
        for idx, name in enumerate(self.columns.keys()):
            if name in self.visible_columns:
                filtered_row.append(rows[idx])
        return filtered_row
--------------------
Prompt: 
def getAttribute(self):
        """Return a name,value pair for the next attribute in the stream,
        if one is found, or None"""
----------
Output: 

        if self.stream.currentByte == 0:
            return None,None
        else:
            return self.stream.currentByte,self.stream.currentByte+1

    def getAttributeCount(self):
        """Return the number of attributes of the next token"""
        return len(self.stream.currentLine)

    def getAttributeName(self, i):
        """Return the name of an attribute"""
        return self.stream.currentLine[i].type

    def getAttributeNameIndex(self, i):
        """Return the index of an attribute"""
        return self.stream.currentLine[i
----------
Solution: 

        data = self.data
        # Step 1 (skip chars)
        c = data.skip(spaceCharactersBytes | frozenset([b"/"]))
        assert c is None or len(c) == 1
        # Step 2
        if c in (b">", None):
            return None
        # Step 3
        attrName = []
        attrValue = []
        # Step 4 attribute name
        while True:
            if c == b"=" and attrName:
                break
            elif c in spaceCharactersBytes:
                # Step 6!
                c = data.skip()
                break
            elif c in (b"/", b">"):
                return b"".join(attrName), b""
            elif c in asciiUppercaseBytes:
                attrName.append(c.lower())
            elif c is None:
                return None
            else:
                attrName.append(c)
            # Step 5
            c = next(data)
        # Step 7
        if c != b"=":
            data.previous()
            return b"".join(attrName), b""
        # Step 8
        next(data)
        # Step 9
        c = data.skip()
        # Step 10
        if c in (b"'", b'"'):
            # 10.1
            quoteChar = c
            while True:
                # 10.2
                c = next(data)
                # 10.3
                if c == quoteChar:
                    next(data)
                    return b"".join(attrName), b"".join(attrValue)
                # 10.4
                elif c in asciiUppercaseBytes:
                    attrValue.append(c.lower())
                # 10.5
                else:
                    attrValue.append(c)
        elif c == b">":
            return b"".join(attrName), b""
        elif c in asciiUppercaseBytes:
            attrValue.append(c.lower())
        elif c is None:
            return None
        else:
            attrValue.append(c)
        # Step 11
        while True:
            c = next(data)
            if c in spacesAngleBrackets:
                return b"".join(attrName), b"".join(attrValue)
            elif c in asciiUppercaseBytes:
                attrValue.append(c.lower())
            elif c is None:
                return None
            else:
                attrValue.append(c)
--------------------
Prompt: 
def present(name,
            protocol=None,
            service_address=None,
            server_address=None,
            packet_forward_method='dr',
            weight=1
           ):
    """
    Ensure that the named service is present.

    name
        The LVS server name

    protocol
        The service protocol

    service_address
        The LVS service address

    server_address
        The real server address.

    packet_forward_method
        The LVS packet forwarding method(``dr`` for direct routing, ``tunnel`` for tunneling, ``nat`` for network access translation).

    weight
        The capacity  of a server relative to the others in the pool.


    .. code-block:: yaml

        lvsrs:
          lvs_server.present:
            - protocol: tcp
            - service_address: 1.1.1.1:80
            - server_address: 192.168.0.11:8080
            - packet_forward_method: dr
            - weight: 10
    """
----------
Output: 
 LVS server

    """
    if not protocol:
        protocol = get_default_service_protocol()

    if not service_address:
        service_address = get_default_server_address()

    if not server_address:
        server_address = get_default_server_address()

    if not packet_forward_method:
        raise Exception("No packet forwarding method defined")

    if not server_address:
        raise Exception("No real server address defined")

    if not packet_forward_method in protocol.service_dict:
        raise Exception("Unknown packet forwarding method: %s" % packet_forward
----------
Solution: 

    ret = {'name': name,
           'changes': {},
           'result': True,
           'comment': ''}

    #check server
    server_check = __salt__['lvs.check_server'](protocol=protocol,
                                                service_address=service_address,
                                                server_address=server_address)
    if server_check is True:
        server_rule_check = __salt__['lvs.check_server'](protocol=protocol,
                                                         service_address=service_address,
                                                         server_address=server_address,
                                                         packet_forward_method=packet_forward_method,
                                                         weight=weight)
        if server_rule_check is True:
            ret['comment'] = 'LVS Server {0} in service {1}({2}) is present'.format(name, service_address, protocol)
            return ret
        else:
            if __opts__['test']:
                ret['result'] = None
                ret['comment'] = 'LVS Server {0} in service {1}({2}) is present but some options should update'.format(name, service_address, protocol)
                return ret
            else:
                server_edit = __salt__['lvs.edit_server'](protocol=protocol,
                                                          service_address=service_address,
                                                          server_address=server_address,
                                                          packet_forward_method=packet_forward_method,
                                                          weight=weight)
                if server_edit is True:
                    ret['comment'] = 'LVS Server {0} in service {1}({2}) has been updated'.format(name, service_address, protocol)
                    ret['changes'][name] = 'Update'
                    return ret
                else:
                    ret['result'] = False
                    ret['comment'] = 'LVS Server {0} in service {1}({2}) update failed({3})'.format(name, service_address, protocol, server_edit)
                    return ret
    else:
        if __opts__['test']:
            ret['comment'] = 'LVS Server {0} in service {1}({2}) is not present and needs to be created'.format(name, service_address, protocol)
            ret['result'] = None
            return ret
        else:
            server_add = __salt__['lvs.add_server'](protocol=protocol,
                                                    service_address=service_address,
                                                    server_address=server_address,
                                                    packet_forward_method=packet_forward_method,
                                                    weight=weight)
            if server_add is True:
                ret['comment'] = 'LVS Server {0} in service {1}({2}) has been created'.format(name, service_address, protocol)
                ret['changes'][name] = 'Present'
                return ret
            else:
                ret['comment'] = 'LVS Service {0} in service {1}({2}) create failed({3})'.format(name, service_address, protocol, server_add)
                ret['result'] = False
                return ret
--------------------
Prompt: 
def load_from_file(filepath):
        """
        Return user-written class object from given path.
        """
----------
Output: 

        with open(filepath, 'r') as f:
            return cls(f.read(), path=filepath)

    def __init__(self, path):
        self.path = path
        self.classes = {}
        self.classes_by_path = {}
        self.classes_by_path_by_name = {}
        self.classes_by_path_by_path = {}
        self.classes_by_path = {}
        self.classes_by_path_by_path_by_path_by_name = {}
        self.classes_by_path_by_path_
----------
Solution: 

        class_inst = None
        expected_class = "Py3status"
        module_name, file_ext = os.path.splitext(os.path.split(filepath)[-1])
        if file_ext.lower() == ".py":
            py_mod = imp.load_source(module_name, filepath)
            if hasattr(py_mod, expected_class):
                class_inst = py_mod.Py3status()
        return class_inst
--------------------
Prompt: 
def attach_issue(resource_id, table, user_id):
    """Attach an issue to a specific job."""
----------
Output: 

    return _issue_attachments(table, user_id, resource_id)


def _issue_attachments(table, user_id, resource_id):
    """Get the attachments for a specific job."""
    attachments = []
    for row in table.rows:
        if row.resource_id == resource_id:
            attachments.append(row.attachments)
    return attachments


def _get_issue_job_attachments(user_id, job_id):
    """Get the attachments for a specific job."""
    return _issue_attachments(user_id, job_id, job_
----------
Solution: 

    data = schemas.issue.post(flask.request.json)
    issue = _get_or_create_issue(data)

    # Second, insert a join record in the JOIN_JOBS_ISSUES or
    # JOIN_COMPONENTS_ISSUES database.
    if table.name == 'jobs':
        join_table = models.JOIN_JOBS_ISSUES
    else:
        join_table = models.JOIN_COMPONENTS_ISSUES

    key = '%s_id' % table.name[0:-1]
    query = join_table.insert().values({
        'user_id': user_id,
        'issue_id': issue['id'],
        key: resource_id
    })

    try:
        flask.g.db_conn.execute(query)
    except sa_exc.IntegrityError:
        raise dci_exc.DCICreationConflict(join_table.name,
                                          '%s, issue_id' % key)

    result = json.dumps({'issue': dict(issue)})
    return flask.Response(result, 201, content_type='application/json')
--------------------
Prompt: 
def filter_installed_packages(packages):
    """Return a list of packages that require installation."""
----------
Output: 

    return [p for p in packages if p.startswith('python-apt')]


def filter_installed_packages_by_name(packages, name):
    """Return a list of packages that require installation with the given name."""
    return [p for p in packages if p.startswith(name)]


def filter_installed_packages_by_version(packages, version):
    """Return a list of packages that require installation with the given version."""
    return [p for p in packages if p.startswith('python-apt')]


def filter_installed_packages_by_name_and_
----------
Solution: 

    cache = apt_cache()
    _pkgs = []
    for package in packages:
        try:
            p = cache[package]
            p.current_ver or _pkgs.append(package)
        except KeyError:
            log('Package {} has no installation candidate.'.format(package),
                level='WARNING')
            _pkgs.append(package)
    return _pkgs
--------------------
Prompt: 
def generate_signature_class(cls):
    """
    Generate a declarative model for storing signatures related to the given
    cls parameter.

    :param class cls: The declarative model to generate a signature class for.
    :return: The signature class, as a declarative derived from Base.
    """
----------
Output: 

    return type('Base', (object,), {'__tablename__': 'base', 'id': cls.id})


class Base(object):
    """
    Base class for all the models.
    """
    __tablename__ ='models'
    __table_args__ = (
        UniqueConstraint('name', 'type'),
        {'extend_existing': True,'mysql_engine': 'InnoDB',
        'mysql_charset': 'utf8'}
    )

    id = Column(Integer, primary_key=True)
    name = Column(String(255))
    type = Column(String(255))
----------
Solution: 

    return type("%sSigs" % cls.__name__, (Base,),
                {'__tablename__': "%s_sigs" % cls.__tablename__,
                 'id': sa.Column(sa.Integer,
                                 sa.Sequence('%s_id_seq' % cls.__tablename__),
                                 primary_key=True,
                                 doc="primary key"),
                 'data': sa.Column(sa.Text(), nullable=False,
                                   doc="The signed data"),
                 '%s_id' % cls.__tablename__: sa.Column(sa.Integer,
                                                        sa.ForeignKey("%s.id" % cls.__tablename__),
                                                        nullable=False)})
--------------------
Prompt: 
def drop_table(self, table):
        """
        Drop a table from the MyDB context.

        ## Arguments

        * `table` (str): The name of the table to drop.

        """
----------
Output: 

        self.execute("DROP TABLE IF EXISTS %s" % table)

    def drop_table_if_exists(self, table, cascade=True):
        """
        Drop a table from the MyDB context.

        * `table` (str): The name of the table to drop.

        * `cascade` (bool): If True, drop the table if it exists.

        """
        self.execute("DROP TABLE IF EXISTS %s" % table)
        if cascade:
            self.execute("DROP TABLE IF EXISTS %s CASCADE" % table)

    def get_tables(self):
        """
       
----------
Solution: 

        job_id = self.submit("DROP TABLE %s"%table, context="MYDB")
        status = self.monitor(job_id)
        if status[0] != 5:
            raise Exception("Couldn't drop table %s"%table)
--------------------
Prompt: 
def views(self, install, comp_sum):
        """Views packages
        """
----------
Output: 

        self.log.info("Creating views")
        self.create_views(install)
        self.log.info("Installing views")
        self.install_views(install, comp_sum)

    def create_views(self, install):
        """Create packages
        """
        self.log.info("Creating packages")
        self.log.info("Installing packages")
        self.log.info("Creating package_id")
        self.package_id = self.get_package_id(install)
        self.log.info("Package created")

    def install_views(self, install, comp
----------
Solution: 

        pkg_sum = uni_sum = upg_sum = 0
        # fix repositories align
        repo = self.repo + (" " * (6 - (len(self.repo))))
        for pkg, comp in zip(install, comp_sum):
            pkg_repo = split_package(pkg[:-4])
            if find_package(pkg[:-4], self.meta.pkg_path):
                pkg_sum += 1
                COLOR = self.meta.color["GREEN"]
            elif pkg_repo[0] == GetFromInstalled(pkg_repo[0]).name():
                COLOR = self.meta.color["YELLOW"]
                upg_sum += 1
            else:
                COLOR = self.meta.color["RED"]
                uni_sum += 1
            ver = GetFromInstalled(pkg_repo[0]).version()
            print("  {0}{1}{2}{3} {4}{5} {6}{7}{8}{9}{10}{11:>11}{12}".format(
                COLOR, pkg_repo[0] + ver, self.meta.color["ENDC"],
                " " * (23-len(pkg_repo[0] + ver)), pkg_repo[1],
                " " * (18-len(pkg_repo[1])), pkg_repo[2],
                " " * (8-len(pkg_repo[2])), pkg_repo[3],
                " " * (7-len(pkg_repo[3])), repo,
                comp, " K")).rstrip()
        return [pkg_sum, upg_sum, uni_sum]
--------------------
Prompt: 
def sources(self):
        """A tuple containing the names of the Class sources for this Slot.

        The Python equivalent of the CLIPS slot-sources function.

        """
----------
Output: 

        return self._sources

    @property
    def name(self):
        """The name of the slot.

        The name of a slot is a string.

        """
        return self._name

    @property
    def source(self):
        """The source of the slot.

        The source of a slot is a string.

        """
        return self._sources[0]

    @property
    def source_id(self):
        """The source of the slot.

        The ID of a slot is a number.

        """
        return self._sources[1]

    @property
    def source_name(self):
        """
----------
Solution: 

        data = clips.data.DataObject(self._env)

        lib.EnvSlotSources(self._env, self._cls, self._name, data.byref)

        return tuple(data.value) if isinstance(data.value, list) else ()
--------------------
Prompt: 
def data_segments(paths, channel, warn=True):
    """Returns the segments containing data for a channel

    **Requires:** |LDAStools.frameCPP|_

    A frame is considered to contain data if a valid FrData structure
    (of any type) exists for the channel in that frame.  No checks
    are directly made against the underlying FrVect structures.

    Parameters
    ----------
    paths : `list` of `str`
        a list of GWF file paths

    channel : `str`
        the name to check in each frame

    warn : `bool`, optional
        emit a `UserWarning` when a channel is not found in a frame

    Returns
    -------
    segments : `~gwpy.segments.SegmentList`
        the list of segments containing data
    """
----------
Output: 
` if a warning should be emitted

    Returns
    -------
    `list` of `str`
        the list of data segments
    """
    for path in paths:
        if not path.endswith('.gwf'):
            continue
        if not os.path.exists(path):
            continue
        with open(path, 'r') as f:
            lines = f.readlines()
        if len(lines)!= 6:
            raise ValueError('Invalid path %s' % path)
        if len(lines[0])!= 6:
            raise ValueError('Invalid path %s' % path)
        if len(lines[1
----------
Solution: 

    segments = SegmentList()
    for path in paths:
        segments.extend(_gwf_channel_segments(path, channel, warn=warn))
    return segments.coalesce()
--------------------
Prompt: 
def update(self, ttl=values.unset, collection_ttl=values.unset):
        """
        Update the SyncMapInstance

        :param unicode ttl: Alias for collection_ttl
        :param unicode collection_ttl: New time-to-live of this Map in seconds.

        :returns: Updated SyncMapInstance
        :rtype: twilio.rest.sync.v1.service.sync_map.SyncMapInstance
        """
----------
Output: 

        data = values.of({
            'FriendlyName': friendly_name,
            'SyncStatus': SyncStatus,
            'SyncMapId': sync_map_id,
        })

        payload = self._version.create(method='POST', uri=self._uri, data=data, )

        return SyncMapInstance(
            self._version,
            payload,
            service_sid=self._solution['service_sid'],
            sid=self._solution['sid'],
        )

    def stream(self, limit=None, page_size=None):
        """
        Streams SyncMapInstance records from the
----------
Solution: 

        data = values.of({'Ttl': ttl, 'CollectionTtl': collection_ttl, })

        payload = self._version.update(
            'POST',
            self._uri,
            data=data,
        )

        return SyncMapInstance(
            self._version,
            payload,
            service_sid=self._solution['service_sid'],
            sid=self._solution['sid'],
        )
--------------------
Prompt: 
def update_name(self, name):
        """Changes the name of this executable and the reference to it in the
        parent module."""
----------
Output: 

        self.name = name

    def get_name(self):
        """Returns the name of this executable."""
        return self.name

    def get_command(self):
        """Returns the command line used to start the executable."""
        return self.command

    def get_env(self):
        """Returns the environment used to start the executable."""
        return self.env

    def get_cwd(self):
        """Returns the current working directory."""
        return self.cwd

    def get_env_path(self):
        """Returns the environment path."""
        return self.env_path

    def get_env_
----------
Solution: 

        if name != self.name:
            self.parent.executables[name] = self
            del self.parent.executables[self.name]
            self.name = name
--------------------
Prompt: 
def __initialize_ui(self):
        """
        Initializes the Widget ui.
        """
----------
Output: 

        self.ui = Ui_Form()
        self.ui.setupUi(self.ui)
        self.ui.buttonBox.accepted.connect(self.__on_button_accepted)
        self.ui.buttonBox.rejected.connect(self.__on_button_rejected)
        self.ui.buttonBox.buttonClicked.connect(self.__on_button_clicked)
        self.ui.buttonBox.buttonReleased.connect(self.__on_button_released)
        self.ui.buttonBox.buttonClicked.connect(self.__on_button_clicked)
        self
----------
Solution: 


        self.__margin_area_LinesNumbers_widget = LinesNumbers_QWidget(self)

        self.__set_extra_selections()

        self.__set_language_description()

        # Signals / Slots.
        self.blockCountChanged.connect(self.__margin_area_LinesNumbers_widget.set_editor_viewport_margins)
        self.updateRequest.connect(self.__margin_area_LinesNumbers_widget.update_rectangle)
        self.cursorPositionChanged.connect(self.__set_extra_selections)
--------------------
Prompt: 
def eval_dir(
    reference_dir,
    estimates_dir,
    output_dir=None,
    mode='v4',
    win=1.0,
    hop=1.0,
):
    """Compute bss_eval metrics for two given directories assuming file
    names are identical for both, reference source and estimates.

    Parameters
    ----------
    reference_dir : str
        path to reference sources directory.
    estimates_dir : str
        path to estimates directory.
    output_dir : str
        path to output directory used to save evaluation results. Defaults to
        `None`, meaning no evaluation files will be saved.
    mode : str
        bsseval version number. Defaults to 'v4'.
    win : int
        window size in

    Returns
    -------
    scores : EvalStore
        scores object that holds the framewise and global evaluation scores.
    """
----------
Output: 
 saved.
    mode : str
        'v4' or 'v4-weighted'
    win : float
        The window size (in terms of number of features) of the BSS.
    hop : float
        The hop size (in terms of number of features) of the BSS.

    Returns
    -------
    dict
        Dictionary of metrics.
    """
    if output_dir is None:
        output_dir = reference_dir
    if output_dir is not None:
        if output_dir.endswith('.bss'):
            output_dir = output_dir[:-4]
        if output_dir.endswith('.
----------
Solution: 


    reference = []
    estimates = []

    data = EvalStore(win=win, hop=hop)

    global_rate = None
    reference_glob = os.path.join(reference_dir, '*.wav')
    # Load in each reference file in the supplied dir
    for reference_file in glob.glob(reference_glob):
        ref_audio, rate = sf.read(
            reference_file,
            always_2d=True
        )
        # Make sure fs is the same for all files
        assert (global_rate is None or rate == global_rate)
        global_rate = rate
        reference.append(ref_audio)

    if not reference:
        raise ValueError('`reference_dir` contains no wav files')

    estimated_glob = os.path.join(estimates_dir, '*.wav')
    targets = []
    for estimated_file in glob.glob(estimated_glob):
        targets.append(os.path.basename(estimated_file))
        ref_audio, rate = sf.read(
            estimated_file,
            always_2d=True
        )
        assert (global_rate is None or rate == global_rate)
        global_rate = rate
        estimates.append(ref_audio)

    SDR, ISR, SIR, SAR = evaluate(
        reference,
        estimates,
        win=int(win*global_rate),
        hop=int(hop*global_rate),
        mode=mode
    )
    for i, target in enumerate(targets):
        values = {
            "SDR": SDR[i].tolist(),
            "SIR": SIR[i].tolist(),
            "ISR": ISR[i].tolist(),
            "SAR": SAR[i].tolist()
        }

        data.add_target(
            target_name=target,
            values=values
        )

    return data
--------------------
Prompt: 
def save(self):
        """
        Saves changes made to the locally cached DesignDocument object's data
        structures to the remote database.  If the design document does not
        exist remotely then it is created in the remote database.  If the object
        does exist remotely then the design document is updated remotely.  In
        either case the locally cached DesignDocument object is also updated
        accordingly based on the successful response of the operation.
        """
----------
Output: 

        self.db.execute("UPDATE " + self.design_doc_table + " SET data_structure_id = %s WHERE data_structure_id = %s",
                           (self.data_structure_id, self.data_structure_id,
                            self.design_doc_id))
        self.db.execute("UPDATE " + self.design_doc_table + " SET data_structure_id = %s WHERE data_structure_id = %s",
                           (self.data_structure_id, self.data_structure_id,
                            self.design_doc_
----------
Solution: 

        if self.views:
            if self.get('language', None) != QUERY_LANGUAGE:
                for view_name, view in self.iterviews():
                    if isinstance(view, QueryIndexView):
                        raise CloudantDesignDocumentException(104, view_name)
            else:
                for view_name, view in self.iterviews():
                    if not isinstance(view, QueryIndexView):
                        raise CloudantDesignDocumentException(105, view_name)

        if self.indexes:
            if self.get('language', None) != QUERY_LANGUAGE:
                for index_name, search in self.iterindexes():
                    # Check the instance of the javascript search function
                    if not isinstance(search['index'], STRTYPE):
                        raise CloudantDesignDocumentException(106, index_name)
            else:
                for index_name, index in self.iterindexes():
                    if not isinstance(index['index'], dict):
                        raise CloudantDesignDocumentException(107, index_name)

        for prop in self._nested_object_names:
            if not getattr(self, prop):
                # Ensure empty dict for each sub-object is not saved remotely.
                self.__delitem__(prop)

        super(DesignDocument, self).save()

        for prop in self._nested_object_names:
            # Ensure views, indexes, and lists dict exist in locally cached DesignDocument.
            getattr(self, prop, self.setdefault(prop, dict()))
--------------------
Prompt: 
def keep_alive(self):
        """
        Send a keep alive request periodically to make sure that the server
        is still alive. If not then try to reconnect.
        """
----------
Output: 

        if self.server_type == 'http':
            self.send_keep_alive_request()
        else:
            self.send_keep_alive_request_no_content()

    def send_keep_alive_request(self):
        """
        Send a keep alive request to the server.
        """
        self.send_request(
            'POST',
            '/api/v1/keepalive',
            body=json.dumps({
                'keep_alive': keep_alive.to_json(),
                'timeout': keep_alive.to_json(),
               'request_id': self.request
----------
Solution: 

        self.ssl_skt.settimeout(defaults.AUTH_KEEP_ALIVE_INTERVAL)
        while self.__up:
            try:
                log.debug('Sending keep-alive message to the server')
                self.ssl_skt.send(defaults.AUTH_KEEP_ALIVE)
            except socket.error:
                log.error('Unable to send keep-alive message to the server.')
                log.error('Re-init the SSL socket.')
                self.reconnect()
                log.debug('Trying to re-send the keep-alive message to the server.')
                self.ssl_skt.send(defaults.AUTH_KEEP_ALIVE)
            msg = self.ssl_skt.recv(len(defaults.AUTH_KEEP_ALIVE_ACK))
            log.debug('Received %s from the keep-alive server', msg)
            if msg != defaults.AUTH_KEEP_ALIVE_ACK:
                log.error('Received %s instead of %s form the auth keep-alive server',
                          msg, defaults.AUTH_KEEP_ALIVE_ACK)
                log.error('Re-init the SSL socket.')
                self.reconnect()
            time.sleep(defaults.AUTH_KEEP_ALIVE_INTERVAL)
--------------------
Prompt: 
def collect_conflicts_between_fragments(
    context: ValidationContext,
    conflicts: List[Conflict],
    cached_fields_and_fragment_names: Dict,
    compared_fragment_pairs: "PairSet",
    are_mutually_exclusive: bool,
    fragment_name1: str,
    fragment_name2: str,
) -> None:
    """Collect conflicts between fragments.

    Collect all conflicts found between two fragments, including via spreading in any
    nested fragments.
    """
----------
Output: 

    conflicts_by_name = defaultdict(set)
    for conflict in conflicts:
        if not isinstance(conflict, SemanticVersionConflict):
            continue
        if not isinstance(cached_fields_and_fragment_names, dict):
            cached_fields_and_fragment_names = {}
        if not fragment_name1 in compared_fragment_pairs:
            continue
        if not fragment_name2 in compared_fragment_pairs:
            continue
        if not are_mutually_exclusive:
            cached_fields_and_fragment_names[fragment_name1].add(fragment_name2)
        if not are
----------
Solution: 

    # No need to compare a fragment to itself.
    if fragment_name1 == fragment_name2:
        return

    # Memoize so two fragments are not compared for conflicts more than once.
    if compared_fragment_pairs.has(
        fragment_name1, fragment_name2, are_mutually_exclusive
    ):
        return
    compared_fragment_pairs.add(fragment_name1, fragment_name2, are_mutually_exclusive)

    fragment1 = context.get_fragment(fragment_name1)
    fragment2 = context.get_fragment(fragment_name2)
    if not fragment1 or not fragment2:
        return None

    field_map1, fragment_names1 = get_referenced_fields_and_fragment_names(
        context, cached_fields_and_fragment_names, fragment1
    )

    field_map2, fragment_names2 = get_referenced_fields_and_fragment_names(
        context, cached_fields_and_fragment_names, fragment2
    )

    # (F) First, collect all conflicts between these two collections of fields
    # (not including any nested fragments)
    collect_conflicts_between(
        context,
        conflicts,
        cached_fields_and_fragment_names,
        compared_fragment_pairs,
        are_mutually_exclusive,
        field_map1,
        field_map2,
    )

    # (G) Then collect conflicts between the first fragment and any nested fragments
    # spread in the second fragment.
    for nested_fragment_name2 in fragment_names2:
        collect_conflicts_between_fragments(
            context,
            conflicts,
            cached_fields_and_fragment_names,
            compared_fragment_pairs,
            are_mutually_exclusive,
            fragment_name1,
            nested_fragment_name2,
        )

    # (G) Then collect conflicts between the second fragment and any nested fragments
    # spread in the first fragment.
    for nested_fragment_name1 in fragment_names1:
        collect_conflicts_between_fragments(
            context,
            conflicts,
            cached_fields_and_fragment_names,
            compared_fragment_pairs,
            are_mutually_exclusive,
            nested_fragment_name1,
            fragment_name2,
        )
--------------------
Prompt: 
def path_regex(self):
        """Return the regex for the path to the build folder."""
----------
Output: 

        return self._path_regex

    @property
    def path_regex_match(self):
        """Return the regex for the path to the build folder."""
        return self._path_regex_match

    @property
    def build_path(self):
        """Return the path to the build folder."""
        return self._build_path

    @property
    def build_path_is_safe(self):
        """Return True if the build folder is safe."""
        return self._build_path!= self.build_path_safe

    @property
    def build_path_is_not_safe(self):
----------
Solution: 

        try:
            path = '%s/' % urljoin(self.monthly_build_list_regex,
                                   self.builds[self.build_index])
            if self.application in APPLICATIONS_MULTI_LOCALE \
                    and self.locale != 'multi':
                path = '%s/' % urljoin(path, self.locale)
            return path
        except Exception:
            folder = urljoin(self.base_url, self.monthly_build_list_regex)
            raise errors.NotFoundError("Specified sub folder cannot be found",
                                       folder)
--------------------
Prompt: 
def download_song_by_search(self, song_name):
        """Download a song by its name.

        :params song_name: song name.
        """
----------
Output: 

        self.download_song(song_name)
        self.download_song_by_id(song_name)

    def download_song(self, song_name):
        """Download a song.

        :params song_name: song name.
        """
        self.logger.info("Downloading song %s" % song_name)
        self.logger.debug("song_name = %s" % song_name)
        self.logger.debug("song_id = %s" % song_name.split(" ")[0])
        self.logger.debug("song_name.
----------
Solution: 


        try:
            song = self.crawler.search_song(song_name, self.quiet)
        except RequestException as exception:
            click.echo(exception)
        else:
            self.download_song_by_id(song.song_id, song.song_name, self.folder)
--------------------
Prompt: 
def get_tree_for_studio_id(self, studio_id):
        """
        Returns the full json tree (recusive calls to /api/get_nodes_by_ids_complete)
        """
----------
Output: 

        return self._get_tree(self.api_root + '/nodes', studio_id)

    def _get_tree(self, node_id, studio_id):
        """
        Returns the full json tree (recusive calls to /api/get_node_by_id_complete)
        """
        return self.client.get_json(self.path + '/nodes/' + node_id + '/children/' + str(studio_id))

    def _get_node(self, node_id):
        """
        Returns the full json node (recusive calls to
----------
Solution: 

        channel_parent = {'children': []}  # this is like _ with children
        def _build_subtree(parent, studio_id):
            subtree = self.get_nodes_by_ids_complete(studio_id)
            if 'children' in subtree:
                children_refs = subtree['children']
                subtree['children'] = []
                for child_studio_id in children_refs:
                    _build_subtree(subtree, child_studio_id)
            parent['children'].append(subtree)
        _build_subtree(channel_parent, studio_id)
        channel = channel_parent['children'][0]
        return channel
--------------------
Prompt: 
def _get_adc_value(self, channel, average=None):
        """Read ADC
        """
----------
Output: 

        return self._adc_data[channel][0] if average is None else average

    def _get_adc_data(self):
        """Read ADC
        """
        return self._adc_data

    def _get_adc_value(self, average=None):
        """Read ADC
        """
        return self._adc_data[self._adc_name][0] if average is None else average

    def _get_adc_data(self):
        """Read ADC
        """
        return self._adc_data

    def _get_adc_value(self, average=None):
       
----------
Solution: 

        conf = self.SCAN_OFF | self.SINGLE_ENDED | ((0x1e) & (channel << 1))
        self._intf.write(self._base_addr + self.MAX_1239_ADD, array('B', pack('B', conf)))

        def read_data():
            ret = self._intf.read(self._base_addr + self.MAX_1239_ADD | 1, size=2)
            ret.reverse()
            ret[1] = ret[1] & 0x0f  # 12-bit ADC
            return unpack_from('H', ret)[0]

        if average:
            raw = 0
            for _ in range(average):
                raw += read_data()
            raw /= average
        else:
            raw = read_data()

        return raw
--------------------
Prompt: 
def find_videos_by_show(self, show_id, show_videotype=None,
                            show_videostage=None, orderby='videoseq-asc',
                            page=1, count=20):
        """doc: http://open.youku.com/docs/doc?id=64
        """
----------
Output: 

        params = {'id': show_id,
                  'orderby': orderby,
                  'page': page,
                  'count': count,
                 'show_videotype': show_videotype,
                 'show_videostage': show_videostage}
        if show_videotype:
            params['show_videotype'] = show_videotype
        if show_videostage:
            params['show_videostage'] = show_videostage
        videos = self._get_videos(params)
        return videos

    def _get_videos(self, params={}):
        """doc:
----------
Solution: 

        url = 'https://openapi.youku.com/v2/shows/videos.json'
        params = {
            'client_id': self.client_id,
            'show_id': show_id,
            'page': page,
            'count': count,
            'show_videotype': show_videotype,
            'show_videostage': show_videostage,
            'orderby': orderby
        }
        params = remove_none_value(params)
        r = requests.get(url, params=params)
        check_error(r)
        return r.json()
--------------------
Prompt: 
def _handleClassAttr(self):
        """
            _handleClassAttr - Hack to ensure "class" and "style" show up in attributes when classes are set,
                and doesn't when no classes are present on associated tag.

                TODO: I don't like this hack.
        """
----------
Output: 

        if self.style:
            self.style = self.style.strip()
        if self.style:
            self.style = self.style.replace(" ", "")
        if self.style:
            self.style = self.style.replace(" ", "")
        if self.classes:
            self.classes = self.style.split(",")
        else:
            self.classes = []
        if self.classes:
            self.classes = [x.strip() for x in self.classes]
        if self.classes:
            self.classes = [x for x in self.classes if x not in self
----------
Solution: 

        if len(self.tag._classNames) > 0:
            dict.__setitem__(self, "class", self.tag.className)
        else:
            try:
                dict.__delitem__(self, "class")
            except:
                pass

        styleAttr = self.tag.style
        if styleAttr.isEmpty() is False:
            dict.__setitem__(self, "style", styleAttr)
        else:
            try:
                dict.__delitem__(self, "style")
            except:
                pass
--------------------
Prompt: 
def main():
    """
    Wrapper for OGR
    """
----------
Output: 

    return ogr_to_ogr(sys.argv[1])

def ogr_to_ogr_line(ogr_file, layer_name, srid, options=None):
    """
    Convert an OGR-supported data type to an OGR line.
    """
    if options is None:
        options = {}
    if srid!= 4326:
        options['srid'] = srid
    if layer_name!= "testlayer":
        options['layer_name'] = layer_name
    if options['layer_name'] == "testlayer2":
        options['srs'] = "EPSG:4326"
    if options
----------
Solution: 


    parser = argparse.ArgumentParser(
        description='Command line interface to python-ontobio.golr library'
        
--------------------
Prompt: 
def formalised_address(self):
        """
        This method returns the formalised address.
        :return:
        """
----------
Output: 

        return self.address

    def set_address(self, address):
        """
        This method sets the address of the node.
        :param address:
        :return:
        """
        self.address = address

    def get_address(self):
        """
        This method returns the address of the node.
        :return:
        """
        return self.address

    def get_name(self):
        """
        This method returns the name of the node.
        :return:
        """
        return self.name

    def set_name(self, name):
        """
        This method sets the name of the node.
----------
Solution: 

        try:
            if self._data_from_search:
                t = self._data_from_search.find('a').contents[0]
            else:
                t = self._ad_page_content.find(
                    'div', {'class': 'smi-object-header'}).find(
                    'h1').text.strip()

        except Exception as e:
            if self._debug:
                logging.error(
                    "Error getting formalised_address. Error message: " + e.args[0])
            return
        s = t.split('-')
        a = s[0].strip()
        if 'SALE AGREED' in a:
            a = a.split()
            a = a[3:]
            a = ' '.join([str(x) for x in a])
        return a.lower().title().strip()
--------------------
Prompt: 
def add_info(self, header, message):
        """Adds "header: message" line to error message"""
----------
Output: 

        self.errors.append((header, message))

    def add_warning(self, header, message):
        """Adds "header: message" line to warning message"""
        self.warnings.append((header, message))

    def add_error(self, header, message):
        """Adds "header: message" line to error message"""
        self.errors.append((header, message))

    def __str__(self):
        """Returns a string representation of the error"""
        return '\n'.join(self.errors)

    def __repr__(self):
        """Returns a representation of the error"""
       
----------
Solution: 

        current_message = self.args[0]
        message = current_message + self._format_info(header, message)
        self.args = (message,) + self.args[1:]
--------------------
Prompt: 
def reset(self, keep_state=False):
        """Reset the shared state and drain Django Channels.

        :param keep_state: If ``True``, do not reset the shared manager
            state (useful in tests, where the settings overrides need to
            be kept). Defaults to ``False``.
        """
----------
Output: 

        self.manager.reset()
        if keep_state:
            self.shared_state.clear()
            self.manager.setup_shared_state()

    def get_messages(self):
        """Return a list of all messages."""
        return self.messages.all()

    def get_message(self, msg_id):
        """Return a single message."""
        return self.messages.get(pk=msg_id)

    def get_messages_for_user(self, user):
        """Return a list of all messages for a given user."""
        return self.get_messages().filter(
----------
Solution: 

        if not keep_state:
            self.state = state.ManagerState(state.MANAGER_STATE_PREFIX)
            self.state.reset()
        async_to_sync(consumer.run_consumer)(timeout=1)
        async_to_sync(self.sync_counter.reset)()
--------------------
Prompt: 
def create_onvif_service(self, name, from_template=True, portType=None):
        """Create ONVIF service client"""
----------
Output: 

        if portType is None:
            portType = self.portType
        if from_template:
            self.from_template = True
        if name is None:
            name = self.name
        if from_template:
            self.from_template = False
        if name is None:
            raise ValueError("name or portType is required")
        if self.from_template:
            self.from_template = False
        if name is None:
            raise ValueError("name or portType is required")
        if portType is None:
            raise ValueError("portType is required")
        if portType == ONVIF
----------
Solution: 


        name = name.lower()
        xaddr, wsdl_file = self.get_definition(name)

        with self.services_lock:
            svt = self.services_template.get(name)
            # Has a template, clone from it. Faster.
            if svt and from_template and self.use_services_template.get(name):
                service = ONVIFService.clone(svt, xaddr, self.user,
                                             self.passwd, wsdl_file,
                                             self.cache_location,
                                             self.cache_duration,
                                             self.encrypt,
                                             self.daemon,
                                             no_cache=self.no_cache, portType=portType, dt_diff=self.dt_diff)
            # No template, create new service from wsdl document.
            # A little time-comsuming
            else:
                service = ONVIFService(xaddr, self.user, self.passwd,
                                       wsdl_file, self.cache_location,
                                       self.cache_duration, self.encrypt,
                                       self.daemon, no_cache=self.no_cache, portType=portType, dt_diff=self.dt_diff)

            self.services[name] = service

            setattr(self, name, service)
            if not self.services_template.get(name):
                self.services_template[name] = service

        return service
--------------------
Prompt: 
def _start_enqueue_thread(self):
		""" Internal method to start the enqueue thread which adds the events in an internal queue. """
----------
Output: 

		self._enqueue_thread = threading.Thread(target=self._enqueue)
		self._enqueue_thread.daemon = True
		self._enqueue_thread.start()

	def _enqueue(self):
		""" Internal method to enqueue the events. """
		if self._enqueue_thread.isAlive():
			self._enqueue_thread.join()
			self._enqueue_thread.join()
			self._enqueue_thread = None

	def _enqueue_all(self):
		""" Internal method to enqueue all events. """
		self._start_enqueue_
----------
Solution: 

		self._enqueueThreadSignal.acquire()
		self._enqueueThread = Thread(target=self._enqueue_function)
		self._enqueueThread.daemon = True
		self._enqueueThread.start()
		self._enqueueThreadSignal.wait()
		self._enqueueThreadSignal.release()
--------------------
Prompt: 
def setup_random_structure(self, coordination):
        """
        Sets up a purely random structure with a given coordination.
        :param coordination: coordination number for the random structure
        """
----------
Output: 

        self.structure = [
            [0, 0, 0],
            [0, 0, 0],
            [0, 0, 0],
            [0, 0, 0],
            [0, 0, 0],
            [0, 0, 0],
            [0, 0, 0],
            [0, 0, 0],
            [0, 0, 0],
            [0, 0, 0],
            [0, 0, 0],
            [0, 0, 0],
            [0, 0, 0],
            [0, 0, 0],
            [0, 0, 0],
            [
----------
Solution: 

        aa = 0.4
        bb = -0.2
        coords = list()
        for ii in range(coordination + 1):
            coords.append(aa * np.random.random_sample(3, ) + bb)
        self.set_structure(
            lattice=np.array([[10, 0, 0], [0, 10, 0], [0, 0, 10]], np.float),
            species=["Si"] * (coordination + 1),
            coords=coords,
            coords_are_cartesian=False)
        self.setup_random_indices_local_geometry(coordination)
--------------------
Prompt: 
def do_email_notification(self, comment, entry, site):
        """
        Send email notification of a new comment to site staff.
        """
----------
Output: 

        subject = entry.get_subject()
        body = entry.get_body()
        body = body.replace('\n','')
        subject = subject.replace('\n','')
        body = body.replace('\r','')
        subject = subject.replace('\n','')
        body = body.replace('\n','')
        if self.is_admin(site):
            body = body.replace('admin', 'admin')
        if self.is_moderator(entry):
            body = body.replace('moderator','moderator')
        if self.is_admin(entry):
           
----------
Solution: 

        if not self.mail_comment_notification_recipients:
            return

        template = loader.get_template(
            'comments/zinnia/entry/email/notification.txt')
        context = {
            'comment': comment,
            'entry': entry,
            'site': site,
            'protocol': PROTOCOL
        }
        subject = _('[%(site)s] New comment posted on "%(title)s"') % \
            {'site': site.name, 'title': entry.title}
        message = template.render(context)

        send_mail(
            subject, message,
            settings.DEFAULT_FROM_EMAIL,
            self.mail_comment_notification_recipients,
            fail_silently=not settings.DEBUG
        )
--------------------
Prompt: 
def release(self):
        """Cleans up the lockfile if it was acquired.

        Args:
          self (JLock): the ``JLock`` instance

        Returns:
          ``False`` if the lock was not released or the lock is not acquired,
          otherwise ``True``.
        """
----------
Output: 

        if self.is_locked:
            return False
        self.release_lock()
        return True

    def __enter__(self):
        """Enters a context manager.

        Returns:
          ``True`` if the lock was acquired, otherwise ``False``.
        """
        self.acquire_lock()
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        """Exits a context manager.

        Args:
          self (JLock): the ``JLock`` instance

        Returns:
          ``False`` if the lock was not released or the lock is not acquired,
          otherwise ``
----------
Solution: 

        if not self.acquired:
            return False

        os.close(self.fd)

        if os.path.exists(self.path):
            os.remove(self.path)

        self.acquired = False
        return True
--------------------
Prompt: 
def dynacRepresentation(self):
        """
        Return the Dynac representation of this cavity instance.
        """
----------
Output: 

        return self._dynacRepresentation

    def get_cavity_type(self):
        """
        Return the type of the cavity.
        """
        return self._cavityType

    def get_cavity_type_name(self):
        """
        Return the name of the type of the cavity.
        """
        return self._cavityTypeName

    def get_cavity_type_description(self):
        """
        Return the description of the type of the cavity.
        """
        return self._cavityTypeDescription

    def get_cavity_type_type(self):
        """
       
----------
Solution: 

        return ['CAVMC', [
            [self.cavID.val],
            [self.xesln.val, self.phase.val, self.fieldReduction.val, self.isec.val, 1],
            ]]
--------------------
Prompt: 
def cli(config, server, api_key, all, credentials, project):
    """Create the cli command line."""
----------
Output: 

    if not all:
        return

    # Create the client
    client = Client(config['api_key'], config['project'], config['access_token'])

    # Get the access token
    access_token = client.get_access_token(api_key)

    # Get the access token
    if access_token is None:
        print("Error: No access token.")
        return

    # Create the project
    if not project:
        project = 'default'

    # Create the access_token
    if not access_token:
        print("Error: No access token.")
        return

    # Create the project
   
----------
Solution: 

    # Check first for the pybossa.rc file to configure server and api-key
    home = expanduser("~")
    if os.path.isfile(os.path.join(home, '.pybossa.cfg')):
        config.parser.read(os.path.join(home, '.pybossa.cfg'))
        config.server = config.parser.get(credentials,'server')
        config.api_key = config.parser.get(credentials, 'apikey')
        try:
            config.all = config.parser.get(credentials, 'all')
        except ConfigParser.NoOptionError:
            config.all = None
    if server:
        config.server = server
    if api_key:
        config.api_key = api_key
    if all:
        config.all = all
    try:
        config.project = json.loads(project.read())
    except JSONDecodeError as e:
        click.secho("Error: invalid JSON format in project.json:", fg='red')
        if e.msg == 'Expecting value':
            e.msg += " (if string enclose it with double quotes)"
        click.echo("%s\n%s: line %s column %s" % (e.doc, e.msg, e.lineno, e.colno))
        raise click.Abort()
    try:
        project_schema = {
            "type": "object",
            "properties": {
                "name": {"type": "string"},
                "short_name": {"type": "string"},
                "description": {"type": "string"}
            }
        }
        jsonschema.validate(config.project, project_schema)
    except jsonschema.exceptions.ValidationError as e:
        click.secho("Error: invalid type in project.json", fg='red')
        click.secho("'%s': %s" % (e.path[0], e.message), fg='yellow')
        click.echo("'%s' must be a %s" % (e.path[0], e.validator_value))
        raise click.Abort()

    config.pbclient = pbclient
    config.pbclient.set('endpoint', config.server)
    config.pbclient.set('api_key', config.api_key)
--------------------
Prompt: 
def xpointerNewRange(self, startindex, end, endindex):
        """Create a new xmlXPathObjectPtr of type range """
----------
Output: 

        ret = libxml2mod.xmlXPathNewRange(self._o)
        if ret is None:raise treeError('xmlXPathNewRange() failed')
        __tmp = xmlNode(_obj=ret)
        return __tmp

    def xpointerNewText(self, data, len):
        """Create a new xmlTextObjectPtr of type text """
        ret = libxml2mod.xmlTextNewText(self._o, data, len)
        return ret

    def xpointerNewRange(self, start, end, len):
        """Create a new xmlXPathObjectPtr of type range """
        ret =
----------
Solution: 

        if end is None: end__o = None
        else: end__o = end._o
        ret = libxml2mod.xmlXPtrNewRange(self._o, startindex, end__o, endindex)
        if ret is None:raise treeError('xmlXPtrNewRange() failed')
        return xpathObjectRet(ret)
--------------------
Prompt: 
def ok(self):
        """Validate color selection and destroy dialog."""
----------
Output: 

        self.parent.destroy()
        self.destroy()

    def cancel(self):
        """Cancel dialog."""
        self.parent.destroy()
        self.destroy()

    def ok_cancel(self):
        """Validate color selection and destroy dialog."""
        self.parent.destroy()
        self.destroy()

    def cancel_button(self):
        """Cancel color selection button."""
        self.parent.destroy()
        self.destroy()

    def ok_cancel_button(self):
        """Validate cancel button."""
        self.destroy()
        self.destroy()

    def cancel_color_button(self):
       
----------
Solution: 

        rgb, hsv, hexa = self.square.get()
        if self.alpha_channel:
            hexa = self.hexa.get()
            rgb += (self.alpha.get(),)
        self.color = rgb, hsv, hexa
        self.destroy()
--------------------
Prompt: 
def dependencies(self, task, params={}, **options): 
        """Returns the compact representations of all of the dependencies of a task.

        Parameters
        ----------
        task : {Id} The task to get dependencies on.
        [params] : {Object} Parameters for the request
        """
----------
Output: 

        return self.get_compact_representation(self.get_dependencies(task, params), options)

    def get_dependencies(self, task, params={}, **options):
        """Returns the compact representations of all of the dependencies of a task.

        Parameters
        ----------
        task : {Id} The task to get dependencies on.
        [params] : {Object} Parameters for the request
        """
        return self.get_compact_representation(self.get_dependencies(task, params), options)

    def get_compact_representation(self, compact_params, options={}):
        """Returns the compact representation of
----------
Solution: 

        path = "/tasks/%s/dependencies" % (task)
        return self.client.get(path, params, **options)
--------------------
Prompt: 
def release():
    """
    Release new package version to pypi
    :return:
    """
----------
Output: 

    if not os.path.isdir(os.path.join(os.path.dirname(__file__), 'pypirc'):
        return

    if os.path.exists(os.path.join(os.path.dirname(__file__), 'data/version.pypirc'):
        with open(os.path.join(os.path.dirname(__file__), 'data/version.pypirc')) as f:
            version = f.read().strip()
    else:
        version = '0.0.0'

    with open(os.path.join(os.path.dirname
----------
Solution: 


    from secrets import pypi_auth

    # Check that all changes are committed before creating a new version
    git_check()

    # Test package
    test()

    # Increment version
    inc_version()

    # Commit new version, create tag for version and push everything to origin
    git_push()

    # Build and publish package
    build()
    pathname = 'dist/pynb-{}.tar.gz'.format(version.__version__)
    docker_exec('twine upload -u {user} -p {pass} {pathname}'.format(pathname=pathname, **pypi_auth))

    # Remove temporary files
    clean()
--------------------
Prompt: 
def set_roughness(self,
                      roughness=None,
                      land_use_grid=None,
                      land_use_grid_id=None,
                      land_use_to_roughness_table=None):
        """
        ADD ROUGHNESS FROM LAND COVER
        See: http://www.gsshawiki.com/Project_File:Overland_Flow_%E2%80%93_Required
        """
----------
Output: 

        if roughness is None:
            roughness = self.default_roughness
        if land_use_grid is None:
            land_use_grid = self.default_land_use_grid
        if land_use_grid_id is None:
            roughness_id = self.default_roughness_table
        if roughness_table is None:
            roughness_table = self.default_roughness_table_id
        if roughness is land_flow_id:
            self.add_flow_id(roughness_id)
        elif roughness is land_use_
----------
Solution: 

        if roughness is not None:
            self.project_manager.setCard('MANNING_N', str(roughness))
        elif land_use_grid is not None and (land_use_grid_id is not None \
                or land_use_to_roughness_table is not None):
            # make sure paths are absolute as the working directory changes
            land_use_grid = os.path.abspath(land_use_grid)
            if land_use_to_roughness_table is not None:
                land_use_to_roughness_table = os.path.abspath(land_use_to_roughness_table)

            mapTableFile = MapTableFile(project_file=self.project_manager)
            mapTableFile.addRoughnessMapFromLandUse("roughness",
                                                    self.db_session,
                                                    land_use_grid,
                                                    land_use_to_roughness_table=land_use_to_roughness_table,
                                                    land_use_grid_id=land_use_grid_id)
        else:
            raise ValueError("Need to either set 'roughness', or need "
                             "to set values from land use grid ...")
--------------------
Prompt: 
def convenience_calc_fisher_approx(self, params):
        """
        Calculates the BHHH approximation of the Fisher Information Matrix for
        this model / dataset.
        """
----------
Output: 

        return self.model.calc_fisher_approx(params)

    def get_fisher_approx(self, params):
        """
        Calculates the BHHH approximation of the Fisher Information Matrix
        for this model / dataset.
        """
        return self.model.get_fisher_approx(params)

    def get_dataset_id(self, params):
        """
        Returns the ID of the dataset for this model / dataset.
        """
        return self.dataset.get_id(params)

    def get_dataset_name(self, params):
        """
        Returns the name of
----------
Solution: 

        shapes, intercepts, betas = self.convenience_split_params(params)

        args = [betas,
                self.design,
                self.alt_id_vector,
                self.rows_to_obs,
                self.rows_to_alts,
                self.choice_vector,
                self.utility_transform,
                self.calc_dh_d_shape,
                self.calc_dh_dv,
                self.calc_dh_d_alpha,
                intercepts,
                shapes,
                self.ridge,
                self.weights]

        return cc.calc_fisher_info_matrix(*args)
--------------------
Prompt: 
def streamnet(np, filleddem, flowdir, acc, streamRaster, modifiedOutlet,
                  streamOrder, chNetwork, chCoord, streamNet, subbasin, workingdir=None,
                  mpiexedir=None, exedir=None, log_file=None, runtime_file=None, hostfile=None):
        """Run streamnet"""
----------
Output: 

        self.log.info("Running streamnet")
        self.log.info("streamRaster: %s acc: %s" % (log_file, str(acc)))
        self.log.info("flowdir: %s flowdir: %s" % (mpiexedir, str(flowdir)))
        self.log.info("acc: %s" % str(acc))
        self.log.info("streamRaster: %s" % str(streamRaster))
        self.log.info("modifiedOutlet: %s" % str(modifiedOutlet))
        self.log
----------
Solution: 

        fname = TauDEM.func_name('streamnet')
        return TauDEM.run(FileClass.get_executable_fullpath(fname, exedir),
                          {'-fel': filleddem, '-p': flowdir, '-ad8': acc, '-src': streamRaster,
                           '-o': modifiedOutlet}, workingdir,
                          None,
                          {'-ord': streamOrder, '-tree': chNetwork, '-coord': chCoord,
                           '-net': streamNet, '-w': subbasin},
                          {'mpipath': mpiexedir, 'hostfile': hostfile, 'n': np},
                          {'logfile': log_file, 'runtimefile': runtime_file})
--------------------
Prompt: 
def can_delete_asset_contents(self, asset_id=None):
        """Tests if this user can delete ``AssetsContent`` from ``Assets``.

        A return of true does not guarantee successful authorization. A
        return of false indicates that it is known deleting an
        ``AssetContent`` will result in a ``PermissionDenied``. This is
        intended as a hint to an application that may opt not to offer
        delete operations to an unauthorized user.

        :param asset_id: the ``Id`` of an ``Asset``
        :type asset_id: ``osid.id.Id``
        :return: ``false`` if ``AssetContent`` deletion is not authorized, ``true`` otherwise
        :rtype: ``boolean``
        :raise: ``NullArgument`` -- ``asset_id`` is ``null``

        *compliance: mandatory -- This method must be implemented.*

        """
----------
Output: 
 :return: ``true`` if ``asset_id`` is ``null``
        :rtype: ``boolean``


        *compliance: mandatory -- This method must be implemented.*

        """
        return  # boolean

    @utilities.arguments_not_none
    def delete_asset_contents(self, asset_id=None):
        """Deletes the contents of the ``AssetsContent``.

        :param asset_id: the ``Id`` of an ``Asset``
        :type asset_id: ``osid.id.Id``
        :return: ``true`` if ``asset_id`` is ``null``
----------
Solution: 

        url_path = construct_url('authorization',
                                 bank_id=self._catalog_idstr)
        return self._get_request(url_path)['assetHints']['canDelete']
--------------------
Prompt: 
def insert(self, point, number, axis):
        """Inserts number of rows/cols/tabs into selection at point on axis
        Parameters
        ----------

        point: Integer
        \tAt this point the rows/cols are inserted or deleted
        number: Integer
        \tNumber of rows/cols to be inserted, negative number deletes
        axis: Integer in 0, 1
        \tDefines whether rows or cols are affected

        """
----------
Output: 

        if axis == 0:
            self.axis = 1
        elif axis == 1:
            self.axis = 0
        else:
            raise ValueError("axis must be 0 or 1")
        self.row_index = number
        self.row_count = number
        self.selected_row = self.row_index - 1
        self.selected_col = self.selected_row + number
        self.axis = axis
        self.selected_row = self.selected_col - 1
        self.selected_col = self.selected_row + number
        self.row_index = self.row_count
----------
Solution: 


        def build_tuple_list(source_list, point, number, axis):
            
--------------------
Prompt: 
def sample_cc(self, nsamples=1, weighted=True):
        """
        Returns a random polygon of any class. The probability of each polygon to be sampled
        is proportional to its area if weighted is True.
        """
----------
Output: 

        if weighted:
            return self.random_polygon(nsamples)
        else:
            return self.random_polygon(nsamples, True)

    def random_polygon(self, nsamples=1, weighted=True):
        """
        Returns a random polygon of any class. The probability of each polygon to be
        sampled is proportional to its area if weighted is True.
        """
        if weighted:
            return self.random_polygon_weighted(nsamples)
        else:
            return self.random_polygon_weighted(nsamples, True)

    def random_polygon_weighted(self, nsamples
----------
Solution: 

        weights = self.areas / np.sum(self.areas) if weighted else None
        for index in np.random.choice(a=len(self.geometries), size=nsamples, p=weights):
            yield self.geometries[index]
--------------------
Prompt: 
def set_temp_url_key(self, key=None):
        """
        Sets the key for the Temporary URL for the account. It should be a key
        that is secret to the owner.

        If no key is provided, a UUID value will be generated and used. It can
        later be obtained by calling get_temp_url_key().
        """
----------
Output: 

        if key is None:
            key = uuid.uuid4().hex
        if not key:
            key = str(uuid.uuid4())
        self.temp_url_key = key

    def get_temp_url_key(self):
        """
        Returns the key for the temporary URL.
        """
        return self.temp_url_key

    def get_id(self):
        """
        Returns the ID for the user.
        """
        return self.id

    def get_name(self):
        """
        Returns the name for the user.
        """
        return self.name

    def get_
----------
Solution: 

        if key is None:
            key = uuid.uuid4().hex
        meta = {"Temp-Url-Key": key}
        self.set_account_metadata(meta)
        self._cached_temp_url_key = key
--------------------
Prompt: 
def get_username(sciper):
    """
    Return username of user
    """
----------
Output: 

    return sciper.get_user_name()

def get_password(sciper):
    """
    Return password of user
    """
    return sciper.get_user_password()

def get_user_id(sciper, username):
    """
    Return user id
    """
    return sciper.get_user_id(username, sciper.get_password())

def get_user_name(sciper, username):
    """
    Return username of user
    """
    return sciper.get_user_name()

def get_user_password(sciper
----------
Solution: 

    attribute = 'uid'
    response = LDAP_search(
        pattern_search='(uniqueIdentifier={})'.format(sciper),
        attribute=attribute
    )
    try:
        username = get_attribute(response, attribute)
    except Exception:
        raise EpflLdapException("No username corresponds to sciper {}".format(sciper))
    return username
--------------------
Prompt: 
def delay(name, args, kwargs):
    """Run a job asynchronously"""
----------
Output: 

    return _job_delay(name, args, kwargs, False)


def _job_delay(name, args, kwargs, delay=0):
    """
    Helper function to run a job asynchronously
    """
    if delay > 0:
        time.sleep(delay)
    return job(name, args, kwargs, False)


def _get_job_status(job_id):
    """
    Helper function to get the status of a job
    """
    return _get_job_info(job_id)['status']


def _get_job_info(job_id):
   
----------
Solution: 

    args = args or []
    kwargs = dict(k.split() for k in kwargs) if kwargs else {}
    if name not in celery.tasks:
        log.error('Job %s not found', name)
    job = celery.tasks[name]
    log.info('Sending job %s', name)
    async_result = job.delay(*args, **kwargs)
    log.info('Job %s sended to workers', async_result.id)
--------------------
Prompt: 
def _abort_workflow(pb: ProcessingBlock, workflow_stage_dict: dict,
                    docker: DockerSwarmClient):
    """Abort the workflow.

    TODO(BMo): This function currently does nothing as the abort flag
        is hardcoded to False!

    This function is used by `execute_processing_block`.

    Args:
        pb (ProcessingBlock): Configuration database Processing block object.
        workflow_stage_dict (dict): Workflow stage metadata dictionary.
        docker (DockerClient): Docker Swarm Client object.

    Returns:
        bool, True if the stage is aborted, otherwise False.

    """
----------
Output: 

    try:
        pb.abort()
    except Exception as e:
        _logger.error("Aborting workflow: %s", e)
        return False
    return True


def execute_processing_block(workflow_id: str, pb: ProcessingBlock,
                               docker: DockerSwarmClient) -> bool:
    """Execute the processing block.

    Args:
        workflow_id (str): Workflow ID.
        pb (pb.ProcessBlock): Configuration object.
        docker (docker.DockerClient): Docker client object.

    Returns:
        bool, True if the process block was successfully executed, otherwise False.

   
----------
Solution: 

    # TODO(BMo) Ask the database if the abort flag on the PB is set.
    _abort_flag = False
    if _abort_flag:
        for workflow_stage in pb.workflow_stages:
            for service_id, _ in \
                    workflow_stage_dict[workflow_stage.id]['services'].items():
                docker.delete_service(service_id)
                LOG.info("Deleted Service Id %s", service_id)
        return True
    return False
--------------------
Prompt: 
def plot(self, data):
    """ Plots a featire slice view on given data.

    Args:
      data: Can be one of:
          A string of sql query.
          A sql query module defined by "%%sql --module module_name".
          A pandas DataFrame.
        Regardless of data type, it must include the following columns:
          "feature": identifies a slice of features. For example: "petal_length:4.0-4.2".
          "count": number of instances in that slice of features.
        All other columns are viewed as metrics for its feature slice. At least one is required.
    """
----------
Output: 
.

    Returns:
      None
    """
    if isinstance(data, str):
      return self.plot_sql(data)
    elif isinstance(data, pd.DataFrame):
      return self.plot_sql(data.to_sql())
    else:
      raise ValueError("Can only plot string or pandas DataFrame")

  def plot_sql(self, sql):
    """ Plots a SQL query view on given data.

    Args:
      sql: Can be one of:
          A string of sql query.
          A sql query module defined by "%%sql --module module_name".
          A pandas DataFrame.

    Returns
----------
Solution: 

    import IPython

    if ((sys.version_info.major > 2 and isinstance(data, str)) or
       (sys.version_info.major <= 2 and isinstance(data, basestring))):
      data = bq.Query(data)

    if isinstance(data, bq.Query):
      df = data.execute().result().to_dataframe()
      data = self._get_lantern_format(df)
    elif isinstance(data, pd.core.frame.DataFrame):
      data = self._get_lantern_format(data)
    else:
      raise Exception('data needs to be a sql query, or a pandas DataFrame.')

    HTML_TEMPLATE = 
--------------------
Prompt: 
def is_alert_present(self):
        """Tests if an alert is present

        @return: True if alert is present, False otherwise
        """
----------
Output: 

        return self.browser.find_element_by_id('alert-success')

    def is_alert_present_by_css(self, css_selector):
        """Tests if an alert is present by CSS selector

        @param css_selector: CSS selector
        @return: True if alert is present, False otherwise
        """
        return self.browser.find_element_by_css_selector(css_selector)

    def is_alert_present_by_xpath(self, xpath):
        """Tests if an alert is present by XPath

        @param xpath: XPath
        @return:
----------
Solution: 

        current_frame = None
        try:
            current_frame = self.driver.current_window_handle
            a = self.driver.switch_to_alert()
            a.text
        except NoAlertPresentException:
            # No alert
            return False
        except UnexpectedAlertPresentException:
            # Alert exists
            return True
        finally:
            if current_frame:
                self.driver.switch_to_window(current_frame)
        return True
--------------------
Prompt: 
def _condense(self, data):
    """
    Condense by adding together all of the lists.
    """
----------
Output: 

    if self.sparse_keys is not None:
      if not isinstance(data, sparse_tensor.SparseTensor):
        raise TypeError("Expected SparseTensor, got %s" % type(data))
      self.sparse_keys.extend(data.indices)
      self.sparse_keys = self.sparse_keys + data.values.tolist()

    return data

  def _to_dnn_input_layer(self,
                          input_tensor,
                          weight_collections=None,
                          trainable=True,
                          output_rank=2):
    if weight_collections and not trainable:
      raise ValueError("weight_
----------
Solution: 

    rval = {}
    for resolution,histogram in data.items():
      for value,count in histogram.items():
        rval[ value ] = count + rval.get(value,0)
    return rval
--------------------
Prompt: 
def transmit_ack_bpdu(self):
        """ Send Topology Change Ack BPDU. """
----------
Output: 

        return self.send_pdu(self.PDU_BPDU_CHANGE_BPDU, self.PDU_BPDU_CHANGE_BPDU,
                             self.PDU_BPDU_CHANGE_BPDU,
                             self.PDU_BPDU_CHANGE_BPDU)

    def send_pdu(self, pdu_pdu):
        """ Send PDU. """
        if self.debug:
            print("Sending PDU: %s" % pdu_pdu)

        if pdu_pdu.pdu_type == PDU_TYPE_BPDU:
            self.write_b(PDU_
----------
Solution: 

        ack_flags = 0b10000001
        bpdu_data = self._generate_config_bpdu(ack_flags)
        self.ofctl.send_packet_out(self.ofport.port_no, bpdu_data)
--------------------
Prompt: 
def template(basedir, text, vars, lookup_fatal=True, expand_lists=False):
    """ run a text buffer through the templating engine until it no longer changes """
----------
Output: 

    global _thread_id
    _thread_id += 1

    try:
        res = None
        while not res:
            res = _read_thread_template(basedir, text, vars)
            if not res:
                time.sleep(0.01)
            else:
                break
    finally:
        if _thread_id > 0:
            _thread.exit()

    return res


def _read_thread_template(basedir, text, vars):
    (fd, path) = tempfile.mkstemp(dir=basedir)
    f = os.fdopen(fd, 'w')
    try:
       
----------
Solution: 


    try:
        text = text.decode('utf-8')
    except UnicodeEncodeError:
        pass # already unicode
    text = varReplace(basedir, unicode(text), vars, lookup_fatal=lookup_fatal, expand_lists=expand_lists)
    return text
--------------------
Prompt: 
def QA_fetch_stock_basic_info_tushare(collections=DATABASE.stock_info_tushare):
    """
    purpose:
        tushare 股票列表数据库

        code,代码
        name,名称
        industry,所属行业
        area,地区
        pe,市盈率
        outstanding,流通股本(亿)
        totals,总股本(亿)
        totalAssets,总资产(万)
        liquidAssets,流动资产
        fixedAssets,固定资产
        reserved,公积金
        reservedPerShare,每股公积金
        esp,每股收益
        bvps,每股净资
        pb,市净率
        timeToMarket,上市日期
        undp,未分利润
        perundp, 每股未分配
        rev,收入同比(%)
        profit,利润同比(%)
        gpr,毛利率(%)
        npr,净利润率(%)
        holders,股东人数

        add by tauruswang,

    :param collections: stock_info_tushare 集合
    :return:
    """
----------
Output: 
abilities,所属资产
        liabilityAssets,所属资产
        liabilityAssetsAssets,所属资产
        liabilityAssetsAssets,所属资产
        liabilityAssetsAssets,所属资产
        liabilityAssetsAssets,所属资产
        liabilityAssetsAssets,所属资产
        liabilityAssetsAssets,所属资产
        liabilityAssetsAssets,所属资产
        liabilityAssetsAssets,所�
----------
Solution: 

    '获取股票基本信息'
    items = [item for item in collections.find()]
    # 🛠todo  转变成 dataframe 类型数据
    return items
--------------------
Prompt: 
def cudnnCreate():
    """
    Initialize cuDNN.

    Initializes cuDNN and returns a handle to the cuDNN context.

    Returns
    -------

    handle : cudnnHandle
        cuDNN context
    """
----------
Output: 

    return cudnn.cudnnCreate()


def cudnnDestroy():
    """
    Destroy cuDNN.

    Destroys a previously created cuDNN context.

    Returns
    -------

    handle : cudnnHandle
        cuDNN context
    """
    cudnn.cudnnDestroy()


def cudnnCreate_tensor(handle, name, dataType, size, data,
                       init=None,
                       dataTypeDesc=None,
                       dataType=mx_real_t,
                       initDesc=None,
                       dataType_grad=None,
                       dataType_grad_grad=None,
                       infer
----------
Solution: 


    handle = ctypes.c_void_p()
    status = _libcudnn.cudnnCreate(ctypes.byref(handle))
    cudnnCheckStatus(status)
    return handle.value
--------------------
Prompt: 
def get_next_sibling_tags(mention):
    """Return the HTML tag of the Mention's next siblings.

    Next siblings are Mentions which are at the same level in the HTML tree as
    the given mention, but are declared after the given mention.
    If a candidate is passed in, only the next siblings of its last Mention
    are considered in the calculation.

    :param mention: The Mention to evaluate
    :rtype: list of strings
    """
----------
Output: 

    if mention.is_root:
        return [root.tag for root in mention.root.children]
    else:
        return [mention.tag]


def get_mention_html(mention):
    """Return the HTML of the Mention's root node.

    :param mention: The Mention to evaluate
    :rtype: list of strings
    """
    return [
        root.tag for root in get_mentions(mention)
        if root.tag.name == mention.tag.name
    ][0]


def get_mentions(mention):
    """Return the list of
----------
Solution: 

    span = _to_span(mention)
    next_sibling_tags = []
    i = _get_node(span.sentence)
    while i.getnext() is not None:
        next_sibling_tags.append(str(i.getnext().tag))
        i = i.getnext()
    return next_sibling_tags
--------------------
Prompt: 
def bygroups(*args):
    """
    Callback that yields multiple actions for each group in the match.
    """
----------
Output: 

    def wrap(func):
        def wrapped(*args):
            for group in args:
                for action in group:
                    yield action
            yield ''
        return func(*args)
    return wrapped

def _get_default_actions():
    """
    Return a list of actions to be used in all re.match actions.
    """
    return [
        ('group', [
            (re.compile(r'^%s$' % x),
            (re.compile(r'^%s$' % x),
            _group_actions,
            dict(
                (x, getattr(re, x)) for x in dir
----------
Solution: 

    def callback(lexer, match, ctx=None):
        for i, action in enumerate(args):
            if action is None:
                continue
            elif type(action) is _TokenType:
                data = match.group(i + 1)
                if data:
                    yield match.start(i + 1), action, data
            else:
                data = match.group(i + 1)
                if data is not None:
                    if ctx:
                        ctx.pos = match.start(i + 1)
                    for item in action(lexer,
                                       _PseudoMatch(match.start(i + 1), data), ctx):
                        if item:
                            yield item
        if ctx:
            ctx.pos = match.end()
    return callback
--------------------
Prompt: 
def save_excel(self, fd):
        """ Saves the case as an Excel spreadsheet.
        """
----------
Output: 

        self.workbook.save(fd)

    def save_csv(self, fd):
        """ Saves the case as a CSV file.
        """
        self.workbook.save(fd)

    def save_excel_sheet(self, fd):
        """ Saves the case as an Excel spreadsheet.
        """
        self.workbook.save_sheet(fd)

    def save_csv_sheet_name(self, fd):
        """ Saves the case as a CSV file.
        """
        self.workbook.save_sheet_name(fd, self.csv_sheet_name)

    def save
----------
Solution: 

        from pylon.io.excel import ExcelWriter
        ExcelWriter(self).write(fd)
--------------------
Prompt: 
def get_policies_by_id(profile_manager, policy_ids):
    """
    Returns a list of policies with the specified ids.

    profile_manager
        Reference to the profile manager.

    policy_ids
        List of policy ids to retrieve.
    """
----------
Output: 

    return profile_manager.get_policies_by_ids(policy_ids)

  def get_policy_by_name(self, name):
    """
    Returns a reference to the specified policy.

    name
        Name of the policy.

    policy_name
        Name of the policy.

    policy_id
        Id of the policy.
    """
    return self.policies[name]

  def get_policy_by_id(self, policy_id):
    """
    Returns a reference to the specified policy.

    policy_id
        Id of the policy.
    """
    return self.policies_by_
----------
Solution: 

    try:
        return profile_manager.RetrieveContent(policy_ids)
    except vim.fault.NoPermission as exc:
        log.exception(exc)
        raise VMwareApiError('Not enough permissions. Required privilege: '
                             '{0}'.format(exc.privilegeId))
    except vim.fault.VimFault as exc:
        log.exception(exc)
        raise VMwareApiError(exc.msg)
    except vmodl.RuntimeFault as exc:
        log.exception(exc)
        raise VMwareRuntimeError(exc.msg)
--------------------
Prompt: 
async def send(self, data, id=None, event=None, retry=None):
        """Send data using EventSource protocol

        :param str data: The data field for the message.
        :param str id: The event ID to set the EventSource object's last
            event ID value to.
        :param str event: The event's type. If this is specified, an event will
            be dispatched on the browser to the listener for the specified
            event name; the web site would use addEventListener() to listen
            for named events. The default event type is "message".
        :param int retry: The reconnection time to use when attempting to send
            the event. [What code handles this?] This must be an integer,
            specifying the reconnection time in milliseconds. If a non-integer
            value is specified, the field is ignored.
        """
----------
Output: 
 retry: The number of seconds to wait before retrying.
            If this is specified, the function will return immediately
            if the browser does not support the retry argument.
        """
        if id is not None:
            data = await self.get_event_source(event, data)
        else:
            data = await self.get_event_source(retry)

        return await self.send_async(data, id=id, event=event)

    async def send_async(self, data, id=None, retry=None):
        """Send data using asynchronous protocol

        :param str data: The
----------
Solution: 

        buffer = io.StringIO()
        if id is not None:
            buffer.write(self.LINE_SEP_EXPR.sub('', 'id: {}'.format(id)))
            buffer.write(self._sep)

        if event is not None:
            buffer.write(self.LINE_SEP_EXPR.sub('', 'event: {}'.format(event)))
            buffer.write(self._sep)

        for chunk in self.LINE_SEP_EXPR.split(data):
            buffer.write('data: {}'.format(chunk))
            buffer.write(self._sep)

        if retry is not None:
            if not isinstance(retry, int):
                raise TypeError('retry argument must be int')
            buffer.write('retry: {}'.format(retry))
            buffer.write(self._sep)

        buffer.write(self._sep)
        await self.write(buffer.getvalue().encode('utf-8'))
--------------------
Prompt: 
def get_queryset(self):
        """
        Inspired by Glen Roberton's django-geojson-tiles view
        """
----------
Output: 

        return self.geo_query_set.filter(geom__isnull=False)

    def get_geom_type(self):
        """
        Returns the geometry type for the geometry column
        """
        return OGRGeomType(capi.geom_type(self.geom))

    def get_geom_name(self):
        "Returns the name of the geometry column for the geographic column."
        return capi.geom_name(self.geom)

    def get_geom_placeholder(self):
        """
        Returns the placeholder for the geometry column if a
        placeholder is provided.
        """
        if hasattr(self, 'placeholder
----------
Solution: 

        self.z, self.x, self.y = self._parse_args()
        nw = self.tile_coord(self.x, self.y, self.z)
        se = self.tile_coord(self.x + 1, self.y + 1, self.z)
        bbox = Polygon((nw, (se[0], nw[1]),
                       se, (nw[0], se[1]), nw))
        qs = super(TiledGeoJSONLayerView, self).get_queryset()
        qs = qs.filter(**{
            '%s__intersects' % self.geometry_field: bbox
        })
        self.bbox = bbox.extent

        # Simplification dict by zoom level
        simplifications = self.simplifications or {}
        z = self.z
        self.simplify = simplifications.get(z)
        while self.simplify is None and z < 32:
            z += 1
            self.simplify = simplifications.get(z)

        # Won't trim point geometries to a boundary
        model_field = qs.model._meta.get_field(self.geometry_field)
        self.trim_to_boundary = (self.trim_to_boundary and
                                 not isinstance(model_field, PointField) and
                                 Intersection is not None)
        if self.trim_to_boundary:
            if django.VERSION < (1, 9):
                qs = qs.intersection(bbox)
            else:
                qs = qs.annotate(intersection=Intersection(self.geometry_field, bbox))
            self.geometry_field = 'intersection'

        return qs
--------------------
Prompt: 
def publish_queue(self):
        """
        Publish all messages that have been added to the queue for configured protocol
        :return: None
        """
----------
Output: 

        if self.protocol_id:
            self.protocol.publish(self.queue_id, self.protocol_id, self.message_id)
        else:
            self.protocol.publish(self.queue_id, self.protocol_id, self.message_id)

    def get_messages(self):
        """
        Get all messages from the queue
        :return: list of messages
        """
        return self.queue.get_messages()

    def get_message_count(self):
        """
        Get the number of messages in the queue
        :return: number of messages in the queue
----------
Solution: 

        self.last_send_time = time.time()
        try:
            self._tx_queue_lock.acquire()
            start_length = len(self._rx_queue)
            publish_amount = len(self._tx_queue)
            if self.config.protocol == PublisherConfig.Protocol.GRPC:
                self._publish_queue_grpc()
            else:
                self._publish_queue_wss()
            self._tx_queue = []
        finally:
            self._tx_queue_lock.release()

        if self.config.publish_type == self.config.Type.SYNC:
            start_time = time.time()
            while time.time() - start_time < self.config.sync_timeout and \
                                    len(self._rx_queue) - start_length < publish_amount:
                pass
            return self._rx_queue
--------------------
Prompt: 
def start_engine(self):
        """
        Start the child processes (one per device OS)
        """
----------
Output: 

        self.log.info("Starting engine")
        self.start_time = time.time()
        self.log.debug("Initializing the device")
        self.init_device()
        self.log.debug("Initializing the device")
        self.init_device_time = time.time() - self.start_time
        self.log.debug("Initializing the device time")
        self.init_device_time = time.time() - self.start_time
        self.log.debug("Initializing the device")
        self.init_device_time = time.time()
----------
Solution: 

        if self.disable_security is True:
            log.warning('***Not starting the authenticator process due to disable_security being set to True***')
        else:
            log.debug('Generating the private key')
            self.__priv_key = nacl.utils.random(nacl.secret.SecretBox.KEY_SIZE)
            log.debug('Generating the signing key')
            self.__signing_key = nacl.signing.SigningKey.generate()
            # start the keepalive thread for the auth sub-process
            self._processes.append(self._start_auth_proc())
        log.debug('Starting the internal proxy')
        proc = self._start_pub_px_proc()
        self._processes.append(proc)
        # publisher process start
        pub_id = 0
        for pub in self.publisher:
            publisher_type, publisher_opts = list(pub.items())[0]
            proc = self._start_pub_proc(publisher_type,
                                        publisher_opts,
                                        pub_id)
            self._processes.append(proc)
            pub_id += 1
        # device process start
        log.info('Starting child processes for each device type')
        started_os_proc = []
        for device_os, device_config in self.config_dict.items():
            if not self._whitelist_blacklist(device_os):
                log.debug('Not starting process for %s (whitelist-blacklist logic)', device_os)
                # Ignore devices that are not in the whitelist (if defined),
                #   or those operating systems that are on the blacklist.
                # This way we can prevent starting unwanted sub-processes.
                continue
            log.debug('Will start %d worker process(es) for %s', self.device_worker_processes, device_os)
            for proc_index in range(self.device_worker_processes):
                self._processes.append(self._start_dev_proc(device_os,
                                                            device_config))
            started_os_proc.append(device_os)
        # start the server process
        self._processes.append(self._start_srv_proc(started_os_proc))
        # start listener process
        for lst in self.listener:
            listener_type, listener_opts = list(lst.items())[0]
            proc = self._start_lst_proc(listener_type,
                                        listener_opts)
            self._processes.append(proc)
        thread = threading.Thread(target=self._check_children)
        thread.start()
--------------------
Prompt: 
def du(self, paths, include_toplevel=False, include_children=True):
        """Returns size information for paths

        :param paths: Paths to du
        :type paths: list
        :param include_toplevel: Include the given path in the result. If the path is a file, include_toplevel is always True.
        :type include_toplevel: boolean
        :param include_children: Include child nodes in the result.
        :type include_children: boolean
        :returns: a generator that yields dictionaries

        **Examples:**

        Children:

        >>> list(client.du(['/']))
        [{'path': '/Makefile', 'length': 6783L}, {'path': '/build', 'length': 244778L}, {'path': '/index.asciidoc', 'length': 100L}, {'path': '/source', 'length': 8524L}]

        Directory only:

        >>> list(client.du(['/'], include_toplevel=True, include_children=False))
        [{'path': '/', 'length': 260185L}]

        """
----------
Output: 
size': 0, 'type': 'file', 'name': 'du','size_human': 0},
         {'size': 1, 'type': 'file', 'name': 'file','size_human': 1},
         {'size': 0, 'type': 'folder', 'name': 'du','size_human': 0},
         {'size': 0, 'type': 'folder', 'name': 'file','size_human': 1},
         {'size': 0, 'type': 'folder', 'name': 'du','size_human': 0}]

        """
        for path
----------
Solution: 

        if not isinstance(paths, list):
            raise InvalidInputException("Paths should be a list")
        if not paths:
            raise InvalidInputException("du: no path given")

        processor = lambda path, node: self._handle_du(path, node)
        for item in self._find_items(paths, processor, include_toplevel=include_toplevel,
                                     include_children=include_children, recurse=False):
            if item:
                yield item
--------------------
Prompt: 
def new_closure(vals):
    """
    Build a new closure
    """
----------
Output: 

    return ast.FunctionDef(
        name=vals.name,
        args=vals.args,
        body=vals.body,
        returns=vals.returns,
        args_types=vals.args_types,
        returns_types=vals.returns_types,
        type_names=type_names(vals.type),
        type_var_names=type_var_names(vals.type),
        var_names=var_names(vals.type),
        body_type_var_names=body_type_var_names(vals.type),
        returns_type_var_
----------
Solution: 

    args = ','.join('x%i' % i for i in range(len(vals)))
    f = eval("lambda %s:lambda:(%s)" % (args, args))
    if sys.version_info[0] >= 3:
        return f(*vals).__closure__
    return f(*vals).func_closure
--------------------
Prompt: 
def export(self, name, columns, points):
        """Write the points in Riemann."""
----------
Output: 

        self.write_points(name, points, columns)
        self.write_points(name, points, columns)
        self.write_points(name, points, columns)
        self.write_points(name, points, columns)
        self.write_points(name, points, columns)
        self.write_points(name, points, columns)
        self.write_points(name, points, columns)
        self.write_points(name, points, columns)
        self.write_points(name, points, columns)
        self.write_points(name, points,
----------
Solution: 

        for i in range(len(columns)):
            if not isinstance(points[i], Number):
                continue
            else:
                data = {'host': self.hostname, 'service': name + " " + columns[i], 'metric': points[i]}
                logger.debug(data)
                try:
                    self.client.send(data)
                except Exception as e:
                    logger.error("Cannot export stats to Riemann (%s)" % e)
--------------------
Prompt: 
def launch_notebook(request, username, notebook_context):
    """Renders a IPython Notebook frame wrapper.

    Starts or reattachs ot an existing Notebook session.
    """
----------
Output: 

    session = notebook_context.session
    ot = session.get_notebook(username)
    if ot is None:
        return HttpResponseBadRequest("No such notebook: %s" % username)
    session.set_notebook(ot)
    session.save()
    return HttpResponse("Session saved")


def _get_session_data(request, username, session_id):
    """Returns the session data for a given session.

    This is used by the Backbone application to display the session info.
    """
    notebook_context = _get_notebook_context(request)
    session = notebook_
----------
Solution: 

    # The notebook manage now tries too hard to get the port allocated for the notebook user, making it slow
    # TODO: Manage a proper state e.g. using Redis
    notebook_info = launch_on_demand(request, username, notebook_context)

    # Jump to the detault notebook
    proxy_route = request.route_url("notebook_proxy", remainder="notebooks/{}".format(notebook_info["notebook_name"]))
    proxy_route = route_to_alt_domain(request, proxy_route)

    return HTTPFound(proxy_route)
--------------------
Prompt: 
def get_cel_to_gal_angle(skydir):
    """Calculate the rotation angle in radians between the longitude
    axes of a local projection in celestial and galactic coordinates.

    Parameters
    ----------
    skydir : `~astropy.coordinates.SkyCoord`
        Direction of projection center.

    Returns
    -------
    angle : float
        Rotation angle in radians.
    """
----------
Output: 

    return skydir.transform_to(Galactic).transform_to(Galactic).get_g_tran()


def get_cel_to_skydir(x, y, skydir):
    """Calculate the sky coordinates of the local axes of a celestial
    image.

    Parameters
    ----------
    x, y : int
        Coordinates of the center of the local axes.
    x, y : int
        Coordinates of the center of the axes.
    """
    return skydir.transform_to(SkyCoord).transform_to(SkyCoord
----------
Solution: 

    wcs0 = create_wcs(skydir, coordsys='CEL')
    wcs1 = create_wcs(skydir, coordsys='GAL')
    x, y = SkyCoord.to_pixel(SkyCoord.from_pixel(1.0, 0.0, wcs0), wcs1)
    return np.arctan2(y, x)
--------------------
Prompt: 
def handler(key_file=None, cert_file=None, timeout=None):
    """This class returns an instance of the default HTTP request handler using
    the values you provide.

    :param `key_file`: A path to a PEM (Privacy Enhanced Mail) formatted file containing your private key (optional).
    :type key_file: ``string``
    :param `cert_file`: A path to a PEM (Privacy Enhanced Mail) formatted file containing a certificate chain file (optional).
    :type cert_file: ``string``
    :param `timeout`: The request time-out period, in seconds (optional).
    :type timeout: ``integer`` or "None"
    """
----------
Output: 
 timeout in seconds.
    :type timeout: ``integer``
    :return: A default HTTP request handler.
    :rtype: :class:`httplib.HTTPConnection`
    """
    def __init__(self, key_file=None, cert_file=None, strict=True):
        """
        This class can be used to define a default request handler.

        :param `key_file`: A path to a PEM (Privacy Enhanced Mail) formatted file containing your private key (optional).
        :type key_file: ``string``
        :param `cert_file`: A path to a PEM (Pri
----------
Solution: 


    def connect(scheme, host, port):
        kwargs = {}
        if timeout is not None: kwargs['timeout'] = timeout
        if scheme == "http":
            return httplib.HTTPConnection(host, port, **kwargs)
        if scheme == "https":
            if key_file is not None: kwargs['key_file'] = key_file
            if cert_file is not None: kwargs['cert_file'] = cert_file
            return httplib.HTTPSConnection(host, port, **kwargs)
        raise ValueError("unsupported scheme: %s" % scheme)

    def request(url, message, **kwargs):
        scheme, host, port, path = _spliturl(url)
        body = message.get("body", "")
        head = {
            "Content-Length": str(len(body)),
            "Host": host,
            "User-Agent": "splunk-sdk-python/0.1",
            "Accept": "*/*",
        } # defaults
        for key, value in message["headers"]:
            head[key] = value
        method = message.get("method", "GET")

        connection = connect(scheme, host, port)
        try:
            connection.request(method, path, body, head)
            if timeout is not None:
                connection.sock.settimeout(timeout)
            response = connection.getresponse()
        finally:
            connection.close()

        return {
            "status": response.status,
            "reason": response.reason,
            "headers": response.getheaders(),
            "body": ResponseReader(response),
        }

    return request
--------------------
Prompt: 
def getScoringVector(self, profile):
        """
        Returns the scoring vector. This function is called by getCandScoresMap().

        :ivar Profile profile: A Profile object that represents an election profile.
        """
----------
Output: 

        return self._getCandScoresMap(profile, self._scoringVector)

    def getCandScores(self, profile):
        """
        Returns the election's scores. This function is called by getCandScoresMap().

        :ivar Profile profile: A Profile object that represents the election profile.
        """
        return self._getCandScoresMap(profile, self._candScores)

    def _getCandScoresMap(self, profile, scoresMap):
        """
        Returns the election's scores map.

        :param Profile profile: The election profile.
        :param dict scores
----------
Solution: 


        # Check to make sure that the scoring vector contains a score for every possible rank in a
        # ranking.
        if len(self.scoringVector) != profile.numCands:
            print("ERROR: scoring vector is not the correct length")
            exit()

        return self.scoringVector
--------------------
Prompt: 
def failed(self, reason=None):
        """
        Set the test status to Report.FAILED, and set the failure reason

        :param reason: failure reason (default: None)
        """
----------
Output: 

        if reason is None:
            reason = self.report_failures
        self.report_failures = 0
        self.report_errors = 0
        self.report_errors_no_data = 0
        self.report_errors_no_data_no_data = 0
        self.report_errors_data_no_data = 0
        self.report_errors_data_no_data_no_data_no_data = 0
        self.report_errors_data_no_data_no_data_no_data = 0
        self.report_errors_data_no_data
----------
Solution: 

        self.set_status(Report.FAILED)
        if reason:
            self.add('reason', reason)
--------------------
Prompt: 
def _set_interface_type(self, v, load=False):
    """
    Setter method for interface_type, mapped from YANG variable /brocade_interface_ext_rpc/get_interface_detail/input/interface_type (enumeration)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_interface_type is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_interface_type() directly.

    YANG Description: The type of the interface. An 'unknown' type 
represents error scenario and should not be used.
    """
----------
Output: 
 type indicates
that the peer is not present.
    """
    def _set_interface_type(self, v, load=False):
        """
    Setter method for interface_type, mapped from YANG variable /brocade_interface_ext_rpc/get_interface_detail/input/interface_type (enumeration)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_interface_type is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_interface_type() directly.

   
----------
Solution: 

    if hasattr(v, "_utype"):
      v = v._utype(v)
    try:
      t = YANGDynClass(v,base=RestrictedClassType(base_type=unicode,                                     restriction_type="dict_key",                                     restriction_arg={u'port-channel': {'value': 5}, u'loopback': {'value': 7}, u'fortygigabitethernet': {'value': 4}, u'unknown': {'value': 1}, u'gigabitethernet': {'value': 2}, u'tengigabitethernet': {'value': 3}, u'hundredgigabitethernet': {'value': 9}, u'fibrechannel': {'value': 8}, u'l2vlan': {'value': 6}},), is_leaf=True, yang_name="interface-type", rest_name="interface-type", parent=self, choice=(u'request-type', u'get-request'), path_helper=self._path_helper, extmethods=self._extmethods, register_paths=False, extensions={u'tailf-common': {u'info': u"The type of the interface. An 'unknown' type \nrepresents error scenario and should not be used."}}, namespace='urn:brocade.com:mgmt:brocade-interface-ext', defining_module='brocade-interface-ext', yang_type='enumeration', is_config=True)
    except (TypeError, ValueError):
      raise ValueError({
          'error-string': 
--------------------
Prompt: 
def setDevice(self, device): 
        """Sets the video stream
        
        :param device:      A rather generic device class.  In this case DataModel.RTSPCameraDevice.
        """
----------
Output: 

        self.device = device
        self.stream = device.stream
        self.video = device.video
        self.video.set_stream(self.stream)
        self.video.set_frame_rate(self.frame_rate)
        self.video.set_format(self.format)
        self.video.set_frame_size(self.frame_size)
        self.video.set_frame_rate(self.frame_rate)
        self.video.set_format(self.format)
        self.video.set_frame_rate(self.frame_rate
----------
Solution: 

        print(self.pre, "setDevice :", device)
        
        if (not device and not self.device): # None can be passed as an argument when the device has not been set yet
            return
            
        if (self.device):
            if self.device == device:
                print(self.pre, "setDevice : same device")
                return
            
        if self.filterchain: # there's video already
            self.clearDevice()
        
        self.device = device
        self.video.setDevice(self.device) # inform the video widget so it can start drags
        
        # ManagedFilterChain.addViewPort accepts ViewPort instance
        self.filterchain = self.filterchain_group.get(_id = self.device._id)
        
        if self.filterchain:
            self.viewport.setXScreenNum(self.n_xscreen)
            self.viewport.setWindowId  (int(self.video.winId()))
            self.filterchain.addViewPort(self.viewport)
--------------------
Prompt: 
def get_groups_from_category(self, category) -> typing.Iterator['Group']:
        """
        Args:
            category: group category

        Returns: generator over all groups from a specific category in this coalition
        """
----------
Output: 

        for group in self.categories:
            if group.category == category:
                yield from group.groups

    def get_coalition_groups(self, category) -> typing.Iterator['Group']:
        """
        Args:
            category: group category

        Yields:
            group

        Raises:
            KeyError: if the category is not found in this coalition
        """
        for coalition in self.get_groups_from_category(category):
            yield from coalition.get_coalitions()

    def get_coalition_group(self, group_id: int) ->
----------
Solution: 

        Mission.validator_group_category.validate(category, 'get_groups_from_category')
        for group in self.groups:

            if group.group_category == category:
                yield group
--------------------
Prompt: 
def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):
        """
        See documentation for method `GroundShakingIntensityModel` in
        :class:~`openquake.hazardlib.gsim.base.GSIM`
        """
----------
Output: 

        return self._get_mean_and_stddevs_for_sites(sites, rup, dists, imt,
                                                  stddev_types)

    def _get_mean_and_stddevs_for_sites(self, sites, rup, dists, imt, stddev_types):
        """
        Compute the mean and stddevs for the given sites.

        :param sites:
            Number of sites.
        :param rup:
            Rupture parameters.
        :param dists:
            Distances parameters.
        :param imt:
            IMT parameters.
        :param stddev_types:
----------
Solution: 

        # This is just used for testing purposes
        if len(stddev_types) == 0:
            stddev_types = [StdDev.TOTAL]
        mean, stds = self._get_mean_and_stddevs(sites, rup, dists, imt,
                                                stddev_types)
        stddevs = [np.ones(len(dists.repi))*get_sigma(imt)]
        delta = self._get_delta(stds, dists)
        mean = mean + stds + delta
        mean = np.squeeze(mean)
        return mean, stddevs
--------------------
Prompt: 
def stderr_file(self):
        """Filename to save kernel stderr output."""
----------
Output: 

        return self._stderr_file

    @property
    def log(self):
        """Log file to use for logging."""
        return self._log

    @log.setter
    def log(self, value):
        """Set the log file to use for logging."""
        self._log = value

    @property
    def log_level(self):
        """Log level to use for logging."""
        return self._log_level

    @log_level.setter
    def log_level(self, value):
        """Set the log level to use for logging."""
        self._log_level = value

    @property
    def
----------
Solution: 

        stderr_file = None
        if self.connection_file is not None:
            stderr_file = self.kernel_id + '.stderr'
            if self.stderr_dir is not None:
                stderr_file = osp.join(self.stderr_dir, stderr_file)
            else:
                try:
                    stderr_file = osp.join(get_temp_dir(), stderr_file)
                except (IOError, OSError):
                    stderr_file = None
        return stderr_file
--------------------
Prompt: 
def mk_message(org, user, key):
    """
    Make message
    """
----------
Output: 

    return Message(org=org, user=user, key=key)


def mk_message_from_file(filename):
    """
    Make message from file
    """
    return Message.from_file(filename)


def mk_org_file(org, filename):
    """
    Make org file
    """
    return OrgFile(org=org, filename=filename)


def mk_user_file(user, filename):
    """
    Make user file
    """
    return UserFile(user=user, filename=filename)


def mk_key_file(org, filename
----------
Solution: 


    m = Message()
    m['client'] = "taskc-py {0}".format(__version__)
    m['protocol'] = "v1"
    m['org'] = org
    m['user'] = user
    m['key'] = key

    return m
--------------------
Prompt: 
def _prepare_headers(self, additional_headers=None, **kwargs):
        """Prepare headers for http communication.

        Return dict of header to be used in requests.

        Args:
            .. versionadded:: 0.3.2
                **additional_headers**: (optional) Additional headers
                to be used with request

        Returns:
            Headers dict. Key and values are string

        """
----------
Output: 

        headers = {}
        if additional_headers:
            headers.update(additional_headers)
        if self.user_agent:
            headers.update({
                'User-Agent': self.user_agent,
            })
        if self.request_headers:
            headers.update(self.request_headers)
        if self.request_params:
            params = self.request_params.copy()
            params.update(kwargs)
            headers.update({
                'Content-Type': 'application/x-www-form-urlencoded',
                'User-Agent': self.user_agent,
                'Content
----------
Solution: 

        user_agent = "pyseaweed/{version}".format(version=__version__)
        headers = {"User-Agent": user_agent}
        if additional_headers is not None:
            headers.update(additional_headers)
        return headers
--------------------
Prompt: 
def enzyme(self, ec_number=None, hgnc_symbol=None, hgnc_identifier=None, limit=None, as_df=False):
        """Method to query :class:`.models.Enzyme` objects in database

        :param ec_number: Enzyme Commission number (EC number)(s)
        :type ec_number: str or tuple(str) or None

        :param hgnc_symbol: HGNC symbol(s)
        :type hgnc_symbol: str or tuple(str) or None

        :param hgnc_identifier: identifiers(s) in :class:`.models.HGNC`
        :type hgnc_identifier: int or tuple(int) or None

        :param limit:
            - if `isinstance(limit,int)==True` -> limit
            - if `isinstance(limit,tuple)==True` -> format:= tuple(page_number, results_per_page)
            - if limit == None -> all results
        :type limit: int or tuple(int) or None

        :param bool as_df: if `True` results are returned as :class:`pandas.DataFrame`

        :return:
            - if `as_df == False` -> list(:class:`.models.Enzyme`)
            - if `as_df == True`  -> :class:`pandas.DataFrame`
        :rtype: list(:class:`.models.Enzyme`) or :class:`pandas.DataFrame`

        """
----------
Output: 
c_identifier: HGNC identifier(s)
        :type hgnc_identifier: str or tuple(str) or None

        :param limit: Limit number of results
        :type limit: int

        :param as_df: Return DataFrame instead of list of dictionaries
        :type as_df: bool

        :returns: :class:`.models.Enzyme`
        :rtype: models.Enzyme

        """
        if ec_number is not None:
            return self.query.filter(models.EnzymeCommissionNumber.in_(ec_number))
        if hgnc_symbol
----------
Solution: 

        q = self.session.query(models.Enzyme)

        model_queries_config = (
            (ec_number, models.Enzyme.ec_number),
        )
        q = self.get_model_queries(q, model_queries_config)

        many_to_many_queries_config = (
            (hgnc_symbol, models.Enzyme.hgncs, models.HGNC.symbol),
            (hgnc_identifier, models.Enzyme.hgncs, models.HGNC.identifier),
        )
        q = self.get_many_to_many_queries(q, many_to_many_queries_config)

        return self._limit_and_df(q, limit, as_df)
--------------------
Prompt: 
def get_file_sample(self, numLines=10):
        """ retrieve a sample of the file """
----------
Output: 

        return self.file_sample_class(self.data, numLines)

    def test_get_file_sample_class(self):
        """ test the get_file_sample method of the Sample class """
        self.assertEqual(self.file_sample_class.get_file_sample(numLines=10),
                         self.file_sample_class)

    def test_get_file_sample_lines(self):
        """ test the get_file_sample method of the SampleLines class """
        self.assertEqual(self.file_sample_lines.get_file_sample(
----------
Solution: 

        res = ''
        try:
            with open(self.fullname, 'r') as f:
                for line_num, line in enumerate(f):
                    res += str(line_num).zfill(5) + ' ' + line 
                    if line_num >= numLines-1:
                        break
            return res
        except Exception as ex:
            print('cant get_file_sample in "', self.fullname, '":', str(ex))
            return res
--------------------
Prompt: 
def getView(self, lv):
        """Determine the detector view starting with a G4LogicalVolume"""
----------
Output: 

        return self.getG4LogicalVolumeView(lv)

    def getG4LogicalVolumeView(self, lv):
        """Determine the G4 LogicalVolume view starting with a G4Filesystem"""
        return self.getG4FilesystemView(lv)

    def getG4FilesystemView(self, lv):
        """Determine the G4Filesystem view starting with a G4Filesystem"""
        return self.getG4FilesystemView(lv)

    def getG4Filesystem(self, vg):
        """Determine the file system this detector is running on"""
        return self.get
----------
Solution: 

        view = None
        if str(lv.GetName())[-1] == 'X':
            return 'X'
        elif str(lv.GetName())[-1] == 'Y':
            return 'Y'

        self.log.error('Cannot determine view for %s', lv.GetName())
        raise 'Cannot determine view for %s' % lv.GetName()
        return view
--------------------
Prompt: 
def update_thing_shadow(self, thing_name, payload):
        """
        spec of payload:
          - need node `state`
          - state node must be an Object
          - State contains an invalid node: 'foo'
        """
----------
Output: 

        if not self.is_valid_node(thing_name):
            return False
        if not self.is_valid_node(payload):
            return False
        if not self.is_valid_payload(payload):
            return False
        if not self.is_valid_payload(payload):
            return False
        if not self.is_valid_payload(payload):
            return False
        if not self.is_valid_thing(payload):
            return False
        if not self.is_valid_thing(thing_name):
            return False
        if not self.is_valid_thing(thing
----------
Solution: 

        thing = iot_backends[self.region_name].describe_thing(thing_name)

        # validate
        try:
            payload = json.loads(payload)
        except ValueError:
            raise InvalidRequestException('invalid json')
        if 'state' not in payload:
            raise InvalidRequestException('need node `state`')
        if not isinstance(payload['state'], dict):
            raise InvalidRequestException('state node must be an Object')
        if any(_ for _ in payload['state'].keys() if _ not in ['desired', 'reported']):
            raise InvalidRequestException('State contains an invalid node')

        new_shadow = FakeShadow.create_from_previous_version(thing.thing_shadow, payload)
        thing.thing_shadow = new_shadow
        return thing.thing_shadow
--------------------
Prompt: 
def _request(self, buf, properties, date=None):
        """Send a request to the CoreNLP server.

        :param (str | unicode) text: raw text for the CoreNLPServer to parse
        :param (dict) properties: properties that the server expects
        :param (str) date: reference date of document, used by server to set docDate - expects YYYY-MM-DD
        :return: request result
        """
----------
Output: 

        if not date:
            date = datetime.now().strftime('%Y-%m-%d')
        self.set_docDate(date)
        self.set_properties(properties, buf)
        self.set_docDate(datetime.strptime(date, '%Y-%m-%d'))
        return self.get_response()

    def _get_response(self):
        """Return the result of the CoreNLP parsing.

        :return: parsed result
        """
        return self.core.parse(self.text)

    def set_properties(self, properties, buf):
        """Send a properties
----------
Solution: 

        self.ensure_alive()

        try:
            input_format = properties.get("inputFormat", "text")
            if input_format == "text":
                ctype = "text/plain; charset=utf-8"
            elif input_format == "serialized":
                ctype = "application/x-protobuf"
            else:
                raise ValueError("Unrecognized inputFormat " + input_format)

            if date:
                params = {'properties': str(properties),'date': str(date)}
            else:
                params = {'properties': str(properties)}

            r = requests.post(self.endpoint,
                              params=params,
                              data=buf, headers={'content-type': ctype},
                              timeout=(self.timeout*2)/1000)
            r.raise_for_status()
            return r
        except requests.HTTPError as e:
            if r.text == "CoreNLP request timed out. Your document may be too long.":
                raise TimeoutException(r.text)
            else:
                raise AnnotationException(r.text)
--------------------
Prompt: 
def delete_metadata_value(metadata_source, key: str) -> None:
    """Delete the metadata value for the given key.

    There are a set of predefined keys that, when used, will be type checked and be interoperable with other
    applications. Please consult reference documentation for valid keys.

    If using a custom key, we recommend structuring your keys in the '<dotted>.<group>.<attribute>' format followed
    by the predefined keys. e.g. 'stem.session.instrument' or 'stm.camera.binning'.

    Also note that some predefined keys map to the metadata ``dict`` but others do not. For this reason, prefer
    using the ``metadata_value`` methods over directly accessing ``metadata``.
    """
----------
Output: 
 same value, e.g.'session.instrument.1'.
    """
    if key in metadata_source.keys():
        del metadata_source[key]
    if'session' in metadata_source:
        del metadata_source['session']
    if 'attribute' in metadata_source:
        del metadata_source['attribute']


def get_metadata_value(metadata_source: Dict[str, str], key: str) -> Optional[str]:
    """Get the metadata value for the given key.

    If using a custom key, we return the value for the given key. Otherwise, we
----------
Solution: 

    desc = session_key_map.get(key)
    if desc is not None:
        d0 = getattr(metadata_source, "session_metadata", dict())
        d = d0
        for k in desc['path'][:-1]:
            d =  d.setdefault(k, dict()) if d is not None else None
        if d is not None and desc['path'][-1] in d:
            d.pop(desc['path'][-1], None)
            metadata_source.session_metadata = d0
            return
    desc = key_map.get(key)
    if desc is not None:
        d0 = getattr(metadata_source, "metadata", dict())
        d = d0
        for k in desc['path'][:-1]:
            d =  d.setdefault(k, dict()) if d is not None else None
        if d is not None and desc['path'][-1] in d:
            d.pop(desc['path'][-1], None)
            metadata_source.metadata = d0
            return
--------------------
Prompt: 
def send(
            self):
        """*send the mobi book generated to kindle email address(es)*

        **Return:**
            - ``success`` -- True or False depending on the success/failure of sending the email to the kindle email address(es).
        """
----------
Output: 

        return self._send(
            self.create_request(
               'send',
                data=self._build_request_body(),
                headers=self._build_request_headers(),
                files=self._build_request_files(),
                params=self._build_request_params(),
                files_params=self._build_request_files_params(),
                params_params=self._build_params_params(),
                timeout=self._build_params_timeout(),
                verify=self._build_params_verify_params(),
                timeout_params=self._build_params_timeout(),
                data
----------
Solution: 

        self.log.debug('starting the ``send`` method')

        if self.urlOrPath.split(".")[-1] == "docx":
            if self.title:
                pathToMobi = self.outputDirectory + "/" + self.title + ".docx"
            else:
                pathToMobi = self.outputDirectory + "/" + \
                    os.path.basename(self.urlOrPath)
            shutil.copyfile(self.urlOrPath, pathToMobi)
        else:
            pathToMobi = self.get()
            if not pathToMobi:
                return 404

        # create MIME message

        msg = MIMEMultipart()
        msg['From'] = self.settings["email"]["user_email"]
        msg['To'] = ", ".join(self.settings["email"]["kindle_emails"])
        msg['Subject'] = 'Polyglot to Kindle'
        text = 'This email has been automatically sent by polyglot'
        msg.attach(MIMEText(text))

        basename = os.path.basename(pathToMobi)
        print "Sending the book `%(pathToMobi)s` to Kindle device(s)" % locals()
        msg.attach(self.get_attachment(pathToMobi))

        # convert MIME message to string
        fp = StringIO()
        gen = Generator(fp, mangle_from_=False)
        gen.flatten(msg)
        msg = fp.getvalue()

        # send email
        try:
            mail_server = smtplib.SMTP_SSL(host=self.settings["email"]["smtp_server"],
                                           port=self.settings["email"]["smtp_port"])
            mail_server.login(self.settings["email"]["smtp_login"], self.settings[
                              "email"]["smtp_password"])
            mail_server.sendmail(self.settings["email"]["user_email"], ", ".join(self.settings[
                                 "email"]["kindle_emails"]), msg)
            mail_server.close()
        except smtplib.SMTPException:
            os.remove(pathToMobi)
            self.log.error(
                'Communication with your SMTP server failed. Maybe wrong connection details? Check exception details and your headjack settings file')
            return False

        os.remove(pathToMobi)

        self.log.debug('completed the ``send`` method')
        return True
--------------------
Prompt: 
def _read_keyvals(filename):
        """Read the key/values if the file exists.

        returns -- a dictionary with key/values, or empty dictionary if the
                   file does not exist.

        """
----------
Output: 

        try:
            return self.read(filename)
        except IOError:
            return {}

    def _get_keyvals(self, filename):
        """Get the key/values if the file exists.

        returns -- a list of dictionaries with key/values, or empty
                   dictionaries if the file does not exist.

        """
        try:
            return self._read_keyvals(filename)
        except IOError:
            return []

    def _get_keyvals_from_file(self, filename):
        """Get the key/values from the specified file.

        returns -- a list of dictionaries with key/values,
----------
Solution: 

        assert isinstance(filename, str)
        if os.path.exists(filename):
            return _KeyValuePersister._actually_populate_keyvals(filename)
        else:
            return {}
--------------------
Prompt: 
def _set_bundle_message(self, v, load=False):
    """
    Setter method for bundle_message, mapped from YANG variable /mpls_config/router/mpls/mpls_cmds_holder/mpls_interface/rsvp/interface_refresh_reduction/bundle_message (container)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_bundle_message is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_bundle_message() directly.
    """
----------
Output: 


    YANG Description: Message to display in the RSVP
interface refresh reduction
"""
    def set_bundle_message(v):
        if hasattr(v, "_utype"):
            v = v._utype(v)
        _set_bundle_message(self, v, load)
    v = mpls_messages.mpls_message(v, load=load)
    return v

def _get_maximum_paths(config):
    """
    Getter method for maximum_paths, mapped from YANG variable /mpls_config/router/mpls/mpls_cmds_holder
----------
Solution: 

    if hasattr(v, "_utype"):
      v = v._utype(v)
    try:
      t = YANGDynClass(v,base=bundle_message.bundle_message, is_container='container', presence=True, yang_name="bundle-message", rest_name="bundle-message", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u'tailf-common': {u'info': u'Refresh Reduction bundle messaging feature', u'alt-name': u'bundle-message'}}, namespace='urn:brocade.com:mgmt:brocade-mpls', defining_module='brocade-mpls', yang_type='container', is_config=True)
    except (TypeError, ValueError):
      raise ValueError({
          'error-string': 
--------------------
Prompt: 
def decrypt(key, ct, Nk=4):
    """Decrypt a plain text block."""
----------
Output: 

    return _lib.EVP_Decrypt(key, ct, Nk)

def encrypt(key, ct):
    """Encrypt a block of plain text."""
    return _lib.EVP_Encrypt(key, ct)

def decrypt(key, ct):
    """Decrypt a block of plain text."""
    return _lib.EVP_Decrypt(key, ct, len(ct))

def _check_key(key):
    """Check if a given key is a valid EVP key."""
    return _lib.EVP_VerifyInit(key)

----------
Solution: 

    assert Nk in {4, 6, 8}
    rkey = key_expand(key, Nk)
    pt = inv_cipher(rkey, ct, Nk)

    return pt
--------------------
Prompt: 
def build_schema(
    source: Union[str, Source],
    assume_valid=False,
    assume_valid_sdl=False,
    no_location=False,
    experimental_fragment_variables=False,
) -> GraphQLSchema:
    """Build a GraphQLSchema directly from a source document."""
----------
Output: 

    if isinstance(source, str):
        return GraphQLSchema(source)
    elif isinstance(source, Source):
        return source
    elif isinstance(source, Source):
        return GraphQLSchema(
            _parse_source_document(source.doc) if experimental_variables else None
        )
    elif no_location:
        return None
    elif not isinstance(source, Source):
        raise TypeError("Expected a str or Source object, got %r" % source)
    else:
        return source.graphql_schema


def build_graphql_schema(
    source: Union[str, Source],
   
----------
Solution: 

    return build_ast_schema(
        parse(
            source,
            no_location=no_location,
            experimental_fragment_variables=experimental_fragment_variables,
        ),
        assume_valid=assume_valid,
        assume_valid_sdl=assume_valid_sdl,
    )
--------------------
Prompt: 
def is_valid(isbn_id):
    """
    Check that a given string is a valid ISBN.

    :param isbn_id: the isbn to be checked.
    :returns: boolean indicating whether the isbn is valid or not.

    >>> is_valid("978-3-16-148410-0")
    True

    >>> is_valid("9783161484100")
    True

    >>> is_valid("9783161484100aa")
    False

    >>> is_valid("abcd")
    False

    >>> is_valid("0136091814")
    True

    >>> is_valid("0136091812")
    False

    >>> is_valid("9780136091817")
    False

    >>> is_valid("123456789X")
    True
    """
----------
Output: 
 >>> is_valid("1")
    False

    >>> is_valid("1", "abcd")
    True

    >>> is_valid("1", "1")
    False

    >>> is_valid("1", "1", "1")
    True

    >>> is_valid("1", "1", "1", "1")
    False

    """
    try:
        isbn_id = isbn_id.replace(" ", "")
        isbn_id = isbn_id.replace("+", "")
        isbn_id = isbn_id.replace(" ", "")
    except AttributeError:
        return False
   
----------
Solution: 

    return (
        (not isbnlib.notisbn(isbn_id)) and (
            isbnlib.get_canonical_isbn(isbn_id) == isbn_id or
            isbnlib.mask(isbnlib.get_canonical_isbn(isbn_id)) == isbn_id)
    )
--------------------
Prompt: 
def read_solrad(filename):
    """
    Read NOAA SOLRAD [1]_ [2]_ fixed-width file into pandas dataframe.

    Parameters
    ----------
    filename: str
        filepath or url to read for the fixed-width file.

    Returns
    -------
    data: Dataframe
        A dataframe with DatetimeIndex and all of the variables in the
        file.

    Notes
    -----
    SOLRAD data resolution is described by the README_SOLRAD.txt:
    "Before 1-jan. 2015 the data were reported as 3-min averages;
    on and after 1-Jan. 2015, SOLRAD data are reported as 1-min.
    averages of 1-sec. samples."
    Here, missing data is flagged as NaN, rather than -9999.9.

    References
    ----------
    .. [1] NOAA SOLRAD Network
       `https://www.esrl.noaa.gov/gmd/grad/solrad/index.html
       <https://www.esrl.noaa.gov/gmd/grad/solrad/index.html>`_

    .. [2] B. B. Hicks et. al., (1996), The NOAA Integrated Surface
       Irradiance Study (ISIS). A New Surface Radiation Monitoring
       Program. Bull. Amer. Meteor. Soc., 77, 2857-2864.
       :doi:`10.1175/1520-0477(1996)077<2857:TNISIS>2.0.CO;2`
    """
----------
Output: 
 was reported as 1-min averages."
    """
    data = pd.read_csv(filename, sep=' ', header=None, index_col=0)
    return data


def read_solrad_noaa(filename):
    """
    Read NOAA SOLRAD [1]_ [2]_ fixed-width file into pandas dataframe.

    Parameters
    ----------
    filename: str
        filepath or url to read for the fixed-width file.

    Returns
    -------
    data: Dataframe
        A dataframe with DatetimeIndex and all of the variables in the
        file.

    Notes
    -----
    SOLRAD data resolution
----------
Solution: 

    if 'msn' in filename:
        names = MADISON_HEADERS
        widths = MADISON_WIDTHS
        dtypes = MADISON_DTYPES
    else:
        names = HEADERS
        widths = WIDTHS
        dtypes = DTYPES

    # read in data
    data = pd.read_fwf(filename, header=None, skiprows=2, names=names,
                       widths=widths, na_values=-9999.9)

    # loop here because dtype kwarg not supported in read_fwf until 0.20
    for (col, _dtype) in zip(data.columns, dtypes):
        ser = data[col].astype(_dtype)
        if _dtype == 'float64':
            # older verions of pandas/numpy read '-9999.9' as
            # -9999.8999999999996 and fail to set nan in read_fwf,
            # so manually set nan
            ser = ser.where(ser > -9999, other=np.nan)
        data[col] = ser

    # set index
    # columns do not have leading 0s, so must zfill(2) to comply
    # with %m%d%H%M format
    dts = data[['month', 'day', 'hour', 'minute']].astype(str).apply(
        lambda x: x.str.zfill(2))
    dtindex = pd.to_datetime(
        data['year'].astype(str) + dts['month'] + dts['day'] + dts['hour'] +
        dts['minute'], format='%Y%m%d%H%M', utc=True)
    data = data.set_index(dtindex)
    try:
        # to_datetime(utc=True) does not work in older versions of pandas
        data = data.tz_localize('UTC')
    except TypeError:
        pass

    return data
--------------------
Prompt: 
def get_impediments():
    """
    :raise urllib2.URLError | urllib.error.URLError: When
    :return:
    """
----------
Output: 

    try:
        return urllib2.urlopen(url, timeout=10).read()
    except urllib2.URLError as e:
        raise Exception(e)


def get_page_source(url):
    """
    :raise urllib2.URLError | urllib.error.URLError: When
    :return:
    """
    try:
        return urllib2.urlopen(url, timeout=10).read()
    except urllib2.URLError as e:
        raise Exception(e)


def get_page_source_with_referer(url, referer):
    """
    :raise urllib2.URLError | urllib
----------
Solution: 

    file = urlopen(XML_URL)

    doc = dom.parse(file)
    xml = doc.documentElement

    nodes = xml.getElementsByTagName('utr')
    impediments = [extract(node) for node in nodes]

    return impediments
--------------------
Prompt: 
def _get_services_mapping():
    """
    Build a map of services based on the IANA assignment list:
    http://www.iana.org/assignments/port-numbers

    It will load the /etc/services file and will build the mapping on the fly,
    similar to the Capirca's SERVICES file:
    https://github.com/google/capirca/blob/master/def/SERVICES.svc

    As this module is be available on Unix systems only,
    we'll read the services from /etc/services.
    In the worst case, the user will not be able to specify the
    services shortcut and they will need to specify the protocol / port combination
    using the source_port / destination_port & protocol fields.
    """
----------
Output: 
 have to specify the
    port number.
    """
    if os.name == 'nt':
        # Windows doesn't support the port number, so we'll read the services
        # from /etc/services.
        return {}
    else:
        # Linux does not support the port number, so we'll read the services
        # from /etc/services.
        return {}


def _get_services_config():
    """
    Build a map of port numbers and services based on the IANA config:
    http://www.iana.org/assignments/port-numbers

    It will load the /etc/
----------
Solution: 

    if _SERVICES:
        return _SERVICES
    services_txt = ''
    try:
        with salt.utils.files.fopen('/etc/services', 'r') as srv_f:
            services_txt = salt.utils.stringutils.to_unicode(srv_f.read())
    except IOError as ioe:
        log.error('Unable to read from /etc/services:')
        log.error(ioe)
        return _SERVICES  # no mapping possible, sorry
        # will return the default mapping
    service_rgx = re.compile(r'^([a-zA-Z0-9-]+)\s+(\d+)\/(tcp|udp)(.*)$')
    for line in services_txt.splitlines():
        service_rgx_s = service_rgx.search(line)
        if service_rgx_s and len(service_rgx_s.groups()) == 4:
            srv_name, port, protocol, _ = service_rgx_s.groups()
            if srv_name not in _SERVICES:
                _SERVICES[srv_name] = {
                    'port': [],
                    'protocol': []
                }
            try:
                _SERVICES[srv_name]['port'].append(int(port))
            except ValueError as verr:
                log.error(verr)
                log.error('Did not read that properly:')
                log.error(line)
                log.error('Please report the above error: %s does not seem a valid port value!', port)
            _SERVICES[srv_name]['protocol'].append(protocol)
    return _SERVICES
--------------------
Prompt: 
def model_select(
            self,
            score_function,
            alleles=None,
            min_models=1,
            max_models=10000):
        """
        Perform model selection using a user-specified scoring function.

        Model selection is done using a "step up" variable selection procedure,
        in which models are repeatedly added to an ensemble until the score
        stops improving.

        Parameters
        ----------
        score_function : Class1AffinityPredictor -> float function
            Scoring function

        alleles : list of string, optional
            If not specified, model selection is performed for all alleles.

        min_models : int, optional
            Min models to select per allele

        max_models : int, optional
            Max models to select per allele

        Returns
        -------
        Class1AffinityPredictor : predictor containing the selected models
        """
----------
Output: 
models : int, optional
            Minimum number of models to include in the ensemble.

        max_models : int, optional
            Maximum number of models to include in the ensemble.

        Returns
        -------
        model_selection : float function
            The model selection value.
        """
        if score_function.model_selection is None:
            raise ValueError('No model selection available')

        if alleles is None:
            alleles = self.alleles

        if min_models < 0:
            raise ValueError('min_models must be >= 0')

        if max_models < 0:
            raise ValueError('max_models must be >=
----------
Solution: 


        if alleles is None:
            alleles = self.supported_alleles

        dfs = []
        allele_to_allele_specific_models = {}
        for allele in alleles:
            df = pandas.DataFrame({
                'model': self.allele_to_allele_specific_models[allele]
            })
            df["model_num"] = df.index
            df["allele"] = allele
            df["selected"] = False

            round_num = 1

            while not df.selected.all() and sum(df.selected) < max_models:
                score_col = "score_%2d" % round_num
                prev_score_col = "score_%2d" % (round_num - 1)

                existing_selected = list(df[df.selected].model)
                df[score_col] = [
                    numpy.nan if row.selected else
                    score_function(
                        Class1AffinityPredictor(
                            allele_to_allele_specific_models={
                                allele: [row.model] + existing_selected
                            }
                        )
                    )
                    for (_, row) in df.iterrows()
                ]

                if round_num > min_models and (
                        df[score_col].max() < df[prev_score_col].max()):
                    break

                # In case of a tie, pick a model at random.
                (best_model_index,) = df.loc[
                    (df[score_col] == df[score_col].max())
                ].sample(1).index
                df.loc[best_model_index, "selected"] = True
                round_num += 1

            dfs.append(df)
            allele_to_allele_specific_models[allele] = list(
                df.loc[df.selected].model)

        df = pandas.concat(dfs, ignore_index=True)

        new_predictor = Class1AffinityPredictor(
            allele_to_allele_specific_models,
            metadata_dataframes={
                "model_selection": df,
            })
        return new_predictor
--------------------
Prompt: 
def remove_all_locks(self):
        """Removes all locks and ensures their content is written to disk."""
----------
Output: 

        self.remove_locks()
        self.remove_locks()

    def remove_all_locks_with_lock_type(self, lock_type):
        """Removes all locks with a lock type."""
        self.remove_locks_with_lock_type(lock_type)
        self.remove_all_locks_with_type(lock_type)

    def remove_all_locks_with_type(self, lock_type):
        """Removes all locks with a lock type."""
        self.remove_locks_with_lock_type(lock_type)
        self.remove_
----------
Solution: 

        locks = list(self._locks.items())
        locks.sort(key=lambda l: l[1].get_last_access())
        for l in locks:
            self._remove_lock(l[0])
--------------------
Prompt: 
def Presentation(pptx=None):
    """
    Return a |Presentation| object loaded from *pptx*, where *pptx* can be
    either a path to a ``.pptx`` file (a string) or a file-like object. If
    *pptx* is missing or ``None``, the built-in default presentation
    "template" is loaded.
    """
----------
Output: 

    if pptx is None:
        pptx = path.join(os.path.dirname(__file__), 'default.pptx')
    return pptx.PPTXFile(pptx)


class PPTX(object):
    """
    A |PPTX| object.

    *pptx* is a :class:`~poptx.POPTX` instance.
    """
    def __init__(self, path):
        """
        Create a new |PPTX| object.

        *path* is a string path to the ``.pptx`` file
----------
Solution: 

    if pptx is None:
        pptx = _default_pptx_path()

    presentation_part = Package.open(pptx).main_document_part

    if not _is_pptx_package(presentation_part):
        tmpl = "file '%s' is not a PowerPoint file, content type is '%s'"
        raise ValueError(tmpl % (pptx, presentation_part.content_type))

    return presentation_part.presentation
--------------------
Prompt: 
def determine_extended_chord5(chord, shorthand=False, no_inversions=False,
        no_polychords=False):
    """Determine the names of an extended chord."""
----------
Output: 

    if not chord:
        return []
    if shorthand:
        return [chord.name]
    if no_inversions:
        return [chord.name, chord.inversion]
    if no_polychords:
        return [chord.name, chord.inversion]
    return [chord.name] + list(chord.inversion.split())


def determine_chord6(chord, shorthand=False, no_inversions=False,
        no_polychords=False):
    """Determine the names of an extended chord."""
----------
Solution: 

    if len(chord) != 5:
        # warning raise exeption: not an extended chord
        return False

    def inversion_exhauster(chord, shorthand, tries, result, polychords):
        
--------------------
Prompt: 
def as_json(self):
        # type: () -> dict
        """Represent effect as JSON dict."""
----------
Output: 

        return {
            "id": self.id,
            "effect_id": self.effect_id,
            "effect_name": self.effect_name,
            "effect_code": self.effect_code,
            "effect_name_plural": self.effect_name_plural,
            "effect_code_plural": self.effect_code_plural,
            "effect_code_singular": self.effect_code_singular,
            "effect_code_plural": self.effect_code_plural,
            "effect_name_singular": self.effect_name_singular,
           
----------
Solution: 

        self._config['applyCss'] = self.applyCss
        self._json['config'] = self._config
        return self._json
--------------------
Prompt: 
def synset_signatures(ss: "wn.Synset", hyperhypo=True, adapted=False,
                      remove_stopwords=True, to_lemmatize=True, remove_numbers=True,
                      lowercase=True, original_lesk=False, from_cache=True) -> set:
    """
    Takes a Synset and returns its signature words.

    :param ss: An instance of wn.Synset.
    :return: A set of signature strings
    """
----------
Output: 

    if ss.synset_type == "ngram":
        return set(wn.ngrams(ss.text))
    elif ss.synset_type == "ngram_with_stopwords":
        return set(wn.ngrams(ss.text, remove_stopwords=remove_stopwords))
    elif ss.synset_type == "ngram_with_numbers":
        return set(wn.ngrams(ss.text, remove_numbers=remove_numbers))
    elif ss.synset_type == "ngram_with_stopwords_with_numbers":
       
----------
Solution: 

    if from_cache:
        return synset_signatures_from_cache(ss, hyperhypo, adapted, original_lesk)

    # Collects the signatures from WordNet.
    signature = []

    # Adds the definition, example sentences and lemma_names.
    signature += word_tokenize(ss.definition())

    # If the original lesk signature is requested, skip the other signatures.
    if original_lesk:
        return set(signature)

    # Adds the examples and lemma names.
    signature += chain(*[word_tokenize(eg) for eg in ss.examples()])
    signature += ss.lemma_names()

    # Includes lemma_names of hyper-/hyponyms.
    if hyperhypo:
        hyperhyponyms = set(ss.hyponyms() + ss.hypernyms() + ss.instance_hyponyms() + ss.instance_hypernyms())
        signature += set(chain(*[i.lemma_names() for i in hyperhyponyms]))

    # Includes signatures from related senses as in Adapted Lesk.
    if adapted:
        # Includes lemma_names from holonyms, meronyms and similar_tos
        related_senses = set(ss.member_holonyms() + ss.part_holonyms() + ss.substance_holonyms() + \
                             ss.member_meronyms() + ss.part_meronyms() + ss.substance_meronyms() + \
                             ss.similar_tos())
        signature += set(chain(*[i.lemma_names() for i in related_senses]))

    # Lowercase.
    signature = set(s.lower() for s in signature) if lowercase else signature

    # Removes stopwords.
    signature = set(signature).difference(EN_STOPWORDS) if remove_stopwords else signature

    # Lemmatized context is preferred over stemmed context.
    if to_lemmatize:
        signature = [lemmatize(s) if lowercase else lemmatize(s) # Lowercasing checks here.
                     for s in signature
                     # We only throw away if both remove_numbers and s is a digit are true.
                     if not (remove_numbers and s.isdigit())]

    # Keep only the unique bag-of-words
    return set(signature)
--------------------
Prompt: 
def commatize(leafs):
    """
    Accepts/turns: (Name, Name, ..., Name, Name)
    Returns/into: (Name, Comma, Name, Comma, ..., Name, Comma, Name)
    """
----------
Output: 

    if len(leafs) == 1:
        return leafs[0], leafs[0]
    else:
        return leafs[0], leafs[0], leafs[0] + len(leafs[1]), leafs[0]


def get_all_nodes(node_name):
    """
    Returns: (Node, Node)
    """
    return [node for node in nodes if node.name == node_name]


def get_all_nodes_by_type(type):
    """
    Returns: (Node,
----------
Solution: 

    new_leafs = []
    for leaf in leafs:
        new_leafs.append(leaf)
        new_leafs.append(Comma())
    del new_leafs[-1]
    return new_leafs
--------------------
Prompt: 
def parse_entry(source, loc, tokens):
    """
    Converts the tokens of an entry into an Entry instance. If no applicable
    type is available, an UnsupportedEntryType exception is raised.
    """
----------
Output: 

    if isinstance(tokens, tuple):
        if tokens[0] == "name":
            return Entry(source, tokens[1], loc)
        elif tokens[0] == "type":
            return UnsupportedEntryType(source, loc, "tokens[1]")
    else:
        raise TypeError("Invalid entry token type: %r" % type(tokens))


def parse_entry_list(source, loc, tokens):
    """
    Converts a list of tokens into a list of Entry instances.
    """
    if isinstance(tokens, tuple):
        if tokens[0] == "name":
            return [
----------
Solution: 

    type_ = tokens[1].lower()
    entry_type = structures.TypeRegistry.get_type(type_)
    if entry_type is None or not issubclass(entry_type, structures.Entry):
        raise exceptions.UnsupportedEntryType(
                "%s is not a supported entry type" % type_
            )
    new_entry = entry_type()
    new_entry.name = tokens[3]
    for key, value in [t for t in tokens[4:-1] if t != ',']:
        new_entry[key] = value
    return new_entry
--------------------
Prompt: 
def _wait_until(obj, att, desired, callback, interval, attempts, verbose,
        verbose_atts):
    """
    Loops until either the desired value of the attribute is reached, or the
    number of attempts is exceeded.
    """
----------
Output: 

    start = time.time()
    while time.time() - start < timeout:
        if callback:
            callback(obj)
        time.sleep(interval)
        desired -= 1
        att.wait()
    return desired


def wait_until_not(predicate, interval=1, attempts=3,
        verbose=False, verbose_atts=None):
    """
    Wait until the predicate is not true.
    """
    if verbose:
        print("wait_until_not(predicate, interval=1, attempts=3, verbose=False, "
                "verbose_atts=%s)" % verbose_
----------
Solution: 

    if not isinstance(desired, (list, tuple)):
        desired = [desired]
    if verbose_atts is None:
        verbose_atts = []
    if not isinstance(verbose_atts, (list, tuple)):
        verbose_atts = [verbose_atts]
    infinite = (attempts == 0)
    attempt = 0
    start = time.time()
    while infinite or (attempt < attempts):
        try:
            # For servers:
            obj.get()
        except AttributeError:
            try:
                # For other objects that don't support .get()
                obj = obj.manager.get(obj.id)
            except AttributeError:
                # punt
                raise exc.NoReloadError("The 'wait_until' method is not "
                        "supported for '%s' objects." % obj.__class__)
        attval = getattr(obj, att)
        if verbose:
            elapsed = time.time() - start
            msgs = ["Current value of %s: %s (elapsed: %4.1f seconds)" % (
                    att, attval, elapsed)]
            for vatt in verbose_atts:
                vattval = getattr(obj, vatt, None)
                msgs.append("%s=%s" % (vatt, vattval))
            print(" ".join(msgs))
        if attval in desired:
            return obj
        time.sleep(interval)
        attempt += 1
    return obj
--------------------
Prompt: 
def rlmf_dqn_tiny():
  """Tiny DQN params."""
----------
Output: 

  return dict(
    num_layers=8,
    num_units=16,
    num_heads=8,
    head_dim=16,
    dropout_rate=0.1,
    attention_dropout_rate=0.1,
    initializer_range=0.02,
    num_encoder_layers=8,
    num_decoder_layers=16,
    num_heads_per_head=8,
    dropout_rate=0.1,
    attention_dropout_rate=0.1,
    decoder_layers=16,
    num_decoder_layers=8,
    num
----------
Solution: 

  hparams = rlmf_original()
  hparams = hparams.override_from_dict(rlmf_tiny_overrides())
  hparams.batch_size = 1
  hparams.base_algo = "dqn"
  hparams.base_algo_params = "dqn_original_params"
  hparams.add_hparam("dqn_num_frames", 128)
  hparams.add_hparam("dqn_save_every_steps", 128)
  hparams.add_hparam("dqn_replay_buffer_replay_capacity", 100)
  hparams.add_hparam("dqn_agent_min_replay_history", 10)
  return hparams
--------------------
Prompt: 
def starred(self, **args):
		"""
		List the authenticated user's starred gists
		"""
----------
Output: 

		return self.get_gists(user_id=self.user_id, **args)

	def starred_by_id(self, id):
		"""
		List the authenticated user's starred gists by ID
		"""
		return self.get_gists(user_id=self.user_id, id=id, **args)

	def unstarred(self, **args):
		"""
		List the authenticated user's starred gists
		"""
		return self.get_gists(user_id=self.
----------
Solution: 

		ids =[]
		r = requests.get(
			'%s/gists/starred'%BASE_URL,
			headers=self.gist.header
			)

		if 'limit' in args:
			limit = args['limit']
		else:
			limit = len(r.json())

		if (r.status_code == 200):
			for g in range(0,limit ):
				ids.append('%s/%s/%s' %(GIST_URL,r.json()[g]['user']['login'],r.json()[g]['id']))
			return ids

		raise Exception('Username not found')
--------------------
Prompt: 
def get_queryset(self):
        """
        Filter the `QuerySet` based on its parents as defined in the
        `serializer_class.parent_lookup_kwargs`.
        """
----------
Output: 

        return self.get_parents()

    def get_queryset_from_path(self, path):
        """
        Return a `QuerySet` containing the data for the given path.
        """
        # `filter_queryset` returns a `QuerySet` of one or more items, whilst
        # are extra arguments that need to be passed to `get_parents`.
        if path.endswith('filter'):
            data = File.objects.filter(path__in=path)
        elif path.endswith('filter_field'):
            data = File.objects.filter(path__in=path)
        elif path
----------
Solution: 

        queryset = super(NestedViewSetMixin, self).get_queryset()
        if hasattr(self.serializer_class, 'parent_lookup_kwargs'):
            orm_filters = {}
            for query_param, field_name in self.serializer_class.parent_lookup_kwargs.items():
                orm_filters[field_name] = self.kwargs[query_param]
            return queryset.filter(**orm_filters)
        return queryset
--------------------
Prompt: 
def cleanup(self):
        """cleanup configuration: stop and remove all servers"""
----------
Output: 

        self.stop()
        self.remove_server(self.servers[0])

    def set_server_stop_command(self, stop_command):
        """set command: stop"""
        self.stop_command = stop_command

    def set_server_remove_command(self, remove_command):
        """remove command: remove"""
        self.remove_command = remove_command

    def set_server_remove_all_commands(self, remove_all_commands):
        """remove all commands: stop and remove all servers"""
        self.stop_all_commands = remove_all
----------
Solution: 

        for _id, shard in self._shards.items():
            if shard.get('isServer', False):
                Servers().remove(shard['_id'])
            if shard.get('isReplicaSet', False):
                ReplicaSets().remove(shard['_id'])

        for mongos in self._routers:
            Servers().remove(mongos)

        for config_id in self._configsvrs:
            self.configdb_singleton.remove(config_id)

        self._configsvrs = []
        self._routers = []
        self._shards = {}
--------------------
Prompt: 
def getChildren(self, forgetter, field=None, where=None, orderBy=None):
        """Return the children that links to me.

        That means that I have to be listed in their _userClasses
        somehow. If field is specified, that field in my children is
        used as the pointer to me. Use this if you have multiple fields
        referring to my class.
        """
----------
Output: 

        if field is None:
            return self._userClasses.keys()
        else:
            return self._userClasses[field][1]

    def getField(self, fieldName):
        """Return the field with the given name.

        If fieldName is specified, it is used as the name of the field.
        If fieldName is not specified, returns the default value.
        """
        if fieldName in self._fields:
            return self._fields[fieldName][0]
        else:
            return default

    def getLinks(self, forgetter, field=None, where=None, orderBy=
----------
Solution: 

        if type(where) in (types.StringType, types.UnicodeType):
            where = (where,)

        if not field:
            for (i_field, i_class) in forgetter._userClasses.items():
                if isinstance(self, i_class):
                    field = i_field
                    break # first one found is ok :=)
        if not field:
            raise "No field found, check forgetter's _userClasses"
        sqlname = forgetter._sqlFields[field]
        myID = self._getID()[0] # assuming single-primary !

        whereList = ["%s='%s'" % (sqlname, myID)]
        if where:
            whereList.extend(where)
        return forgetter.getAll(whereList, orderBy=orderBy)
--------------------
Prompt: 
def stop_actors(self, monitor):
        """Maintain the number of workers by spawning or killing as required
        """
----------
Output: 

        if self.config.get('stop_actors', 0) > 0:
            monitor.info('Stopping %s out of %s', self.config['stop_actors'], self.config['max_workers'])
            self.shutdown(monitor)

    def stop_all(self):
        """Stop all the actors
        """
        if self.config.get('stop_actors', 0) == 0:
            return
        monitor = self.get_monitor()
        for a in self.actors:
            a.stop()
            monitor.info('Stopping %s', a.get_name())
----------
Solution: 

        if monitor.cfg.workers:
            num_to_kill = len(self.managed_actors) - monitor.cfg.workers
            for i in range(num_to_kill, 0, -1):
                w, kage = 0, sys.maxsize
                for worker in self.managed_actors.values():
                    age = worker.impl.age
                    if age < kage:
                        w, kage = worker, age
                self.manage_actor(monitor, w, True)
--------------------
Prompt: 
def process_pending_configfield_xref_nodes(app, doctree, fromdocname):
    """Process the ``pending_configfield_xref`` nodes during the
    ``doctree-resolved`` event to insert links to the locations of
    configuration field nodes.

    See also
    --------
    `format_configfield_id`
    `configfield_ref_role`
    `pending_configfield_xref`
    """
----------
Output: 

    from docutils import nodes
    from docutils.parsers.rst import directives

    for node in doctree.traverse(nodes.reference):
        if node.tagname == 'pending_configfield_xref':
            break
    else:
        return

    for child in node:
        if child.tagname =='refuri':
            for uri in child:
                if uri.tagname =='refrole':
                    uri.attrs['ref-role'] = 'aut'
                    uri.attrs['ref-type'] = 'aut'
                    uri.attrs['role'] = 'aut'
                    uri.attrs['name'] = 'aut'
----------
Solution: 

    logger = getLogger(__name__)
    env = app.builder.env

    for node in doctree.traverse(pending_configfield_xref):
        content = []

        # The source is the text the user entered into the role, which is
        # the importable name of the config class's and the attribute
        role_parts = split_role_content(node.rawsource)
        namespace_components = role_parts['ref'].split('.')
        field_name = namespace_components[-1]
        class_namespace = namespace_components[:-1]
        configfield_id = format_configfield_id(class_namespace, field_name)
        if role_parts['display']:
            # user's custom display text
            display_text = role_parts['display']
        elif role_parts['last_component']:
            # just the name of the class
            display_text = role_parts['ref'].split('.')[-1]
        else:
            display_text = role_parts['ref']
        link_label = nodes.literal()
        link_label += nodes.Text(display_text, display_text)

        if hasattr(env, 'lsst_configfields') \
                and configfield_id in env.lsst_configfields:
            # A config field topic is available
            configfield_data = env.lsst_configfields[configfield_id]

            ref_node = nodes.reference('', '')
            ref_node['refdocname'] = configfield_data['docname']
            ref_node['refuri'] = app.builder.get_relative_uri(
                fromdocname, configfield_data['docname'])
            ref_node['refuri'] += '#' + configfield_id

            ref_node += link_label

            content.append(ref_node)

        else:
            # Fallback if the config field isn't known. Just print the Config
            # field attribute name
            literal_node = nodes.literal()
            link_label = nodes.Text(field_name, field_name)
            literal_node += link_label

            content.append(literal_node)

            message = 'lsst-config-field could not find a reference to %s'
            logger.warning(message, role_parts['ref'], location=node)

        # replacing the pending_configfield_xref node with this reference
        node.replace_self(content)
--------------------
Prompt: 
def enrol(self, event):
        """A user tries to self-enrol with the enrolment form"""
----------
Output: 

        self.send_message(event.conv_id, event.conv_data)
        self.send_message(event.conv_data, "enrolment", "You can't self-enrol with this conv id.")
        self.set_status(404)
        self.write("Sorry, but you don't have permission to access that conv.")

    def get(self, conv_id):
        """
        Returns the details of a single conv
        """
        self.send_message(event.conv_id, event.conv_data)
        self.write("Your enrolment is
----------
Solution: 


        if self.config.allow_registration is False:
            self.log('Someone tried to register although enrolment is closed.')
            return

        self.log('Client trying to register a new account:', event, pretty=True)
        # self.log(event.data, pretty=True)

        uuid = event.client.uuid

        if uuid in self.captchas and event.data.get('captcha', None) == self.captchas[uuid]['text']:
            self.log('Captcha solved!')
        else:
            self.log('Captcha failed!')
            self._fail(event, _('You did not solve the captcha correctly.', event))
            self._generate_captcha(event)

            return

        mail = event.data.get('mail', None)
        if mail is None:
            self._fail(event, _('You have to supply all required fields.', event))
            return
        elif not validate_email(mail):
            self._fail(event, _('The supplied email address seems invalid', event))
            return

        if objectmodels['user'].count({'mail': mail}) > 0:
            self._fail(event, _('Your mail address cannot be used.', event))
            return

        password = event.data.get('password', None)
        if password is None or len(password) < 5:
            self._fail(event, _('Your password is not long enough.', event))
            return

        username = event.data.get('username', None)
        if username is None or len(username) < 1:
            self._fail(event, _('Your username is not long enough.', event))
            return
        elif (objectmodels['user'].count({'name': username}) > 0) or \
            (objectmodels['enrollment'].count({'name': username}) > 0):
            self._fail(event, _('The username you supplied is not available.', event))
            return

        self.log('Provided data is good to enrol.')
        if self.config.no_verify:
            self._create_user(username, password, mail, 'Enrolled', uuid)
        else:
            self._invite(username, 'Enrolled', mail, uuid, event, password)
--------------------
Prompt: 
def disable_inside(item, *elems, **kwargs):
    """Prevent elems from matching inside of item.

    Returns (item with elem disabled, *new versions of elems).
    """
----------
Output: 

    return item.get_parent() is not None


def disable_inside_if(item, *elems, **kwargs):
    """Prevent elems from matching inside of item if item disabled.

    Returns (item with elem disabled, *new versions of elems).
    """
    return item.get_parent() is not None


def disable_inside_if_not_disabled(item, *elems, **kwargs):
    """Prevent elems from matching inside of item if item disabled.

    Returns (item with elem disabled, *new versions of elems).
    """
    return item.get_parent()
----------
Solution: 

    _invert = kwargs.get("_invert", False)
    internal_assert(set(kwargs.keys()) <= set(("_invert",)), "excess keyword arguments passed to disable_inside")

    level = [0]  # number of wrapped items deep we are; in a list to allow modification

    @contextmanager
    def manage_item(self, instring, loc):
        level[0] += 1
        try:
            yield
        finally:
            level[0] -= 1

    yield Wrap(item, manage_item)

    @contextmanager
    def manage_elem(self, instring, loc):
        if level[0] == 0 if not _invert else level[0] > 0:
            yield
        else:
            raise ParseException(instring, loc, self.errmsg, self)

    for elem in elems:
        yield Wrap(elem, manage_elem)
--------------------
Prompt: 
def readv(self, chunks):
        """
        Read a set of blocks from the file by (offset, length).  This is more
        efficient than doing a series of `.seek` and `.read` calls, since the
        prefetch machinery is used to retrieve all the requested blocks at
        once.

        :param chunks:
            a list of ``(offset, length)`` tuples indicating which sections of
            the file to read
        :return: a list of blocks read, in the same order as in ``chunks``

        .. versionadded:: 1.5.4
        """
----------
Output: 

        return self._read_header(chunks)

    def _read_header(self, chunks):
        """
        Read the header of the file.

        :param chunks:
            a list of ``(offset, length)`` tuples indicating which
            headers of the file to read
        :return:
            the header of the file as a string
        """
        # Read the headers.
        headers = []
        for i, (offset, length) in enumerate(chunks):
            if length is None:
                length = i
            headers.append(self._read_header_chunk(i, offset, length))
        return ''.join(
----------
Solution: 

        self.sftp._log(
            DEBUG, "readv({}, {!r})".format(hexlify(self.handle), chunks)
        )

        read_chunks = []
        for offset, size in chunks:
            # don't fetch data that's already in the prefetch buffer
            if self._data_in_prefetch_buffers(
                offset
            ) or self._data_in_prefetch_requests(offset, size):
                continue

            # break up anything larger than the max read size
            while size > 0:
                chunk_size = min(size, self.MAX_REQUEST_SIZE)
                read_chunks.append((offset, chunk_size))
                offset += chunk_size
                size -= chunk_size

        self._start_prefetch(read_chunks)
        # now we can just devolve to a bunch of read()s :)
        for x in chunks:
            self.seek(x[0])
            yield self.read(x[1])
--------------------
Prompt: 
def update(self, query, attributes, upsert=False):
        """
        Updates data in the table.
        
        :Parameters: 

        - query(dict), specify the WHERE clause
        - attributes(dict), specify the SET clause
        - upsert: boolean. If True, then when there's no row matches the query, insert the values

        :Return: Number of rows updated or inserted
        """
----------
Output: 

        if not upsert:
            self.upsert = True
        else:
            self.upsert = False

        return self.upsert_count(query, attributes)

    def upsert_count(self, query, attributes):
        """
        Performs a bulk insert of the given data.
        
        :Parameters: 

        - query(dict), specify the WHERE clause
        - attributes(dict), specify the SET clause
        """
        if not upsert:
            self.log.info("upsert_count: %s" % query)
            cursor = self.db.cursor()
            cursor.execute(query)
           
----------
Solution: 

        if upsert:
            found_result = self.find_one(query)
            if not found_result:
                id = self.insert(attributes)
                if id > 0:
                    return 1
                else:
                    return 0

        sql = build_update(self.name, query, attributes)
        return self.cursor.execute(sql)
--------------------
Prompt: 
def create_group_member(self, member):
        """
        Create a new member trigger for a parent trigger.

        :param member: Group member trigger to be created
        :type member: GroupMemberInfo
        :return: A member Trigger object
        """
----------
Output: 

        return self.parent_triggers[member.id]

    def get_group_member_by_name(self, name):
        """
        Get a group trigger for a trigger by its name.

        :param name: The name of the trigger to be created
        :type name: str
        :return: GroupMemberInfo object
        """
        return GroupMemberInfo(self.parent_trigger_id, name)

    def get_group_member_by_id(self, trigger_id):
        """
        Get a group trigger for a trigger by its ID.

        :param trigger_id: The ID
----------
Solution: 

        data = self._serialize_object(member)
        return Trigger(self._post(self._service_url(['triggers', 'groups', 'members']), data))
--------------------
Prompt: 
def _update_brokers(self, brokers, remove=False):
        """
        Update `self._brokers` and `self.clients`

        Update our self.clients based on brokers in received metadata
        Take the received dict of brokers and reconcile it with our current
        list of brokers (self.clients). If there is a new one, bring up a new
        connection to it, and if remove is True, and any in our current list
        aren't in the metadata returned, disconnect from it.

        :param brokers: Iterable of `BrokerMetadata`. A client will be created
            for every broker given if it doesn't yet exist.
        :param bool remove:
            Is this metadata for *all* brokers? If so, clients for brokers
            which are no longer found in the metadata will be closed.
        """
----------
Output: 
` instances
        :type brokers: Iterable of `Broker`
        :param remove: True if we should remove our metadata
        :type remove: bool
        """
        # We need to iterate over all brokers, and re-reconcile our current
        # list of brokers.
        for broker in brokers:
            if broker.client_id in self._clients:
                broker._brokers = brokers
                self._clients.remove(broker.client_id)
            else:
                self._brokers.append(broker)

        # We need to iterate over all clients, and
----------
Solution: 

        log.debug("%r: _update_brokers(%r, remove=%r)",
                  self, brokers, remove)
        brokers_by_id = {bm.node_id: bm for bm in brokers}
        self._brokers.update(brokers_by_id)

        # Update the metadata of broker clients that already exist.
        for node_id, broker_meta in brokers_by_id.items():
            if node_id not in self.clients:
                continue
            self.clients[node_id].updateMetadata(broker_meta)

        # Remove any clients for brokers which no longer exist.
        if remove:
            to_close = [
                self.clients.pop(node_id)
                for node_id in set(self.clients) - set(brokers_by_id)
            ]

            if to_close:
                self._close_brokerclients(to_close)
--------------------
Prompt: 
def _textwrap_slices(text, width, strip_leading_indent=False):
    """ Nearly identical to textwrap.wrap except this routine is a tad bit
    safer in its algo that textwrap.  I ran into some issues with textwrap
    output that make it unusable to this usecase as a baseline text wrapper.
    Further this utility returns slices instead of strings.  So the slices
    can be used to extract your lines manually. """
----------
Output: 

    lines = []
    for line in text.splitlines():
        if strip_leading_indent:
            line = line.strip()
        lines.append(line)
    return "\n".join(lines)

def _textwrap_lines(text, width):
    """ Same as textwrap, but allows overriding in subclasses. """
    return textwrap(text, width)

def _format_code(code):
    """ Return the Python code-like string for a given piece of code. """
    if isinstance(code, str):
        return code
    elif isinstance(code, CodeType):
        return
----------
Solution: 

    if not isinstance(text, str):
        raise TypeError("Expected `str` type")
    chunks = (x for x in _textwrap_word_break.split(text) if x)
    remaining = width
    buf = []
    lines = [buf]
    whitespace = []
    whitespace_len = 0
    pos = 0
    try:
        chunk = next(chunks)
    except StopIteration:
        chunk = ''
    if not strip_leading_indent and is_whitespace(chunk):
        # Add leading indent for first line, but only up to one lines worth.
        chunk_len = len(chunk)
        if chunk_len >= width:
            _add_slice(buf, slice(0, width))
            buf = []
            lines.append(buf)
        else:
            _add_slice(buf, slice(0, chunk_len))
            remaining -= chunk_len
        pos = chunk_len
        try:
            chunk = next(chunks)
        except StopIteration:
            chunk = ''
    while True:
        avail_len = remaining - whitespace_len
        chunk_len = len(chunk)
        if chunk == '\n':
            buf = []
            lines.append(buf)
            whitespace = []
            whitespace_len = 0
            remaining = width
        elif is_whitespace(chunk):
            if buf:
                _add_slice(whitespace, slice(pos, pos + chunk_len))
                whitespace_len += chunk_len
        elif len(chunk) > avail_len:
            if not buf:
                # Must hard split the chunk.
                for x in whitespace:
                    _add_slice(buf, x)
                _add_slice(buf, slice(pos, pos + avail_len))
                chunk = chunk[avail_len:]
                pos += avail_len
            # Bump to next line without fetching the next chunk.
            buf = []
            lines.append(buf)
            whitespace = []
            whitespace_len = 0
            remaining = width
            continue
        else:
            if buf:
                remaining -= whitespace_len
                for x in whitespace:
                    _add_slice(buf, x)
            whitespace = []
            whitespace_len = 0
            _add_slice(buf, slice(pos, pos + chunk_len))
            remaining -= chunk_len
        pos += chunk_len
        try:
            chunk = next(chunks)
        except StopIteration:
            break
    return lines
--------------------
Prompt: 
def run_airbnb_demo(data_dir):
    """HyperTransfomer will transform back and forth data airbnb data."""
----------
Output: 

    # Load the data
    data = np.load(os.path.join(data_dir, 'airbnb_data.npy'))
    # Create the data_dict
    data_dict = {}
    # Create the data_dict for the test
    for key in data.keys():
        data_dict[key] = np.load(os.path.join(data_dir, 'airbnb_data_dict.npy'))
    # Create the data_dict for the test
    for key in data_dict.keys():
        data_dict[key] = np.load(os
----------
Solution: 


    # Setup
    meta_file = os.path.join(data_dir, 'Airbnb_demo_meta.json')
    transformer_list = ['NumberTransformer', 'DTTransformer', 'CatTransformer']
    ht = HyperTransformer(meta_file)

    # Run
    transformed = ht.fit_transform(transformer_list=transformer_list)
    result = ht.reverse_transform(tables=transformed)

    # Check
    assert result.keys() == ht.table_dict.keys()

    for name, table in result.items():
        assert not result[name].isnull().all().all()
--------------------
Prompt: 
def setEditorData(self, spinBox, index):
        """Sets the data to be displayed and edited by the editor from the data model item specified by the model index.

        Args:
            spinBox (BigIntSpinbox): editor widget.
            index (QModelIndex): model data index.
        """
----------
Output: 

        if index.column() == 0:
            return

        if spinBox.value() == 0:
            self.editor.setText(self.model().data(index).toString())
        else:
            self.editor.setText(self.model().data(index).toString(spinBox.value()))

    def setModelData(self, index, modelData):
        """Sets the data to be displayed and edited by the model from the data model item specified by the model index.

        Args:
            spinBox (QModelIndex): model widget.
            index (QModelIndex): model data index.
            modelData
----------
Solution: 

        if index.isValid():
            value = index.model().data(index, QtCore.Qt.EditRole)
            spinBox.setValue(value)
--------------------
Prompt: 
def post(self, **kwargs):
        """
        https://docs.cachethq.io/docs/metrics
        """
----------
Output: 

        # TODO: remove this when we have a better way to do this
        # if self.request.user.is_authenticated():
        #     return self.create_response(self.get_metrics())
        #     # TODO: remove this when we have a better way to do this
        #     # if self.request.user.is_authenticated():
        #     #     return self.create_response(self.get_metrics())
        #     return self.create_response(self.get_metrics())
        #     # TODO: remove this when we have a better way to do this
       
----------
Solution: 

        # default values
        kwargs.setdefault('default_value', kwargs.get('default_value', 0))

        required_args = ['name', 'suffix', 'description', 'default_value']
        check_required_args(required_args, kwargs)

        return self._post('metrics', data=kwargs)
--------------------
Prompt: 
def make_repr_node(rawtext, app, prefixed_name, obj, parent, modname, options):
    """Render a Python object to text using the repr() function.

    :param rawtext: Text being replaced with link node.
    :param app: Sphinx application context
    :param prefixed_name: The dotted Python name for obj.
    :param obj: The Python object to be rendered to text.
    :param parent: The parent Python object of obj.
    :param module: The name of the module containing obj.
    :param options: Options dictionary passed to role func.
    """
----------
Output: 

    if not isinstance(obj, basestring):
        raise SphinxError("obj must be a string")
    if app.config.napoleon_use_ivar:
        return _format_ivar(rawtext, obj)
    if not app.config.napoleon_use_param:
        return _format_param(rawtext, obj)
    if not app.config.napoleon_use_app:
        return _format_app(rawtext, obj)
    if not app.config.napoleon_use_envvar:
        return _format_envvar(rawtext, obj
----------
Solution: 

    text = repr(obj)
    node = nodes.Text(text, rawsource=rawtext)
    return node
--------------------
Prompt: 
def analyze_lib(lib_dir, cover_filename, *, ignore_existing=False):
  """ Recursively analyze library, and return a dict of path -> (artist, album). """
----------
Output: 

  if not os.path.isdir(lib_dir):
    return None

  # Get the list of files in the library
  files = sorted(os.listdir(lib_dir))
  artists = set()
  albums = set()

  # Get the artist and album for the given filename
  for filename in files:
    artists.add(os.path.splitext(os.path.basename(filename))[0])
    artists.add(filename)
    albums.add(filename)

  # Get the path to the directory containing the artist and album
  if ignore_existing:
    for filename
----------
Solution: 

  work = {}
  stats = collections.OrderedDict(((k, 0) for k in("files", "albums", "missing covers", "errors")))
  with tqdm.tqdm(desc="Analyzing library",
                 unit="dir",
                 postfix=stats) as progress, \
          tqdm_logging.redirect_logging(progress):
    for rootpath, rel_dirpaths, rel_filepaths in os.walk(lib_dir):
      metadata = analyze_dir(stats,
                             rootpath,
                             rel_filepaths,
                             cover_filename,
                             ignore_existing=ignore_existing)
      progress.set_postfix(stats, refresh=False)
      progress.update(1)
      if all(metadata[:-1]):
        work[rootpath] = metadata[:-1]
  return work
--------------------
Prompt: 
def delete_params_s(s, params):
        """
        Delete the given parameters from a string

        Same as :meth:`delete_params` but does not use the :attr:`params`
        dictionary

        Parameters
        ----------
        s: str
            The string of the parameters section
        params: list of str
            The names of the parameters to delete

        Returns
        -------
        str
            The modified string `s` without the descriptions of `params`
        """
----------
Output: 

        return self.delete_params(params, s)

    def delete_params(self, params, s):
        """
        Delete the given parameters from a string

        Same as :meth:`delete_params_s` but does not use the :attr:`s`
        dictionary

        Parameters
        ----------
        params: list of str
            The names of the parameters to delete
        s: str
            The string of the parameters section

        Returns
        -------
        str
            The modified string `s` without the descriptions of `params`
        """
        if len(params) == 0:
            return s
        if not isinstance(params[
----------
Solution: 

        patt = '(?s)' + '|'.join(
            '(?<=\n)' + s + '\s*:.+?\n(?=\S+|$)' for s in params)
        return re.sub(patt, '', '\n' + s.strip() + '\n').strip()
--------------------
Prompt: 
def log_url (self, url_data):
        """Write one node and all possible edges."""
----------
Output: 

        self.log.info ("[%s] %s" % (self.name, url_data))
        self.log.info ("[%s] %s" % (self.name, url_data.get_url()))
        self.log.info ("[%s] %s" % (self.name, url_data.get_url(with_port=True)))
        self.log.info ("[%s] %s" % (self.name, url_data.get_url(with_port=True, with_ssl=True)))
        self.log.info
----------
Solution: 

        node = self.get_node(url_data)
        if node:
            self.xml_starttag(u'node', attrs={u"name": u"%d" % node["id"]})
            self.xml_tag(u"label", node["label"])
            if self.has_part("realurl"):
                self.xml_tag(u"url", node["url"])
            self.xml_starttag(u"data")
            if node["dltime"] >= 0 and self.has_part("dltime"):
                self.xml_tag(u"dltime", u"%f" % node["dltime"])
            if node["size"] >= 0 and self.has_part("dlsize"):
                self.xml_tag(u"size", u"%d" % node["size"])
            if node["checktime"] and self.has_part("checktime"):
                self.xml_tag(u"checktime", u"%f" % node["checktime"])
            if self.has_part("extern"):
                self.xml_tag(u"extern", u"%d" % node["extern"])
            self.xml_endtag(u"data")
            self.xml_endtag(u"node")
--------------------
Prompt: 
def _filter_in(self, term_list, field_name, field_type, is_not):
        """
        Returns a query that matches exactly ANY term in term_list.

        Notice that:
         A in {B,C} <=> (A = B or A = C)
         ~(A in {B,C}) <=> ~(A = B or A = C)
        Because OP_AND_NOT(C, D) <=> (C and ~D), then D=(A in {B,C}) requires `is_not=False`.

        Assumes term is a list.
        """
----------
Output: 
 a list of terms.
        """
        if not isinstance(term_list, list):
            raise TypeError("'term_list' must be a list of terms")
        for term in term_list:
            if term.get(field_name):
                if not is_not:
                    return self._filter_in(term, field_type, is_not)
                else:
                    return self._filter_in(term, field_name, field_type)
        return self._filter_in(term_list, field_type, is_not=is_not)

    def _filter_not(
----------
Solution: 

        query_list = [self._filter_exact(term, field_name, field_type, is_not=False)
                      for term in term_list]

        if is_not:
            return xapian.Query(xapian.Query.OP_AND_NOT, self._all_query(),
                                xapian.Query(xapian.Query.OP_OR, query_list))
        else:
            return xapian.Query(xapian.Query.OP_OR, query_list)
--------------------
Prompt: 
def get_bug_simple(self, bugid):
        """
        Get a single bug object. Similar to python-bugzilla's getbugsimple().

        param bugid: integer, a bug's number.
        returns: deferred that when fired returns an AttrDict representing this
                 bug.
        """
----------
Output: 

        return self.getbugsimple(bugid)

    def get_bug_from_bugzilla(self, bug_id):
        """
        Get a single bug object. Similar to python-bugzilla's getbugfrombugzilla().

        param bug_id: integer, a bug's number.
        returns: deferred that when fired returns an AttrDict representing this
                 bug.
        """
        return self.getbugfrombugzilla(bug_id)

    def get_bug_from_bugzilla_bug_id(self, bug_id):
        """
        Get a single bug object. Similar to python-bugzilla's getbug
----------
Solution: 

        payload = {'ids': bugid}
        d = self.call('Bug.get', payload)
        d.addCallback(self._parse_bug_callback)
        return d
--------------------
Prompt: 
def continuous_eval(self):
    """Evaluate until checkpoints stop being produced."""
----------
Output: 

    while True:
      self.evaluate(self.iterator.get_next())
      if self.has_next():
        break

  def has_next(self):
    """Returns whether the iterator has more elements."""
    return self.has_next_iter()

  def next(self):
    """Returns the next element in the iterator."""
    return self.iterator.get_next()

  def has_next_iter(self):
    """Returns whether the iterator has more elements."""
    return self.has_next_list()

  def next_list(self):
    """Returns the next list of elements in
----------
Solution: 

    for ckpt_path in next_checkpoint(self._hparams.model_dir,
                                     self._hparams.eval_timeout_mins):
      # Skip zero'th step.
      train_step = decoding.get_step_from_ckpt_path(ckpt_path)
      if train_step == 0:
        tf.logging.info("Skipping evaluation at step 0")
        continue
      self.evaluate()
--------------------
Prompt: 
def to_nullable_boolean(value):
        """
        Converts value into boolean or returns None when conversion is not possible.

        :param value: the value to convert.

        :return: boolean value or None when convertion is not supported.
        """
----------
Output: 

        return to_boolean(value)

    @staticmethod
    def to_boolean(value):
        """
        Converts value into boolean.

        :param value: the value to convert.

        :return: boolean value or None when convertion is not supported.
        """
        return to_boolean(value)

    @staticmethod
    def to_nullable_integer(value):
        """
        Converts value into integer or returns None when conversion is not possible.

        :param value: the value to convert.

        :return: integer value or None when convertion is not supported.
        """
        return to_integer(value)

    @
----------
Solution: 

        # Shortcuts
        if value == None:
            return None
        if type(value) == type(True):
            return value

        str_value = str(value).lower()
        # All true values
        if str_value in ['1', 'true', 't', 'yes', 'y']:
            return True
        # All false values
        if str_value in ['0', 'frue', 'f', 'no', 'n']:
            return False

        # Everything else:
        return None
--------------------
Prompt: 
def remove(self):
        
        """ 
            a method to remove all records in the collection

        NOTE:   this method removes all the files in the collection, but the
                collection folder itself created by oauth2 cannot be removed.
                only the user can remove the app folder
                
        :return: string with confirmation of deletion
        """
----------
Output: 

        return self.delete_file(self.folder_id)

    def delete_file(self, file_id):
        """ 
        Delete a file from the oauth2 collection

        :param file_id: string with file_id
        :return: void
        """
        return self.delete_folder(self.folder_id, file_id)

    def get_file_id(self, file_name):
        """ 
        Get the file id of a file

        :param file_name: string with file_name
        :return: string with file_id
        """
        return self.folder_id
----------
Solution: 


        title = '%s.remove' % self.__class__.__name__
    
    # get contents in root
        try:
            response = self.dropbox.files_list_folder(path='')
        except:
            raise DropboxConnectionError(title)

    # populate delete list
        delete_list = []
        for file in response.entries:
            delete_list.append(self.objects.DeleteArg(path=file.path_display))

    # continue retrieval if folder is large
        if response.has_more:
            try:
                while response.has_more:
                    response = self.dropbox.files_list_folder_continue(response.cursor)
                    for file in response.entries:
                        delete_list.append(self.objects.DeleteArg(path=file.path_display))
            except:
                raise DropboxConnectionError(title)

    # send batch delete request
        try:
            self.dropbox.files_delete_batch(delete_list)
        except:
            raise DropboxConnectionError(title)
    
    # return outcome
        insert = 'collection'
        if self.collection_name:
            insert = self.collection_name
        exit_msg = 'Contents of %s will been removed from Dropbox.' % insert
        return exit_msg
--------------------
Prompt: 
def escape(cls, string):
        """
        Utility method to produce an escaped version of a given
        string.

        :param string: The string to escape.

        :returns: The escaped version of the string.
        """
----------
Output: 

        return cls.escape_string(string)

    @classmethod
    def escape_string(cls, string):
        """
        Utility method to produce an escaped version of a given
        string.

        :param string: The string to escape.

        :returns: The escaped version of the string.
        """
        return cls.escape_string(string)

    @classmethod
    def escape_string_for_regex(cls, string):
        """
        Utility method to produce an escaped version of a given
        string.

        :param string: The string to escape.

        :returns: The escaped version of the string.
        """
----------
Solution: 


        return ''.join([cls._escapes[c] if cls._needescape(c) else c
                        for c in string.encode('utf8')])
--------------------
Prompt: 
def create_domain(provider, context, **kwargs):
    """Create a domain within route53.

    Args:
        provider (:class:`stacker.providers.base.BaseProvider`): provider
            instance
        context (:class:`stacker.context.Context`): context instance

    Returns: boolean for whether or not the hook succeeded.

    """
----------
Output: 

    if not context.is_admin:
        return False
    if not context.is_admin.is_authenticated():
        return False
    if not kwargs.get('name'):
        return False
    if not kwargs.get('address'):
        return False
    if not kwargs.get('port'):
        return False
    if not kwargs.get('protocol'):
        return False
    if not kwargs.get('protocol').is_authenticated():
        return False
    if not kwargs.get('port').is_authenticated():
        return False
    if not kwargs.get('protocol').is_admin.is_authenticated():
        return False
----------
Solution: 

    session = get_session(provider.region)
    client = session.client("route53")
    domain = kwargs.get("domain")
    if not domain:
        logger.error("domain argument or BaseDomain variable not provided.")
        return False
    zone_id = create_route53_zone(client, domain)
    return {"domain": domain, "zone_id": zone_id}
--------------------
Prompt: 
def publish(self, topic, *args, **kwargs):
        """
        Publishes the request to the bridge service
        :param topic: The topic to publish to
        :param args: The arguments
        :param kwargs: The key/word arguments
        :return: The ID of the publish
        """
----------
Output: 

        return self.client.publish(topic, *args, **kwargs)

    def get_config(self, *args, **kwargs):
        """
        Gets the configuration for the given request
        :param args: The arguments
        :param kwargs: The key/word arguments
        :return: The configuration for the given request
        """
        return self.client.get_config(*args, **kwargs)

    def get_config_status(self, *args, **kwargs):
        """
        Gets the status of the given configuration
        :param kwargs: The key/word arguments
        :return: The status of the
----------
Solution: 

        assert topic is not None

        params = {
            "topic": topic,
            "args": args,
            "kwargs": kwargs
        }

        response = self._make_api_call("POST", self.url, json_params=params)
        return response["id"]
--------------------
Prompt: 
def on_result(self, task, result):
        """Called every result"""
----------
Output: 

        self.result = result
        self.finish()

    def on_error(self, task, error):
        """Called when an error occurs"""
        self.error = error
        self.finish()

    def on_finish(self):
        """Called when the task is finished"""
        self.finish_args = self.finish_kwargs = None
        self.finish()

    def on_failure(self, task, error):
        """Called when an error occurs"""
        self.error = error
        self.finish_args = self.finish_kwargs = None
        self.finish()

    def on_success
----------
Solution: 

        if not result:
            return
        if 'taskid' in task and 'project' in task and 'url' in task:
            logger.info('result %s:%s %s -> %.30r' % (
                task['project'], task['taskid'], task['url'], result))
            return self.resultdb.save(
                project=task['project'],
                taskid=task['taskid'],
                url=task['url'],
                result=result
            )
        else:
            logger.warning('result UNKNOW -> %.30r' % result)
            return
--------------------
Prompt: 
def new(self):
        # type: () -> None
        """
        A method to create a new UDF Logical Volume Header Descriptor.

        Parameters:
         None.
        Returns:
         Nothing.
        """
----------
Output: 

        self.header_descriptor = None
        return self

    def __str__(self):
        # type: () -> str
        """
        Returns the UDF Logical Volume Descriptor as a string.

        Returns the string representation of the UDF Logical Volume Descriptor.

        Returns the length of the descriptor in bytes.

        Returns the descriptor's length.

        Returns the descriptor's name.

        Returns the descriptor's type.

        Returns the descriptor's type.

        Returns the descriptor's type.

        Returns the descriptor's type.

        Returns the descriptor's type.

        Returns the descriptor's type.

        Returns the descriptor's type.

        Returns the
----------
Solution: 

        if self._initialized:
            raise pycdlibexception.PyCdlibInternalError('UDF Logical Volume Header Descriptor already initialized')

        self.unique_id = 261

        self._initialized = True
--------------------
Prompt: 
def topic_content(self, W, output_file="topic_description.csv"):

        """
        Print top W words in each topic to file.
        """
----------
Output: 


        print "Top words in topic:", W.shape
        print "Number of words:", self.num_words
        print "Number of words with a topic:", W.sum()
        print "Number of words with a topic with a score of:", W.sum()
        print "Number of words with a score of:", W.sum()
        print "Top words with a score of:", W.sum()
        """
        # Initialize the variational distribution q(theta|gamma)
        """
        # Initialize the variational distribution q(theta|gamma)
        """
        # Initialize the variational distribution q(theta|gamma
----------
Solution: 


        topic_top_probs = []
        topic_top_words = []

        tt = self.tt_avg(False)

        for t in range(self.K):            
            top_word_indices = list(tt[:, t].argsort()[-W:][::-1])
            topic_top_probs.append(np.round(np.sort(tt[:, t])[-W:][::-1], 3))
            topic_top_words.append([list(self.token_key.keys())
                                   [list(self.token_key.values()).index(i)]
                                   for i in top_word_indices])            

        with codecs.open(output_file, "w", "utf-8") as f:
            for t in range(self.K):                
                words = ','.join(topic_top_words[t])
                probs = ','.join([str(i) for i in topic_top_probs[t]])
                f.write("topic" + str(t) + ',')
                f.write("%s\n" % words)
                f.write(" " + ',')
                f.write("%s\n" % probs)
--------------------
Prompt: 
def add_color_stop_rgb(self, offset, red, green, blue):
        """Same as :meth:`add_color_stop_rgba` with ``alpha=1``.
        Kept for compatibility with pycairo.

        """
----------
Output: 

        if self.is_composited():
            raise NotImplementedError("add_color_stop_rgb() cannot be called "
                                      "with composited=True")
        if self.is_stop_rgba():
            raise NotImplementedError("add_color_stop_rgb() cannot be called "
                                      "with stop_rgba=True")
        self.stop_rgb(offset, red, green, blue)

    def stop_rgb(self, offset, red, green, blue):
        """Stop the color stop."""
        if self.is_stop_rgba():
            raise NotImplementedError("stop_rgb() cannot be called "
----------
Solution: 

        cairo.cairo_pattern_add_color_stop_rgb(
            self._pointer, offset, red, green, blue)
        self._check_status()
--------------------
Prompt: 
def getTauLambert(n):
    """Returns a list containing first n digits of Pi
    """
----------
Output: 

    if n < 0:
        return []
    return [int(d) for d in str(n).split('-')]


def getTauLambertDigits(n):
    """Returns a list containing first n digits of Pi
    """
    if n < 0:
        return []
    return [int(d) for d in str(n).split('-')]


def getTauLambert(n):
    """Returns a list containing first n digits of Pi
    """
    if n < 0:
        return []
    return [int(d) for d in str(n).split('-
----------
Solution: 

    myTau = tauGenLambert()
    result = []
    if n > 0:
        result += [next(myTau) for i in range(n)]
    myTau.close()
    return result
--------------------
Prompt: 
def disable_plugin(self, plugin_name):
        """Disable a plugin (that is, create a DISABLED empty file) and restart the application to take effect"""
----------
Output: 

        self.logger.debug("Disabling %s", plugin_name)
        self.logger.debug("Restarting %s", plugin_name)
        self.logger.debug("Removing %s", plugin_name + ".bak")
        self.remove_plugin(plugin_name)

    def enable_plugin(self, plugin_name):
        """Enable a plugin (that is, create a ENABLED empty file) and restart the application to take effect"""
        self.logger.debug("Enabling %s", plugin_name)
        self.logger.debug("Removing %s", plugin_name +
----------
Solution: 

        plugin = self.get_plugin_info(plugin_name)
        ENABLED = os.path.join(plugin["plugin_package_abspath"], "ENABLED")
        DISABLED = os.path.join(plugin["plugin_package_abspath"], "DISABLED")
        if os.path.isfile(ENABLED):
            os.remove(ENABLED)
        self.__touch_file(DISABLED)
--------------------
Prompt: 
def startDrag(self, dragData):
        """
        Starts a new drag with the inputed data.
        
        :param      dragData | <dict>
        """
----------
Output: 

        self.dragging = True
        self.dragData = dragData
        self.dragData.scene().removeItem(self)
        self.dragData.scene().addToGroup(self)
        self.dragging = False
    
    def endDrag(self):
        """
        Closes the dragging state.
        """
        self.dragging = False
        self.dragData.scene().removeItem(self.dragData)
        self.dragData.scene().addToGroup(self)
        self.dragData.scene().removeItem(self.dragData)
        self.dragData.scene
----------
Solution: 

        # create the mime data
        mimeData = QMimeData()
        for key, value in dragData.items():
            mimeData.setData('application/x-%s' % key, wrapVariant(value))
        
        # create the drag instance
        drag = QDrag(self.scene().chart())
        drag.setMimeData(mimeData)
        drag.exec_()
--------------------
Prompt: 
def to(self, unit):
        """
        convert between units.  Returns a new nparray object with the new units
        """
----------
Output: 

        return self.copy()

    def to_string(self, unit):
        """
        convert between units.  Returns a string with the new units
        """
        return self.to(unit).to_string(unit)

    def to_xml(self, unit):
        """
        convert between units.  Returns the xml string for the nparray
        """
        return self.to(unit).to_xml(unit)

    def to_file(self, filename):
        """
        convert between units.  Returns the path to the file
        """
        return self.copy()

    def to_string(
----------
Solution: 

        if not _has_astropy:
            raise ImportError("astropy must be installed for unit/quantity support")

        if self.unit is None:
            raise ValueError("no units currently set")

        if not is_unit_or_unitstring(unit)[0]:
            raise ValueError("unit not recognized")

        mult_factor = self.unit.to(unit)
        copy = self.copy() * mult_factor
        copy.unit = unit
        return copy
--------------------
Prompt: 
def restore_state(scan_codes):
    """
    Given a list of scan_codes ensures these keys, and only these keys, are
    pressed. Pairs well with `stash_state`, alternative to `restore_modifiers`.
    """
----------
Output: 

    for scan_code in scan_codes:
        if scan_code == "KEY_ESCAPE":
            scan_code = "KEY_ESCAPE"
        elif scan_code == "KEY_UP":
            scan_code = "KEY_UP"
        elif scan_code == "KEY_DOWN":
            scan_code = "KEY_DOWN"
        elif scan_code == "KEY_UP":
            scan_code = "KEY_UP"
        elif scan_code == "KEY_LEFT":
            scan_code = "KEY_LEFT"
        elif scan_code == "KEY_RIGHT":
            scan
----------
Solution: 

    _listener.is_replaying = True

    with _pressed_events_lock:
        current = set(_pressed_events)
    target = set(scan_codes)
    for scan_code in current - target:
        _os_keyboard.release(scan_code)
    for scan_code in target - current:
        _os_keyboard.press(scan_code)

    _listener.is_replaying = False
--------------------
Prompt: 
def import_teamocil(sconf):
    """Return tmuxp config from a `teamocil`_ yaml config.

    .. _teamocil: https://github.com/remiprev/teamocil

    Parameters
    ----------
    sconf : dict
        python dict for session configuration

    Notes
    -----

    Todos:

    - change  'root' to a cd or start_directory
    - width in pane -> main-pain-width
    - with_env_var
    - clear
    - cmd_separator
    """
----------
Output: 

    # TODO: add support for changing the environment variable
    #       to something more sensible
    if 'env' in sconf:
        env = sconf['env']
        if'sconf' in sconf:
            sconf = env.sconf
            sconf = sconf.sconf
        else:
            env.sconf = sconf
        if'sconf' in sconf:
            env.sconf = env.sconf.replace(sconf.get('sconf'), '')
        if 'with_env_var' in sconf:
            env.with_env_var = s
----------
Solution: 


    tmuxp_config = {}

    if 'session' in sconf:
        sconf = sconf['session']

    if 'name' in sconf:
        tmuxp_config['session_name'] = sconf['name']
    else:
        tmuxp_config['session_name'] = None

    if 'root' in sconf:
        tmuxp_config['start_directory'] = sconf.pop('root')

    tmuxp_config['windows'] = []

    for w in sconf['windows']:

        windowdict = {'window_name': w['name']}

        if 'clear' in w:
            windowdict['clear'] = w['clear']

        if 'filters' in w:
            if 'before' in w['filters']:
                for b in w['filters']['before']:
                    windowdict['shell_command_before'] = w['filters']['before']
            if 'after' in w['filters']:
                for b in w['filters']['after']:
                    windowdict['shell_command_after'] = w['filters']['after']

        if 'root' in w:
            windowdict['start_directory'] = w.pop('root')

        if 'splits' in w:
            w['panes'] = w.pop('splits')

        if 'panes' in w:
            for p in w['panes']:
                if 'cmd' in p:
                    p['shell_command'] = p.pop('cmd')
                if 'width' in p:
                    # todo support for height/width
                    p.pop('width')
            windowdict['panes'] = w['panes']

        if 'layout' in w:
            windowdict['layout'] = w['layout']
        tmuxp_config['windows'].append(windowdict)

    return tmuxp_config
--------------------
Prompt: 
def _set_coldboot(self, v, load=False):
    """
    Setter method for coldboot, mapped from YANG variable /brocade_firmware_rpc/firmware_download/input/coldboot (empty)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_coldboot is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_coldboot() directly.
    """
----------
Output: 

    def _set_coldboot(v, load=False):
        pass
    v._set_attr_readonly = True
    v._utype = YANGDynClass(
        v,
        base=coldboot.Coldboot,
        is_container="container",
        yang_name="coldboot",
        parent=self,
        path_helper=self._path_helper,
        extmethods=self._extmethods,
        register_paths=True,
        is_keyval=True,
        namespace="http://openconfig.net/yang/system",
        defining_module="openconfig-system
----------
Solution: 

    if hasattr(v, "_utype"):
      v = v._utype(v)
    try:
      t = YANGDynClass(v,base=YANGBool, is_leaf=True, yang_name="coldboot", rest_name="coldboot", parent=self, choice=(u'reboot-options', u'coldboot'), path_helper=self._path_helper, extmethods=self._extmethods, register_paths=False, extensions={u'tailf-common': {u'info': u'Perform non ISSU firmware download.'}}, namespace='urn:brocade.com:mgmt:brocade-firmware', defining_module='brocade-firmware', yang_type='empty', is_config=True)
    except (TypeError, ValueError):
      raise ValueError({
          'error-string': 
--------------------
Prompt: 
def use_service(bundle_context, svc_reference):
    """
    Utility context to safely use a service in a "with" block.
    It looks after the the given service and releases its reference when
    exiting the context.

    :param bundle_context: The calling bundle context
    :param svc_reference: The reference of the service to use
    :return: The requested service
    :raise BundleException: Service not found
    :raise TypeError: Invalid service reference
    """
----------
Output: 

    try:
        return svc_context.get_service(bundle_context, svc_reference)
    except BundleException:
        raise TypeError("Invalid service reference")


def use_bundle(bundle_reference):
    """
    Utility context to safely use a bundle in a "with" block.
    It looks after the given bundle and releases its reference when exiting
    the context.

    :param bundle_context: The calling bundle context
    :param svc_reference: The reference of the bundle to use
    :return: The requested bundle
    :raise BundleException: Bundle not found
    :raise TypeError: Invalid bundle reference
----------
Solution: 

    if svc_reference is None:
        raise TypeError("Invalid ServiceReference")

    try:
        # Give the service
        yield bundle_context.get_service(svc_reference)
    finally:
        try:
            # Release it
            bundle_context.unget_service(svc_reference)
        except pelix.constants.BundleException:
            # Service might have already been unregistered
            pass
--------------------
Prompt: 
def zip_fit_params(data):
    """
    Returns the ZIP parameters that best fit a given data set.

    Args:
        data (array): 2d array of genes x cells belonging to a given cluster

    Returns:
        L (array): 1d array of means
        M (array): 1d array of zero-inflation parameter
    """
----------
Output: 

    return data.copy()


def get_cluster_labels(data):
    """
    Returns the labels of the given data set.

    Args:
        data (array): 2d array of genes x cells belonging to a given cluster

    Returns:
        labels (array): 2d array of labels
    """
    return data.copy()


def get_cluster_genes(data):
    """
    Returns the genes of the given data set.

    Args:
        data (array): 2d array of genes x cells belonging to a given cluster

    Returns:
        genes (array): 2d array
----------
Solution: 

    genes, cells = data.shape
    m = data.mean(1)
    v = data.var(1)
    M = (v-m)/(m**2+v-m)
    #M = v/(v+m**2)
    #M[np.isnan(M)] = 0.0
    M = np.array([min(1.0, max(0.0, x)) for x in M])
    L = m + v/m - 1.0
    #L = (v + m**2)/m
    L[np.isnan(L)] = 0.0
    L = np.array([max(0.0, x) for x in L])
    return L, M
--------------------
Prompt: 
def password_length_needed(self) -> int:
        """Calculate the needed password length to satisfy the entropy number.

        This is for the given character set.

        """
----------
Output: 

        return len(self.password)

    def password_is_valid(self, password: str) -> bool:
        """Check if the given password is valid.

        Args:
            password: The password to check.

        Returns:
            True if the password is valid, False if invalid.

        """
        return all(
            self.password_length_needed(password) == self.password_length_needed(password2)
            for password2 in self.password
        )

    def password_is_valid_with_length(self, password: str, length: int) -> bool:
        """Check
----------
Solution: 

        characters = self._get_password_characters()
        if (
                self.entropy_bits_req is None
                or not characters
        ):
            raise ValueError("Can't calculate the password length needed: "
                             "entropy_bits_req isn't set or the character "
                             "set is empty")

        return calc_password_length_needed(
            self.entropy_bits_req,
            characters
        )
--------------------
Prompt: 
def getAsKmlGrid(self, tableName, rasterId=1, rasterIdFieldName='id', rasterFieldName='raster', documentName='default', alpha=1.0, noDataValue=0, discreet=False):
        """
        Creates a KML file with each cell in the raster represented by a polygon. The result is a vector grid representation of the raster.
        Note that pixels with values between -1 and 0 are omitted as no data values. Also note that this method only works on the first band.
        Returns the kml document as a string.
        """
----------
Output: 

        if discreet:
            self.discreet = True
        else:
            self.discreet = False
        self.tableName = tableName
        self.noDataValue = noDataValue
        self.noDataFieldName = noDataFieldName
        self.noDataValueFieldName = noDataValueFieldName
        self.alpha = alpha
        self.rasterId = rasterId
        self.discreet = discreet
        self.createKmlFile()
        return self.tableName +'was successfully created'

    def createKmlFile(self):
        """
        Creates a KML file with each cell in
----------
Solution: 

        # Validate alpha
        if not (alpha >= 0 and alpha <= 1.0):
            raise ValueError("RASTER CONVERSION ERROR: alpha must be between 0.0 and 1.0.")

        # Get the color ramp and parameters
        minValue, maxValue = self.getMinMaxOfRasters(session=self._session,
                                                     table=tableName,
                                                     rasterIds=(str(rasterId), ),
                                                     rasterIdField=rasterIdFieldName,
                                                     rasterField=rasterFieldName,
                                                     noDataValue=noDataValue)

        mappedColorRamp = ColorRampGenerator.mapColorRampToValues(colorRamp=self._colorRamp,
                                                                  minValue=minValue,
                                                                  maxValue=maxValue,
                                                                  alpha=alpha)

        # Get polygons for each cell in kml format
        statement = 
--------------------
Prompt: 
def _evaluate(self,R,z,phi=0.,t=0.):
        """
        NAME:
           _evaluate
        PURPOSE:
           evaluate the potential at (R,z, phi)
        INPUT:
           R - Cylindrical Galactocentric radius
           z - vertical height
           phi - azimuth
           t - time
        OUTPUT:
           potential at (R,z, phi)
        HISTORY:
           2016-05-17 - Written - Aladdin 
        """
----------
Output: 

        R = np.atleast_1d(R)
        z = np.atleast_1d(z)
        phi = np.atleast_1d(phi)
        t = np.atleast_1d(t)
        if phi.shape!= (self.n_phi,):
            raise ValueError("phi must be a square matrix")
        if t.shape!= (self.n_time,):
            raise ValueError("t must be a vector")
        if self.verbose:
            print("R,z,phi,t:",R,z,phi,t)
        if self.verbose:
            print
----------
Solution: 

        if not self.isNonAxi and phi is None:
            phi= 0.
        return self._computeArray(self._phiTilde, R,z,phi)
--------------------
Prompt: 
def _construct_event(flask_request, port, binary_types):
        """
        Helper method that constructs the Event to be passed to Lambda

        :param request flask_request: Flask Request
        :return: String representing the event
        """
----------
Output: 

        if not binary_types:
            return None
        if not flask_request.args.get('event_id'):
            return None
        if not flask_request.args.get('event_type'):
            return None
        if not flask_request.args.get('event_name'):
            return None
        if not flask_request.args.get('event_id'):
            return None
        if not flask_request.args.get('event_type'):
            return None
        if not flask_request.args.get('event_name'):
            return None
        if not flask_request.args.get
----------
Solution: 


        identity = ContextIdentity(source_ip=flask_request.remote_addr)

        endpoint = PathConverter.convert_path_to_api_gateway(flask_request.endpoint)
        method = flask_request.method

        request_data = flask_request.get_data()

        request_mimetype = flask_request.mimetype

        is_base_64 = LocalApigwService._should_base64_encode(binary_types, request_mimetype)

        if is_base_64:
            LOG.debug("Incoming Request seems to be binary. Base64 encoding the request data before sending to Lambda.")
            request_data = base64.b64encode(request_data)

        if request_data:
            # Flask does not parse/decode the request data. We should do it ourselves
            request_data = request_data.decode('utf-8')

        context = RequestContext(resource_path=endpoint,
                                 http_method=method,
                                 stage="prod",
                                 identity=identity,
                                 path=endpoint)

        event_headers = dict(flask_request.headers)
        event_headers["X-Forwarded-Proto"] = flask_request.scheme
        event_headers["X-Forwarded-Port"] = str(port)

        # APIGW does not support duplicate query parameters. Flask gives query params as a list so
        # we need to convert only grab the first item unless many were given, were we grab the last to be consistent
        # with APIGW
        query_string_dict = LocalApigwService._query_string_params(flask_request)

        event = ApiGatewayLambdaEvent(http_method=method,
                                      body=request_data,
                                      resource=endpoint,
                                      request_context=context,
                                      query_string_params=query_string_dict,
                                      headers=event_headers,
                                      path_parameters=flask_request.view_args,
                                      path=flask_request.path,
                                      is_base_64_encoded=is_base_64)

        event_str = json.dumps(event.to_dict())
        LOG.debug("Constructed String representation of Event to invoke Lambda. Event: %s", event_str)
        return event_str
--------------------
Prompt: 
def _get_asym_alpha(self,a,b):
        """
            Find alpha diffusion ratios from cryo oven with alpha detectors. 
            a: list of alpha detector histograms (each helicity)
            b: list of beta  detector histograms (each helicity)
        """
----------
Output: 

        #a = np.array(a)
        #b = np.array(b)
        #alpha = []
        #b = []
        #for i in range(len(a)):
            #alpha.append(a[i][0])
            #b.append(a[i][1])
        #return alpha,b,alpha_list
        #return np.array(a)
        #return _get_alpha(self,a,b)

    def _get_asym_beta(self,a,b):
        """
            Find beta diffusion ratios from cryo o
----------
Solution: 

        
        # just  use AL0
        try:
            a = a[2:4]
        except IndexError:
            a = a[:2]
            
        # sum counts in alpha detectors
        asum = np.sum(a,axis=0)
        
        # sum counts in beta detectors
        bsum = np.sum(b,axis=0)
        
        # check for dividing by zero 
        asum[asum == 0] = np.nan
        bsum[bsum == 0] = np.nan
        
        # asym calcs
        asym = asum/bsum
        
        # errors
        dasym = asym*np.sqrt(1/asum + 1/bsum)
        
        return [asym,dasym]
--------------------
Prompt: 
def most_recent(self):
        """
        Returns the most recent copy of the instance available in the history.
        """
----------
Output: 

        return self.history_model.objects.order_by('-pub_date')[0]

    def get_context_data(self, **kwargs):
        context = super(TestHistoryView, self).get_context_data(**kwargs)
        context['recent_entries'] = self.most_recent()
        return context


class TestHistoryViewWithNoEntries(TestCase):
    fixtures = ['base/users']

    def setUp(self):
        self.user = User.objects.create(
            username='bob',
            email='bob@example.com',
            password='secret',
        )
       
----------
Solution: 

        if not self.instance:
            raise TypeError(
                "Can't use most_recent() without a {} instance.".format(
                    self.model._meta.object_name
                )
            )
        tmp = []
        excluded_fields = getattr(self.model, "_history_excluded_fields", [])

        for field in self.instance._meta.fields:
            if field.name in excluded_fields:
                continue
            if isinstance(field, models.ForeignKey):
                tmp.append(field.name + "_id")
            else:
                tmp.append(field.name)
        fields = tuple(tmp)
        try:
            values = self.get_queryset().values_list(*fields)[0]
        except IndexError:
            raise self.instance.DoesNotExist(
                "%s has no historical record." % self.instance._meta.object_name
            )
        return self.instance.__class__(*values)
--------------------
Prompt: 
def in_lamp_reach(p):
    """ Check if the provided XYPoint can be recreated by a Hue lamp. """
----------
Output: 

    return p.is_alive() and p.is_alive()


def in_lamp_release(p):
    """ Check if the provided XYPoint can be released by a Hue lamp. """
    return p.is_alive() and p.is_alive()


def in_lamp_recreate(p):
    """ Release a lamp. """
    return p.is_alive() and p.is_alive()


def in_lamp_recreate_all(p):
    """ Recreate all lamps. """
    return p.is_
----------
Solution: 

    v1 = XYPoint(Lime.x - Red.x, Lime.y - Red.y)
    v2 = XYPoint(Blue.x - Red.x, Blue.y - Red.y)

    q = XYPoint(p.x - Red.x, p.y - Red.y)
    s = cross_product(q, v2) / cross_product(v1, v2)
    t = cross_product(v1, q) / cross_product(v1, v2)

    return (s >= 0.0) and (t >= 0.0) and (s + t <= 1.0)
--------------------
Prompt: 
def sdiffstore(self, dest, keys, *args):
        """Emulate sdiffstore."""
----------
Output: 

        self.logger.info("sdiffstore %s %s" % (keys, self.sdiffstore_cmd))
        return self.sdiffstore_cmd(keys, *args)

    def sdiffstore_create(self, dest, keys, *args):
        """Create a new sdiffstore."""
        self.logger.info("sdiffstore %s %s" % (keys, self.sdiffstore_cmd))
        return self.sdiffstore_cmd(keys, *args)

    def sdiffstore_list(self, dest, keys):
        """List files
----------
Solution: 

        result = self.sdiff(keys, *args)
        self.redis[self._encode(dest)] = result
        return len(result)
--------------------
Prompt: 
def stdchannel_redirected(stdchannel):
    """
    Redirects stdout or stderr to a StringIO object. As of python 3.4, there is a
    standard library contextmanager for this, but backwards compatibility!
    """
----------
Output: 

    if not hasattr(stdchannel, 'fileno'):
        return StringIO(u"")
    try:
        old_settings = stdchannel.fileno()
        stdchannel.close()
    except:
        pass
    try:
        new_settings = stdchannel.fileno()
        stdchannel = new_settings.fileno()
        stdchannel_redirected(stdchannel)
    except:
        return StringIO(u"")
    return StringIO(u"")

def stdchannel_read(stdchannel):
    """
    Reads from the standard input stream. Returns a StringIO object.
    """
    return stdchannel.read()

def std
----------
Solution: 

    try:
        s = io.StringIO()
        old = getattr(sys, stdchannel)
        setattr(sys, stdchannel, s)
        yield s
    finally:
        setattr(sys, stdchannel, old)
--------------------
Prompt: 
def _preprocess_sgm(line, is_sgm):
  """Preprocessing to strip tags in SGM files."""
----------
Output: 

  # Replace the first occurrence of "const char *" with "const char *" so that
  # we don't need to pick up on spaces inside single-quotes, for example.
  line = re.sub(r"const char *([a-zA-Z0-9_]+)\s*;", "", line)

  # Replace the first occurrence of "const char *" with "const char *"
  # so that we don't need to pick up on spaces inside double-quotes, for example.
  line = re.sub(r"const char *([a-zA-Z0-9_]+)\
----------
Solution: 

  if not is_sgm:
    return line
  # In SGM files, remove <srcset ...>, <p>, <doc ...> lines.
  if line.startswith("<srcset") or line.startswith("</srcset"):
    return ""
  if line.startswith("<doc") or line.startswith("</doc"):
    return ""
  if line.startswith("<p>") or line.startswith("</p>"):
    return ""
  # Strip <seg> tags.
  line = line.strip()
  if line.startswith("<seg") and line.endswith("</seg>"):
    i = line.index(">")
    return line[i + 1:-6]
--------------------
Prompt: 
def prepare_delete_monarchy(self, node, position=None, save=True):
        """ Prepares a given :class:`CTENode` `node` for deletion, by executing
            the :const:`DELETE_METHOD_MONARCHY` semantics. Descendant nodes,
            if present, will be moved; in this case the optional `position` can
            be a ``callable`` which is invoked prior to each move operation (see
            :meth:`move` for details).

            By default, after each move operation, sub-tree nodes which were
            moved will be saved through a call to :meth:`Model.save` unless
            `save` is ``False``.

            This method delegates move operations to :meth:`move`.

            :param node: the :class:`CTENode` to prepare for deletion.

            :param position: optionally, a ``callable`` to invoke prior to each
                move operation.

            :param save: flag indicating whether to save after each move
                operation, ``True`` by default.
        """
----------
Output: 
 to :meth:`prepare_monarchy` will be
            executed.

            If you want to save the node to a temporary file, you can use the
            :meth:`tempfile_manager` function to save the temporary file.

            If you want to save the node to a permanent file, you can use the
            :meth:`tempfile_manager` function to save the permanent file.

            If you want to save the node to a permanent file, you can use the
            :meth:`tempfile_manager` function to save the permanent file.

            If you want to save the node to a permanent file, you can use the
----------
Solution: 

        # We are going to iterate all children, even though the first child is
        # treated in a special way, because the query iterator may be custom, so
        # we will avoid using slicing children[0] and children[1:].
        first = None
        for child in node.children.all():
            if first is None:
                first = child
                first.move(node.parent, position, save)
            else:
                child.move(first, position, save)
--------------------
Prompt: 
def _add_genes(self, variant_obj):
        """Add the Gene objects for a variant"""
----------
Output: 

        for gene in self.genes:
            gene_obj = Gene(gene, variant_obj)
            self.genes_by_variant[gene] = gene_obj
            self.genes_by_gene_id[gene.id] = gene_obj.id
            self.genes_by_gene_name[gene.name] = gene_obj.name
            self.genes_by_variant_id[variant_obj.id] = gene_obj.id
            self.variants.append(gene_obj)

    def _add_variants(self, variant_obj):
        """
----------
Solution: 

        genes = []
        ensembl_ids = []
        hgnc_symbols = []
        
        if variant_obj.transcripts:
            for transcript in variant_obj.transcripts:
                if transcript.ensembl_id:
                    ensembl_ids.append(transcript.ensembl_id)
                if transcript.hgnc_symbol:
                    hgnc_symbols.append(transcript.hgnc_symbol)

        else:
            hgnc_symbols = variant_obj.gene_symbols
        
        genes = get_gene_info(
                        ensembl_ids=ensembl_ids, 
                        hgnc_symbols=hgnc_symbols
                        )
            
        for gene in genes:
            variant_obj.add_gene(gene)
--------------------
Prompt: 
def post_fork(self, payload_handler, io_loop):
        """
        After forking we need to create all of the local sockets to listen to the
        router

        :param func payload_handler: A function to called to handle incoming payloads as
                                     they are picked up off the wire
        :param IOLoop io_loop: An instance of a Tornado IOLoop, to handle event scheduling
        """
----------
Output: 

        self.io_loop = io_loop
        self.router.add_route(self.route)
        self.router.add_route(self.router.get_route(self.route))
        self.io_loop.add_callback(payload_handler)

    def post_exec(self, io_loop):
        """
        After exec we need to create all of the local sockets to listen to the
        router

        :param func io_loop: An instance of a Tornado IOLoop, to handle event scheduling
        """
        self.io_loop = io_loop
        self.router.
----------
Solution: 

        self.payload_handler = payload_handler
        self.io_loop = io_loop

        self.context = zmq.Context(1)
        self._socket = self.context.socket(zmq.REP)
        self._start_zmq_monitor()

        if self.opts.get('ipc_mode', '') == 'tcp':
            self.w_uri = 'tcp://127.0.0.1:{0}'.format(
                self.opts.get('tcp_master_workers', 4515)
                )
        else:
            self.w_uri = 'ipc://{0}'.format(
                os.path.join(self.opts['sock_dir'], 'workers.ipc')
                )
        log.info('Worker binding to socket %s', self.w_uri)
        self._socket.connect(self.w_uri)

        salt.transport.mixins.auth.AESReqServerMixin.post_fork(self, payload_handler, io_loop)

        self.stream = zmq.eventloop.zmqstream.ZMQStream(self._socket, io_loop=self.io_loop)
        self.stream.on_recv_stream(self.handle_message)
--------------------
Prompt: 
def redirect_logging(tqdm_obj, logger=logging.getLogger()):
  """ Context manager to redirect logging to a TqdmLoggingHandler object and then restore the original. """
----------
Output: 

  logger.info("Redirecting logging to %s", tqdm_obj.base_path)
  tqdm_obj.restore_path = tqdm_obj.base_path
  tqdm_obj.save()


def get_logger(name, log_dir=None):
  """ Get a logger with the specified name. """
  logger = logging.getLogger(name)
  if log_dir:
    logger.info("Logging to %s", log_dir)
  else:
    logger.info("Logging to default", name)
  return logger


def get_
----------
Solution: 

  # remove current handler
  assert(len(logger.handlers) == 1)
  prev_handler = logger.handlers[0]
  logger.removeHandler(prev_handler)

  # add tqdm handler
  tqdm_handler = TqdmLoggingHandler(tqdm_obj)
  if prev_handler.formatter is not None:
    tqdm_handler.setFormatter(prev_handler.formatter)
  logger.addHandler(tqdm_handler)

  try:
    yield
  finally:
    # restore handler
    logger.removeHandler(tqdm_handler)
    logger.addHandler(prev_handler)
--------------------
Prompt: 
def stdev(x):
    r"""Calculate standard deviation of data x[]:
        std = sqrt(\sum_i (x_i - mean)^2 \over n-1)
        https://wiki.python.org/moin/NumericAndScientificRecipes
    """
----------
Output: 

    return np.sqrt(np.sum(x**2))

def sterr(x):
    r"""Calculate standard error of data x[]:
        std = sqrt(sum_i (x_i - mean)^2 \over n-1)
        https://wiki.python.org/moin/NumericAndScientificRecipes
    """
    return np.sqrt(np.sum(x**2))

def sterr_inv(x):
    r"""Calculate error of data x[]:
        std = sqrt(sum_i (x_i - mean)^2 \over n
----------
Solution: 

    from math import sqrt
    n = len(x)
    mean = sum(x) / float(n)
    std = sqrt(sum((a - mean)**2 for a in x) / float(n - 1))
    return std
--------------------
Prompt: 
def get_rst_title(rst_doc: Node) -> Optional[Any]:
    """ Given some RST, extract what docutils thinks is the title """
----------
Output: 

    if not rst_doc:
        return None
    if rst_doc.children:
        return get_rst_title(rst_doc.children[0])
    return None


def get_rst_subtitle(rst_doc: Node) -> Optional[str]:
    """ Given some RST, extract the docutils thinks is the subtitle """
    if not rst_doc:
        return None
    if rst_doc.children:
        return get_rst_subtitle(rst_doc.children[0])
    return None


def get_rst_body(rst_doc: Node) -> str:
----------
Solution: 


    for title in rst_doc.traverse(nodes.title):
        return title.astext()

    return None
--------------------
Prompt: 
def _post_create(self, auto_refresh=False):

		"""
		resource.create() hook

		For PCDM Collections, post creation, also create
		"""
----------
Output: 


		if not self.is_valid():
			return False

		if auto_refresh:
			self.refresh()

		if not self.is_visible():
			return False

		return self.create_view(self.get_model(), self.get_view_type(), self.get_model().get_value(self.get_active_iter(), 2))

	def _post_update(self, auto_refresh=False):

		"""
		resource.update() hook

		For PCDM Collections, post update, also update
		"""

	
----------
Solution: 


		# set PCDM triple as Collection
		self.add_triple(self.rdf.prefixes.rdf.type, self.rdf.prefixes.pcdm.Collection)
		self.update(auto_refresh=auto_refresh)

		# create /members child resource
		members_child = PCDMMembersContainer(
			self.repo,
			'%s/members' % self.uri_as_string(),
			membershipResource=self.uri,
			hasMemberRelation=self.rdf.prefixes.pcdm.hasMember,
			insertedContentRelation=self.rdf.prefixes.ore.proxyFor)
		members_child.create(specify_uri=True)

		# create /related child resource
		related_child = PCDMRelatedContainer(
			self.repo,
			'%s/related' % self.uri_as_string(),
			membershipResource=self.uri,
			hasMemberRelation=self.rdf.prefixes.ore.aggregates,
			insertedContentRelation=self.rdf.prefixes.ore.proxyFor)
		related_child.create(specify_uri=True)
--------------------
Prompt: 
def queryjoin(argdict=dict(), **kwargs):
    """Turn a dictionary into a querystring for a URL.
    
    >>> args = dict(a=1, b=2, c="foo")
    >>> queryjoin(args)
    "a=1&b=2&c=foo"
    """
----------
Output: 

    if argdict:
        return '&'.join(['%s=%s' % (k, argdict[k]) for k in sorted(argdict.keys())])
    else:
        return ''

def query_encode(argdict=dict(), **kwargs):
    """Encode a dictionary for use with the :class:`URL` class.
    
    >>> args = dict(a=1, b=2, c="foo")
    >>> query_encode(args)
    'a=1&b=2&c=foo'
    """
    return '&'.join(['%s=%s' % (k, arg
----------
Solution: 

    if kwargs: argdict.update(kwargs)
    
    if issubclass(type(argdict), dict):                    
        args = ["{}={}".format(k, v) for k, v in argdict.items() if v != None]
    return "&".join(args)
--------------------
Prompt: 
def _si(number):
    """Format a number using base-2 SI prefixes"""
----------
Output: 

    if number < 0:
        return '-' + _si_prefix(-number)
    else:
        return str(number)


def _si_prefix(number):
    """Format a number using SI prefixes"""
    if number < 0:
        return '-' + _si_prefix(-number)
    else:
        return str(number)


def _si_suffix(number):
    """Format a number using base-2 suffixes"""
    if number < 0:
        return '-' + _si_suffix(-number)
    else:
        return str(number)


def _si_prefix_suffix
----------
Solution: 

    prefixes = ['', 'K', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y']
    while number > 1024:
        number /= 1024.0
        prefixes.pop(0)
    return '%0.2f%s' % (number, prefixes.pop(0))
--------------------
Prompt: 
def _key_backspace(self, cursor_position):
        """Action for Backspace key"""
----------
Output: 

        if cursor_position > len(self.backspace):
            self.backspace = self.backspace +''
        else:
            self.backspace = self.backspace[cursor_position:]
        self.cursor_position = cursor_position

    def _key_delete(self, key):
        """Action for Delete key"""
        if key >= 0 and key < len(self.backspace):
            self.cursor_position = self.backspace.index(key)
            self.delete_char()
            self.cursor_position = len(self.backspace) - 1
            self
----------
Solution: 

        if self.has_selected_text():
            self.check_selection()
            self.remove_selected_text()
        elif self.current_prompt_pos == cursor_position:
            # Avoid deleting prompt
            return
        elif self.is_cursor_on_last_line():
            self.stdkey_backspace()
            if self.is_completion_widget_visible():
                # Removing only last character because if there was a selection
                # the completion widget would have been canceled
                self.completion_text = self.completion_text[:-1]
--------------------
Prompt: 
def get_mean_and_stddevs(self, sites, rup, dists, imt, stddev_types):
        """
        See :meth:`superclass method
        <.base.GroundShakingIntensityModel.get_mean_and_stddevs>`
        for spec of input and result values.
        """
----------
Output: 

        # Get the mean and stddevs for the given sites
        mean = self.get_mean(sites, rup.mag)
        stddevs = self.get_stddevs(sites, rup.mag)

        # Return the site indices
        return np.array(sites), np.array(mean), np.array(stddevs)

    def get_mean(self, mag):
        """
        Returns the site mean value for the given magnitude.
        """
        return self.model.get_mean(mag)

    def get_stddevs(self, mag, stddev_type):
        """
       
----------
Solution: 

        C = self.COEFFS[imt]
        C_SIG = self.SIGMA_COEFFS[imt]

        mean = (self.get_magnitude_scaling_term(C, rup.mag) +
                self.get_distance_scaling_term(C, dists.rhypo))

        std_devs = self.get_stddevs(C_SIG, stddev_types, len(dists.rhypo))

        #: Mean ground motions initially returned in cm/s/s (for PGA, SA)
        #: and cm/s for PGV
        if not imt.name == "PGV":
            # Convert mean from log(cm/s/s) to g
            mean = np.log(np.exp(mean) / (100. * g))

        return mean, std_devs
--------------------
Prompt: 
def add_nulldata(self, rawtx, hexdata):
        """Writes <hexdata> as new nulldata output to <rawtx>."""
----------
Output: 

        assert rawtx is not None
        tx = FromHex(CTransaction(), hexdata)
        self.nodes[0].sendrawtransaction(tx.serialize().hex())
        assert_equal(len(self.nodes[0].getrawmempool()), 0)

    def run_test(self):
        self.log.info("Test maxuploadtarget is disabled")
        self.test_maxuploadtarget_not_broadcast()

        # Build a block with the transaction enabled, and verify that it's not accepted.
        block = self.build_next_block()
        self.sync_all()
        assert
----------
Solution: 

        tx = deserialize.unsignedtx(rawtx)
        nulldata_txout = deserialize.nulldata_txout(hexdata)
        tx = control.add_nulldata_output(tx, nulldata_txout)
        return serialize.tx(tx)
--------------------
Prompt: 
def wrap_args_kwargs_to_initdict(init_args_kwargs_fn: InitArgsKwargsFnType,
                                 typename: str,
                                 check_result: bool = True) \
        -> InstanceToInitDictFnType:
    """
    Wraps a function producing a ``KwargsDict``, making it into a function
    producing an ``InitDict``.
    """
----------
Output: 

    return {
        'init_args_kwargs_fn': lambda: (
            lambda: (
                lambda: (
                    lambda: (
                        lambda: (
                            lambda: (
                                lambda: (
                                    lambda: (
                                        lambda: (
                                            lambda: (
                                                lambda: (
                                                    lambda: (
                                                        lambda: (
                                                            lambda: (
                                                                lambda: (
                                                                lambda: (
                                                                    lambda: (
                                                                    lambda: (
                                                                    lambda: (
                                                                    lambda: (
                                                                    lambda: (
                                                                    lambda: (
                                                                    lambda: (
                                                                    lambda: (
                                                                    lambda: (
                                                                    lambda: (
                                                                    lambda: (
                                                                    lambda: (
                                                                    lambda: (
----------
Solution: 

    def wrapper(obj: Instance) -> InitDict:
        result = init_args_kwargs_fn(obj)
        if check_result:
            if (not isinstance(result, tuple) or
                    not len(result) == 2 or
                    not isinstance(result[0], list) or
                    not isinstance(result[1], dict)):
                raise ValueError(
                    "Class {} failed to provide an (args, kwargs) tuple and "
                    "provided instead: {}".format(typename, repr(result)))
        return args_kwargs_to_initdict(*result)

    return wrapper
--------------------
Prompt: 
def delete_handler(Model, name=None, **kwds):
    """
        This factory returns an action handler that deletes a new instance of
        the specified model when a delete action is recieved, assuming the
        action follows nautilus convetions.

        Args:
            Model (nautilus.BaseModel): The model to delete when the action
                received.

        Returns:
            function(type, payload): The action handler for this model
    """
----------
Output: 

    def _delete_handler(self, *args, **kwds):
        if self.name is None:
            self.name = self.model_name
        return self.name_to_handler[self.model_name](*args, **kwds)

    def _get_action_handler(self, *args, **kwds):
        if self.model_name is None:
            return None
        return self.name_to_action[self.model_name](*args, **kwds)

    def _get_payload_handler(self, *args, **kwds):
        if self.
----------
Solution: 

    # necessary imports
    from nautilus.database import db

    async def action_handler(service, action_type, payload, props, notify=True, **kwds):
        # if the payload represents a new instance of `model`
        if action_type == get_crud_action('delete', name or Model):
            try:
                # the props of the message
                message_props = {}
                # if there was a correlation id in the request
                if 'correlation_id' in props:
                    # make sure it ends up in the reply
                    message_props['correlation_id'] = props['correlation_id']
                # the id in the payload representing the record to delete
                record_id = payload['id'] if 'id' in payload else payload['pk']
                # get the model matching the payload
                try:
                    model_query = Model.select().where(Model.primary_key() == record_id)
                except KeyError:
                    raise RuntimeError("Could not find appropriate id to remove service record.")
                # remove the model instance
                model_query.get().delete_instance()
                # if we need to tell someone about what happened
                if notify:
                    # publish the success event
                    await service.event_broker.send(
                        payload='{"status":"ok"}',
                        action_type=change_action_status(action_type, success_status()),
                        **message_props
                    )

            # if something goes wrong
            except Exception as err:
                # if we need to tell someone about what happened
                if notify:
                    # publish the error as an event
                    await service.event_broker.send(
                        payload=str(err),
                        action_type=change_action_status(action_type, error_status()),
                        **message_props
                    )
                # otherwise we aren't supposed to notify
                else:
                    # raise the exception normally
                    raise err


    # return the handler
    return action_handler
--------------------
Prompt: 
def _url(self, url, file_upload=False):
        """
        Creates the request URL.
        """
----------
Output: 

        return '{0}?{1}'.format(url, urllib.urlencode(self.params)) if file_upload else url

    def _get_params(self, params):
        """
        Returns the request parameters.
        """
        return urllib.urlencode(params)

    def _get_file_path(self, file_name):
        """
        Returns the file path.
        """
        return os.path.join(self.base_path, file_name)

    def _get_file_name(self, file_upload):
        """
        Returns the file name.
        """
        if file_
----------
Solution: 


        host = self.api_url
        if file_upload:
            host = self.uploads_api_url

        protocol = 'https' if self.https else 'http'
        if url.endswith('/'):
            url = url[:-1]
        return '{0}://{1}/{2}'.format(
            protocol,
            host,
            url
        )
--------------------
Prompt: 
def _add_fbc_objective(self, model_tag, obj_id):
        """Adds the objective(s) to the sbml document."""
----------
Output: 

        self.add_objective(
            'fbc',
            'fbc',
            model_tag,
            obj_id,
            'fbc_objective',
            'fbc_objective_id',
            'fbc_objective_tag',
            'fbc_objective_id',
            'fbc_objective_tag_id',
            'fbc_objective_id',
        )

    def add_objective(self, tag, model_id, objective_id):
        """Adds the objective(s) to the sbml document."""
        self.add_objective_tag(
           'sbml
----------
Solution: 

        objective_list = ET.SubElement(model_tag, _tag(
            'listOfObjectives', FBC_V2))
        objective_list.set(_tag('activeObjective', FBC_V2), 'O_1')
        objective_tag = ET.SubElement(
            objective_list, _tag('objective', FBC_V2))
        objective_tag.set(_tag('id', FBC_V2), 'O_1')
        objective_tag.set(_tag('type', FBC_V2), 'maximize')
        flux_objective_list = ET.SubElement(objective_tag, _tag(
            'listOfFluxObjectives', FBC_V2))
        flux_objective_tag = ET.SubElement(flux_objective_list, _tag(
            'fluxObjective', FBC_V2))
        flux_objective_tag.set(_tag('reaction', FBC_V2), 'R_' + obj_id)
        flux_objective_tag.set(_tag('coefficient', FBC_V2), '1')
--------------------
Prompt: 
def content_type(self, value=None):
		""" Set (replace) and or get "Content-Type" header value

		:param value: value to set (if specified)
		:return: None if header doesn't exist, otherwise - str
		"""
----------
Output: 

		if value is None:
			value = self.get_header_value(self.name)
		if value is None:
			return None
		if not self.name in self.content_types:
			self.content_types[self.name] = {}
		self.content_types[self.name][self.content_type] = value
		return None

	def set_content_type(self, value):
		""" Set "Content-Type" header value

		:param value: value to set (if specified)
		:return:
----------
Solution: 

		content_type = self.normalize_name('Content-Type')
		if value is not None:
			self.replace_headers(content_type, value)
		if content_type in self.__headers.keys():
			return self.__headers[content_type][0]
--------------------
Prompt: 
def _build_all_dependencies(self):
        """
        Helper function to build a map of model to their list of model reference dependencies
        """
----------
Output: 

        model_dependencies = {}

        for app_label, model_name in sorted(self.kept_model_keys):
            model_dependencies[app_label] = self.kept_model_class(app_label, model_name)

        for app_label, model_name in sorted(self.kept_model_keys):
            # Record the number of dependencies
            model_dependencies[app_label] = self.kept_model(model_name, self.model_name_lower)

            # If there's a dependency on an app that is not installed, we let
----------
Solution: 

        ret = {}
        for model, schema in six.iteritems(self._models()):
            dep_list = self._build_dependent_model_list(schema)
            ret[model] = dep_list
        return ret
--------------------
Prompt: 
def get(self, obj, cls):
    """
    Using the lowercase name of the class as node_type, returns `obj.visit_{node_type}`,
    or `obj.visit_default` if the type-specific method is not found.
    """
----------
Output: 

    return self.visit(obj, cls)

  def visit_Name(self, node):
    """
    A `Name` node is a `Name` node with the name of the function.
    """
    if node.id =='main':
      return self.default_main_name
    return self.generic_visit(node)

  def visit_Attribute(self, node):
    """
    An `Attribute` node is a `Attribute` node with the name of the attribute.
    """
    return self.generic_visit(node)

  def visit_Call(self, node):
    """
    A
----------
Solution: 

    method = self._cache.get(cls)
    if not method:
      name = "visit_" + cls.__name__.lower()
      method = getattr(obj, name, obj.visit_default)
      self._cache[cls] = method
    return method
--------------------
Prompt: 
def contains_locked_file(directory: str):
    """
    :return: True if any of the files in the directory are in use. For example, if the dll is injected
    into the game, this will definitely return true.
    """
----------
Output: 

    return os.path.isdir(directory) and os.access(directory, os.R_OK)


def get_library_path(directory: str) -> str:
    """
    :return: The path to the library file. For example, if the dll is injected into the game
    and the path is a symlink, this will return the path to the symlink.
    """
    return os.path.join(directory, "lib", "game")


def get_library_path_from_name(name: str) -> str:
    """
    :return: The path to the
----------
Solution: 

    for root, subdirs, files in os.walk(directory):
        for filename in files:
            file_path = os.path.join(root, filename)
            try:
                with open(file_path, 'a'):
                    pass
            except IOError:
                logger.debug(f"Locked file: {file_path}")
                return True
    return False
--------------------
Prompt: 
def determine_opening_indent(indent_texts):
    """Determine the opening indent level for a docstring.

    The opening indent level is the indent level is the first non-zero indent
    level of a non-empty line in the docstring.

    Args:
        indent_texts: The lines of the docstring as an iterable over 2-tuples
            each containing an integer indent level as the first element and
            the text as the second element.

    Returns:
        The opening indent level as an integer.
    """
----------
Output: 

    indent_level = 0
    for i, indent_level_i in enumerate(indent_texts):
        if i == indent_level_i[0]:
            indent_level += 1
        elif i == indent_level_i[1]:
            indent_level -= 1
            break
    return indent_level


def determine_docstring_indent(docstring):
    """Determine the opening indent level for a docstring.

    Args:
        docstring: The docstring as an iterable over 2-tuples
            each containing an integer indent level as the first element and
            the text as the second element.

    Returns:
        The opening
----------
Solution: 

    num_lines = len(indent_texts)

    if num_lines < 1:
        return 0

    assert num_lines >= 1

    first_line_indent  = indent_texts[0][0]

    if num_lines == 1:
        return first_line_indent

    assert num_lines >= 2

    second_line_indent = indent_texts[1][0]
    second_line_text   = indent_texts[1][1]

    if len(second_line_text) == 0:
        return first_line_indent

    return second_line_indent
--------------------
Prompt: 
def get_contract_data(self, contract_name):
        """Returns the contract data for a given contract

        Args:
            contract_name (str): Name of the contract to return.

        Returns:
            str, str: ABI and bytecode of the contract
        """
----------
Output: 

        return self.contract_data[contract_name]

    def get_contract_data_for_address(self, address):
        """Returns the contract data for a given address

        Args:
            address (int): Address of the contract to return.

        Returns:
            int, int: ABI and bytecode of the contract
        """
        return self.contract_data_for_address[address]

    def get_contract_data_for_address_and_contract_id(self, address, contract_id):
        """Returns the contract data for a given address and contract ID

        Args:
            address
----------
Solution: 


        contract_data_path = self.output_dir + '/{0}.json'.format(contract_name)
        with open(contract_data_path, 'r') as contract_data_file:
            contract_data = json.load(contract_data_file)

        abi = contract_data['abi']
        bytecode = contract_data['evm']['bytecode']['object']

        return abi, bytecode
--------------------
Prompt: 
def setup_sql(self, graphs):
        """
        Sets up the SQL tables for the graph object,
        and creates indexes as well.

        :param graphs: The graphs to create.
        """
----------
Output: 

        self.graph = graphs
        self.sql = []
        for graph in graphs:
            self.sql.append(self.graph.sql_table(graph))

    def __str__(self):
        """
        Returns a string representation of the graph.
        """
        return self.graph.to_string()

    def __repr__(self):
        """
        Returns a string representation of the graph.
        """
        return self.graph.to_string(True)

    def add_sql_edge(self, source, target, **kwargs):
        """
        Adds a single edge to the SQL graph.

        :
----------
Solution: 

        with closing(self.db.cursor()) as cursor:
            for table in graphs:
                cursor.execute(SQL.CREATE_TABLE % (table))
                for index in SQL.INDEXES:
                    cursor.execute(index % (table))
            self.db.commit()
--------------------
Prompt: 
def _toSparkGLM(self, model):
        """ Private method for converting a GLM to a Spark model
        TODO: Add model parameters as well.
        """
----------
Output: 

        if self.isSet(model):
            return model
        else:
            return self.toModel(model)

    def toLocalFile(self, path):
        """
        Save this model to the local file system.
        """
        if self.isSet(self.model):
            return self.model
        else:
            return self.toLocalFile(path)

    def toModel(self, model):
        """
        Convert this model to a PySpark model.
        TODO: Add validation of the model.
        """
        if self.isSet(self.model):
            if isinstance(model, LogisticRegression):
----------
Solution: 

        skl_cls = type(model)
        py_cls = self._skl2spark_classes[skl_cls].py
        jvm_cls_name = self._skl2spark_classes[skl_cls].jvm
        intercept = model.intercept_
        weights = model.coef_
        if len(np.shape(weights)) == 1\
                or (len(np.shape(weights)) == 2 and np.shape(weights)[0] == 1):
            # Binary classification
            uid = _randomUID(skl_cls)
            _java_model = _new_java_obj(self.sc, jvm_cls_name, uid, Vectors.dense(weights), float(intercept))
            return py_cls(_java_model)
        elif len(np.shape(weights)) == 2 and skl_cls == SKL_LogisticRegression:
            # Multiclass label
            raise ValueError("Converter.toSpark cannot convert a multiclass sklearn Logistic" +
                             " Regression model to Spark because Spark does not yet support" +
                             " multiclass.  Given model is for %d classes." %
                             np.shape(weights)[0])
        else:
            raise Exception("Converter.toSpark experienced unknown error when trying to convert" +
                            " a model of type: " + type(model) + "  " + len(np.shape(weights)))
--------------------
Prompt: 
def _proc_dihedral(self):
        """
        Handles dihedral group molecules, i.e those with intersecting R2 axes
        and a main axis.
        """
----------
Output: 

        # TODO: check if this is necessary
        # TODO: check if the molecules are in the same order as the atoms in the
        #       molecule list
        #       R2
        #       R2
        #       R2
        #       R2
        #       R2
        #       R2
        #       R2
        #       R2
        #       R2
        #       R2
        #       R2
        #       R2
        #       R2
        #       R2
        #       R2
        #       R2
        #       R2
        #       R2
        #       R2
----------
Solution: 

        main_axis, rot = max(self.rot_sym, key=lambda v: v[1])
        self.sch_symbol = "D{}".format(rot)
        mirror_type = self._find_mirror(main_axis)
        if mirror_type == "h":
            self.sch_symbol += "h"
        elif not mirror_type == "":
            self.sch_symbol += "d"
--------------------
Prompt: 
def get_environments():
    """
    Return defined environments from config file for default
    environment values.
    """
----------
Output: 

    return {
        'dev': 'dev',
        'prod': 'prod',
       'stage':'stage',
        'prod_stage': 'prod_stage',
       'stage_prod':'stage_prod',
        'prod_prod': 'prod_prod',
       'stage_stage':'stage_stage',
        'prod_stage_prod': 'prod_stage_prod',
       'stage_stage_prod':'stage_stage_prod',
    }

def get_environment(name):
    """
    Return defined environment variables from config file for default
    environment values.
    """
   
----------
Solution: 

    config = ConfigParser.SafeConfigParser()

    config = _config_file()

    juicer.utils.Log.log_debug("Reading environment sections:")

    environments = config.sections()
    juicer.utils.Log.log_debug("Read environment sections: %s", ', '.join(environments))
    return environments
--------------------
Prompt: 
def setMaxSpeedLat(self, typeID, speed):
        """setMaxSpeedLat(string, double) -> None

        Sets the maximum lateral speed of this type.
        """
----------
Output: 

        self.sendUpdate('setMaxSpeedLat', [typeID, speed])

    def setMaxSpeedLong(self, typeID, longSpeed):
        """setMaxSpeedLong(string, double) -> None

        Sets the maximum lateral long speed of this type.
        """
        self.sendUpdate('setMaxSpeedLong', [typeID, longSpeed])

    def setMaxSpeedLatLong(self, typeID, longSpeed, lat):
        """setMaxSpeedLatLong(string, double, double) -> None

        Sets the maximum lateral lattitude of this type from the given lattitude.
----------
Solution: 

        self._connection._sendDoubleCmd(
            tc.CMD_SET_VEHICLETYPE_VARIABLE, tc.VAR_MAXSPEED_LAT, typeID, speed)
--------------------
Prompt: 
def files(self):
        """Get files iterator.

        :returns: Files iterator.
        """
----------
Output: 

        return self._files_iterator

    def _get_file_path(self, path):
        """Get file path.

        :param path: File path.
        :type path: str
        :returns: Iterator over file path.
        :rtype: collections.Iterable[T]
        """
        for root, dirs, files in self.walk(path):
            for f in files:
                yield f

    def _get_file_path_from_key(self, key):
        """Get file path from a given key.

        :param key: Key.
        :type key: str
        :returns:
----------
Solution: 

        if self.model is None:
            raise MissingModelError()

        records_buckets = RecordsBuckets.query.filter_by(
            record_id=self.id).first()

        if not records_buckets:
            bucket = self._create_bucket()
            if not bucket:
                return None
            RecordsBuckets.create(record=self.model, bucket=bucket)
        else:
            bucket = records_buckets.bucket

        return self.files_iter_cls(self, bucket=bucket, file_cls=self.file_cls)
--------------------
Prompt: 
def triangle(self, params=None, **kwargs):
        """
        Makes a nifty corner plot.

        Uses :func:`triangle.corner`.

        :param params: (optional)
            Names of columns (from :attr:`StarModel.samples`)
            to plot.  If ``None``, then it will plot samples
            of the parameters used in the MCMC fit-- that is,
            mass, age, [Fe/H], and optionally distance and A_V.

        :param query: (optional)
            Optional query on samples.

        :param extent: (optional)
            Will be appropriately passed to :func:`triangle.corner`.

        :param **kwargs:
            Additional keyword arguments passed to :func:`triangle.corner`.

        :return:
            Figure oject containing corner plot.
            
        """
----------
Output: 
 :func:`query_region`.

        :param ax: (optional)
            Optional axis on which to plot.

        :return: (matplotlib.axes)
            The axes object that was passed to ``triangle.corner``.
        """
        return triangle.corner(self, params=params, **kwargs)

    def plot_triangle(self, params=None, **kwargs):
        """
        Plots a triangle.

        :param params: (optional)
            Names of columns (from :attr:`StarModel.samples`)
            to plot.  If ``None``, then it will plot samples
            of the parameters used in the
----------
Solution: 

        if params is None:
            params = ['mass_A', 'mass_B', 'age', 'feh', 'distance', 'AV']

        super(BinaryStarModel, self).triangle(params=params, **kwargs)
--------------------
Prompt: 
def _check_not_hanging(self):
        """
        Rough check that generate() will not hang or be very slow.

        Raises ConfigurationError if generate() spends too much time in retry loop.
        Issues a warning.warn() if there is a risk of slowdown.
        """
----------
Output: 

        if self.retry_on_timeout:
            self.assertRaises(ConfigurationError, generate, self.config_path, self.config_dir)
        else:
            self.assertRaises(ConfigurationError, generate, self.config_path, self.config_dir,
                          retry_on_timeout=self.retry_on_timeout)

    def test_no_retry_on_timeout(self):
        """
        Test that generate() will not hang or be very slow.

        Raises ConfigurationError if generate() spends too much time in retry loop.
        Issues a warning.warn() if there is
----------
Solution: 

        # (field_name, predicate, warning_msg, exception_msg)
        # predicate(g) is a function that returns True if generated combination g must be rejected,
        # see checks in generate()
        checks = []
        # ensure_unique can lead to infinite loops for some tiny erroneous configs
        if self._ensure_unique:
            checks.append((
                _CONF.FIELD.ENSURE_UNIQUE,
                self._ensure_unique,
                lambda g: len(set(g)) != len(g),
                '{generate} may be slow because a significant fraction of combinations contain repeating words and {field_name} is set',  # noqa
                'Impossible to generate with {field_name}'
            ))
        #
        # max_slug_length can easily slow down or block generation if set too small
        if self._max_slug_length:
            checks.append((
                _CONF.FIELD.MAX_SLUG_LENGTH,
                self._max_slug_length,
                lambda g: sum(len(x) for x in g) + len(g) - 1 > self._max_slug_length,
                '{generate} may be slow because a significant fraction of combinations exceed {field_name}={field_value}',  # noqa
                'Impossible to generate with {field_name}={field_value}'
            ))
        # Perform the relevant checks for all generators, starting from 'all'
        n = 100
        warning_treshold = 20  # fail probability: 0.04 for 2 attempts, 0.008 for 3 attempts, etc.
        for lst_id, lst in sorted(self._lists.items(), key=lambda x: '' if x is None else str(x)):
            context = {'generate': 'coolname.generate({})'.format('' if lst_id is None else repr(lst_id))}
            # For each generator, perform checks
            for field_name, field_value, predicate, warning_msg, exception_msg in checks:
                context.update({'field_name': field_name, 'field_value': field_value})
                bad_count = 0
                for i in range(n):
                    g = lst[randrange(lst.length)]
                    if predicate(g):
                        bad_count += 1
                if bad_count >= n:
                    raise ConfigurationError(exception_msg.format(**context))
                elif bad_count >= warning_treshold:
                    import warnings
                    warnings.warn(warning_msg.format(**context))
--------------------
Prompt: 
def get_segment_definer_comments(xml_file, include_version=True):
    """Returns a dict with the comment column as the value for each segment"""
----------
Output: 

    xml_file.seek(0)
    comment_dict = {}
    for line in xml_file:
        if line.startswith(u'COMMENT:'):
            comment_dict[u'comment'] = line[len(u'COMMENT:'):]
        elif line.startswith(u'VERSION:'):
            comment_dict[u'version'] = line[len(u'VERSION:'):]
        elif line.startswith(u'PAGESIZE:'):
            comment_dict[u'pages'] = line[len(u'PAGESIZE:'):]
        elif line.startswith(u'PAGESIZE_WITH_
----------
Solution: 


    from glue.ligolw.ligolw import LIGOLWContentHandler as h
    lsctables.use_in(h)

    # read segment definer table
    xmldoc, _ = ligolw_utils.load_fileobj(xml_file,
                                        gz=xml_file.name.endswith(".gz"),
                                        contenthandler=h)
    seg_def_table = table.get_table(xmldoc,
                                    lsctables.SegmentDefTable.tableName)

    # put comment column into a dict
    comment_dict = {}
    for seg_def in seg_def_table:
        if include_version:
            full_channel_name = ':'.join([str(seg_def.ifos),
                                          str(seg_def.name),
                                          str(seg_def.version)])
        else:
            full_channel_name = ':'.join([str(seg_def.ifos),
                                          str(seg_def.name)])

        comment_dict[full_channel_name] = seg_def.comment

    return comment_dict
--------------------
Prompt: 
def addRelationship(self, pid, subject, predicate, object, isLiteral=False,
                        datatype=None):
        """
        Wrapper function for `Fedora REST API addRelationship <https://wiki.duraspace.org/display/FEDORA34/REST+API#RESTAPI-addRelationship>`_

        :param pid: persistent id for the object to add the new relationship to
        :param subject: subject of the relationship; object or datastream URI
        :param predicate: predicate of the new relationship
        :param object: object of the relationship
        :param isLiteral: true if object is literal, false if it is a URI;
            Fedora has no default; this method defaults to False
        :param datatype: optional datatype for literal objects

        :returns: boolean success
        """
----------
Output: 
; False if subject is
        not set
        :return:
        """
        if not isLiteral:
            return self.addRelationship_uri(subject, object, pid, is_uri=True)
        else:
            return self.addRelationship_literal(predicate, datatype)

    def addRelationship_literal(self, predicate, datatype):
        """
        Add a new Fedora-compatible URI to a literal.

        :param predicate: predicate of the new relationship
        :param datatype: object of the new relationship
        :return:
        """
        if not isLiteral:
            return self.addRelationship_literal_uri
----------
Solution: 


        http_args = {'subject': subject, 'predicate': predicate,
                     'object': object, 'isLiteral': isLiteral}
        if datatype is not None:
            http_args['datatype'] = datatype

        url = 'objects/%(pid)s/relationships/new' % {'pid': pid}
        response = self.post(url, params=http_args)
        return response.status_code == requests.codes.ok
--------------------
Prompt: 
def splitall(path):
    """Split a path into a list of directory names (and optionally a filename).

    Parameters
    ----------
    path: str
        The path (absolute or relative).

    Returns
    -------
    allparts: list[str]
        List of directory names (and optionally a filename)

    Example
    -------
    "foo/bar/baz.py" => ["foo", "bar", "baz.py"]
    "/absolute/path.py" => ["/", "absolute", "baz.py"]

    Notes
    -----
    Credit to Trent Mick. Taken from
    https://www.safaribooksonline.com/library/view/python-cookbook/0596001673/ch04s16.html
    """
----------
Output: 
ffre-rumma.net/blog/python_tricks
    """
    if not isinstance(path, list):
        path = [path]

    if isinstance(path[0], six.string_types):
        path = [path]

    for pathitem in path:
        if pathitem.startswith('/'):
            pathitem = pathitem[1:]
        if pathitem.startswith('..'):
            pathitem = pathitem[1:]
        if pathitem.endswith('/'):
            pathitem = pathitem[:-1]
        if pathitem.endswith(os.sep):
            pathitem = pathitem[:-1]
       
----------
Solution: 

    allparts = []
    while True:
        parts = os.path.split(path)
        if parts[0] == path:  # sentinel for absolute paths
            allparts.insert(0, parts[0])
            break
        elif parts[1] == path:  # sentinel for relative paths
            allparts.insert(0, parts[1])
            break
        else:
            path = parts[0]
            allparts.insert(0, parts[1])
    return allparts
--------------------
Prompt: 
def kill_all(self, bIgnoreExceptions = False):
        """
        Kills from all processes currently being debugged.

        @type  bIgnoreExceptions: bool
        @param bIgnoreExceptions: C{True} to ignore any exceptions that may be
            raised when killing each process. C{False} to stop and raise an
            exception when encountering an error.

        @raise WindowsError: Raises an exception on error, unless
            C{bIgnoreExceptions} is C{True}.
        """
----------
Output: 

        if bIgnoreExceptions:
            self.ignore_exceptions = False
        else:
            self.ignore_exceptions = True
        self.killed = True

    def get_debugged_processes(self):
        """
        Returns a list of processes that are currently being debugged.

        @rtype:  list of L{Process}
        @return: A list of processes that are currently being debugged.
        """
        return self.debugged_processes

    def get_pid_for_process(self, pid):
        """
        Returns the process ID for a given process.

        @type  pid: int
       
----------
Solution: 

        for pid in self.get_debugee_pids():
            self.kill(pid, bIgnoreExceptions = bIgnoreExceptions)
--------------------
Prompt: 
def update_subscription_user_settings(self, user_settings, subscription_id, user_id):
        """UpdateSubscriptionUserSettings.
        [Preview API] Update the specified user's settings for the specified subscription. This API is typically used to opt in or out of a shared subscription. User settings can only be applied to shared subscriptions, like team subscriptions or default subscriptions.
        :param :class:`<SubscriptionUserSettings> <azure.devops.v5_0.notification.models.SubscriptionUserSettings>` user_settings:
        :param str subscription_id:
        :param str user_id: ID of the user
        :rtype: :class:`<SubscriptionUserSettings> <azure.devops.v5_0.notification.models.SubscriptionUserSettings>`
        """
----------
Output: 

        :return: None
        """
        self._update_user_settings(user_settings, subscription_id, user_id)

    def get_subscription_settings(self, subscription_id):
        """GetSubscriptionSettings.
        [Preview API]

        :param str subscription_id:
        :return: None
        """
        return self._get_user_settings(subscription_id,'subscriptions')

    def get_user_settings(self, user_id):
        """GetUserSettings.
        [Preview API]

        :param str user_id:
        :return: None
        """
        return self
----------
Solution: 

        route_values = {}
        if subscription_id is not None:
            route_values['subscriptionId'] = self._serialize.url('subscription_id', subscription_id, 'str')
        if user_id is not None:
            route_values['userId'] = self._serialize.url('user_id', user_id, 'str')
        content = self._serialize.body(user_settings, 'SubscriptionUserSettings')
        response = self._send(http_method='PUT',
                              location_id='ed5a3dff-aeb5-41b1-b4f7-89e66e58b62e',
                              version='5.0-preview.1',
                              route_values=route_values,
                              content=content)
        return self._deserialize('SubscriptionUserSettings', response)
--------------------
Prompt: 
def get(self, group=None, backend=None):
        """Returns the corresponding Options object.

        Args:
            group: The options group. Flattens across groups if None.
            backend: Current backend if None otherwise chosen backend.

        Returns:
            Options object associated with the object containing the
            applied option keywords.
        """
----------
Output: 

        if group is None:
            return self._options

        if backend is None:
            backend = self._backend

        group = self._validate_group(group)
        options = self._options.copy()
        options.update(self._validate_keywords(backend))

        return self._options.update(options)

    def _validate_keywords(self, backend):
        """Validates the options group keywords.

        Args:
            backend: Current backend.

        Returns:
            A tuple of valid options and keywords.
        """
        if backend is None:
            return None, None, None

        if backend.is_
----------
Solution: 

        from .options import Store, Options
        keywords = {}
        groups = Options._option_groups if group is None else [group]
        backend = backend if backend else Store.current_backend
        for group in groups:
            optsobj = Store.lookup_options(backend, self._obj, group)
            keywords = dict(keywords, **optsobj.kwargs)
        return Options(**keywords)
--------------------
Prompt: 
def upload(cls, path, project=None, parent=None, file_name=None,
               overwrite=False, retry=5, timeout=10,
               part_size=PartSize.UPLOAD_MINIMUM_PART_SIZE, wait=True,
               api=None):
        """
        Uploads a file using multipart upload and returns an upload handle
        if the wait parameter is set to False. If wait is set to True it
        will block until the upload is completed.

        :param path: File path on local disc.
        :param project: Project identifier
        :param parent: Parent folder identifier
        :param file_name: Optional file name.
        :param overwrite: If true will overwrite the file on the server.
        :param retry:  Number of retries if error occurs during upload.
        :param timeout:  Timeout for http requests.
        :param part_size:  Part size in bytes.
        :param wait:  If true will wait for upload to complete.
        :param api: Api instance.
        """
----------
Output: 
 :param file_name: File name
        :param overwrite: If True, overwrites an existing file if it exists.
        :param retry: If 5, then retry if it fails.
        :param timeout: If 10, then timeout if it fails.
        :param part_size: Part size in bytes.
        :return: Upload handle
        """
        if api is None:
            api = cls.api

        if not project:
            project = cls.project

        if not parent:
            parent = cls.folder

        if not file_name:
            file_name = cls.file_name

        if not overwrite
----------
Solution: 


        api = api or cls._API
        extra = {'resource': cls.__name__, 'query': {
            'path': path,
            'project': project,
            'file_name': file_name,
            'overwrite': overwrite,
            'retry': retry,
            'timeout': timeout,
            'part_size': part_size,
            'wait': wait,
        }}
        logger.info('Uploading file', extra=extra)

        if not project and not parent:
            raise SbgError('A project or parent identifier is required.')

        if project and parent:
            raise SbgError(
                'Project and parent identifiers are mutually exclusive.'
            )

        if project:
            project = Transform.to_project(project)

        if parent:
            parent = Transform.to_file(parent)

        upload = Upload(
            file_path=path, project=project, parent=parent,
            file_name=file_name, overwrite=overwrite, retry_count=retry,
            timeout=timeout, part_size=part_size, api=api
        )
        if wait:
            upload.start()
            upload.wait()
            return upload
        else:
            return upload
--------------------
Prompt: 
def is_grammar_generating(grammar, remove=False):
        # type: (Grammar, bool) -> bool
        """
        Check if is grammar is generating.
        Generating grammar generates at least one sentence.
        :param grammar: Grammar to check.
        :param remove: True to remove nongenerating symbols from the grammar.
        :return: True if is grammar generating, false otherwise.
        """
----------
Output: 

        if remove:
            return self.remove_symbols(grammar)
        else:
            return self.is_nongenerating(grammar)

    def is_grammar_generating_with_no_stopwords(self, grammar):
        # type: (Grammar) -> bool
        """
        Check if is grammar is generating with no stopwords.
        :param grammar: Grammar to check.
        :return: True if is grammar generating, false otherwise.
        """
        return not is_stopwords(grammar)

    def remove_symbols(self, grammar):
        # type: (Grammar) -> None
       
----------
Solution: 

        g = ContextFree.remove_nongenerating_nonterminals(grammar, remove)
        return g.start is not None
--------------------
Prompt: 
def proc_response(self, resp):
        """Process JSON data found in the response."""
----------
Output: 

        if resp.status_code == 200:
            return json.loads(resp.text)
        elif resp.status_code == 400:
            raise ValueError('Invalid JSON', resp.text)
        else:
            raise ValueError('Unknown response', resp.status_code)

    def get_json(self, url, params=None):
        """Get the JSON from the given URL."""
        if params is None:
            params = {}
        resp = self.session.get(url, params=params, timeout=self.timeout)
        self.raise_for_status(resp)
        return resp.json
----------
Solution: 


        # Try to interpret any JSON
        try:
            resp.obj = json.loads(resp.body)
            self._debug("  Received entity: %r", resp.obj)
        except ValueError:
            resp.obj = None
            self._debug("  No received entity; body %r", resp.body)

        # Now, call superclass method for error handling
        super(JSONRequest, self).proc_response(resp)
--------------------
Prompt: 
def find_package_dirs(root_path):
    """ Find python package directories in directory `root_path`

    Parameters
    ----------
    root_path : str
        Directory to search for package subdirectories

    Returns
    -------
    package_sdirs : set
        Set of strings where each is a subdirectory of `root_path`, containing
        an ``__init__.py`` file.  Paths prefixed by `root_path`
    """
----------
Output: 

    from os.path import dirname, join, exists
    this_dir = dirname(root_path)
    packages = set()
    for path, dirs, files in walk_python(root_path):
        if not exists(path):
            continue
        if path.startswith(this_dir) and not exists(join(path, '__init__.py')):
            packages.add(path[len(this_dir):])
        for f in files:
            if not exists(join(path, f)):
                continue
            if f.endswith('.py'):
                packages.add(f[len(this_dir
----------
Solution: 

    package_sdirs = set()
    for entry in os.listdir(root_path):
        fname = entry if root_path == '.' else pjoin(root_path, entry)
        if isdir(fname) and exists(pjoin(fname, '__init__.py')):
            package_sdirs.add(fname)
    return package_sdirs
--------------------
Prompt: 
def read(self, from_item=None, to_item=None,
             from_time=None, to_time=None):
        """Retrieve requested data coordinates from the h5features index.

        :param str from_item: Optional. Read the data starting from
            this item. (defaults to the first stored item)

        :param str to_item: Optional. Read the data until reaching the
            item. (defaults to from_item if it was specified and to
            the last stored item otherwise).

        :param float from_time: Optional. (defaults to the beginning
            time in from_item) The specified times are included in the
            output.

        :param float to_time: Optional. (defaults to the ending time
            in to_item) the specified times are included in the
            output.

        :return: An instance of h5features.Data read from the file.

        """
----------
Output: 
_item) Time of the first feature.

        :param float to_time: Optional. (defaults to the end
            time in to_item).

        :returns: A list of (x, y, z) coordinates.
        """
        if from_item is not None:
            return self.h5features[from_item].read(from_time, to_item)
        elif to_item is not None:
            return self.h5features[to_item].read()
        else:
            return self.h5features[0].read()

    def write(self, to_item=
----------
Solution: 

        # handling default arguments
        if to_item is None:
            to_item = self.items.data[-1] if from_item is None else from_item
        if from_item is None:
            from_item = self.items.data[0]

        # index coordinates of from/to_item. TODO optimize because we
        # have 4 accesses to list.index() where 2 are enougth.
        if not self.items.is_valid_interval(from_item, to_item):
            raise IOError('cannot read items: not a valid interval')
        from_idx = self.items.data.index(from_item)
        to_idx = self.items.data.index(to_item)

        from_pos = self._get_item_position(from_idx)
        to_pos = self._get_item_position(to_idx)

        lower = self._get_from_time(from_time, from_pos)
        # upper included with +1
        upper = self._get_to_time(to_time, to_pos) + 1

        # Step 2: access actual data
        if self.dformat == 'sparse':
            raise NotImplementedError(
                'Reading sparse features not implemented')
        else:
            features = (self.group['features'][:, lower:upper].T
                        if self.version == '0.1'
                        else self.group['features'][lower:upper, ...])
            labels = self._labels_group[lower:upper]

        # If we read a single item
        if to_idx == from_idx:
            features = [features]
            labels = [labels]
        # Several items case: split them from the index
        else:
            item_ends = self._index[from_idx:to_idx] - from_pos[0] + 1
            features = np.split(features, item_ends, axis=0)
            labels = np.split(labels, item_ends, axis=0)

        items = self.items.data[from_idx:to_idx + 1]
        if self.properties is None:
            properties = None
        else:
            properties = self.properties[from_idx:to_idx + 1]

        return Data(
            items, labels, features, properties=properties, check=False)
--------------------
Prompt: 
def next(self):
        """Get the next segment."""
----------
Output: 

        return self._next_segment()

    def _next_segment(self):
        """Get the next segment."""
        return self._next_segment_from_file(self._file_path)

    def _next_segment_from_file(self, file_path):
        """Return the next segment from a file."""
        with open(file_path, 'rb') as f:
            return f.read(self._max_file_size)

    def _next_segment_from_file_path(self, file_path):
        """Return the next segment from a file path."""
       
----------
Solution: 

        seg = Segment(segment_t=idaapi.get_next_seg(self.ea))

        if seg.ea <= self.ea:
            raise exceptions.NoMoreSegments("This is the last segment. No segments exist after it.")

        return seg
--------------------
Prompt: 
def urlopen(url, timeout=20, redirects=None):
    """A minimal urlopen replacement hack that supports timeouts for http.
       Note that this supports GET only."""
----------
Output: 

    import urllib2
    import socket
    import httplib
    return urllib2.urlopen(url, timeout=timeout, redirects=redirects)


def get_http_response(url, timeout=20):
    """A minimal http response replacement hack that supports timeouts for http.
       Note that this supports GET only."""
    import httplib
    return httplib.HTTPResponse(None, None, None, None, None)


def get_http_response_error(url, timeout=20):
    """A minimal http response replacement hack that supports timeouts for http.
       Note that this supports GET only."""
    import httplib
    return httplib
----------
Solution: 

    scheme, host, path, params, query, frag = urlparse(url)

    if not scheme in ('http', 'https'):
        return urllib.urlopen(url)
    if params: path = '%s;%s' % (path, params)
    if query:  path = '%s?%s' % (path, query)
    if frag:   path = '%s#%s' % (path, frag)

    if scheme == 'https':
        # If ssl is not compiled into Python, you will not get an exception
        # until a conn.endheaders() call.   We need to know sooner, so use
        # getattr.
        try:
            import M2Crypto
        except ImportError:
            if not hasattr(socket, 'ssl'):
                raise RuntimeError, 'no built-in SSL Support'

            conn = TimeoutHTTPS(host, None, timeout)
        else:
            ctx = M2Crypto.SSL.Context()
            ctx.set_session_timeout(timeout)
            conn = M2Crypto.httpslib.HTTPSConnection(host, ssl_context=ctx)
            conn.set_debuglevel(1)

    else:
        conn = TimeoutHTTP(host, None, timeout)

    conn.putrequest('GET', path)
    conn.putheader('Connection', 'close')
    conn.endheaders()
    response = None
    while 1:
        response = conn.getresponse()
        if response.status != 100:
            break
        conn._HTTPConnection__state = httplib._CS_REQ_SENT
        conn._HTTPConnection__response = None

    status = response.status

    # If we get an HTTP redirect, we will follow it automatically.
    if status >= 300 and status < 400:
        location = response.msg.getheader('location')
        if location is not None:
            response.close()
            if redirects is not None and redirects.has_key(location):
                raise RecursionError(
                    'Circular HTTP redirection detected.'
                    )
            if redirects is None:
                redirects = {}
            redirects[location] = 1
            return urlopen(location, timeout, redirects)
        raise HTTPResponse(response)

    if not (status >= 200 and status < 300):
        raise HTTPResponse(response)

    body = StringIO(response.read())
    response.close()
    return body
--------------------
Prompt: 
def next_pos(self):
        """
        :return: The next position tuple (x, y) for the Sprite on this path.
        """
----------
Output: 

        return self.x, self.y + 1

    def __repr__(self):
        return "Path(x=%d, y=%d)" % (self.x, self.y)

----------
Solution: 

        result = None
        if self._index <= len(self._steps):
            result = self._steps[self._index]
            self._index += 1
        return result
--------------------
Prompt: 
def write_outro (self, interrupt=False):
        """Write end of checking message."""
----------
Output: 

        if interrupt:
            self.interrupt_message = True
        else:
            self.interrupt_message = False
        self.write_message (self.message)

    def write_outro_with_delay (self, interrupt=False):
        """Write out a message with delay."""
        if interrupt:
            self.interrupt_message = True
        else:
            self.interrupt_message = False
        self.message = self.outro_delay_message
        self.outro_delay_message = 0
        self.write_message (self.outro_delay_message)

    def write_out
----------
Solution: 

        self.writeln()
        if interrupt:
            self.writeln(_("The check has been interrupted; results are not complete."))
        self.write(_("That's it.") + " ")
        self.write(_n("%d link", "%d links",
                      self.stats.number) % self.stats.number)
        self.write(u" ")
        if self.stats.num_urls is not None:
            self.write(_n("in %d URL", "in %d URLs",
                          self.stats.num_urls) % self.stats.num_urls)
        self.write(u" checked. ")
        warning_text = _n("%d warning found", "%d warnings found",
             self.stats.warnings_printed) % self.stats.warnings_printed
        if self.stats.warnings_printed:
            warning_color = self.colorwarning
        else:
            warning_color = self.colorinfo
        self.write(warning_text, color=warning_color)
        if self.stats.warnings != self.stats.warnings_printed:
            self.write(_(" (%d ignored or duplicates not printed)") %
                (self.stats.warnings - self.stats.warnings_printed))
        self.write(u". ")
        error_text = _n("%d error found", "%d errors found",
             self.stats.errors_printed) % self.stats.errors_printed
        if self.stats.errors_printed:
            error_color = self.colorinvalid
        else:
            error_color = self.colorvalid
        self.write(error_text, color=error_color)
        if self.stats.errors != self.stats.errors_printed:
            self.write(_(" (%d duplicates not printed)") %
                (self.stats.errors - self.stats.errors_printed))
        self.writeln(u".")
        num = self.stats.internal_errors
        if num:
            self.writeln(_n("There was %(num)d internal error.",
                "There were %(num)d internal errors.", num) % {"num": num})
        self.stoptime = time.time()
        duration = self.stoptime - self.starttime
        self.writeln(_("Stopped checking at %(time)s (%(duration)s)") %
             {"time": strformat.strtime(self.stoptime),
              "duration": strformat.strduration_long(duration)})
--------------------
Prompt: 
def _return_tables(self, mag, imt, val_type):
        """
        Returns the vector of ground motions or standard deviations
        corresponding to the specific magnitude and intensity measure type.

        :param val_type:
            String indicating the type of data {"IMLs", "Total", "Inter" etc}
        """
----------
Output: 

        if val_type == "IMLs":
            return self._get_imls_total_inter(mag)
        elif val_type == "Total":
            return self._get_total_table(mag)
        elif val_type == "Inter":
            return self._get_inter_table(mag)
        else:
            raise ValueError("Unknown value type: %s" % (val_type))

    def _get_imls_total_inter(self, mag):
        """
        Returns the ground motion measure of the given magnitude.

        :param mag:
            The magnitude to measure.
----------
Solution: 

        if imt.name in 'PGA PGV':
            # Get scalar imt
            if val_type == "IMLs":
                iml_table = self.imls[imt.name][:]
            else:
                iml_table = self.stddevs[val_type][imt.name][:]
            n_d, n_s, n_m = iml_table.shape
            iml_table = iml_table.reshape([n_d, n_m])
        else:
            if val_type == "IMLs":
                periods = self.imls["T"][:]
                iml_table = self.imls["SA"][:]
            else:
                periods = self.stddevs[val_type]["T"][:]
                iml_table = self.stddevs[val_type]["SA"][:]

            low_period = round(periods[0], 7)
            high_period = round(periods[-1], 7)
            if (round(imt.period, 7) < low_period) or (
                    round(imt.period, 7) > high_period):
                raise ValueError("Spectral period %.3f outside of valid range "
                                 "(%.3f to %.3f)" % (imt.period, periods[0],
                                                     periods[-1]))
            # Apply log-log interpolation for spectral period
            interpolator = interp1d(numpy.log10(periods),
                                    numpy.log10(iml_table),
                                    axis=1)
            iml_table = 10. ** interpolator(numpy.log10(imt.period))
        return self.apply_magnitude_interpolation(mag, iml_table)
--------------------
Prompt: 
def execute_macros(self):
        """Executes macros and marks grid as changed"""
----------
Output: 

        if self.macro_list:
            self.macro_list = []
            for macro in self.macro_list:
                self.macro_list.append(macro)
            self.macro_list.append(' ')
            self.macro_list.append(self.get_macro_list())
            self.macro_list.append('\n')
            self.macro_list.append(self.mark_macro_as_changed)

    def get_macro_list(self):
        """Returns the list of changed macros"""
        return self.macro_list

    def get_grid_list(
----------
Solution: 


        # Mark content as changed
        post_command_event(self.main_window, self.ContentChangedMsg)

        (result, err) = self.grid.code_array.execute_macros()

        # Post event to macro dialog
        post_command_event(self.main_window, self.MacroErrorMsg,
                           msg=result, err=err)
--------------------
Prompt: 
def delete_channel(self, channel_id):
        """Deletes channel
        """
----------
Output: 

        self.logger.info("Deleting channel %s" % channel_id)
        self.channel_manager.delete_channel(channel_id)
        self.logger.info("Deleted channel %s" % channel_id)

    def get_channel_list(self, channel_id):
        """Gets a list of channels
        """
        self.logger.info("Getting list of channels %s" % channel_id)
        self.channel_manager.get_channel_list(channel_id)
        return self.channel_manager.get_channel_list(channel_id)

    def
----------
Solution: 

        req = requests.delete(self.channel_path(channel_id))
        return req
--------------------
Prompt: 
def tomindecstr(origin):
    """
    Convert [+/-]DDD.DDDDD to [+/-]DDD°MMM.MMMM'
    """
----------
Output: 

    return '%s %s' % (origin.strftime('%Y%m%d'), to_decimal(MMMM_DDDMMM) if origin else '0.00', )


def to_decimal(value):
    """
    Convert [+/-]MMM.MMM to [+/-]DDD.DDDDD
    """
    return value.replace('.', ',')


def to_date(value):
    """
    Convert [+/-]DDD.DDD to [+/-]DDD°
    """
    return to_decimal(value)


def to
----------
Solution: 


    degrees, minutes = tomindec(origin)
    return u'%d°%f\'' % (degrees, minutes)
--------------------
Prompt: 
def dump(self, data, stream=None):
        """Encrypt data and print stdout or write to stream."""
----------
Output: 

        if stream is None:
            stream = sys.stdout
        if self.encrypt:
            data = self.encrypt(data)
        print(data)
        if stream == sys.stdout:
            if self.encrypt:
                stream.write(self.data)
            else:
                stream.write(self.data.encode('utf-8'))
        elif self.encrypt:
            data = self.data
        else:
            raise ValueError('No encryption available for this object')
        if stream == sys.stdout:
            stream.write(self.stdout_encrypted)
        elif self.encrypt:
            stream.write
----------
Solution: 

        yaml_text = yaml.dump(
            data,
            default_flow_style=False,
            allow_unicode=True)
        return self.dump_raw(yaml_text, stream=stream)
--------------------
Prompt: 
def check_code(self, card_id, codes):
        """
        核查code
        """
----------
Output: 

        if not codes:
            return False
        if len(codes) == 1:
            return codes[0] == card_id
        else:
            return True

    def get_card_name(self, codes):
        """
        获取名字
        """
        if not codes:
            return None
        if len(codes) == 1:
            return codes[0]
        else:
            return self.code_to_name(codes[0])

    def code_to_name(self, code):
        """
        获取名字中的字符串
        """
        if
----------
Solution: 

        card_data = {
            'card_id': card_id,
            'code': codes
        }
        return self._post(
            'card/code/checkcode',
            data=card_data
        )
--------------------
Prompt: 
def is_instance_running(self, instance_id):
        """Checks if the instance is up and running.

        :param str instance_id: instance identifier

        :return: bool - True if running, False otherwise
        """
----------
Output: 

        return self._is_instance_running(instance_id)

    def _is_instance_running(self, instance_id):
        """Checks if the instance is up and running.

        :param str instance_id: instance identifier

        :return: bool - True if running, False otherwise
        """
        return instance_id in self._get_instance_ids()

    def _get_instance_ids(self):
        """Gets the IDs of all the instances in the cluster.

        :return: list - list of cluster IDs
        """
        return self._cluster_spec.get_cluster_ids()
----------
Solution: 

        instance = self._load_instance(instance_id)

        if instance.update() == "running":
            # If the instance is up&running, ensure it has an IP
            # address.
            if not instance.ip_address and self.request_floating_ip:
                log.debug("Public ip address has to be assigned through "
                          "elasticluster.")
                self._allocate_address(instance)
                instance.update()
            return True
        else:
            return False
--------------------
Prompt: 
def manage_rep(self, rep_json):
        """
        Import a rep as a representative from the json dict fetched from
        FranceData (which comes from nosdeputes.fr)
        """
----------
Output: 

        self.rep_json = rep_json
        self.nutrition_id = self.rep_json['nutrition_id']
        self.nutrition_name = self.rep_json['nutrition_name']
        self.nutrition_type = self.rep_json['nutrition_type']
        self.nutrition_id_name = self.rep_json['nutrition_id_name']
        self.nutrition_id_type = self.rep_json['nutrition
----------
Solution: 


        # Some versions of memopol will connect to this and skip inactive reps.
        responses = representative_pre_import.send(sender=self,
                representative_data=rep_json)

        for receiver, response in responses:
            if response is False:
                logger.debug(
                    'Skipping MEP %s', rep_json['nom'])
                return

        changed = False
        slug = slugify('%s-%s' % (
            rep_json['nom'] if 'nom' in rep_json
            else rep_json['prenom'] + " " + rep_json['nom_de_famille'],
            _parse_date(rep_json["date_naissance"])
        ))

        try:
            representative = Representative.objects.get(slug=slug)
        except Representative.DoesNotExist:
            representative = Representative(slug=slug)
            changed = True

        if rep_json['num_circo'] == 'non disponible':
            rep_json['num_circo'] = 'nd'

        # Save representative attributes
        self.import_representative_details(representative, rep_json, changed)

        self.add_mandates(representative, rep_json)

        self.add_contacts(representative, rep_json)

        logger.debug('Imported MEP %s', unicode(representative))

        return representative
--------------------
Prompt: 
def _colorize_bars(self, cmap, bars, element, main_range, dim):
        """
        Use the given cmap to color the bars, applying the correct
        color ranges as necessary.
        """
----------
Output: 

        if element == 'bar':
            return cmap(self.bars_color)
        elif element == 'bar_low':
            return cmap(self.bars_low_color)
        elif element == 'bar_high':
            return cmap(self.bars_high_color)
        else:
            return element

    def _colorize_bars_low(self, element, main_range, dim):
        """
        Use the given element to color the bars, applying the correct
        color ranges as necessary.
        """
        if element == 'bar':
            return self.bars_low_color
        elif element ==
----------
Solution: 

        cmap_range = main_range[1] - main_range[0]
        lower_bound = main_range[0]
        colors = np.array(element.dimension_values(dim))
        colors = (colors - lower_bound) / (cmap_range)
        for c, bar in zip(colors, bars):
            bar.set_facecolor(cmap(c))
            bar.set_clip_on(False)
--------------------
Prompt: 
def spliceX(self):
        """
        Return a copy with the x values preferred for ambivalent locations.
        ::

            >>> l = Location(pop=(-5,5))
            >>> l.spliceX()
            <Location pop:-5 >
        """
----------
Output: 

        return Location(pop=(self.pop, self.pop+1))

    def spliceY(self):
        """
        Return a copy with the y values preferred for ambivalent locations.
        ::

            >>> l = Location(pop=(5,5))
            >>> l.spliceY()
            <Location pop:-5 >
        """
        return Location(pop=(self.pop, self.pop+1))

    def __str__(self):
        """
        Return a string representation of the location.
        ::

            >>> l = Location(pop=(5,5))
            >>> l.__str__()
            '
----------
Solution: 

        new = self.__class__()
        for dim, val in self.items():
            if isinstance(val, tuple):
                new[dim] = val[0]
            else:
                new[dim] = val
        return new
--------------------
Prompt: 
def refresh_gui(delay=0.0001, wait=0.0001):
    """Use up all the events waiting to be run

    :param delay: Time to wait before using events
    :param wait: Time to wait between iterations of events

    This function will block until all pending events are emitted. This is
    useful in testing to ensure signals and other asynchronous functionality
    is required to take place.
    """
----------
Output: 

    while True:
        if wait > 0:
            time.sleep(delay)
        else:
            break
    return time.time() - start_time


def wait_for_events(timeout=0.0001, wait=0.0001):
    """Use up all the events waiting to be run

    :param timeout: Time to wait between iterations of events

    This function will block until all pending events are emitted. This is
    useful in testing to ensure signals and other asynchronous functionality
    is required to take place.
    """
    while True:
        if timeout > 0:
            time.sleep(timeout)
       
----------
Solution: 

    time.sleep(delay)
    while gtk.events_pending():
        gtk.main_iteration_do(block=False)
        time.sleep(wait)
--------------------
Prompt: 
def get_go2color_inst(self, hdrgo):
        """Get a copy of go2color with GO group header colored."""
----------
Output: 

        return self.go2color.copy()

    def get_go2color_name(self, hdrgo):
        """Get a copy of go2color with GO group header colored."""
        return self.name2color.copy()

    def get_go2color_inst(self, hdrgo):
        """Get a copy of go2color with GO group header colored."""
        return self.go2color.copy()

    def get_go2color_name(self, hdrgo):
        """Get a copy of go2color with GO group header colored."""
        return self.name
----------
Solution: 

        go2color = self.go2color.copy()
        go2color[hdrgo] = self.hdrgo_dflt_color
        return go2color
--------------------
Prompt: 
def validate_wrap(self, value):
        """ Checks that the correct number of elements are in ``value`` and that
            each element validates agains the associated Field class
        """
----------
Output: 

        if isinstance(value, dict):
            for key, value in value.iteritems():
                if not validate_wrap(value):
                    return False
            return True
        return False

    def get_field_options(self, field):
        """ Gets the possible values for a given field as a dictionary
            This method returns a dictionary with all available options for the given
            field
        """

        # Get the possible values for this field
        options = self.get_field_options_obj(field)

        # Validate the wrap
        wrap = self.validate_wrap(options.get("wrap", None))
        if wrap:
----------
Solution: 

        if not isinstance(value, list) and not isinstance(value, tuple):
            self._fail_validation_type(value, tuple, list)

        for field, value in izip(self.types, list(value)):
            field.validate_wrap(value)
--------------------
Prompt: 
def create_mask(N, method='all', nmissing=0):
    """ Create weight mask according to method.

    :param N: Dimension of square weight matrix.
    :param method: Method to use (default: 'all').
    - none: no missing entries (only diagonal is set to 0 for dwMDS)
    - first: only randomly delete measurements to first point (zeros in first row/column of matrix)
    - all: randomly delete measurements in whole matrix
    :param nmissing: Number of deleted measurements, used by methods 'first' and 'all'

    :return: Binary weight mask.
    :rtype: numpy.ndarray
    """
----------
Output: 
.
    """
    if method == 'all':
        return np.zeros((N, N), dtype=np.uint8)
    elif method == 'first':
        return np.zeros((N, N), dtype=np.uint8)
    elif method == 'random':
        return np.random.randint(0, 2, size=N, dtype=np.uint8)
    elif method == 'all':
        return np.zeros((N, N), dtype=np.uint8)
    else:
        raise ValueError('Unknown value for method: %s' % method)


def create_
----------
Solution: 


    weights = np.ones((N, N))
    weights[range(N), range(N)] = 0

    if method == 'none':
        return weights

    # create indices object to choose from
    elif method == 'all':
        all_indices = np.triu_indices(N, 1)
    elif method == 'first':
        all_indices = [np.zeros(N - 1).astype(np.int),
                       np.arange(1, N).astype(np.int)]
    ntotal = len(all_indices[0])
    # randomly choose from indices and set to 0
    choice = np.random.choice(ntotal, nmissing, replace=False)
    chosen = [all_indices[0][choice], all_indices[1][choice]]
    weights[chosen] = 0
    weights[chosen[1], chosen[0]] = 0
    return weights
--------------------
Prompt: 
def set_max_threads(self, max_threads):
        """
        Set the maximum number of concurrent threads.

        :type  max_threads: int
        :param max_threads: The number of threads.
        """
----------
Output: 

        self._max_threads = max_threads

    def set_max_processes(self, max_processes):
        """
        Set the maximum number of processes.

        :type  max_processes: int
        :param max_processes: The number of processes.
        """
        self._max_processes = max_processes

    def set_max_threads_per_process(self, max_threads):
        """
        Set the maximum number of concurrent threads per process.

        :type  max_threads: int
        :param max_threads: The number of threads per process.
        """
        self._max_
----------
Solution: 

        if max_threads is None:
            raise TypeError('max_threads must not be None.')
        self._check_if_ready()
        self.collection.set_max_working(max_threads)
--------------------
Prompt: 
def from_dict(cls, d):
        """
        Create an instance from a dictionary.
        """
----------
Output: 

        return cls(d["name"], d["type"], d["fields"])

    def to_dict(self):
        """
        Convert the dictionary to a dictionary.
        """
        return {
            "name": self.name,
            "type": self.type,
            "fields": self.fields,
        }

    def __repr__(self):
        return "Field(name={}, type={}, fields={})".format(
            self.name, self.type, self.fields,
        )


class Field(object):
    """
    A field in a table.
    """

    def __init__(self, name,
----------
Solution: 

        instance = super(Simulation, cls).from_dict(d)
        #  The instance's input_files and cmd_line_args members still point to data structures in the original
        #  dictionary.  Copy them to avoid surprises if they are changed in the original dictionary.
        instance.input_files = dict(instance.input_files)
        instance.cmd_line_args = list(instance.cmd_line_args)
        return instance
--------------------
Prompt: 
def wait_for_ready(self, timeout=None, times=None, delay=None, delay_between=None, abort=None):
        """Determine the ready state of the device and wait until device is ready.

        Parameters
        ----------
        timeout : int, float
            The maximum amount of time to wait in seconds. Reaching the timeout will raise a RuntimeError.
        times : int
            Maximum number of times reading the ready state.
        delay : int, float
            The number of seconds to sleep before checks. Defaults to 0.
        delay_between : int, float
            The number of seconds to sleep between each check. Defaults to 0.
        abort : Threading.Event
            Breaking the loop from other threads.

        Returns
        -------
        True if state is ready, else False.
        """
----------
Output: 
 read. Defaults to 0.
        abort : bool
            If True, the device will be closed and the function will return True if the
            device is ready.
        """
        if timeout is None:
            timeout = self.timeout
        if times is None:
            times = self.times
        if delay_between is None:
            delay_between = self.delay_between
        if abort is None:
            abort = self.abort
        if delay_between is None:
            raise RuntimeError("Cannot determine the time between reads.")
        if abort:
            time.sleep(delay_between)
        if delay:
            time.sleep
----------
Solution: 

        if delay:
            try:
                sleep(delay)
            except IOError:  # negative values
                pass
        if timeout is not None:
            if timeout < 0:
                raise ValueError("timeout is smaller than 0")
            else:
                stop = time() + timeout
        times_checked = 0
        while not self.is_ready:
            now = time()
            times_checked += 1
            if abort and abort.is_set():
                False
            if timeout is not None and stop <= now:
                raise RuntimeError('Time out while waiting for ready in %s, module %s' % (self.name, self.__class__.__module__))
            if times and times > times_checked:
                False
            if delay_between:
                try:
                    sleep(delay_between)
                except IOError:  # negative values
                    pass
        return True
--------------------
Prompt: 
def get_bss_load(_, data):
    """http://git.kernel.org/cgit/linux/kernel/git/jberg/iw.git/tree/scan.c?id=v3.17#n935.

    Positional arguments:
    data -- bytearray data to read.

    Returns:
    Dict.
    """
----------
Output: 

    return _bss_load_helper(data, _bss_scan_bss_load)


def _bss_load_helper(data, bss_load_helper):
    """Helper function for reading BSS data."""
    if data[0] == 0x00:
        return bss_load_helper
    elif data[0] == 0x01:
        return bss_load_helper_data
    elif data[0] == 0x02:
        return bss_load_helper_data_0
    elif data[0] == 0x03:
       
----------
Solution: 

    answers = {
        'station count': (data[1] << 8) | data[0],
        'channel utilisation': data[2] / 255.0,
        'available admission capacity': (data[4] << 8) | data[3],
    }
    return answers
--------------------
Prompt: 
def case_comments(self):
        """Return only comments made on the case."""
----------
Output: 

        return self.comments.filter(
            Q(user=self.user) | Q(user_email=self.user.email) |
            Q(user_email=self.user.email) | Q(user_name=self.user.name)
        ).order_by('-created')

    def get_absolute_url(self):
        """Return the absolute URL for this case."""
        return reverse('case:details', args=[self.id])

    def get_case_type(self):
        """Return the case type."""
        return self.case_type

    def get_case
----------
Solution: 

        comments = (comment for comment in self.comments if
                    comment.variant_id is None)
        return comments
--------------------
Prompt: 
def run(self, globs=None):
        """Import user module and start __main__
           passing globals() is required when subclassing in another module
        """
----------
Output: 

        if globs is None:
            frame = sys._getframe(1)
            globs = frame.f_globals
            if frame.f_globals is None:
                globs = globals()
            globs.update(frame.f_globals)
        else:
            globs = globs.copy()
        globs.update(self.distribution.get_option_dict('sconsign', section='sconsign'))
        if self.ext_modules:
            globs.update(self.ext_modules)
        if self.include_package_data:
            self.announce("include_package_data")
            self.include
----------
Solution: 

        # Without this, the underneath import clashes with the top-level one
        global scoop

        if globs is None:
            globs = globals()

        # import the user module
        if scoop.MAIN_MODULE:
            globs.update(self.setupEnvironment(self))

        # Start the user program
        from scoop import futures

        def futures_startup():
            
--------------------
Prompt: 
def loadFeatures(path, opt):
    """
    Load features. For BED, only BED4 columns are loaded.
    For narrowPeak, all columns are loaded.
    """
----------
Output: 

    if opt.verbose:
        print "Loading %s" % path

    # Load the BED file
    if opt.verbose:
        print "Loading BED file"

    # Load the narrowPeak file
    if opt.narrowPeak:
        f = open(path, "r")
        f.readline()
        f.readline()
        f.readline()
        f.readline()
        f.readline()
        f.readline()
        f.readline()
        f.readline()
        f.readline()
        f.readline()
        f.readline()
        f.readline()
        f.readline()
        f.
----------
Solution: 


    log.info("loading from %s ..." % path)
    data = []
    if opt.in_format == "BED":        
        with open(path) as fd:
            for line in fd:
                cols = line.split()
                data.append( (cols[0], int(cols[1]), int(cols[2]), cols[3]) )
        data = np.array(data, dtype=elem_t)
    else:
        with open(path) as fd:
            for line in fd:
                cols = line.split()
                data.append( (cols[0], int(cols[1]), int(cols[2]), cols[3], int(cols[4]),
                              cols[5], float(cols[6]), float(cols[7]), float(cols[8]),
                              int(cols[-1])+int(cols[1])) )
        data = np.array(data, dtype=narrowPeak_t)
    return data
--------------------
Prompt: 
def repack_all(self):
        """Repacks the side chains of all Polymers in the Assembly."""
----------
Output: 

        self.repack_side_chains()
        self.repack_side_chains_by_name()

    def repack_side_chains(self):
        """Repacks the side chains of all Polymers in the Assembly."""
        self.repack_side_chains_by_id()
        self.repack_side_chains_by_name()

    def repack_side_chains_by_name(self):
        """Repacks the side chains of all Polymers by name."""
        self.repack_side_chains_by_name_by_id()
----------
Solution: 

        non_na_sequences = [s for s in self.sequences if ' ' not in s]
        self.pack_new_sequences(non_na_sequences)
        return
--------------------
Prompt: 
def init_lstm_(lstm, init_weight=0.1):
    """
    Initializes weights of LSTM layer.
    Weights and biases are initialized with uniform(-init_weight, init_weight)
    distribution.

    :param lstm: instance of torch.nn.LSTM
    :param init_weight: range for the uniform initializer
    """
----------
Output: 

    return LSTM(lstm.input_size, lstm.hidden_size, init_weight)


class LSTM(nn.Module):
    """
    This class implements a multi-layer perceptron.
    The input is a tensor of shape (batch_size, input_size)
    where the first dimension is the batch size and the second dimension
    is the input size.
    """

    def __init__(self, input_size, hidden_size, init_weight=0.1):
        super(LSTM, self).__init__()
        self.input_size = input_size
       
----------
Solution: 

    # Initialize hidden-hidden weights
    init.uniform_(lstm.weight_hh_l0.data, -init_weight, init_weight)
    # Initialize input-hidden weights:
    init.uniform_(lstm.weight_ih_l0.data, -init_weight, init_weight)

    # Initialize bias. PyTorch LSTM has two biases, one for input-hidden GEMM
    # and the other for hidden-hidden GEMM. Here input-hidden bias is
    # initialized with uniform distribution and hidden-hidden bias is
    # initialized with zeros.
    init.uniform_(lstm.bias_ih_l0.data, -init_weight, init_weight)
    init.zeros_(lstm.bias_hh_l0.data)

    if lstm.bidirectional:
        init.uniform_(lstm.weight_hh_l0_reverse.data, -init_weight, init_weight)
        init.uniform_(lstm.weight_ih_l0_reverse.data, -init_weight, init_weight)

        init.uniform_(lstm.bias_ih_l0_reverse.data, -init_weight, init_weight)
        init.zeros_(lstm.bias_hh_l0_reverse.data)
--------------------
Prompt: 
def example1():
    """
    Compute the GRADEV of a white phase noise. Compares two different 
    scenarios. 1) The original data and 2) ADEV estimate with gap robust ADEV.
    """
----------
Output: 

    data1 = np.array(
        [[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00],
          [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0.00000000e+00],
          [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,   0
----------
Solution: 

    N = 1000
    f = 1
    y = np.random.randn(1,N)[0,:]
    x = [xx for xx in np.linspace(1,len(y),len(y))]
    x_ax, y_ax, (err_l, err_h), ns = allan.gradev(y,data_type='phase',rate=f,taus=x)
    plt.errorbar(x_ax, y_ax,yerr=[err_l,err_h],label='GRADEV, no gaps')
    
    
    y[int(np.floor(0.4*N)):int(np.floor(0.6*N))] = np.NaN # Simulate missing data
    x_ax, y_ax, (err_l, err_h) , ns = allan.gradev(y,data_type='phase',rate=f,taus=x)
    plt.errorbar(x_ax, y_ax,yerr=[err_l,err_h], label='GRADEV, with gaps')
    plt.xscale('log')
    plt.yscale('log')
    plt.grid()
    plt.legend()
    plt.xlabel('Tau / s')
    plt.ylabel('Overlapping Allan deviation')
    plt.show()
--------------------
Prompt: 
def setGameScore(self, user_id, score, game_message_identifier,
                     force=None,
                     disable_edit_message=None):
        """
        See: https://core.telegram.org/bots/api#setgamescore

        :param game_message_identifier: Same as ``msg_identifier`` in :meth:`telepot.Bot.editMessageText`
        """
----------
Output: 

        assert isinstance(game_message_identifier, str)
        assert isinstance(score, (int, long))
        assert isinstance(game_message_identifier, str)
        assert game_message_identifier in self.user_data
        if not force:
            self.user_data[game_message_identifier][game_message_identifier] = score
        elif disable_edit_message:
            del self.user_data[game_message_identifier][game_message_identifier]
        else:
            self.user_data[game_message_identifier][game_message_identifier] = score

    def
----------
Solution: 

        p = _strip(locals(), more=['game_message_identifier'])
        p.update(_dismantle_message_identifier(game_message_identifier))
        return self._api_request('setGameScore', _rectify(p))
--------------------
Prompt: 
def kcenter(I,J,c,k):
    """kcenter -- minimize the maximum travel cost from customers to k facilities.
    Parameters:
        - I: set of customers
        - J: set of potential facilities
        - c[i,j]: cost of servicing customer i from facility j
        - k: number of facilities to be used
    Returns a model, ready to be solved.
    """
----------
Output: 

    model = KCenter(I,J,c)
    ready = model.solve(I,J,k)
    return model, ready

def kcenter_k(I,k,c):
    """kcenter -- minimize the maximum cost from customers to facilities.
    Parameters:
        - I: set of customers
        - J: set of potential facilities
        - c[i,k]: cost of servicing customer k from facility k
        - k: number of facilities to be used
        Returns a model, solved.
    """
    model = KCenter_kcenter(
----------
Solution: 


    model = Model("k-center")
    z = model.addVar(vtype="C", name="z")
    x,y = {},{}

    for j in J:
        y[j] = model.addVar(vtype="B", name="y(%s)"%j)
        for i in I:
            x[i,j] = model.addVar(vtype="B", name="x(%s,%s)"%(i,j))


    for i in I:
        model.addCons(quicksum(x[i,j] for j in J) == 1, "Assign(%s)"%i)

        for j in J:
            model.addCons(x[i,j] <= y[j], "Strong(%s,%s)"%(i,j))
            model.addCons(c[i,j]*x[i,j] <= z, "Max_x(%s,%s)"%(i,j))

    model.addCons(quicksum(y[j] for j in J) == k, "Facilities")

    model.setObjective(z, "minimize")
    model.data = x,y

    return model
--------------------
Prompt: 
def roll_group(group):
    """
    Rolls a group of dice in 2d6, 3d10, d12, etc. format

    :param group: String of dice group
    :return: Array of results
    """
----------
Output: 

    if isinstance(group, str):
        return [roll_dice(group) for i in range(6)]
    elif isinstance(group, list):
        return [roll_group(i) for i in range(10)]
    elif isinstance(group, int):
        return [roll_group(group) for i in range(12)]
    elif isinstance(group, float):
        return [roll_group(group) for i in range(12)]
    elif isinstance(group, int):
        return [roll_group(group) for i in range(10)]
    elif isinstance(group, float
----------
Solution: 

    group = regex.match(r'^(\d*)d(\d+)$', group, regex.IGNORECASE)
    num_of_dice = int(group[1]) if group[1] != '' else 1
    type_of_dice = int(group[2])
    assert num_of_dice > 0

    result = []
    for i in range(num_of_dice):
        result.append(random.randint(1, type_of_dice))
    return result
--------------------
Prompt: 
def _save_to_hdx(self, action, id_field_name, file_to_upload=None):
        # type: (str, str, Optional[str]) -> None
        """Creates or updates an HDX object in HDX, saving current data and replacing with returned HDX object data
        from HDX

        Args:
            action (str): Action to perform: 'create' or 'update'
            id_field_name (str): Name of field containing HDX object identifier
            file_to_upload (Optional[str]): File to upload to HDX

        Returns:
            None
        """
----------
Output: 

        """
        if file_to_upload is not None:
            file_to_upload = self._get_file_path(file_to_upload)
            with open(file_to_upload, 'rb') as f:
                return self.upload_to_hdx(f, file_to_upload)
        else:
            # If no file_to_upload is provided, use the file_to_upload parameter
            return self.create_hdx(action, id_field_name)

    def upload_to_hdx(self, file_to_upload, file_
----------
Solution: 

        result = self._write_to_hdx(action, self.data, id_field_name, file_to_upload)
        self.old_data = self.data
        self.data = result
--------------------
Prompt: 
def device_type_from_string(cl_device_type_str):
    """Converts values like ``gpu`` to a pyopencl device type string.

    Supported values are: ``accelerator``, ``cpu``, ``custom``, ``gpu``. If ``all`` is given, None is returned.

    Args:
        cl_device_type_str (str): The string we want to convert to a device type.

    Returns:
        cl.device_type: the pyopencl device type.
    """
----------
Output: 

    if cl_device_type_str is None:
        return None
    elif isinstance(cl_device_type_str, str):
        return cl_device_type_str
    else:
        raise ValueError("Unknown device type: %s" % cl_device_type_str)


def device_type_from_string_list(cl_device_strings, all=False):
    """Converts values like ``gpu`` to a pyopencl device type string list.

    Supported values are: ``accelerator``, ``cpu``, ``custom``, ``gpu``. If ``all`` is given, None
----------
Solution: 

    cl_device_type_str = cl_device_type_str.upper()
    if hasattr(cl.device_type, cl_device_type_str):
        return getattr(cl.device_type, cl_device_type_str)
    return None
--------------------
Prompt: 
def from_array(array):
        """
        Deserialize a new PassportData from a given dictionary.

        :return: new PassportData instance.
        :rtype: PassportData
        """
----------
Output: 

        self.data = {}
        self.data_size = len(array)
        self.data_offset = 0
        self.data_length = len(array[0])
        self.data_type = array[0][0]
        self.data_offset = array[0][1]
        self.data_length = array[0][2]
        self.data_type_size = array[0][3]
        self.data_type_offset = array[0][4]
        self.data_type_size = array[0][5]
        self.data_type
----------
Solution: 

        if array is None or not array:
            return None
        # end if
        assert_type_or_raise(array, dict, parameter_name="array")


        data = {}
        data['data'] = EncryptedPassportElement.from_array_list(array.get('data'), list_level=1)
        data['credentials'] = EncryptedCredentials.from_array(array.get('credentials'))
        data['_raw'] = array
        return PassportData(**data)
--------------------
Prompt: 
def setup_config(
        config_directories=None,
        config_file=None,
        default_filename="opentc.yml"
):
    """Setup configuration

    """
----------
Output: 

    if config_directories is None:
        config_directories = get_config_directories()
    if config_file is None:
        config_file = default_filename
    if not os.path.isfile(config_file):
        return False
    with open(config_file) as f:
        config = yaml.load(f)
    return True


def get_config_directories():
    """Get config directories

    """
    return [
        os.path.join(os.path.dirname(__file__), "opentc", "templates"),
        os.path.join(os.path.dirname
----------
Solution: 

    config_found = False
    config_file_path = None
    if config_file:
        config_file_path = config_file
        if os.path.isfile(config_file_path) and os.access(config_file_path, os.R_OK):
            config_found = True
    else:
        for directory in config_directories:
            if directory is None:
                continue
            config_file_path = os.path.join(directory, default_filename)
            if os.path.isfile(config_file_path) and os.access(config_file_path, os.R_OK):
                config_found = True
                break
    if config_found:
        with open(config_file_path, 'rt') as ymlfile:
            config = yaml.safe_load(ymlfile.read())
        return config
    else:
        print("The configuration file is not found.")
        exit(1)
--------------------
Prompt: 
def set_siren_volume(self, volume):
        """
        :param volume: one of [low, medium, high]
        """
----------
Output: 

        self.siren_volume = volume
        self.siren_volume_label.setText(str(volume))
        self.siren_volume_combo.setCurrentIndex(self.siren_volume)
        self.siren_volume_combo.blockSignals(False)
        self.siren_volume_combo.blockSignals(True)
        self.siren_volume_combo.setDisabled(True)
        self.siren_volume_combo.setCurrentIndex(self.siren_volume_combo.findText(self.siren_volume))
        self.siren_volume_
----------
Solution: 

        values = {
            "desired_state": {
                "siren_volume": volume
            }
        }
        response = self.api_interface.set_device_state(self, values)
        self._update_state_from_response(response)
--------------------
Prompt: 
def hash_full_tree(self, leaves):
        """Hash a set of leaves representing a valid full tree."""
----------
Output: 

        return hash(leaves)

    def get_leaves(self, node):
        """Get the set of leaves that are in the tree."""
        leaves = set()
        for child in node.children:
            leaves.add(self.hash_full_tree(child.children))
        return leaves

    def get_node_at_index(self, index):
        """Get the node at the given index."""
        for child in self.get_leaves(index):
            if child.parent is None:
                continue
            if child.parent.children[0] == index:
                return child
        return None

   
----------
Solution: 

        root_hash, hashes = self._hash_full(leaves, 0, len(leaves))
        assert len(hashes) == count_bits_set(len(leaves))
        assert (self._hash_fold(hashes) == root_hash if hashes else
                root_hash == self.hash_empty())
        return root_hash
--------------------
Prompt: 
def check_counts(self):
        """
        Disables tweet parsing if the count API is used.
        """
----------
Output: 

        self.tweet_parsing = False

    def get_tweets(self, count=None):
        """
        Returns a list of the most recent tweets posted by the user.
        """
        if count is None:
            count = self.tweet_count
        return self.tweet_storage.get_tweets(count)

    def get_tweets_since(self, count=None):
        """
        Returns the most recent tweets posted by the user since the
        specified number of tweets is known.
        """
        if count is None:
            count = self.tweet_count
        return self.
----------
Solution: 

        if "counts" in re.split("[/.]", self.endpoint):
            logger.info("disabling tweet parsing due to counts API usage")
            self._tweet_func = lambda x: x
--------------------
Prompt: 
def parse(self, valstr):
        # type: (bytes) -> None
        """
        A method to parse an El Torito Entry out of a string.

        Parameters:
         valstr - The string to parse the El Torito Entry out of.
        Returns:
         Nothing.
        """
----------
Output: 

        if not valstr:
            return

        # Parse the entry.
        self.parse_entry(valstr)

        # Parse the entry.
        self.parse_entry_table(valstr)

        # Parse the entry.
        self.parse_entry_table_data(valstr)

        # Parse the entry.
        self.parse_entry_table_data_table(valstr)

        # Parse the entry.
        self.parse_entry_data_table(valstr)

        # Parse the entry.
        self.parse_entry_data_table_entry(valstr)

    def
----------
Solution: 

        if self._initialized:
            raise pycdlibexception.PyCdlibInternalError('El Torito Entry already initialized')

        (self.boot_indicator, self.boot_media_type, self.load_segment,
         self.system_type, unused1, self.sector_count, self.load_rba,
         self.selection_criteria_type,
         self.selection_criteria) = struct.unpack_from(self.FMT, valstr, 0)

        if self.boot_indicator not in (0x88, 0x00):
            raise pycdlibexception.PyCdlibInvalidISO('Invalid El Torito initial entry boot indicator')
        if self.boot_media_type > 4:
            raise pycdlibexception.PyCdlibInvalidISO('Invalid El Torito boot media type')

        # FIXME: check that the system type matches the partition table

        if unused1 != 0:
            raise pycdlibexception.PyCdlibInvalidISO('El Torito unused field must be 0')

        # According to the specification, the El Torito unused end field (bytes
        # 0xc - 0x1f, unused2 field) should be all zero.  However, we have found
        # ISOs in the wild where that is not the case, so skip that particular
        # check here.

        self._initialized = True
--------------------
Prompt: 
def init_auth(username, password):
    """Initializes the auth settings for accessing MyAnimeList
    through its official API from a given username and password.
    :param username Your MyAnimeList account username.
    :param password Your MyAnimeList account password.
    :return A tuple containing your credentials.
    """
----------
Output: 

    return (username, password)


def get_my_anime_list_api(username, password):
    """Retrieves the API object for MyAnimeList
    from the given username and password.
    :param username Your MyAnimeList account username.
    :param password Your MyAnimeList password.
    :return A tuple containing your API object.
    """
    return (get_my_anime_list(username, password), get_my_anime_list_by_id(username,'my_anime_list'))


def get_my_anime_
----------
Solution: 

    username = username.strip()
    password = password.strip()
    credentials = (username, password)
    if helpers.verif_auth(credentials, header):
        return credentials
    else:
        raise ValueError(constants.INVALID_CREDENTIALS)
--------------------
Prompt: 
def dedent_block_string_value(raw_string: str) -> str:
    """Produce the value of a block string from its parsed raw value.

    Similar to CoffeeScript's block string, Python's docstring trim or Ruby's
    strip_heredoc.

    This implements the GraphQL spec's BlockStringValue() static algorithm.
    """
----------
Output: 

    return BlockStringValue(
        raw_string=raw_string,
        start_mark=raw_string.find('\n') + 1,
        end_mark=raw_string.find('\n') + 1,
        start_mark_line=raw_string.find('\n', start_mark + 1),
        end_mark_line=raw_string.find('\n', end_mark + 1),
        start_mark_column=raw_string.find('\n', start_mark + 1),
        end_mark_column=raw_string.find('\n', end_mark
----------
Solution: 

    lines = raw_string.splitlines()

    common_indent = None
    for line in lines[1:]:
        indent = leading_whitespace(line)
        if indent < len(line) and (common_indent is None or indent < common_indent):
            common_indent = indent
        if common_indent == 0:
            break

    if common_indent:
        lines[1:] = [line[common_indent:] for line in lines[1:]]

    while lines and not lines[0].strip():
        lines = lines[1:]

    while lines and not lines[-1].strip():
        lines = lines[:-1]

    return "\n".join(lines)
--------------------
Prompt: 
def async_get_measurements(self, uid, fields='*'):
        """Get measurements of a device."""
----------
Output: 

        return await self.hass.async_add_executor_job(self._get_measurements, uid, fields)

    async def _get_measurements(self, uid, fields='*', timeout=None):
        """Get measurements of a device."""
        if timeout is None:
            timeout = self.timeout

        if timeout is not None:
            fields += ['{}_{}'.format(k, v) for k, v in timeout.items()]
            return self.hass.async_add_executor_job(self._measures_timeout, uid, fields)

        return await self.hass.async_add
----------
Solution: 

        return (yield from self._get('/pods/{}/measurements'.format(uid),
                                     fields=fields))[0]
--------------------
Prompt: 
def resolved_args(self):
        """Parse args if they have not already been parsed and return the Namespace for args.

        .. Note:: Accessing args should only be done directly in the App.

        Returns:
            (namespace): ArgParser parsed arguments with Playbook variables automatically resolved.
        """
----------
Output: 

        return self._parse_args(self._args, self._namespace)

    def _parse_args(self, args, namespace):
        """Parse the arguments.

        Args:
            args (list): Arguments to parse.
            namespace (str): Namespace to use for parsing.
        """
        args = [arg.strip() for arg in args.split(',')] if args else []
        if namespace:
            args.append(namespace)
        return args

    def _resolve_namespace(self, namespace):
        """Resolve a namespace.

        Args:
            namespace (str): Namespace to use for resolving.
        """
        if
----------
Solution: 


        if not self._parsed_resolved:  # only resolve once
            self.args()

            # create new args Namespace for resolved args
            self._default_args_resolved = Namespace()

            # iterate over args and resolve any playbook variables
            for arg in vars(self._default_args):
                arg_val = getattr(self._default_args, arg)
                if arg not in self.tc_reserved_args:
                    if isinstance(arg_val, (str)):
                        arg_val = self.tcex.playbook.read(arg_val)
                setattr(self._default_args_resolved, arg, arg_val)

            # set parsed bool to ensure args are only parsed once
            self._parsed_resolved = True

        return self._default_args_resolved
--------------------
Prompt: 
def scale_app(self, app_id, instances=None, delta=None, force=False):
        """Scale an app.

        Scale an app to a target number of instances (with `instances`), or scale the number of
        instances up or down by some delta (`delta`). If the resulting number of instances would be negative,
        desired instances will be set to zero.

        If both `instances` and `delta` are passed, use `instances`.

        :param str app_id: application ID
        :param int instances: [optional] the number of instances to scale to
        :param int delta: [optional] the number of instances to scale up or down by
        :param bool force: apply even if a deployment is in progress

        :returns: a dict containing the deployment id and version
        :rtype: dict
        """
----------
Output: 
 [optional] the number of instances up/down by delta
        :param bool force: [optional] if True, force the scaling.
        :rtype: dict
        """
        params = {}
        if instances is not None:
            params['Instances'] = instances
        if delta is not None:
            params['Delta'] = delta
        if params:
            return self._get_response_by_key('Scale', params)
        else:
            return self._get_response_by_key('Scale', params)

    def list_app_instances(self, app_id=None):
        """List all instances of
----------
Solution: 

        if instances is None and delta is None:
            marathon.log.error('instances or delta must be passed')
            return

        try:
            app = self.get_app(app_id)
        except NotFoundError:
            marathon.log.error('App "{app}" not found'.format(app=app_id))
            return

        desired = instances if instances is not None else (
            app.instances + delta)
        return self.update_app(app.id, MarathonApp(instances=desired), force=force)
--------------------
Prompt: 
def list_groups(refresh=False):
    """
    Return a list of groups

    Args:

        refresh (bool):
            Refresh the info for all groups in ``__context__``. If False only
            the groups in ``__context__`` will be returned. If True, the
            ``__context__`` will be refreshed with current data and returned.
            Default is False

    Returns:
        list: A list of groups on the machine

    CLI Example:

    .. code-block:: bash

        salt '*' group.list_groups
    """
----------
Output: 

    if refresh:
        return __context__['refresh_groups']
    else:
        return __context__['groups']


def list_users(refresh=False):
    """
    Return a list of users

    Args:

        refresh (bool):
            Refresh the info for all users in ``__context__``. If False only
            the users in ``__context__`` will be returned. If True, the
            ``__context__`` will be refreshed with current data and returned.
            Default is False

    Returns:
        list: A list of users on the machine

    CLI Example:

   .. code-block:: bash

        salt '*' group.list
----------
Solution: 

    if 'group.list_groups' in __context__ and not refresh:
        return __context__['group.list_groups']

    results = _get_all_groups()

    ret = []

    for result in results:
        ret.append(result.Name)

    __context__['group.list_groups'] = ret

    return ret
--------------------
Prompt: 
def run(command, **kwargs):
    """Run a shell command and wait for the response. The result object will
    resolve to True if result.code == 0 and output/error results can be
    retrieved from result.stdout and result.stderr variables.

      >>> result = run('echo ${content}', content='Some $%^$## "" + \' content')
      >>> result.code
      0
      >>> if result:
      ...     print('Stdout: %s' % result.stdout)
      ... else:
      ...     print('Stderr: %s' % result.stderr)
      Stdout: Some $%^$## "" + ' content
      >>> print(result.command)
      echo 'Some $%^$## "" + '"'"' content'
    """
----------
Output: 
 content
      """
    if'shell' not in kwargs:
        kwargs['shell'] = True
    return _run(command, **kwargs)


def _run(command, **kwargs):
    """Execute a shell command and return the output.

    This function will execute the command in a shell and wait for the
    response. If the command is a string, it will be sent to the
    subprocess.Popen object. Otherwise it will be sent to the
    subprocess.Popen object.

    If'stdout' is a subprocess.PIPE, it will be used for stdout.
    Otherwise, it will be used for stderr.
----------
Solution: 

    env = None
    if 'env' in kwargs:
        if kwargs.get('env_empty'):
            env = {}
        else:
            env = copy.deepcopy(os.environ)
        env.update(kwargs['env'])
    stdin = kwargs.get('stdin')
    stdout = kwargs.get('stdout', False)
    stderr = kwargs.get('stderr', False)
    combine = kwargs.get('combine', False)
    if stdout is True:
        stdout = sys.stdout.write
    if stderr is True:
        stderr = sys.stderr.write
    if kwargs:
        args = {}
        q = pipes.quote
        for name, value in kwargs.items():
            if isinstance(value, basestring_type):
                args[name] = q(value)
            elif isinstance(value, (list, tuple)):
                args[name] = u' '.join([q(unicode_type(v)) for v in value])
            elif isinstance(value, dict):
                args[name] = u' '.join([u'%s %s' % (q(n), q(v)) for n, v in value.items()])
            else:
                args[name] = pipes.quote(unicode_type(value))
        command = string.Template(command).safe_substitute(args)
    log.debug('run: %s' % command)
    ref = subprocess.Popen(
        command,
        stdin=None if stdin is None else subprocess.PIPE,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT if combine is True else subprocess.PIPE,
        shell=kwargs.get('shell', True),
        close_fds=kwargs.get('close_fds', True),
        env=env,
        cwd=kwargs.get('cwd', tempfile.gettempdir()),
    )
    if stdin is not None:
        if not isinstance(stdin, basestring_type):
            stdin = unicode_type(stdin)
        if not isinstance(stdin, bytes_type):
            stdin = stdin.encode('utf-8')
        ref.stdin.write(stdin)
        ref.stdin.flush()
        ref.stdin.close()
    fds = [ref.stdout]
    if combine is not True:
        fds.append(ref.stderr)
    stdout_result = b''
    stderr_result = b''
    while fds:
        for fd in select.select(fds, tuple(), tuple())[0]:
            line = fd.readline()
            if line:
                if fd == ref.stdout:
                    if stdout:
                        stdout(line)
                    stdout_result += line
                elif fd == ref.stderr:
                    if stderr:
                        stderr(line)
                    stderr_result += line
            else:
                fds.remove(fd)
    ref.wait()
    return obj({
        'code': ref.returncode,
        'command': command,
        'stdout': stdout_result,
        'stderr': stderr_result,
    }, bool=ref.returncode == 0, grow=False)
--------------------
Prompt: 
def set_rainbow(self, duration):
        """Turn the bulb on and create a rainbow."""
----------
Output: 

        self.rainbow = Rainbow(duration)
        self.rainbow.start()

    def get_rainbow(self):
        """Return the rainbow."""
        return self.rainbow

    def stop(self):
        """Stop the thread."""
        self.running = False

    def run(self):
        """Run the thread."""
        self.set_thread_stop()
        self.join()
        self.set_thread_stop()

    def join(self):
        """Wait for the thread to exit."""
        self.running = True
        self.join_thread.join()
        self
----------
Solution: 

        for i in range(0, 359):
            self.set_color_hsv(i, 100, 100)
            time.sleep(duration/359)
--------------------
Prompt: 
def hill_i(self,x,threshold=0.1,power=2):
        """ Inhibiting hill function.

            Is equivalent to 1-hill_a(self,x,power,threshold).
        """
----------
Output: 

        return 1.0 - np.log(x) - threshold*np.log(power)

    def hill_j(self,x,threshold=0.1,power=2):
        """ Inhibiting hill function.

            Is equivalent to 1-hill_a(self,x,power,threshold).
        """
        return 1.0 - np.log(x) - threshold*np.log(power)

    def hill_k(self,x,threshold=0.1,power=2):
        """ Inhibiting hill function.

            Is equivalent
----------
Solution: 

        x_pow = np.power(x,power)
        threshold_pow = np.power(threshold,power)
        return threshold_pow / (x_pow + threshold_pow)
--------------------
Prompt: 
def _run_genotype_gvcfs_genomicsdb(genomics_db, region, out_file, data):
    """GenotypeGVCFs from a merged GenomicsDB input: GATK4.
            ropts += [str(x) for x in resources.get("options", [])]

    No core scaling -- not yet supported in GATK4.
    """
----------
Output: 

    if not resources.get("options", []):
        raise ValueError("No core scaling options specified")
    if not region.endswith("."):
        region = region + "."
    ropts = ropts + [str(x) for x in resources.get("options", [])]
    if not out_file.endswith(".gvcf"):
        out_file = out_file + ".gvcf"
    with open(data, "w") as out_handle:
        for ropt in ropts:
            if ropt.startswith("options:"):
                out_handle.write(genomics_db.options
----------
Solution: 

    if not utils.file_exists(out_file):
        with file_transaction(data, out_file) as tx_out_file:
            broad_runner = broad.runner_from_config(data["config"])
            params = ["-T", "GenotypeGVCFs",
                      "--variant", "gendb://%s" % genomics_db,
                      "-R", dd.get_ref_file(data),
                      "--output", tx_out_file,
                      "-L", bamprep.region_to_gatk(region)]
            params += ["-ploidy", str(ploidy.get_ploidy([data], region))]
            # Avoid slow genotyping runtimes with improved quality score calculation in GATK4
            # https://gatkforums.broadinstitute.org/gatk/discussion/11471/performance-troubleshooting-tips-for-genotypegvcfs/p1
            params += ["--use-new-qual-calculator"]
            resources = config_utils.get_resources("gatk", data["config"])
            params += [str(x) for x in resources.get("options", [])]
            cores = dd.get_cores(data)
            memscale = {"magnitude": 0.9 * cores, "direction": "increase"} if cores > 1 else None
            broad_runner.run_gatk(params, memscale=memscale)
    return vcfutils.bgzip_and_index(out_file, data["config"])
--------------------
Prompt: 
def emergency_cast(colx, value):
  """ugly: this is a huge hack. get serious about where this belongs in the architecture.
  For now, most types rely on being fed in as SubbedLiteral.
  """
----------
Output: 

  if isinstance(value, Literal):
    return value.to_string()
  return value


def emergency_cast_to_string(colx, value):
  """ugly: this is a huge hack. get serious about where this belongs in the architecture.
  For now, most types rely on being fed in as SubbedLiteral.
  """
  if isinstance(value, SubbedLiteral):
    return value.to_string()
  return value


def emergency_cast_to_string_or_none(colx, value):
  """ugly: this is
----------
Solution: 

  if colx.coltp.type.lower()=='boolean':
    if isinstance(value,sqparse2.NameX): value = value.name
    if isinstance(value,bool): return value
    return dict(true=True, false=False)[value.lower()] # keyerror if other
  else:
    return value
--------------------
Prompt: 
def p_portfolio(I,sigma,r,alpha,beta):
    """p_portfolio -- modified markowitz model for portfolio optimization.
    Parameters:
        - I: set of items
        - sigma[i]: standard deviation of item i
        - r[i]: revenue of item i
        - alpha: acceptance threshold
        - beta: desired confidence level
    Returns a model, ready to be solved.
    """
----------
Output: 

    model = {}
    ready = {}
    for i in I:
        if i not in model:
            model[i] = {}
        if i not in ready:
            ready[i] = set()
        if sigma[i] == r[i]:
            model[i][r] = alpha
        if beta == 0:
            ready[i].add(i)
        if sigma[i] == 0:
            ready[i].add(r)
            model[i][r].add(i)
            ready[i].add(r)
            model[i][r].add(i)
           
----------
Solution: 


    model = Model("p_portfolio")

    x = {}
    for i in I:
        x[i] = model.addVar(vtype="C", name="x(%s)"%i)  # quantity of i to buy
    rho = model.addVar(vtype="C", name="rho")
    rhoaux = model.addVar(vtype="C", name="rhoaux")

    model.addCons(rho == quicksum(r[i]*x[i] for i in I))
    model.addCons(quicksum(x[i] for i in I) == 1)

    model.addCons(rhoaux == (alpha - rho)*(1/phi_inv(beta))) #todo
    model.addCons(quicksum(sigma[i]**2 * x[i] * x[i] for i in I) <=  rhoaux * rhoaux)

    model.setObjective(rho, "maximize")

    model.data = x
    return model
--------------------
Prompt: 
def get_memory_cache(self, key=None):
        """Return result in memory cache for key 'key' or None if not found."""
----------
Output: 

        return self._memory_cache

    def get_memory_usage(self, key=None):
        """Return result in bytes for key 'key' or None if not found."""
        return self._memory_usage

    def get_memory_info(self, key=None):
        """Return result in bytes for key 'key' or None if not found."""
        return self._memory_cache

    def get_memory_info_ex(self, key=None):
        """Return result in bytes for key 'key' or None if not found."""
        return self.get_memory_info(key).
----------
Solution: 

        key = self.model.hash if key is None else key
        self._results = self.memory_cache.get(key)
        return self._results
--------------------
Prompt: 
def service_timeouts(self):
        """
        run callbacks on all expired timers
        Called from the event thread
        :return: next end time, or None
        """
----------
Output: 

        if self.timer_list:
            for timer in self.timer_list:
                if timer.is_expired():
                    self.timer_list.remove(timer)
        return self.timer_list

    def _get_timer_list(self):
        """
        get all timers that have expired
        :return: list of expired timers
        """
        return self.timers.values()

    def _get_timer_list_by_name(self, name):
        """
        get all timers by name
        :param name: name of the timer
        :return: list of timers
       
----------
Solution: 

        queue = self._queue
        if self._new_timers:
            new_timers = self._new_timers
            while new_timers:
                heappush(queue, new_timers.pop())

        if queue:
            now = time.time()
            while queue:
                try:
                    timer = queue[0][1]
                    if timer.finish(now):
                        heappop(queue)
                    else:
                        return timer.end
                except Exception:
                    log.exception("Exception while servicing timeout callback: ")
--------------------
Prompt: 
def grid_list(data):
    """
    #=================================================
    /process the grid data
    /convert to list data for poly fitting
    #=================================================
    """
----------
Output: 

    #=================================================
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
    #
----------
Solution: 

    a = []
    b = []
    M = []
    for i in data:
        a.append(i[0])  # np.array([i[1] for i in data], dtype=np.float64)
        b.append(i[1])  # np.array([i[0] for i in data], dtype=np.float64)
        M.append(i[2])  # np.array([i[2] for i in data], dtype=np.float64)
    a = np.array(a, dtype=np.float64).tolist()
    b = np.array(b, dtype=np.float64).tolist()
    M = np.array(M, dtype=np.float64).tolist()
    a = list(set(a))
    b = list(set(b))
    return a, b, M
--------------------
Prompt: 
def set_progress_message(self, message, line_break=False):
        """Set a progress message.

        The string needs to contain `{progress}`.

        """
----------
Output: 

        if line_break:
            self.line_break()
        self.status_message = message
        self.progress = message

    def set_progress_percentage(self, percentage):
        """Set a progress percentage.

        The string needs to contain `{progress}`.

        """
        if percentage < 0 or percentage > 100:
            raise ValueError("Invalid progress percentage: %r" % (percentage,))
        self.line_percentage = percentage
        self.status_percentage = percentage

    def set_progress_bar(self, bar):
        """Set a progress bar.

        The string needs to contain `{progress
----------
Solution: 


        end = '\r' if not line_break else None

        @self.connect
        def on_progress(value, value_max, **kwargs):
            kwargs['end'] = None if value == value_max else end
            _default_on_progress(message, value, value_max, **kwargs)
--------------------
Prompt: 
def add_ip(self, family='IPv4'):
        """
        Allocate a new (random) IP-address to the Server.
        """
----------
Output: 

        if family == 'IPv4':
            self.server_ip = random.choice(self.ip_addresses)
        else:
            self.server_ip = self.ip_addresses[0]

    def add_ip_range(self, ip_range, family='IPv4'):
        """
        Allocate a new (random) IP-range to the Server.
        """
        if family == 'IPv4':
            self.server_ip = random.choice(ip_range)
        else:
            self.server_ip = ip_range[0]
            self.server_ip_range
----------
Solution: 

        IP = self.cloud_manager.attach_ip(self.uuid, family)
        self.ip_addresses.append(IP)
        return IP
--------------------
Prompt: 
def exception(self, timeout=None):
        """Wait for the async function to complete and return its exception.

        If the function did not raise an exception this returns ``None``.
        """
----------
Output: 

        return self._wait_for_function(self.async_function, timeout)

    def _wait_for_function(self, function, timeout=None):
        """Wait for a function to complete.

        If the function did not raise an exception this returns ``None``.
        """
        start_time = time.time()
        while function.func_name in self._async_functions:
            if function.func_name == function.func_name:
                return
            time.sleep(0.1)
        self.fail('timed out waiting for %s' % function.func_name)

   
----------
Solution: 

        if not self._done.wait(timeout):
            raise Timeout('timeout waiting for future')
        if self._state == self.S_EXCEPTION:
            return self._result
--------------------
Prompt: 
def set_options(self, **options):
        """
        Set instance variables based on an options dict
        """
----------
Output: 

        self.options.update(options)
        self.options['format'] = 'json'
        self.options['indent'] = 4
        self.options['sort_keys'] = True
        self.options['separators'] = (',', ':')
        self.options['sort_keys_first'] = True
        self.options['sort_keys_last'] = True
        self.options['sort_keys_last_item'] = True
        self.options['sort_keys_last_item_item'] = True
        self.options['sort_keys_last_item_text'] = True
       
----------
Solution: 

        self.interactive = False
        self.verbosity = options['verbosity']
        self.symlink = ""
        self.clear = False
        ignore_patterns = []
        self.ignore_patterns = list(set(ignore_patterns))
        self.page_themes_updated = 0
        self.skins_updated = 0
--------------------
Prompt: 
def use_comparative_assessment_taken_view(self):
        """Pass through to provider AssessmentTakenLookupSession.use_comparative_assessment_taken_view"""
----------
Output: 

        # Implemented from kitosid template for -
        # osid.resource.ResourceLookupSession.use_comparative_assessment_taken_view
        self._provider_session.use_comparative_assessment_taken_view()

    def use_plenary_assessment_taken_view(self):
        """Pass through to provider PlenaryTakenLookupSession.use_plenary_assessment_taken_view"""
        # Implemented from kitosid template for -
        # osid.resource.ResourceLookupSession.use_plenary_assessment_taken_view
       
----------
Solution: 

        self._object_views['assessment_taken'] = COMPARATIVE
        # self._get_provider_session('assessment_taken_lookup_session') # To make sure the session is tracked
        for session in self._get_provider_sessions():
            try:
                session.use_comparative_assessment_taken_view()
            except AttributeError:
                pass
--------------------
Prompt: 
def is_html_link(cls, tag, attribute):
        """Return whether the link is likely to be external object."""
----------
Output: 

        return (tag.name == 'a' and attribute.name == 'href') or \
               (tag.name == 'img' and attribute.name =='src')

    @classmethod
    def get_external_url(cls, url):
        """Return the external URL for the given URL."""
        if not url:
            return None
        if url.startswith('http'):
            return url
        return 'http://www.w3.org/1999/xhtml'

    @classmethod
    def get_link_url(cls, link):
        """Return the external URL for the given link."""
        if not link
----------
Solution: 

        if tag in cls.TAG_ATTRIBUTES \
           and attribute in cls.TAG_ATTRIBUTES[tag]:
            attr_flags = cls.TAG_ATTRIBUTES[tag][attribute]
            return attr_flags & cls.ATTR_HTML

        return attribute == 'href'
--------------------
Prompt: 
def from_file(cls, filename):
        """Initialize datasource from file (.tds ot .tdsx)"""
----------
Output: 

        with open(filename, 'rb') as f:
            tds = pickle.load(f)
        return cls(tds)

    def to_file(self, filename):
        """Write the datasource to a file (.tds ot.tdsx)"""
        with open(filename, 'wb') as f:
            pickle.dump(self.tds, f, pickle.HIGHEST_PROTOCOL)

    def from_file(self, filename):
        """Initialize datasource from file (.tds ot.tdsx)"""
        with open(filename, 'rb') as f:
            tds
----------
Solution: 


        dsxml = xml_open(filename, 'datasource').getroot()
        return cls(dsxml, filename)
--------------------
Prompt: 
def get_sqlalchemy_url(database=None, host=None, port=None, username=None, password=None, driver='postgres'):
        # type: (Optional[str], Optional[str], Union[int, str, None], Optional[str], Optional[str], str) -> str
        """Gets SQLAlchemy url from database connection parameters

        Args:
            database (Optional[str]): Database name
            host (Optional[str]): Host where database is located
            port (Union[int, str, None]): Database port
            username (Optional[str]): Username to log into database
            password (Optional[str]): Password to log into database
            driver (str): Database driver. Defaults to 'postgres'.

        Returns:
            db_url (str): SQLAlchemy url
        """
----------
Output: 
]): Password to log into database

        Returns:
            str: SQLAlchemy url
        """
        if database is not None:
            return database + '@' + str(host) + ':' + str(port)
        elif username is not None:
            return 'postgresql://' + str(host) + ':' + str(port)
        else:
            return None

    def get_table_name(self, table_name, database=None, driver='postgres'):
        # type: (str, Optional[str], Union[int, str, None], Optional[str], Optional[str]) -> str
        """Gets table name from connection
----------
Solution: 

        strings = ['%s://' % driver]
        if username:
            strings.append(username)
            if password:
                strings.append(':%s@' % password)
            else:
                strings.append('@')
        if host:
            strings.append(host)
        if port is not None:
            strings.append(':%d' % int(port))
        if database:
            strings.append('/%s' % database)
        return ''.join(strings)
--------------------
Prompt: 
def bed(args):
    """
    %prog bed anchorsfile

    Convert ANCHORS file to BED format.
    """
----------
Output: 

    from jcvi.formats.bed import Bed
    from jcvi.formats.bed import BedLine
    from jcvi.utils.cbook import is_string
    from jcvi.utils.cbook import is_bool
    from jcvi.utils.cbook import is_bool_seq
    from jcvi.utils.cbook import is_valid_name
    from jcvi.utils.cbook import is_valid_bool
    from jcvi.utils.cbook import is_valid_bool_name
    from jcvi.utils.c
----------
Solution: 

    from collections import defaultdict
    from jcvi.compara.synteny import AnchorFile, check_beds
    from jcvi.formats.bed import Bed
    from jcvi.formats.base import get_number

    p = OptionParser(bed.__doc__)
    p.add_option("--switch", default=False, action="store_true",
                 help="Switch reference and aligned map elements")
    p.add_option("--scale", type="float",
                 help="Scale the aligned map distance by factor")
    p.set_beds()
    p.set_outfile()
    opts, args = p.parse_args(args)

    if len(args) != 1:
        sys.exit(not p.print_help())

    anchorsfile, = args
    switch = opts.switch
    scale = opts.scale
    ac = AnchorFile(anchorsfile)
    pairs = defaultdict(list)
    for a, b, block_id in ac.iter_pairs():
        pairs[a].append(b)

    qbed, sbed, qorder, sorder, is_self = check_beds(anchorsfile, p, opts)
    bd = Bed()
    for q in qbed:
        qseqid, qstart, qend, qaccn = q.seqid, q.start, q.end, q.accn
        if qaccn not in pairs:
            continue
        for s in pairs[qaccn]:
            si, s = sorder[s]
            sseqid, sstart, send, saccn = s.seqid, s.start, s.end, s.accn
        if switch:
            qseqid, sseqid = sseqid, qseqid
            qstart, sstart = sstart, qstart
            qend, send = send, qend
            qaccn, saccn = saccn, qaccn
        if scale:
            sstart /= scale
        try:
            newsseqid = get_number(sseqid)
        except ValueError:
            raise ValueError("`{0}` is on `{1}` with no number to extract".\
                                format(saccn, sseqid))
        bedline = "\t".join(str(x) for x in (qseqid, qstart - 1, qend,
                            "{0}:{1}".format(newsseqid, sstart)))
        bd.add(bedline)

    bd.print_to_file(filename=opts.outfile, sorted=True)
--------------------
Prompt: 
def verify_recipient(self, recipient):
        """
        Verify that I'm the recipient of the assertion

        :param recipient: A URI specifying the entity or location to which an
            attesting entity can present the assertion.
        :return: True/False
        """
----------
Output: 

        return self.get_assertion(recipient)

    def verify_audience(self, audience):
        """
        Verify that the audience is valid

        :param audience: URI specifying the audience to which an
            attesting entity can present the assertion.
        :return: True/False
        """
        return self.get_audience(audience)

    def verify_assertion(self, uri):
        """
        Verify that the URI is valid

        :param uri: URI specifying the assertion
        :return: True/False
        """
        return self.get_uri_type(uri
----------
Solution: 

        if not self.conv_info:
            return True

        _info = self.conv_info

        try:
            if recipient == _info['entity_id']:
                return True
        except KeyError:
            pass

        try:
            if recipient in self.return_addrs:
                return True
        except KeyError:
            pass

        return False
--------------------
Prompt: 
def read_tsv(f, delimiter="\t"):
    """
    Take a tsv filepath and return an array; e.g. [[],[],[]].

    For example, a file containing:

    1   2   3
    4   5   6
    7   8   9

    would return the array: [['1','2','3'], ['4','5','6'], ['7','8','9']]

    :param tsv_filepath:
    :return: tsv_array
    """
----------
Output: 

    with open(f, "r") as f_in:
        return [line.strip().split(delimiter) for line in f_in]


def read_json(f, delimiter="\t"):
    """
    Take a json filepath and return an array; e.g. [[], [1,2,3]].

    For example, a file containing:

    {"a": {"b": {"c": "d"}}}

    would return the array: [['a', 'b', 'c'], ['d', 'a', 'b', 'c']]

    :param json_filepath:
    :
----------
Solution: 

    tsv_array = []
    with open(f, "r") as f:
        data_file = csv.reader(f, delimiter=delimiter)
        for line in data_file:
            tsv_array.append(line)
    return (tsv_array)
--------------------
Prompt: 
def grid_destroy_from_ids(oargrid_jobids):
    """Destroy all the jobs with corresponding ids

    Args:
        oargrid_jobids (list): the ``(site, oar_job_id)`` list of tuple
            identifying the jobs for each site. """
----------
Output: 

    for jobid in oargrid_jobids:
        oarjob = OarJob.objects.get(id=jobid[0], site=None)
        if oarjob.status!= 'DELETED':
            oargrid_jobids.remove(jobid)

    oargrid_jobs = OargridJob.objects.filter(status='DELETED')
    grid_ids = []
    for oargrid_job in oargrid_jobs:
        grid_ids.append(oargrid_job.id)

    grid_ids = tuple(set(grid_ids))
----------
Solution: 

    jobs = grid_reload_from_ids(oargrid_jobids)
    for job in jobs:
        job.delete()
        logger.info("Killing the jobs %s" % oargrid_jobids)
--------------------
Prompt: 
def files(context, yes, tag, bundle, before, notondisk):
    """Delete files based on tags."""
----------
Output: 

    if not ondisk:
        return
    for f in notondisk:
        if f.tag == tag:
            f.delete()
    return


def _get_files_by_type(context, type):
    """Get files by type."""
    if type == 'all':
        return _get_all_files()
    elif type == 'notondisk':
        return _get_noton_all_files()
    else:
        raise ValueError('Invalid type: %s' % type)


def _get_all_files():
    """Get all files."""
    files = []
   
----------
Solution: 

    file_objs = []

    if not tag and not bundle:
        click.echo("I'm afraid I can't let you do that.")
        context.abort()

    if bundle:
        bundle_obj = context.obj['store'].bundle(bundle)
        if bundle_obj is None:
            click.echo(click.style('bundle not found', fg='red'))
            context.abort()

    query = context.obj['store'].files_before(bundle = bundle, tags = tag, before = before)

    if notondisk:
        file_objs = set(query) - context.obj['store'].files_ondisk(query)
    else:
        file_objs = query.all()

    if len(file_objs) > 0 and len(yes) < 2:
        if not click.confirm(f"Are you sure you want to delete {len(file_objs)} files?"):
            context.abort()

    for file_obj in file_objs:
        if yes or click.confirm(f"remove file from disk and database: {file_obj.full_path}"):
            file_obj_path = Path(file_obj.full_path)
            if file_obj.is_included and (file_obj_path.exists() or file_obj_path.is_symlink()):
                file_obj_path.unlink()
            file_obj.delete()
            context.obj['store'].commit()
            click.echo(f'{file_obj.full_path} deleted')
--------------------
Prompt: 
def get_ssh_gateway_config(vm_):
    """
    Return the ssh_gateway configuration.
    """
----------
Output: 

    vm_nics = vm_['nic']
    ssh_gateway = {}
    ssh_gateway['name'] = vm_nics['nic'][0]['network']
    ssh_gateway['properties'] = {}
    ssh_gateway['properties']['networkProfile'] = network_profile
    ssh_gateway['properties']['cpuCount'] = len(vm_nics['nic'])
    ssh_gateway['properties']['memory'] = int(vm_nics['nic'][0]['memory'])
    ssh_gateway['properties']['cpuSpeed'] = int(vm_nics['nic'][0]['cpuSpeed'])
    ssh_gateway['properties']['
----------
Solution: 

    ssh_gateway = config.get_cloud_config_value(
        'ssh_gateway', vm_, __opts__, default=None,
        search_global=False
    )

    # Check to see if a SSH Gateway will be used.
    if not isinstance(ssh_gateway, six.string_types):
        return None

    # Create dictionary of configuration items

    # ssh_gateway
    ssh_gateway_config = {'ssh_gateway': ssh_gateway}

    # ssh_gateway_port
    ssh_gateway_config['ssh_gateway_port'] = config.get_cloud_config_value(
        'ssh_gateway_port', vm_, __opts__, default=None,
        search_global=False
    )

    # ssh_gateway_username
    ssh_gateway_config['ssh_gateway_user'] = config.get_cloud_config_value(
        'ssh_gateway_username', vm_, __opts__, default=None,
        search_global=False
    )

    # ssh_gateway_private_key
    ssh_gateway_config['ssh_gateway_key'] = config.get_cloud_config_value(
        'ssh_gateway_private_key', vm_, __opts__, default=None,
        search_global=False
    )

    # ssh_gateway_password
    ssh_gateway_config['ssh_gateway_password'] = config.get_cloud_config_value(
        'ssh_gateway_password', vm_, __opts__, default=None,
        search_global=False
    )

    # ssh_gateway_command
    ssh_gateway_config['ssh_gateway_command'] = config.get_cloud_config_value(
        'ssh_gateway_command', vm_, __opts__, default=None,
        search_global=False
    )

    # Check if private key exists
    key_filename = ssh_gateway_config['ssh_gateway_key']
    if key_filename is not None and not os.path.isfile(key_filename):
        raise SaltCloudConfigError(
            'The defined ssh_gateway_private_key \'{0}\' does not exist'
            .format(key_filename)
        )
    elif (
        key_filename is None and
        not ssh_gateway_config['ssh_gateway_password']
    ):
        raise SaltCloudConfigError(
            'No authentication method. Please define: '
            ' ssh_gateway_password or ssh_gateway_private_key'
        )

    return ssh_gateway_config
--------------------
Prompt: 
def write(self, ontol, **args):
        """
        Write a `ontology` object
        """
----------
Output: 

        self.write_ontology(ontol, *args)

    def write_ontology(self, ontol, **args):
        """
        Write a `ontology` object
        """
        self.write_ontology_file(ontol, ontol, *args)

    def write_ontology_file(self, ontol, **args):
        """
        Write a `ontology` object
        """
        self.write_ontology_file_as_json(ontol, ontol, *args)

    def write_ontology_file_as_json(self, on
----------
Solution: 

        s = self.render(ontol, **args)
        if self.outfile is None:
            print(s)
        else:
            f = open(self.outfile, 'w')
            f.write(s)
            f.close()
--------------------
Prompt: 
def pack(self, value=None):
        """Pack the value as a binary representation.

        Returns:
            bytes: The binary representation.

        """
----------
Output: 

        return self._write_length(len(value)) + self._write_length(len(self._value))

    def unpack(self, data, offset=0):
        """Unpack the value into a binary representation.

        Args:
            data: The binary representation.
            offset: The number of bytes to skip at the start of the value.

        Returns:
            tuple[bytes, int]: The unpacked binary representation.

        """
        return self._read_length(data, offset)

    def __len__(self):
        """Return the number of bytes in the buffer."""
        return self._read_length(
----------
Solution: 

        if isinstance(value, type(self)):
            return value.pack()

        if value is None:
            value = self
        else:
            container = type(self)(items=None)
            container.extend(value)
            value = container

        bin_message = b''
        try:
            for item in value:
                bin_message += item.pack()
            return bin_message
        except exceptions.PackException as err:
            msg = "{} pack error: {}".format(type(self).__name__, err)
            raise exceptions.PackException(msg)
--------------------
Prompt: 
def bench(image, thread_count):
  """Encode `image` to PNG on `thread_count` threads in parallel.

  Returns:
    A `float` representing number of seconds that it takes all threads
    to finish encoding `image`.
  """
----------
Output: 

  return (time.time() - start) * thread_count


def main():
  parser = argparse.ArgumentParser()
  parser.add_argument(
      '--image',
      help='Path to the input image file, e.g., /path/to/example.png')
  parser.add_argument(
      '--threads',
      type=int,
      default=1,
      help='Number of threads to run, e.g., 1')
  parser.add_argument(
      '--output',
      help='Path to the output image file, e.g., /path/to/output.
----------
Solution: 

  threads = [threading.Thread(target=lambda: encoder.encode_png(image))
             for _ in xrange(thread_count)]
  start_time = datetime.datetime.now()
  for thread in threads:
    thread.start()
  for thread in threads:
    thread.join()
  end_time = datetime.datetime.now()
  delta = (end_time - start_time).total_seconds()
  return delta
--------------------
Prompt: 
def schedule_to_array(schedule, events, slots):
    """Convert a schedule from schedule to array form

    Parameters
    ----------
    schedule : list or tuple
        of instances of :py:class:`resources.ScheduledItem`
    events : list or tuple
        of :py:class:`resources.Event` instances
    slots : list or tuple
        of :py:class:`resources.Slot` instances

    Returns
    -------
    np.array
        An E by S array (X) where E is the number of events and S the
        number of slots. Xij is 1 if event i is scheduled in slot j and
        zero otherwise
    """
----------
Output: 
 else 0.
    """
    if isinstance(schedule, (tuple, list)):
        return np.array(events)
    elif isinstance(schedule, (list, tuple)):
        return np.array(slots)
    else:
        raise ValueError("Schedule must be a tuple or list of E instances")


class Epochs(object):
    """Container for epochs

    Parameters
    ----------
    epochs : list
        The epochs to be iterated over
    """
    def __init__(self, epochs):
        self.epochs = epochs

    def __iter__(self):
        return self.epochs

    def next(self):
        raise NotImplementedError
----------
Solution: 

    array = np.zeros((len(events), len(slots)), dtype=np.int8)
    for item in schedule:
        array[events.index(item.event), slots.index(item.slot)] = 1
    return array
--------------------
Prompt: 
def activate(self, span, finish_on_close):
        """
        Make a :class:`~opentracing.Span` instance active.

        :param span: the :class:`~opentracing.Span` that should become active.
        :param finish_on_close: whether *span* should automatically be
            finished when :meth:`Scope.close()` is called.

        :return: a :class:`~opentracing.Scope` instance to control the end
            of the active period for the :class:`~opentracing.Span`.
            It is a programming error to neglect to call :meth:`Scope.close()`
            on the returned instance.
        """
----------
Output: 
 the user.
        """
        self._set_state(self.STATE_ACTIVE)
        self._set_span(span)
        self._set_finish_on_close(finish_on_close)
        self._set_span_state(self.STATE_ACTIVE)
        return self

    def deactivate(self, span):
        """
        Make a :class:`~opentracing.Span` instance inactive.

        :param span: the :class:`~opentracing.Span` that should become inactive.
        :return: a :class:`~opentracing.Scope instance to control the end
----------
Solution: 


        scope = _GeventScope(self, span, finish_on_close)
        self._set_greenlet_scope(scope)

        return scope
--------------------
Prompt: 
def info(gandi, resource, id, altnames, csr, cert, all_status):
    """ Display information about a certificate.

    Resource can be a CN or an ID
    """
----------
Output: 

    if not id:
        return
    if not resource:
        return
    if not all_status:
        return
    if not isinstance(altnames, list):
        altnames = [altnames]
    if not isinstance(csr, str):
        csr = str(csr)
    if not isinstance(cert, str):
        cert = str(cert)
    if not isinstance(all_status, list):
        all_status = [all_status]
    if not all_status:
        return
    if isinstance(gandi, str):
        gandi = [gandi]
    if isinstance(resource, str
----------
Solution: 

    output_keys = ['cn', 'date_created', 'date_end', 'plan', 'status']

    if id:
        output_keys.append('id')

    if altnames:
        output_keys.append('altnames')

    if csr:
        output_keys.append('csr')

    if cert:
        output_keys.append('cert')

    ids = []
    for res in resource:
        ids.extend(gandi.certificate.usable_ids(res))

    result = []
    for num, id_ in enumerate(set(ids)):
        cert = gandi.certificate.info(id_)
        if not all_status and cert['status'] not in ['valid', 'pending']:
            continue
        if num:
            gandi.separator_line()
        cert['plan'] = package_desc(gandi, cert['package'])
        output_cert(gandi, cert, output_keys)
        result.append(cert)

    return result
--------------------
Prompt: 
def draw(self):
        """
        Clear the terminal screen and redraw all of the sub-windows
        """
----------
Output: 

        self.clear()
        self.draw_windows()

    def draw_windows(self):
        """
        Draw the windows in the current window
        """
        for win in self.windows:
            win.draw()
        self.windows = []

    def draw_windows_with_label(self, win_label):
        """
        Draw the windows in the current window with a label
        """
        for win in self.windows:
            win.draw_label(win_label)
        self.windows_label.draw()

    def draw_windows_with_label_and_window(self, win_
----------
Solution: 

        n_rows, n_cols = self.term.stdscr.getmaxyx()
        if n_rows < self.term.MIN_HEIGHT or n_cols < self.term.MIN_WIDTH:
            # TODO: Will crash when you try to navigate if the terminal is too
            # small at startup because self._subwindows will never be populated
            return

        self._row = 0
        self._draw_header()
        self._draw_banner()
        self._draw_content()
        self._draw_footer()
        self.term.clear_screen()
        self.term.stdscr.refresh()
--------------------
Prompt: 
def publish(build_entry: dict):
    """Publishes the specified build entry to docker hub"""
----------
Output: 

    if build_entry.get('docker_version') is not None:
        return _publish_docker_hub(build_entry['docker_version'], build_entry['docker_image'])
    else:
        return _publish_docker_hub(None, None)


def _publish_docker_hub(version: str, image: str) -> None:
    """Publishes a docker hub to the DockerHub"""
    if version is None:
        version = 'latest'
    if image is None:
        image = 'alpine:latest'
    try:
        client = docker.from_
----------
Solution: 

    for tag in build_entry['tags']:
        print('[PUSHING]:', tag)
        os.system('docker push {}'.format(tag))
--------------------
Prompt: 
def _read_import_root_map_file(path):
    """Reads a file mapping import paths to roots (e.g., example.org/pkg/foo -> example.org)."""
----------
Output: 

    with open(path, 'r') as f:
        return {
            os.path.basename(path): os.path.dirname(path)
            for path in f.read().splitlines()
        }


def _get_cache_dir(cache_dir):
    """Returns the cache directory for the given directory."""
    return os.path.join(cache_dir, '.cache')


def _get_cache_file(cache_file):
    """Returns the path to the given directory in the cache."""
    return os.path.join(_get_cache_dir(cache_dir
----------
Solution: 

    if os.path.exists(path):
      with open(path, 'r') as fp:
        return dict({import_path: root for import_path, root in
                     (x.strip().split('\t') for x in fp.readlines())})
    else:
      return {}
--------------------
Prompt: 
def save_config(self, cmd="write mem", confirm=False, confirm_response=""):
        """Saves Config Using Copy Run Start"""
----------
Output: 

        return self.send_command(cmd, "copy running-config startup-config")

    def get_config(self, cmd="read mem", confirm=False, confirm_response=""):
        """Gets Config using Copy Run Start"""
        return self.send_command(cmd, "running-config")

    def get_config_status(self, cmd="get running-config status", confirm=False):
        """Gets the running-config status using Get Running Config"""
        return self.send_command(cmd, "running-config status")

    def get_config_info(self, cmd="get
----------
Solution: 

        return super(CiscoIosBase, self).save_config(
            cmd=cmd, confirm=confirm, confirm_response=confirm_response
        )
--------------------
Prompt: 
def pick_cert_for_twisted(netloc, possible):
    """
    Pick the right client key/certificate to use for the given server and
    return it in the form Twisted wants.

    :param NetLocation netloc: The location of the server to consider.
    :param dict[TLSCredentials] possible: The available credentials from which
        to choose.

    :return: A two-tuple.  If no credentials were found, the elements are
        ``None`` and ``[]``.  Otherwise, the first element is a
        ``twisted.internet.ssl.PrivateCertificate`` instance representing the
        client certificate to use and the second element is a ``tuple`` of
        ``twisted.internet.ssl.Certificate`` instances representing the rest
        of the chain necessary to validate the client certificate.
    """
----------
Output: 
 use, and the second element is a
        ``twisted.internet.ssl.Certificate`` instance representing the client
        certificate to use.
    """
    for cert in possible.values():
        if cert is not None:
            return (cert, None)
    for entry in NetLocation.get_all():
        if entry is not None:
            return (entry.get_peer_certificate(), None)
    raise ValueError("No suitable certificate found for %s" % (netloc,))

def _get_client_certificate(netloc):
    """
    Pick the right client certificate to use for the given server and
    return it
----------
Solution: 

    try:
        creds = possible[netloc]
    except KeyError:
        return (None, ())

    key = ssl.KeyPair.load(creds.key.as_bytes(), FILETYPE_PEM)
    return (
        ssl.PrivateCertificate.load(
            creds.chain.certificates[0].as_bytes(), key, FILETYPE_PEM,
        ),
        tuple(
            ssl.Certificate.load(cert.as_bytes(), FILETYPE_PEM)
            for cert
            in creds.chain.certificates[1:]
        ),
    )
--------------------
Prompt: 
def fillna(self, value, limit=None, inplace=False, downcast=None):
        """ fillna on the block with the value. If we fail, then convert to
        ObjectBlock and try again
        """
----------
Output: 

        if inplace:
            raise ValueError("Cannot fillna with a block with a block inplace")
        values = np.asarray(value)
        values = values.fillna(value, limit=limit)
        return self.make_block_same_class(values)

    def fillna_indexer(self, value, limit=None):
        """ fillna on the block with the value.  If we fail, then convert to
        ObjectBlock and try again
        """
        if inplace:
            raise ValueError("Cannot fillna with a block with a block inplace")
        values = np.asarray(value)
        values
----------
Solution: 

        inplace = validate_bool_kwarg(inplace, 'inplace')

        if not self._can_hold_na:
            if inplace:
                return self
            else:
                return self.copy()

        mask = isna(self.values)
        if limit is not None:
            if not is_integer(limit):
                raise ValueError('Limit must be an integer')
            if limit < 1:
                raise ValueError('Limit must be greater than 0')
            if self.ndim > 2:
                raise NotImplementedError("number of dimensions for 'fillna' "
                                          "is currently limited to 2")
            mask[mask.cumsum(self.ndim - 1) > limit] = False

        # fillna, but if we cannot coerce, then try again as an ObjectBlock
        try:
            values, _ = self._try_coerce_args(self.values, value)
            blocks = self.putmask(mask, value, inplace=inplace)
            blocks = [b.make_block(values=self._try_coerce_result(b.values))
                      for b in blocks]
            return self._maybe_downcast(blocks, downcast)
        except (TypeError, ValueError):

            # we can't process the value, but nothing to do
            if not mask.any():
                return self if inplace else self.copy()

            # operate column-by-column
            def f(m, v, i):
                block = self.coerce_to_target_dtype(value)

                # slice out our block
                if i is not None:
                    block = block.getitem_block(slice(i, i + 1))
                return block.fillna(value,
                                    limit=limit,
                                    inplace=inplace,
                                    downcast=None)

            return self.split_and_operate(mask, f, inplace)
--------------------
Prompt: 
def CaffeLMDB(lmdb_path, shuffle=True, keys=None):
    """
    Read a Caffe LMDB file where each value contains a ``caffe.Datum`` protobuf.
    Produces datapoints of the format: [HWC image, label].

    Note that Caffe LMDB format is not efficient: it stores serialized raw
    arrays rather than JPEG images.

    Args:
        lmdb_path, shuffle, keys: same as :class:`LMDBData`.

    Returns:
        a :class:`LMDBDataDecoder` instance.

    Example:
        .. code-block:: python

            ds = CaffeLMDB("/tmp/validation", keys='{:0>8d}')
    """
----------
Output: 
 python

            # Decode the raw label file.
            caffe_label = caffe.proto.caffe_pb2.DatumProto()
            caffe_label.ParseFromString(open(label_file, 'rb').read())

            # Decode the raw image file.
            image = caffe.io.decode_caffe_pb(image_data)

            # Decode the label.
            labels = caffe.io.decode_labels(image, caffe_label)
    """
    if keys is None:
        keys = []
    if shuffle:
        random.shuffle(keys)
    if keys:
        datums
----------
Solution: 


    cpb = get_caffe_pb()
    lmdb_data = LMDBData(lmdb_path, shuffle, keys)

    def decoder(k, v):
        try:
            datum = cpb.Datum()
            datum.ParseFromString(v)
            img = np.fromstring(datum.data, dtype=np.uint8)
            img = img.reshape(datum.channels, datum.height, datum.width)
        except Exception:
            log_once("Cannot read key {}".format(k), 'warn')
            return None
        return [img.transpose(1, 2, 0), datum.label]
    logger.warn("Caffe LMDB format doesn't store jpeg-compressed images, \
        it's not recommended due to its inferior performance.")
    return LMDBDataDecoder(lmdb_data, decoder)
--------------------
Prompt: 
def _init_metadata(self):
        """Have to call these all separately because they are "end" classes,
        with no super() in them. Non-cooperative."""
----------
Output: 

        self.metadata = {}
        self.metadata['id'] = self.id
        self.metadata['name'] = self.name
        self.metadata['description'] = self.description
        self.metadata['author'] = self.author
        self.metadata['version'] = self.version
        self.metadata['url'] = self.url
        self.metadata['license'] = self.license
        self.metadata['copyright'] = self.copyright
        self.metadata['version'] = self.version
        self.metadata['author'] = self.author
        self.metadata['email'] = self.email
       
----------
Solution: 

        ItemTextsFormRecord._init_metadata(self)
        ItemFilesFormRecord._init_metadata(self)
        edXBaseFormRecord._init_metadata(self)
        IRTItemFormRecord._init_metadata(self)
        TimeValueFormRecord._init_metadata(self)
        ProvenanceFormRecord._init_metadata(self)
        super(edXItemFormRecord, self)._init_metadata()
--------------------
Prompt: 
def update_paths_and_config(self, config, pkg_dir_name,
                                pkg_cache_dir=None):
        """Handle remote source defined sys.paths & configs.

        Args:
            config (dict): git config dictionary
            pkg_dir_name (string): directory name of the stacker archive
            pkg_cache_dir (string): fully qualified path to stacker cache
                                    cache directory

        """
----------
Output: 

        if pkg_dir_name is None:
            pkg_dir_name = os.path.join(config['git_dir'], 'packages')
        if pkg_cache_dir is None:
            pkg_cache_dir = os.path.join(config['git_dir'],'stacker-cache')
        if pkg_dir_name is None:
            pkg_dir_name = os.path.join(config['git_dir'], 'config')
        if pkg_dir_name is None:
            return
        if pkg_cache_dir is None:
            pkg_cache_dir =
----------
Solution: 

        if pkg_cache_dir is None:
            pkg_cache_dir = self.package_cache_dir
        cached_dir_path = os.path.join(pkg_cache_dir, pkg_dir_name)

        # Add the appropriate directory (or directories) to sys.path
        if config.get('paths'):
            for path in config['paths']:
                path_to_append = os.path.join(cached_dir_path,
                                              path)
                logger.debug("Appending \"%s\" to python sys.path",
                             path_to_append)
                sys.path.append(path_to_append)
        else:
            sys.path.append(cached_dir_path)

        # If the configuration defines a set of remote config yamls to
        # include, add them to the list for merging
        if config.get('configs'):
            for config_filename in config['configs']:
                self.configs_to_merge.append(os.path.join(cached_dir_path,
                                                          config_filename))
--------------------
Prompt: 
async def destroy_tournament(self, t: Tournament):
        """ completely removes a tournament from Challonge

        |methcoro|

        Note:
            |from_api| Deletes a tournament along with all its associated records. There is no undo, so use with care!

        Raises:
            APIException

        """
----------
Output: 

        await self.bot.say("You are not allowed to destroy a challonge.")

    @commands.command(pass_context=True, aliases=['add_tournament'])
    async def add_tournament_record(self, t: TournamentRecord):
        """ adds a tournament record to the current list of records

        Args:
            t: Tournament record

        Raises:
            APIException

        """
        await self.to_api.add_tournament_record(t)

    @commands.command(pass_context=True, aliases=['destroy_tournament'])
    async def destroy_
----------
Solution: 

        await self.connection('DELETE', 'tournaments/{}'.format(t.id))
        if t in self.tournaments:
            self.tournaments.remove(t)
--------------------
Prompt: 
def get_issuers(self):
        """
        Gets the issuers (from message and from assertion)

        :returns: The issuers
        :rtype: list
        """
----------
Output: 

        return self._issuers

    def get_issuers_with_message(self, message):
        """
        Gets the issuers with a message

        :param message: The message
        :type message: str
        :returns: The issuers with a message
        :rtype: list
        """
        return self._issuers

    def get_issuers_with_assertion(self, assertions):
        """
        Gets the issuers with a list of assertions

        :param assertions: The assertions
        :type assertions: list
        :returns: The issuers with a list of assertions
       
----------
Solution: 

        issuers = []

        message_issuer_nodes = OneLogin_Saml2_Utils.query(self.document, '/samlp:Response/saml:Issuer')
        if len(message_issuer_nodes) > 0:
            if len(message_issuer_nodes) == 1:
                issuers.append(OneLogin_Saml2_Utils.element_text(message_issuer_nodes[0]))
            else:
                raise OneLogin_Saml2_ValidationError(
                    'Issuer of the Response is multiple.',
                    OneLogin_Saml2_ValidationError.ISSUER_MULTIPLE_IN_RESPONSE
                )

        assertion_issuer_nodes = self.__query_assertion('/saml:Issuer')
        if len(assertion_issuer_nodes) == 1:
            issuers.append(OneLogin_Saml2_Utils.element_text(assertion_issuer_nodes[0]))
        else:
            raise OneLogin_Saml2_ValidationError(
                'Issuer of the Assertion not found or multiple.',
                OneLogin_Saml2_ValidationError.ISSUER_NOT_FOUND_IN_ASSERTION
            )

        return list(set(issuers))
--------------------
Prompt: 
def blob(self, sha):
        """Get the blob indicated by ``sha``.

        :param str sha: (required), sha of the blob
        :returns: :class:`Blob <github3.git.Blob>` if successful, otherwise
            None
        """
----------
Output: 

        return self._get_single_ref(self._get_blob, sha)

    def _get_single_ref(self, ref, sha):
        """Get a single reference.

        :param str ref: (required), reference name
        :param str sha: (required), SHA of the reference
        :returns: :class:`Reference <github3.git.Reference>` if successful,
            otherwise None
        """
        return self._get_ref(self._get_blob, ref, sha)

    def _get_ref(self, ref, ref_type, sha):
        """Get a reference
----------
Solution: 

        url = self._build_url('git', 'blobs', sha, base_url=self._api)
        json = self._json(self._get(url), 200)
        return Blob(json) if json else None
--------------------
Prompt: 
def _get_data_from_bigquery(self, queries):
    """Get data from bigquery table or query."""
----------
Output: 

    if self.table_schema:
      return self.table_schema.get_table(self.table_name, queries)
    else:
      return self.bigquery_client.get_table(self.dataset_name, queries)

  def _get_table_schema(self):
    """Get table schema."""
    if self.table_schema:
      return self.table_schema
    else:
      return self.bigquery_client.get_schema(self.dataset_name)

  def _get_table_name(self):
    """Get table name."""
    if self.dataset_name
----------
Solution: 


    all_df = []
    for query in queries:
      all_df.append(query.execute().result().to_dataframe())
    df = pd.concat(all_df, ignore_index=True)
    return df
--------------------
Prompt: 
def _parse_canonical_double(doc):
    """Decode a JSON double to python float."""
----------
Output: 

    if not isinstance(doc, dict):
        return None
    return float(doc["double"])


def _parse_canonical_string(doc):
    """Decode a JSON string to python str."""
    if not isinstance(doc, dict):
        return None
    return doc["string"]


def _parse_canonical_string_list(doc):
    """Decode a JSON string to python list."""
    if not isinstance(doc, list):
        return None
    return doc["list"]


def _parse_canonical_time(doc):
    """Decode a JSON time to python datetime."""
    if not isinstance
----------
Solution: 

    d_str = doc['$numberDouble']
    if len(doc) != 1:
        raise TypeError('Bad $numberDouble, extra field(s): %s' % (doc,))
    if not isinstance(d_str, string_type):
        raise TypeError('$numberDouble must be string: %s' % (doc,))
    return float(d_str)
--------------------
Prompt: 
def version(self, value):
        """
        Setter for **self.__version** attribute.

        :param value: Attribute value.
        :type value: unicode
        """
----------
Output: 


        self.__version = value

    def __init__(self, value):
        """
        Setter for **self.__version** attribute.

        :param value: Attribute value.
        :type value: unicode
        """

        self.set_attr(value)

    def set_attr(self, value):
        """
        Setter for **self.__attributes** attribute.

        :param value: Attribute value.
        :type value: unicode
        """

        self.attr = value

    def version(self):
        """
        Getter for **self.__version** attribute.

        :return: Attribute value.
        :rtype:
----------
Solution: 


        if value is not None:
            assert type(value) is unicode, "'{0}' attribute: '{1}' type is not 'unicode'!".format(
                "version", value)
        self.__version = value
--------------------
Prompt: 
def get_portchannel_info_by_intf_output_lacp_actor_priority(self, **kwargs):
        """Auto Generated Code
        """
----------
Output: 

        config = ET.Element("config")
        portchannel_info = ET.SubElement(config, "portchannel-info", xmlns="urn:brocade.com:mgmt:brocade-interface")
        portchannel_info_name = ET.SubElement(portchannel_info, "portchannel-info-name")
        portchannel_info_name.text = kwargs.pop('name')

        callback = kwargs.pop('callback', self._callback)
        return callback(portchannel_info, **kwargs)

    def get_portchannel_info_output_lacp_actor_priority_info
----------
Solution: 

        config = ET.Element("config")
        get_portchannel_info_by_intf = ET.Element("get_portchannel_info_by_intf")
        config = get_portchannel_info_by_intf
        output = ET.SubElement(get_portchannel_info_by_intf, "output")
        lacp = ET.SubElement(output, "lacp")
        actor_priority = ET.SubElement(lacp, "actor-priority")
        actor_priority.text = kwargs.pop('actor_priority')

        callback = kwargs.pop('callback', self._callback)
        return callback(config)
--------------------
Prompt: 
def sparse_arrays(self, value):
        """Validate and enable spare arrays."""
----------
Output: 

        if not isinstance(value, (np.ndarray, SparseArray)):
            raise TypeError("SparseArray.sparse_arrays must be called with a "
                            "sparse matrix as its input.")
        if value.dtype.kind!= "b":
            raise TypeError("SparseArray.sparse_arrays must be called with a "
                            "b matrix as its input.")
        if value.dtype.kind!= "c":
            raise TypeError("SparseArray.sparse_arrays must be called with a "
                            "c matrix as its input.")

    def __array_ufunc__(self, ufunc, method, *inputs, **kwargs):
----------
Solution: 

        if not isinstance(value, bool):
            raise TypeError('sparse_arrays attribute must be a logical type.')
        self._sparse_arrays = value
--------------------
Prompt: 
def police_priority_map_conform_map_pri7_conform(self, **kwargs):
        """Auto Generated Code
        """
----------
Output: 

        config = ET.Element("config")
        police_priority_map = ET.SubElement(config, "police-priority-map")
        police_priority_map_name = ET.SubElement(police_priority_map, "police-priority-map-name")
        police_priority_map_name.text = kwargs.pop('police_priority_map_name')

        callback = kwargs.pop('callback', self._callback)
        return callback(config)
        
    def police_priority_map_conform_map_pri_map_name(self,
----------
Solution: 

        config = ET.Element("config")
        police_priority_map = ET.SubElement(config, "police-priority-map", xmlns="urn:brocade.com:mgmt:brocade-policer")
        name_key = ET.SubElement(police_priority_map, "name")
        name_key.text = kwargs.pop('name')
        conform = ET.SubElement(police_priority_map, "conform")
        map_pri7_conform = ET.SubElement(conform, "map-pri7-conform")
        map_pri7_conform.text = kwargs.pop('map_pri7_conform')

        callback = kwargs.pop('callback', self._callback)
        return callback(config)
--------------------
Prompt: 
def save(self, **kwargs):
        """
        Save and return a list of object instances.
        """
----------
Output: 

        if self.pk is None:
            raise Exception("Cannot save an unsaved object")
        return [self.get_object(pk=o.pk) for o in self.get_queryset(**kwargs)]

    def get_queryset(self, **kwargs):
        """
        Returns a queryset matching the given keyword arguments.
        """
        return self.model._default_manager.filter(**kwargs)

    def get_context_data(self, **kwargs):
        """
        Insert the list of messages into the context for the response.
        """
        context = super(MessageListView, self).get_context_
----------
Solution: 

        # Guard against incorrect use of `serializer.save(commit=False)`
        assert 'commit' not in kwargs, (
            "'commit' is not a valid keyword argument to the 'save()' method. "
            "If you need to access data before committing to the database then "
            "inspect 'serializer.validated_data' instead. "
            "You can also pass additional keyword arguments to 'save()' if you "
            "need to set extra attributes on the saved model instance. "
            "For example: 'serializer.save(owner=request.user)'.'"
        )

        validated_data = [
            dict(list(attrs.items()) + list(kwargs.items()))
            for attrs in self.validated_data
        ]

        if self.instance is not None:
            self.instance = self.update(self.instance, validated_data)
            assert self.instance is not None, (
                '`update()` did not return an object instance.'
            )
        else:
            self.instance = self.create(validated_data)
            assert self.instance is not None, (
                '`create()` did not return an object instance.'
            )

        return self.instance
--------------------
Prompt: 
def create_transaction_output(address, output_value):
    """
    Create a single transaction output
    :param address:
    :param output_value:
    :return:
    """
----------
Output: 

    return TransactionOutput(
        address=address,
        amount=output_value,
        fee=output_value,
        fee_currency=output_value,
        fee_value=output_value,
        fee_currency_id=output_value,
        fee_value_currency=output_value,
        fee_value_value=output_value,
        fee_value_value_currency_id=output_value,
        fee_value_value=output_value,
        fee_value_id=output_value,
        fee_value,
        fee_currency,
        fee_value_currency
----------
Solution: 

    bitcoin_address = CBitcoinAddress(address)
    tx_out = CMutableTxOut(output_value, bitcoin_address.to_scriptPubKey())
    return tx_out
--------------------
Prompt: 
def set_years(self, startyear=1765, endyear=2100):
        """
        Set the start and end dates of the simulations.

        Parameters
        ----------
        startyear : int
            Start year of the simulation

        endyear : int
            End year of the simulation

        Returns
        -------
        dict
            The contents of the namelist
        """
----------
Output: 

        self.namelist['years'] = startyear, endyear
        return self

    def set_time_units(self, units='seconds'):
        """
        Set the time units of the simulation.

        Parameters
        ----------
        units : str
            The units of the time dimension.

        Returns
        -------
        dict
            The contents of the namelist
        """
        self.namelist['time_units'] = units,
        return self

    def set_time_units_units(self, units='seconds'):
        """
        Set the units of the time dimension.

        Parameters
        ----------
        units : str
            The units of the time
----------
Solution: 

        # TODO: test altering stepsperyear, I think 1, 2 and 24 should all work
        return self.set_config(
            "MAGCFG_NMLYEARS.CFG",
            "nml_years",
            endyear=endyear,
            startyear=startyear,
            stepsperyear=12,
        )
--------------------
Prompt: 
def target_sdp_state(self, state):
        """Update the target state of SDP."""
----------
Output: 

        self._update_device_info(state)
        self._state = state

    def _update_device_info(self, state):
        """Update the device info."""
        self._device_info = {
            "identifiers": {(DOMAIN, self._device_id)},
            "name": state.name,
            "manufacturer": state.manufacturer,
            "model": state.model,
            "sw_version": state.swversion,
        }


class SsdpEntity(MediaPlayerDevice):
    """Representation of a SSDP entity."""

    def __init__(self, ssdp_
----------
Solution: 

        LOG.info('Setting SDP target state to %s', state)
        if self._sdp_state.current_state == state:
            LOG.info('Target state ignored, SDP is already "%s"!', state)
        if state == 'on':
            self.set_state(DevState.ON)
        if state == 'off':
            self.set_state(DevState.OFF)
        if state == 'standby':
            self.set_state(DevState.STANDBY)
        if state == 'disable':
            self.set_state(DevState.DISABLE)
        self._sdp_state.update_target_state(state)
--------------------
Prompt: 
def presenter(self, presenter_name):
		""" :meth:`.WWebPresenterCollectionProto.presenter` method implementation
		"""
----------
Output: 

		return WWebPresenter(self, presenter_name)

	def get_presenter(self, presenter_name):
		""" :meth:`.WWebPresenterCollectionProto.get_presenter` implementation
		"""
		return WWebPresenter(self, presenter_name)

	def get_presenter_by_name(self, presenter_name):
		""" :meth:`.WWebPresenterCollectionProto.get_presenter_by_name` implementation
		"""
		return WWebPresenterCollection(self, presenter_name
----------
Solution: 

		if presenter_name in self.__presenters.keys():
			return self.__presenters[presenter_name]
--------------------
Prompt: 
def work_once(self, free_pool_slots=1, max_jobs=None):
        """ Does one lookup for new jobs, inside the inner work loop """
----------
Output: 

        if max_jobs is None:
            max_jobs = self.max_jobs
        if max_jobs < 1:
            max_jobs = 1
        if max_jobs > 1:
            max_jobs = 1
        if max_jobs == 0:
            max_jobs = self.max_jobs
        if max_jobs > 1:
            max_jobs = 1
        if max_jobs > 0:
            self.logger.info("Using %d jobs", max_jobs)
        else:
            self.logger.info("Using 1 jobs")
        self.logger.info("Using %d slots",
----------
Solution: 


        dequeued_jobs = 0

        available_queues = [
            queue for queue in self.queues
            if queue.root_id not in self.paused_queues and
            queue.id not in self.paused_queues
        ]

        for queue_i in range(len(available_queues)):

            queue = available_queues[(queue_i + self.queue_offset) % len(available_queues)]

            max_jobs_per_queue = free_pool_slots - dequeued_jobs

            if max_jobs_per_queue <= 0:
                queue_i -= 1
                break

            if self.config["dequeue_strategy"] == "parallel":
                max_jobs_per_queue = max(1, int(max_jobs_per_queue / (len(available_queues) - queue_i)))

            for job in queue.dequeue_jobs(
                max_jobs=max_jobs_per_queue,
                job_class=self.job_class,
                worker=self
            ):
                dequeued_jobs += 1

                self.gevent_pool.spawn(self.perform_job, job)

        # At the next pass, start at the next queue to avoid always dequeuing the same one
        if self.config["dequeue_strategy"] == "parallel":
            self.queue_offset = (self.queue_offset + queue_i + 1) % len(self.queues)

        # TODO consider this when dequeuing jobs to have strict limits
        if max_jobs and self.done_jobs >= max_jobs:
            self.log.info("Reached max_jobs=%s" % self.done_jobs)
            return "break", dequeued_jobs

        # We seem to have exhausted available jobs, we can sleep for a
        # while.
        if dequeued_jobs == 0:

            if self.config["dequeue_strategy"] == "burst":
                self.log.info("Burst mode: stopping now because queues were empty")
                return "break", dequeued_jobs

            return "wait", dequeued_jobs

        return None, dequeued_jobs
--------------------
Prompt: 
def serveMonth(self, request, year=None, month=None):
        """Monthly calendar view."""
----------
Output: 

        if year is None:
            year = self.currentYear
        if month is None:
            month = self.currentMonth
        if year is None:
            month = self.currentYear + 1
        if month is None:
            month = 1
        if month is None:
            month = 12
        if year is None:
            year = self.currentYear
        if month is None:
            month = self.currentMonth + 1
        if year is None:
            year = self.currentYear
        if month is None:
            month = 12
        if year is None:
            year = self.currentYear
        if month
----------
Solution: 

        myurl = self.get_url(request)
        def myUrl(urlYear, urlMonth):
            if 1900 <= urlYear <= 2099:
                return myurl + self.reverse_subpage('serveMonth',
                                                    args=[urlYear, urlMonth])
        today = timezone.localdate()
        if year is None: year = today.year
        if month is None: month = today.month
        year = int(year)
        month = int(month)

        if year == today.year and month == today.month:
            weekNum = gregorian_to_week_date(today)[1]
        else:
            weekNum = gregorian_to_week_date(dt.date(year, month, 7))[1]
        weeklyUrl = myurl + self.reverse_subpage('serveWeek',
                                                 args=[year, weekNum])
        listUrl = myurl + self.reverse_subpage('serveUpcoming')

        prevMonth = month - 1
        prevMonthYear = year
        if prevMonth == 0:
            prevMonth = 12
            prevMonthYear -= 1

        nextMonth = month + 1
        nextMonthYear = year
        if nextMonth == 13:
            nextMonth = 1
            nextMonthYear += 1

        # TODO Consider changing to a TemplateResponse
        # https://stackoverflow.com/questions/38838601
        return render(request, "joyous/calendar_month.html",
                      {'self':         self,
                       'page':         self,
                       'version':      __version__,
                       'year':         year,
                       'month':        month,
                       'today':        today,
                       'yesterday':    today - dt.timedelta(1),
                       'lastweek':     today - dt.timedelta(7),
                       'prevMonthUrl': myUrl(prevMonthYear, prevMonth),
                       'nextMonthUrl': myUrl(nextMonthYear, nextMonth),
                       'prevYearUrl':  myUrl(year - 1, month),
                       'nextYearUrl':  myUrl(year + 1, month),
                       'weeklyUrl':    weeklyUrl,
                       'listUrl':      listUrl,
                       'thisMonthUrl': myUrl(today.year, today.month),
                       'monthName':    MONTH_NAMES[month],
                       'weekdayAbbr':  weekday_abbr,
                       'events':       self._getEventsByWeek(request, year, month)})
--------------------
Prompt: 
def remove_each(text, items, count=None, strip=False):
    """
    Like ``remove``, where each occurrence in ``items`` is ``what`` to remove.
    """
----------
Output: 

    if count is not None:
        return [i for i in items if i.text == text][0]
    else:
        return [i for i in items if i.text!= text][0]


def remove_all(text, items):
    """
    Like ``remove_each``, but removes all occurrences of ``text`` in ``items``.
    """
    if count is not None:
        return [i for i in items if i.text == text]
    else:
        return [i for i in items if i.text!= text]


def remove_all_by_text
----------
Solution: 

    for item in items:
        text = remove(text, item, count=count, strip=strip)
    return text
--------------------
Prompt: 
def triangulate(self, **kwargs):
        """
        Create a region- aware triangulation of the 2D path.

        Parameters
        -------------
        **kwargs : dict
          Passed to trimesh.creation.triangulate_polygon

        Returns
        -------------
        vertices : (n, 2) float
          2D vertices of triangulation
        faces : (n, 3) int
          Indexes of vertices for triangles
        """
----------
Output: 

        return trimesh.creation.create_polygon(self, **kwargs)

    def triangulate_points(self, **kwargs):
        """
        Create a region-aware triangulation of the 2D path.

        Parameters
        -------------
        **kwargs : dict
          Passed to trimesh.creation.triangulate_polygon

        Returns
        -------------
        vertices : (n, 3) float
          2D points of triangulation
        faces : (n, 3) int
          Indexes of vertices for triangles
        """
        return trimesh.creation.create_points(self, **kwargs)

   
----------
Solution: 

        from ..creation import triangulate_polygon

        # append vertices and faces into sequence
        v_seq = []
        f_seq = []

        # loop through polygons with interiors
        for polygon in self.polygons_full:
            v, f = triangulate_polygon(polygon, **kwargs)
            v_seq.append(v)
            f_seq.append(f)

        return util.append_faces(v_seq, f_seq)
--------------------
Prompt: 
def fix_indentation(code, new_indents):
    """Change the indentation of `code` to `new_indents`"""
----------
Output: 

    if not code:
        return code
    if not new_indents:
        return '\n'
    return '\n'.join(new_indents) + '\n'


def get_indent(code):
    """Return the indentation of `code`"""
    if not code:
        return ''
    return fix_indentation(code, get_indents(code))


def get_indents(code):
    """Return the number of spaces in `code`"""
    if not code:
        return ''
    return fix_indentation(code, get_indents(code))


def get
----------
Solution: 

    min_indents = find_minimum_indents(code)
    return indent_lines(code, new_indents - min_indents)
--------------------
Prompt: 
def general_setting(key, default=None, expected_type=None, qsettings=None):
    """Helper function to get a value from settings.

    :param key: Unique key for setting.
    :type key: basestring

    :param default: The default value in case of the key is not found or there
        is an error.
    :type default: basestring, None, boolean, int, float

    :param expected_type: The type of object expected.
    :type expected_type: type

    :param qsettings: A custom QSettings to use. If it's not defined, it will
        use the default one.
    :type qsettings: qgis.PyQt.QtCore.QSettings

    :returns: The value of the key in the setting.
    :rtype: object

    Note:
    The API for QSettings to get a value is different for PyQt and Qt C++.
    In PyQt we can specify the expected type.
    See: http://pyqt.sourceforge.net/Docs/PyQt4/qsettings.html#value
    """
----------
Output: 

        use the default value.
    :type qsettings: QSettings

    :returns: The value of the setting.
    :rtype: basestring, QPyNullVariant

    """
    if qsettings is None:
        qsettings = QSettings()
    if expected_type is None:
        expected_type = key
    if expected_type == 'bool':
        return expected_type.lower() == 'true'
    elif expected_type == 'int':
        return expected_type.lower() == 'int'
    elif expected_type == 'float':
        return expected_type.lower() == 'float'
   
----------
Solution: 

    if qsettings is None:
        qsettings = QSettings()
    try:
        if isinstance(expected_type, type):
            return qsettings.value(key, default, type=expected_type)
        else:
            return qsettings.value(key, default)

    except TypeError as e:
        LOGGER.debug('exception %s' % e)
        LOGGER.debug('%s %s %s' % (key, default, expected_type))
        return qsettings.value(key, default)
--------------------
Prompt: 
def requires(*params):
    """
    Raise ValueError if any ``params`` are omitted from the decorated kwargs.

    None values are considered omissions.

    Example usage on an AWS() method:

        @requires('zone', 'security_groups')
        def my_aws_method(self, custom_args, **kwargs):
            # We'll only get here if 'kwargs' contained non-None values for
            # both 'zone' and 'security_groups'.
    """
----------
Output: 

    def requires(*params):
        """
        Return True if any ``params`` are omitted from the decorated kwargs.
        """
        return True
    return None not in params and not kwargs


def _get_security_group_ids(zone, security_groups):
    """
    Return a list of security group IDs for a given zone.

    :param zone: The name of the zone.
    :param security_groups: The security groups to filter by.
    :return: A list of security group IDs.
    """
    if zone is not None:
        security_groups = security_groups.filter(
            zone
----------
Solution: 

    def requires(f, self, *args, **kwargs):
        missing = filter(lambda x: kwargs.get(x) is None, params)
        if missing:
            msgs = ", ".join([PARAMETERS[x]['msg'] for x in missing])
            raise ValueError("Missing the following parameters: %s" % msgs)
        return f(self, *args, **kwargs)
    return decorator(requires)
--------------------
Prompt: 
def normalize_parameters(params):
        """ Normalize parameters """
----------
Output: 

        if params is None:
            return None
        if isinstance(params, dict):
            return params
        return {k: float(v) for k, v in params.items() if v is not None}

    def _get_default_values(self):
        """ Returns default values for parameters """
        return {
           'max_length': self.max_length,
           'min_length': self.min_length,
           'max_length_in_bytes': self.max_length_in_bytes,
           'min_length_in_bytes': self.min_length_in_
----------
Solution: 

        params = params or {}
        normalized_parameters = OrderedDict()

        def get_value_like_as_php(val):
            
--------------------
Prompt: 
def get_replies(self, new=True):
        """
        Return all reply notifications for this user.

        :param new: False for all notifications, True for only non-viewed
            notifications.
        """
----------
Output: 

        if new:
            return self.replies.filter(user=self).order_by('-created')
        else:
            return self.replies.filter(user=self).order_by('-created')

    def get_last_login(self):
        """
        Return the last login of this user.
        """
        return self.last_login

    def get_last_activity(self):
        """
        Return the last activity of this user.
        """
        return self.last_activity

    def get_last_activity_by_id(self, activity_id):
        """
        Return the
----------
Solution: 

        url = (self._imgur._base_url + "/3/account/{0}/"
               "notifications/replies".format(self.name))
        return self._imgur._send_request(url, needs_auth=True)
--------------------
Prompt: 
def run(self, fastafile, params=None, tmp=None):
        """
        Run the tool and predict motifs from a FASTA file.

        Parameters
        ----------
        fastafile : str
            Name of the FASTA input file.

        params : dict, optional
            Optional parameters. For some of the tools required parameters
            are passed using this dictionary.

        tmp : str, optional
            Directory to use for creation of temporary files.
       
        Returns
        -------
        motifs : list of Motif instances
            The predicted motifs.

        stdout : str
            Standard out of the tool.
        
        stderr : str
            Standard error of the tool.
        """
----------
Output: 
 : str
            Standard error of the tool.
        """
        if params is None:
            params = {}
        if tmp is None:
            tmp = tempfile.mkdtemp()
        if not os.path.exists(tmp):
            os.makedirs(tmp)
        if not os.path.exists(self.outputdir):
            os.makedirs(self.outputdir)
        if not os.path.exists(self.output_dir):
            raise ValueError("Output directory %s does not exist" % self.outputdir)
        if not os.path.exists(self.output_dir):
            raise ValueError("
----------
Solution: 

        if not self.is_configured():
            raise ValueError("%s is not configured" % self.name)

        if not self.is_installed():
            raise ValueError("%s is not installed or not correctly configured" % self.name)
        
        self.tmpdir = mkdtemp(prefix="{0}.".format(self.name), dir=tmp)
        fastafile = os.path.abspath(fastafile)
 
        try:
            return self._run_program(self.bin(), fastafile, params)
        except KeyboardInterrupt:
            return ([], "Killed", "Killed")
--------------------
Prompt: 
def get_adapter_path(obj, to_cls):
    """
    Returns the adapter path that would be used to adapt `obj` to `to_cls`.
    """
----------
Output: 

    return '%s.%s' % (obj.__class__.__module__, to_cls.__name__)


def get_concrete_function(model, attrs):
    """
    Given a model class, returns the `ForProxyModel` that should be used
    to retrieve the concrete model class.
    """
    opts = model._meta
    return (opts.app_label, opts.module_name) + get_concrete_fields(opts.fields, opts.exclude)


def get_concrete_fields(fields, exclude=None):
    """
    Given any model field, returns a dict of all the
----------
Solution: 

    from_cls = type(obj)
    key = (from_cls, to_cls)
    if key not in __mro__:
        __mro__[key] = list(itertools.product(inspect.getmro(from_cls), inspect.getmro(to_cls)))

    return __mro__[key]
--------------------
Prompt: 
def merge_featurecollection(*jsons):
    """
    merge features into one featurecollection

    Keyword arguments:
    jsons   -- jsons object list 

    return geojson featurecollection
    """
----------
Output: 

    merged_jsons = []
    for json in jsons:
        merged_json = merge_json(json)
        merged_jsons.append(merged_json)
    return featurecollection.merge(merged_jsons)

def merge_featurecollection_by_name(*json_list):
    """
    merge features into one featurecollection by name

    Keyword arguments:
    json_list -- list of geojson objects 

    return geojson featurecollection
    """
    merged_json = []
    for json in json_list:
        merged_json.append(merge_json(json, name=None
----------
Solution: 

    features = []
    for json in jsons:
        if json['type'] == 'FeatureCollection':
            for feature in json['features']:
                features.append(feature)
    return {"type":'FeatureCollection', "features":features}
--------------------
Prompt: 
def get_block_adjustments(crypto, points=None, intervals=None, **modes):
    """
    This utility is used to determine the actual block rate. The output can be
    directly copied to the `blocktime_adjustments` setting.
    """
----------
Output: 

    if points is None:
        points = crypto.get_points()
    if intervals is None:
        intervals = crypto.get_intervals()
    if modes is None:
        modes = crypto.get_modes()
    if crypto.has_key('blocktime'):
        blocktime_points = []
        for point in points:
            blocktime_points.append(crypto.get_points(point))
        blocktime_points = sorted(blocktime_points, key=lambda x: x[1])
        for point in blocktime_points:
            if point[0] in modes:
                point[
----------
Solution: 

    from moneywagon import get_block
    all_points = []

    if intervals:
        latest_block_height = get_block(crypto, latest=True, **modes)['block_number']
        interval = int(latest_block_height / float(intervals))
        all_points = [x * interval for x in range(1, intervals - 1)]

    if points:
        all_points.extend(points)

    all_points.sort()

    adjustments = []
    previous_point = 0
    previous_time = (crypto_data[crypto.lower()].get('genesis_date').replace(tzinfo=pytz.UTC)
        or get_block(crypto, block_number=0, **modes)['time']
    )

    for point in all_points:
        if point == 0:
            continue
        point_time = get_block(crypto, block_number=point, **modes)['time']
        length = point - previous_point
        minutes = (point_time - previous_time).total_seconds() / 60
        rate = minutes / length
        adjustments.append([previous_point, rate])

        previous_time = point_time
        previous_point = point

    return adjustments
--------------------
Prompt: 
def pycode_to_body(co, context):
    """
    Convert a Python code object to a list of AST body elements.
    """
----------
Output: 

    if isinstance(co, (ast.Tuple, ast.List)):
        return [pycode_to_tuple(co, context) for co in co]
    elif isinstance(co, ast.Dict):
        return {key: pycode_to_body(value, context) for key, value in co.items()}
    elif isinstance(co, (ast.ListComp, ast.Tuple)):
        return pycode_to_body(co, context)
    elif isinstance(co, ast.List):
        return [pycode_to_list(co, context) for co in co]
   
----------
Solution: 

    code = Code.from_pycode(co)

    # On each instruction, temporarily store all the jumps to the **next**
    # instruction.  This is used in _make_expr to determine when an expression
    # is part of a short-circuiting expression.
    for a, b in sliding_window(2, code.instrs):
        a._next_target_of = b._target_of
    b._next_target_of = set()

    try:
        body = instrs_to_body(deque(code.instrs), context)
        if context.in_function_block:
            return make_global_and_nonlocal_decls(code.instrs) + body
        return body
    finally:
        # Clean up jump target data.
        for i in code.instrs:
            del i._next_target_of
--------------------
Prompt: 
def get_params_from_list(params_list):
    """Transform params list to dictionary.
    """
----------
Output: 

    params = {}
    for param in params_list:
        if param in ('page', 'page_size'):
            params['page_' + param] = int(params_list.pop(0)[param])
        elif param in ('sort','sort_dir'):
            params['sort__' + param] = params_list.pop(0)[param]
        elif param in ('sort_key','sort_key_dir'):
            params['sort__' + param] = params_list.pop(0)[param]
        elif param in ('sort_key_type','sort_key
----------
Solution: 

    params = {}
    for i in range(0, len(params_list)):
        if '=' not in params_list[i]:
            try:
                if not isinstance(params[key], list): params[key] = [params[key]]
                params[key] += [params_list[i]]
            except KeyError:
                raise ValueError('Pass parameters like `key1=a key2=b c d key3=...`.')
        else:
            key_val = params_list[i].split('=')
            key, val = key_val
            params[key] = convert_string(val)
    return params
--------------------
Prompt: 
def gui():
    """remove libraries by GUI."""
----------
Output: 

    for lib in list(libraries):
        if lib.is_installed():
            libraries.remove(lib)
    return libraries


def main():
    """Run the script."""
    if len(sys.argv) < 2:
        print("Usage: %s <path to library>" % sys.argv[0])
        sys.exit(1)

    # get the path to the library
    library_path = sys.argv[1]

    # get the path to the script
    script_path = sys.argv[2]

    # get the path to the output file
    output_path = sys.
----------
Solution: 


    sel = psidialogs.multi_choice(libraries(),
                                  'select libraries to remove from %s!' % libraries_dir(),
                                  title='remove boards')
    print('%s selected' % sel)

    if sel:
        if psidialogs.ask_yes_no('Do you really want to remove selected libraries?\n' + '\n'.join(sel)):
            for x in sel:
                remove_lib(x)
                print('%s was removed' % x)
--------------------
Prompt: 
def rock(cls, data, eps, number_clusters, threshold=0.5, ccore=False):
        """
        Constructor of the ROCK cluster analysis algorithm

        :param eps: Connectivity radius (similarity threshold), points are neighbors if distance between them is less than connectivity radius
        :param number_clusters: Defines number of clusters that should be allocated from the input data set
        :param threshold: Value that defines degree of normalization that influences on choice of clusters for merging during processing
        :param ccore: Defines should be CCORE (C++ pyclustering library) used instead of Python code or not.
        :return: The resulting clustering.rst object
        """
----------
Output: 

        :return:
        """
        super(ROCK, cls).__init__(data, number_clusters, threshold, ccore)
        self.rocks = {}
        self.rocks_count = {}
        self.rocks_count_count = {}
        self.rocks_count_count = {}
        for i in range(number_clusters):
            self.rocks[i] = data.rocks[i]
            self.rocks_count[i] = data.rocks_count[i]
            self.rocks_count_count[i] = data.rocks
----------
Solution: 

        data = cls.input_preprocess(data)
        model = rock(data, eps, number_clusters, threshold, ccore)
        return cls(model)
--------------------
Prompt: 
def preview(self, stream=sys.stdout):
        """A quick preview of docpie. Print all the parsed object"""
----------
Output: 

        print(self.parse(stream))

    def parse(self, stream):
        """Parse the given stream and return a list of (text, tag, attrs)"""
        parser = DocpieParser(stream)
        parser.parse()
        return parser.get_text(), parser.get_tag(), parser.get_attrs()

    def get_attrs(self):
        """Return a list of (tag, attrs) for the parsed object"""
        return self.attrs

    def get_text(self):
        """Return the text of the parsed object"""
        return ''.join(self.attrs)

    def get
----------
Solution: 


        write = stream.write

        write(('[Quick preview of Docpie %s]' % self._version).center(80, '='))
        write('\n')

        write(' sections '.center(80, '-'))

        write('\n')
        write(self.usage_text)
        write('\n')
        option_sections = self.option_sections
        if option_sections:
            write('\n')
            write('\n'.join(option_sections.values()))
        write('\n')

        write(' str '.center(80, '-'))
        write('\n[%s]\n' % self.usage_name)
        for each in self.usages:
            write('    %s\n' % each)
        write('\n[Options:]\n\n')
        for title, sections in self.options.items():
            if title:
                full_title = '%s %s' % (title, self.option_name)
            else:
                full_title = self.option_name

            write(full_title)
            write('\n')

            for each in sections:
                write('    %s\n' % each)

            write('\n')

        write(' repr '.center(80, '-'))
        write('\n[%s]\n' % self.usage_name)
        for each in self.usages:
            write('    %r\n' % each)
        write('\n[Options:]\n\n')
        for title, sections in self.options.items():
            if title:
                full_title = '%s %s' % (title, self.option_name)
            else:
                full_title = self.option_name

            write(full_title)
            write('\n')

            for each in sections:
                write('    %r\n' % each)

            write('\n')

        write(' auto handlers '.center(80, '-'))
        write('\n')
        for key, value in self.extra.items():
            write('%s %s\n' % (key, value))
--------------------
Prompt: 
def issuer(self):
        """The certificate issuer field as :py:class:`~django_ca.subject.Subject`."""
----------
Output: 

        return self.cleaned_data.get('issuer', None)

    def __init__(self, *args, **kwargs):
        """
        :param args: The arguments to use when instantiating the
            :py:class:`~django_ca.certificate.Certificate` object.
        :param kwargs: The keyword arguments to use when instantiating the
            :py:class:`~django_ca.certificate.Certificate` object.
        """
        super(CertificateForm, self).__init__(*args, **kwargs)
        self.fields['certificate'].choices = self.fields['certificate'].choices + (
            (cert.pk
----------
Solution: 

        return Subject([(s.oid, s.value) for s in self.x509.issuer])
--------------------
Prompt: 
def sample_following_dist(handle_iter, n, totalf):
    """Samples n passwords following the distribution from the handle
    @handle_iter is an iterator that gives (pw,f) @n is the total
    number of samle asked for @totalf is the total number of users,
    which is euqal to sum(f for pw,f in handle_iter)
    As, handle_iterator is an iterator and can only traverse once, @totalf
    needs to be supplied to the funciton.

    Returns, an array of @n tuples (id, pw) sampled from @handle_iter.
    """
----------
Output: 
 from the handle_iter
    """
    for pw, f in handle_iter:
        if f == 0:
            yield (id, pw)
        else:
            yield (id, f) * totalf + (1 - f) * (1 - 1/totalf)


def sample_user_dist(n, totalf):
    """Samples n passwords user's password distribution
    @n is the total number of users, which is euqal to sum(f for f in handle_iter)
    @totalf is the total number of users, which is euqal to sum(
----------
Solution: 

    multiplier = 1.0
    if totalf == 1.0:
        multiplier = 1e8
        # print "WARNING!! I don't except probabilities"

    totalf = totalf * multiplier
    print("# Population Size", totalf)
    A = np.sort(np.unique(np.random.randint(0, totalf, size=n*2))[:n])
    A = A[::-1]
    # Uniqueness check, non necessarily required, but not very
    # computationally intensive
    assert len(A) == n, "Not enough randomnumbers generated"\
        "Requried {}, generated only {}".format(n, len(A))
    j = 0
    sampled = 0
    val = A.pop()
    # print handle_iter
    for _,w,f in handle_iter:
        j += f*multiplier
        if not A: break
        while val<j:
            sampled += 1
            if sampled %5000 == 0:
                print ("Sampled:",sampled)
            yield (val, w)
            if A:
                val = A.pop()
            else:
                break

    print ("# Stopped at:", w, f, j, '\n')
    while A and val<j:
        yield (val, w)
        if A:
            i, val = A.pop()
        else:
            break
--------------------
Prompt: 
def is_probabilistic_classifier(clf):
    # type: (Any) -> bool
    """ Return True if a classifier can return probabilities """
----------
Output: 

    return hasattr(clf, 'probabilistic_classifier') and clf.probabilistic_classifier is not None


def is_probabilistic_classifier_with_name(clf_name):
    # type: (str) -> bool
    """ Return True if a classifier can return a given name """
    return is_probabilistic_classifier(clf_name) and hasattr(clf_name, 'name') and clf_name.name == clf_name.name


def is_probabilistic_classifier_with_id(clf_id):
    # type
----------
Solution: 

    if not hasattr(clf, 'predict_proba'):
        return False
    if isinstance(clf, OneVsRestClassifier):
        # It currently has a predict_proba method, but does not check if
        # wrapped estimator has a predict_proba method.
        return hasattr(clf.estimator, 'predict_proba')
    return True
--------------------
Prompt: 
def read_next_into_buf(self):
        """Read data from the file in self.bufsize chunks until we're
           certain we have a full line in the buffer.
        """
----------
Output: 

        # See if we have a newline before the next buffer
        if self.newlines and self.newlines[-1]!= '\n':
            self.newlines.append('')

        while True:
            line = self.fp.readline(self.bufsize)
            if not line:
                break
            if line[-2:] == '\r\n':
                line = line[:-2]
            self.newlines.append(line)
            if self.newlines and line[-1]!= '\n':
                self.newlines.append('')
        return self.buf

    def readlines(self):
        """Return a list of all lines
----------
Solution: 

        file_pos = self.fileobject.tell()
        if (file_pos == 0) and (self.buf == b''):
            raise StopIteration
        while file_pos and (self.get_start_of_line() == 0):
            bytes_to_read = min(self.bufsize, file_pos)
            file_pos = file_pos - bytes_to_read
            self.fileobject.seek(file_pos)
            new_stuff = self.fileobject.read(bytes_to_read)[:bytes_to_read]
            self.fileobject.seek(file_pos)
            self.buf = new_stuff + self.buf
--------------------
Prompt: 
def export(self, title, data):
        """Generate graph from the data.

        Example for the mem plugin:
        {'percent': [
            (datetime.datetime(2018, 3, 24, 16, 27, 47, 282070), 51.8),
            (datetime.datetime(2018, 3, 24, 16, 27, 47, 540999), 51.9),
            (datetime.datetime(2018, 3, 24, 16, 27, 50, 653390), 52.0),
            (datetime.datetime(2018, 3, 24, 16, 27, 53, 749702), 52.0),
            (datetime.datetime(2018, 3, 24, 16, 27, 56, 825660), 52.0),
            ...
            ]
        }

        Return:
        * True if the graph have been generated
        * False if the graph have not been generated
        """
----------
Output: 
), 53.0),
            (datetime.datetime(2018, 3, 24, 16, 27, 50, 667790), 52.0),
            (datetime.datetime(2018, 3, 24, 16, 27, 53, 749702), 53.0),
            (datetime.datetime(2018, 3, 24, 16, 27, 53, 749702), 54.0),
            (datetime.datetime(2018, 3, 24, 16, 27, 53, 749702), 55.0),
            (datetime.datetime(2018, 3, 24, 16, 27, 53, 74
----------
Solution: 

        if data == {}:
            return False

        chart = DateTimeLine(title=title.capitalize(),
                             width=self.width,
                             height=self.height,
                             style=self.style,
                             show_dots=False,
                             legend_at_bottom=True,
                             x_label_rotation=20,
                             x_value_formatter=lambda dt: dt.strftime('%Y/%m/%d %H:%M:%S'))
        for k, v in iteritems(time_serie_subsample(data, self.width)):
            chart.add(k, v)
        chart.render_to_file(os.path.join(self.path,
                                          title + '.svg'))
        return True
--------------------
Prompt: 
def format(self, result):
        """Generate HTML report.

        :return: Report body
        :rtype: str
        """
----------
Output: 

        return self.html_report(result,'report_result')

    def html_report(self, result, type):
        """Generate HTML report.

        :param result: Result object.
        :type result: Result
        :param type: Report type.
        :type type: str
        :return: HTML report body.
        :rtype: str
        """
        html_report = result.html_report()
        html_report += '\n<h3>%s</h3>' % type
        html_report += '\n<ul>'
        for key in sorted(self.html_report_attrs
----------
Solution: 

        css = "\n".join(self.css)
        content = "{}{}".format(self._header(), self._body(result))
        if self._email:
            text = 
--------------------
Prompt: 
def get_client_unread_messages_count(self, client_name=None):
        """Gets count of unread messages from client
        """
----------
Output: 

        if client_name is None:
            client_name = self.client_name

        if not client_name:
            return self.get_unread_messages(count=True)
        else:
            return self.get_unread_messages(count=True, client_name=client_name)

    def get_client_unread_count_display(self, client_name=None):
        """Gets count of unread messages from client
        """
        if client_name is None:
            client_name = self.client_name

        if not client_name:
            return self.get_unread
----------
Solution: 

        client = self._clients.get_with_name(client_name)[0]
        return client.get_messages_count_in_buffer()
--------------------
Prompt: 
async def join(self, channel, password=None):
        """ Join channel, optionally with password. """
----------
Output: 

        await self.bot.say("Joining the bot")
        await self.bot.say("```{}```".format(self.bot.escape_name(channel.mention)) +
                           "```")
        try:
            await self.bot.change_presence(presence=discord.Game(name=self.bot.escape_name(self.bot.guild.name))['presence']
        except discord.HTTPException:
            await self.bot.say("Failed to change presence")
        else:
            await self.bot.say("Joined the bot")

    @commands.command(pass
----------
Solution: 

        if self.in_channel(channel):
            raise AlreadyInChannel(channel)

        if password:
            await self.rawmsg('JOIN', channel, password)
        else:
            await self.rawmsg('JOIN', channel)
--------------------
Prompt: 
def get_parameters(rq, variables, endpoint, query_metadata, auth=None):
    """
        ?_name The variable specifies the API mandatory parameter name. The value is incorporated in the query as plain literal.
        ?__name The parameter name is optional.
        ?_name_iri The variable is substituted with the parameter value as a IRI (also: number or literal).
        ?_name_en The parameter value is considered as literal with the language 'en' (e.g., en,it,es, etc.).
        ?_name_integer The parameter value is considered as literal and the XSD datatype 'integer' is added during substitution.
        ?_name_prefix_datatype The parameter value is considered as literal and the datatype 'prefix:datatype' is added during substitution. The prefix must be specified according to the SPARQL syntax.
    """
----------
Output: 
 integer. The value is incorporated in the query as plain integer.
       ?_name_float The parameter value is considered as literal and float. The value is incorporated in the query as plain float.
       ?_name_boolean The parameter value is considered as literal and boolean. The value is incorporated in the query as plain boolean.
       ?_name_float The parameter value is considered as literal and float. The value is incorporated in the query as plain float.
       ?_name_boolean The parameter value is considered as literal and boolean. The value is incorporated in the query as plain boolean.
       ?
----------
Solution: 


    # variables = translateQuery(Query.parseString(rq, parseAll=True)).algebra['_vars']

    ## Aggregates
    internal_matcher = re.compile("__agg_\d+__")
    ## Basil-style variables
    variable_matcher = re.compile(
        "(?P<required>[_]{1,2})(?P<name>[^_]+)_?(?P<type>[a-zA-Z0-9]+)?_?(?P<userdefined>[a-zA-Z0-9]+)?.*$")

    parameters = {}
    for v in variables:
        if internal_matcher.match(v):
            continue

        match = variable_matcher.match(v)
        # TODO: currently only one parameter per triple pattern is supported
        if match:
            vname = match.group('name')
            vrequired = True if match.group('required') == '_' else False
            vtype = 'string'
            # All these can be None
            vcodes = get_enumeration(rq, vname, endpoint, query_metadata, auth)
            vdefault = get_defaults(rq, vname, query_metadata)
            vlang = None
            vdatatype = None
            vformat = None

            mtype = match.group('type')
            muserdefined = match.group('userdefined')

            if mtype in ['number', 'literal', 'string']:
                vtype = mtype
            elif mtype in ['iri']:  # TODO: proper form validation of input parameter uris
                vtype = 'string'
                vformat = 'iri'
            elif mtype:
                vtype = 'string'

                if mtype in static.XSD_DATATYPES:
                    vdatatype = 'xsd:{}'.format(mtype)
                elif len(mtype) == 2:
                    vlang = mtype
                elif muserdefined:
                    vdatatype = '{}:{}'.format(mtype, muserdefined)

            parameters[vname] = {
                'original': '?{}'.format(v),
                'required': vrequired,
                'name': vname,
                'type': vtype
            }

            # Possibly None parameter attributes
            if vcodes is not None:
                parameters[vname]['enum'] = sorted(vcodes)
            if vlang is not None:
                parameters[vname]['lang'] = vlang
            if vdatatype is not None:
                parameters[vname]['datatype'] = vdatatype
            if vformat is not None:
                parameters[vname]['format'] = vformat
            if vdefault is not None:
                parameters[vname]['default'] = vdefault

            glogger.info('Finished parsing the following parameters: {}'.format(parameters))

    return parameters
--------------------
Prompt: 
def _cast_value(self, value):
        """Internal method that makes sure every value in dictionary
        is properly cast into the correct types, instead of
        just treating everything like a string from the csv file.

        Args:
            value : The value to be casted

        Returns:
            A casted Value.
        """
----------
Output: 

        if value is None:
            return None
        if isinstance(value, str):
            return self._cast_csv_value(value)
        elif isinstance(value, list):
            return self._cast_csv_value(value)
        else:
            raise TypeError("Can't cast %s to a string" % type(value))

    def _cast_csv_value(self, value):
        """Internal method that makes sure every value in csv
        file is properly cast into the correct types, instead of
        just treating everything like a string from the csv file.

        Args:
            value : The value to
----------
Solution: 

        # Try to convert to a datetime (if requested)
        if (self.convert_datetimes):
            try:
                date_time = datetime.datetime.fromtimestamp(float(value))
                if datetime.datetime(1970, 1, 1) > date_time:
                    raise ValueError
                else:
                    return date_time

            # Next try a set of primitive types
            except ValueError:
                pass

        # Try conversion to basic types
        tests = (int, float, str)
        for test in tests:
            try:
                return test(value)
            except ValueError:
                continue
        return value
--------------------
Prompt: 
def update_build_properties(self, document, project, build_id):
        """UpdateBuildProperties.
        [Preview API] Updates properties for a build.
        :param :class:`<[JsonPatchOperation]> <azure.devops.v5_0.build.models.[JsonPatchOperation]>` document: A json-patch document describing the properties to update.
        :param str project: Project ID or project name
        :param int build_id: The ID of the build.
        :rtype: :class:`<object> <azure.devops.v5_0.build.models.object>`
        """
----------
Output: 
.JsonPatchOperation>`
        """
        if isinstance(document, str):
            document = json.loads(document)

        if not isinstance(project, str):
            raise ValueError("Parameter 'project' must be of type'str'.")

        if not document:
            raise ValueError("Parameter 'document' must not be empty.")

        if not build_id:
            raise ValueError("Parameter 'build_id' must not be empty.")

        if 'provisioning_state' in document:
            if document['provisioning_state'] == ProvisioningState.Succeeded:
                return ProvisioningState.CreateOrUpdateSucceeded
            elif document
----------
Solution: 

        route_values = {}
        if project is not None:
            route_values['project'] = self._serialize.url('project', project, 'str')
        if build_id is not None:
            route_values['buildId'] = self._serialize.url('build_id', build_id, 'int')
        content = self._serialize.body(document, '[JsonPatchOperation]')
        response = self._send(http_method='PATCH',
                              location_id='0a6312e9-0627-49b7-8083-7d74a64849c9',
                              version='5.0-preview.1',
                              route_values=route_values,
                              content=content,
                              media_type='application/json-patch+json')
        return self._deserialize('object', response)
--------------------
Prompt: 
def TransformedRandomVariable(random_variable,  # pylint: disable=invalid-name
                              reversible_layer,
                              name=None,
                              sample_shape=(),
                              value=None):
  """Random variable for f(x), where x ~ p(x) and f is reversible."""
----------
Output: 

  if name is None:
    name = "trans_masked_random_variable"
  if name not in random_variable.name:
    random_variable.name.append(name)
  if sample_shape:
    sample_shape = [sample_shape]
  if value is not None:
    value = np.array(value, dtype=np.float32)
  if random_variable.shape:
    sample_shape = random_variable.shape
  if reversible_layer:
    return ReversibleLayer(name, sample_shape, sample_shape, sample_shape)
  else:
----------
Solution: 

  return ed.RandomVariable(
      distribution=TransformedDistribution(random_variable.distribution,
                                           reversible_layer,
                                           name=name),
      sample_shape=sample_shape,
      value=value)
--------------------
Prompt: 
def nodes_in(self, name) -> iter:
        """Yield all nodes contained in given (power) node"""
----------
Output: 

        for node in self.nodes:
            if node.name == name:
                yield node

    def nodes_out(self, name) -> iter:
        """Yield all nodes contained in given (power) node"""
        for node in self.nodes:
            if node.name == name:
                yield node

    def get_nodes_in(self, name) -> iter:
        """Return all nodes contained in given (power) node"""
        for node in self.nodes:
            if node.name == name:
                yield node

    def get_nodes_out(self, name) -> iter:
        """
----------
Solution: 

        yield from (node for node in self.all_in(name) if self.is_node(node))
--------------------
Prompt: 
def write_stats_to_json(cls, file_name, stats):
    """Write stats to a local json file."""
----------
Output: 

    with open(file_name, 'w') as f:
      json.dump(stats, f)

  @classmethod
  def load_stats_from_json(cls, file_name):
    """Load stats from a local json file."""
    with open(file_name, 'r') as f:
      return json.load(f)

  @classmethod
  def get_stats(cls, stats_file):
    """Read stats from a local json file."""
    with open(stats_file, 'r') as f:
      return cls.load_stats_from_json(f)

 
----------
Solution: 

    params = cls._json_dump_options(stats)
    mode = 'w' if PY3 else 'wb'
    try:
      safe_file_dump(file_name, params, mode=mode)
    except Exception as e: # Broad catch - we don't want to fail in stats related failure.
      print('WARNING: Failed to write stats to {} due to Error: {}'.format(file_name, e),
            file=sys.stderr)
--------------------
Prompt: 
def compare_files(path1, path2):
    # type: (str, str) -> List[str]
    """Returns the delta between two files using -, ?, + format excluding
    lines that are the same

    Args:
        path1 (str): Path to first file
        path2 (str): Path to second file

    Returns:
        List[str]: Delta between the two files

    """
----------
Output: 

    with open(path1, 'r') as f:
        file1 = f.readlines()
    with open(path2, 'r') as f:
        file2 = f.readlines()
    delta = difflib.unified_diff(file1, file2,
        fromfile=lambda x: x.replace(path2, path1).splitlines(1),
        tofile=lambda x: x.replace(file2, path2).splitlines(1))
    return list(delta)


def compare_files_ignore_case(path1, path2):
    # type: (str
----------
Solution: 

    diff = difflib.ndiff(open(path1).readlines(), open(path2).readlines())
    return [x for x in diff if x[0] in ['-', '+', '?']]
--------------------
Prompt: 
def get_common_parameters(input_files, collection=None):
    """Gets a list of variable params that are common across all input files.

    If no common parameters are found, a ``ValueError`` is raised.

    Parameters
    ----------
    input_files : list of str
        List of input files to load.
    collection : str, optional
        What group of parameters to load. Can be the name of a list of
        parameters stored in the files' attrs (e.g., "variable_params"), or
        "all". If "all", will load all of the parameters in the files'
        samples group. Default is to load all.

    Returns
    -------
    list :
        List of the parameter names.
    """
----------
Output: 
.

    Returns
    -------
    dict
        Dictionary of all the parameters.
    """
    if collection is None:
        collection = "all"
    return {
        "variable_params": collection,
        "variable_params_mask": "mask",
        "variable_params_name": "variable_params",
        "variable_params_type": "list",
        "group_name": "samples",
        "group_type": "samples_group",
        "group_label": "samples",
        "group_label_mask": "mask",
        "group_label_name": "samples",
        "group
----------
Solution: 

    if collection is None:
        collection = "all"
    parameters = []
    for fn in input_files:
        fp = loadfile(fn, 'r')
        if collection == 'all':
            ps = fp[fp.samples_group].keys()
        else:
            ps = fp.attrs[collection]
        parameters.append(set(ps))
        fp.close()
    parameters = list(set.intersection(*parameters))
    if parameters == []:
        raise ValueError("no common parameters found for collection {} in "
                         "files {}".format(collection, ', '.join(input_files)))
    return parameters
--------------------
Prompt: 
def allow(ctx, foreign_account, permission, weight, threshold, account):
    """ Add a key/account to an account's permission
    """
----------
Output: 

    if not isinstance(ctx, dict):
        raise errors.AnsibleFilterError("expected a dictionary")

    if not isinstance(weight, int):
        raise errors.AnsibleFilterError("weight must be an integer")
    if not isinstance(threshold, int):
        raise errors.AnsibleFilterError("threshold must be an integer")
    if agefilter(age, ctx, foreign_account, permission, ctx['age_days']):
        return False

    # Check that age is not older than threshold
    if agefilter(time.time(), ctx, foreign_account, '1d'):
        return False

    # Check that there
----------
Solution: 

    if not foreign_account:
        from peerplaysbase.account import PasswordKey

        pwd = click.prompt(
            "Password for Key Derivation", hide_input=True, confirmation_prompt=True
        )
        foreign_account = format(
            PasswordKey(account, pwd, permission).get_public(), "PPY"
        )
    pprint(
        ctx.peerplays.allow(
            foreign_account,
            weight=weight,
            account=account,
            permission=permission,
            threshold=threshold,
        )
    )
--------------------
Prompt: 
def classify(self, tweets):
        """
        Classify tweet or tweets
        :param tweets: String or array of strings to classify.
        :return: String or array of strings depicting sentiment. Sentiment can be POSITIVE, NEGATIVE or NEUTRAL.
        """
----------
Output: 

        if isinstance(tweets, str):
            tweets = [tweets]
        if isinstance(tweets, ndarray):
            tweets = np.array(tweets)
        if isinstance(tweets, np.ndarray):
            tweets = np.array(tweets)
        if isinstance(tweets, np.ndarray):
            tweets = np.array(tweets)
        if isinstance(tweets, np.ndarray):
            tweets = np.array(tweets)
        if isinstance(tweets, np.ndarray):
            tweets = np.array(tweets)
        if isinstance
----------
Solution: 

        if type(tweets) == str:
            return self.classifier.classify(tweets)

        return list(map(lambda tweet: self.classifier.classify(tweet), tweets))
--------------------
Prompt: 
def Contradiction(expr1: Expression, expr2: Expression) -> Expression:
    """Return expression which is the contradiction of `expr1` and `expr2`."""
----------
Output: 

    return expr1.value


def Contradiction(expr: Expression) -> Expression:
    """Return expression which is the contradiction of `expr` and `expr`."""
    return expr.value


def Contradiction(expr: Expression) -> Expression:
    """Return expression which is the contradiction of `expr` and `expr`."""
    return expr.value


def Expression(expr: Expression) -> Expression:
    """Return expression which is the expression `expr`."""
    return expr.value


def test_contract_simple(contradiction: Contradiction, expression: Expression) ->
----------
Solution: 

    expr = Disjunction(Conjunction(expr1, Negation(expr2)),
                       Conjunction(Negation(expr1), expr2))
    return ast.fix_missing_locations(expr)
--------------------
Prompt: 
def _type_translation(self, str_type):
        """
        Internal method to translate the named CRITs TLO type to a URL
        specific string.
        """
----------
Output: 

        if str_type == 'url':
            return self.url_prefix
        elif str_type == 'url-prefix':
            return self.url_prefix
        else:
            raise ValueError("Invalid URL type: %s" % str_type)

    def _get_url_prefix(self, url):
        """
        Internal method to get the URL prefix for a given URL.
        """
        if url.startswith(self.url_prefix):
            return url[len(self.url_prefix):]
        else:
            return self.url_prefix

    def _get_url_type(self,
----------
Solution: 

        if str_type == 'Indicator':
            return 'indicators'
        if str_type == 'Domain':
            return 'domains'
        if str_type == 'IP':
            return 'ips'
        if str_type == 'Sample':
            return 'samples'
        if str_type == 'Event':
            return 'events'
        if str_type == 'Actor':
            return 'actors'
        if str_type == 'Email':
            return 'emails'
        if str_type == 'Backdoor':
            return 'backdoors'

        raise CRITsInvalidTypeError('Invalid object type specified: '
                                    '{}'.format(str_type))
--------------------
Prompt: 
def copy(self, repository=None, tag=None,
             source_transport=None,
             target_transport=SkopeoTransport.DOCKER,
             source_path=None, target_path=None,
             logs=True):
        """ Copy this image

        :param repository to be copied to
        :param tag
        :param source_transport Transport
        :param target_transport Transport
        :param source_path needed to specify for dir, docker-archive or oci transport
        :param target_path needed to specify for dir, docker-archive or oci transport
        :param logs enable/disable logs
        :return: the new DockerImage
        """
----------
Output: 

        """
        if repository is None:
            repository = self.repository
        if tag is None:
            tag = self.tag

        if not self.exists:
            raise ImageFactoryException("Image %s does not exist" % self.name)

        if not self.is_public:
            raise ImageFactoryException("Image %s is not public" % self.name)

        if not self.is_protected:
            raise ImageFactoryException("Image %s is protected" % self.name)

        if not self.is_public:
            raise ImageFactoryException("Image %s is not public" % self
----------
Solution: 

        if not repository:
            repository = self.name
        if not tag:
            tag = self.tag if self.tag else "latest"
        if target_transport == SkopeoTransport.OSTREE and tag and logs:
            logging.warning("tag was ignored")
        target = (DockerImage(repository, tag, pull_policy=DockerImagePullPolicy.NEVER)
                  .using_transport(target_transport, target_path))
        self.using_transport(source_transport, source_path)

        try:
            run_cmd(["skopeo", "copy",
                     transport_param(self),
                     transport_param(target)])
        except subprocess.CalledProcessError:
            raise ConuException("There was an error while copying repository", self.name)

        return target
--------------------
Prompt: 
def sample(name, reads_in_tuple):
    """	Create a new sample.
	"""
----------
Output: 

    return [reads_in_tuple[0]] + reads_in_tuple[1:]

def get_reads_in_file(name):
    """	Get the reads in a file.
	"""
    return os.path.join(os.path.dirname(os.path.abspath(__file__)), name + ".fasta"

def get_reads_in_file_2(name):
    """	Get the reads in a file.
	"""
    return os.path.join(os.path.dirname(os.path.abspath(__file__)), name + ".fasta"
----------
Solution: 

    if name in [sample_x.get_name() for sample_x in __SAMPLES__]:
        rnftools.utils.error(
            "Multiple samples have the same name. Each sample must have a unique name.",
            program="RNFtools",
            subprogram="MIShmash",
            exception=ValueError,
        )

    Sample(
        name=name,
        reads_in_tuple=reads_in_tuple,
    )
    add_input(current_sample().fq_fns())
--------------------
Prompt: 
def associate(self, from_email, new_parent_incident=None):
        """Associate an alert with an incident using a valid email address."""
----------
Output: 

        if new_parent_incident is None:
            new_parent_incident = self.incident
        if new_parent_incident is None:
            new_parent_incident = self.parent_incident
        if from_parent_incident is None:
            from_parent_incident = self.parent_incident
        if from_parent_incident.email_address == new_email:
            return
        if from_parent_incident.parent_incident_id == self.id:
            self.parent_incident = new_parent_incident
            self.incident = new_parent_incident.parent_incident_id
----------
Solution: 

        if from_email is None or not isinstance(from_email, six.string_types):
            raise MissingFromEmail(from_email)

        if new_parent_incident is None:
            raise InvalidArguments(new_parent_incident)

        parent_incident_id = self['incident']['id']
        endpoint_format = 'incidents/{0}/alerts/{1}'
        endpoint = endpoint_format.format(parent_incident_id, self['id'])

        if isinstance(new_parent_incident, Entity):
            new_parent_incident_id = new_parent_incident['id']
        else:
            new_parent_incident_id = new_parent_incident

        add_headers = {'from': from_email, }
        data = {
            'alert': {
                'id': self['id'],
                'type': 'alert',
                'incident': {
                    'type': 'incident',
                    'id': new_parent_incident_id,
                }
            }
        }

        result = self.request('PUT',
                              endpoint=endpoint,
                              add_headers=add_headers,
                              data=data,)
        return result
--------------------
Prompt: 
def get_member(self, name):
        """return a member by *name*"""
----------
Output: 

        return self.get_member_raw(name)

    def get_member_raw(self, name):
        """return a member by *name*"""
        return self.get_member_info_raw(name)

    def get_member_info_raw(self, name):
        """return a member by *name*"""
        return self.get_member_info_info_raw(name)

    def get_member_info_info_list(self, name):
        """return a list of members by *name*"""
        return self.get_member_info_info_
----------
Solution: 

        for member in self.members:
            if member.name == name:
                return member

        return None
--------------------
Prompt: 
def slugify(value):
    """
    Converts to lowercase, removes non-word characters (alphanumerics and
    underscores) and converts spaces to hyphens. Also strips leading and
    trailing whitespace.
    """
----------
Output: 

    value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore')
    value = re.sub(r'\s+','', value)
    value = re.sub(r'\s+$', '', value)
    value = re.sub(r'[-\s]+','', value)
    return re.sub(r'[-\s]+', '-', value)


def get_random_string(length=12, allowed_chars='abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789'):
    """
    Returns a securely generated random string.
    The default length of 12 with the a-z, A-Z
----------
Solution: 

    if six.PY3:
        value = str(value)
    else:
        value = unicode(value)

    value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')
    value = to_remove.sub('', value).strip().lower()
    return remove_dup.sub('-', value)
--------------------
Prompt: 
def menu_pages(colors, page=1, print_keys=True, per_page=15):
    """return menu items by page from list: colors"""
----------
Output: 

    if per_page < 1:
        per_page = 15
    if page > 1:
        page = 1
    if print_keys:
        print(colors[0])
        print(colors[1])
        print(colors[2])
        print(colors[3])
        print(colors[4])
        print(colors[5])
        print(colors[6])
        print(colors[7])
        print(colors[8])
        print(colors[9])
        print(colors[10])
        print(colors[11])
        print(colors[12])
        print(colors[13])
----------
Solution: 


    c = os.system('clear')
    length = len(colors)
    last_page = length / per_page
    if (last_page * per_page) < length:
        last_page += 1

    page_display = "page (%d/%d)" % (page, last_page)
    start = per_page * (page - 1)
    keys = 
--------------------
Prompt: 
def renameToNamespaceScopes(self):
        """
        Helper method for :func:`~exhale.graph.ExhaleRoot.reparentAll`. Some compounds in
        Breathe such as functions and variables do not have the namespace name they are
        declared in before the name of the actual compound.  This method prepends the
        appropriate (nested) namespace name before the name of any child that does not
        already have it.

        For example, the variable ``MAX_DEPTH`` declared in namespace ``external`` would
        have its ExhaleNode's ``name`` attribute changed from ``MAX_DEPTH`` to
        ``external::MAX_DEPTH``.
        """
----------
Output: 
`` to
        ``external``.

        :return: A list of the renamed namespace scopes.
        """
        return self._renameToScopes(self.node.name)

    def _renameToScopes(self, name):
        """
        Helper method for :func:`~exhale.graph.ExhaleRoot.renameToNamespaceScopes`.
        """
        if name =='maxDepth':
            return self.node.maxDepth
        elif name == 'name':
            return self.node.name
        else:
            raise ValueError("Unknown name '%s'" % name)

    def _renameToScope(self
----------
Solution: 

        for n in self.namespaces:
            namespace_name = "{0}::".format(n.name)
            for child in n.children:
                if namespace_name not in child.name:
                    child.name = "{0}{1}".format(namespace_name, child.name)
--------------------
Prompt: 
def node_link_graph(data, directed=False, attrs=_attrs):
    """Return graph from node-link data format.

    Parameters
    ----------
    data : dict
        node-link formatted graph data

    directed : bool
        If True, and direction not specified in data, return a directed graph.

    attrs : dict
        A dictionary that contains three keys 'id', 'source', 'target'.
        The corresponding values provide the attribute names for storing
        Dynetx-internal graph data. Default value:
        :samp:`dict(id='id', source='source', target='target')`.

    Returns
    -------
    G : DyNetx graph
       A DyNetx graph object

    Examples
    --------
    >>> from dynetx.readwrite import json_graph
    >>> G = dn.DynGraph([(1,2)])
    >>> data = json_graph.node_link_data(G)
    >>> H = json_graph.node_link_graph(data)

    See Also
    --------
    node_link_data
    """
----------
Output: 
 -------
    NetworkX graph

    Examples
    --------
    >>> G = nx.DiGraph()
    >>> G.add_path([0,1,2,3])
    >>> G.add_node(0)
    >>> G.add_node(1)
    >>> G.add_edge(0, 1, weight=2)
    >>> G.add_edge(1, 2, weight=3)
    >>> G.add_edge(2, 3, weight=1)
    >>> G.node[0]['weight'] = 4
    >>> G.node[1]['weight'] = 3
    >>> G
----------
Solution: 


    directed = data.get('directed', directed)
    graph = dn.DynGraph()
    if directed:
        graph = graph.to_directed()

    id_ = attrs['id']
    mapping = []
    graph.graph = data.get('graph', {})
    c = count()
    for d in data['nodes']:
        node = d.get(id_, next(c))
        mapping.append(node)
        nodedata = dict((make_str(k), v) for k, v in d.items() if k != id_)
        graph.add_node(node, **nodedata)
    for d in data['links']:
        graph.add_interaction(d['source'], d["target"], d['time'])

    return graph
--------------------
Prompt: 
def learningCurve(expPath, suite):
  """
  Print the test and overall noise errors from each iteration of this experiment
  """
----------
Output: 

  print "Learning Curve"
  print "====================="
  print "Test: ", suite, "Path: ", expPath, "Noise: ", np.sqrt(np.mean(expPath**2))

def runExperiment(suite, noise, expPath, expName, learningCurve, learningCurveName):
  """
  Run the experiment
  """
  # Create the experiment object
  exp = Experiment(suite, noise, expPath, expName, learningCurve, learningCurveName)
  # Run the experiment
  runExperimentWithNoise(suite, exp, learningCurve, learningCurveName)


----------
Solution: 

  print("\nLEARNING CURVE ================",expPath,"=====================")
  try:
    headers=["testerror","totalCorrect","elapsedTime","entropy"]
    result = suite.get_value(expPath, 0, headers, "all")
    info = []
    for i,v in enumerate(zip(result["testerror"],result["totalCorrect"],
                             result["elapsedTime"],result["entropy"])):
      info.append([i, v[0], v[1], int(v[2]), v[3]])
    headers.insert(0,"iteration")
    print(tabulate(info, headers=headers, tablefmt="grid"))
  except:
    print("Couldn't load experiment",expPath)
--------------------
Prompt: 
def _clean_check(cmd, target):
    """
    Run the command to download target. If the command fails, clean up before
    re-raising the error.
    """
----------
Output: 

    try:
        subprocess.check_call(cmd, shell=True)
    except subprocess.CalledProcessError:
        if os.path.exists(target):
            os.remove(target)
        raise


def _get_file_list(path):
    """
    Return a list of files in a given path.
    """
    with open(path, 'r') as f:
        return f.read().splitlines()


def _get_file_list_from_url(url):
    """
    Return a list of files from a given url.
    """
    with urllib.request.urlopen(url
----------
Solution: 

    try:
        subprocess.check_call(cmd)
    except subprocess.CalledProcessError:
        if os.access(target, os.F_OK):
            os.unlink(target)
        raise
--------------------
Prompt: 
def new(self, kind, **extra):
        """
        Return a host name for a new node of the given kind.

        The new name is formed by interpolating ``{}``-format
        specifiers in the string given as ``pattern`` argument to the
        class constructor.  The following names can be used in the
        ``{}``-format specifiers:

        * ``kind`` -- the `kind` argument
        * ``index`` -- a positive integer number, garanteed to be unique (per kind)
        * any other keyword argument used in the call to :meth:`new`

        Example::

          >>> p = NodeNamingPolicy(pattern='node-{kind}-{index}{spec}')
          >>> p.new('foo', spec='bar')
          'node-foo-1bar'
          >>> p.new('foo', spec='quux')
          'node-foo-2quux'
        """
----------
Output: 
 = Path('/usr/local/lib/python2.7/site-packages/nikola.py')
          >>> p.new('python', kind='py')
          'python2.7'

          >>> p.new('python', index=2)
          'python2.7'

          >>> p.new('python', pattern='/usr/local/lib/python2.7/site-packages/nikola.py')
          'python2.7-site-packages/nikola.py'

        """
        return self.interpolate(pattern, kind, **extra)

    def interpolate(self,
----------
Solution: 

        if self._free[kind]:
            index = self._free[kind].pop()
        else:
            self._top[kind] += 1
            index = self._top[kind]
        return self._format(self.pattern, kind=kind, index=index, **extra)
--------------------
Prompt: 
def get_current():
    """return current Xresources color theme"""
----------
Output: 

    return _Xresources.get_current()

def set_current(color):
    """set current Xresources color theme"""
    _Xresources.set_current(color)

def get_current_color():
    """return current Xresources color theme"""
    return _Xresources.get_current_color()

def set_current_color(color):
    """set current Xresources color theme"""
    _Xresources.set_current_color(color)

def get_current_resource():
    """return Xresources resource color theme"""
    return _Xresources.get_
----------
Solution: 

    global current
    if exists( SETTINGSFILE ):
        f = open( SETTINGSFILE ).read()
        current = re.findall('config[^\s]+.+', f)[1].split('/')[-1]
        return current
    else:
        return "** Not Set **"
--------------------
Prompt: 
def _filter_schema(schema, schema_tables, exclude_table_columns):
    """Filters a schema to only include the specified tables in the
       schema_tables parameter.  This will also filter out any colums for
       included tables that reference tables that are not included
       in the schema_tables parameter

    :param schema: Schema dict to be filtered
    :param schema_tables: List of table names to filter on.
                          EX: ['Bridge', 'Controller', 'Interface']
                          NOTE: This list is case sensitive.
    :return: Schema dict:
                filtered if the schema_table parameter contains table names,
                else the original schema dict
    """
----------
Output: 

                if the schema_tables parameter contains tables that reference more than one
                if the schema_table parameter contains tables that
                are included more than one if the schema_tables parameter contains
                tables that reference more than one

    """
    if 'tables' in schema:
        schema['tables'] = schema_tables
    if 'exclude_columns' in schema:
        schema['exclude_columns'] = schema_exclude_columns(schema['exclude_columns'], exclude_table_columns)
    return schema

def _get_schema_from_table(table):
    """Returns a schema dict for the specified table

    :
----------
Solution: 


    tables = {}
    for tbl_name, tbl_data in schema['tables'].items():
        if not schema_tables or tbl_name in schema_tables:
            columns = {}

            exclude_columns = exclude_table_columns.get(tbl_name, [])
            for col_name, col_data in tbl_data['columns'].items():
                if col_name in exclude_columns:
                    continue

                # NOTE(Alan Quillin) Needs to check and remove
                # and columns that have references to tables that
                # are not to be configured
                type_ = col_data.get('type')
                if type_:
                    if type_ and isinstance(type_, dict):
                        key = type_.get('key')
                        if key and isinstance(key, dict):
                            ref_tbl = key.get('refTable')
                            if ref_tbl and isinstance(ref_tbl,
                                                      six.string_types):
                                if ref_tbl not in schema_tables:
                                    continue
                        value = type_.get('value')
                        if value and isinstance(value, dict):
                            ref_tbl = value.get('refTable')
                            if ref_tbl and isinstance(ref_tbl,
                                                      six.string_types):
                                if ref_tbl not in schema_tables:
                                    continue

                columns[col_name] = col_data

            tbl_data['columns'] = columns
            tables[tbl_name] = tbl_data

    schema['tables'] = tables

    return schema
--------------------
Prompt: 
def get_object_query_dict(self):
        """returns objects keys according to self.object_query_code
        which can be json encoded queryset filter dict or key=value set
         in the following format: ```"key=val, key2 = val2 , key3= value with spaces"```

        Returns:
             (dict): Queryset filtering dicqt
        """
----------
Output: 

        if self.object_query_code == 'json':
            return self.get_keys_from_json()
        elif self.object_query_code == 'filter':
            return self.get_keys_from_filter()
        elif self.object_query_code == 'key':
            return self.get_keys_from_key()
        elif self.object_query_code == 'value':
            return self.get_keys_from_value()
        else:
            raise ValueError('Invalid object query code')

    def get_keys_from_json(self):
        """returns keys
----------
Solution: 

        if isinstance(self.object_query_code, dict):
            # _DATE_ _DATETIME_
            return self.object_query_code
        else:
            # comma separated, key=value pairs. wrapping spaces will be ignored
            # eg: "key=val, key2 = val2 , key3= value with spaces"
            return dict(pair.split('=') for pair in self.object_query_code.split(','))
--------------------
Prompt: 
def add(self, src):
        """ store an audio file to storage dir

        :param src: audio file path
        :return: checksum value
        """
----------
Output: 

        self.logger.debug("add %s", src)
        self.logger.debug("add %s", os.path.join(self.storage_dir, src))
        self.logger.debug("add %s", os.path.join(self.storage_dir, src.split('/')[-1]))
        self.logger.debug("add %s", os.path.join(self.storage_dir, src))
        self.logger.debug("add %s", os.path.join(self.storage_dir, src.split('/')[-1]))
        self.logger.debug("
----------
Solution: 

        if not audio.get_type(src):
            raise TypeError('The type of this file is not supported.')

        return super().add(src)
--------------------
Prompt: 
def _add_sub_parsers(self, top_level_parser, methods_to_parse, class_name):
        """Add all the sub-parsers to the top_level_parser.

        Args:
            top_level_parser: the top level parser
            methods_to_parse: dict of method name pointing to their associated
            argument parser
            class_name: name of the decorated class

        Returns:
            a dict of registered name of the parser i.e. sub command name
            pointing to the method real name
        """
----------
Output: 

        methods_to_parse[class_name] = top_level_parser
        for name, method in methods_to_parse.items():
            if isinstance(method, types.FunctionType):
                self._add_function_parsers(name, method, types.FunctionType)
            elif isinstance(method, types.MethodType):
                self._add_sub_parsers(name, method, class_name)
            elif isinstance(method, types.FunctionType):
                self._add_function_parsers(name, method, types.FunctionType)
            elif isinstance(method, types.MethodType):
                self._add_sub
----------
Solution: 

        description = "Accessible methods of {}".format(class_name)
        sub_parsers = top_level_parser.add_subparsers(description=description,
                                                      dest="method")
        # Holds the mapping between the name registered for the parser
        # and the method real name. It is useful in the 'inner_call'
        # method retrieve the real method
        parser_to_method = {}
        for method_name, parser in methods_to_parse.items():
            # We use the name provided in 'create_parser` or the name of the
            # decorated method
            parser_name = parser.get_name() or method_name
            # Make the method name compatible for the argument parsing
            if parser_name.startswith("_"):
                if not self._parse_private:
                    # We skip private methods if the caller asked not to
                    # parse them
                    continue
                # 'Private' methods are exposed without their leading or
                # trailing '_'s. Also works for 'special' methods.
                parser_name = parser_name.strip("_")
            parser_name = parser_name.replace("_", "-")
            parser_to_method[parser_name] = method_name
            sub_parsers.add_parser(parser_name, parents=[parser],
                                   add_help=False,
                                   description=parser.description)
        return parser_to_method
--------------------
Prompt: 
def _get_csv_fieldnames(csv_reader):
    """Finds fieldnames in Polarion exported csv file."""
----------
Output: 

    fieldnames = []
    for row in csv_reader:
        if row[0] == 'id':
            fieldnames.append(row[1])
        elif row[0] == 'name':
            fieldnames.append(row[1])
        elif row[0] == 'description':
            fieldnames.append(row[1])
        elif row[0] == 'type':
            fieldnames.append(row[1])
        else:
            raise ValueError('Unknown field type: %s' % row[1])
    return fieldnames


def _get_csv_data(csv_
----------
Solution: 

    fieldnames = []
    for row in csv_reader:
        for col in row:
            field = (
                col.strip()
                .replace('"', "")
                .replace(" ", "")
                .replace("(", "")
                .replace(")", "")
                .lower()
            )
            fieldnames.append(field)
        if "id" in fieldnames:
            break
        else:
            # this is not a row with fieldnames
            del fieldnames[:]
    if not fieldnames:
        return None
    # remove trailing unannotated fields
    while True:
        field = fieldnames.pop()
        if field:
            fieldnames.append(field)
            break
    # name unannotated fields
    suffix = 1
    for index, field in enumerate(fieldnames):
        if not field:
            fieldnames[index] = "field{}".format(suffix)
            suffix += 1

    return fieldnames
--------------------
Prompt: 
def _set_choices(self, value):
        """
        Also update the widget's choices.
        """
----------
Output: 

        self.widget.choices = value

    choices = property(_get_choices, _set_choices)

    def _get_choices(self):
        """
        Getter method for the 'choices' property.
        """
        return self.widget.choices

    def _set_help_text(self, value):
        """
        Also update the widget's help text.
        """
        self.widget.help_text = value

    help_text = property(_get_help_text, _set_help_text)

    def _get_help_text(self):
        """
        Getter method for the
----------
Solution: 

        super(LazyChoicesMixin, self)._set_choices(value)
        self.widget.choices = value
--------------------
Prompt: 
def context_register(self, func, *args):
        """
        :term:`Context manager <context manager>` which temporarily registers a
        filter function.

        :param func: The filter function to register.
        :param order: The sorting key for the filter function.
        :rtype: :term:`context manager`
        :return: Context manager which temporarily registers the filter
                 function.

        If :meth:`register` does not require `order` because it has been
        overridden in a subclass, the `order` argument can be omitted here,
        too.

        .. versionadded:: 0.9
        """
----------
Output: 

        if not callable(func):
            raise TypeError('filter_function must be callable')
        if not isinstance(order, basestring):
            raise TypeError('order must be a string')
        if not order:
            order = '*'
        self.register(order, func, *args)
        return self

    def context_unregister(self, ctx):
        """
        :term:`Context manager <context manager>` which unregisters a
        filter function.

        :param ctx: The filter function to unregister.
        :rtype: :term:`context manager`
        :return: Context manager which unregisters the filter
                 function.

        If
----------
Solution: 

        token = self.register(func, *args)
        try:
            yield
        finally:
            self.unregister(token)
--------------------
Prompt: 
def GetArgs():
   """
   Supports the command-line arguments listed below.
   """
----------
Output: 

   parser = argparse.ArgumentParser(description='Run the tests.')
   parser.add_argument('-c', '--config', default='config.json', help='Path to the configuration file.')
   parser.add_argument('-d', '--debug', action='store_true', help='Enable debug mode.')
   parser.add_argument('-e', '--export', action='store_true', help='Export the tests.')
   parser.add_argument('-f', '--force', action='store_true', help='Force the test run even if the test fails.')
   parser.add_argument('-s', '--suite
----------
Solution: 

   parser = argparse.ArgumentParser(
       description='Process args for retrieving all the Virtual Machines')
   parser.add_argument('-s', '--host', required=True, action='store',
                       help='Remote host to connect to')
   parser.add_argument('-o', '--port', type=int, default=443, action='store',
                       help='Port to connect on')
   parser.add_argument('-u', '--user', required=True, action='store',
                       help='User name to use when connecting to host')
   parser.add_argument('-p', '--password', required=False, action='store',
                       help='Password to use when connecting to host')
   args = parser.parse_args()
   return args
--------------------
Prompt: 
def set_secure(self, section, option, value):
        """Set an option and mark it as secure.

        Any subsequent uses of 'set' or 'get' will also
        now know that this option is secure as well.
        """
----------
Output: 

        if not self.secure:
            self.secure = value
        else:
            self.secure_flag = True

    def get_secure(self):
        """Get the current value of the'secure' option."""
        return self.secure_flag

    def set_httponly(self, section, option, value):
        """Set an option and mark it as httponly.

        Any subsequent uses of'set' or 'get' will also
        now know that this option is httponly as well.
        """
        if not self.httponly:
            self.httponly = value
        else:
            self.
----------
Solution: 

        if self.keyring_available:
            s_option = "%s%s" % (section, option)
            self._unsaved[s_option] = ('set', value)
            value = self._secure_placeholder
        ConfigParser.set(self, section, option, value)
--------------------
Prompt: 
def load_alias_hash(self):
        """
        Load (create, if not exist) the alias hash file.
        """
----------
Output: 

        if not os.path.isfile(self.alias_hash_file):
            return False

        self.alias_hash = open(self.alias_hash_file, 'r').read()
        return True

    def get_alias_hash(self):
        """
        Return the hash of the alias.
        """
        return self.alias_hash

    def get_hash(self):
        """
        Return the hash of the alias.
        """
        return self.hash

    def get_alias_list(self):
        """
        Return the list of all aliases.
        """
        return self.alias_list
----------
Solution: 

        # w+ creates the alias hash file if it does not exist
        open_mode = 'r+' if os.path.exists(GLOBAL_ALIAS_HASH_PATH) else 'w+'
        with open(GLOBAL_ALIAS_HASH_PATH, open_mode) as alias_config_hash_file:
            self.alias_config_hash = alias_config_hash_file.read()
--------------------
Prompt: 
def to_dict(self):
        """Return a dictionary representation of the Predicate."""
----------
Output: 

        return {
            'id': self.id,
            'name': self.name,
            'type': self.type,
            'description': self.description,
            'is_draft': self.is_draft,
            'is_active': self.is_active,
            'is_retired': self.is_retired,
            'is_archived': self.is_archived,
            'is_active_text': self.is_active_text,
            'is_retired_text': self.is_retired_text,
            'is
----------
Solution: 

        return {
            'predicate': self.predicate,
            'parents': list(self.supertypes),
            'synopses': [[role.to_dict() for role in synopsis]
                         for synopsis in self.synopses]
        }
--------------------
Prompt: 
def check_thickness(s):
    """
    Check and parse thickness specs as either a single [s] or a list of [s,s,s,...]
    """
----------
Output: 

    if isinstance(s, list):
        for s in s:
            check_thickness(s)
    elif isinstance(s, str):
        s = s.split(',')
        for s in s:
            check_thickness(s)
    else:
        raise ValueError("thickness must be a string or a list of strings")


def check_color(s):
    """
    Check and parse color specs as either a single [s] or a list of [s,s,...]
    """
    if isinstance(s, list):
        for s in s:
            check_color(s)
    elif isinstance(
----------
Solution: 


    s = check_1d(s, "thickness")
    if any(map(lambda d: d <= 0, s)):
        raise Exception('Thickness cannot be 0 or negative')

    return s
--------------------
Prompt: 
def detect_sv(items, all_items=None, stage="standard"):
    """Top level parallel target for examining structural variation.
    """
----------
Output: 

    if all_items is None:
        all_items = items
    if all_items is not None:
        for item in all_items:
            if item.name == item.structural_variation:
                if item.stage == stage:
                    return item
                else:
                    return None
    else:
        return None


def detect_sv_target(stage="standard"):
    """
    Detect structural variation target.
    """
    if stage == "standard":
        return "std"
    elif stage == "gpu":
        return "gpu"
    else:
        return None


def detect_nv
----------
Solution: 

    items = [utils.to_single_data(x) for x in items]
    items = cwlutils.unpack_tarballs(items, items[0])
    svcaller = items[0]["config"]["algorithm"].get("svcaller")
    caller_fn = _get_callers(items, stage, special_cases=True).get(svcaller)
    out = []
    if svcaller and caller_fn:
        if (all_items and svcaller in _NEEDS_BACKGROUND and
                not vcfutils.is_paired_analysis([x.get("align_bam") for x in items], items)):
            names = set([dd.get_sample_name(x) for x in items])
            background = [x for x in all_items if dd.get_sample_name(x) not in names]
            for svdata in caller_fn(items, background):
                out.append([svdata])
        else:
            for svdata in caller_fn(items):
                out.append([svdata])
    else:
        for data in items:
            out.append([data])
    # Avoid nesting of callers for CWL runs for easier extraction
    if cwlutils.is_cwl_run(items[0]):
        out_cwl = []
        for data in [utils.to_single_data(x) for x in out]:
            # Run validation directly from CWL runs since we're single stage
            data = validate.evaluate(data)
            data["svvalidate"] = {"summary": tz.get_in(["sv-validate", "csv"], data)}
            svs = data.get("sv")
            if svs:
                assert len(svs) == 1, svs
                data["sv"] = svs[0]
            else:
                data["sv"] = {}
            data = _add_supplemental(data)
            out_cwl.append([data])
        return out_cwl
    return out
--------------------
Prompt: 
def apply_theme(self, themename, themeoptions):
        """Apply a new theme to the document.

        This will store the existing theme configuration and apply a new one.

        """
----------
Output: 

        self.themename = themename
        self.themeoptions = themeoptions
        self.filename = themename + '.xml'
        self.xml = etree.parse(self.filename)
        self.styles.append(self.xml)
        self.xml = self.xml.getroot()
        self.styles.sort(key=lambda x: x.get('order', 'normal'))
        self.styles.insert(0, self.xml)
        self.xml.write(open(self.filename, 'w'))

    def get_theme_xml(self):
        """Return the current configuration
----------
Solution: 


        # push the existing values onto the Stack
        self._theme_stack.append(
            (self.theme, self.theme_options)
        )

        theme_factory = HTMLThemeFactory(self.app)
        theme_factory.load_additional_themes(self.get_builtin_theme_dirs() + self.config.slide_theme_path)

        self.theme = theme_factory.create(themename)
        self.theme_options = themeoptions.copy()
        self.templates.init(self, self.theme)
        self.templates.environment.filters['json'] = json.dumps

        if self.theme not in self._additional_themes:
            self._additional_themes.append(self.theme)
--------------------
Prompt: 
def prefer_replica(nodes: List[DiscoveredNode]) -> Optional[DiscoveredNode]:
    """
    Select a random replica if any are available or fall back to the master.
    """
----------
Output: 

    if not nodes:
        return None
    if len(nodes) == 1:
        return nodes[0]
    return random.choice(nodes)


def _get_master_ip_address(node: DiscoveredNode) -> Optional[str]:
    """
    Return the master IP address if any are available.
    """
    if not node.master_ip:
        return None
    if len(node.master_ip) == 1:
        return node.master_ip[0]
    return _get_master_ip_address(node.master_ip[0])


def _get
----------
Solution: 

    masters = [node for node in nodes if node.state == NodeState.Master]
    replicas = [node for node in nodes if node.state != NodeState.Master]

    if replicas:
        return random.choice(replicas)
    else:
        # if you have more than one master then you're on your own, bud.

        return masters[0]
--------------------
Prompt: 
def uncancel_confirmation(self, confirmation_id):
        """
        Uncancelles an confirmation

        :param confirmation_id: the confirmation id
        """
----------
Output: 

        self.db.execute("UPDATE users SET status='canceled' WHERE id=%s",
                       (confirmation_id,))
        self.db.commit()

    def get_user_by_id(self, id):
        """
        Get a user by id

        :param id: the id of the user
        """
        return self.db.execute("SELECT * FROM users WHERE id=%s", (id,)).fetchone()

    def get_user_by_email(self, email):
        """
        Get a user by email

        :param email: the email of the user
        """
        return self.
----------
Solution: 

        return self._create_put_request(
            resource=CONFIRMATIONS,
            billomat_id=confirmation_id,
            command=UNCANCEL,
        )
--------------------
Prompt: 
def patch_datasette():
    """
    Monkey patching for original Datasette
    """
----------
Output: 

    # Create a new dataset
    dataset = Dataset(
        name='test_dataset',
        description='test dataset',
        files=[
            File(name='test_file', path='/test/file', size=10, content='a'),
            File(name='test_file2', path='/test/file2', size=10, content='b'),
            File(name='test_file3', path='/test/file3', size=10, content='c'),
            File(name='test_file4', path='/test/file4', size=10, content='d'),
            File(
----------
Solution: 


    def inspect(self):
        " Inspect the database and return a dictionary of table metadata "
        if self._inspect:
            return self._inspect

        _inspect = {}
        files = self.files

        for filename in files:
            self.files = (filename,)
            path = Path(filename)
            name = path.stem
            if name in _inspect:
                raise Exception("Multiple files with the same stem %s" % name)
            try:
                _inspect[name] = self.original_inspect()[name]
            except sqlite3.DatabaseError:
                tables, views, dbtype = connectors.inspect(path)
                _inspect[name] = {
                    "hash": inspect_hash(path),
                    "file": str(path),
                    "dbtype": dbtype,
                    "tables": tables,
                    "views": views,
                }

        self.files = files
        self._inspect = _inspect
        return self._inspect

    datasette.app.Datasette.original_inspect = datasette.app.Datasette.inspect
    datasette.app.Datasette.inspect = inspect


    async def execute(self, db_name, sql, params=None, truncate=False, custom_time_limit=None, page_size=None):
        
--------------------
Prompt: 
def sendOACK(self):
        """This method sends an OACK packet with the options from the current
        context."""
----------
Output: 

        self.send(self.getOACK())

    def getOACK(self):
        """This method returns the OACK packet that is sent to the
        other side of the connection."""
        return self.__getOACK()

    def __getOACK(self):
        """This method returns the OACK packet that is sent to the
        other side of the connection."""
        return self.send(self.getOACK())

    def __setOACK(self, value):
        """This method sets the OACK packet that is sent to the
        other side of the connection."""
        self.send(self
----------
Solution: 

        log.debug("In sendOACK with options %s", self.context.options)
        pkt = TftpPacketOACK()
        pkt.options = self.context.options
        self.context.sock.sendto(pkt.encode().buffer,
                                 (self.context.host,
                                  self.context.tidport))
        self.context.last_pkt = pkt
--------------------
Prompt: 
def get_cmdclass():
    """
    Returns-->dict: to be assigned to setuptools.setup
                    cmdclass param
    """
----------
Output: 

    return {
        'install': InstallCommand,
        'develop': DevelopCommand,
       'sdist': SdistCommand,
       'sdist_with_egg_info': SdistWithEggInfoCommand,
       'sdist_with_data_files': SdistWithDataFilesCommand,
       'sdist_with_egg_info_scripts': SdistWithEggInfoScriptsCommand,
       'sdist_with_egg_info_data': SdistWithEggDataFilesCommand,
       'sdist_with_scripts': SdistWithScriptsCommand,
        'install_scripts': InstallScriptsCommand,
       
----------
Solution: 

    ret = {}
    cfg = __get_config()
    if cfg is None:
        return
    mapdict = {
        'build': CustomBuild,
        'build_clib': CustomBuildCLib,
        'build_ext': CustomBuildExt,
        'build_py': CustomBuildPy,
        'build_scripts': CustomBuildScripts,
        'install': CustomInstall,
        'install_data': CustomInstallData,
        'install_headers': CustomInstallHeaders,
        'install_lib': CustomInstallLib,
        'install_scripts': CustomInstallScripts,
    }
    for step in cfg.keys():
        if step in mapdict:
            ret[step] = mapdict[step]
    return ret
--------------------
Prompt: 
def generate_apiary_doc(task_router):
    """Generate apiary documentation.

    Create a Apiary generator and add application packages to it.

    :param task_router: task router, injected
    :type task_router: TaskRouter
    :return: apiary generator
    :rtype: ApiaryDoc
    """
----------
Output: 

    if not task_router.is_task_router:
        yield task_router.create_document(
            'apiary/index.html',
            task_name='index',
            title='APIary',
            content=html_generator.HTML_TEMPLATE.format(
                name=task_name,
                title_link=link_generator.generate_link(
                    'apiary',
                    'index',
                    task_name='index'
                ),
                link_path=link_generator.generate_link(
                    'apiary',
                    'index',
                    task_name='index'
                ),
               
----------
Solution: 

    generator = ApiaryDoc()

    for m in task_router.get_task_packages() + get_method_packages():
        m = importlib.import_module(m)
        generator.docmodule(m)

    return generator
--------------------
Prompt: 
def fetch_limb(self, diam_sun, base_shape=(1280, 1280)):
        """
        reads the solar limb template file and adjust to the requested solar diameter
        :param diam_sun: diameter of sun in suvi pixels
        :param base_shape: the shape of a suvi image
        :return: product name, None, scaled image
        """
----------
Output: 

        # read the template file
        template_file = self.read_suvi_template(diam_sun)

        # read the scaled image
        scaled_name = self.read_suvi_scaled_template(template_file)

        # if the scale is not a square, then it is a square
        if not base_shape:
            base_shape = (1280, 1280)

        # if the shape is not a tuple, then it is a tuple
        if not isinstance(base_shape, tuple):
            base_shape = (1280, 1280)

        # read the template
----------
Solution: 

        from scipy.ndimage.interpolation import zoom

        fn = pkg_resources.resource_filename("suvitrainer", "path_length_280_noisy.fits")
        calculated_diam = int(os.path.basename(fn).split("_")[2]) * 2
        with fits.open(fn) as hdus:
            limb_unscaled = hdus[0].data
        scale_factor = diam_sun / calculated_diam
        limb_scaled = zoom(limb_unscaled, scale_factor)
        shape = limb_scaled.shape[0]
        excess = int((shape - base_shape[0]) / 2.0)
        limb_scaled = limb_scaled[excess:shape - excess, excess:shape - excess]
        limb_scaled = limb_scaled[0:base_shape[0], 0:base_shape[1]]
        return "limb", None, limb_scaled
--------------------
Prompt: 
def csvdir_bundle(environ,
                  asset_db_writer,
                  minute_bar_writer,
                  daily_bar_writer,
                  adjustment_writer,
                  calendar,
                  start_session,
                  end_session,
                  cache,
                  show_progress,
                  output_dir,
                  tframes=None,
                  csvdir=None):
    """
    Build a zipline data bundle from the directory with csv files.
    """
----------
Output: 

    if tframes is None:
        tframes = []
    if csvdir is None:
        csvdir = environ['HOME'] + '/data/csv'
    if not os.path.exists(csvdir):
        raise Exception('csvdir does not exist: %s' % csvdir)
    if not os.path.isdir(output_dir):
        raise Exception('output_dir does not exist: %s' % output_dir)
    if not os.path.exists(minute_bar_path):
        raise Exception('minute_bar_path does not exist: %s' % minute_
----------
Solution: 

    if not csvdir:
        csvdir = environ.get('CSVDIR')
        if not csvdir:
            raise ValueError("CSVDIR environment variable is not set")

    if not os.path.isdir(csvdir):
        raise ValueError("%s is not a directory" % csvdir)

    if not tframes:
        tframes = set(["daily", "minute"]).intersection(os.listdir(csvdir))

        if not tframes:
            raise ValueError("'daily' and 'minute' directories "
                             "not found in '%s'" % csvdir)

    divs_splits = {'divs': DataFrame(columns=['sid', 'amount',
                                              'ex_date', 'record_date',
                                              'declared_date', 'pay_date']),
                   'splits': DataFrame(columns=['sid', 'ratio',
                                                'effective_date'])}
    for tframe in tframes:
        ddir = os.path.join(csvdir, tframe)

        symbols = sorted(item.split('.csv')[0]
                         for item in os.listdir(ddir)
                         if '.csv' in item)
        if not symbols:
            raise ValueError("no <symbol>.csv* files found in %s" % ddir)

        dtype = [('start_date', 'datetime64[ns]'),
                 ('end_date', 'datetime64[ns]'),
                 ('auto_close_date', 'datetime64[ns]'),
                 ('symbol', 'object')]
        metadata = DataFrame(empty(len(symbols), dtype=dtype))

        if tframe == 'minute':
            writer = minute_bar_writer
        else:
            writer = daily_bar_writer

        writer.write(_pricing_iter(ddir, symbols, metadata,
                     divs_splits, show_progress),
                     show_progress=show_progress)

        # Hardcode the exchange to "CSVDIR" for all assets and (elsewhere)
        # register "CSVDIR" to resolve to the NYSE calendar, because these
        # are all equities and thus can use the NYSE calendar.
        metadata['exchange'] = "CSVDIR"

        asset_db_writer.write(equities=metadata)

        divs_splits['divs']['sid'] = divs_splits['divs']['sid'].astype(int)
        divs_splits['splits']['sid'] = divs_splits['splits']['sid'].astype(int)
        adjustment_writer.write(splits=divs_splits['splits'],
                                dividends=divs_splits['divs'])
--------------------
Prompt: 
def _dedup_index(self, df_a):
        """Build an index for deduplicating a dataset.

        Parameters
        ----------
        df_a : (tuple of) pandas.Series
            The data of the DataFrame to build the index with.

        Returns
        -------
        pandas.MultiIndex
            A pandas.MultiIndex with record pairs. Each record pair
            contains the index values of two records. The records are
            sampled from the lower triangular part of the matrix.
        """
----------
Output: 

        if len(df_a) == 1:
            return df_a[0]
        else:
            return df_a[1]

    def _dedup_matrix(self, df_a):
        """Build an index for deduplicating a matrix.

        Parameters
        ----------
        df_a : (tuple of) pandas.Series
            The data of the DataFrame to build the index with.

        Returns
        -------
        pandas.MultiIndex
            A pandas.MultiIndex with record pairs. Each record pair
            contains the index values of two records. The records are
            sampled from the lower triangular part of
----------
Solution: 

        pairs = self._link_index(df_a, df_a)

        # Remove all pairs not in the lower triangular part of the matrix.
        # This part can be inproved by not comparing the level values, but the
        # level itself.
        pairs = pairs[pairs.labels[0] > pairs.labels[1]]

        return pairs
--------------------
Prompt: 
def smallest_prime_factor(Q):
    """Find the smallest number factorable by the small primes 2, 3, 4, and 7 
    that is larger than the argument Q"""
----------
Output: 

    if Q.size < 3:
        return 2
    elif Q.size > 4:
        return 3
    elif Q.size > 7:
        return 4
    else:
        return 7

def largest_prime_factor(Q):
    """Find the largest number factorable by the small primes 2, 3, 4, and 7 
    that is larger than the argument Q"""
    if Q.size < 3:
        return 3
    elif Q.size > 4:
        return 4
    else:
        return 7

def is_prime(Q):
    """Check if the argument Q is a prime number
----------
Solution: 


    A = Q;
    while(A != 1):
        if(np.mod(A, 2) == 0):
            A = A / 2
        elif(np.mod(A, 3) == 0):
            A = A / 3
        elif(np.mod(A, 5) == 0):
            A = A / 5
        elif(np.mod(A, 7) == 0):
            A = A / 7;
        else:
            A = Q + 1;
            Q = A;

    return Q
--------------------
Prompt: 
def name(self):
        """Define table name based on its metadata (filename used on import)

        If `filename` is not available, return `table1`.
        """
----------
Output: 

        return self.schema.name

    def _get_table_key(self, table):
        """Define the key name based on its table's schema

        :param table: The table to define the key name for
        :type table: str
        :rtype: str
        """
        return self.schema.name, table

    def _get_table_name(self, table):
        """Define the name of a key in the table

        :param table: The table to define the key name for
        :type table: str
        :rtype: str
        """
        return self.schema.name, table

    def
----------
Solution: 


        from rows.plugins.utils import slug

        name = self.meta.get("name", None)
        if name is not None:
            return slug(name)

        source = self.meta.get("source", None)
        if source and source.uri:
            return slug(os.path.splitext(Path(source.uri).name)[0])

        return "table1"
--------------------
Prompt: 
def record_participation(self, client, dt=None):
        """Record a user's participation in a test along with a given variation"""
----------
Output: 

        if dt is None:
            dt = datetime.datetime.now()
        if not dt:
            dt = datetime.datetime.now()
        if not client.is_authenticated():
            return
        participation = Participation(
            user=client.user,
            variation=dt
        )
        participation.save()
        if client.has_perm('testplans.change_participation', self.get_object().slug):
            client.delete()

    def record_test_participation(self, client, participation):
        """Record a
----------
Solution: 

        if dt is None:
            date = datetime.now()
        else:
            date = dt

        experiment_key = self.experiment.name

        pipe = self.redis.pipeline()

        pipe.sadd(_key("p:{0}:years".format(experiment_key)), date.strftime('%Y'))
        pipe.sadd(_key("p:{0}:months".format(experiment_key)), date.strftime('%Y-%m'))
        pipe.sadd(_key("p:{0}:days".format(experiment_key)), date.strftime('%Y-%m-%d'))

        pipe.execute()

        keys = [
            _key("p:{0}:_all:all".format(experiment_key)),
            _key("p:{0}:_all:{1}".format(experiment_key, date.strftime('%Y'))),
            _key("p:{0}:_all:{1}".format(experiment_key, date.strftime('%Y-%m'))),
            _key("p:{0}:_all:{1}".format(experiment_key, date.strftime('%Y-%m-%d'))),
            _key("p:{0}:{1}:all".format(experiment_key, self.name)),
            _key("p:{0}:{1}:{2}".format(experiment_key, self.name, date.strftime('%Y'))),
            _key("p:{0}:{1}:{2}".format(experiment_key, self.name, date.strftime('%Y-%m'))),
            _key("p:{0}:{1}:{2}".format(experiment_key, self.name, date.strftime('%Y-%m-%d'))),
        ]
        msetbit(keys=keys, args=([self.experiment.sequential_id(client), 1] * len(keys)))
--------------------
Prompt: 
def check_version_info(redis_client):
    """Check if various version info of this process is correct.

    This will be used to detect if workers or drivers are started using
    different versions of Python, pyarrow, or Ray. If the version
    information is not present in Redis, then no check is done.

    Args:
        redis_client: A client for the primary Redis shard.

    Raises:
        Exception: An exception is raised if there is a version mismatch.
    """
----------
Output: 

    if not redis_client:
        raise Exception('Unable to initialize Redis shard.')

    if redis_version_info is None:
        raise Exception('Unable to check version info of primary Redis shard.')

    if not redis_version_info.get('version'):
        raise Exception('Unable to check version info of primary Redis shard.')

    if not redis_version_info.get('version_info'):
        raise Exception('Unable to check version info of primary Redis shard.')

    if not redis_version_info.get('version_info'):
        raise Exception('Unable to check version info of primary Redis shard.')

    if
----------
Solution: 

    redis_reply = redis_client.get("VERSION_INFO")

    # Don't do the check if there is no version information in Redis. This
    # is to make it easier to do things like start the processes by hand.
    if redis_reply is None:
        return

    true_version_info = tuple(json.loads(ray.utils.decode(redis_reply)))
    version_info = _compute_version_info()
    if version_info != true_version_info:
        node_ip_address = ray.services.get_node_ip_address()
        error_message = ("Version mismatch: The cluster was started with:\n"
                         "    Ray: " + true_version_info[0] + "\n"
                         "    Python: " + true_version_info[1] + "\n"
                         "    Pyarrow: " + str(true_version_info[2]) + "\n"
                         "This process on node " + node_ip_address +
                         " was started with:" + "\n"
                         "    Ray: " + version_info[0] + "\n"
                         "    Python: " + version_info[1] + "\n"
                         "    Pyarrow: " + str(version_info[2]))
        if version_info[:2] != true_version_info[:2]:
            raise Exception(error_message)
        else:
            logger.warning(error_message)
--------------------
Prompt: 
def set_dict_item(dct, name_string, set_to):
    """Sets dictionary item identified by name_string to set_to.

    name_string is the indentifier generated using flatten_dict.

    Maintains the type of the orginal object in dct and tries to convert set_to
    to that type.
    """
----------
Output: 

    if not isinstance(set_to, set):
        raise TypeError("set_to must be a set")
    for name in set_to:
        if name == name_string:
            dct[name] = set_to[name_string]
            break
    else:
        raise KeyError("dictionary item not identified by name_string")


def flatten_dict(d):
    """Flattens a dictionary.

    d is a dictionary.
    """
    if isinstance(d, dict):
        for key, value in d.items():
            if isinstance(value, dict):
                flatten_dict(value)
----------
Solution: 

    key_strings = str(name_string).split('-->')
    d = dct
    for ks in key_strings[:-1]:
        d = d[ks]
    item_type = type(d[key_strings[-1]])
    d[key_strings[-1]] = item_type(set_to)
--------------------
Prompt: 
def add_upsert(self, value, criteria):
        """Add a tag or populator to the batch by value and criteria"""
----------
Output: 

        if isinstance(value, dict):
            value = add_upsert(self, value, criteria)
        else:
            raise TypeError("value must be a dict or a list")

        if self.tag_type == "tag":
            self.tags.append(value)
        elif self.tag_type == "populator":
            self.populators.append(value)

    def __len__(self):
        """Return the number of tags in the batch"""
        return len(self.tags)

    def __getitem__(self, key):
        """Get a batch by key"""
        return self.tags
----------
Solution: 


        value = value.strip()
        v = value.lower()
        self.lower_val_to_val[v] = value
        criteria_array = self.upserts.get(v)
        if criteria_array is None:
            criteria_array = []
            # start with # '{"value": "some_value", "criteria": []}, '
            self.upserts_size[v] = 31 + len(value)
        criteria_array.append(criteria.to_dict())
        self.upserts[v] = criteria_array
        self.upserts_size[v] += criteria.json_size()
--------------------
Prompt: 
def _make_title_node(self, node, increment=True):
        """Generate a new title node for ``node``.

        ``node`` is a ``nextslide`` node. The title will use the node's
        parent's title, or the title specified as an argument.

        """
----------
Output: 

        if increment:
            self.title_counter += 1
        else:
            self.title_counter = 0
        return node.title

    def _make_slide_node(self, node, increment=True):
        """Generate a new slide node for ``node``.

        ``node`` is a ``nextslide`` node. The slide will use the node's
        parent's slide, or the slide specified as an argument.

        """
        if increment:
            self.slide_counter += 1
        else:
            self.slide_counter = 0
        return node.slide

    def _make_slide_node(self
----------
Solution: 


        parent_title_node = node.parent.next_node(nodes.title)
        nextslide_info = getattr(
            parent_title_node, 'nextslide_info',
            (parent_title_node.deepcopy().children, 1),
        )
        nextslide_info = (
            nextslide_info[0],
            nextslide_info[1] + 1,
        )

        if node.args:
            textnodes, messages = node.state.inline_text(
                node.args[0],
                1,
            )
            new_title = nodes.title(node.args[0], '', *textnodes)

        else:

            title_nodes = nextslide_info[0][:]

            if 'increment' in node.attributes:
                title_nodes.append(
                    nodes.Text(' (%s)' % nextslide_info[1])
                )

            new_title = nodes.title(
                '', '',
                *title_nodes
            )

        new_title.nextslide_info = nextslide_info
        return new_title
--------------------
Prompt: 
def flush(self):
        """Flushes this instance's cache.

        The driver of this instance should call this method every
        `flush_interval`.

        Returns:
          list['ServicecontrolServicesAllocateQuotaRequest']: corresponding
          to AllocateQuotaRequests that were pending

        """
----------
Output: 

        return self._flush_impl(self._pending_requests)

    def _flush_impl(self, pending_requests):
        """Flushes this instance's cache.

        Args:
          pending_requests (list['ServicecontrolServicesAllocateQuotaRequest']): pending
          list['ServicecontrolServicesList']: list of ServicecontrolServices

        Returns:
          list['ServicecontrolServicesFlushQuotaResponse']: corresponding
          to FlushQuotaRequests that were pending

        """
        if not pending_requests:
            return []

        if len(pending_requests) == 1:
            return pending_requests[0]

        for pending_request in pending
----------
Solution: 

        if self._cache is None:
            return []
        with self._cache as c, self._out as out:
            c.expire()
            now = self._timer()
            for item in c.values():
                if (not self._in_flush_all) and (not self._should_expire(item)):
                    if (not item.is_in_flight) and item._op_aggregator is not None:
                        item.is_in_flight = True
                        item.last_refresh_timestamp = now
                        out.append(item.extract_request())  # pylint: disable=no-member
            flushed_items = list(out)
            out.clear()  # pylint: disable=no-member
            for req in flushed_items:
                assert isinstance(req, sc_messages.ServicecontrolServicesAllocateQuotaRequest)
            return flushed_items
--------------------
Prompt: 
def sort_key_process(request, sort_key='sort'):
    """
        process sort-parameter value (for example, "-name")
        return:
            current_param - field for sorting ("name)
            current_reversed - revers flag (True)
    """
----------
Output: 

    if request.method == 'POST':
        if'sort' in request.POST:
            sort_key = request.POST['sort']
            sort = True if sort_key == 'name' else False
            if sort:
                sort_key = sort_key.lower()
                sort_dir = 'desc'
            else:
                sort_key ='reversed'
                sort_dir = 'desc'
            if'sort' in request.POST:
                sort_key = request.POST['sort']
                sort_reversed = True if sort_key =='reversed' else False
                if sort_key
----------
Solution: 

    current = request.GET.get(sort_key)
    current_reversed = False
    current_param = None
    if current:
        mo = re.match(r'^(-?)(\w+)$', current)    # exclude first "-" (if exist)
        if mo:
            current_reversed = mo.group(1) == '-'
            current_param = mo.group(2)

    return current_param, current_reversed
--------------------
Prompt: 
def emitCurrentRecordChanged(self, item):
        """
        Emits the record changed signal for the given item, provided the
        signals are not currently blocked.
        
        :param      item | <QTreeWidgetItem>
        """
----------
Output: 

        self.emit(SIGNAL("recordChanged(QTreeWidgetItem *, QTreeWidgetItem *)"), item)
    
    def emitCurrentRecordChanged(self, item):
        """
        Emits the record changed signal for the given item, provided the
        signals are not currently blocked.
        
        :param      item | <QTreeWidgetItem>
        """
        self.emit(SIGNAL("recordChanged(QTreeWidgetItem *, QTreeWidgetItem *)"), item)
    
    def emitCurrentRecord(self, item):
        """
        Emits the current record signal for the given item.
        
        :param      item | <QTreeWidgetItem>
        """
----------
Solution: 

        if self.signalsBlocked():
            return
        
        # emit that the record has been clicked
        if isinstance(item, XOrbRecordItem):
            self.currentRecordChanged.emit(item.record())
        else:
            self.currentRecordChanged.emit(None)
--------------------
Prompt: 
def add(self, model):
        """Add a index method.

        This method is used to add index algorithms. If multiple algorithms
        are added, the union of the record pairs from the algorithm is taken.

        Parameters
        ----------
        model : list, class
            A (list of) index algorithm(s) from
            :mod:`recordlinkage.index`.
        """
----------
Output: 

        if len(model) == 1:
            model = recordlinkage.index(model[0])
        else:
            model = recordlinkage.index(model[0])

        if len(model) == 0:
            raise ValueError('No index method found for model {0}'.format(model))

        if len(model) == 1:
            model = recordlinkage.index(model[0])
        else:
            model = recordlinkage.index(model[0])

        if len(model) == 0:
            raise ValueError('No index method found for model {0}'.format(model))

        self.index
----------
Solution: 

        if isinstance(model, list):
            self.algorithms = self.algorithms + model
        else:
            self.algorithms.append(model)
--------------------
Prompt: 
def get_pwhash_bits(params):
    """ Extract bits for password hash validation from params. """
----------
Output: 

    if params['password'] is not None:
        return params['password'], params['password_hash']

    return None, None


def get_pwhash_context(params):
    """ Extracts the pwhash context from params. """
    if params['password'] is not None:
        return params['password_context']

    return None, None


def get_pwhash_context_for_password(params):
    """ Extracts the pwhash context from params. """
    if params['password'] is not None:
        return params['password_context'], params['password_
----------
Solution: 

    if not "pwhash" in params or \
            not "nonce" in params or \
            not "aead" in params or \
            not "kh" in params:
        raise Exception("Missing required parameter in request (pwhash, nonce, aead or kh)")
    pwhash = params["pwhash"][0]
    nonce = params["nonce"][0]
    aead = params["aead"][0]
    key_handle = pyhsm.util.key_handle_to_int(params["kh"][0])
    return pwhash, nonce, aead, key_handle
--------------------
Prompt: 
def temporal_efficiency(tnet=None, paths=None, calc='global'):
    r"""
    Returns temporal efficiency estimate. BU networks only.

    Parameters
    ----------
    Input should be *either* tnet or paths.

    data : array or dict

        Temporal network input (graphlet or contact). nettype: 'bu', 'bd'.

    paths : pandas dataframe

        Output of TenetoBIDS.networkmeasure.shortest_temporal_paths

    calc : str
        Options: 'global' (default) - measure averages over time and nodes;
        'node' or 'node_from' average over nodes (i) and time. Giving average efficiency for i to j;
        'node_to' measure average over nodes j and time;
         Giving average efficiency using paths to j from  i;

    Returns
    -------

    E : array
        Global temporal efficiency

    """
----------
Output: 
_avg' - node averages over time and nodes;
        'paths' - path averages over time and nodes;

    Returns
    -------
    float
        The average of the shortest paths.

    Notes
    -----
    The averages are calculated over the time of the network.
    """
    if tnet is None and paths is None and calc == 'node':
        return node_avg(paths=paths, data=data)
    elif tnet is None and paths is None and calc == 'node_avg':
        return node_avg(data=data)
    else:
        raise ValueError("Invalid value for 't
----------
Solution: 


    if tnet is not None and paths is not None:
        raise ValueError('Only network or path input allowed.')
    if tnet is None and paths is None:
        raise ValueError('No input.')
    # if shortest paths are not calculated, calculate them
    if tnet is not None:
        paths = shortest_temporal_path(tnet)

    pathmat = np.zeros([paths[['from', 'to']].max().max(
    )+1, paths[['from', 'to']].max().max()+1, paths[['t_start']].max().max()+1]) * np.nan
    pathmat[paths['from'].values, paths['to'].values,
            paths['t_start'].values] = paths['temporal-distance']

    # Calculate efficiency which is 1 over the mean path.
    if calc == 'global':
        eff = 1 / np.nanmean(pathmat)
    elif calc == 'node' or calc == 'node_from':
        eff = 1 / np.nanmean(np.nanmean(pathmat, axis=2), axis=1)
    elif calc == 'node_to':
        eff = 1 / np.nanmean(np.nanmean(pathmat, axis=2), axis=0)

    return eff
--------------------
Prompt: 
def generate_association_rules(patterns, confidence_threshold):
    """
    Given a set of frequent itemsets, return a dict
    of association rules in the form
    {(left): ((right), confidence)}
    """
----------
Output: 

    result = {}
    for p in patterns:
        result[p] = []
        for i in range(1, len(p)+1):
            if i == 0:
                result[p[0]].append((p[i], 0))
            else:
                result[p[0]].append((p[i], confidence_threshold*i))
                result[p[i]].append((p[i-1], confidence_threshold*i))
            if p[i] == 0:
                result[p[i]].append((p[i], 0))
            else:
                result[p[i
----------
Solution: 

    rules = {}
    for itemset in patterns.keys():
        upper_support = patterns[itemset]

        for i in range(1, len(itemset)):
            for antecedent in itertools.combinations(itemset, i):
                antecedent = tuple(sorted(antecedent))
                consequent = tuple(sorted(set(itemset) - set(antecedent)))

                if antecedent in patterns:
                    lower_support = patterns[antecedent]
                    confidence = float(upper_support) / lower_support

                    if confidence >= confidence_threshold:
                        rules[antecedent] = (consequent, confidence)

    return rules
--------------------
Prompt: 
def _read_generic_csv(file):
        """ Read and parse generic CSV file.

        Notes
        -----
        Assumes columns are 'rsid', 'chrom' / 'chromosome', 'pos' / 'position', and 'genotype';
        values are comma separated; unreported genotypes are indicated by '--'; and one header row
        precedes data. For example:

            rsid,chromosome,position,genotype
            rs1,1,1,AA
            rs2,1,2,CC
            rs3,1,3,--

        Parameters
        ----------
        file : str
            path to file

        Returns
        -------
        pandas.DataFrame
            individual's genetic data normalized for use with `lineage`
        str
            name of data source
        """
----------
Output: 

        """
        with open(file, 'r') as f:
            rsid, chromosome, pos, header = next(f)
            if header.startswith('#'):
                header = header[1:]
            header = header.split()
            pos = int(pos)
            if rsid == '':
                raise ValueError('No RSID found in file %s' % file)
            rsid = int(rsid)
            if chromosome == '':
                raise ValueError('No chromosome found in file %s' % file)
            chromosome = int(chromosome)
            if pos == 0:
                raise ValueError
----------
Solution: 

        df = pd.read_csv(
            file,
            skiprows=1,
            na_values="--",
            names=["rsid", "chrom", "pos", "genotype"],
            index_col=0,
            dtype={"chrom": object, "pos": np.int64},
        )

        return sort_snps(df), "generic"
--------------------
Prompt: 
def weird_log_graph(self, stream):
        """ Build up a graph (nodes and edges from a Bro weird.log) """
----------
Output: 

        for i, node in enumerate(self.nodes):
            for j, edge in enumerate(node.edges):
                if edge.source == 'Bro':
                    if i == 0:
                        self.graph.add_edge(node, edge)
                    else:
                        self.graph.add_edge(node, edge, weight=1)
        return self.graph

    def get_node_attributes(self, node):
        """ Build up the attributes for a node """
        for attr in node.attributes:
            if attr.name =='source':
                return attr.value
            elif attr.name ==
----------
Solution: 

        weird_log = list(stream)
        print 'Entering weird_log_graph...(%d rows)' % len(weird_log)

        # Here we're just going to capture that something weird
        # happened between two hosts
        weird_pairs = set()
        for row in weird_log:
            weird_pairs.add((row['id.orig_h'], row['id.resp_h']))

        # Okay now make the weird node for each pair
        for pair in weird_pairs:

            # Skip '-' hosts
            if (pair[0] == '-'):
                continue

            # Add the originating host
            self.add_node(pair[0], pair[0], ['host', 'origin'])

            # Add the response host
            self.add_node(pair[1], pair[1], ['host'])

            # Add a weird node
            weird_name = 'weird'+pair[0]+'_'+pair[1]
            self.add_node(weird_name, 'weird', ['weird'])

            # The relationships between the nodes
            self.add_rel(pair[0], weird_name, 'weird')
            self.add_rel(weird_name, pair[1], 'weird')
--------------------
Prompt: 
def block_sep1(self, Y):
        r"""Separate variable into component corresponding to
        :math:`\mathbf{y}_1` in :math:`\mathbf{y}\;\;`.
        """
----------
Output: 

        return self.component(Y)

    def block_sep2(self, Y):
        r"""Separate variable into component corresponding to
        :math:`\mathbf{y}_2` in :math:`\mathbf{y}\;\;`.
        """
        return self.component(Y)

    def block_sep3(self, Y):
        r"""Separate variable into component corresponding to
        :math:`\mathbf{y}_3` in :math:`\mathbf{y}\;\;`.
        """
        return self.component(Y)

    def block_sep4(self, Y):
       
----------
Solution: 


        # This method is overridden because we have to change the
        # mechanism for combining the Y0 and Y1 blocks into a single
        # array (see comment in the __init__ method).
        shp = Y.shape[0:self.cri.axisC] + self.y1shp[self.cri.axisC:]
        return Y[(slice(None),)*self.cri.axisC +
                 (slice(self.y0I, None),)].reshape(shp)
--------------------
Prompt: 
def fingerprint(blob):
    """
    Compute SSH fingerprint for specified blob.

    See https://en.wikipedia.org/wiki/Public_key_fingerprint for details.
    """
----------
Output: 

    return hashlib.sha1(blob.encode()).hexdigest()


def get_ssh_fingerprint(key_id):
    """
    Compute SSH fingerprint for specified key_id.

    See https://en.wikipedia.org/wiki/Public_key_fingerprint for details.
    """
    return get_ssh_fingerprint(key_id).replace(':', '')


def get_ssh_public_key(key_id):
    """
    Get SSH public key for specified key_id.

    See https://en.wikipedia.org/wiki/RSA_public_key for details.
    """
    return get
----------
Solution: 

    digest = hashlib.md5(blob).digest()
    return ':'.join('{:02x}'.format(c) for c in bytearray(digest))
--------------------
Prompt: 
def getskyimg(self,chip):
        """
        Notes
        =====
        Return an array representing the sky image for the detector.  The value
        of the sky is what would actually be subtracted from the exposure by
        the skysub step.

        :units: electrons

        """
----------
Output: 

        return self.sky_array[chip]

    def get_sky_array(self):
        """
        Returns the list of sky arrays for the detector.

        :return: list of sky arrays

        """
        return self.sky_array

    def get_exposure_array(self):
        """
        Notes
        =====
        Return an array representing the exposure image for the detector.
        The value of the exposure is what would actually be subtracted from the exposure
        by the skysub step.

        :return: list of exposure arrays

        """
        return self.exposure_array

    def get_exposure_
----------
Solution: 

        sci_chip = self._image[self.scienceExt,chip]
        skyimg = np.ones(sci_chip.image_shape,dtype=sci_chip.image_dtype)*sci_chip.subtractedSky
        if sci_chip._conversionFactor != 1.0: # If units are not already ELECTRONS
            skyimg *= self.getexptimeimg(chip)
        return skyimg
--------------------
Prompt: 
def update_view_data(self, properties=None):
        """Update the local object from the server-side object.
        
        >>> vm = VirtualMachine.find_one(client, filter={"name": "genesis"})
        >>> # Update all properties
        >>> vm.update_view_data()
        >>> # Update the config and summary properties
        >>> vm.update_view_data(properties=["config", "summary"]

        :param properties: A list of properties to update.
        :type properties: list

        """
----------
Output: 

        if properties is None:
            properties = self.default_view_properties
        if not properties:
            return
        vm = self.get_vm()
        for prop in properties:
            if prop == "config":
                vm.config = vm.config.to_dict()
            elif prop == "summary":
                vm.summary = vm.summary.to_dict()
            else:
                raise ValueError("Unknown property %s" % prop)

    def update_view_properties(self, properties=None):
        """Update the local object from the server-side object.
        
        >>> # Update all properties
        >>>
----------
Solution: 

        if properties is None:
            properties = []
        logger.info("Updating view data for object of type %s",
                    self._mo_ref._type)
        property_spec = self._client.create('PropertySpec')
        property_spec.type = str(self._mo_ref._type)
        # Determine which properties to retrieve from the server
        if properties is None:
            properties = []
        else:
            if properties == "all":
                logger.debug("Retrieving all properties")
                property_spec.all = True
            else:
                logger.debug("Retrieving %s properties", len(properties))
                property_spec.all = False
                property_spec.pathSet = properties

        object_spec = self._client.create('ObjectSpec')
        object_spec.obj = self._mo_ref

        pfs = self._client.create('PropertyFilterSpec')
        pfs.propSet = [property_spec]
        pfs.objectSet = [object_spec]

        # Create a copy of the property collector and call the method
        pc = self._client.sc.propertyCollector
        object_content = pc.RetrieveProperties(specSet=pfs)[0]
        if not object_content:
            # TODO: Improve error checking and reporting
            logger.error("Nothing returned from RetrieveProperties!")

        self._set_view_data(object_content)
--------------------
Prompt: 
def emap(self, func):
        """
        :param func:
        :type func: T, int -> U
        :rtype: TList[U]

        Usage:

            >>> TList([10, 20, 30, 40, 50]).emap(lambda x, i: (x+1, i))
            [(11, 0), (21, 1), (31, 2), (41, 3), (51, 4)]
        """
----------
Output: 

        if func == 0:
            return [self.head]
        elif func == 1:
            return [self.tail]
        elif func == 2:
            return [self.tail]
        elif func == 3:
            return [self.tail]
        elif func == 4:
            return [self.tail]
        elif func == 5:
            return [self.tail]
        elif func == 6:
            return [self.tail]
        elif func == 7:
            return [self.tail]
        elif func == 8:
            return [self.tail]
        elif func == 9:
            return [self.
----------
Solution: 

        return TList([func(x, i) for i, x in enumerate(self)])
--------------------
Prompt: 
def select_unrectified_slitlet(image2d, islitlet, csu_bar_slit_center,
                               params, parmodel, maskonly):
    """Returns image with the indicated slitlet (zero anywhere else).

    Parameters
    ----------
    image2d : numpy array
        Initial image from which the slitlet data will be extracted.
    islitlet : int
        Slitlet number.
    csu_bar_slit_center : float
        CSU bar slit center.
    params : :class:`~lmfit.parameter.Parameters`
        Parameters to be employed in the prediction of the distorted
        boundaries.
    parmodel : str
        Model to be assumed. Allowed values are 'longslit' and
        'multislit'.
    maskonly : bool
        If True, returns simply a mask (1 in the slitlet region and
        zero anywhere else.

    Returns
    -------
    image2d_output : numpy array
        2D image with the pixel information corresponding to the
        selected slitlet and zero everywhere else.

    """
----------
Output: 
loyed.
    parmodel : :class:`~lmfit.parameter.Parameters`
        Model parameters.
    maskonly : bool
        Mask the image with the indicated island (True) or island (False).

    Returns
    -------
    :class:`~numpy.ndarray`
        Image with the indicated island (1) or island (0)
    """
    if maskonly:
        return image2d
    else:
        return image2d[maskonly, :]


def select_island(image, island):
    """Returns image with the indicated island (1 anywhere else).

    Parameters
    ----------
----------
Solution: 


    # protection
    if image2d.shape != (EMIR_NAXIS2, EMIR_NAXIS1):
        raise ValueError("NAXIS1, NAXIS2 unexpected for EMIR detector")

    # initialize image output
    image2d_output = np.zeros_like(image2d)

    # expected slitlet frontiers
    list_expected_frontiers = expected_distorted_frontiers(
        islitlet, csu_bar_slit_center,
        params, parmodel, numpts=101, deg=5, debugplot=0
    )
    pol_lower_expected = list_expected_frontiers[0].poly_funct
    pol_upper_expected = list_expected_frontiers[1].poly_funct

    # main loop: compute for each channel the minimum and maximum scan
    for j in range(EMIR_NAXIS1):
        xchannel = j + 1
        y0_lower = pol_lower_expected(xchannel)
        y0_upper = pol_upper_expected(xchannel)
        n1, n2 = nscan_minmax_frontiers(y0_frontier_lower=y0_lower,
                                        y0_frontier_upper=y0_upper,
                                        resize=True)
        # note that n1 and n2 are scans (ranging from 1 to NAXIS2)
        if maskonly:
            image2d_output[(n1 - 1):n2, j] = np.repeat(
                [1.0], (n2 - n1 + 1)
            )
        else:
            image2d_output[(n1 - 1):n2, j] = image2d[(n1 - 1):n2, j]

    return image2d_output
--------------------
Prompt: 
def endswith(self, suffix, start=0, end=None):
        """Return True if ends with the specified suffix, False otherwise.

        With optional start, test beginning at that position. With optional end, stop comparing at that position.
        suffix can also be a tuple of strings to try.

        :param str suffix: Suffix to search.
        :param int start: Beginning position.
        :param int end: Stop comparison at this position.
        """
----------
Output: 

        if end is not None:
            if end[0] > start[0]:
                return False
            if end[1] > start[1]:
                return False
        if not suffix[0]:
            return False
        if not suffix[1]:
            return True
        if not start[0]:
            return True
        if not end[0]:
            return False
        if not start[1]:
            return True
        if not end[1]:
            return False
        if not start[0]:
            return True
        if end[0]:
            return False
        return self.find(suffix[0], start, end)
----------
Solution: 

        args = [suffix, start] + ([] if end is None else [end])
        return self.value_no_colors.endswith(*args)
--------------------
Prompt: 
def _base_placeholder(self):
        """
        Return the layout placeholder this slide placeholder inherits from.
        Not to be confused with an instance of |BasePlaceholder|
        (necessarily).
        """
----------
Output: 

        return self._placeholder

    def _get_slide_layout(self):
        """
        Return a |SlideLayout| object that represents this slide.
        """
        return self._slide_layout

    def _set_slide_layout(self, value):
        """
        Set the SlideLayout object this slide placeholder inherits from.
        Not to be confused with an instance of |SlideLayout|
        (necessarily).
        """
        self._slide_layout = value

    slide_layout = property(_get_slide_layout, _set_slide_layout)
    """
    The |SlideLayout|
----------
Solution: 

        layout, idx = self.part.slide_layout, self._element.ph_idx
        return layout.placeholders.get(idx=idx)
--------------------
Prompt: 
def read_validating_webhook_configuration(self, name, **kwargs):
        """
        read the specified ValidatingWebhookConfiguration
        This method makes a synchronous HTTP request by default. To make an
        asynchronous HTTP request, please pass async_req=True
        >>> thread = api.read_validating_webhook_configuration(name, async_req=True)
        >>> result = thread.get()

        :param async_req bool
        :param str name: name of the ValidatingWebhookConfiguration (required)
        :param str pretty: If 'true', then the output is pretty printed.
        :param bool exact: Should the export be exact.  Exact export maintains cluster-specific fields like 'Namespace'. Deprecated. Planned for removal in 1.18.
        :param bool export: Should this value be exported.  Export strips fields that a user can not specify. Deprecated. Planned for removal in 1.18.
        :return: V1beta1ValidatingWebhookConfiguration
                 If the method is called asynchronously,
                 returns the request thread.
        """
----------
Output: 
param str label_selector: A selector to restrict the list of returned objects by their labels. Defaults to everything.
        :param str field_selector: A selector to restrict the list of returned objects by their fields. Defaults to everything.
        :return: ValidatingWebhookConfiguration
                 If the method is called asynchronously,
                 returns the request thread.
        """

        all_params = ['name', 'pretty', 'label_selector', 'field_selector', 'async_req']
        all_params.append('async_req')
        all_params.append('_return_http_data_only')

        params =
----------
Solution: 

        kwargs['_return_http_data_only'] = True
        if kwargs.get('async_req'):
            return self.read_validating_webhook_configuration_with_http_info(name, **kwargs)
        else:
            (data) = self.read_validating_webhook_configuration_with_http_info(name, **kwargs)
            return data
--------------------
Prompt: 
def create_int_or_none(help_string=NO_HELP, default=NO_DEFAULT):
        # type: (str, Union[int, None, NO_DEFAULT_TYPE]) -> Union[int, None]
        """
        Create an int parameter
        :param help_string:
        :param default:
        :return:
        """
----------
Output: 

        if not help_string:
            return None
        return int(help_string)

    def create_bool_or_none(help_string=NO_HELP, default=NO_DEFAULT):
        # type: (str, Union[bool, None, NO_DEFAULT_TYPE]) -> Union[bool, None]
        """
        Create an bool parameter
        :param help_string:
        :param default:
        :return:
        """
        if not help_string:
            return None
        return bool(help_string)

    def create_float_or_none(help_string=NO_HELP
----------
Solution: 

        # noinspection PyTypeChecker
        return ParamFunctions(
            help_string=help_string,
            default=default,
            type_name="Union[int, None]",
            function_s2t=convert_string_to_int_or_none,
            function_t2s=convert_int_or_none_to_string,
        )
--------------------
Prompt: 
def angle_between(v1, v2):
    """Returns the angle in radians between vectors 'v1' and 'v2'.

    >>> angle_between((1, 0, 0), (0, 1, 0))
    1.5707963267948966
    >>> angle_between((1, 0, 0), (1, 0, 0))
    0.0
    >>> angle_between((1, 0, 0), (-1, 0, 0))
    3.141592653589793

    """
----------
Output: 

    return math.acos(v1 * v2)

def angle_between_vectors(v1, v2):
    """Returns the angle in radians between vectors 'v1' and 'v2'.

    >>> angle_between_vectors((1, 0, 0), (0, 1, 0))
    1.5707963267948966
    >>> angle_between_vectors((1, 0, 0), (1, 0, 0))
    0.0
    >>> angle_between_vectors((1, 0, 0), (0, 1, 0))
    1.57
----------
Solution: 

    v1_u = unit_vector(v1)
    v2_u = unit_vector(v2)
    # Don't use `np.dot`, does not work with all shapes
    angle = np.arccos(np.inner(v1_u, v2_u))
    return angle
--------------------
Prompt: 
def recycle(self):
        """Clean up and return connections back to the pool.

        Connections should be kept alive if supported.
        """
----------
Output: 

        if self.use_greenlets:
            self.greenlets.remove(self)
            return self.greenlets
        else:
            return self.pool.connections_count()

    def _recycle_greenlets(self):
        """Clean up greenlets.

        This method should be called when the greenlet pool is recycled.
        """
        if self.use_greenlets:
            self.greenlets.remove(self)
            return self.greenlets
        else:
            return self.pool.greenlet_connections_count()

    def _recycle_greenlets_pool(self
----------
Solution: 


        for connection in self._connections:
            self._connection_pool.no_wait_release(connection)

        self._connections.clear()
--------------------
Prompt: 
def analog_units(self):
        """
        Shortcut to retrieve all analog points units [Used by Bokeh trending feature]
        """
----------
Output: 

        return self._analog_units

    @property
    def analog_units_str(self):
        """
        Shortcut to retrieve all analog points units [Used by Bokeh trending feature]
        """
        return self._analog_units_str

    @property
    def analog_units_int(self):
        """
        Shortcut to retrieve the number of analog points units [Used by Bokeh trending feature]
        """
        return self._analog_units_int

    @property
    def analog_units_float(self):
        """
        Shortcut to retrieve the analog points
----------
Solution: 

        au = []
        us = []
        for each in self.points:
            if isinstance(each, NumericPoint):
                au.append(each.properties.name)
                us.append(each.properties.units_state)
        return dict(zip(au, us))
--------------------
Prompt: 
def GetEntries(self, parser_mediator, match=None, **unused_kwargs):
    """Extracts relevant Airport entries.

    Args:
      parser_mediator (ParserMediator): mediates interactions between parsers
          and other components, such as storage and dfvfs.
      match (Optional[dict[str: object]]): keys extracted from PLIST_KEYS.
    """
----------
Output: 

    for entry in self.entries:
      if 'airport' in entry:
        return entry['airport']
    return None

  def GetEntriesWithin(self, match=None, **unused_kwargs):
    """Extracts relevant Airport entries with interactions.

    Args:
      match (Optional[dict[str: object]]): keys extracted from PLIST_KEYS.
      **unused_kwargs: unused arguments.

    Returns:
      list[dict]: list of Airport entries with interactions between
              parsers and other components.
    """
    for entry in self.entries:
      if 'interactions'
----------
Solution: 

    if 'RememberedNetworks' not in match:
      return

    for wifi in match['RememberedNetworks']:
      ssid = wifi.get('SSIDString', 'UNKNOWN_SSID')
      security_type = wifi.get('SecurityType', 'UNKNOWN_SECURITY_TYPE')

      event_data = plist_event.PlistTimeEventData()
      event_data.desc = (
          '[WiFi] Connected to network: <{0:s}> using security {1:s}').format(
              ssid, security_type)
      event_data.key = 'item'
      event_data.root = '/RememberedNetworks'

      datetime_value = wifi.get('LastConnected', None)
      if datetime_value:
        event = time_events.PythonDatetimeEvent(
            datetime_value, definitions.TIME_DESCRIPTION_WRITTEN)

      else:
        date_time = dfdatetime_semantic_time.SemanticTime('Not set')
        event = time_events.DateTimeValuesEvent(
            date_time, definitions.TIME_DESCRIPTION_NOT_A_TIME)

      parser_mediator.ProduceEventWithEventData(event, event_data)
--------------------
Prompt: 
def gft(self, s):
        r"""Compute the graph Fourier transform.

        The graph Fourier transform of a signal :math:`s` is defined as

        .. math:: \hat{s} = U^* s,

        where :math:`U` is the Fourier basis attr:`U` and :math:`U^*` denotes
        the conjugate transpose or Hermitian transpose of :math:`U`.

        Parameters
        ----------
        s : array_like
            Graph signal in the vertex domain.

        Returns
        -------
        s_hat : ndarray
            Representation of s in the Fourier domain.

        Examples
        --------
        >>> G = graphs.Logo()
        >>> G.compute_fourier_basis()
        >>> s = np.random.normal(size=(G.N, 5, 1))
        >>> s_hat = G.gft(s)
        >>> s_star = G.igft(s_hat)
        >>> np.all((s - s_star) < 1e-10)
        True

        """
----------
Output: 
.

        Notes
        -----
        The Fourier transform of a tensor :math:`A` is defined as

       .. math:: \hat{A} = U^T A,

        where :math:`U^T` is the conjugate transpose or Hermitian transpose of
        :math:`U`.

        The tensor :math:`A` is the tensor that is to be transformed.
        The tensor :math:`A` is the tensor that is to be inverted.
        The tensor :math:`U` is the tensor that is to be used to transform the
        tensor.

        The tensor :math:`A` is the tensor that
----------
Solution: 

        s = self._check_signal(s)
        U = np.conjugate(self.U)  # True Hermitian. (Although U is often real.)
        return np.tensordot(U, s, ([0], [0]))
--------------------
Prompt: 
def get_datetext(year, month, day):
    """year=2005, month=11, day=16 => '2005-11-16 00:00:00'"""
----------
Output: 

    return datetext_from_string(datetext_from_date(year, month, day))


def get_datetext_from_string(datetext):
    """datetext=2005, day=16 => '2005-11-16 00:00:00'"""
    return datetext_from_date(datetext)


def get_datetext_from_date(datetext):
    """datetext=2005, day=16 => '2005-11-16 16:16:16'"""
    return datetext_from_date_string(datetext_from_string(
----------
Solution: 

    input_format = "%Y-%m-%d"
    try:
        datestruct = time.strptime("%i-%i-%i" % (year, month, day),
                                   input_format)
        return strftime(datetext_format, datestruct)
    except:
        return datetext_default
--------------------
Prompt: 
def load(self, fobj, index=None):
        """
        Loads given DataFile object. **tolerant with None**

        Args:
            fobj: object of one of accepted classes
            index: tab index to load fobj into. If not passed, loads into current tab
        """
----------
Output: 

        if index is None:
            index = self.tab_index
        if index is None:
            index = self.tab_index + 1
        if fobj.is_binary:
            self.read_binary(fobj)
        elif fobj.is_text:
            self.read_text(fobj)
        elif fobj.is_binary:
            self.read_binary(fobj)
        elif fobj.is_text:
            self.read_text(fobj)
        elif fobj.is_text:
            self.read_text(fobj)
        elif fobj.is_text:
            self.read_
----------
Solution: 

        if index is None:
            index = self._get_tab_index()
        page = self.pages[index]

        if fobj is None:
            return


        if not isinstance(fobj, tuple(page.clss_load)):
            raise RuntimeError('Object to load must be in {0!s} (not a {1!s})'.format(
             [x.__name__ for x in page.clss_load], fobj.__class__.__name__))

        page.editor.load(fobj)
        self._update_gui_text_tabs()
--------------------
Prompt: 
def install_host(trg_queue, *hosts, **kwargs):
    """ Atomically install host queues """
----------
Output: 

    for host in hosts:
        if host not in trg_queue:
            trg_queue[host] = {}
        if not host in trg_queue[host]:
            trg_queue[host][host] = {}
        if not host in trg_queue[host][host_name]:
            trg_queue[host][host_name][host] = {}
        if not host_name in trg_queue[host]:
            trg_queue[host][host_name][host] = {}
        if not host_name_short in trg_queue[host]:
----------
Solution: 

    user = kwargs.pop('user', None)
    group = kwargs.pop('group', None)
    mode = kwargs.pop('mode', None)
    item_user = kwargs.pop('item_user', None)
    item_group = kwargs.pop('item_group', None)
    item_mode = kwargs.pop('item_mode', None)
    is_down = kwargs.pop('is_down', False)

    #set modes
    mode, user, group, item_user, item_group, item_mode =\
        _def_mode(mode, user, group, item_user, item_group, item_mode)
    uid, gid = uid_gid(user, group)
    host_path = fsq_path.hosts(trg_queue)
    try:
        _instdir(host_path, mode, uid, gid)
    except (OSError, IOError, ), e:
        if e.errno not in ( errno.EEXIST, errno.ENOTEMPTY, ):
            raise FSQInstallError(e.errno, wrap_io_os_err(e))
    if hosts:
        for host in hosts:
            host = fsq_path.valid_name(host)
            # uid_gid makes calls to the pw db and|or gr db, in addition to
            # potentially stat'ing, as such, we want to avoid calling it
            # unless we absoultely have to
            uid, gid = uid_gid(user, group)
            tmp_full, tmp_queue = _tmp_trg(host, host_path)
            try:
                # open once to cut down on stat/open for chown/chmod combo
                fd = os.open(tmp_full, os.O_RDONLY)
                try:
                    # always fchmod here as mkdtemp is different than normal
                    # mkdir
                    os.fchmod(fd, mode)
                    if -1 != uid or -1 != gid:
                        os.fchown(fd, uid, gid)
                finally:
                    os.close(fd)

                # bless our queue with its children
                _instdir(fsq_path.tmp(trg_queue, tmp_queue), mode, uid, gid)
                _instdir(fsq_path.queue(trg_queue, tmp_queue), mode, uid, gid)
                _instdir(fsq_path.done(trg_queue, tmp_queue), mode, uid, gid)
                _instdir(fsq_path.fail(trg_queue, tmp_queue), mode, uid, gid)

                # down via configure.down if necessary
                if is_down:
                    down_host(tmp_queue, host, user=item_user,
                              group=item_group, mode=item_mode)

                # atomic commit -- by rename
                os.rename(tmp_full, fsq_path.base(trg_queue, host))
            except (OSError, IOError, ), e:
                shutil.rmtree(tmp_full)
                if e.errno == errno.ENOTEMPTY:
                    raise FSQInstallError(e.errno, u'queue exists: {0}'.format(
                                          trg_queue))
                if isinstance(e, FSQError):
                    raise e
--------------------
Prompt: 
def register(cls, encryptor: Encryptor):
        """
        Register this encryptable with an encryptor.

        Instances of this encryptor will be encrypted on initialization and decrypted on load.

        """
----------
Output: 

        cls.encryptor = encryptor

    @classmethod
    def _get_key_handle(cls, key_handle: KeyHandle) -> KeyHandle:
        """
        Get a key handle for this encryptable.

        :param key_handle: The key handle to be used.
        :return: The key handle.
        """
        return key_handle

    @classmethod
    def _get_key_handle_and_iv(cls, key_handle: KeyHandle, iv: int) -> KeyHandle:
        """
        Get a key handle and iv for this encryptable.

        :param key_handle:
----------
Solution: 

        # save the current encryptor statically
        cls.__encryptor__ = encryptor

        # NB: we cannot use the before_insert listener in conjunction with a foreign key relationship
        # for encrypted data; SQLAlchemy will warn about using 'related attribute set' operation so
        # late in its insert/flush process.
        listeners = dict(
            init=on_init,
            load=on_load,
        )

        for name, func in listeners.items():
            # If we initialize the graph multiple times (as in many unit testing scenarios),
            # we will accumulate listener functions -- with unpredictable results. As protection,
            # we need to remove existing listeners before adding new ones; this solution only
            # works if the id (e.g. memory address) of the listener does not change, which means
            # they cannot be closures around the `encryptor` reference.
            #
            # Hence the `__encryptor__` hack above...
            if contains(cls, name, func):
                remove(cls, name, func)
            listen(cls, name, func)
--------------------
Prompt: 
def vec2ang(vectors, lonlat=False):
    """Drop-in replacement for healpy `~healpy.pixelfunc.vec2ang`."""
----------
Output: 

    if lonlat:
        return np.deg2rad(np.arctan2(*vectors[::-1]))
    else:
        return np.rad2deg(np.arctan2(*vectors[::-1]))

def vec2vec(vectors, lonlat=False):
    """Drop-in replacement for healpy `~healpy.pixelfunc.vec2vec`."""
    if lonlat:
        return np.deg2rad(np.arctan2(*vectors[::-1]))
    else:
        return np.rad2deg(np.arctan2(*vectors[::-1]))

def vec2vec
----------
Solution: 

    x, y, z = vectors.transpose()
    rep_car = CartesianRepresentation(x, y, z)
    rep_sph = rep_car.represent_as(UnitSphericalRepresentation)
    return _lonlat_to_healpy(rep_sph.lon.ravel(), rep_sph.lat.ravel(), lonlat=lonlat)
--------------------
Prompt: 
def from_dict(cls, **kwargs):
        """
        Construct an object an mass-assign its attributes using +kwargs+,
        ignoring all protections of id and created_at. Intended for
        constructing objects already present in the database (i.e for use by
        methods such as find or within Query).
        """
----------
Output: 

        if cls._meta.abstract:
            return cls.abstract
        if cls._meta.ordering:
            return cls.query.order_by(*cls._meta.ordering)
        if cls._meta.ordering_aliases:
            return cls.query.order_by(*cls.ordering_aliases)
        if cls._meta.ordering_nulls:
            return cls.query.order_by(*cls.ordering_nulls)
        return cls.query.order_by(*cls.ordering)

    @classmethod
    def get_default_columns(cls):
        return [field.column_name for field in cls
----------
Solution: 

        # Create the object
        obj = cls()
        # By default, objects are initialized with attribute values of None
        # We need to clear those out so that we get AttributeError on access
        for attr in cls.__all_attributes__:
            delattr(obj, "_" + attr)
        del obj._id
        # Set the attributes that were passed
        for attr, val in kwargs.items():
            setattr(obj, "_" + attr, val)
        return obj
--------------------
Prompt: 
def months_between(date1, date2, roundOff=True):
    """
    Returns number of months between dates date1 and date2.
    If date1 is later than date2, then the result is positive.
    If date1 and date2 are on the same day of month, or both are the last day of month,
    returns an integer (time of day will be ignored).
    The result is rounded off to 8 digits unless `roundOff` is set to `False`.

    >>> df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['date1', 'date2'])
    >>> df.select(months_between(df.date1, df.date2).alias('months')).collect()
    [Row(months=3.94959677)]
    >>> df.select(months_between(df.date1, df.date2, False).alias('months')).collect()
    [Row(months=3.9495967741935485)]
    """
----------
Output: 
-30')], ['d'])
    >>> df.select(months_between(df.d, 10), df.months_between(df.date1, df.date2))
    df.select(months_between(date2, 10, False), df.months_between(date1, False, True))
    """
    sc = SparkContext._active_spark_context
    return Column(sc._jvm.functions.months_between(_to_java_column(date1), _to_java_column(date2), roundOff))


@ignore_unicode_prefix
@since(1
----------
Solution: 

    sc = SparkContext._active_spark_context
    return Column(sc._jvm.functions.months_between(
        _to_java_column(date1), _to_java_column(date2), roundOff))
--------------------
Prompt: 
def make_temp_path(path, new_ext=None):
    """
    Arguments:

      new_ext: the new file extension, including the leading dot.
        Defaults to preserving the existing file extension.

    """
----------
Output: 

    if new_ext is None:
      new_ext = tempfile.template
    if new_ext.startswith(tempfile.template_prefix):
      return new_ext[len(tempfile.template_prefix):]
    else:
      return new_ext

  def _GetTempFile(self, path):
    """Returns a temporary file name for the given path.

    Arguments:

      path: the path of the file to create, relative to the test file.

    """
    fd, temp_file = tempfile.mkstemp(dir=path)
    os.close(fd)
    if self.filesystem:
      temp_
----------
Solution: 

    root, ext = os.path.splitext(path)
    if new_ext is None:
        new_ext = ext
    temp_path = root + TEMP_EXTENSION + new_ext
    return temp_path
--------------------
Prompt: 
def _exec_template(callable_, context, args=None, kwargs=None):
    """execute a rendering callable given the callable, a
    Context, and optional explicit arguments

    the contextual Template will be located if it exists, and
    the error handling options specified on that Template will
    be interpreted here.
    """
----------
Output: 

    if args is None:
        args = []
    if kwargs is None:
        kwargs = {}
    if callable_ is not None:
        callable_ = callable_.__name__
    if context is not None:
        context = context.__name__
    if callable_ is not None:
        callable_ = callable_.__name__
    if context:
        context = context.__name__
    try:
        callable_ = getattr(builtins, callable_)
    except AttributeError:
        raise ImportError("no builtins function %r" % callable_)
    if not callable_():
        raise ImportError("no such type %r" % type(
----------
Solution: 

    template = context._with_template
    if template is not None and \
            (template.format_exceptions or template.error_handler):
        try:
            callable_(context, *args, **kwargs)
        except Exception:
            _render_error(template, context, compat.exception_as())
        except:
            e = sys.exc_info()[0]
            _render_error(template, context, e)
    else:
        callable_(context, *args, **kwargs)
--------------------
Prompt: 
def newpin(digits=4):
    """
    Return a random numeric string with the specified number of digits,
    default 4.

    >>> len(newpin())
    4
    >>> len(newpin(5))
    5
    >>> newpin().isdigit()
    True
    """
----------
Output: 

    return ''.join(random.choice(string.digits) for _ in range(randint(0, digits))


def newpinlist(digits=4):
    """
    Return a list of random strings with the specified number of
    digits, default 4.

    >>> newpinlist(5)
    [random.choice('a'), random.choice('b'), random.choice('c')]
    """
    return [newpin(digits) for _ in range(randint(0, digits))]


def newpinlist2(digits=4):
    """
    Return a list of random strings with the specified
----------
Solution: 

    randnum = randint(0, 10 ** digits)
    while len(str(randnum)) > digits:
        randnum = randint(0, 10 ** digits)
    return (u'%%0%dd' % digits) % randnum
--------------------
Prompt: 
def wateryear(self):
        """The actual hydrological year according to the selected
        reference month.

        The reference mont reference |Date.refmonth| defaults to November:

        >>> october = Date('1996.10.01')
        >>> november = Date('1996.11.01')
        >>> october.wateryear
        1996
        >>> november.wateryear
        1997

        Note that changing |Date.refmonth| affects all |Date| objects:

        >>> october.refmonth = 10
        >>> october.wateryear
        1997
        >>> november.wateryear
        1997
        >>> october.refmonth = 'November'
        >>> october.wateryear
        1996
        >>> november.wateryear
        1997
        """
----------
Output: 
ryear()
        Traceback (most recent call last):
       ...
        ValueError: Date.refmonth is not a leap year.

        """
        return self._wateryear

    @wateryear.setter
    def wateryear(self, value):
        self._wateryear = value

    @property
    def leapyear(self):
        """The actual year according to the selected reference
        month.

        The reference mont reference |Date.leapyear| defaults to November:

        >>> november = Date('1996.10.01')
        >>> november.leapyear
----------
Solution: 

        if self.month < self._firstmonth_wateryear:
            return self.year
        return self.year + 1
--------------------
Prompt: 
def create_qcos_client(self, app_uri):
        """创建资源管理客户端

        """
----------
Output: 

        self.app_uri = app_uri
        self.app_secret = app_uri.split(' ')[1]
        self.app_secret = app_secret
        self.client = QcosClient(app_uri, self.app_secret)
        self.client.set_app_id(self.app_id)
        self.client.set_app_key(self.app_key)
        self.client.set_app_secret(self.app_secret)
        return self.client

    def get_user_info(self):
        """获取用户
----------
Solution: 


        if (self.auth is None):
            return QcosClient(None)

        products = self.get_app_region_products(app_uri)
        auth = self.get_valid_app_auth(app_uri)

        if products is None or auth is None:
            return None

        return QcosClient(auth, products.get('api'))
--------------------
Prompt: 
def __init_configrs(self, rs_cfg):
        """Create and start a config replica set."""
----------
Output: 

        self.rs_cfg = rs_cfg
        self.config_replica_set = rs_cfg.get('config_replica_set', {})
        self.config_replica_set_name = rs_cfg.get('config_replica_set_name', 'default')
        self.config_replica_set_ip = rs_cfg.get('config_replica_set_ip', '')
        self.config_replica_set_port = rs_cfg.get('config_replica_set_port', '')
        self.config_replica_set_timeout = rs_cfg.get
----------
Solution: 

        # Use 'rs_id' to set the id for consistency, but need to rename
        # to 'id' to use with ReplicaSets.create()
        rs_cfg['id'] = rs_cfg.pop('rs_id', None)
        for member in rs_cfg.setdefault('members', [{}]):
            member['procParams'] = self._strip_auth(
                member.get('procParams', {}))
            member['procParams']['configsvr'] = True
            if self.enable_ipv6:
                common.enable_ipv6_single(member['procParams'])
        rs_cfg['sslParams'] = self.sslParams
        self._configsvrs.append(ReplicaSets().create(rs_cfg))
--------------------
Prompt: 
def get_stack_info(
    frames,
    with_locals=True,
    library_frame_context_lines=None,
    in_app_frame_context_lines=None,
    include_paths_re=None,
    exclude_paths_re=None,
    locals_processor_func=None,
):
    """
    Given a list of frames, returns a list of stack information
    dictionary objects that are JSON-ready.

    We have to be careful here as certain implementations of the
    _Frame class do not contain the necessary data to lookup all
    of the information we want.

    :param frames: a list of (Frame, lineno) tuples
    :param with_locals: boolean to indicate if local variables should be collected
    :param include_paths_re: a regex to determine if a frame is not a library frame
    :param exclude_paths_re: a regex to exclude frames from not being library frames
    :param locals_processor_func: a function to call on all local variables
    :return:
    """
----------
Output: 
param frames: A list of frames.
    :param with_locals: Whether to include the paths in the output.
    :param library_frame_context_lines: A list of lines of context for the library.
    :param in_app_frame_context_lines: A list of lines of context for the app.
    :param include_paths_re: Whether to include the paths in the output.
    :param exclude_paths_re: Whether to exclude the paths from the output.
    :param locals_processor_func: A function that is called to get the
        locals for a given frame.
----------
Solution: 

    results = []
    for frame, lineno in frames:
        result = get_frame_info(
            frame,
            lineno,
            library_frame_context_lines=library_frame_context_lines,
            in_app_frame_context_lines=in_app_frame_context_lines,
            with_locals=with_locals,
            include_paths_re=include_paths_re,
            exclude_paths_re=exclude_paths_re,
            locals_processor_func=locals_processor_func,
        )
        if result:
            results.append(result)
    return results
--------------------
Prompt: 
def iter_surrounding(self, center_key):
        """Iterate over all bins surrounding the given bin"""
----------
Output: 

        for i in range(self.bins):
            yield self.bins[i], self.bins[i+1]

    def get_bin_edges(self, center_key):
        """Get the edges of the given bin"""
        for i in range(self.bins):
            yield self.bins[i], self.bins[i+1]

    def get_bin_centers(self, center_key):
        """Get the centers of the given bin"""
        for i in range(self.bins):
            yield self.bins[i], self.bins[i+1]

    def
----------
Solution: 

        for shift in self.neighbor_indexes:
            key = tuple(np.add(center_key, shift).astype(int))
            if self.integer_cell is not None:
                key = self.wrap_key(key)
            bin = self._bins.get(key)
            if bin is not None:
                yield key, bin
--------------------
Prompt: 
def add_model(self, ic, N=1, index=0):
        """
        Should only be able to do this to a leaf node.

        Either N and index both integers OR index is
        list of length=N
        """
----------
Output: 

        if isinstance(ic, int):
            ic = self.get_int(ic)
        elif isinstance(ic, list):
            ic = self.get_list(ic)
        elif isinstance(ic, int):
            ic = self.get_int(ic, index)
        elif isinstance(ic, float):
            ic = self.get_float(ic, index)
        elif isinstance(ic, str):
            ic = self.get_string(ic, index)
        elif isinstance(ic, int):
            ic = self.get_int(ic, index)
----------
Solution: 

        if type(index) in [list,tuple]:
            if len(index) != N:
                raise ValueError('If a list, index must be of length N.')
        else:
            index = [index]*N

        for idx in index:
            existing = self.get_system(idx)
            tag = len(existing)
            self.add_child(ModelNode(ic, index=idx, tag=tag))
--------------------
Prompt: 
def generateRevision(self):
        """
        Generates the revision file for this builder.
        """
----------
Output: 

        return self.revision

    def getBuild(self, buildid):
        """
        Returns the Build object for this builder.
        """
        return self.builder_status.getBuild(buildid)

    def getBuildByNumber(self, number):
        """
        Returns the Build object for this builder.
        """
        return self.builder_status.getBuildByNumber(number)

    def getBuildByFlavor(self, flavor):
        """
        Returns the Build object for this builder.
        """
        return self.builder_status.getBuildByFlavor(flavor)

    def getBuildByVersion
----------
Solution: 

        revpath = self.sourcePath()
        if not os.path.exists(revpath):
            return

        # determine the revision location
        revfile = os.path.join(revpath, self.revisionFilename())
        mode = ''
        # test for svn revision
        try:
            args = ['svn', 'info', revpath]
            proc = subprocess.Popen(args, stdout=subprocess.PIPE)
            mode = 'svn'
        except WindowsError:
            try:
                args = ['git', 'rev-parse', 'HEAD', revpath]
                proc = subprocess.Popen(args, stdout=subprocess.PIPE)
                mode = 'git'
            except WindowsError:
                return

        # process SVN revision
        rev = None

        if mode == 'svn':
            for line in proc.stdout:
                data = re.match('^Revision: (\d+)', line)
                if data:
                    rev = int(data.group(1))
                    break

        if rev is not None:
            try:
                f = open(revfile, 'w')
                f.write('__revision__ = {0}\n'.format(rev))
                f.close()
            except IOError:
                pass
--------------------
Prompt: 
def _unify_sources_and_hashes(source=None, source_hash=None,
                              sources=None, source_hashes=None):
    """
    Silly little function to give us a standard tuple list for sources and
    source_hashes
    """
----------
Output: 

    if source_hash and source:
        raise ValueError("Can't combine sources and hashes with the same "
                         "source")
    if source_hash:
        return tuple(source), tuple(source_hash)
    elif source_hashes:
        return tuple(source_hashes), tuple(source_hashes)
    else:
        return None, None


def _get_source_and_hash(source, source_hash):
    """
    Get the hash of the source and the hash of the hash of the
    binary data.
    """
    if source_hash:
        return source, source_hash
    elif
----------
Solution: 

    if sources is None:
        sources = []

    if source_hashes is None:
        source_hashes = []

    if source and sources:
        return (False,
                "source and sources are mutually exclusive", [])

    if source_hash and source_hashes:
        return (False,
                "source_hash and source_hashes are mutually exclusive", [])

    if source:
        return (True, '', [(source, source_hash)])

    # Make a nice neat list of tuples exactly len(sources) long..
    return True, '', list(zip_longest(sources, source_hashes[:len(sources)]))
--------------------
Prompt: 
def _find_loopback(self, use_loopback=True, var_name='loopback'):
        """Finds a free loopback device that can be used. The loopback is stored in :attr:`loopback`. If *use_loopback*
        is True, the loopback will also be used directly.

        :returns: the loopback address
        :raises NoLoopbackAvailableError: if no loopback could be found
        """
----------
Output: 

        for device in self.loopback:
            if device.is_connected:
                if use_loopback:
                    return device.address
                else:
                    return device.get_address()
        raise NoLoopbackAvailableError('No loopback device found.')

    def _find_loopback_device(self, use_loopback=True):
        """Finds a free loopback device that can be used.

        :param use_loopback: if *use_loopback* is True, the device will be found in
        :returns: the free loopback address
        :raises NoLoopbackAvailableError: if no free loopback could be found
----------
Solution: 


        # noinspection PyBroadException
        try:
            loopback = _util.check_output_(['losetup', '-f']).strip()
            setattr(self, var_name, loopback)
        except Exception:
            logger.warning("No free loopback device found.", exc_info=True)
            raise NoLoopbackAvailableError()

        # noinspection PyBroadException
        if use_loopback:
            try:
                cmd = ['losetup', '-o', str(self.offset), '--sizelimit', str(self.size),
                       loopback, self.get_raw_path()]
                if not self.disk.read_write:
                    cmd.insert(1, '-r')
                _util.check_call_(cmd, stdout=subprocess.PIPE)
            except Exception:
                logger.exception("Loopback device could not be mounted.")
                raise NoLoopbackAvailableError()
        return loopback
--------------------
Prompt: 
def check( state_engine, token_op, block_id, checked_ops ):
    """
    Verify that a token transfer operation is permitted.
    * the token feature must exist
    * the sender must be unlocked---i.e. able to send at this point
    * the sender must have enough balance of the given token to send the amount requested
    * the token value must be positive
    * the consensus hash must be valid

    Return True if accepted
    Return False if not
    """
----------
Output: 

    return state_engine.verify_op( token_op, "check", [block_id] )


def unlock( state_engine, token_hash, block_id, checked_hash, consensus_hash, sender_id, sender_hash ):
    """
    Verify that a token transfer operation is not permitted.
    * the token feature must exist
    * the sender must be unlocked---i.e. not able to send at this point
    * the hash must be valid

    Return True if not accepted
    Return False if not
    """
    return state_engine.verify_op( token
----------
Solution: 


    epoch_features = get_epoch_features(block_id)
    if EPOCH_FEATURE_TOKEN_TRANSFER not in epoch_features:
        log.warning("Token transfers are not enabled in this epoch")
        return False

    consensus_hash = token_op['consensus_hash']
    address = token_op['address']
    recipient_address = token_op['recipient_address']
    token_type = token_op['token_units']
    token_value = token_op['token_fee']

    # token value must be positive
    if token_value <= 0:
        log.warning("Zero-value token transfer from {}".format(address))
        return False

    # can't send to ourselves 
    if address == recipient_address:
        log.warning('Cannot transfer token from the account to itself ({})'.format(address))
        return False

    # consensus hash must be valid
    if not state_engine.is_consensus_hash_valid(block_id, consensus_hash):
        log.warning('Invalid consensus hash {}'.format(consensus_hash))
        return False

    # sender account must exist
    account_info = state_engine.get_account(address, token_type)
    if account_info is None:
        log.warning("No account for {} ({})".format(address, token_type))
        return False

    # sender must not be transfer-locked
    if block_id < account_info['lock_transfer_block_id']:
        log.warning('Account {} is blocked from transferring tokens until block height {}'.format(address, account_info['lock_transfer_block_id']))
        return False

    # sender must have enough balance of the token  
    account_balance = state_engine.get_account_balance(account_info)
    if account_balance < token_value:
        log.warning('Account {} has {} {}; tried to send {}'.format(address, account_balance, token_type, token_value))
        return False
    
    receiver_account = state_engine.get_account(recipient_address, token_type)
    if receiver_account is not None:
        if not receiver_account['receive_whitelisted']:
            log.warning('Receiver account {} is not whitelisted'.format(recipient_address))
            return False

    log.debug("Account {} will pay {} {} to {}".format(address, token_value, token_type, recipient_address))

    # will execute a debit against the sender address
    token_operation_put_account_payment_info(token_op, address, token_type, token_value)

    # will execute a credit against the receiver address 
    token_operation_put_account_credit_info(token_op, recipient_address, token_type, token_value)

    # preserve token_fee as a string to prevent overflow
    token_op['token_fee'] = '{}'.format(token_op['token_fee'])
    return True
--------------------
Prompt: 
def to_cfn_resource_name(name):
    # type: (str) -> str
    """Transform a name to a valid cfn name.

    This will convert the provided name to a CamelCase name.
    It's possible that the conversion to a CFN resource name
    can result in name collisions.  It's up to the caller
    to handle name collisions appropriately.

    """
----------
Output: 

    return re.sub(r'[^a-zA-Z0-9_]', '_', name)


def to_camel_case(name):
    # type: (str) -> str
    """Transform a name to a valid camelCase name.

    This will convert the provided name to a CamelCase name.
    It's possible that the conversion to a camelCase
    name can result in name collisions.  It's up to the caller
    to handle name collisions appropriately.

    """
    return re.sub(r'[^a-zA-Z0-9_]', '_', name)


def to_
----------
Solution: 

    if not name:
        raise ValueError("Invalid name: %r" % name)
    word_separators = ['-', '_']
    for word_separator in word_separators:
        word_parts = [p for p in name.split(word_separator) if p]
        name = ''.join([w[0].upper() + w[1:] for w in word_parts])
    return re.sub(r'[^A-Za-z0-9]+', '', name)
--------------------
Prompt: 
def action(self, relationship):
        """Add a nested File Action."""
----------
Output: 

        self.add_file_action(relationship, 'add')

    def add_file_action(self, relationship, action_type):
        """Add a nested File Action."""
        self.add_file_action(relationship, 'add_file')

    def add_file_action_file(self, relationship, action_type):
        """Add a file to the action."""
        self.add_file_action(relationship, 'add_file_file')

    def add_file_action_file_system(self, relationship, action_type):
        """Add a file to the action."""
----------
Solution: 

        action_obj = FileAction(self.xid, relationship)
        self._children.append(action_obj)
--------------------
Prompt: 
def find(self, path):
        """Return the node for a path, or None."""
----------
Output: 

        return self._nodes.get(path, None)

    def find_all(self):
        """Return all nodes."""
        return self._nodes.values()

    def find_path(self, path):
        """Return the node for a path, or None."""
        return self._nodes.get(path, None)

    def find_path_to(self, path):
        """Return the node for a path, or None."""
        return self._nodes.get(path, None)

    def find_path_to_regex(self, path):
        """Return the node for a path, or
----------
Solution: 

        path = path.split('.')
        node = self
        while node._parent:
            node = node._parent
        for name in path:
            node = node._tree.get(name, None)
            if node is None or type(node) is float:
                return None
        return node
--------------------
Prompt: 
def generate_object_graphs_by_class(classlist):
    """
    Generate reference and backreference graphs
    for objects of type class for each class given in classlist.
    Useful for debugging reference leaks in framework etc.

    Usage example to generate graphs for class "someclass":
    >>> import someclass
    >>> someclassobject = someclass()
    >>> generate_object_graphs_by_class(someclass)

    Needs "objgraph" module installed.
    """
----------
Output: 

    if not classlist:
        return None
    if not hasattr(classlist, '__iter__'):
        return None
    if not hasattr(classlist, '__next__'):
        return None
    if len(classlist) == 1:
        return generate_object_graphs_by_class(classlist[0])
    return None


def generate_object_graphs_by_module(module):
    """
    Generate reference and backreference modules
    for object of type module for each module given in module.

    Needs "module" module installed.
    """
    if not module:
        return None
    if hasattr(module,
----------
Solution: 

    try:
        import objgraph
        import gc
    except ImportError:
        return
    graphcount = 0
    if not isinstance(classlist, list):
        classlist = [classlist]
    for class_item in classlist:
        for obj in gc.get_objects():
            if isinstance(obj, class_item):
                graphcount += 1
                objgraph.show_refs([obj], filename='%d_%s_%d_refs.png' % (
                    ogcounter, obj.__class__.__name__, graphcount))
                objgraph.show_backrefs([obj], filename='%d_%s_%d_backrefs.png' % (
                    ogcounter, obj.__class__.__name__, graphcount))
--------------------
Prompt: 
def ParseGshadowEntry(self, line):
    """Extract the members of each group from /etc/gshadow.

    Identifies the groups in /etc/gshadow and several attributes of the group,
    including how the password is crypted (if set).

    gshadow files have the format group_name:passwd:admins:members
    admins are both group members and can manage passwords and memberships.

    Args:
      line: An entry in gshadow.
    """
----------
Output: 

    if not line:
      return

    if self.password_set_name:
      self.password_set_name = line.strip()

    if self.group_set_name:
      self.group_set_name = line.strip()

    if self.group_set_name:
      self.group_set_name = line.strip()

    if self.group_set_name:
      self.group_set_name = line.strip()

    if self.group_set_name:
      self.group_set_name = line.strip()

    if self.group_
----------
Solution: 

    fields = ("name", "passwd", "administrators", "members")
    if line:
      rslt = dict(zip(fields, line.split(":")))
      # Add the shadow state to the internal store.
      name = rslt["name"]
      pw_entry = self.shadow.setdefault(name, rdf_client.PwEntry())
      pw_entry.store = self.shadow_store
      pw_entry.hash_type = self.GetHashType(rslt["passwd"])
      # Add the members to the internal store.
      members = self.gshadow_members.setdefault(name, set())
      for accts in rslt["administrators"], rslt["members"]:
        if accts:
          members.update(accts.split(","))
--------------------
Prompt: 
def as_es2_command(command):
    """ Modify a desktop command so it works on es2.
    """
----------
Output: 

    if command.startswith('es2:'):
        command = command.replace('es2:', 'es2_desktop')
    return command


def get_es_version():
    """ Get the version of the ES installation.
    """
    return __salt__['cmd.run']('equery -v', python_shell=True)


def get_es_version_info():
    """ Get the version of the ES installation.
    """
    return __salt__['cmd.run']('equery -v', python_shell=True, output_loglevel='debug')


def get_es_version
----------
Solution: 


    if command[0] == 'FUNC':
        return (command[0], re.sub(r'^gl([A-Z])',
                lambda m: m.group(1).lower(), command[1])) + command[2:]
    if command[0] == 'SHADERS':
        return command[:2] + convert_shaders('es2', command[2:])
    if command[0] == 'UNIFORM':
        return command[:-1] + (command[-1].tolist(),)
    return command
--------------------
Prompt: 
def p_example(self, p):
        """example : KEYWORD ID NL INDENT docsection example_fields DEDENT
                   | KEYWORD ID NL"""
----------
Output: 

        p[0] = ast.Constant(p[1], p[3])

    def p_error(self, p):
        """error : ERROR ID NL INDENT docsection_errors DEDENT"""
        p[0] = ast.Constant(p[1], p[3])

    def p_field(self, p):
        """field : ID COLON field_fields RBRACE field_fields RBRACE"""
        p[0] = ast.ID(p[3], p[5])

    def p_field_list(self, p):
        """field_list : ID COLON
----------
Solution: 

        if len(p) > 4:
            seen_fields = set()
            for example_field in p[6]:
                if example_field.name in seen_fields:
                    self.errors.append(
                        ("Example with label '%s' defines field '%s' more "
                        "than once." % (p[2], example_field.name),
                        p.lineno(1), self.path))
                seen_fields.add(example_field.name)
            p[0] = AstExample(
                self.path, p.lineno(1), p.lexpos(1), p[2], p[5],
                OrderedDict((f.name, f) for f in p[6]))
        else:
            p[0] = AstExample(
                self.path, p.lineno(1), p.lexpos(1), p[2], None, OrderedDict())
--------------------
Prompt: 
def load_from_file(cls, file_path):
        """Load the meta data given a file_path or empty meta data"""
----------
Output: 

        if not os.path.isfile(file_path):
            return None

        with open(file_path, 'r') as f:
            return cls.from_json(json.load(f))

    @classmethod
    def from_json(cls, file_obj):
        """
        Create a new instance of this class from a json file
        """
        if isinstance(file_obj, dict):
            return cls.from_json(file_obj)
        elif file_obj is None:
            return None

    @classmethod
    def from_file_json(cls, file_path):
        """
       
----------
Solution: 

        data = None
        if os.path.exists(file_path):
            metadata_file = open(file_path)
            data = json.loads(metadata_file.read())
        return cls(initial=data)
--------------------
Prompt: 
def _forward_gradients(ys, xs, d_xs):
        """
        Forward-mode pushforward analogous to the pullback defined by tf.gradients.
        With tf.gradients, grad_ys is the vector being pulled back, and here d_xs is
        the vector being pushed forward, i.e. this computes (∂ys / ∂xs)^T ∂xs.

        This is adapted from https://github.com/HIPS/autograd/pull/175#issuecomment-306984338

        :param ys: list of variables being differentiated (tensor)
        :param xs: list of variables to differentiate wrt (tensor)
        :param d_xs: list of gradients to push forward (same shapes as ys)
        :return: the specified moment of the variational distribution
        """
----------
Output: 
)
        :param xs: list of variables being pushed forward (tensor)
        :param d_xs: list of gradients (tensor)
        :return: list of variables being pushed forward (tensor)
        """
        return [y - d_ys[i] for i, y in enumerate(ys)]

    def _backward_gradients(self, ys, xs, d_ys):
        """
        Backward-mode popback analogous to the autograd defined by
        tf.gradients.

        :param ys: list of variables being differentiated (tensor)
        :param xs: list of variables being pushed backward (
----------
Solution: 

        # this should be v = [tf.placeholder(y.dtype) for y in ys], but tensorflow
        # wants a value for the placeholder, even though it never gets used
        v = [tf.placeholder_with_default(tf.zeros(y.get_shape(), dtype=y.dtype),
                                         shape=y.get_shape()) for y in ys]

        g = tf.gradients(ys, xs, grad_ys=v)
        return tf.gradients(g, v, grad_ys=d_xs)
--------------------
Prompt: 
def process(self, versions):
        """Logging version sorted ascending by tool name."""
----------
Output: 

        if self.args.verbose:
            self.log.setLevel(logging.DEBUG)
        else:
            self.log.setLevel(logging.INFO)

        # Set up logging
        self.log = logging.getLogger(__name__)
        self.log.debug("Starting logging for %s" % self.args.tool)
        self.log.debug("Using %s" % self.args.tool)
        self.log.debug("Using %s" % self.args.tool)
        self.log.debug("Using %s" % self.args.tool)

        # Parse the version
----------
Solution: 

        for tool_name in sorted(versions.keys()):
            version = versions[tool_name]
            self._log("Using tool '%s', %s" % (tool_name, version))
--------------------
Prompt: 
def cancel(self):
        """Cancel the future.

        Returns True if any of the elements of the iterables is cancelled.
        False otherwise.
        """
----------
Output: 

        return False

    def __iter__(self):
        """Return an iterator over the iterables in the stream."""
        return self

    def __next__(self):
        """Return the next item in the stream."""
        return self.next()

    def __enter__(self):
        """Enter the stream context and return a context manager that will
        enter the future.

        Returns:
            ContextManager: The stream context manager.
        """
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        """Exit the stream context and clean up.

        Args:
            exc_type: The exception
----------
Solution: 

        super(ProcessMapFuture, self).cancel()

        return any(tuple(f.cancel() for f in self._futures))
--------------------
Prompt: 
def north_arrow_path(feature, parent):
    """Retrieve the full path of default north arrow logo."""
----------
Output: 

    return os.path.join(parent.image_path, 'images', 'default_north_arrow.png')


def get_default_icon_path(icon_name):
    """Retrieve the full path of the default icon."""
    return os.path.join(
        get_icon_path(icon_name),
        'icons',
        'default_icon.png'
    )


def get_icon_path(icon_name):
    """Retrieve the full path of the icon."""
    return os.path.join(
        get_icon_path(icon_name),
----------
Solution: 

    _ = feature, parent  # NOQA

    north_arrow_file = setting(inasafe_north_arrow_path['setting_key'])
    if os.path.exists(north_arrow_file):
        return north_arrow_file
    else:
        LOGGER.info(
            'The custom north arrow is not found in {north_arrow_file}. '
            'Default north arrow will be used.').format(
            north_arrow_file=north_arrow_file)
        return inasafe_default_settings['north_arrow_path']
--------------------
Prompt: 
def filter_localization_probability(df, threshold=0.75):
    """
    Remove rows with a localization probability below 0.75

    Return a ``DataFrame`` where the rows with a value < `threshold` (default 0.75) in column 'Localization prob' are removed.
    Filters data to remove poorly localized peptides (non Class-I by default).

    :param df: Pandas ``DataFrame``
    :param threshold: Cut-off below which rows are discarded (default 0.75)
    :return: Pandas ``DataFrame``
    """
----------
Output: 

    return df.filter(lambda row: row[localization_probability] > threshold, axis='columns')


def filter_class_probability(df, threshold=0.75):
    """
    Remove rows with a class-specific probability below 0.75

    Return a ``DataFrame`` where the rows with a value < `threshold` (default 0.75) in column 'Class prob' are removed.
    Filters data to remove poorly localized classes (Class I by default).

    :param df: Pandas ``DataFrame``
    :param threshold: Cut-off below which rows are removed (default
----------
Solution: 

    df = df.copy()
    localization_probability_mask = df['Localization prob'].values >= threshold
    return df.iloc[localization_probability_mask, :]
--------------------
Prompt: 
def request_upload_token(self, file):
        """
        Request an upload token.

        :param file: A file handler pointing to the file to upload.
        :returns: True if the file uploaded successfully, False otherwise, \
            and the JSON response from the API.
        :rtype: tuple
        """
----------
Output: 

        return self._post(
            'upload_token',
            data=json.dumps({'file': file}),
            headers={'Content-Type': 'application/json'},
            files={'file': ('file', file, file)},
            params={'token': self.token}
        )

    def request_upload_token_v2(self, file):
        """
        Request an upload token.

        :param file: A file handler pointing to the file to upload.
        :returns: True if the file uploaded successfully, False otherwise \
            and the JSON response from the API.
        :rtype: tuple
        """
        return
----------
Solution: 

        self.kwargs['name'] = os.path.basename(file.name)
        self.kwargs['size'] = os.fstat(file.fileno()).st_size

        response = self._requester.request(
            'POST',
            self.url,
            _kwargs=combine_kwargs(**self.kwargs)
        )

        return self.upload(response, file)
--------------------
Prompt: 
def _single_resource_html_response(resource):
    """Return the HTML representation of *resource*.

    :param resource: :class:`sandman.model.Model` to render
    :type resource: :class:`sandman.model.Model`
    :rtype: :class:`flask.Response`

    """
----------
Output: 

    return render_template('single_resource.html',
                           resource=resource,
                           title=resource.title,
                           description=resource.description,
                           url=resource.url,
                           resource_type=resource.type,
                           resource_id=resource.id,
                           resource_title=resource.title,
                           resource_description=resource.description,
                           resource_type_id=resource.type.id,
                           resource_type_title=resource.type.title,
                           resource_type_description=resource.type.description,
                           resource_id_list=resource.id_list,
                           resource
----------
Solution: 

    tablename = resource.__tablename__
    resource.pk = getattr(resource, resource.primary_key())
    resource.attributes = resource.as_dict()
    return make_response(render_template(
        'resource.html',
        resource=resource,
        tablename=tablename))
--------------------
Prompt: 
def load_configuration():
    """Load the configuration"""
----------
Output: 

    global configuration
    configuration = ConfigParser.ConfigParser()
    configuration.read(configuration_file)
    return configuration

def get_configuration():
    """Return the configuration"""
    return load_configuration()

def get_config_file():
    """Return the configuration file"""
    return get_config_file()

def get_config_section(section):
    """Return the configuration section"""
    return get_config_section_name(section) + ".ini"

def get_config_section_name(section):
    """Return the configuration section name"""
    return get_config_section_
----------
Solution: 


    (belbio_conf_fp, belbio_secrets_fp) = get_belbio_conf_files()
    log.info(f"Using conf: {belbio_conf_fp} and secrets files: {belbio_secrets_fp} ")

    config = {}
    if belbio_conf_fp:
        with open(belbio_conf_fp, "r") as f:
            config = yaml.load(f, Loader=yaml.SafeLoader)
            config["source_files"] = {}
            config["source_files"]["conf"] = belbio_conf_fp

    if belbio_secrets_fp:
        with open(belbio_secrets_fp, "r") as f:
            secrets = yaml.load(f, Loader=yaml.SafeLoader)
            config["secrets"] = copy.deepcopy(secrets)
            if "source_files" in config:
                config["source_files"]["secrets"] = belbio_secrets_fp

    get_versions(config)

    # TODO - needs to be completed
    # add_environment_vars(config)

    return config
--------------------
Prompt: 
def get_paths(folder, ignore_endswith=ignore_endswith):
    """Return hologram file paths

    Parameters
    ----------
    folder: str or pathlib.Path
        Path to search folder
    ignore_endswith: list
        List of filename ending strings indicating which
        files should be ignored.
    """
----------
Output: 

    if isinstance(folder, str):
        return [folder]
    elif isinstance(folder, pathlib.Path):
        return [folder.resolve()]
    else:
        raise ValueError("Invalid path type: %s" % type(folder))


def get_paths_from_file(filepath):
    """Return hologram file paths from a file

    Parameters
    ----------
    filepath: str
        Path to search folder
    """
    if isinstance(filepath, str):
        return [filepath]
    elif isinstance(filepath, pathlib.Path):
        return [filepath.resolve()]
    else:
        raise ValueError("Invalid path type
----------
Solution: 

    folder = pathlib.Path(folder).resolve()
    files = folder.rglob("*")
    for ie in ignore_endswith:
        files = [ff for ff in files if not ff.name.endswith(ie)]
    return sorted(files)
--------------------
Prompt: 
def _read_json_binary_field(deserialized, field):
    """ Read the value of a JSON field that may be string or base64-encoded.
    """
----------
Output: 

    if isinstance(field, six.text_type):
        field = field.encode('utf-8')
    return json.loads(deserialized.decode('utf-8')) if field else None


def _read_json_object(deserialized, klass):
    """ Read the value of a JSON object.
    """
    if isinstance(klass, six.text_type):
        klass = klass.encode('utf-8')
    return klass(deserialized.decode('utf-8')) if deserialized else None


def _read_json_array(deserialized, klass):
    """ Read
----------
Solution: 

    val = deserialized.get(field)
    if val is not None:
        return utils.convert_to_bytes(val)
    val = deserialized.get(field + '64')
    if val is None:
        return None
    return utils.raw_urlsafe_b64decode(val)
--------------------
Prompt: 
def read_series(source, name, match=None):
    """Read a `Series` from LIGO_LW-XML

    Parameters
    ----------
    source : `file`, `str`, :class:`~ligo.lw.ligolw.Document`
        file path or open LIGO_LW-format XML file

    name : `str`
        name of the relevant `LIGO_LW` element to read

    match : `dict`, optional
        dict of (key, value) `Param` pairs to match correct LIGO_LW element,
        this is useful if a single file contains multiple `LIGO_LW` elements
        with the same name
    """
----------
Output: 
IGO_LW` elements.
        If `None`, this is a valid match.

    Returns
    -------
    series : `LIGO_LW.Series`
        series data
    """
    if match is None:
        match = {}
    return read_ligolw_xml(source, name, **match)


def read_series_xml(source, **kwargs):
    """Read a `LIGO_LW-format` XML file

    Parameters
    ----------
    source : `str`
        path to the XML file

    Returns
    -------
    ligolw.ligolw.Document
        series
----------
Solution: 

    from ligo.lw.ligolw import (LIGO_LW, Time, Array, Dim)
    from ligo.lw.param import get_param

    # read document
    xmldoc = read_ligolw(source, contenthandler=series_contenthandler())

    # parse match dict
    if match is None:
        match = dict()

    def _is_match(elem):
        try:
            if elem.Name != name:
                return False
        except AttributeError:  # Name is not set
            return False
        for key, value in match.items():
            try:
                if get_param(elem, key).pcdata != value:
                    return False
            except ValueError:  # no Param with this Name
                return False
        return True

    # parse out correct element
    matches = filter(_is_match, xmldoc.getElementsByTagName(LIGO_LW.tagName))
    try:
        elem, = matches
    except ValueError as exc:
        if not matches:
            exc.args = ("no LIGO_LW elements found matching request",)
        else:
            exc.args = ('multiple LIGO_LW elements found matching request, '
                        'please consider using `match=` to select the '
                        'correct element',)
        raise

    # get data
    array, = elem.getElementsByTagName(Array.tagName)

    # parse dimensions
    dims = array.getElementsByTagName(Dim.tagName)
    xdim = dims[0]
    x0 = xdim.Start
    dx = xdim.Scale
    xunit = xdim.Unit
    try:
        ndim = dims[1].n
    except IndexError:
        pass
    else:
        if ndim > 2:
            raise ValueError("Cannot parse LIGO_LW Array with {} "
                             "dimensions".format(ndim))

    # parse metadata
    array_kw = {
        'name': array.Name,
        'unit': array.Unit,
        'xunit': xunit,
    }
    try:
        array_kw['epoch'] = to_gps(
            elem.getElementsByTagName(Time.tagName)[0].pcdata)
    except IndexError:
        pass
    for key in ('channel',):
        try:
            array_kw[key] = get_param(elem, key)
        except ValueError:
            pass

    # build Series
    try:
        xindex, value = array.array
    except ValueError:  # not two dimensions stored
        return Series(array.array[0], x0=x0, dx=dx, **array_kw)
    return Series(value, xindex=xindex, **array_kw)
--------------------
Prompt: 
def fetch_reference_restriction(self, ):
        """Fetch whether referencing is restricted

        :returns: True, if referencing is restricted
        :rtype: :class:`bool`
        :raises: None
        """
----------
Output: 

        return self._get_element(self.REFS_REFERENCE_RESTRICTIONS,
                                 xsi_type="boolean",
                                 name="xsi:reference_restriction",
                                 value=True)

    def _get_element(self, element, xsi_type="string", name=None):
        """Create a new :class:`Element` object

        :param element: element to be created
        :type element: :class:`lxml.etree._Element`
        :param xsi_type: type of the element
        :type xsi_type: :class:`str`
        :param name
----------
Solution: 

        inter = self.get_refobjinter()
        restricted = self.status() is not None
        return restricted or inter.fetch_action_restriction(self, 'reference')
--------------------
Prompt: 
def _extend(self, newsub):
        """
        Append a subclass (extension) after the base class. For parser internal use.
        """
----------
Output: 

        self.submodules.extend(newsub.submodules)
        self.submodules.extend(newsub.submodules)
        self.submodules.extend(newsub.submodules)
        self.submodules.extend(newsub.submodules)
        self.submodules.extend(newsub.submodules)
        self.submodules.extend(newsub.submodules)
        self.submodules.extend(newsub.submodules)
        self.submodules.extend(newsub.submodules)
        self.submodules.extend(_get_extension_modules(newsub
----------
Solution: 

        current = self
        while hasattr(current, '_sub'):
            current = current._sub
        _set(current, '_sub', newsub)
        try:
            object.__delattr__(self, '_extra')
        except:
            pass
--------------------
Prompt: 
def sct_report_string(report):
    """Return a human-readable string representation of the error report
    returned by lxml's schematron validator.
    """
----------
Output: 

    return etree.tostring(report, pretty_print=True)


def sct_report_error_handler(report, error):
    """Return a function to be used for reporting
    errors.
    """
    if error is None:
        return lambda x: None
    if error == 'error':
        return lambda x: None
    if error == 'warning':
        return lambda x: None
    if error == 'info':
        return lambda x: None
    if error == 'warning':
        return lambda x: None
    if error == 'error':
        return lambda x: None
    if error == 'error
----------
Solution: 

    ret = []
    namespaces = {"svrl": "http://purl.oclc.org/dsdl/svrl"}
    for index, failed_assert_el in enumerate(
        report.findall("svrl:failed-assert", namespaces=namespaces)
    ):
        ret.append(
            "{}. {}".format(
                index + 1,
                failed_assert_el.find("svrl:text", namespaces=namespaces).text,
            )
        )
        ret.append("   test: {}".format(failed_assert_el.attrib["test"]))
        ret.append("   location: {}".format(failed_assert_el.attrib["location"]))
        ret.append("\n")
    return "\n".join(ret)
--------------------
Prompt: 
def extend(klass, name=None):
    """A function decorator for extending an existing class.

    Use as a decorator for functions to add to an existing class.

    Args:
        klass: The class to be decorated.

        name: The name the new method is to be given in the klass class.

    Returns:
        A decorator function which accepts a single function as its only
        argument.  The decorated function will be added to class klass.

    Raises:
        ValueError: If klass already has an attribute with the same name as the
            extension method.
    """
----------
Output: 

    def wrap(f):
        def wrapped_f(*args, **kwargs):
            return f(*args, **kwargs)
        return wrapped_f
    return klass(wrap)

def make_cookie(name, value, secret=None, max_age=None, path='/', domain=None, secure=False, httponly=False):
    """Create a new cookie.

    Args:
        name: The name the cookie should be created under.

        value: The value the cookie should be created under.

        secret: The value the cookie should be created under.

        max_age: The maximum age in
----------
Solution: 

    def decorator(f):
        return add_method(f, klass, name)

    return decorator
--------------------
Prompt: 
def handle_get_version_command(self):
        """ Handles <get_version> command.

        @return: Response string for <get_version> command.
        """
----------
Output: 

        if self.version:
            return self.version

        if self.module.params['state'] in ('present', 'enabled', 'disabled'):
            return self.execute_show_command()

        data = self.send_request(self.hostname, data='version')
        match = re.search(r'Version (\S+)', data)
        if match:
            ver = match.group(1)
        else:
            ver = None

        if ver:
            self.version = ver

        data = self.send_request(self.hostname, data='show version')
        match = re.search(
----------
Solution: 

        protocol = Element('protocol')
        for name, value in [('name', 'OSP'), ('version', self.get_protocol_version())]:
            elem = SubElement(protocol, name)
            elem.text = value

        daemon = Element('daemon')
        for name, value in [('name', self.get_daemon_name()), ('version', self.get_daemon_version())]:
            elem = SubElement(daemon, name)
            elem.text = value

        scanner = Element('scanner')
        for name, value in [('name', self.get_scanner_name()), ('version', self.get_scanner_version())]:
            elem = SubElement(scanner, name)
            elem.text = value

        content = [protocol, daemon, scanner]

        if self.get_vts_version():
            vts = Element('vts')
            elem = SubElement(vts, 'version')
            elem.text = self.get_vts_version()
            content.append(vts)

        return simple_response_str('get_version', 200, 'OK', content)
--------------------
Prompt: 
def _redirect_complete(self, text: str, line: str, begidx: int, endidx: int, compfunc: Callable) -> List[str]:
        """Called by complete() as the first tab completion function for all commands
        It determines if it should tab complete for redirection (|, <, >, >>) or use the
        completer function for the current command

        :param text: the string prefix we are attempting to match (all returned matches must begin with it)
        :param line: the current input line with leading whitespace removed
        :param begidx: the beginning index of the prefix text
        :param endidx: the ending index of the prefix text
        :param compfunc: the completer function for the current command
                         this will be called if we aren't completing for redirection
        :return: a list of possible tab completions
        """
----------
Output: 

        :param endidx: the end index of the prefix text
        :param compfunc: the completer function to use (if any)
        :return: the list of possible matches
        """
        if len(line) > 1:
            return compfunc(text, line[1:begidx], begidx, endidx)
        else:
            return compfunc(text, line, begidx, endidx)

    def _complete_path(self, path: str) -> str:
        """Called by path() as the first path completion function for all commands
        It determines if it should return
----------
Solution: 

        if self.allow_redirection:

            # Get all tokens through the one being completed. We want the raw tokens
            # so we can tell if redirection strings are quoted and ignore them.
            _, raw_tokens = self.tokens_for_completion(line, begidx, endidx)
            if not raw_tokens:
                return []

            if len(raw_tokens) > 1:

                # Check if there are redirection strings prior to the token being completed
                seen_pipe = False
                has_redirection = False

                for cur_token in raw_tokens[:-1]:
                    if cur_token in constants.REDIRECTION_TOKENS:
                        has_redirection = True

                        if cur_token == constants.REDIRECTION_PIPE:
                            seen_pipe = True

                # Get token prior to the one being completed
                prior_token = raw_tokens[-2]

                # If a pipe is right before the token being completed, complete a shell command as the piped process
                if prior_token == constants.REDIRECTION_PIPE:
                    return self.shell_cmd_complete(text, line, begidx, endidx)

                # Otherwise do path completion either as files to redirectors or arguments to the piped process
                elif prior_token in constants.REDIRECTION_TOKENS or seen_pipe:
                    return self.path_complete(text, line, begidx, endidx)

                # If there were redirection strings anywhere on the command line, then we
                # are no longer tab completing for the current command
                elif has_redirection:
                    return []

        # Call the command's completer function
        return compfunc(text, line, begidx, endidx)
--------------------
Prompt: 
def get_website_configuration(self, headers=None):
        """
        Returns the current status of website configuration on the bucket.

        :rtype: dict
        :returns: A dictionary containing a Python representation
                  of the XML response from S3. The overall structure is:

        * WebsiteConfiguration

          * IndexDocument

            * Suffix : suffix that is appended to request that
              is for a "directory" on the website endpoint
            * ErrorDocument

              * Key : name of object to serve when an error occurs
        """
----------
Output: 

        if not headers:
            headers = {}
        return self._get_bucket_website_configuration(headers)

    def _get_bucket_website_configuration(self, headers):
        """
        Returns the current status of website configuration on the bucket.

        :rtype: dict
        :returns: A dictionary containing a Python representation
                  of the XML response from S3. The overall structure is:

        * WebsiteConfiguration

          * IndexDocument

            * Suffix : suffix that is appended to request that
              is for a "directory" on the website endpoint
            * ErrorDocument

              * Key : name of object to serve when an
----------
Solution: 

        response = self.connection.make_request('GET', self.name,
                query_args='website', headers=headers)
        body = response.read()
        boto.log.debug(body)
        if response.status == 200:
            e = boto.jsonresponse.Element()
            h = boto.jsonresponse.XmlHandler(e, None)
            h.parse(body)
            return e
        else:
            raise self.connection.provider.storage_response_error(
                response.status, response.reason, body)
--------------------
Prompt: 
def download_parallel_gui(root, urls, directory, min_file_size, max_file_size, no_redirects):
	"""
	called when paralled downloading is true
	"""
----------
Output: 

	global _progress_bar
	global _progress_bar_lock
	global _progress_bar_lock_timeout

	if not no_redirects:
		urls = [url for url in urls if url.startswith('http://') and url.endswith('index.html')]
		if urls:
			print('Downloading %d files from %s' % (len(urls), directory))
			for url in urls:
				if not _download_file(url, directory, min_file_size, max_file_size):
					print('Failed to download %s
----------
Solution: 

	global parallel

	# create directory to save files
	if not os.path.exists(directory):
		os.makedirs(directory)
	parallel = True
	app = progress_class(root, urls, directory, min_file_size, max_file_size, no_redirects)
--------------------
Prompt: 
def add_hashed_value(self, hash_value, store_key):
        """Add hashed value to the index.

        :param hash_value: The hashed value to be added to the index
        :type hash_value: str
        :param store_key: The key for the document in the store
        :type store_key: object

        """
----------
Output: 

        self.hashed_values[store_key] = hash_value

    def get_hashed_value(self, hash_value):
        """Get hashed value.

        :param hash_value: The hashed value
        :type hash_value: str
        :return: The hashed value
        :rtype: str

        """
        return self.hashed_values[hash_value]

    def get_document_id(self, document_id):
        """Get the document id.

        :param document_id: The document id
        :type document_id: str

        """
        return document_id

    def get
----------
Solution: 

        if self._unique and hash_value in self._index:
            raise NonUnique('Hash value {} already in index'.format(hash_value))
        if store_key not in self._index[hash_value]:
            self._index[hash_value].append(store_key)
        if hash_value not in self._reverse_index[store_key]:
            self._reverse_index[store_key].append(hash_value)
--------------------
Prompt: 
def iuptri(items, diago=True, with_inds=False):
    """
    A generator that yields the upper triangle of the matrix (items x items)

    Args:
        items: Iterable object with elements [e0, e1, ...]
        diago: False if diagonal matrix elements should be excluded
        with_inds: If True, (i,j) (e_i, e_j) is returned else (e_i, e_j)

    >>> for (ij, mate) in iuptri([0,1], with_inds=True):
    ...     print("ij:", ij, "mate:", mate)
    ij: (0, 0) mate: (0, 0)
    ij: (0, 1) mate: (0, 1)
    ij: (1, 1) mate: (1, 1)
    """
----------
Output: 
(ij, mate)
    0
    1
    2
    3
    4
    5

    >>> for (ij, mate) in iuptri([0,1], diago=True):
   ...     print(ij, mate)
    0 1
    2 3
    4 5
    0
    1
    2 5
    3

    >>> for (ij, mate) in iuptri([0,1], diago=True, with_inds=True):
   ...     print(ij, mate)
    1
    2
    3
    4 5
    0
    1
    2 3
    4 5
   
----------
Solution: 

    for (ii, item1) in enumerate(items):
        for (jj, item2) in enumerate(items):
            do_yield = (jj >= ii) if diago else (jj > ii)
            if do_yield:
                if with_inds:
                    yield (ii, jj), (item1, item2)
                else:
                    yield item1, item2
--------------------
Prompt: 
def add(self, effect):
        """
        Add an LV2 plugin encapsulated as a jack client

        :param Lv2Effect effect: Effect that will be loaded as LV2 plugin encapsulated
        """
----------
Output: 

        self.add_hook(effect)
        self.add_hook(self.hook)

    def remove(self, effect):
        """
        Remove an LV2 plugin encapsulated as a jack client

        :param Lv2Effect effect: Effect that will be loaded as LV2 plugin encapsulated
        """
        self.remove_hook(effect)
        self.remove_hook(self.hook)

    def add_hook(self, effect):
        """
        Add a LV2 hook

        :param Lv2Effect effect: Effect that will be added as a LV2 hook
----------
Solution: 

        effect.instance = self.instance_index
        self.instance_index += 1

        self.connection.send(ProtocolParser.add(effect))
--------------------
Prompt: 
def cftime_to_nptime(times):
    """Given an array of cftime.datetime objects, return an array of
    numpy.datetime64 objects of the same size"""
----------
Output: 

    return np.array([t.year, t.month, t.day, t.hour, t.minute, t.second, t.microsecond,
                    t.tzinfo.tzname(t.tzinfo.get_current_timezone()).astype(np.int64),
                    t.tzinfo.dst(t.tzinfo.get_current_timezone()).astype(np.int64)])


def npdatetime_to_date(times):
    """Given an array of npdatetime.datetime objects, return an array of
    datetime.date objects of the same size"""
    return np.array([t.
----------
Solution: 

    times = np.asarray(times)
    new = np.empty(times.shape, dtype='M8[ns]')
    for i, t in np.ndenumerate(times):
        try:
            # Use pandas.Timestamp in place of datetime.datetime, because
            # NumPy casts it safely it np.datetime64[ns] for dates outside
            # 1678 to 2262 (this is not currently the case for
            # datetime.datetime).
            dt = pd.Timestamp(t.year, t.month, t.day, t.hour, t.minute,
                              t.second, t.microsecond)
        except ValueError as e:
            raise ValueError('Cannot convert date {} to a date in the '
                             'standard calendar.  Reason: {}.'.format(t, e))
        new[i] = np.datetime64(dt)
    return new
--------------------
Prompt: 
def random(cls, span=1, seed=None):
        """
        Creates a randomized coordinate system.

        Useful for confirming that an *assembly* does not rely on its
        origin coordinate system to remain intact.

        For example, the :class:`CoordSysIndicator` *assembly* aligns 3 boxes
        along each of the :math:`XYZ` axes.
        Positioning it randomly by setting its ``world_coords`` shows that each
        box is always positioned orthogonally to the other two.

        .. doctest::

            from cqparts_misc.basic.indicators import CoordSysIndicator
            from cqparts.display import display
            from cqparts.utils import CoordSystem

            cs = CoordSysIndicator()
            cs.world_coords = CoordSystem.random()

            display(cs)  # doctest: +SKIP


        :param span: origin of return will be :math:`\pm span` per axis
        :param seed: if supplied, return is psudorandom (repeatable)
        :type seed: hashable object
        :return: randomized coordinate system
        :rtype: :class:`CoordSystem`
        """
----------
Output: 
SysIndicator
            from cqparts.utils.random import random
            from cqparts.utils.math import Math
            from cqparts.utils.geometry import Geometry
            from cqparts.utils.math import Vector
            from cqparts.utils.shapes import Rectangle, Ellipse, Square, SquareShape

            # Test randomization
            random(1, seed=1)
            # Test the calculation
            # Positioning it randomly by setting its ``world_coords`` shows that
            # each box is always positioned to the origin
            for i in range(span):
                for j in range(
----------
Solution: 

        if seed is not None:
            random.seed(seed)

        def rand_vect(min, max):
            return (
                random.uniform(min, max),
                random.uniform(min, max),
                random.uniform(min, max),
            )

        while True:
            try:
                return cls(
                    origin=rand_vect(-span, span),
                    xDir=rand_vect(-1, 1),
                    normal=rand_vect(-1, 1),
                )
            except RuntimeError:  # Base.FreeCADError inherits from RuntimeError
                # Raised if xDir & normal vectors are parallel.
                # (the chance is very low, but it could happen)
                continue
--------------------
Prompt: 
def is_removable(self, device):
        """Check if device can be removed with ``auto_remove``."""
----------
Output: 

        return device in self.auto_remove_devices

    def turn_on(self, **kwargs):
        """Turn the device on."""
        if self._turn_on_action:
            self._turn_on_action()

    def turn_off(self, **kwargs):
        """Turn the device off."""
        if self._turn_off_action:
            self._turn_off_action()

    @property
    def is_on(self):
        """Return true if switch is on."""
        return self._turn_on_state

    @property
    def device_state_attributes(self):
       
----------
Solution: 

        if not self.is_handleable(device):
            return False
        if device.is_filesystem:
            return device.is_mounted
        if device.is_crypto:
            return device.is_unlocked
        if device.is_partition_table or device.is_drive:
            return any(self.is_removable(dev)
                       for dev in self.get_all_handleable()
                       if _is_parent_of(device, dev))
        return False
--------------------
Prompt: 
def integer(self, x):
        """
        returns a plain integer
        """
----------
Output: 

        return int(x)

    def float(self, x):
        """
        returns a plain float
        """
        return float(x)

    def int(self, x):
        """
        returns a plain integer
        """
        return int(x)

    def bool(self, x):
        """
        returns a plain boolean
        """
        return x

    def string(self, x):
        """
        returns a plain string
        """
        return str(x)

    def bool_string(self, x):
        """
        returns a plain boolean
        """
        return x

    def string_list(self, x):
       
----------
Solution: 

        if type(x) is str:
            hex = binascii.unhexlify(x)
            return int.from_bytes(hex, 'big')

        return x.value if isinstance(x, FiniteField.Value) else x
--------------------
Prompt: 
def _O_dot(self, Mt):
        """
        Mt is dim_r x dim_c x d tensor
        """
----------
Output: 

        return tensor.dot(Mt, self.W)

    def _O_transpose(self, M):
        """
        M is the transpose of the matrix A
        """
        return tensor.transpose(M, axes=(0, 2, 1))

    def _O_solve(self, rhs, x, y):
        """
        rhs is the solution to the linear system
        x is the solution to the linear system
        y is the solution to the linear system
        """
        raise NotImplementedError

    def _O_trace(self, M):
        """
        M is the trace of the matrix A
        """
        raise
----------
Solution: 

        DMt = self.D()[:, :, sp.newaxis] * Mt
        WrDMtWc = vei_CoR_veX(DMt, R=self.Wr().T, C=self.Wc().T)
        ve_WrDMtWc = sp.reshape(WrDMtWc, (WrDMtWc.shape[0] * WrDMtWc.shape[1], Mt.shape[2]), order='F')
        Hi_ve_WrDMtWc = la.cho_solve((self.H_chol(), True), ve_WrDMtWc)
        vei_HiveWrDMtWc = Hi_ve_WrDMtWc.reshape(WrDMtWc.shape, order = 'F')
        Wr_HiveWrDMtWc_Wc = vei_CoR_veX(vei_HiveWrDMtWc, R=self.Wr(), C=self.Wc())
        DWrHiveWrDMtWcWc = self.D()[:,:,sp.newaxis] * Wr_HiveWrDMtWc_Wc
        RV = DMt - DWrHiveWrDMtWcWc
        return RV
--------------------
Prompt: 
def _get_primary_index_in_altered_table(self, diff):
        """
        :param diff: The table diff
        :type diff: orator.dbal.table_diff.TableDiff

        :rtype: dict
        """
----------
Output: 

        return diff.primary_index

    def _get_primary_index_in_table_and_column(self, diff, column):
        """
        :param diff: The table diff
        :type diff: orator.dbal.table_diff.TableDiff

        :rtype: dict
        """
        return orator.dbal.table_diff.get_table_index(diff.table, column)

    def _get_primary_index_in_altered_table_and_row(self, diff, row):
        """
        :param diff: The table diff
        :type diff
----------
Solution: 

        primary_index = {}

        for index in self._get_indexes_in_altered_table(diff).values():
            if index.is_primary():
                primary_index = {index.get_name(): index}

        return primary_index
--------------------
Prompt: 
def make_series_url(key):
    """For internal use. Given a series key, generate a valid URL to the series
    endpoint for that key.

    :param string key: the series key
    :rtype: string"""
----------
Output: 

    return "%s/api/v1/series/%s/series_json" % (BASE_URL, key)


def make_series_url_from_id(id):
    """For internal use. Given a series id, generate a valid URL to the series
    endpoint for that id.

    :param string id: the series id
    :rtype: string"""
    return "%s/api/v1/series/%s/series_json_id" % (make_series_url(id), id)


def make_series_url_from_name(name):
   
----------
Solution: 


    url = urlparse.urljoin(endpoint.SERIES_ENDPOINT, 'key/')
    url = urlparse.urljoin(url, urllib.quote(key))
    return url
--------------------
Prompt: 
def BackAssign(cls,
                   other_entity_klass,
                   this_entity_backpopulate_field,
                   other_entity_backpopulate_field,
                   is_many_to_one=False):
        """
        Assign defined one side mapping relationship to other side.

        For example, each employee belongs to one department, then one department
        includes many employees. If you defined each employee's department,
        this method will assign employees to ``Department.employees`` field.
        This is an one to many (department to employee) example.

        Another example would be, each employee has multiple tags. If you defined
        tags for each employee, this method will assign employees to
        ``Tag.employees`` field. This is and many to many (employee to tag) example.

        Support:

        - many to many mapping
        - one to many mapping

        :param other_entity_klass: a :class:`Constant` class.
        :param this_entity_backpopulate_field: str
        :param other_entity_backpopulate_field: str
        :param is_many_to_one: bool
        :return:
        """
----------
Output: 
 you
        defined each employee's tag, this method will assign tags to ``EmployeeTag.tags``
        field.
        """
        return cls._back_assign(
            other_entity_klass,
            this_entity_backpopulate_field,
            other_entity_backpopulate_field,
            is_many_to_one=is_many_to_one
        )

    @classmethod
    def _back_assign(cls,
                   other_entity_klass,
                   this_entity_backpopulate_field,
                   this_entity_backpopulate_field,
                   is_many_to_one=False):
----------
Solution: 

        data = dict()
        for _, other_klass in other_entity_klass.Subclasses():
            other_field_value = getattr(
                other_klass, this_entity_backpopulate_field)
            if isinstance(other_field_value, (tuple, list)):
                for self_klass in other_field_value:
                    self_key = self_klass.__name__
                    try:
                        data[self_key].append(other_klass)
                    except KeyError:
                        data[self_key] = [other_klass, ]
            else:
                if other_field_value is not None:
                    self_klass = other_field_value
                    self_key = self_klass.__name__
                    try:
                        data[self_key].append(other_klass)
                    except KeyError:
                        data[self_key] = [other_klass, ]

        if is_many_to_one:
            new_data = dict()
            for key, value in data.items():
                try:
                    new_data[key] = value[0]
                except:  # pragma: no cover
                    pass
            data = new_data

        for self_key, other_klass_list in data.items():
            setattr(getattr(cls, self_key),
                    other_entity_backpopulate_field, other_klass_list)
--------------------
Prompt: 
def scale_ps(lat):
    """
    This function calculates the scaling factor for a polar stereographic
    projection (ie. SSM/I grid) to correct area calculations. The scaling
    factor is defined (from Snyder, 1982, Map Projections used by the U.S.
    Geological Survey) as:

    k = (mc/m)*(t/tc), where:

    m = cos(lat)/sqrt(1 - e2*sin(lat)^2)
    t = tan(Pi/4 - lat/2)/((1 - e*sin(lat))/(1 + e*sin(lat)))^(e/2)
    e2 = 0.006693883 is the earth eccentricity (Hughes ellipsoid)
    e = sqrt(e2)
    mc = m at the reference latitude (70 degrees)
    tc = t at the reference latitude (70 degrees)

    The ratio mc/tc is precalculated and stored in the variable m70_t70.

    From Ben Smith PS scale m file (7/12/12)
    """
----------
Output: 
**2 + 1)

    where:

    - mc/m is the distance between the x-axis and the stereographic
      projection and the x-axis of the projection (in degrees)
    - t is the angle between the stereographic projection and the stereographic
      projection (in degrees)
    """
    return math.cos(lat) * math.tan(math.radians(90 - lat))


def scale_map(lat):
    """
    This function calculates the scaling factor for a map stereographic
    projection (ie. SSM/I grid) to correct area calculations. The
----------
Solution: 

    lat = np.array(lat)
    if np.any(lat > 0):
        m70_t70 = 1.9332279 
        #Hack to deal with pole
        lat[lat>=90.0] = 89.999999999
    else:
        # for 71 deg, southern PS  -- checked BS 5/2012
        m70_t70 = 1.93903005  
        lat[lat<=-90.0] = -89.999999999

    #for WGS84, a=6378137, 1/f = 298.257223563 -> 1-sqrt(1-e^2) = f
    #-> 1-(1-f)^2 = e2 =    0.006694379990141
    #e2 = 0.006693883
    e2 = 0.006694379990141  # BS calculated from WGS84 parameters 5/2012
    e = np.sqrt(e2)

    lat = np.abs(np.deg2rad(lat))
    slat = np.sin(lat)
    clat = np.cos(lat)

    m = clat/np.sqrt(1. - e2*slat**2)
    t = np.tan(np.pi/4 - lat/2)/((1. - e*slat)/(1. + e*slat))**(e/2)
    k = m70_t70*t/m

    scale=(1./k)
    return scale
--------------------
Prompt: 
def mean_centroid(candidates):
    """Take the mean of all centroids in set of reference triangles.

    .. note::

       This is used **only** as a helper for :func:`locate_point`.

    Args:
        candidates (List[Tuple[float, float, float, numpy.ndarray]): List of
            4-tuples, each of which has been produced by :func:`locate_point`.
            Each 4-tuple contains

            * Three times centroid ``x``-value
            * Three times centroid ``y``-value
            * "Width" of a parameter space for a surface
            * Control points for a surface

            We only use the first two values, which are triple the desired
            value so that we can put off division by three until summing in
            our average. We don't use the other two values, they are just an
            artifact of the way ``candidates`` is constructed by the caller.

    Returns:
        Tuple[float, float]: The mean of all centroids.
    """
----------
Output: 
 ``x``-value and ``y``-values
            * Control points ``x``-value and ``y``-values
            * "Height" of a parameter space for a surface
            * "Depth" of a parameter space for a surface
            * "Height" of a surface
            * "Depth" of a parameter space for a surface

    Returns:
        numpy.ndarray: Mean of all points in the surface.

   .. note::

       This is used **only** as a helper for :func:`locate_point`.

    """
    return np.mean(candidates, axis=0)


def get_surface
----------
Solution: 

    sum_x = 0.0
    sum_y = 0.0
    for centroid_x, centroid_y, _, _ in candidates:
        sum_x += centroid_x
        sum_y += centroid_y
    denom = 3.0 * len(candidates)
    return sum_x / denom, sum_y / denom
--------------------
Prompt: 
def _check_bounds(x, bounds):
    """
    Checks whether `x` is within `bounds`. JIT-compiled in `nopython` mode
    using Numba.

    Parameters
    ----------
    x : ndarray(float, ndim=1)
        1-D array with shape (n,) of independent variables.

    bounds: ndarray(float, ndim=2)
        Sequence of (min, max) pairs for each element in x.

    Returns
    ----------
    bool
        `True` if `x` is within `bounds`, `False` otherwise.

    """
----------
Output: 

    if len(x)!= len(bounds):
        return False
    for i, (min, max) in enumerate(zip(bounds, x)):
        if not np.all(min <= i) and not np.all(max >= i):
            return False
    return True


def _check_bounds_shape(x, bounds):
    """
    Checks whether `x` has the right shape.

    Parameters
    ----------
    x : ndarray(float, ndim=1)
        1-D array with shape (n,) of independent variables.

    bounds: ndarray(float, ndim=2)
        Sequence
----------
Solution: 

    if bounds.shape == (0, 2):
        return True
    else:
        return ((np.atleast_2d(bounds)[:, 0] <= x).all() and
                (x <= np.atleast_2d(bounds)[:, 1]).all())
--------------------
Prompt: 
def set_tare(self, tare):
        """
        :param tare: weight of tank as printed on can
        :return: nothing
        tare is not set in desired state, but on the main device.
        """
----------
Output: 

        if tare == None:
            return
        if tare == None:
            self.write_error("Invalid weight")
            return
        self.write_line("TANK: %s" % tare)
        self.write_line("WEIGHT: %s" % self.get_tare(tare))

    def get_tare(self, tare):
        """
        :param tare: weight of tank as printed on can
        :return: weight of tank as printed on the main device.
        """
        if tare == None:
            return
        if tare == None
----------
Solution: 

        response = self.api_interface.set_device_state(self, {"tare": tare})
        self._update_state_from_response(response)
--------------------
Prompt: 
def _get_label(placeholder, label_mapping, label_i):
    """Helper function to either get the appropriate label for a given placeholder or generate
    a new label and update the mapping.

    See :py:func:`instantiate_labels` for usage.
    """
----------
Output: 

    if placeholder is None:
        return None
    if isinstance(placeholder, dict):
        return generate_label(placeholder, label_mapping, label_i)
    elif isinstance(placeholder, str):
        return generate_label(placeholder, label_mapping, label_i)
    else:
        raise TypeError("Unknown type of label: {}".format(type(placeholder)))


def generate_label(placeholder, label_mapping, label_i):
    """Generate a label for a given placeholder.

    :param placeholder: The name of the placeholder.
    :param label_mapping: A mapping from placeholder names to the
----------
Solution: 

    if placeholder in label_mapping:
        return label_mapping[placeholder], label_mapping, label_i

    new_target = Label("{}{}".format(placeholder.prefix, label_i))
    label_i += 1
    label_mapping[placeholder] = new_target
    return new_target, label_mapping, label_i
--------------------
Prompt: 
def set_alias(cls, domain, login, aliases):
        """Update aliases on a mailbox."""
----------
Output: 

        return cls._simple_command('SET', domain, login, aliases)

    @classmethod
    def setacl(cls, domain, acl):
        """Set the ACL on a mailbox."""
        return cls._simple_command('SETACL', domain, acl)

    @classmethod
    def setquota(cls, domain, mailbox, value):
        """Set the quota on a mailbox."""
        return cls._simple_command('SETQUOTA', domain, mailbox, value)

    @classmethod
    def setretr(cls, domain, mailbox, retr):
        """Set the retr on a mailbox."""
        return cls
----------
Solution: 

        return cls.call('domain.mailbox.alias.set', domain, login, aliases)
--------------------
Prompt: 
def validate():
    """Validate the Swagger/OpenAPI specification with your config"""
----------
Output: 

    if not request.json:
        abort(400, "Missing request body")
    if not request.data:
        abort(400, "Missing request body")
    if not request.headers.get('Content-Type', None):
        abort(400, "Missing Content-Type header")
    if not request.headers.get('Accept', None):
        abort(400, "Missing Accept header")
    if not request.headers.get('Authorization', None):
        abort(400, "Missing Authorization header")


def get_user_from_token(token):
    """Get the user object from the
----------
Solution: 

    with current_app.test_request_context():
        schema = json.loads(json.dumps(api.__schema__))
    try:
        schemas.validate(schema)
        success('API specifications are valid')
    except schemas.SchemaValidationError as e:
        exit_with_error('API specifications are not valid', e)
--------------------
Prompt: 
def get_string(self):
        """ Get file format as string

        :return: String describing the file format
        :rtype: str
        """
----------
Output: 

        return self.file_format

    @property
    def name(self):
        """ Get file name

        :return: Name of the file
        :rtype: str
        """
        return self.file_name

    @property
    def path(self):
        """ Get file path

        :return: Path of the file
        :rtype: str
        """
        return self.file_path

    @property
    def size(self):
        """ Get file size

        :return: Size of the file
        :rtype: int
        """
        return self.file_size

    @property
    def md5(self):
        """ Get
----------
Solution: 

        if self in [MimeType.TIFF_d8, MimeType.TIFF_d16, MimeType.TIFF_d32f]:
            return 'image/{}'.format(self.value)
        if self is MimeType.JP2:
            return 'image/jpeg2000'
        if self in [MimeType.RAW, MimeType.REQUESTS_RESPONSE]:
            return self.value
        return mimetypes.types_map['.' + self.value]
--------------------
Prompt: 
def closing_plugin(self, cancelable=False):
        """Perform actions before parent main window is closed."""
----------
Output: 

        if cancelable:
            self.close_action.setEnabled(False)
        else:
            self.close_action.setEnabled(True)

    def set_plugin_state(self, state):
        """Set the plugin state."""
        self.plugin_state = state
        self.plugin_name = state.plugin_name

    def set_plugin_name(self, name):
        """Set the plugin name."""
        self.plugin_name = name
        self.plugin_state = state.plugin_state

    def set_plugin_state_from_text(self, text):
        """Set the
----------
Solution: 

        for cl in self.clients:
            cl.close()
        self.set_option('recent_notebooks', self.recent_notebooks)
        return True
--------------------
Prompt: 
def lookup(source, keys, fallback = None):
  """Traverses the source, looking up each key.  Returns None if can't find anything instead of raising an exception."""
----------
Output: 

  for key in keys:
    if key in source:
      return key
    elif fallback is not None:
      return fallback
    else:
      if source[key] == None:
        return None
      elif source[key] == '':
        return None
      else:
        return lookup(source[key], keys, fallback)
  return None

def get_all_keys(source):
  """Returns a list of all keys in the source."""
  return [lookup(x, keys, x) for x in lookup(source, ['key', 'key', 'key', 'key', 'key', 'key',
----------
Solution: 

  try:
    for key in keys:
      source = source[key]
    return source
  except (KeyError, AttributeError, TypeError):
    return fallback
--------------------
Prompt: 
def _indicator_multitraj(ss, i, j):
    """ Returns conditional sequence for transition i -> j given all conditional sequences """
----------
Output: 

    if i == j:
        return ss[j]
    else:
        return [ss[i], ss[j]]

def _get_transition_matrix(ss, i, j):
    """ Returns conditional matrix for transition i -> j given all conditional sequences """
    if i == j:
        return ss[j]
    else:
        return [ss[i], ss[j]]

def _get_transition_matrix_from_file(filename):
    """ Returns the transition matrix from a file """
    with open(filename) as f:
        ss = f.read()
    return ss
----------
Solution: 

    iseqs = ss[i]
    res = []
    for iseq in iseqs:
        x = np.zeros(len(iseq))
        I = np.where(iseq == j)
        x[I] = 1.0
        res.append(x)
    return res
--------------------
Prompt: 
def valid_value(value, quote=default_cookie_quote, unquote=default_unquote):
    """Validate a cookie value string.

    This is generic across quote/unquote functions because it directly verifies
    the encoding round-trip using the specified quote/unquote functions.
    So if you use different quote/unquote functions, use something like this
    as a replacement for valid_value::

        my_valid_value = lambda value: valid_value(value, quote=my_quote,
                                                          unquote=my_unquote)
    """
----------
Output: 

    if value is None:
        return default_unquote(quote, unquote)
    if not value:
        return default_cookie_quote(u'', u'')
    # This is the old version which used to be supported
    # I've seen this happening because I don't know what it means for now.
    # I'm not sure if I can use the new version.
    #    old_version = None
    #    try:
    #        old_version = parse_cookie(value)
    #        if old_version is None:
    #            return default_unquote(my_valid_value('
----------
Solution: 

    if value is None:
        return False

    # Put the value through a round trip with the given quote and unquote
    # functions, so we will know whether data will get lost or not in the event
    # that we don't complain.
    encoded = encode_cookie_value(value, quote=quote)
    decoded = parse_string(encoded, unquote=unquote)

    # If the original string made the round trip, this is a valid value for the
    # given quote and unquote functions. Since the round trip can generate
    # different unicode forms, normalize before comparing, so we can ignore
    # trivial inequalities.
    decoded_normalized = (normalize("NFKD", decoded)
                          if not isinstance(decoded, bytes) else decoded)
    value_normalized = (normalize("NFKD", value)
                        if not isinstance(value, bytes) else value)
    if decoded_normalized == value_normalized:
        return True
    return False
--------------------
Prompt: 
def atlas_peer_download_zonefile_inventory( my_hostport, peer_hostport, maxlen, bit_offset=0, timeout=None, peer_table={} ):
    """
    Get the zonefile inventory from the remote peer
    Start from the given bit_offset

    NOTE: this doesn't update the peer table health by default;
    you'll have to explicitly pass in a peer table (i.e. setting
    to {} ensures that nothing happens).
    """
----------
Output: 

    if timeout is None:
        timeout = 0
    if timeout > 0:
        timeout = int(timeout)
    if bit_offset > 0:
        bit_offset = int(bit_offset)
    if timeout > maxlen:
        maxlen = int(maxlen)
    if timeout > 0:
        timeout = int(timeout)
    if not timeout:
        timeout = 0
    if timeout > 0:
        timeout = int(timeout)
    if timeout > 0:
        my_hostport = int(my_hostport)
        peer_hostport = int(peer_hostport)
        if not atlas_
----------
Solution: 


    if timeout is None:
        timeout = atlas_inv_timeout()

    interval = 524288       # number of bits in 64KB
    peer_inv = ""

    log.debug("Download zonefile inventory %s-%s from %s" % (bit_offset, maxlen, peer_hostport))

    if bit_offset > maxlen:
        # synced already
        return peer_inv

    for offset in xrange( bit_offset, maxlen, interval):
        next_inv = atlas_peer_get_zonefile_inventory_range( my_hostport, peer_hostport, offset, interval, timeout=timeout, peer_table=peer_table )
        if next_inv is None:
            # partial failure
            log.debug("Failed to sync inventory for %s from %s to %s" % (peer_hostport, offset, offset+interval))
            break

        peer_inv += next_inv
        if len(next_inv) < interval:
            # end-of-interval
            break

    return peer_inv
--------------------
Prompt: 
def get(self, pid, record, **kwargs):
        """Get a record.

        Permissions: ``read_permission_factory``

        Procedure description:

        #. The record is resolved reading the pid value from the url.

        #. The ETag and If-Modifed-Since is checked.

        #. The HTTP response is built with the help of the link factory.

        :param pid: Persistent identifier for record.
        :param record: Record object.
        :returns: The requested record.
        """
----------
Output: 

        return self._get_link(pid, record, **kwargs)

    def _get_link(self, pid, record, **kwargs):
        """Get a record.

        Permissions: ``read_permission_factory``

        Procedure description:

        #. The url is resolved using the record.

        :param pid: Persistent identifier for record.
        :param record: Record object.
        :returns: The requested record.
        """
        return record.links.filter(
            id=pid,
            record_id=record.id,
            record_type=record.type,
            status=record.
----------
Solution: 

        etag = str(record.revision_id)
        self.check_etag(str(record.revision_id))
        self.check_if_modified_since(record.updated, etag=etag)

        return self.make_response(
            pid, record, links_factory=self.links_factory
        )
--------------------
Prompt: 
def _dictload(l: Loader, value, type_) -> Dict:
    """
    This loads into something like Dict[str,str]

    Recursively loads both keys and values.
    """
----------
Output: 

    if isinstance(value, dict):
        return _dictload(value, l, type_)
    elif isinstance(value, list):
        return _listload(value, l)
    elif isinstance(value, str):
        return _strload(l, value)
    elif isinstance(value, str):
        return _strload(value, value)
    elif isinstance(value, str):
        return _strload(value, value)
    elif isinstance(value, str):
        return _strload(value, value)
    elif isinstance(value, str):
        return _strload(value, value)
----------
Solution: 

    key_type, value_type = type_.__args__
    try:
        return {
            l.load(k, key_type, annotation=Annotation(AnnotationType.KEY, k)): l.load(v, value_type, annotation=Annotation(AnnotationType.VALUE, v))
            for k, v in value.items()}
    except AttributeError as e:
        raise TypedloadAttributeError(str(e), type_=type_, value=value)
--------------------
Prompt: 
def parse_json(json_file):
    """Parse a whole json record from the given file.

    Return None if the json file does not exists or exception occurs.

    Args:
        json_file (str): File path to be parsed.

    Returns:
        A dict of json info.
    """
----------
Output: 

    with open(json_file, 'r') as f:
        return json.load(f)


def main():
    """Main function.

    Args:
        None.
    """
    parser = argparse.ArgumentParser(description='Generate a JSON file.')
    parser.add_argument('--output', '-o', type=str, default='output.json',
                        help='Output file path. Defaults to output.json.')
    parser.add_argument('--output-format', '-f', type=str, default='json',
                        help='Output file format. Defaults to json.')
    parser.add_argument
----------
Solution: 

    if not os.path.exists(json_file):
        return None

    try:
        with open(json_file, "r") as f:
            info_str = f.readlines()
            info_str = "".join(info_str)
            json_info = json.loads(info_str)
            return unicode2str(json_info)
    except BaseException as e:
        logging.error(e.message)
        return None
--------------------
Prompt: 
def get_gradebook_lookup_session(self, proxy):
        """Gets the OsidSession associated with the gradebook lookup service.

        arg:    proxy (osid.proxy.Proxy): a proxy
        return: (osid.grading.GradebookLookupSession) - a
                ``GradebookLookupSession``
        raise:  NullArgument - ``proxy`` is ``null``
        raise:  OperationFailed - unable to complete request
        raise:  Unimplemented - ``supports_gradebook_lookup() is false``
        *compliance: optional -- This method must be implemented if
        ``supports_gradebook_lookup()`` is true.*

        """
----------
Output: 
_gradebook_lookup()`` is true.*

        """
        return # osid.grading.GradebookLookupSession

    gradebook_lookup_session = property(fget=get_gradebook_lookup_session)

    def get_gradebook_form_for_create(self, gradebook_form_id):
        """Gets the gradebook form for creating new gradebook.

        arg:    gradebook_form_id (osid.id.Id): the ``Id`` of the
                ``Gradebook``
        return: (osid.grading.GradebookForm) - the gradebook
----------
Solution: 

        if not self.supports_gradebook_lookup():
            raise errors.Unimplemented()
        # pylint: disable=no-member
        return sessions.GradebookLookupSession(proxy=proxy, runtime=self._runtime)
--------------------
Prompt: 
def from_jd(jd):
    """Calculate Mayan long count from Julian day"""
----------
Output: 

    return int(jd - jd.utcoffset()) * 86400 + int(jd.dst() / 86400)

def from_jd_date(jd):
    """Calculate Mayan date from Julian day"""
    return datetime.date(jd - jd.utcoffset() * 86400, jd.dst() // 86400, jd.utcoffset() % 86400)

def from_jd_time(jd):
    """Calculate Mayan time from Julian day"""
    return jd - jd.utcoffset(), jd - jd.utcoffset()

def from_jd_datetime(jd):
----------
Solution: 

    d = jd - EPOCH
    baktun = trunc(d / 144000)
    d = (d % 144000)
    katun = trunc(d / 7200)
    d = (d % 7200)
    tun = trunc(d / 360)
    d = (d % 360)
    uinal = trunc(d / 20)
    kin = int((d % 20))

    return (baktun, katun, tun, uinal, kin)
--------------------
Prompt: 
def _write(self, session, openFile, replaceParamFile):
        """
        Raster Map File Write to File Method
        """
----------
Output: 

        if self.is_local:
            self.file.write(openFile.read())
        else:
            self.file.write(openFile.read().replace(replaceParamFile, paramFile))
        self.file.write(openFile.read())

    def _write_header(self, session, header, data):
        """
        Write the header to the file
        """
        if self.is_local:
            self.file.write(header)
        else:
            self.file.write(data)

    def _write_data(self, session, openFile, replaceParamFile
----------
Solution: 

        # If the raster field is not empty, write from this field
        if self.raster is not None:
            # Configure RasterConverter
            converter = RasterConverter(session)

            # Use MapKit RasterConverter to retrieve the raster as a GRASS ASCII Grid
            grassAsciiGrid = converter.getAsGrassAsciiRaster(rasterFieldName='raster',
                                                             tableName=self.__tablename__,
                                                             rasterIdFieldName='id',
                                                             rasterId=self.id)
            # Write to file
            openFile.write(grassAsciiGrid)

        elif self.rasterText is not None:
            # Write file
            openFile.write(self.rasterText)
--------------------
Prompt: 
def object_from_string(name):
    """Creates a Python class or function from its fully qualified name.

    :param name: A fully qualified name of a class or a function. In Python 3 this
        is only allowed to be of text type (unicode). In Python 2, both bytes and unicode
        are allowed.
    :return: A function or class object.

    This method is used by serialization code to create a function or class
    from a fully qualified name.

    """
----------
Output: 

    if isinstance(name, bytes):
        name = name.decode('utf-8')
    if isinstance(name, str):
        name = name.split('.')[-1]
    return getattr(sys.modules[__name__], name)


def _get_callable_name(callable_):
    """Returns the fully qualified callable name of a callable."""
    if isinstance(callable_, types.FunctionType):
        return callable_.__name__
    return callable_.__class__.__name__


def _get_function_code(callable_):
    """Returns the code object for a callable."""
    if isinstance(callable_, types
----------
Solution: 

    if six.PY3:
        if not isinstance(name, str):
            raise TypeError("name must be str, not %r" % type(name))
    else:
        if isinstance(name, unicode):
            name = name.encode("ascii")
        if not isinstance(name, (str, unicode)):
            raise TypeError("name must be bytes or unicode, got %r" % type(name))

    pos = name.rfind(".")
    if pos < 0:
        raise ValueError("Invalid function or class name %s" % name)
    module_name = name[:pos]
    func_name = name[pos + 1 :]
    try:
        mod = __import__(module_name, fromlist=[func_name], level=0)
    except ImportError:
        # Hail mary. if the from import doesn't work, then just import the top level module
        # and do getattr on it, one level at a time. This will handle cases where imports are
        # done like `from . import submodule as another_name`
        parts = name.split(".")
        mod = __import__(parts[0], level=0)
        for i in range(1, len(parts)):
            mod = getattr(mod, parts[i])
        return mod

    else:
        return getattr(mod, func_name)
--------------------
Prompt: 
def adjust_for_length(self, key, r, kwargs):
        """
        Converts the response to a string and compares its length to a max
        length specified in settings. If the response is too long, an error is
        logged, and an abbreviated response is returned instead.
        """
----------
Output: 

        if len(r) > 1000:
            raise ValueError("The length of the response is too long to fit in a single query")
        if len(r) < 1:
            raise ValueError("The length of the response is too short to fit in a single query")
        return self.connection.ops.convert_length(key, r)

    def get_for_update(self, key, value):
        """
        Returns the max length for the given key.
        """
        if len(value) > 1000:
            raise ValueError("The length of the response is too long to fit in a single update")
----------
Solution: 

        length = len(str(kwargs))
        if length > settings.defaults["max_detail_length"]:
            self._log_length_error(key, length)
            r["max_detail_length_error"] = length
            return r
        return kwargs
--------------------
Prompt: 
def heapreplace(heap, item):
    """Pop and return the current smallest value, and add the new item.

    This is more efficient than heappop() followed by heappush(), and can be
    more appropriate when using a fixed-size heap.  Note that the value
    returned may be larger than item!  That constrains reasonable uses of
    this routine unless written as part of a conditional replacement:

        if item > heap[0]:
            item = heapreplace(heap, item)
    """
----------
Output: 


    def merge(heap, item):
        if heap[0] < item:
            return heap[0], heap[0] + 1
        elif heap[0] > item:
            return heap[0] - 1, heap[0]

    def heappush(heap, item):
        heappush(heap, item, item)
        heappushpop(heap, item)

    def heappop(heap):
        """
        Merge the elements of a heap object into a single heappop.

        This is a heappushed version of a heap.  It is similar to the 2-
----------
Solution: 

    returnitem = heap[0]    # raises appropriate IndexError if heap is empty
    heap[0] = item
    _siftup(heap, 0)
    return returnitem
--------------------
Prompt: 
def Close(self, abort=False):
    """Closes the queue.

    Args:
      abort (Optional[bool]): whether the Close is the result of an abort
          condition. If True, queue contents may be lost.

    Raises:
      QueueAlreadyClosed: if the queue is not started, or has already been
          closed.
      RuntimeError: if closed or terminate event is missing.
    """
----------
Output: 

    if abort:
      self.queue.abort()
    self.closed = True
    self.queue.close()

  def Add(self, item):
    """Adds an item to the queue.

    Args:
      item (object): an item to add to the queue.

    Raises:
      ValueError: if the item is not a string.
      TypeError: if the item is not a class.
      ValueError: if the item is not a string.
      TypeError: if the item is not a class.
      ValueError: if the item is not a string.
    """
    if not isinstance(item, basestring):
      raise
----------
Solution: 

    if not self._closed_event or not self._terminate_event:
      raise RuntimeError('Missing closed or terminate event.')

    if not abort and self._closed_event.is_set():
      raise errors.QueueAlreadyClosed()

    self._closed_event.set()

    if abort:
      if not self._closed_event.is_set():
        logger.warning(
            '{0:s} queue aborting. Contents may be lost.'.format(self.name))

      self._linger_seconds = 0

      # We can't determine whether a there might be an operation being performed
      # on the socket in a separate method or thread, so we'll signal that any
      # such operation should cease.
      self._terminate_event.set()

    else:
      logger.debug(
          '{0:s} queue closing, will linger for up to {1:d} seconds'.format(
              self.name, self._linger_seconds))
--------------------
Prompt: 
def get_classname(class_, local=False):
    r"""
    Args:
        class_ (type):
        local (bool): (default = False)

    Returns:
        str: classname

    CommandLine:
        python -m utool.util_class --exec-get_classname --show

    Example:
        >>> # DISABLE_DOCTEST
        >>> from utool.util_class import *  # NOQA
        >>> import utool as ut
        >>> class_ = ReloadingMetaclass
        >>> local = False
        >>> assert get_classname(class_, local) == 'utool.util_class.ReloadingMetaclass'
        >>> assert get_classname(class_, local=True) == 'ReloadingMetaclass'
    """
----------
Output: 
class.ReloadingMetaclass'
        >>> # DISABLE_DOCTEST
        >>> from utool.util_class import *  # NOQA
        >>> import utool.util.testing as tm
        >>> # build test data
        >>> data = TestData()
        >>> data.add_reloading_class(ReloadingClass)
        >>> data.add_reloading_data(data)
        >>> # execute
        >>> result = data.execute()
        >>> assert result.shape == data.shape
        >>> assert np.all(result == data)
        >>> # verify
        >>> assert np.all(result ==
----------
Solution: 

    if not local:
        classname = class_.__module__ + '.' + class_.__name__
    else:
        classname = class_.__name__
    return classname
--------------------
Prompt: 
def _replace_global_vars(xs, global_vars):
    """Replace globally shared names from input header with value.

    The value of the `algorithm` item may be a pointer to a real
    file specified in the `global` section. If found, replace with
    the full value.
    """
----------
Output: 

    if isinstance(xs, str):
        return xs.replace('\0', '')
    else:
        return os.path.abspath(os.path.join(os.path.dirname(xs), algorithm))


def _get_c_sources(sources, c_sources):
    """Return a list of C source files, given a list of C source files
    list."""
    if not sources:
        return c_sources
    elif isinstance(sources[0], str):
        return c_sources[sources].replace('\0', '')
    else:
        return [sources] + c_sources


class
----------
Solution: 

    if isinstance(xs, (list, tuple)):
        return [_replace_global_vars(x) for x in xs]
    elif isinstance(xs, dict):
        final = {}
        for k, v in xs.items():
            if isinstance(v, six.string_types) and v in global_vars:
                v = global_vars[v]
            final[k] = v
        return final
    else:
        return xs
--------------------
Prompt: 
def import_vmdk(self):
        """
        All actions necessary to import vmdk (calls s3 upload, and import to aws ec2)
        :param vmdk_location: location of vmdk to import. Can be provided as a string, or the result output of fabric
        execution
        :return:
        """
----------
Output: 

        if self.is_action_import_vmdk_or_s3_upload():
            self.logger.info("Importing vmdk or s3 upload")
            self.import_vmdk()
            self.logger.info("Importing s3 upload")
            self.import_s3()
        else:
            self.logger.info("Skipping import vmdk or s3 upload")

    def is_action_import_vmdk_or_s3_upload(self):
        """
        Check if the action is import to AWS EC2 or S3.
        :return:
        """
       
----------
Solution: 

        # Set the inital upload to be the first region in the list
        first_upload_region = self.aws_regions[0]

        print "Initial AMI will be created in: {}".format(first_upload_region)
        self.upload_to_s3(region=first_upload_region)
        # If the upload was successful, the name to reference for import is now the basename
        description = "AMI upload of: {}".format(os.path.basename(self.upload_file))
        temp_fd, file_location = self.create_config_file(os.path.basename(self.upload_file), description)
        import_id = self.run_ec2_import(file_location, description, first_upload_region)
        self.wait_for_import_to_complete(import_id)
        self.rename_image(import_id, self.ami_name, source_region=first_upload_region)
        return import_id
--------------------
Prompt: 
def _input_as_lines(self,data):
        """Writes data to tempfile and sets -infile parameter

        data -- list of lines, ready to be written to file
        """
----------
Output: 

        if self.infile:
            self.infile.write(data)
        self.infile.write('\n'.join(data))
        self.infile.flush()

    def _output_as_lines(self,data):
        """Writes data to tempfile and sets -outfile parameter

        data -- list of lines, ready to be written to file
        """
        if self.outfile:
            self.outfile.write(data)
        self.outfile.flush()

    def _get_file_path(self,filename):
        """Returns the full path to the file

        filename -- full path to the file
        """
----------
Solution: 

        if data:
            self.Parameters['-infile']\
                .on(super(Clustalw,self)._input_as_lines(data))
        return ''
--------------------
Prompt: 
def makeSoftwareVersion(store, version, systemVersion):
    """
    Return the SoftwareVersion object from store corresponding to the
    version object, creating it if it doesn't already exist.
    """
----------
Output: 

    if not store.hasFile(version.filename):
        return None
    return store.getFile(version.filename)


def makePythonVersion(pythonVersion):
    """
    Return the PythonVersion object from the given PythonVersion,
    creating it if it doesn't already exist.
    """
    if not pythonVersion.isPython:
        return None
    return pythonVersion.getVersion()


def makePythonVersion(store, version, pythonVersion):
    """
    Return the PythonVersion object from the given PythonVersion,
    creating it if it doesn't already exist.
    """
    if not store.has
----------
Solution: 

    return store.findOrCreate(SoftwareVersion,
                              systemVersion=systemVersion,
                              package=unicode(version.package),
                              version=unicode(version.short()),
                              major=version.major,
                              minor=version.minor,
                              micro=version.micro)
--------------------
Prompt: 
def add_vcenter(self, **kwargs):
        """
        Add vCenter on the switch

        Args:
            id(str) : Name of an established vCenter
            url (bool) : vCenter URL
            username (str): Username of the vCenter
            password (str): Password of the vCenter
            callback (function): A function executed upon completion of the
                 method.

        Returns:
           Return value of `callback`.

        Raises:
            None
        """
----------
Output: 

        return self.rest.add_vcenter(id, **kwargs)

    def delete_vcenter(self, **kwargs):
        """
        Delete vCenter on the switch

        Args:
            id(str) : Name of an established vCenter
            url (bool) : vCenter URL
            username (str): Username of the vCenter
            password (str): Password of the vCenter
            callback (function): A function executed upon completion of the
                 method.

        Returns:
           Return value of `callback`.

        Raises:
            None
        """
        return self.rest.delete_vcenter(id, **kwargs
----------
Solution: 

        config = ET.Element("config")
        vcenter = ET.SubElement(config, "vcenter",
                                xmlns="urn:brocade.com:mgmt:brocade-vswitch")
        id = ET.SubElement(vcenter, "id")
        id.text = kwargs.pop('id')
        credentials = ET.SubElement(vcenter, "credentials")
        url = ET.SubElement(credentials, "url")
        url.text = kwargs.pop('url')
        username = ET.SubElement(credentials, "username")
        username.text = kwargs.pop('username')
        password = ET.SubElement(credentials, "password")
        password.text = kwargs.pop('password')

        try:
            self._callback(config)
            return True

        except Exception as error:
            logging.error(error)
            return False
--------------------
Prompt: 
def is_correct(self):
        """Check if this object configuration is correct ::

        * Check our own specific properties
        * Call our parent class is_correct checker

        :return: True if the configuration is correct, otherwise False
        :rtype: bool
        """
----------
Output: 

        return self._is_correct

    @property
    def is_correct_type(self):
        """Check if this object configuration is correct ::

        * Check our own specific properties
        * Call our parent class is_correct checker

        :return: True if the configuration is correct, otherwise False
        :rtype: bool
        """
        return self._is_correct_type

    @property
    def is_correct_value(self):
        """Check if this object configuration is correct ::

        * Check our own specific properties
        * Call our parent class is_correct checker

        :return: True if the configuration is correct, otherwise
----------
Solution: 

        state = True

        # Internal checks before executing inherited function...
        if not hasattr(self, 'check_command'):
            msg = "[checkmodulation::%s] do not have any check_command defined" % (
                self.get_name()
            )
            self.add_error(msg)
            state = False
        else:
            if self.check_command is None:
                msg = "[checkmodulation::%s] a check_command is missing" % (self.get_name())
                self.add_error(msg)
                state = False
            if self.check_command and not self.check_command.is_valid():
                msg = "[checkmodulation::%s] a check_command is invalid" % (self.get_name())
                self.add_error(msg)
                state = False

        # Ok just put None as check_period, means 24x7
        if not hasattr(self, 'check_period'):
            self.check_period = None

        return super(CheckModulation, self).is_correct() and state
--------------------
Prompt: 
def get_attribute_classes() -> Dict[str, Attribute]:
    """
    Lookup all builtin Attribute subclasses, load them, and return a dict
    """
----------
Output: 

    return {
        attr.name: attr.value
        for attr in Attribute.attributes
        if isinstance(attr, Attribute)
    }


def get_attribute_class(name: str) -> Attribute:
    """
    Lookup a builtin attribute by name, load it, and return a Value
    """
    return get_attribute_classes()[name]


def get_attribute_value(name: str, default: Any = None) -> Any:
    """
    Lookup a value by name, load it, and return the value of the attribute
    """
    return get_attribute_value(name, default,
----------
Solution: 

    attribute_children = pkgutil.iter_modules(
        importlib.import_module('jawa.attributes').__path__,
        prefix='jawa.attributes.'
    )

    result = {}
    for _, name, _ in attribute_children:
        classes = inspect.getmembers(
            importlib.import_module(name),
            lambda c: (
                inspect.isclass(c) and issubclass(c, Attribute) and
                c is not Attribute
            )
        )

        for class_name, class_ in classes:
            attribute_name = getattr(class_, 'ATTRIBUTE_NAME', class_name[:-9])
            result[attribute_name] = class_

    return result
--------------------
Prompt: 
def set_as_environment_variables(self, override=False):
        """
        Load the current dotenv as system environemt variable.
        """
----------
Output: 

        if override:
            self.dotenv.update(override_env(self.env))
        else:
            self.dotenv.update(self.env)

    def get_as_dict(self):
        """
        Return the current environment as a dict.
        """
        return self.dotenv


class Environment(object):
    """
    A class representing a single environment.
    """

    def __init__(self, name, data, description=None,
                 version=None,
                 environment_variables=None,
                 override_environment_variables=False):
        """
        Initialize a new Environment.

       
----------
Solution: 

        for k, v in self.dict().items():
            if k in os.environ and not override:
                continue
            # With Python2 on Windows, force environment variables to str to avoid
            # "TypeError: environment can only contain strings" in Python's subprocess.py.
            if PY2 and WIN:
                if isinstance(k, text_type) or isinstance(v, text_type):
                    k = k.encode('ascii')
                    v = v.encode('ascii')
            os.environ[k] = v

        return True
--------------------
Prompt: 
def mock_cmd(self, release, *cmd, **kwargs):
        """Run a mock command in the chroot for a given release"""
----------
Output: 

        if release == '1':
            return ['chroot', 'add', 'path', '-p', cmd]
        else:
            return ['chroot', 'add', 'path', '-t', cmd]

    def test_add_with_no_args(self):
        """Test adding a path with no arguments"""
        self.mock_cmd(self.release, *self.cmd)
        self.assertEqual(self.mock_cmd(self.release, *self.cmd),
                         ['add', 'path', '--release', self.release, '--args', ''])

    def test_add_with
----------
Solution: 

        fmt = '{mock_cmd}'
        if kwargs.get('new_chroot') is True:
            fmt +=' --new-chroot'
        fmt += ' --configdir={mock_dir}'
        return self.call(fmt.format(**release).split()
                         + list(cmd))
--------------------
Prompt: 
def cached_instance(model, timeout=None, **filters):
    """ Auto cached model instance.
    """
----------
Output: 

    if timeout is None:
        timeout = config.get('CACHE_TIMEOUT', 300)
    if timeout is None:
        timeout = config.get('CACHE_TIMEOUT', 60)
    return model.objects.get(timeout=timeout)


def get_cache_key(model, timeout=None):
    """ Auto cached model instance.
    """
    if timeout is None:
        timeout = config.get('CACHE_TIMEOUT', 60)
    if timeout is None:
        timeout = config.get('CACHE_TIMEOUT', 60)
    return model.objects.get(cache_timeout=timeout)



----------
Solution: 

    if isinstance(model, basestring):
        model = _str_to_model(model)

    cache_key = generate_cache_key(model, **filters)
    return get_cached(cache_key, model.objects.select_related().get, kwargs=filters)
--------------------
Prompt: 
def zoom_delta(self, d, p=(0., 0.), c=1.):
        """Zoom the view by a given amount."""
----------
Output: 

        self.view_limits = (c, p, p)
        self.update()

    def zoom_to_data(self, d, zoom_factor=1.):
        """Zoom the view to a given factor."""
        self.view_limits = (zoom_factor, d, d)
        self.update()

    def update(self):
        """Update the view limits."""
        self.view_limits = self.zoom_limits
        self.zoom_factor = self.view_limits[0]
        self.p = self.view_limits[1]
        self.zoom_factor
----------
Solution: 

        dx, dy = d
        x0, y0 = p

        pan_x, pan_y = self._pan
        zoom_x, zoom_y = self._zoom
        zoom_x_new, zoom_y_new = (zoom_x * math.exp(c * self._zoom_coeff * dx),
                                  zoom_y * math.exp(c * self._zoom_coeff * dy))

        zoom_x_new = max(min(zoom_x_new, self._zmax), self._zmin)
        zoom_y_new = max(min(zoom_y_new, self._zmax), self._zmin)

        self.zoom = zoom_x_new, zoom_y_new

        if self._zoom_to_pointer:
            zoom_x, zoom_y = self._zoom_aspect((zoom_x,
                                                zoom_y))
            zoom_x_new, zoom_y_new = self._zoom_aspect((zoom_x_new,
                                                        zoom_y_new))

            self.pan = (pan_x - x0 * (1. / zoom_x - 1. / zoom_x_new),
                        pan_y - y0 * (1. / zoom_y - 1. / zoom_y_new))

        self.update()
--------------------
Prompt: 
def _known_populations(row, pops):
    """Find variants present in substantial frequency in population databases.
    """
----------
Output: 

    if row['populations'] is not None:
        return [row['populations'][x] for x in pops if x in row['populations']]
    else:
        return None

def _known_populations_with_db(row, pops):
    """Find variants present in several pops with a database.
    """
    if row['populations_with_db'] is not None:
        return [x for x in pops if x in row['populations_with_db']]
    else:
        return None

def _known_populations_with_pops
----------
Solution: 

    cutoff = 0.01
    out = set([])
    for pop, base in [("esp", "af_esp_all"), ("1000g", "af_1kg_all"),
                      ("exac", "af_exac_all"), ("anypop", "max_aaf_all")]:
        for key in [x for x in pops if x.startswith(base)]:
            val = row[key]
            if val and val > cutoff:
                out.add(pop)
    return sorted(list(out))
--------------------
Prompt: 
def tokenize_sent(mtokens, raw='', auto_strip=True):
    """ Tokenize a text to multiple sentences """
----------
Output: 

    if not raw:
        return mtokens
    if not auto_strip:
        return [token.strip() for token in raw.split('\n') if token.strip()]
    return [token.strip() for token in mtokens.split(raw) if token.strip()]


def tokenize_words(stokens, raw='', auto_strip=True):
    """ Tokenize a text to multiple words """
    if not raw:
        return stokens
    if not auto_strip:
        return [token.strip() for token in tokenize_sent(stokens, raw, auto_strip)]
    return
----------
Solution: 

    sents = []
    bucket = []
    cfrom = 0
    cto = 0
    token_cfrom = 0
    logger = getLogger()
    logger.debug("raw text: {}".format(raw))
    logger.debug("tokens: {}".format(mtokens))
    for t in mtokens:
        if t.is_eos:
            continue
        token_cfrom = raw.find(t.surface, cto)
        cto = token_cfrom + len(t.surface)  # also token_cto
        logger.debug("processing token {} <{}:{}>".format(t, token_cfrom, cto))
        bucket.append(t)
        # sentence ending
        if t.pos == '記号' and t.sc1 == '句点':
            sent_text = raw[cfrom:cto]
            getLogger().debug("sent_text = {} | <{}:{}>".format(sent_text, cfrom, cto))
            if auto_strip:
                sent_text = sent_text.strip()
            sents.append(MeCabSent(sent_text, bucket))
            logger.debug("Found a sentence: {}".format(sent_text))
            cfrom = cto
            bucket = []
    if bucket:
        logger.debug("Bucket is not empty: {}".format(bucket))
        sent_text = raw[cfrom:cto]
        logger.debug("remaining text: {} [{}:{}]".format(sent_text, cfrom, cto))
        if auto_strip:
            sent_text = sent_text.strip()
        sents.append(MeCabSent(sent_text, bucket))
    return sents
--------------------
Prompt: 
def update_aliases(self):
        """Get aliases information from room state.

        Returns:
            boolean: True if the aliases changed, False if not
        """
----------
Output: 

        if self.aliases_changed:
            self.aliases = self.get_aliases()
            return True
        return False

    def get_aliases(self):
        """Get the room aliases information.

        Returns:
            list: List of room aliases
        """
        if self.room_aliases:
            return self.room_aliases

        self.aliases_changed = True
        self.room_aliases = []

        for alias in self.aliases:
            if alias in self.aliases_dict:
                self.room_aliases.append(self.aliases_dict[alias])
            else:
                self.aliases_changed
----------
Solution: 

        try:
            response = self.client.api.get_room_state(self.room_id)
            for chunk in response:
                if "content" in chunk and "aliases" in chunk["content"]:
                    if chunk["content"]["aliases"] != self.aliases:
                        self.aliases = chunk["content"]["aliases"]
                        return True
                    else:
                        return False
        except MatrixRequestError:
            return False
--------------------
Prompt: 
def parse_args():
    """
    Argument parser and validator
    """
----------
Output: 

    parser = argparse.ArgumentParser(description='A test script to test the SAML 2.0 SAML provider')
    parser.add_argument('-s', '--saml-configuration', type=str, dest='saml_configuration',
                        help='The path to the SAML configuration file')
    parser.add_argument('-c', '--saml-configuration-class', type=str, dest='saml_configuration_class',
                        help='The class of the SAML configuration to use')
    parser.add_argument('-a', '--attribute-mapping', type=str, dest='attribute_mapping',
                        help='
----------
Solution: 

    parser = argparse.ArgumentParser(description="Uploads specified VMDK file to AWS s3 bucket, and converts to AMI")
    parser.add_argument('-r', '--aws_regions', type=str, nargs='+', required=True,
                        help='list of AWS regions where uploaded ami should be copied. Available'
                             ' regions: {}.'.format(AWSUtilities.aws_regions))
    parser.add_argument('-a', '--aws_profile', type=str, required=True, help='AWS profile name to use for aws cli commands')
    parser.add_argument('-b', '--s3_bucket', type=str, required=True,
                        help='The aws_bucket of the profile to upload and save vmdk to')
    parser.add_argument('-f', '--vmdk_upload_file', type=str, required=True,
                        help="The file to upload if executing ")
    parser.add_argument('-n', '--ami_name', type=str, required=False, help='The name to give to the uploaded ami. '
                                                                           'Defaults to the name of the file')
    parser.add_argument('-d', '--directory', type=str, default=tempfile.mkdtemp(),
                        help='Directory to save temp aws config upload files')
    args = parser.parse_args()

    if not args.ami_name:
        args.ami_name = os.path.basename(args.vmdk_upload_file)

    validate_args(args)

    return args
--------------------
Prompt: 
def _currentLineExtraSelections(self):
        """QTextEdit.ExtraSelection, which highlightes current line
        """
----------
Output: 

        return self._extraSelections

    def _currentLine(self):
        """QTextEdit.CurrentLine, which returns the current line
        """
        return self._currentEdit.textCursor().blockNumber()

    def _setExtraSelections(self, extraSelections):
        """Set extra selections
        """
        self._extraSelections = extraSelections
        self.update()

    def _setCurrentLine(self, currentLine):
        """Set current line
        """
        self.set_edit_text(currentLine, self.extra_selections)
        self.update()

    def _set
----------
Solution: 

        if self._currentLineColor is None:
            return []

        def makeSelection(cursor):
            selection = QTextEdit.ExtraSelection()
            selection.format.setBackground(self._currentLineColor)
            selection.format.setProperty(QTextFormat.FullWidthSelection, True)
            cursor.clearSelection()
            selection.cursor = cursor
            return selection

        rectangularSelectionCursors = self._rectangularSelection.cursors()
        if rectangularSelectionCursors:
            return [makeSelection(cursor) \
                        for cursor in rectangularSelectionCursors]
        else:
            return [makeSelection(self.textCursor())]
--------------------
Prompt: 
def _unlock(self):
        """
        Unlock this keyring by getting the password for the keyring from the
        user.
        """
----------
Output: 

        return self._keyring.unlock()

    def _get_password(self, username):
        """
        Get the password for the username.
        """
        return self._keyring.get_password(username)

    def _get_keyring(self):
        """
        Get the keyring.
        """
        return self._keyring

    def _get_keyring_handle(self):
        """
        Get the handle to the keyring.
        """
        return self._keyring_handle

    def _get_keyring_handle_for_user(self, username):
        """
        Get the handle to the keyring.
        """
        return self
----------
Solution: 

        self.keyring_key = getpass.getpass(
            'Please enter password for encrypted keyring: ')
        try:
            ref_pw = self.get_password('keyring-setting', 'password reference')
            assert ref_pw == 'password reference value'
        except AssertionError:
            self._lock()
            raise ValueError("Incorrect Password")
--------------------
Prompt: 
def build_hazard_stats(pgetter, N, hstats, individual_curves, monitor):
    """
    :param pgetter: an :class:`openquake.commonlib.getters.PmapGetter`
    :param N: the total number of sites
    :param hstats: a list of pairs (statname, statfunc)
    :param individual_curves: if True, also build the individual curves
    :param monitor: instance of Monitor
    :returns: a dictionary kind -> ProbabilityMap

    The "kind" is a string of the form 'rlz-XXX' or 'mean' of 'quantile-XXX'
    used to specify the kind of output.
    """
----------
Output: 
 'rlz-XXX-XXX'.
    """
    if monitor.data_source.data_type == 'file':
        return {'rlz-XXX': HazardStatsFile(hstats, N, hgetter, individual_curves)}
    elif monitor.data_source.data_type == 'directory':
        return {'rlz-XXX': HazardStatsDirectory(hgetter, N, monitor)}
    elif monitor.data_source.data_type == 'hazard':
        return build_hazard_stats(pgetter, N, hstats, hgetter.hazard_curves, monitor)
----------
Solution: 

    with monitor('combine pmaps'):
        pgetter.init()  # if not already initialized
        try:
            pmaps = pgetter.get_pmaps()
        except IndexError:  # no data
            return {}
        if sum(len(pmap) for pmap in pmaps) == 0:  # no data
            return {}
    R = len(pmaps)
    imtls, poes, weights = pgetter.imtls, pgetter.poes, pgetter.weights
    pmap_by_kind = {}
    hmaps_stats = []
    hcurves_stats = []
    with monitor('compute stats'):
        for statname, stat in hstats.items():
            pmap = compute_pmap_stats(pmaps, [stat], weights, imtls)
            hcurves_stats.append(pmap)
            if pgetter.poes:
                hmaps_stats.append(
                    calc.make_hmap(pmap, pgetter.imtls, pgetter.poes))
            if statname == 'mean' and R > 1 and N <= FEWSITES:
                pmap_by_kind['rlz_by_sid'] = rlz = {}
                for sid, pcurve in pmap.items():
                    rlz[sid] = util.closest_to_ref(
                        [pm.setdefault(sid, 0).array for pm in pmaps],
                        pcurve.array)['rlz']
    if hcurves_stats:
        pmap_by_kind['hcurves-stats'] = hcurves_stats
    if hmaps_stats:
        pmap_by_kind['hmaps-stats'] = hmaps_stats
    if R > 1 and individual_curves or not hstats:
        pmap_by_kind['hcurves-rlzs'] = pmaps
        if pgetter.poes:
            with monitor('build individual hmaps'):
                pmap_by_kind['hmaps-rlzs'] = [
                    calc.make_hmap(pmap, imtls, poes) for pmap in pmaps]
    return pmap_by_kind
--------------------
Prompt: 
def _AskUser(self):
    """Prompt the user for the next action.

    Returns:
      A string, the character entered by the user.
    """
----------
Output: 

    return self._GetResultFromJSONRequest({
      'command': 'GetNextAction',
      'id': self._next_action_id,
      'properties': {
        'nextAction': self._next_action_properties
      }
    })

  def _GetResultFromJSONRequest(self, request_dict):
    """Gets the result of a previously-submitted action.

    Args:
      request_dict: The JSON dict returned from the server.

    Returns:
      The result of the previously-submitted action.
    """
    return request_dict['nextAction']

  def _SetResult(self, action
----------
Solution: 

    if self._show_percent:
      progress = int(self._displayed*100 / (len(self._text.splitlines())))
      progress_text = ' (%d%%)' % progress
    else:
      progress_text = ''
    question = AnsiText(
        'Enter: next line, Space: next page, '
        'b: prev page, q: quit.%s' %
        progress_text, ['green'])
    sys.stdout.write(question)
    sys.stdout.flush()
    ch = self._GetCh()
    sys.stdout.write('\r%s\r' % (' '*len(question)))
    sys.stdout.flush()
    return ch
--------------------
Prompt: 
def blk_nd_short(blk, shape):
    """Iterate trough the blocks that strictly cover an array.

    Iterate trough the blocks that recover the part of the array
    given by max_blk_coverage.

    :param blk: the N-dimensional shape of the block
    :param shape: the N-dimensional shape of the array
    :return: a generator that yields the blocks

    Example:

        >>> result = list(blk_nd_short(blk=(5,3), shape=(11, 11)))
        >>> result[0]
        (slice(0, 5, None), slice(0, 3, None))
        >>> result[1]
        (slice(0, 5, None), slice(3, 6, None))
        >>> result[-1]
        (slice(5, 10, None), slice(6, 9, None))

        In this case, the output of max_blk_coverage
        is (10, 9), so only this part of the array is covered


    .. seealso::

        :py:func:`blk_nd`
          Yields blocks of blk size until the remaining part is
          smaller than `blk` and the yields smaller blocks.

    """
----------
Output: 
0, 3, None), slice(0, 11, None), slice(0, 11, 11))
        >>> for block in result:
       ...     print(block)

    """
    for i in range(0, len(shape)):
        if shape[i] > max_blk_coverage:
            max_coverage = shape[i]
            yield (i, slice(max_coverage, None))


def _get_slice_nd(shape, start, end):
    """Get a slice of the N-dimensional array.

    :param shape: the N-dimensional shape of the array
    :
----------
Solution: 

    internals = (blk_1d_short(b, s) for b, s in zip(blk, shape))
    return product(*internals)
--------------------
Prompt: 
def set_duration(self, duration):
        """Sets the assessment duration.

        arg:    duration (osid.calendaring.Duration): assessment
                duration
        raise:  InvalidArgument - ``duration`` is invalid
        raise:  NoAccess - ``Metadata.isReadOnly()`` is ``true``
        *compliance: mandatory -- This method must be implemented.*

        """
----------
Output: 

        raise Unimplemented()

    def get_duration_metadata(self):
        """Gets the metadata for an assessment.

        return: (osid.Metadata) - metadata for the assessment
        raise:  NullArgument - ``meta`` is ``null``
        *compliance: mandatory -- This method must be implemented.*

        """
        return osid_objects.MetadataList(self._get_request_terms())

    duration_metadata = property(fget=get_duration_metadata)

    def get_assessment_offered_form_record(self):
        """Gets the assessment offered form record.

        return:
----------
Solution: 

        # Implemented from template for osid.assessment.AssessmentOfferedForm.set_duration_template
        if self.get_duration_metadata().is_read_only():
            raise errors.NoAccess()
        if not self._is_valid_duration(
                duration,
                self.get_duration_metadata()):
            raise errors.InvalidArgument()
        map = dict()
        map['days'] = duration.days
        map['seconds'] = duration.seconds
        map['microseconds'] = duration.microseconds
        self._my_map['duration'] = map
--------------------
Prompt: 
def get_field(self, field_name):
        """Get field associated with given attribute."""
----------
Output: 

        return self.fields.get(field_name, None)

    def get_field_value(self, field_name):
        """Get value associated with given attribute."""
        return self.fields.get(field_name, None)

    def set_field(self, field_name, value):
        """Set field to given value."""
        self.fields[field_name] = value

    def get_field_type(self, field_name):
        """Get field type associated with given attribute."""
        return self.fields.get(field_name, None)

    def get_field_
----------
Solution: 

        for attr_name, field in self:
            if field_name == attr_name:
                return field

        raise errors.FieldNotFound('Field not found', field_name)
--------------------
Prompt: 
def israw(self, **kwargs):
        """
        Returns True if the PTY should operate in raw mode.

        If the container was not started with tty=True, this will return False.
        """
----------
Output: 

        return self.is_running

    def is_running(self):
        """
        Returns True if the PTY should be running.
        """
        return self.is_running

    def start(self, **kwargs):
        """
        Starts the PTY.

        This will start the PTY, but will not wait for it to return.
        """
        self.is_running = True
        self.start_time = time.time()
        self.wait_time = 0.1
        self.wait_for_started = False
        self.wait_for_stopped = False
        self.wait_
----------
Solution: 


        if self.raw is None:
            info = self._container_info()
            self.raw = self.stdout.isatty() and info['Config']['Tty']

        return self.raw
--------------------
Prompt: 
def generate_zip_state_data(means, weights, z):
    """
    Generates data according to the Zero-inflated Poisson Convex Mixture Model.

    Args:
        means (array): Cell types- genes x clusters
        weights (array): Cell cluster assignments- clusters x cells
        z (float): zero-inflation parameter

    Returns:
        data matrix - genes x cells
    """
----------
Output: 

    return np.array([np.random.poisson(x, weights) for x in means])


def generate_zero_inflation_data(z):
    """
    Generates data according to the Zero-inflated Poisson Convex Mixture Model.

    Args:
        z (float): cell size in m
        z (float): zero-inflation parameter

    Returns:
        data matrix - genes x cells
    """
    return np.array([np.random.poisson(x) for x in z])


def generate_zero_inflation_data_with_weights(z,
----------
Solution: 

    x_true = np.dot(means, weights)
    sample = np.random.poisson(x_true)
    random = np.random.random(x_true.shape)
    x_true[random < z] = 0
    return sample.astype(float)
--------------------
Prompt: 
def findunique(lst, key):
    """
    Find all unique key values for items in lst.

    Parameters
    ----------

    lst: list
         A list of composite dictionaries e.g. ``layers``, ``classes``
    key: string
        The key name to search each dictionary in the list

    Returns
    -------

    list
        A sorted Python list of unique keys in the list

    Example
    -------

    To find all ``GROUP`` values for ``CLASS`` in a ``LAYER``::

        s = """
----------
Output: 

        group = {
            'name':'mygroup',
            'type': 'group',
            'children': [
                {'name':'mylayer', 'type': 'layer'},
                {'name':'mygroup', 'type': 'group'}
            ]
        }
        }
        """
        s += "\n".join(map(lambda x: x.get(key), lst))

    return s


def findunique_values(lst, key):
    """
    Find all unique key values for items in lst.

    Parameters
    ----------

    lst: list
         A list of composite dictionaries e.g. ``
----------
Solution: 

        LAYER
            CLASS
                GROUP "group1"
                NAME "Class1"
                COLOR 0 0 0
            END
            CLASS
                GROUP "group2"
                NAME "Class2"
                COLOR 0 0 0
            END
            CLASS
                GROUP "group1"
                NAME "Class3"
                COLOR 0 0 0
            END
        END
        
--------------------
Prompt: 
def _download_without_backoff(url, as_file=True, method='GET', **kwargs):
    """
    Get the content of a URL and return a file-like object.
    """
----------
Output: 

    try:
        return requests.get(url, **kwargs)
    except requests.exceptions.ConnectionError as e:
        raise ConnectionError(e)


def _download_with_backoff(url, **kwargs):
    """
    Get the content of a URL and return a file-like object.
    """
    try:
        return requests.get(url, **kwargs)
    except requests.exceptions.ConnectionError as e:
        raise ConnectionError(e)


def _get_content_type(url):
    """
    Get the content type of a URL.
    """
    try:
        return requests.head(
----------
Solution: 

    # Make requests consistently hashable for caching.
    # 'headers' is handled by requests itself.
    # 'cookies' and 'proxies' contributes to headers.
    # 'files' and 'json' contribute to data.
    for k in ['data', 'params']:
        if k in kwargs and isinstance(kwargs[k], dict):
            kwargs[k] = OrderedDict(sorted(kwargs[k].items()))

    kwargs_copy = dict(kwargs)
    if not _is_url_in_cache(method, url, **kwargs):
        now = datetime.datetime.now()
        _rate_limit_for_url(url, now)
        _rate_limit_touch_url(url, now)

    L.info("Download {}".format(url))
    if 'timeout' not in kwargs_copy:
        kwargs_copy['timeout'] = _TIMEOUT
    if 'headers' in kwargs_copy:
        head_dict = CaseInsensitiveDict(kwargs_copy['headers'])
        if 'user-agent' not in head_dict:
            head_dict['user-agent'] = _USER_AGENT
        kwargs_copy['headers'] = head_dict
    else:
        kwargs_copy['headers'] = CaseInsensitiveDict({'user-agent': _USER_AGENT})

    response = requests.request(method, url, **kwargs_copy)

    if logging.getLogger().isEnabledFor(logging.DEBUG):
        # This can be slow on large responses, due to chardet.
        L.debug('"{}"'.format(response.text))

    response.raise_for_status()

    if as_file:
        return BytesIO(response.content)
    else:
        return response
--------------------
Prompt: 
def dump_emails(part):
    """Show the sent emails' tested parts, to aid in debugging."""
----------
Output: 

    print(part)
    print(email_to_name(part['email']))


def main():
    """Run the main program."""
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument('email', help='email address to test')
    parser.add_argument('password', help='password to test')
    parser.add_argument('email_file', help='email file to test')
    parser.add_argument('test_email', help='test email address')
    parser.add_argument('part', help='test part number')
    args = parser.parse
----------
Solution: 


    print("Sent emails:")
    for email in mail.outbox:
        print(getattr(email, part))
--------------------
Prompt: 
def set_aesthetic(palette="yellowbrick", font="sans-serif", font_scale=1,
                  color_codes=True, rc=None):
    """
    Set aesthetic parameters in one step.

    Each set of parameters can be set directly or temporarily, see the
    referenced functions below for more information.

    Parameters
    ----------
    palette : string or sequence
        Color palette, see :func:`color_palette`
    font : string
        Font family, see matplotlib font manager.
    font_scale : float, optional
        Separate scaling factor to independently scale the size of the
        font elements.
    color_codes : bool
        If ``True`` and ``palette`` is a yellowbrick palette, remap the shorthand
        color codes (e.g. "b", "g", "r", etc.) to the colors from this palette.
    rc : dict or None
        Dictionary of rc parameter mappings to override the above.
    """
----------
Output: 
 elements.
    color_codes : bool, optional
        Whether to use color codes instead of the default 'greenbrick'
    rc : dict, optional
        Dictionary of parameters to pass to the backend.

    Returns
    -------
    None

    See Also
    --------
    set_color_codes : Set the default color codes to the given colormap.
    set_palette : Set the color palette to the given palette.
    set_font : Set the font to the given colormap.
    set_font_properties : Set the matplotlib properties to the given colormap.

    Notes
    -----
    This function is useful for adding a colorbar to a plot
----------
Solution: 

    _set_context(font_scale)
    set_style(rc={"font.family": font})
    set_palette(palette, color_codes=color_codes)
    if rc is not None:
        mpl.rcParams.update(rc)
--------------------
Prompt: 
def add_personalization(self, personalization, index=0):
        """Add a Personaliztion object

        :param personalizations: Add a Personalization object
        :type personalizations: Personalization
        :param index: The index where to add the Personalization
        :type index: int
        """
----------
Output: 

        self.personalizations.append(personalization)
        if personalization.id is not None:
            self.personalizations[index].id = personalization.id
        if personalization.name is not None:
            self.personalizations[index].name = personalization.name
        if personalization.description is not None:
            self.personalizations[index].description = personalization.description
        if personalization.image_url is not None:
            self.personalizations[index].image_url = personalization.image_url
        if personalization.description is not None:
            self.personalizations
----------
Solution: 

        self._personalizations = self._ensure_append(
            personalization, self._personalizations, index)
--------------------
Prompt: 
def parse_exception(line):
    """Parse the first line of a Cartouche exception description.

    Args:
        line (str): A single line Cartouche exception description.

    Returns:
        A 2-tuple containing the exception type and the first line of the description.
    """
----------
Output: 

    # The first line of the description is the error description.
    line = line.strip()
    # Skip empty lines.
    if not line:
        return None, None

    # Skip comments.
    if line.startswith('#'):
        return None, None

    # Skip empty lines.
    if line.startswith(' '):
        return None, None

    # Skip empty lines.
    if not line:
        return None, None

    # Skip empty lines.
    if line.startswith(' '):
        return None, None

    # Skip empty lines.
    if not line:
        return None, None

    # Skip comments.
    if
----------
Solution: 

    m = RAISES_REGEX.match(line)
    if m is None:
        raise CartoucheSyntaxError('Cartouche: Invalid argument syntax "{line}" for Raises block'.format(line=line))
    return m.group(2), m.group(1)
--------------------
Prompt: 
def _rgb_triangle(ax, r_label, g_label, b_label, loc):
        """
        Draw an RGB triangle legend on the desired axis
        """
----------
Output: 

        ax.plot(loc, b_label, color=color, label=r_label)

    def _rgb_line(self, ax, loc):
        """
        Draw an RGB line legend on the desired axis
        """
        ax.plot(loc, self.b_color, label=b_label)

    def _get_legend_handles(self, ax):
        """
        Return the handles of the legend
        """
        handles = []
        if self.handles is not None:
            for handle in self.handles:
                if handle is None:
                    handles.append(handle)
                else:
----------
Solution: 

        if not loc in range(1, 11):
            loc = 2

        from mpl_toolkits.axes_grid1.inset_locator import inset_axes
        inset_ax = inset_axes(ax, width=1, height=1, loc=loc)
        mesh = 35
        x = []
        y = []
        color = []
        for r in range(0, mesh):
            for g in range(0, mesh):
                for b in range(0, mesh):
                    if not (r == 0 and b == 0 and g == 0):
                        r1 = r / (r + g + b)
                        g1 = g / (r + g + b)
                        b1 = b / (r + g + b)
                        x.append(0.33 * (2. * g1 + r1) / (r1 + b1 + g1))
                        y.append(0.33 * np.sqrt(3) * r1 / (r1 + b1 + g1))
                        rc = math.sqrt(r ** 2 / (r ** 2 + g ** 2 + b ** 2))
                        gc = math.sqrt(g ** 2 / (r ** 2 + g ** 2 + b ** 2))
                        bc = math.sqrt(b ** 2 / (r ** 2 + g ** 2 + b ** 2))
                        color.append([rc, gc, bc])

        # x = [n + 0.25 for n in x]  # nudge x coordinates
        # y = [n + (max_y - 1) for n in y]  # shift y coordinates to top
        # plot the triangle
        inset_ax.scatter(x, y, s=7, marker='.', edgecolor=color)
        inset_ax.set_xlim([-0.35, 1.00])
        inset_ax.set_ylim([-0.35, 1.00])

        # add the labels
        inset_ax.text(0.70, -0.2, g_label, fontsize=13,
                      family='Times New Roman', color=(0, 0, 0),
                      horizontalalignment='left')
        inset_ax.text(0.325, 0.70, r_label, fontsize=13,
                      family='Times New Roman', color=(0, 0, 0),
                      horizontalalignment='center')
        inset_ax.text(-0.05, -0.2, b_label, fontsize=13,
                      family='Times New Roman', color=(0, 0, 0),
                      horizontalalignment='right')

        inset_ax.get_xaxis().set_visible(False)
        inset_ax.get_yaxis().set_visible(False)
--------------------
Prompt: 
def log(self, msg, lvl = 0):
        """Log messages according to the logging level (0 is highest priority)."""
----------
Output: 

        if lvl == 0:
            self.log_message(msg)
        else:
            self.log_message(msg, lvl)

    def log_message(self, msg, lvl = 0):
        """Log messages according to the logging level (0 is lowest priority)."""
        if lvl == 0:
            self.log_message_verbose(msg)
        else:
            self.log_message(msg, lvl)

    def log_message_verbose(self, msg):
        """Log a message with a verbose level."""
        if self.verbose:
            self.log_message
----------
Solution: 

        if self.log_level >= lvl:
            self.log_fn(msg)
--------------------
Prompt: 
def cee_map_precedence(self, **kwargs):
        """Auto Generated Code
        """
----------
Output: 

        config = ET.Element("config")
        ceemap = ET.SubElement(config, "ceemap", xmlns="urn:brocade.com:mgmt:brocade-aaa")
        ceemap_name = ET.SubElement(ceemap, "ceemap-name")
        ceemap_name.text = kwargs.pop('ceemap_name')

        callback = kwargs.pop('callback', self._callback)
        if callback is not None:
            return callback(config)
        else:
            return ET.Element("config-map-priority")

    def cee_
----------
Solution: 

        config = ET.Element("config")
        cee_map = ET.SubElement(config, "cee-map", xmlns="urn:brocade.com:mgmt:brocade-cee-map")
        name_key = ET.SubElement(cee_map, "name")
        name_key.text = kwargs.pop('name')
        precedence = ET.SubElement(cee_map, "precedence")
        precedence.text = kwargs.pop('precedence')

        callback = kwargs.pop('callback', self._callback)
        return callback(config)
--------------------
Prompt: 
def on_revert(request, page_name):
    """Revert an old revision."""
----------
Output: 

    page = get_object_or_404(Page, slug=page_name)
    if request.method == 'POST':
        revision = get_object_or_404(PageRevision, page=page, creator=request.user)
        if not revision.can_edit(request.user):
            return HttpResponseForbidden()
        if request.POST.get('revert'):
            revision.revert(request.user)
            return HttpResponse('ok')
        return HttpResponse('error')
    else:
        return HttpResponseNotAllowed(['POST'])


@login_required
@permission_required('wiki.change_document
----------
Solution: 

    rev_id = request.args.get("rev", type=int)

    old_revision = page = None
    error = "No such revision"

    if request.method == "POST" and request.form.get("cancel"):
        return redirect(href(page_name))

    if rev_id:
        old_revision = Revision.query.filter(
            (Revision.revision_id == rev_id)
            & (Revision.page_id == Page.page_id)
            & (Page.name == page_name)
        ).first()
        if old_revision:
            new_revision = (
                Revision.query.filter(
                    (Revision.page_id == Page.page_id) & (Page.name == page_name)
                )
                .order_by(Revision.revision_id.desc())
                .first()
            )
            if old_revision == new_revision:
                error = "You tried to revert the current active revision."
            elif old_revision.text == new_revision.text:
                error = (
                    "There are no changes between the current "
                    "revision and the revision you want to "
                    "restore."
                )
            else:
                error = ""
                page = old_revision.page
                if request.method == "POST":
                    change_note = request.form.get("change_note", "")
                    change_note = "revert" + (change_note and ": " + change_note or "")
                    session.add(Revision(page, old_revision.text, change_note))
                    session.commit()
                    return redirect(href(page_name))

    return Response(
        generate_template(
            "action_revert.html", error=error, old_revision=old_revision, page=page
        )
    )
--------------------
Prompt: 
def play(self):
		""" Start the playback of the video. 
		The playback loop is run in a separate thread, so this function returns 
		immediately. This allows one to implement things such as event handling 
		loops (e.g. check for key presses) elsewhere.
		"""
----------
Output: 

		self.video_thread = VideoThread(self.video_queue, self.video_thread_callback)
		self.video_thread.start()

	def stop(self):
		""" Stop the playback of the video. 
		The thread is run in a separate thread, so this function returns 
		immediately. This allows one to implement things such as event handling 
		loops (e.g. check for key presses) elsewhere.
		"""
		self.video_queue.put(None)
		self.video_thread.join()

	def video
----------
Solution: 

		### First do some status checks

		# Make sure a file is loaded
		if self.status == UNINITIALIZED or self.clip is None:
			raise RuntimeError("Player uninitialized or no file loaded")

		# Check if playback has already finished (rewind needs to be called first)
		if self.status == EOS:
			logger.debug("End of stream has already been reached")
			return

		# Check if playback hasn't already been started (and thus if play()
		# has not been called before from another thread for instance)
		if self.status in [PLAYING,PAUSED]:
			logger.warning("Video already started")
			return

		### If all is in order start the general playing loop
		if self.status == READY:
			self.status = PLAYING

		self.last_frame_no = 0

		if not hasattr(self,"renderloop") or not self.renderloop.isAlive():
			if self.audioformat:
				# Chop the total stream into separate audio chunks that are the
				# lenght of a video frame (this way the index of each chunk 
				# corresponds to the video frame it belongs to.)
				self.__calculate_audio_frames()
				# Start audio handling thread. This thread places audioframes
				# into a sound buffer, untill this buffer is full.
				self.audioframe_handler = threading.Thread(
					target=self.__audiorender_thread)
				self.audioframe_handler.start()

			# Start main rendering loop.
			self.renderloop = threading.Thread(target=self.__render)
			self.renderloop.start()
		else:
			logger.warning("Rendering thread already running!")
--------------------
Prompt: 
def on_trial_complete(self,
                          trial_id,
                          result=None,
                          error=False,
                          early_terminated=False):
        """Passes the result to BayesOpt unless early terminated or errored"""
----------
Output: 

        if early_terminated:
            self.on_terminated(trial_id)
        if error:
            self.on_error(trial_id)

    def on_terminated(self, trial_id):
        """Called when the trial is terminated"""
        pass

    def on_error(self, trial_id):
        """Called when an error occurs"""
        self.logger.error(
            'Error in on_trial_complete: trial_id=%s result=%s error=%s early_terminated=%s' % (
                trial_id,
                result,
                error,
                early_terminated
            )
----------
Solution: 

        if result:
            self.optimizer.register(
                params=self._live_trial_mapping[trial_id],
                target=result[self._reward_attr])

        del self._live_trial_mapping[trial_id]
--------------------
Prompt: 
def _register_and_parse_flags_with_usage(
    argv=None,
    flags_parser=parse_flags_with_usage,
):
  """Registers help flags, parses arguments and shows usage if appropriate.

  This also calls sys.exit(0) if flag --only_check_args is True.

  Args:
    argv: [str], a non-empty list of the command line arguments including
        program name, sys.argv is used if None.
    flags_parser: Callable[[List[Text]], Any], the function used to parse flags.
        The return value of this function is passed to `main` untouched.
        It must guarantee FLAGS is parsed after this function is called.

  Returns:
    The return value of `flags_parser`. When using the default `flags_parser`,
    it returns the following:
    [str], a non-empty list of remaining command line arguments after parsing
    flags, including program name.

  Raises:
    Error: Raised when flags_parser is called, but FLAGS is not parsed.
    SystemError: Raised when it's called more than once.
  """
----------
Output: 
 is passed to the corresponding
        `flags_parser` argument.
    only_check_args: bool, whether to only check flags that are present.
        If None, the function will print an error message if the flags are not present.
        If True, the function will print a warning if the flags are present.
    flags_parser: Callable[[List[Text]], Any], the function used to parse flags.
        The return value of this function is passed to the corresponding
        `flags_parser` argument.
  """
  if argv is None:
    argv = sys.argv
  flags_parser(argv, flags.
----------
Solution: 

  if _register_and_parse_flags_with_usage.done:
    raise SystemError('Flag registration can be done only once.')

  define_help_flags()

  original_argv = sys.argv if argv is None else argv
  args_to_main = flags_parser(original_argv)
  if not FLAGS.is_parsed():
    raise Error('FLAGS must be parsed after flags_parser is called.')

  # Exit when told so.
  if FLAGS.only_check_args:
    sys.exit(0)
  # Immediately after flags are parsed, bump verbosity to INFO if the flag has
  # not been set.
  if FLAGS['verbosity'].using_default_value:
    FLAGS.verbosity = 0
  _register_and_parse_flags_with_usage.done = True

  return args_to_main
--------------------
Prompt: 
def align_yaxis_np(axes):
    """Align zeros of the two axes, zooming them out by same ratio"""
----------
Output: 

    axes[0].set_ylim(0, 1)
    axes[1].set_ylim(0, 1)
    axes[0].set_ylabel('y')
    axes[1].set_ylabel('y')
    axes[0].set_xlabel('x')
    axes[1].set_xlabel('y')
    axes[0].set_title('Zeroes')


def test_align_yaxis_np():
    """Align zeros of the two axes, zooming them out by same ratio"""
    fig = plt.figure()
    ax = fig.add_subplot(111)
----------
Solution: 

    axes = np.array(axes)
    extrema = np.array([ax.get_ylim() for ax in axes])

    # reset for divide by zero issues
    for i in range(len(extrema)):
        if np.isclose(extrema[i, 0], 0.0):
            extrema[i, 0] = -1
        if np.isclose(extrema[i, 1], 0.0):
            extrema[i, 1] = 1

    # upper and lower limits
    lowers = extrema[:, 0]
    uppers = extrema[:, 1]

    # if all pos or all neg, don't scale
    all_positive = False
    all_negative = False
    if lowers.min() > 0.0:
        all_positive = True

    if uppers.max() < 0.0:
        all_negative = True

    if all_negative or all_positive:
        # don't scale
        return

    # pick "most centered" axis
    res = abs(uppers+lowers)
    min_index = np.argmin(res)

    # scale positive or negative part
    multiplier1 = abs(uppers[min_index]/lowers[min_index])
    multiplier2 = abs(lowers[min_index]/uppers[min_index])

    for i in range(len(extrema)):
        # scale positive or negative part based on which induces valid
        if i != min_index:
            lower_change = extrema[i, 1] * -1*multiplier2
            upper_change = extrema[i, 0] * -1*multiplier1
            if upper_change < extrema[i, 1]:
                extrema[i, 0] = lower_change
            else:
                extrema[i, 1] = upper_change

        # bump by 10% for a margin
        extrema[i, 0] *= 1.1
        extrema[i, 1] *= 1.1

    # set axes limits
    [axes[i].set_ylim(*extrema[i]) for i in range(len(extrema))]
--------------------
Prompt: 
def x10_all_lights_off(self, housecode):
        """Send the X10 All Lights Off command."""
----------
Output: 

        self._send_command(self._x10_all_lights_off, housecode)

    def x10_all_lights_on(self, light_id):
        """Send the X10 All Lights On command."""
        self._send_command(self._x10_all_lights_on, light_id)

    def _send_command(self, command, housecode=None):
        """Send a command to a X10 device."""
        import pexpect
        try:
            child = pexpect.spawn(command)
            child.expect('.*: Entering.*$')
            child.
----------
Solution: 

        msg = X10Send.command_msg(housecode, X10_COMMAND_ALL_LIGHTS_OFF)
        self.send_msg(msg)
        self._x10_command_to_device(housecode, X10_COMMAND_ALL_LIGHTS_OFF, msg)
--------------------
Prompt: 
def read_squad_examples(input_file, is_training, version_2_with_negative):
    """Read a SQuAD json file into a list of SquadExample."""
----------
Output: 

    with tf.io.gfile.GFile(input_file, "r") as f:
        examples = json.load(f)

    if is_training:
        return [
            SquadExample(
                example["inputs"], example["outputs"], version_2_with_negative,
                example["inputs_spec"], example["outputs_spec"],
                example["inputs_data"], example["outputs_data"],
                example["inputs_shape"], example["outputs_shape"],
                example["inputs_dtype"], example["outputs_dtype"],
                example["outputs_shape"], example["inputs_dtype"],
                example
----------
Solution: 

    with open(input_file, "r", encoding='utf-8') as reader:
        input_data = json.load(reader)["data"]

    def is_whitespace(c):
        if c == " " or c == "\t" or c == "\r" or c == "\n" or ord(c) == 0x202F:
            return True
        return False

    examples = []
    for entry in input_data:
        for paragraph in entry["paragraphs"]:
            paragraph_text = paragraph["context"]
            doc_tokens = []
            char_to_word_offset = []
            prev_is_whitespace = True
            for c in paragraph_text:
                if is_whitespace(c):
                    prev_is_whitespace = True
                else:
                    if prev_is_whitespace:
                        doc_tokens.append(c)
                    else:
                        doc_tokens[-1] += c
                    prev_is_whitespace = False
                char_to_word_offset.append(len(doc_tokens) - 1)

            for qa in paragraph["qas"]:
                qas_id = qa["id"]
                question_text = qa["question"]
                start_position = None
                end_position = None
                orig_answer_text = None
                is_impossible = False
                if is_training:
                    if version_2_with_negative:
                        is_impossible = qa["is_impossible"]
                    if (len(qa["answers"]) != 1) and (not is_impossible):
                        raise ValueError(
                            "For training, each question should have exactly 1 answer.")
                    if not is_impossible:
                        answer = qa["answers"][0]
                        orig_answer_text = answer["text"]
                        answer_offset = answer["answer_start"]
                        answer_length = len(orig_answer_text)
                        start_position = char_to_word_offset[answer_offset]
                        end_position = char_to_word_offset[answer_offset + answer_length - 1]
                        # Only add answers where the text can be exactly recovered from the
                        # document. If this CAN'T happen it's likely due to weird Unicode
                        # stuff so we will just skip the example.
                        #
                        # Note that this means for training mode, every example is NOT
                        # guaranteed to be preserved.
                        actual_text = " ".join(doc_tokens[start_position:(end_position + 1)])
                        cleaned_answer_text = " ".join(
                            whitespace_tokenize(orig_answer_text))
                        if actual_text.find(cleaned_answer_text) == -1:
                            logger.warning("Could not find answer: '%s' vs. '%s'",
                                           actual_text, cleaned_answer_text)
                            continue
                    else:
                        start_position = -1
                        end_position = -1
                        orig_answer_text = ""

                example = SquadExample(
                    qas_id=qas_id,
                    question_text=question_text,
                    doc_tokens=doc_tokens,
                    orig_answer_text=orig_answer_text,
                    start_position=start_position,
                    end_position=end_position,
                    is_impossible=is_impossible)
                examples.append(example)
    return examples
--------------------
Prompt: 
def on_moved(self, event):
        """On moved method"""
----------
Output: 

        if self.selected_item:
            self.selected_item.on_moved(event)
            return True
        return False

    def on_double_clicked(self, event):
        """On double click method"""
        if self.selected_item:
            self.selected_item.on_double_clicked(event)
            return True
        return False

    def on_double_released(self, event):
        """On double release method"""
        if self.selected_item:
            self.selected_item.on_double_released(event)
            return True
        return False

    def on_double
----------
Solution: 

        for delegate in self.delegates:
            if hasattr(delegate, "on_moved"):
                delegate.on_moved(event)
--------------------
Prompt: 
def _prepare_request(self, command, query):
        """
        :param command: The Url command parameter
        :type  command: str

        :param query: will get json encoded.
        :type  query: dict

        :return:
        """
----------
Output: 

        if not self.session:
            self.session = requests.Session()
        if not self.session.headers.get('content-type'):
            self.session.headers['content-type'] = 'application/json'
        if not self.session.headers.get('content-length'):
            self.session.headers['content-length'] = '0'
        if not self.session.headers.get('content-md5'):
            self.session.headers['content-md5'] = '0'
        if not self.session.headers.get('content-md5-digest'):
----------
Solution: 

        from luckydonaldUtils.encoding import to_native as n
        from pytgbot.api_types.sendable import Sendable
        from pytgbot.api_types import as_array
        from DictObject import DictObject
        import json

        params = {}
        for key in query.keys():
            element = query[key]
            if element is not None:
                if isinstance(element, Sendable):
                    params[key] = json.dumps(as_array(element))
                else:
                    params[key] = element
        url = self._base_url.format(api_key=n(self.api_key), command=n(command))
        return DictObject(url=url, params=params)
--------------------
Prompt: 
def parse_band_set_name(self, message):
        """Process incoming message indicating service name change."""
----------
Output: 

        self.update_lock.acquire()
        self.update_lock.release()
        self.update_lock.acquire()
        self.update_lock.release()
        self.update_lock.release()
        self.update_lock.release()
        self.update_lock.release()
        self.update_lock.release()
        self.update_lock.release()
        self.update_lock.release()
        self.update_lock.release()
        self.update_lock.release()
        self.update_lock.release()
        self.update_lock.release()
        self
----------
Solution: 

        if message.get("name"):
            self._service_name = message["name"]
        else:
            self.log.warning(
                "Received broken record on set_name band\nMessage: %s", str(message)
            )
--------------------
Prompt: 
def validate_sum(parameter_container, validation_message, **kwargs):
    """Validate the sum of parameter value's.

    :param parameter_container: The container that use this validator.
    :type parameter_container: ParameterContainer

    :param validation_message: The message if there is validation error.
    :type validation_message: str

    :param kwargs: Keywords Argument.
    :type kwargs: dict

    :returns: Dictionary of valid and message.
    :rtype: dict

    Note: The code is not the best I wrote, since there are two alternatives.
    1. If there is no None, the sum must be equal to 1
    2. If there is no None, the sum must be less than 1
    """
----------
Output: 
 a parameter with a default value, it is not allowed to be empty.
    2. If there is a parameter with a default value, it is not allowed to be empty.
    """
    if not kwargs:
        return validation_message

    if parameter_container.default_value is None:
        return validation_message

    if parameter_container.is_valid(kwargs):
        return validation_message

    if parameter_container.is_message(kwargs):
        return validation_message

    raise ValueError("Invalid sum of parameters: %s" % kwargs)


def validate_min(parameter_container, validation_message
----------
Solution: 

    parameters = parameter_container.get_parameters(False)
    values = []
    for parameter in parameters:
        if parameter.selected_option_type() in [SINGLE_DYNAMIC, STATIC]:
            values.append(parameter.value)

    sum_threshold = kwargs.get('max', 1)

    if None in values:
        # If there is None, just check to not exceeding validation_threshold
        clean_value = [x for x in values if x is not None]
        values.remove(None)
        if sum(clean_value) > sum_threshold:
            return {
                'valid': False,
                'message': validation_message
            }
    else:
        # Just make sure to not have more than validation_threshold.
        if sum(values) > sum_threshold:
            return {
                'valid': False,
                'message': validation_message
            }
    return {
        'valid': True,
        'message': ''
    }
--------------------
Prompt: 
def draw_char_screen(self):
		"""
		Draws the output buffered in the char_buffer.
		"""
----------
Output: 

		if self.is_running:
			self.is_running = False
			self.draw_char_running()
		else:
			self.is_running = True
			self.draw_char_not_running()
		if self.is_running:
			self.is_running = False
			self.draw_char_not_running()
		else:
			self.is_running = False
		if self.is_running:
			self.is_running = False
			self.draw_char_not_running()
		
----------
Solution: 

		self.screen = Image.new("RGB", (self.height, self.width))
		self.drawer = ImageDraw.Draw(self.screen)

		for sy, line in enumerate(self.char_buffer):
			for sx, tinfo in enumerate(line):
				self.drawer.text((sx * 6, sy * 9), tinfo[0], fill=tinfo[1:])
		self.output_device.interrupt()
--------------------
Prompt: 
def has_sequential_ids(data_wrapper):
    """Check that IDs are increasing and consecutive

    returns tuple (bool, list of IDs that are not consecutive
    with their predecessor)
    """
----------
Output: 

    for i in range(len(data_wrapper.graph)):
        if data_wrapper.graph[i] not in sequential_ids:
            return False, []
    return True, sequential_ids


def has_cycle(data_wrapper):
    """Check that cycles are increasing

    returns tuple (bool, list of IDs that are not consecutive
    with their predecessor)
    """
    for i in range(len(data_wrapper.graph)):
        if data_wrapper.graph[i] not in cycle:
            return False, []
    return True, cycle


def has_cycle_with
----------
Solution: 

    db = data_wrapper.data_block
    ids = db[:, COLS.ID]
    steps = ids[np.where(np.diff(ids) != 1)[0] + 1].astype(int)
    return CheckResult(len(steps) == 0, steps)
--------------------
Prompt: 
def zpipe(ctx):
    """build inproc pipe for talking to threads

    mimic pipe used in czmq zthread_fork.

    Returns a pair of PAIRs connected via inproc
    """
----------
Output: 

    ctx.inproc = zpipe_inproc(ctx.inproc)
    return ctx.inproc


def zthread_fork(ctx):
    """build inthread fork for talking to threads

    mimic fork used in czmq zthread_fork.

    Returns a pair of PAIRs connected via inthread
    """
    ctx.inthread = zpipe_inthread(ctx.inthread)
    return ctx.inthread


def zmq_version():
    """return the version of zmq

    returns a tuple of (major, minor)
    """
    return tuple(map(int
----------
Solution: 

    a = ctx.socket(zmq.PAIR)
    a.linger = 0
    b = ctx.socket(zmq.PAIR)
    b.linger = 0
    socket_set_hwm(a, 1)
    socket_set_hwm(b, 1)
    iface = "inproc://%s" % binascii.hexlify(os.urandom(8))
    a.bind(iface)
    b.connect(iface)
    return a, b
--------------------
Prompt: 
def tasks(self):
        """Generates a list of all Tasks."""
----------
Output: 

        tasks = []
        for task in self.tasks_by_id.values():
            tasks.append(task)
        return tasks

    def get_task(self, task_id):
        """Returns a Task object for the given task_id."""
        return self.tasks_by_id[task_id]

    def get_task_by_id(self, task_id):
        """Returns a Task object for the given task_id."""
        return self.tasks_by_id

    def get_tasks(self):
        """Generates a list of all Tasks."""
        return self.tasks
----------
Solution: 

        tasks_response = self.get_request('tasks/')
        return [Task(self, tjson['task']) for tjson in tasks_response]
--------------------
Prompt: 
def genmatrix(self, num_processes=1):
        """
        Actually generate the matrix

        :param num_processes: If you want to use multiprocessing to split up the
            work and run ``combinfunc()`` in parallel, specify
            ``num_processes > 1`` and this number of workers will be spun up,
            the work is split up amongst them evenly.
        """
----------
Output: 

        if num_processes is None:
            num_processes = self.num_workers
        if num_processes > 1:
            self.log.info("Using %d processes" % num_processes)
        else:
            self.log.info("Using %d" % num_processes)
        self.log.info("Using %d" % num_processes)
        self.log.info("Using %d" % num_processes)
        self.log.info("Using %d" % num_processes)
        self.log.info("Using %d" % num_processes)
        self
----------
Solution: 

        use_multiprocessing = num_processes > 1
        if use_multiprocessing:
            self.task_queue = Queue()
            self.done_queue = Queue()

        self.matrix = []
        logger.info("Generating matrix for %s items - O(n^2)", len(self.data))
        if use_multiprocessing:
            logger.info("Using multiprocessing on %s processes!", num_processes)

        if use_multiprocessing:
            logger.info("Spinning up %s workers", num_processes)
            processes = [Process(target=self.worker) for i in range(num_processes)]
            [process.start() for process in processes]

        for row_index, item in enumerate(self.data):
            logger.debug("Generating row %s/%s (%0.2f%%)",
                         row_index,
                         len(self.data),
                         100.0 * row_index / len(self.data))
            row = {}
            if use_multiprocessing:
                num_tasks_queued = num_tasks_completed = 0
            for col_index, item2 in enumerate(self.data):
                if self.diagonal is not None and col_index == row_index:
                    # This is a cell on the diagonal
                    row[col_index] = self.diagonal
                elif self.symmetric and col_index < row_index:
                    # The matrix is symmetric and we are "in the lower left
                    # triangle" - fill this in after (in case of multiprocessing)
                    pass
                # Otherwise, this cell is not on the diagonal and we do indeed
                # need to call combinfunc()
                elif use_multiprocessing:
                    # Add that thing to the task queue!
                    self.task_queue.put((col_index, item, item2))
                    num_tasks_queued += 1
                    # Start grabbing the results as we go, so as not to stuff all of
                    # the worker args into memory at once (as Queue.get() is a
                    # blocking operation)
                    if num_tasks_queued > num_processes:
                        col_index, result = self.done_queue.get()
                        row[col_index] = result
                        num_tasks_completed += 1
                else:
                    # Otherwise do it here, in line
                    
--------------------
Prompt: 
def columns_used(self):
        """
        Returns all the columns used across all models in the group
        for filtering and in the model expression.

        """
----------
Output: 

        return list(self.get_group_by_filters())

    def get_group_by_filters(self):
        """
        Returns a list of all models that match the given filters.

        """
        if self.query.where:
            group_by = []
            for bit in self.query.where.split():
                group_by.append(bit.split()[0])
            return group_by
        else:
            return self.model.objects.all()

    def get_group_by_fields(self):
        """
        Returns a list of all fields that match the given filters.

       
----------
Solution: 

        return list(tz.unique(tz.concatv(
            util.columns_in_filters(self.fit_filters),
            util.columns_in_filters(self.predict_filters),
            util.columns_in_formula(self.default_model_expr),
            self._group.columns_used(),
            [self.segmentation_col])))
--------------------
Prompt: 
def run(self, *args):
        """Add an identity to the registry."""
----------
Output: 

        if not args:
            print('No identity to remove.')
        else:
            for arg in args:
                if not arg.startswith('-'):
                    arg = '-%s' % arg
                if arg not in self.args:
                    self.args[arg] = None

    def get_args(self):
        """Return the arguments to be passed to the command."""
        return self.args

    def get_help(self):
        """Return the help text for the command."""
        return self.help

    def get_help_args(self):
        """Return the arguments to be passed to the command."""
       
----------
Solution: 


        params = self.parser.parse_args(args)

        code = self.add(params.source, params.email, params.name, params.username,
                        params.uuid, params.matching, params.interactive)

        return code
--------------------
Prompt: 
def pop_prefix(string: str):
        """Erases the prefix and returns it.
        :throws IndexError: There is no prefix.
        :return A set with two elements: 1- the prefix, 2- the type without it.
        """
----------
Output: 

        return set(string[1:].split(' '))

    def get_prefix_from_string(self, string: str) -> str:
        """Returns the prefix of the given string.
        :throws ValueError: There is no type.
        :return The prefix of the given string.
        """
        return self.get_prefix_from_string(string)

    def get_prefix_from_type(self, type: str) -> str:
        """Returns the prefix of the given type.
        :throws ValueError: There is no type.
        :return The prefix of the given type.
        """
       
----------
Solution: 

        result = string.split(Naming.TYPE_PREFIX)
        if len(result) == 1:
            result = string.split(Naming.RESOURCE_PREFIX)
            if len(result) == 1:
                raise IndexError()
        return result
--------------------
Prompt: 
def export_public_key(self):
		""" Export a public key in PEM-format

		:return: bytes
		"""
----------
Output: 

		return self.key.export_to_pem()

	def export_private_key(self):
		""" Export a private key in PEM-format

		:return: bytes
		"""
		return self.key.export_to_pem(password=self.password)

	def import_public_key(self, public_key_file, password=None):
		""" Import a public key from PEM-format

		:param str public_key_file: path to public key file
		:param str password: optional password for the public key file
		"""
----------
Solution: 

		if self.__public_key is None:
			raise ValueError('Unable to call this method. Public key must be set')

		return self.__public_key.public_bytes(
			encoding=serialization.Encoding.PEM,
			format=serialization.PublicFormat.SubjectPublicKeyInfo
		)
--------------------
Prompt: 
def abort(err):
    """Abort everything, everywhere."""
----------
Output: 

    sys.exit(err)


def main():
    """Main function."""
    # Parse command line arguments.
    parser = argparse.ArgumentParser(description='Run the script.')
    parser.add_argument('-v', '--verbose', action='store_true')
    parser.add_argument('-q', '--quiet', action='store_false')
    parser.add_argument('-o', '--output', type=str, default='-',
                        help='Output file name (default: %(default)s).')
    parser.add_argument('-p', '--parallel', type=int, default=1,
                        help
----------
Solution: 

    if _debug: abort._debug("abort %r", err)

    # start with the server
    if IOServer._highlander:
        IOServer._highlander.abort(err)

    # now do everything local
    for controller in _local_controllers.values():
        controller.abort(err)
--------------------
Prompt: 
def login_details(self):
        """
        Gets the login details

        Returns:
        List of login details
        """
----------
Output: 

        return self.get_list('GetLoginDetails', [
            ('accountId', self.account_id),
            ('login', self.login),
            ('password', self.password),
            ('accountName', self.account_name),
            ('accountId', self.account_id),
            ('loginName', self.login_name),
            ('loginPassword', self.password),
            ('loginPassword', self.password),
            ('loginPassword', self.password),
            ('loginPassword', self.password),
            ('loginPassword', self.password),
            ('loginPassword', self.password),
           
----------
Solution: 

        if not self.__login_details:
            self.__login_details = LoginDetails(self.__connection)
        return self.__login_details
--------------------
Prompt: 
def role_get(name, user=None, host=None, port=None, maintenance_db=None,
             password=None, runas=None, return_password=False):
    """
    Return a dict with information about users of a Postgres server.

    Set return_password to True to get password hash in the result.

    CLI Example:

    .. code-block:: bash

        salt '*' postgres.role_get postgres
    """
----------
Output: 

    return dict(
        name=name,
        user=user,
        host=host,
        port=port,
        maintenance_db=maintenance_db,
        password=hash_password(password, runas),
        role_status=True,
    )


def role_set(name, user=None, host=None, port=None, maintenance_db=None,
             password=None, runas=None, return_password=False):
    """
    Return a dict with information about users of a Postgres server.

    Set return_password to True to get password hash in the result.
----------
Solution: 

    all_users = user_list(user=user,
                          host=host,
                          port=port,
                          maintenance_db=maintenance_db,
                          password=password,
                          runas=runas,
                          return_password=return_password)
    try:
        return all_users.get(name, None)
    except AttributeError:
        log.error('Could not retrieve Postgres role. Is Postgres running?')
        return None
--------------------
Prompt: 
def process_paths(options, candidates=None, error=True):
    """Process files and log errors."""
----------
Output: 

    if candidates is None:
        candidates = []
    if error:
        print('ERROR: %s' % error, file=sys.stderr)
    for path in options.paths:
        if path.startswith('-'):
            path = path[1:]
        if os.path.isdir(path):
            if options.verbose:
                print('Processing directory %s' % path, file=sys.stderr)
            for root, dirs, files in os.walk(path):
                for f in files:
                    if options.verbose:
                        print('  %s' % f, file=sys.stderr)
----------
Solution: 

    errors = check_path(options, rootdir=CURDIR, candidates=candidates)

    if options.format in ['pycodestyle', 'pep8']:
        pattern = "%(filename)s:%(lnum)s:%(col)s: %(text)s"
    elif options.format == 'pylint':
        pattern = "%(filename)s:%(lnum)s: [%(type)s] %(text)s"
    else:  # 'parsable'
        pattern = "%(filename)s:%(lnum)s:%(col)s: [%(type)s] %(text)s"

    for er in errors:
        if options.abspath:
            er._info['filename'] = op.abspath(er.filename)
        LOGGER.warning(pattern, er._info)

    if error:
        sys.exit(int(bool(errors)))

    return errors
--------------------
Prompt: 
def create_bundle(self, name=None, media_url=None,
                      audio_channel=None, metadata=None, notify_url=None,
                      external_id=None):
        """Create a new bundle.

        'metadata' may be None, or an object that can be converted to a JSON
        string.  See API documentation for restrictions.  The conversion
        will take place before the API call.

        All other parameters are also optional. For information about these
        see https://api.clarify.io/docs#!/audio/v1audio_post_1.

        Returns a data structure equivalent to the JSON returned by the API.

        If the response status is not 2xx, throws an APIException.
        If the JSON to python data struct conversion fails, throws an
        APIDataException."""
----------
Output: 
.
        """
        if name is None:
            name = self.name
        if metadata is None:
            metadata = {}
        if media_channel is None:
            media_channel = self.audio_channel
        if notify_url is None:
            notify_url = self.notify_url
        if external_id is None:
            external_id = self.external_id
        if self.bundle_id is None:
            self.bundle_id = self.audio_id + '-bundle'
        if self.bundle_url is None:
            self.bundle_url = self.audio_url +
----------
Solution: 


        # Prepare the data we're going to include in our bundle creation.
        path = '/' + __api_version__ + '/' + BUNDLES_PATH

        data = None

        fields = {}
        if name is not None:
            fields['name'] = name
        if media_url is not None:
            fields['media_url'] = media_url
        if audio_channel is not None:
            fields['audio_channel'] = audio_channel
        if metadata is not None:
            fields['metadata'] = json.dumps(metadata)
        if notify_url is not None:
            fields['notify_url'] = notify_url
        if external_id is not None:
            fields['external_id'] = external_id

        if len(fields) > 0:
            data = fields

        raw_result = self.post(path, data)

        if raw_result.status < 200 or raw_result.status > 202:
            raise APIException(raw_result.status, raw_result.json)

        # Convert the JSON to a python data struct.

        return self._parse_json(raw_result.json)
--------------------
Prompt: 
def resp_set_label(self, resp, label=None):
        """Default callback for get_label/set_label
        """
----------
Output: 

        return self.set_label(resp, label)

    def resp_set_title(self, resp, title=None):
        """Default callback for set_title
        """
        return self.set_title(resp, title)

    def resp_set_body(self, resp, body=None):
        """Default callback for set_body
        """
        return self.set_body(resp, body)

    def resp_set_body_from_file(self, resp, filename, body, filename_params=None):
        """Default callback for set_body_from_file
        """
----------
Solution: 

        if label:
            self.label=label
        elif resp:
            self.label=resp.label.decode().replace("\x00", "")
--------------------
Prompt: 
def directory_exists(value,
                     allow_empty = False,
                     **kwargs):
    """Validate that ``value`` is a valid directory that exists on the local
    filesystem.

    :param value: The value to validate.

    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if
      ``value`` is empty. If ``False``, raises a
      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`
      if ``value`` is empty. Defaults to ``False``.
    :type allow_empty: :class:`bool <python:bool>`

    :returns: The file name represented by ``value``.
    :rtype: Path-like object / :obj:`None <python:None>`

    :raises EmptyValueError: if ``allow_empty`` is ``False`` and ``value``
      is empty
    :raises NotPathlikeError: if ``value`` is not a path-like object
    :raises PathExistsError: if ``value`` does not exist on the local filesystem
    :raises NotADirectoryError: if ``value`` is not a valid directory

    """
----------
Output: 
 :param kwargs: Keyword arguments to be passed to the
      :class:`DirectoryExists <directory_exists.directories.DirectoryExists>` constructor.

    :raises: :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`
      if ``value`` is empty. Defaults to ``False``.
    """
    if not allow_empty and not value:
      raise EmptyValueError('Empty directory cannot be None')
    elif not allow_empty and value:
      raise EmptyValueError('Empty directory cannot be None')
    elif not allow_empty and not value:
      return None
    else:
      raise ValueError('DirectoryExists must have
----------
Solution: 

    if not value and not allow_empty:
        raise errors.EmptyValueError('value (%s) was empty' % value)
    elif not value:
        return None

    value = path_exists(value, force_run = True)                                # pylint: disable=E1123

    if not os.path.isdir(value):
        raise errors.NotADirectoryError('value (%s) is not a directory' % value)

    return value
--------------------
Prompt: 
def write(self, data):
        """Write samples to an output stream.

        As much as one blocksize of audio data will be played
        without blocking. If more than one blocksize was provided,
        the function will only return when all but one blocksize
        has been played.

        Data will be converted to a numpy matrix. Multichannel data
        should be provided as a (frames, channels) matrix. If the
        data is provided as a 1-dim array, it will be treated as mono
        data and will be played on all channels simultaneously. If the
        data is provided as a 2-dim matrix and fewer tracks are
        provided than channels, silence will be played on the missing
        channels. Similarly, if more tracks are provided than there
        are channels, the extraneous channels will not be played.

        """
----------
Output: 
 array, it will be treated as stereo data.
        """
        if len(data.shape)!= 2:
            raise ValueError("data must be 2-dim")
        if data.ndim!= 1:
            raise ValueError("data must be 1-dim")
        if data.shape[1]!= self.channels:
            raise ValueError("data must be of equal length")
        if data.shape[0]!= 1:
            raise ValueError("data must be of equal length")
        if data.shape[0]!= 2:
            raise ValueError("data must be 2-dim array")
        if data.shape
----------
Solution: 

        frames = len(data)
        _, channels = _split(self.channels)
        _, dtype = _split(self.dtype)

        if (not isinstance(data, np.ndarray) or data.dtype != dtype):
            data = np.array(data, dtype=dtype)
        if len(data.shape) == 1:
            # play mono signals on all channels
            data = np.tile(data, (channels, 1)).T
        if data.shape[1] > channels:
            data = data[:, :channels]
        if data.shape < (frames, channels):
            # if less data is available than requested, pad with zeros.
            tmp = data
            data = np.zeros((frames, channels), dtype=dtype)
            data[:tmp.shape[0], :tmp.shape[1]] = tmp

        data = data.ravel().tostring()
        err = _pa.Pa_WriteStream(self._stream, data, frames)
        self._handle_error(err)
--------------------
Prompt: 
def to_xml(self):
        """
        Serialize all properties as XML
        """
----------
Output: 

        return etree.tostring(self._xml, encoding='unicode')

    def to_str(self):
        """
        Serialize all properties as string
        """
        return etree.tostring(self._str, encoding='unicode')

    def to_xml_string(self):
        """
        Serialize all properties as XML string
        """
        return etree.tostring(self._xml_string, encoding='unicode')

    def to_str_list(self):
        """
        Serialize all properties as list of strings
        """
        return [to_xml(p) for p in self.to_str_list_
----------
Solution: 

        ret = '<exif>'
        for k in self.__dict__:
            ret += '<%s>%s</%s>' % (k, self.__dict__[k], k)
        ret += '</exif>'
        return ret
--------------------
Prompt: 
def parse_irreg(self, l):
        """ Constructeur de la classe Irreg.

        :param l: Ligne de chargement des irréguliers
        :type l: str
        """
----------
Output: 

        self.lignes.append(l)
        self.parse_irreg_amount(l)
        self.parse_irreg_date(l)
        self.parse_irreg_amount(l)
        self.parse_irreg_amount(l)
        self.parse_irreg_date(l)
        self.parse_irreg_date(l)
        self.parse_irreg_date(l)
        self.parse_irreg_amount(l)
        self.parse_irreg_date(l)
        self.parse_irreg_date
----------
Solution: 

        ecl = l.split(':')
        grq = ecl[0]
        exclusif = False
        if grq.endswith("*"):
            grq = grq[:-1]
            exclusif = True
        return Irreg(
            graphie_accentuee=grq, graphie=atone(grq),
            exclusif=exclusif,
            morphos=listeI(ecl[2]),
            lemme=self.lemmatiseur.lemme(ecl[1]),
            parent=self.lemmatiseur
        )
--------------------
Prompt: 
def sigmoid(f=Ellipsis, mu=0, sigma=1, scale=1, invert=False, normalize=False):
    """
    sigmoid() yields a potential function that is equivalent to the integral of gaussian(), i.e.,
      the error function, but scaled to match gaussian().
    sigmoid(f) is equivalent to compose(sigmoid(), f).

    All options that are accepted by the gaussian() function are accepted by sigmoid() with the same
    default values and are handled in an equivalent manner with the exception of the invert option;
    when a sigmoid is inverted, the function approaches its maximum value at -inf and approaches 0
    at inf.

    Note that because sigmoid() explicitly matches gaussian(), the base formula used is as follows:
      f(x) = scale * sigma * sqrt(pi/2) * erf((x - mu) / (sqrt(2) * sigma))
      k*sig*Sqrt[Pi/2] Erf[(x - mu)/sig/Sqrt[2]]
    """
----------
Output: 
1.0.

    :param f: The function to be approximated.
    :param mu: The minimum value of the Gaussian.
    :param sigma: The maximum value of the Gaussian.
    :param scale: The scale of the Gaussian.
    :param invert: If True, the function returns the inverse of the error function.
    :param normalize: If True, the function returns the normalized value.
    :return: The function approximation.
    """
    if normalize:
        f = f / (1.0 + np.sqrt(scale))

    if invert:
        f = 1.0 / (1.
----------
Solution: 

    f = to_potential(f)
    F = erf((f - mu) / (sigma * np.sqrt(2.0)))
    if invert: F = 1 - F
    F = np.sqrt(np.pi / 2) * scale * F
    if normalize: F = F / (np.sqrt(2.0*np.pi) * sigma)
    return F
--------------------
Prompt: 
def accumulate(self, axis: AxisIdentifier) -> HistogramBase:
        """Calculate cumulative frequencies along a certain axis.

        Returns
        -------
        new_hist: Histogram of the same type & size
        """
----------
Output: 

        return HistogramBase(self.data.sum(axis=axis), self.data.size)

    def __repr__(self) -> str:
        return f"Histogram({self.data.shape}, {self.name})"

    def __str__(self) -> str:
        return self.name

    def __eq__(self, other: object) -> bool:
        if isinstance(other, Histogram):
            return self.data == other.data
        return NotImplemented

    def __ne__(self, other: object) -> bool:
        if isinstance(other, HistogramBase):
            return not self
----------
Solution: 

        # TODO: Merge with Histogram1D.cumulative_frequencies
        # TODO: Deal with errors and totals etc.
        # TODO: inplace
        new_one = self.copy()
        axis_id = self._get_axis(axis)
        new_one._frequencies = np.cumsum(new_one.frequencies, axis_id[0])
        return new_one
--------------------
Prompt: 
def decrypt(self, key_password):
        """
        Decrypts the entry using the given password. Has no effect if the entry has already been decrypted.

        :param str key_password: The password to decrypt the entry with. If the entry was loaded from a JCEKS keystore,
                                 the password must not contain any characters outside of the ASCII character set.
        :raises DecryptionFailureException: If the entry could not be decrypted using the given password.
        :raises UnexpectedAlgorithmException: If the entry was encrypted with an unknown or unexpected algorithm
        :raise ValueError: If the entry was loaded from a JCEKS keystore and the password contains non-ASCII characters.
        """
----------
Output: 
 a JCEKS keystore, the given password is invalid.
        """
        if not key_password:
            raise ValueError("The password to decrypt must not be empty.")
        if not self.is_valid_key(key_password):
            raise ValueError("The given password is invalid.")
        if self.is_valid_key(key_password):
            raise ValueError("The given password is invalid.")
        self.key_password = key_password
        self.decrypted = True

    def is_valid_key(self, password):
        """
        Checks if the given password is valid.

        :param str password
----------
Solution: 

        if self.is_decrypted():
            return

        encrypted_info = decoder.decode(self._encrypted, asn1Spec=rfc5208.EncryptedPrivateKeyInfo())[0]
        algo_id = encrypted_info['encryptionAlgorithm']['algorithm'].asTuple()
        algo_params = encrypted_info['encryptionAlgorithm']['parameters'].asOctets()
        encrypted_private_key = encrypted_info['encryptedData'].asOctets()

        plaintext = None
        try:
            if algo_id == sun_crypto.SUN_JKS_ALGO_ID:
                plaintext = sun_crypto.jks_pkey_decrypt(encrypted_private_key, key_password)

            elif algo_id == sun_crypto.SUN_JCE_ALGO_ID:
                if self.store_type != "jceks":
                    raise UnexpectedAlgorithmException("Encountered JCEKS private key protection algorithm in JKS keystore")
                # see RFC 2898, section A.3: PBES1 and definitions of AlgorithmIdentifier and PBEParameter
                params = decoder.decode(algo_params, asn1Spec=rfc2898.PBEParameter())[0]
                salt = params['salt'].asOctets()
                iteration_count = int(params['iterationCount'])
                plaintext = sun_crypto.jce_pbe_decrypt(encrypted_private_key, key_password, salt, iteration_count)
            else:
                raise UnexpectedAlgorithmException("Unknown %s private key protection algorithm: %s" % (self.store_type.upper(), algo_id))

        except (BadHashCheckException, BadPaddingException):
            raise DecryptionFailureException("Failed to decrypt data for private key '%s'; wrong password?" % self.alias)

        # at this point, 'plaintext' is a PKCS#8 PrivateKeyInfo (see RFC 5208)
        private_key_info = decoder.decode(plaintext, asn1Spec=rfc5208.PrivateKeyInfo())[0]
        key = private_key_info['privateKey'].asOctets()
        algorithm_oid = private_key_info['privateKeyAlgorithm']['algorithm'].asTuple()

        self._encrypted = None
        self._pkey = key
        self._pkey_pkcs8 = plaintext
        self._algorithm_oid = algorithm_oid
--------------------
Prompt: 
def _get_relative_reference(self, cursor, ref_key):
        """Returns absolute reference code for key.

        Parameters
        ----------

        cursor: 3-tuple of Integer
        \tCurrent cursor position
        ref_key: 3-tuple of Integer
        \tAbsolute reference key

        """
----------
Output: 

        return cursor[0] + ref_key[0] + ref_key[1]

    def _get_key_code(self, key):
        """Returns the key code for key.

        Parameters
        ----------

        key: Integer
        \tKey position

        """
        return key[0] + key[1]

    def _get_key_name(self, cursor, ref_key):
        """Returns the key name for key.

        Parameters
        ----------

        cursor: 3-tuple of Integer
        \tCurrent cursor position
        ref_key: 3-tuple of Integer
        \tKey name

       
----------
Solution: 


        magics = ["X", "Y", "Z"]

        # mapper takes magic, key, ref_key to build string
        def get_rel_key_ele(cursor_ele, ref_key_ele):
            
--------------------
Prompt: 
def get_data_point(self, n):
        """
        Returns the n'th data point (starting at 0) from all columns.

        Parameters
        ----------
        n       
            Index of data point to return.
        """
----------
Output: 

        return self.data[n]

    def get_column_index(self, column):
        """
        Returns the index of the column in the matrix.

        Parameters
        ----------
        column  
            Column number.
        """
        return self.data.index(column)

    def get_row_index(self, row):
        """
        Returns the index of the row in the matrix.

        Parameters
        ----------
        row  
            Row number.

        Returns
        -------
        """
        return self.data.index(row)

    def get_column_value(self, column, value):
        """
        Returns the value
----------
Solution: 

        # loop over the columns and pop the data
        point = []
        for k in self.ckeys: point.append(self[k][n])
        return point
--------------------
Prompt: 
def energy(self):
        """Compute the energy of the system"""
----------
Output: 

        return self.get_energy()

    def get_energy(self):
        """Return the energy of the system"""
        return self.get_energy_per_atom()

    def get_forces(self):
        """Return the forces"""
        return self.get_forces_per_atom()

    def get_forces_per_atom(self):
        """Return the forces per atom"""
        return self.get_forces_per_atom(self.get_forces())

    def get_forces_atom_type(self, atom_type):
        """Return the forces
----------
Solution: 

        result = 0.0
        for index1 in range(self.numc):
            for index2 in range(index1):
                if self.scaling[index1, index2] > 0:
                    for se, ve in self.yield_pair_energies(index1, index2):
                        result += se*ve*self.scaling[index1, index2]
        return result
--------------------
Prompt: 
def phi( n ):
  """Return the Euler totient function of n."""
----------
Output: 

  if n == 0:
    return 0
  if n == 1:
    return 1
  if n % 2 == 0:
    return 2
  return phi(n/2) + phi(n/3) + phi(n/4)

def main():
  """Main function."""
  # Create the solver.
  solver = pywrapcp.Solver('Euler')

  #
  # data
  #
  #
  #
  #
  #
  #
  #
  # solver.add(
  #    'solver',
  #     pywrapcp.Solver,
  #    'solvers/solver.
----------
Solution: 


  assert isinstance( n, integer_types )

  if n < 3: return 1

  result = 1
  ff = factorization( n )
  for f in ff:
    e = f[1]
    if e > 1:
      result = result * f[0] ** (e-1) * ( f[0] - 1 )
    else:
      result = result * ( f[0] - 1 )
  return result
--------------------
Prompt: 
def get_stp_brief_info_output_has_more(self, **kwargs):
        """Auto Generated Code
        """
----------
Output: 

        config = ET.Element("config")
        get_stp_brief_info = ET.Element("get_stp_brief_info")
        config = get_stp_brief_info
        output = ET.SubElement(get_stp_brief_info, "output")
        for x in kwargs:
            if x in output.attrib:
                return True
        return False

    def get_stp_brief_info_output_table(self, **kwargs):
        """Auto Generated Code
        """
        config = ET.Element("config")
        get_stp_brief_info = ET.Element("get_
----------
Solution: 

        config = ET.Element("config")
        get_stp_brief_info = ET.Element("get_stp_brief_info")
        config = get_stp_brief_info
        output = ET.SubElement(get_stp_brief_info, "output")
        has_more = ET.SubElement(output, "has-more")
        has_more.text = kwargs.pop('has_more')

        callback = kwargs.pop('callback', self._callback)
        return callback(config)
--------------------
Prompt: 
def load_json_file(i):
    """
    Input:  {
              json_file - name of file with json
            }

    Output: {
              return       - return code =  0, if successful
                                         = 16, if file not found (may be warning)
                                         >  0, if error
              (error)  - error text if return > 0

              dict     - dict from json file
            }
    """
----------
Output: 

    try:
        with open(i, 'r') as f:
            return json.load(f)
    except IOError:
        return 0


def save_json_file(json_file, data):
    """
    Input:  {
              json_file - name of file with json
            }

    Output: {
              return       - return code =  0, if successful
                                         =  16, if file not found (may be warning)
                                         >  0, if error
              (error)  - error text if return > 0

              dict     - dict from json file
            }
    """
    try:
----------
Solution: 


    fn=i['json_file']

    try:
      if sys.version_info[0]>2:
         f=open(fn, 'r', encoding='utf8')
      else:
         f=open(fn, 'r')
    except Exception as e:
       return {'return':16, 'error':'problem opening json file='+fn+' ('+format(e)+')'}

    try:
      s=f.read()
    except Exception as e:
       f.close()
       return {'return':1, 'error':'problem reading json file='+fn+' ('+format(e)+')'}

    f.close()

    try:
      if sys.version_info[0]>2:
         d=json.loads(s)
      else:
         d=json.loads(s, encoding='utf8')
    except Exception as e:
       return {'return':1, 'error':'problem parsing json from file='+fn+' ('+format(e)+')'}

    return {'return':0, 'dict': d}
--------------------
Prompt: 
def include(self):
        """Return fields to include

        :return list: a list of include information
        """
----------
Output: 

        return self._get_fields(self._include)

    def exclude(self):
        """Return fields to exclude

        :return list: a list of exclude information
        """
        return self._get_fields(self._exclude)

    def include_all(self):
        """Return fields to include

        :return list: a list of include information
        """
        return self._get_fields(self._include_all)

    def exclude_all(self):
        """Return fields to exclude

        :return list: a list of exclude information
        """
        return self._get_fields(self._exclude_all)
----------
Solution: 

        include_param = self.qs.get('include', [])

        if current_app.config.get('MAX_INCLUDE_DEPTH') is not None:
            for include_path in include_param:
                if len(include_path.split('.')) > current_app.config['MAX_INCLUDE_DEPTH']:
                    raise InvalidInclude("You can't use include through more than {} relationships"
                                         .format(current_app.config['MAX_INCLUDE_DEPTH']))

        return include_param.split(',') if include_param else []
--------------------
Prompt: 
def check_input(prolog_file):
    """
    Check for illegal predicates (like reading/writing, opening sockets, etc).
    """
----------
Output: 

    if not os.path.exists(prolog_file):
        raise Exception('Prolog file not found: %s' % prolog_file)
    with open(prolog_file, 'r') as f:
        for line in f:
            if line.startswith('Error: Invalid input'):
                return False
    return True


def check_output(prolog_file):
    """
    Check for illegal predicates (like reading/writing, opening sockets, etc).
    """
    if not os.path.exists(prolog_file):
        raise Exception('Prolog file not found:
----------
Solution: 

    if prolog_file == None:
        return
    for pred in illegal_predicates:
        if type(pred) == tuple:
            print_name = pred[1]
            pred = pred[0]
        else:
            print_name = pred
        if re.search(r'[^\w]' + pred + r'\s*[\(\)\:\.\,\;]+', prolog_file):
            raise Exception('Illegal predicate "%s" used in your input, aborting. If your own predicate clashes with a predefined YAP predicate, you must rename it.' % print_name)
--------------------
Prompt: 
def convert_simple_rnn(builder, layer, input_names, output_names, keras_layer):
    """Convert an SimpleRNN layer from keras to coreml.

    Parameters
    ----------
    keras_layer: layer
        A keras layer object.

    builder: NeuralNetworkBuilder
        A neural network builder object.
    """
----------
Output: 

    if keras_layer.type == "rnn":
        return convert_rnn(builder, keras_layer, input_names, output_names)
    elif keras_layer.type == "simplernn":
        return convert_simple_rnn(builder, layer, input_names, output_names)
    else:
        raise ValueError("Unknown RNN layer type: %s" % keras_layer.type)


def convert_rnn(builder, keras_layer, input_names, output_names):
    """Convert a keras layer from coreml to coreml.

    Parameters
    ----------
    builder: Neural
----------
Solution: 

    # Get input and output names
    hidden_size = keras_layer.output_dim
    input_size = keras_layer.input_shape[-1]

    output_all = keras_layer.return_sequences
    reverse_input = keras_layer.go_backwards

    if keras_layer.consume_less not in ['cpu', 'gpu']:
        raise ValueError('Cannot convert Keras layer with consume_less = %s' % keras_layer.consume_less)

    W_h = np.zeros((hidden_size, hidden_size))
    W_x = np.zeros((hidden_size, input_size))
    b = np.zeros((hidden_size,))

    if keras_layer.consume_less == 'cpu':
        W_h = keras_layer.get_weights()[1].T
        W_x = keras_layer.get_weights()[0].T
        b = keras_layer.get_weights()[2]
    else:
        W_h = keras_layer.get_weights()[1].T
        W_x = keras_layer.get_weights()[0].T
        b = keras_layer.get_weights()[2]

    # Set actication type
    activation_str = _get_recurrent_activation_name_from_keras(keras_layer.activation)

    # Add to the network
    builder.add_simple_rnn(
        name = layer,
        W_h = W_h, W_x = W_x, b = b,
        hidden_size = hidden_size,
        input_size = input_size,
        activation = activation_str,
        input_names = input_names,
        output_names = output_names,
        output_all=output_all,
        reverse_input=reverse_input)
--------------------
Prompt: 
def GetFileEntryByPathSpec(self, path_spec):
    """Retrieves a file entry for a path specification.

    Args:
      path_spec (PathSpec): a path specification.

    Returns:
      OSFileEntry: a file entry or None if not available.
    """
----------
Output: 

    if not path_spec.HasParent():
      return None

    if path_spec.last.path_spec == path_spec:
      return path_spec.last

    path_spec = path_spec.Copy()
    path_spec.last.path_spec = path_spec.last.path_spec.Copy()
    path_spec.last.path_spec.last = path_spec

    return OSFileEntry(
        path_spec=path_spec,
        path_name=path_spec.name,
        path_name_lower=path_spec.name.lower(),
       
----------
Solution: 

    if not self.FileEntryExistsByPathSpec(path_spec):
      return None
    return os_file_entry.OSFileEntry(self._resolver_context, self, path_spec)
--------------------
Prompt: 
def get_field_to_observations_map(generator, query_for_tag=''):
  """Return a field to `Observations` dict for the event generator.

  Args:
    generator: A generator over event protos.
    query_for_tag: A string that if specified, only create observations for
      events with this tag name.

  Returns:
    A dict mapping keys in `TRACKED_FIELDS` to an `Observation` list.
  """
----------
Output: 

  if query_for_tag:
    return [
        track.observations.add()
        for track in generator.track_list.tracks
        if query_for_tag in track.tags and query_for_tag in track.tags[query_for_tag]
    ]
  if not query_for_tag:
    return [
        observation.add()
        for observation in generator.observation_list.observations
        if query_for_tag in observation.tags and query_for_tag in generator.tags[query_for_tag]
    ]
  if query_for_tag not in
----------
Solution: 


  def increment(stat, event, tag=''):
    assert stat in TRACKED_FIELDS
    field_to_obs[stat].append(Observation(step=event.step,
                                          wall_time=event.wall_time,
                                          tag=tag)._asdict())

  field_to_obs = dict([(t, []) for t in TRACKED_FIELDS])

  for event in generator:
    ## Process the event
    if event.HasField('graph_def') and (not query_for_tag):
      increment('graph', event)
    if event.HasField('session_log') and (not query_for_tag):
      status = event.session_log.status
      if status == event_pb2.SessionLog.START:
        increment('sessionlog:start', event)
      elif status == event_pb2.SessionLog.STOP:
        increment('sessionlog:stop', event)
      elif status == event_pb2.SessionLog.CHECKPOINT:
        increment('sessionlog:checkpoint', event)
    elif event.HasField('summary'):
      for value in event.summary.value:
        if query_for_tag and value.tag != query_for_tag:
          continue

        for proto_name, display_name in SUMMARY_TYPE_TO_FIELD.items():
          if value.HasField(proto_name):
            increment(display_name, event, value.tag)
  return field_to_obs
--------------------
Prompt: 
def debug_process(pid):
    """Interrupt a running process and debug it."""
----------
Output: 

    if pid in _debug_processes:
        _debug_processes.remove(pid)
    _debug_processes.append(pid)


def get_test_instance_id():
    """Return a unique, globally unique string identifier for this test run."""
    return "%s.%s.%s" % (socket.gethostname(), os.getpid(), threading.currentThread().ident)


class TestRunner(object):
    """A test runner class that displays results in a table."""
    def __init__(self, verbosity, failfast, test_source_cls, test_source_cls_kwargs,
----------
Solution: 

    os.kill(pid, signal.SIGUSR1)  # Signal process.
    pipe = NamedPipe(pipename(pid), 1)
    try:
        while pipe.is_open():
            txt=raw_input(pipe.get()) + '\n'
            pipe.put(txt)
    except EOFError:
        pass # Exit.
    pipe.close()
--------------------
Prompt: 
def _start(self):
        """Start the server."""
----------
Output: 

        self.server.start()
        self.server.wait()

    def _stop(self):
        """Stop the server."""
        self.server.stop()

    def _get_server_url(self):
        """Return the server URL."""
        return self.url

    def _get_server_id(self):
        """Return the server ID."""
        return self.id

    def _get_server_name(self):
        """Return the server name."""
        return self.name

    def _get_server_type(self):
        """Return the server type."""
        return self.type

    def
----------
Solution: 

        assert self._tempdir is not None
        assert self._process is None
        self._process = subprocess.Popen(
            [
                self.paths.slapd,
                '-f', self._slapd_conf,
                '-h', self.uri,
                '-d', str(self.slapd_debug),
            ],
            stdout=sys.stdout,
            stderr=sys.stderr,
        )
        self._poll_slapd(timeout=self.max_server_startup_delay)
--------------------
Prompt: 
def Kerr_factor(final_mass, distance):
    """Return the factor final_mass/distance (in dimensionless units) for Kerr
    ringdowns
    """
----------
Output: 

    return final_mass * distance

def Kerr_factor_2(final_mass, distance):
    """Return the factor final_mass/distance (in dimensionless units) for Kerr
    ringdowns
    """
    return final_mass * distance

def Kerr_factor_3(final_mass, distance):
    """Return the factor final_mass/distance (in dimensionless units) for Kerr
    ringdowns
    """
    return Kerr_factor(final_mass, 2) * distance

def Kerr_factor_4(final_mass, distance):
   
----------
Solution: 


    # Convert solar masses to meters
    mass = final_mass * lal.MSUN_SI * lal.G_SI / lal.C_SI**2
    # Convert Mpc to meters
    dist = distance * 1e6 * lal.PC_SI

    return mass / dist
--------------------
Prompt: 
def init_run(shell, no_daemon, daemon_options, daemon_outfile):
    """
    Configure your shell.

    Add the following line in your shell RC file and then you are
    ready to go::

      eval $(%(prog)s)

    To check if your shell is supported, simply run::

      %(prog)s --no-daemon

    If you want to specify shell other than $SHELL, you can give
    --shell option::

      eval $(%(prog)s --shell zsh)

    By default, this command also starts daemon in background to
    automatically index shell history records.  To not start daemon,
    use --no-daemon option like this::

      eval $(%(prog)s --no-daemon)

    To see the other methods to launch the daemon process, see
    ``rash daemon --help``.

    """
----------
Output: 
 use daemons, use this::

      %(prog)s daemonize

  -p, --pidfile=pidfile  Specify the path to the pid file to write to.
    """
    if not no_daemon:
        # first fork
        try:
            pid = os.fork()
        except OSError, e:
            sys.stderr.write("fork #1 failed: %s (%s)\n" % (e.strerror, e.errno))
            sys.exit(1)

        # second fork
        if pid > 0:
            sys.stderr.write("fork #2 failed: pid %d already running as pid
----------
Solution: 

    import sys
    from .__init__ import __version__
    init_file = find_init(shell)
    if os.path.exists(init_file):
        sys.stdout.write(INIT_TEMPLATE.format(
            file=init_file, version=__version__))
    else:
        raise RuntimeError(
            "Shell '{0}' is not supported.".format(shell_name(shell)))

    if not no_daemon:
        from .daemon import start_daemon_in_subprocess
        start_daemon_in_subprocess(daemon_options, daemon_outfile)
--------------------
Prompt: 
def ndim(self) -> int:
        """Get number of dimensions."""
----------
Output: 

        return len(self._data)

    @property
    def ndims(self) -> int:
        """Get number of dimensions."""
        return self._n_dims

    @property
    def n_points(self) -> int:
        """Get number of points."""
        return self._n_points

    @property
    def n_dims_points(self) -> int:
        """Get number of points."""
        return self._n_points

    @property
    def n_dims_dims(self) -> int:
        """Get number of dimensions."""
        return self._n_dims

    @property
    def
----------
Solution: 

        try:
            assert self._ndim is not None
        except (AssertionError, AttributeError):
            if len(self.variables) == 0:
                self._ndim = 0
            else:
                self._ndim = self.variables[0].ndim
        finally:
            return self._ndim
--------------------
Prompt: 
def render_activity(activity, grouped_activity=None, *args, **kwargs):
    """
    Given an activity, will attempt to render the matching template snippet
    for that activity's content object
    or will return a simple representation of the activity.

    Also takes an optional 'grouped_activity' argument that would match up with
    what is produced by utils.group_activity
    """
----------
Output: 

    if grouped_activity is None:
        grouped_activity = activity

    if grouped_activity.is_published:
        return render_published(grouped_activity, *args, **kwargs)
    if grouped_activity.is_draft:
        return render_draft(grouped_activity, *args, **kwargs)
    if grouped_activity.is_published:
        return render_published(grouped_activity, *args, **kwargs)
    if grouped_activity.is_draft:
        return render_draft(grouped_activity, *args, **kwargs)
    if grouped_activity.is_published:
----------
Solution: 

    template_name = 'activity_monitor/includes/models/{0.app_label}_{0.model}.html'.format(activity.content_type)
    try:
        tmpl = loader.get_template(template_name)
    except template.TemplateDoesNotExist:
        return None
    # we know we have a template, so render it
    content_object = activity.content_object
    return tmpl.render(Context({
        'activity': activity,
        'obj': content_object,
        'grouped_activity': grouped_activity
    }))
--------------------
Prompt: 
def clean_blobstore_cache(self):
        """Deletes all of the existing buildpack caches in the blobstore"""
----------
Output: 

        self.blobstore.delete_all_caches()

    def get_cache_key(self, name):
        """Returns a unique key for the given buildpack name"""
        return self.blobstore.get_cache_key(name)

    def get_cache_value(self, name):
        """Returns a value for the given buildpack name"""
        return self.blobstore.get_cache_value(name)

    def get_cache_type(self, name):
        """Returns a unique type for the given buildpack name"""
        return self.blobstore.get_cache_type
----------
Solution: 

        url = self.api_url + self.blobstores_builpack_cache_url
        resp, rcode = self.request('DELETE', url)
        if rcode != 202:
            raise CFException(resp, rcode)
        return resp
--------------------
Prompt: 
def _compute_ogg_page_crc(page):
  """ Compute CRC of an Ogg page. """
----------
Output: 

  return zlib.crc32(page.data[0x0:0x10] + [0x0] * 10) & 0xffffffff

def _compute_ogg_page_id(page):
  """ Compute the Ogg page ID of an Ogg page. """
  return zlib.crc32(page.data[0x10:0x10 + page.data[0x10 + page.data_len(OggPage.OggPage))]

def _compute_ogg_page_size(page):
  """ Compute the size of the page. """
  return len
----------
Solution: 

  page_zero_crc = page[:OGG_FIRST_PAGE_HEADER_CRC_OFFSET] + \
      b"\00" * OGG_FIRST_PAGE_HEADER_CRC.size + \
      page[OGG_FIRST_PAGE_HEADER_CRC_OFFSET + OGG_FIRST_PAGE_HEADER_CRC.size:]
  return ogg_page_crc(page_zero_crc)
--------------------
Prompt: 
def domain_sites():
    """
    Get a list of domains
    
    Each domain is an attribute dict with name, site_id and settings
    """
----------
Output: 

    return [site for site in Site.objects.all() if site.domain_id not in settings.DOMAINS]

def get_domain_by_name(name):
    """
    Get a domain by name
    
    name - name of the domain
    """
    return Site.objects.get(pk=name)

def get_domain_by_site_id(site_id):
    """
    Get a domain by site_id
    
    site_id - id of the site
    """
    return get_domain_by_name(Site.objects.get(pk=site_id))
----------
Solution: 


    if not hasattr(env,'domains'):
        sites = _get_django_sites()
        site_ids = sites.keys()
        site_ids.sort()
        domains = []
        
        for id in site_ids:

            for file in _sitesettings_files():
                domain = _AttributeDict({})

                if file == 'settings.py':
                    domain.name = sites[id]
                else: #prefix indicates subdomain
                    subdomain = file[:-12].replace('_','.')
                    domain.name = ''.join([subdomain,sites[id]])

                domain.settings = file
                domain.site_id = id
                domains.append(domain)
                
        env.domains = domains
        if env.domains: env.root_domain = env.domains[0].name
        else:
            domain.name = _root_domain(); domain.site_id = 1; domain.settings='settings.py'
            env.domains = [domain]
            
    return env.domains
--------------------
Prompt: 
def parse_instance(self, global_params, region, reservation):
        """
        Parse a single EC2 instance

        :param global_params:           Parameters shared for all regions
        :param region:                  Name of the AWS region
        :param instance:                 Cluster
        """
----------
Output: 

        self.debug("Parsing EC2 instance: %s" % instance)
        self.debug("region: %s, reservation: %s" % (region, reservation))

        # Parse the cluster name
        cluster_name = None
        if 'cluster' in global_params:
            cluster_name = global_params['cluster']

        # Parse the instance name
        instance_name = None
        if 'instance' in global_params:
            instance_name = global_params['instance']

        # Parse the cluster size
        instance_size = None
        if 'instance_type' in global_params:
           
----------
Solution: 

        for i in reservation['Instances']:
            instance = {}
            vpc_id = i['VpcId'] if 'VpcId' in i and i['VpcId'] else ec2_classic
            manage_dictionary(self.vpcs, vpc_id, VPCConfig(self.vpc_resource_types))
            instance['reservation_id'] = reservation['ReservationId']
            instance['id'] = i['InstanceId']
            get_name(i, instance, 'InstanceId')
            get_keys(i, instance, ['KeyName', 'LaunchTime', 'InstanceType', 'State', 'IamInstanceProfile', 'SubnetId'])
            # Network interfaces & security groups
            manage_dictionary(instance, 'network_interfaces', {})
            for eni in i['NetworkInterfaces']:
                nic = {}
                get_keys(eni, nic, ['Association', 'Groups', 'PrivateIpAddresses', 'SubnetId', 'Ipv6Addresses'])
                instance['network_interfaces'][eni['NetworkInterfaceId']] = nic
            self.vpcs[vpc_id].instances[i['InstanceId']] = instance
--------------------
Prompt: 
def get_scheduler_info():
    """Gets the scheduler info."""
----------
Output: 

    return {
        'name': 'Home Assistant',
       'version': __version__,
       'model': 'Home Assistant',
       'source': 'home-assistant',
    }


def setup(hass, config):
    """Set up the Home Assistant component."""
    hass.data[DOMAIN] = {}
    hass.data[DOMAIN]['config'] = config

    # Register services
    hass.services.register(DOMAIN, SERVICE_HOMEASSISTANT_STOP,
                           lambda service: service.async_stop(),
                           schema=get_service_schema(config))

    # Register our components
    hass
----------
Solution: 


    scheduler = current_app.apscheduler

    d = OrderedDict([
        ('current_host', scheduler.host_name),
        ('allowed_hosts', scheduler.allowed_hosts),
        ('running', scheduler.running)
    ])

    return jsonify(d)
--------------------
Prompt: 
def add(self, extension):
        """
        Add a extension to the editor.

        :param extension: The extension instance to add.

        """
----------
Output: 

        self.extension = extension
        self.editor.add_extension(self.extension)
        self.editor.set_current_name(self.extension.name)
        self.editor.set_current_id(self.extension.id)
        self.editor.set_current_extension(self.extension)

    def remove(self):
        """
        Remove the extension.

        """
        self.extension.delete()

    def save(self):
        """
        Save the current extension.

        """
        self.editor.save_current_name()
        self.editor.set_current_id(
----------
Solution: 

        logger.debug('adding extension {}'.format(extension.name))
        self._extensions[extension.name] = extension
        extension.on_install(self.editor)
        return extension
--------------------
Prompt: 
def _image_loop(self):
        """Retrieve an iterable of images either with, or without a progress bar."""
----------
Output: 

        if self.progress_bar is None:
            return
        else:
            return self.progress_bar.iter_images()

    def _get_image_from_path(self, path):
        """Return the image data from a given path."""
        image_file = open(path, 'rb')
        image = image_file.read()
        image_file.close()
        return image

    def _get_image_from_path_no_progress(self, path):
        """Return the image data from a given path, but no progress bar."""
        image_file = open(path,
----------
Solution: 

        if self.progress_bar and 'tqdm' in self.progress_bar.lower():
            return tqdm(self.imgs, desc='Saving PNGs as flat PDFs', total=len(self.imgs), unit='PDFs')
        else:
            return self.imgs
--------------------
Prompt: 
def get_sleepstage(self, stage_idx=None):
        """Score the sleep stage, using shortcuts or combobox."""
----------
Output: 

        if stage_idx is None:
            stage_idx = self.stage_idx
        if self.combobox_idx is None:
            self.combobox_idx = stage_idx
        if self.combobox_idx == self.stage_idx:
            return
        self.combobox_idx = stage_idx
        if self.combobox_idx == self.stage_idx:
            self.combobox_idx = stage_idx
        self.combobox.set_value(stage_idx)
        self.combobox.set_active(self.combobox_idx)
        self.
----------
Solution: 

        if self.annot is None:  # remove if buttons are disabled
            error_dialog = QErrorMessage()
            error_dialog.setWindowTitle('Error getting sleep stage')
            error_dialog.showMessage('No score file loaded')
            error_dialog.exec()
            return

        window_start = self.parent.value('window_start')
        window_length = self.parent.value('window_length')

        if window_length != self.epoch_length:
            msg = ('Zoom to ' + str(self.epoch_length) + ' (epoch length) ' +
                   'for sleep scoring.')
            error_dialog = QErrorMessage()
            error_dialog.setWindowTitle('Error getting sleep stage')
            error_dialog.showMessage(msg)
            error_dialog.exec()
            lg.debug(msg)
            return

        try:
            self.annot.set_stage_for_epoch(window_start,
                                           STAGE_NAME[stage_idx])

        except KeyError:
            msg = ('The start of the window does not correspond to any epoch ' +
                   'in sleep scoring file.\n\n'
                   'Switch to the appropriate window length in View, then use '
                   'Navigation --> Line Up with Epoch to line up the window.')
            error_dialog = QErrorMessage()
            error_dialog.setWindowTitle('Error getting sleep stage')
            error_dialog.showMessage(msg)
            error_dialog.exec()
            lg.debug(msg)

        else:
            lg.debug('User staged ' + str(window_start) + ' as ' +
                     STAGE_NAME[stage_idx])

            self.set_stage_index()
            self.parent.overview.mark_stages(window_start, window_length,
                                             STAGE_NAME[stage_idx])
            self.display_stats()
            self.parent.traces.page_next()
--------------------
Prompt: 
def do_rename(argdict):
    """Rename a page."""
----------
Output: 

    if not argdict['name']:
        raise ArgumentError('rename requires a name')
    if argdict['name'] == 'new':
        if len(argdict['aliases']) > 0:
            raise ArgumentError('cannot rename aliases')
        return argdict['name']
    if len(argdict['aliases']) == 1:
        if argdict['aliases'][0] == 'new':
            return do_rename(argdict)
        return argdict['aliases'][0]
    if len(argdict['aliases']) > 1:
        raise ArgumentError('cannot rename aliases')
    return ''

def do
----------
Solution: 

    site = make_site_obj(argdict)
    slug = argdict['slug']
    newtitle = argdict['newtitle']
    try:
        site.rename_page(slug, newtitle)
        print "Renamed page."
    except ValueError: # pragma: no cover
        print "Cannot rename. A page with the given slug does not exist."
        sys.exit()
--------------------
Prompt: 
def sync(self):
        """
        Syncs the information for this settings out to the file system.
        """
----------
Output: 

        self.write_settings_file(self.config_file)
        self.write_settings_file(self.settings_file)

    def set_config_file(self, config_file):
        """
        Sets the settings file.
        """
        self.config_file = config_file

    def set_settings_file(self, settings_file):
        """
        Sets the settings file.
        """
        self.settings_file = settings_file

    def get_config_file(self):
        """
        Returns the settings file.
        """
        return self.config_file

    def get
----------
Solution: 

        if self._customFormat:
            self._customFormat.save(self.fileName())
        else:
            super(XSettings, self).sync()
--------------------
Prompt: 
def categories(self, *category_slugs):
        """
        Return the entries with the given category slugs.
        When multiple tags are provided, they operate as "OR" query.
        """
----------
Output: 

        if not category_slugs:
            return []
        if not isinstance(category_slugs, (list, tuple)):
            category_slugs = [category_slugs]
        return [self.get_entry(tag) for tag in category_slugs]

    def get_category_slugs(self, category):
        """
        Return the entries with the given category slugs.
        When multiple entries are provided, they operate as "OR" query.
        """
        if not isinstance(category, (list, tuple)):
            category = [category]
        return category
----------
Solution: 

        categories_field = getattr(self.model, 'categories', None)
        if categories_field is None:
            raise AttributeError("The {0} does not include CategoriesEntryMixin".format(self.model.__name__))

        if issubclass(categories_field.rel.model, TranslatableModel):
            # Needs a different field, assume slug is translated (e.g django-categories-i18n)
            filters = {
                'categories__translations__slug__in': category_slugs,
            }

            # TODO: should the current language also be used as filter somehow?
            languages = self._get_active_rel_languages()
            if languages:
                if len(languages) == 1:
                    filters['categories__translations__language_code'] = languages[0]
                else:
                    filters['categories__translations__language_code__in'] = languages

            return self.filter(**filters).distinct()
        else:
            return self.filter(categories__slug=category_slugs)
--------------------
Prompt: 
def add_download(self, info, future):
        """
        Hand off a download to the Downloads plugin, if it is present.

        Parameters
        ----------
        info : `~ginga.misc.Bunch.Bunch`
            A bunch of information about the URI as returned by
            `ginga.util.iohelper.get_fileinfo()`

        future : `~ginga.misc.Future.Future`
            A future that represents the future computation to be performed
            after downloading the file.  Resolving the future will trigger
            the computation.
        """
----------
Output: 

        if self.uri is None:
            raise ValueError("No URI to add a download to.")
        if info.get('uri_type', None) == 'file':
            return self.add_fileinfo(info, future)
        elif info.get('uri_type', None) == 'http':
            return self.add_httpinfo(info, future)
        elif info.get('uri_type', None) == 'fileinfo':
            return self.add_fileinfo(info, future)
        elif info.get('uri_type', None) == 'httpinfo':
            return self
----------
Solution: 

        if self.gpmon.has_plugin('Downloads'):
            obj = self.gpmon.get_plugin('Downloads')
            self.gui_do(obj.add_download, info, future)
        else:
            self.show_error("Please activate the 'Downloads' plugin to"
                            " enable download functionality")
--------------------
Prompt: 
def current_task(self, args):
        """Name of current action for progress-bar output.

        The specific task string is depends on the configuration via `args`.

        Returns
        -------
        ctask : str
            String representation of this task.
        """
----------
Output: 

        return self.name

    @property
    def parent_id(self):
        """ID of the parent action for progress-bar output.

        Returns
        -------
        parent_id : int
            ID of the parent action.
        """
        return self.name

    @property
    def parent_name(self):
        """Name of the parent action for progress-bar output.

        Returns
        -------
        parent_name : str
            Name of the parent action.
        """
        return self.name

    @property
    def ctask(self):
        """Current action for progress-bar output.

        The specific task string is depends
----------
Solution: 

        ctask = self.nice_name if self.nice_name is not None else self.name
        if args is not None:
            if args.update:
                ctask = ctask.replace('%pre', 'Updating')
            else:
                ctask = ctask.replace('%pre', 'Loading')
        return ctask
--------------------
Prompt: 
def convert_time(time):
    """Convert a time string into 24-hour time."""
----------
Output: 

    if time[-1] in ('s','seconds'):
        time = time[:-1]
    return time


def convert_datetime(date_str):
    """Convert a datetime string into a 24-hour time."""
    if date_str is None:
        return None
    return datetime.strptime(date_str, '%Y-%m-%dT%H:%M:%S')


def convert_date(date_str):
    """Convert a date string into a 24-hour time."""
    return convert_time(date_str)


def convert_timedelta(date_str):
    """
----------
Solution: 

    split_time = time.split()
    try:
        # Get rid of period in a.m./p.m.
        am_pm = split_time[1].replace('.', '')
        time_str = '{0} {1}'.format(split_time[0], am_pm)
    except IndexError:
        return time
    try:
        time_obj = datetime.strptime(time_str, '%I:%M %p')
    except ValueError:
        time_obj = datetime.strptime(time_str, '%I %p')

    return time_obj.strftime('%H:%M %p')
--------------------
Prompt: 
def slurpChompedLines(file, expand=False):
    r"""Return ``file`` a list of chomped lines. See `slurpLines`."""
----------
Output: 

    with open(file, 'r') as f:
        lines = f.readlines()
    if expand:
        lines = [line.replace(b'\r\n', b'\n') for line in lines]
    return ''.join(lines)


def slurpLines(lines, expand=False):
    r"""Return ``lines`` a list of lines.

    The first line is a list of ``text``, the second line is a list of ``text``
    and the third line is a list of ``text`` and so on.

    >>> slurpLines([b'foo\n
----------
Solution: 

    f=_normalizeToFile(file, "r", expand)
    try: return list(chompLines(f))
    finally: f.close()
--------------------
Prompt: 
def _summarize_losses(self, losses_dict):
    """Adds `tf.summary`s to all terms in the losses dictionary."""
----------
Output: 

    for loss in losses_dict.values():
      self._add_loss(loss)

  def _add_loss(self, loss):
    """Adds a scalar loss to the loss dictionary."""
    if isinstance(loss, tf.Tensor):
      self._losses[loss.name] = tf.summary.scalar(name=loss.name, data=loss)
    else:
      self._losses[loss.name] = tf.summary.scalar(name=loss.name, data=loss)

  def _add_accuracy(self, accuracy):
    """Adds a scalar accuracy to the accuracy dictionary."""
----------
Solution: 

    if common_layers.should_generate_summaries():
      with tf.name_scope("losses"):
        for loss_name, loss_val in sorted(losses_dict.items()):
          tf.summary.scalar(loss_name, loss_val)
--------------------
Prompt: 
def expand_role(self, role):
        """Expand an IAM role name into an ARN.

        If the role is already in the form of an ARN, then the role is simply returned. Otherwise we retrieve the full
        ARN and return it.

        Args:
            role (str): An AWS IAM role (either name or full ARN).

        Returns:
            str: The corresponding AWS IAM role ARN.
        """
----------
Output: 

        if isinstance(role, str):
            return role
        elif isinstance(role, dict):
            return expand_role(role)
        else:
            raise ValueError("Invalid IAM role name: %s" % role)

    def get_role_policy(self, role):
        """Get the policy for an IAM role.

        Args:
            role (str): An AWS IAM role (either name or full ARN).

        Returns:
            dict: The policy for an IAM role.
        """
        if isinstance(role, str):
            return self.get_iam_role_policy_from_name(role)
----------
Solution: 

        if '/' in role:
            return role
        else:
            return self.boto_session.resource('iam').Role(role).arn
--------------------
Prompt: 
def add_multi_sign_transaction(self, m: int, pub_keys: List[bytes] or List[str], signer: Account):
        """
        This interface is used to generate an Transaction object which has multi signature.

        :param tx: a Transaction object which will be signed.
        :param m: the amount of signer.
        :param pub_keys: a list of public keys.
        :param signer: an Account object which will sign the transaction.
        :return: a Transaction object which has been signed.
        """
----------
Output: 

        if not isinstance(m, int):
            raise TypeError("m must be an integer")
        if not isinstance(pub_keys, list):
            raise TypeError("pub_keys must be a list of bytes")
        if len(m)!= m_max_len:
            raise ValueError("len(m) must be m_max_len")
        if len(pub_keys)!= pub_keys_len(self.get_public_key()):
            raise ValueError("len(pub_keys) must be pub_keys_len(self.get_public_key())")
        if len(pub
----------
Solution: 

        for index, pk in enumerate(pub_keys):
            if isinstance(pk, str):
                pub_keys[index] = pk.encode('ascii')
        pub_keys = ProgramBuilder.sort_public_keys(pub_keys)
        tx_hash = self.hash256()
        sig_data = signer.generate_signature(tx_hash)
        if self.sig_list is None or len(self.sig_list) == 0:
            self.sig_list = []
        elif len(self.sig_list) >= TX_MAX_SIG_SIZE:
            raise SDKException(ErrorCode.param_err('the number of transaction signatures should not be over 16'))
        else:
            for i in range(len(self.sig_list)):
                if self.sig_list[i].public_keys == pub_keys:
                    if len(self.sig_list[i].sig_data) + 1 > len(pub_keys):
                        raise SDKException(ErrorCode.param_err('too more sigData'))
                    if self.sig_list[i].m != m:
                        raise SDKException(ErrorCode.param_err('M error'))
                    self.sig_list[i].sig_data.append(sig_data)
                    return
        sig = Sig(pub_keys, m, [sig_data])
        self.sig_list.append(sig)
--------------------
Prompt: 
def create(self, data):
        """ Create object from the given data.

        The given data may or may not have been validated prior to calling
        this function. This function will try its best in creating the object.
        If the resulting object cannot be produced, raises ``ValidationError``.

        The spec can affect how individual fields will be created by
        implementing ``clean()`` for the fields needing customization.

        :param data: the data as a dictionary.
        :return: instance of ``klass`` or dictionary.
        :raises: ``ValidationError`` if factory is unable to create object.
        """
----------
Output: 

        if data is None:
            return None
        return klass(**data)

    def clean(self):
        """ Clean up ``self.cleaned_data`` and ``self.cleaned_data['password']``."""
        if self.cleaned_data['password']!= self.cleaned_data.get('password'):
            raise ValidationError('Passwords do not match')
        return self.cleaned_data

    def save(self, commit=True):
        """ Save object to the database and return ``True`` if the object
        was saved successfully.

        :param commit: if True, the transaction is committed.
        :return
----------
Solution: 


        # todo: copy-paste code from representation.validate -> refactor

        if data is None:
            return None

        prototype = {}
        errors = {}

        # create and populate the prototype
        for field_name, field_spec in self.spec.fields.items():
            try:
                value = self._create_value(data, field_name, self.spec)
            except ValidationError, e:
                if field_name not in self.default_create_values:
                    if hasattr(e, 'message_dict'):
                        # prefix error keys with top level field name
                        errors.update(dict(zip(
                            [field_name + '.' + key for key in e.message_dict.keys()],
                            e.message_dict.values())))
                    else:
                        errors[field_name] = e.messages
            else:
                key_name = self.property_name_map[field_name]
                prototype[key_name] = value

        # check extra fields
        if self.prevent_extra_fields:
            extras = set(data.keys()) - set(self.property_name_map.keys())
            if extras:
                errors[', '.join(extras)] = ['field(s) not allowed']

        # if errors, raise ValidationError
        if errors:
            raise ValidationError(errors)

        # return dict or object based on the prototype
        _data = deepcopy(self.default_create_values)
        _data.update(prototype)
        if self.klass:
            instance = self.klass()
            instance.__dict__.update(prototype)
            return instance
        else:
            return prototype
--------------------
Prompt: 
def _initURL(self):
        """ sets proper URLs for AGOL """
----------
Output: 

        self.url = 'http://www.arcgis.com/sharing/rest/services/World_Imagery/MapServer/MapServer?f=json&token=%s' % self.token
        self.url = 'http://www.arcgis.com/sharing/rest/services/World_Imagery/MapServer/MapServer?f=json&token=%s' % self.token
        self.url = 'http://www.arcgis.com/sharing/rest/services/World_Imagery/MapServer/MapServer?f=json&token
----------
Solution: 


        token = self._getTokenArcMap()
        if 'error' in token:
            self._valid = False
            self._message = token['error']
        else:
            self._valid = True
            self._message = "Token Generated"
        self._org_url = arcpy.GetActivePortalURL()
        if self._org_url.lower().find('/sharing/rest') > -1:
            self._url = self._org_url
        else:
            self._url = self._org_url + "/sharing/rest"

        if self._url.startswith('http://'):
            self._surl = self._url.replace('http://', 'https://')
        else:
            self._surl  =  self._url

        url = '{}/portals/self'.format( self._url)

        parameters = {
            'f': 'json'
        }
        portal_info = self._post(url=url,
                                 param_dict=parameters,
                                 securityHandler=self,
                                 proxy_port=self._proxy_port,
                                 proxy_url=self._proxy_url)

        if 'user' in portal_info:
            if 'username' in portal_info['user']:
                self._username = portal_info['user']['username']


        results = self._get(url= self._surl + '/portals/info',
                               param_dict={'f':'json'},
                               proxy_port=self._proxy_port,
                               proxy_url=self._proxy_url)
        if 'authInfo' in results and 'tokenServicesUrl' in results['authInfo']:

            self._token_url = results['authInfo']['tokenServicesUrl']
        else:
            self._token_url = self._surl  + '/generateToken'
--------------------
Prompt: 
def repl_proc(self, inputstring, log=True, **kwargs):
        """Process using replprocs."""
----------
Output: 

        if log:
            self.log.debug("Replacing %s with %s", inputstring, kwargs)
        self.inputstring = inputstring
        self.log.debug("Input string: %s", inputstring)
        self.kwargs = kwargs
        self.process_line(inputstring)
        self.process_line(kwargs)

    def process_line(self, line):
        """Process a single line of the log file."""
        if self.kwargs.get('log_all', False):
            self.log.debug("Processing line: %s", line)
        if self.process
----------
Solution: 

        return self.apply_procs(self.replprocs, kwargs, inputstring, log=log)
--------------------
Prompt: 
def get_job_class(klass_str):
    """
    Return the job class
    """
----------
Output: 

    return get_job_class_by_name(klass_str, "Job")


def get_job_class_by_name(job_class_str, name):
    """
    Return the job class
    """
    return get_job_class(name).get_job_class()


def get_job_template_name(job_template_str):
    """
    Return the template name for a job template
    """
    return get_job_template(job_template_str).get_job_template_name()


def get_job_template(job_
----------
Solution: 

    mod_name, klass_name = klass_str.rsplit('.', 1)
    try:
        mod = importlib.import_module(mod_name)
    except ImportError as e:
        logger.error("Error importing job module %s: '%s'", mod_name, e)
        return
    try:
        klass = getattr(mod, klass_name)
    except AttributeError:
        logger.error("Module '%s' does not define a '%s' class", mod_name, klass_name)
        return
    return klass
--------------------
Prompt: 
def picard_fastq_to_bam(picard, fastq_one, fastq_two, out_dir, names, order="queryname"):
    """Convert fastq file(s) to BAM, adding sample, run group and platform information.
    """
----------
Output: 

    out_file = os.path.join(out_dir, names[0] + ".bam")
    if not os.path.exists(picard):
        with open(out_file, "w") as out_handle:
            out_handle.write(fastq_one + "\n")
            out_handle.write(fastq_two + "\n")
    else:
        cmd = "samtools view -bS {0} | samtools merge -@ {1} | bamtobam -@ {2}".format(
            fastq_one, fastq_two, out
----------
Solution: 

    out_bam = os.path.join(out_dir, "%s-fastq.bam" %
                           os.path.splitext(os.path.basename(fastq_one))[0])
    if not file_exists(out_bam):
        with tx_tmpdir(picard._config) as tmp_dir:
            with file_transaction(picard._config, out_bam) as tx_out_bam:
                opts = [("FASTQ", fastq_one),
                        ("READ_GROUP_NAME", names["rg"]),
                        ("SAMPLE_NAME", names["sample"]),
                        ("PLATFORM_UNIT", names["pu"]),
                        ("PLATFORM", names["pl"]),
                        ("TMP_DIR", tmp_dir),
                        ("OUTPUT", tx_out_bam),
                        ("SORT_ORDER", order)]
                if fastq_two:
                    opts.append(("FASTQ2", fastq_two))
                picard.run("FastqToSam", opts)
    return out_bam
--------------------
Prompt: 
def _anime_add(self, data):
        """
        Adds an anime to a user's list.

        :param data: A :class:`Pymoe.Mal.Objects.Anime` object with the anime data
        :raises: SyntaxError on invalid data type
        :raises: ServerError on failure to add
        :rtype: Bool
        :return: True on success
        """
----------
Output: 

        if not self.is_valid_anime(data):
            raise SyntaxError(data)

        if data.get_type() == 'file':
            self.anime_files.append(data)
            return True

        if data.get_type() == 'file-list':
            self.anime_files_list.append(data)
            return True

        if data.get_type() == 'file-list-single':
            self.anime_files_list.append(data)
            return True

        if data.get_type() == 'file-list-single-single
----------
Solution: 

        if isinstance(data, Anime):
            xmlstr = data.to_xml()
            r = requests.get(self.apiurl + "animelist/add/{}.xml".format(data.id),
                             params={'data': xmlstr},
                             auth=HTTPBasicAuth(self._username, self._password),
                             headers=self.header)
            if r.status_code != 201:
                raise ServerError(r.text, r.status_code)
            return True
        else:
            raise SyntaxError(
                "Invalid type: data should be a Pymoe.Mal.Objects.Anime object. Got a {}".format(type(data)))
--------------------
Prompt: 
def get_task_df(self):
		"""
		Returns
		-------

		"""
----------
Output: 

		return self.get_task_df_from_file(self.file_name, self.file_path)

	def get_task_df_from_file(self, file_name, path):
		"""
		Returns
		-------

		"""
		df = pd.read_csv(path, sep="\t", header=None, index_col=False)
		df.columns = [col.strip() for col in path.split("\t")]
		return df

	def get_task_df_from_list(self, list_of
----------
Solution: 

		term_time_df = self._get_term_time_df()
		terms_to_include = (
			term_time_df
				.groupby('term')['top']
				.sum()
				.sort_values(ascending=False)
				.iloc[:self.num_terms_to_include].index
		)
		task_df = (
			term_time_df[term_time_df.term.isin(terms_to_include)][['time', 'term']]
				.groupby('term')
				.apply(lambda x: pd.Series(self._find_sequences(x['time'])))
				.reset_index()
				.rename({0: 'sequence'}, axis=1)
				.reset_index()
				.assign(start=lambda x: x['sequence'].apply(lambda x: x[0]))
				.assign(end=lambda x: x['sequence'].apply(lambda x: x[1]))
			[['term', 'start', 'end']]
		)
		return task_df
--------------------
Prompt: 
def delete_and_upload_images(client, image_type, language, base_dir):
    """
    Delete and upload images with given image_type and language.

    Function will stage delete and stage upload all
    found images in matching folders.
    """
----------
Output: 

    for folder in image_type:
        client.delete_image(folder)
        print('Deleted image %s' % folder)
        client.delete_folder(folder)


def main():
    """
    Main function
    """
    parser = argparse.ArgumentParser(description='Upload images to S3')
    parser.add_argument('-u', '--upload-bucket', type=str, required=True,
                        help='The name of the bucket to upload images to')
    parser.add_argument('-p', '--prefix', type=str, required=True,
                        help='The prefix of the bucket names
----------
Solution: 

    print('{0} {1}'.format(image_type, language))
    files_in_dir = os.listdir(os.path.join(base_dir, language))
    delete_result = client.deleteall(
        'images', imageType=image_type, language=language)

    deleted = delete_result.get('deleted', list())
    for deleted_files in deleted:
        print('  delete image: {0}'.format(deleted_files['id']))

    for image_file in files_in_dir[:8]:
        image_file_path = os.path.join(base_dir, language, image_file)
        image_response = client.upload(
            'images',
            imageType=image_type,
            language=language,
            media_body=image_file_path)
        print("  upload image {0} new id {1}".format(image_file, image_response['image']['id']))
--------------------
Prompt: 
def provision_machine(self):
        """Perform the initial provisioning of the target machine.

        :return: bool: The client.AddMachineParams
        :raises: :class:`paramiko.ssh_exception.AuthenticationException`
            if the upload fails
        """
----------
Output: 

        try:
            self.machine.connection.put_file(self.path, self.key, self.get_policy())
            return True
        except paramiko.ssh_exception.AuthenticationException:
            return False

    def close(self):
        """Close the connection to the device.

        :return: bool: The client.Close
        :raises: :class:`paramiko.ssh_exception.AuthenticationException`
            if the upload fails
        """
        self.machine.connection.close()
        return False

    def get_policy(self):
        """Get the policy for the target machine.

        :return:
----------
Solution: 

        params = client.AddMachineParams()

        if self._init_ubuntu_user():
            try:

                ssh = self._get_ssh_client(
                    self.host,
                    self.user,
                    self.private_key_path
                )

                hw = self._detect_hardware_and_os(ssh)
                params.series = hw['series']
                params.instance_id = "manual:{}".format(self.host)
                params.nonce = "manual:{}:{}".format(
                    self.host,
                    str(uuid.uuid4()),  # a nop for Juju w/manual machines
                )
                params.hardware_characteristics = {
                    'arch': hw['arch'],
                    'mem': int(hw['mem']),
                    'cpu-cores': int(hw['cpu-cores']),
                }
                params.addresses = [{
                    'value': self.host,
                    'type': 'ipv4',
                    'scope': 'public',
                }]

            except paramiko.ssh_exception.AuthenticationException as e:
                raise e
            finally:
                ssh.close()

        return params
--------------------
Prompt: 
def _submit(self):
        """submit the question to the board. When we get here we should have 
           (under self.data)
           
                {'record_environment': [('DISPLAY', ':0')],
                 'user_prompt_board': 'http://127.0.0.1',
                 'user_prompt_issue': 'I want to know why dinosaurs are so great!',
                 'user_prompt_title': 'Why are dinosaurs so great?'}

           self.token should be propogated with the personal access token
        """
----------
Output: 

        self.data['token'] = self.token
        self.data['user_prompt_board'] = self.data['token']
        self.data['user_prompt_title'] = self.token
        self.data['user_prompt_issue'] = self.token
        self.data['user_prompt_title'] = self.token
        """submit the question to the issue"""
        self.data['user_prompt_issue'] = self.token
        """submit the issue to the board"""
        self.data['user_prompt_title'] = self.token
        """submit the board to
----------
Solution: 
 
        body = self.data['user_prompt_issue']
        title = self.data['user_prompt_title']
        board = self.data['user_prompt_board']
        username = self.data['user_prompt_username']
        category = self.data['user_prompt_category']

        # Step 1: Token
        if self.token == None:
            self.token = self.request_token(board)
            self._get_and_update_setting('HELPME_DISCOURSE_TOKEN', self.token)

        # Step 1: Environment

        envars = self.data.get('record_environment')        
        body = body + envars_to_markdown(envars)

        # Step 2: Asciinema

        asciinema = self.data.get('record_asciinema')
        if asciinema not in [None, '']:
            url = upload_asciinema(asciinema)

            # If the upload is successful, add a link to it.

            if url is not None:
                body += "\n[View Asciinema Recording](%s)" % url
 
        # Add other metadata about client

        body += "\n\ngenerated by [HelpMe](https://vsoch.github.io/helpme/)"
        body += "\nHelpMe Discourse Id: %s" %(self.run_id)

        # Submit the issue

        post = self.create_post(title, body, board, category, username)
        return post
--------------------
Prompt: 
def sample(self, nsims=1000):
        """ Samples from the posterior predictive distribution

        Parameters
        ----------
        nsims : int (default : 1000)
            How many draws from the posterior predictive distribution

        Returns
        ----------
        - np.ndarray of draws from the data
        """
----------
Output: 

        return self.model.sample(nsims)

    def __str__(self):
        """ Returns a string representation of the posterior predictive distribution

        Returns
        ----------
        - str: string representation of the posterior predictive distribution
        """
        return "Posterior predictive distribution of %s" % self.model.__class__.__name__

    def __repr__(self):
        """ Returns a string representation of the posterior predictive distribution

        Returns
        ----------
        - repr: string representation of the posterior predictive distribution
        """
        return "Posterior predictive distribution of %s" % self.model.__
----------
Solution: 
     
        if self.latent_variables.estimation_method not in ['BBVI', 'M-H']:
            raise Exception("No latent variables estimated!")
        else:
            lv_draws = self.draw_latent_variables(nsims=nsims)
            mus = [self._model(lv_draws[:,i])[0] for i in range(nsims)]
            model_scale, model_shape, model_skewness = self._get_scale_and_shape_sim(lv_draws)
            data_draws = np.array([self.family.draw_variable(self.link(mus[i]), 
                np.repeat(model_scale[i], mus[i].shape[0]), np.repeat(model_shape[i], mus[i].shape[0]), 
                np.repeat(model_skewness[i], mus[i].shape[0]), mus[i].shape[0]) for i in range(nsims)])
            return data_draws
--------------------
Prompt: 
def get(self, element):
        """Gets ith element of a collection in an object of the corresponding \
           class.
        Args:
            output(string): can accept 'jsonstat' or 'dataframe_list'

        Returns:
            Serialized JSONstat or a list of Pandas Dataframes,depending on \
            the 'output' parameter.

        """
----------
Output: 

        return self.get_jsonstat(element)

    def get_jsonstat(self, element):
        """Gets ith element of a collection in an object of the corresponding \
           class.
        Args:
            output(string): can accept 'jsonstat' or 'dataframe_list'

        Returns:
            JSONStat or a list of Pandas Dataframes,depending on \
            the 'output' parameter.

        """
        return element

    def get_dataframe_list(self, element):
        """Gets the list of DataFrames in an object of the corresponding \
           class.

        Args:
            output(
----------
Solution: 


        if self['link']['item'][element]['class'] == 'dataset':
            return Dataset.read(self['link']['item'][element]['href'])
        elif self['link']['item'][element]['class'] == 'collection':
            return Collection.read(self['link']['item'][element]['href'])
        elif self['link']['item'][element]['class'] == 'dimension':
            return Dimension.read(self['link']['item'][element]['href'])
        else:
            raise ValueError(
                "Class not allowed. Please use dataset, collection or "
                "dimension'")
--------------------
Prompt: 
def _GetClientLib(service_class_names, language, output_path, build_system,
                  hostname=None, application_path=None):
  """Fetch client libraries from a cloud service.

  Args:
    service_class_names: A list of fully qualified ProtoRPC service names.
    language: The client library language to generate. (java)
    output_path: The directory to output the discovery docs to.
    build_system: The target build system for the client library language.
    hostname: A string hostname which will be used as the default version
      hostname. If no hostname is specificied in the @endpoints.api decorator,
      this value is the fallback. Defaults to None.
    application_path: A string containing the path to the AppEngine app.

  Returns:
    A list of paths to client libraries.
  """
----------
Output: 
ficed, the default version will be used.
    application_path: The path to the application for the client library.

  Returns:
    A list of client libraries.
  """
  if hostname is None:
    hostname = build_system.GetDefaultHostname()
  client_lib_dir = os.path.join(output_path, 'client')
  if not os.path.isdir(client_lib_dir):
    return []

  client_lib_files = []
  for service_class_name in service_class_names:
    client_lib_files.extend(
        _GetClientLib
----------
Solution: 

  client_libs = []
  service_configs = GenApiConfig(
      service_class_names, hostname=hostname,
      config_string_generator=discovery_generator.DiscoveryGenerator(),
      application_path=application_path)
  for api_name_version, config in service_configs.iteritems():
    client_name = api_name_version + '.zip'
    client_libs.append(
        _GenClientLibFromContents(config, language, output_path,
                                  build_system, client_name))
  return client_libs
--------------------
Prompt: 
def idngram2lm(idngram_file, vocab_file, output_file, context_file=None, vocab_type=1, oov_fraction=0.5, four_byte_counts=False, min_unicount=0, zeroton_fraction=False, n=3, verbosity=2, arpa_output=True, ascii_input=False):
    """
        Takes an idngram-file (in either binary (by default) or ASCII (if specified) format), a vocabulary file, and (optionally) a context cues file. Additional command line parameters will specify the cutoffs, the discounting strategy and parameters, etc. It outputs a language model, in either binary format (to be read by evallm), or in ARPA format.
    """
----------
Output: 
 parameters are passed to the LM.
        Returns a dictionary of {word: idngram}
    """
    if context_file is not None:
        if not isinstance(context_file, str):
            raise TypeError("context file must be a binary or ASCII string")
        if not isinstance(vocab_file, str):
            raise TypeError("vocab file must be a binary or ASCII string")
        if not isinstance(output_file, str):
            raise TypeError("output file must be a binary or ASCII string")
        if not isinstance(idngram_file, str):
            raise TypeError("idngram file must
----------
Solution: 

     # TODO: Args still missing
     # [ -calc_mem | -buffer 100 | -spec_num y ... z ]
     # [ -two_byte_bo_weights   
     #     [ -min_bo_weight nnnnn] [ -max_bo_weight nnnnn] [ -out_of_range_bo_weights] ]
     # [ -linear | -absolute | -good_turing | -witten_bell ]
     # [ -disc_ranges 1 7 7 ]
     # [ -cutoffs 0 ... 0 ]

    cmd = ['idngram2lm', '-idngram', os.path.abspath(idngram_file),
                         '-vocab', os.path.abspath(vocab_file),
                         '-vocab_type', vocab_type,
                         '-oov_fraction', oov_fraction,
                         '-min_unicount',min_unicount,
                         '-verbosity',verbosity,
                         '-n',n]
    if arpa_output:
        cmd.extend(['-arpa',output_file])
    else:
        cmd.extend(['-binary',output_file])

    if four_byte_counts:
        cmd.append('-four_byte_counts')

    if zeroton_fraction:
        cmd.append('-zeroton_fraction')

    if ascii_input:
        cmd.append('-ascii_input')
    else:
        cmd.append('-bin_input')

    # Ensure that every parameter is of type 'str'
    cmd = [str(x) for x in cmd]

    with tempfile.SpooledTemporaryFile() as output_f:
        with  output_to_debuglogger() as err_f:
            exitcode = subprocess.call(cmd, stdout=output_f, stderr=err_f)
        output = output_f.read()
    
    logger = logging.getLogger(__name__)
    logger.debug("Command '%s' returned with exit code '%d'." % (' '.join(cmd), exitcode))

    if exitcode != 0:
        raise ConversionError("'%s' returned with non-zero exit status '%s'" % (cmd[0], exitcode))

    if sys.version_info >= (3,) and type(output) is bytes:
        output = output.decode('utf-8')

    return output.strip()
--------------------
Prompt: 
def get_DID_subdomain(self, did, cur=None):
        """
        Get a subdomain, given its DID
        Raise ValueError if the DID is invalid
        Raise SubdomainNotFound if the DID does not correspond to a subdomain
        """
----------
Output: 

        if cur is None:
            cur = self.cur
            self.cur = None
        if not cur.is_subdomain(did):
            raise ValueError("DID %s does not belong to a subdomain %s" % (did, cur.get_name()))
        return cur

    def get_DID_type(self, did):
        """
        Get the type of a subdomain
        Raise ValueError if the DID is invalid
        """
        if not cur:
            cur = self.cur
        if not did in cur.get_types():
            raise ValueError("DID %s is not a valid DID
----------
Solution: 

        did = str(did)

        try:
            did_info = parse_DID(did)
            assert did_info['name_type'] == 'subdomain', 'Not a subdomain DID'
        except:
            raise ValueError("Invalid DID: {}".format(did))
        
        original_address = did_info['address']
        name_index = did_info['index']

        # find the initial subdomain (the nth subdomain created by this address)
        cmd = 'SELECT fully_qualified_subdomain FROM {} WHERE owner = ? AND sequence = ? ORDER BY parent_zonefile_index, zonefile_offset LIMIT 1 OFFSET ?;'.format(self.subdomain_table)
        args = (original_address, 0, name_index)

        cursor = None
        if cur is None:
            cursor = self.conn.cursor()
        else:
            cursor = cur

        subdomain_name = None

        rows = db_query_execute(cursor, cmd, args)
        for r in rows:
            subdomain_name = r['fully_qualified_subdomain']
            break

        if not subdomain_name:
            raise SubdomainNotFound('Does not correspond to a subdomain: {}'.format(did))

        # get the current form
        subrec = self.get_subdomain_entry(subdomain_name, cur=cur)
        subrec.did_info = did_info
        return subrec
--------------------
Prompt: 
def seek(self, offset, whence=os.SEEK_SET):
    """Seeks to an offset within the file-like object.

    Args:
      offset (int): offset to seek to.
      whence (Optional(int)): value that indicates whether offset is an absolute
          or relative position within the file.

    Raises:
      IOError: if the seek failed.
      OSError: if the seek failed.
    """
----------
Output: 

    self._check_open()
    self._stream.seek(offset, whence)
    self._position += offset

  def tell(self):
    """Returns the current position of the stream.

    Returns:
      int: current position of the stream.
    """
    return self._position

  def __enter__(self):
    """Enters the stream context."""
    self._check_open()
    return self

  def __exit__(self, unused_type, unused_value, unused_traceback):
    """Exits the stream context."""
    self._check_open()
    self._stream.close()

  def __
----------
Solution: 

    if not self._is_open:
      raise IOError('Not opened.')

    if whence == os.SEEK_CUR:
      offset += self._current_offset
    elif whence == os.SEEK_END:
      offset += self._cpio_archive_file_entry.data_size
    elif whence != os.SEEK_SET:
      raise IOError('Unsupported whence.')

    if offset < 0:
      raise IOError('Invalid offset value less than zero.')

    self._current_offset = offset
--------------------
Prompt: 
def registRequest(self, CorpNum, taxinvoice, memo=None, UserID=None):
        """ 즉시 요청
            args
                CorpNum : 팝빌회원 사업자번호
                taxinvoice : 세금계산서 객체
                memo : 메모
                UsreID : 팝빌회원 아이디
            return
                검색결과 정보
            raise
                PopbillException
        """
----------
Output: 
사 요청
            """
        if memo == None:
            memo = {}
        if UserID == None:
            UserID = self.currentUser
        if TaxID == None:
            TaxID = self.currentTaxID
        if memo.has_key(UserID):
            return memo[UserID]
        else:
            return taxinvoice.registRequest(self.currentUser, CorpNum, taxinvoice.id, TaxID)

    def registTax(self, TaxNum, UserID=None):
        """ 즉시 요청
            args
               
----------
Solution: 


        if memo != None and memo != '':
            taxinvoice.memo = memo

        postData = self._stringtify(taxinvoice)

        return self._httppost('/Taxinvoice', postData, CorpNum, UserID, "REQUEST")
--------------------
Prompt: 
def print_(*objects, **kwargs):
    """print_(*objects, sep=None, end=None, file=None, flush=False)

    Args:
        objects (object): zero or more objects to print
        sep (str): Object separator to use, defaults to ``" "``
        end (str): Trailing string to use, defaults to ``"\\n"``.
            If end is ``"\\n"`` then `os.linesep` is used.
        file (object): A file-like object, defaults to `sys.stdout`
        flush (bool): If the file stream should be flushed
    Raises:
        EnvironmentError

    Like print(), but:

    * Supports printing filenames under Unix + Python 3 and Windows + Python 2
    * Emulates ANSI escape sequence support under Windows
    * Never fails due to encoding/decoding errors. Tries hard to get everything
      on screen as is, but will fall back to "?" if all fails.

    This does not conflict with ``colorama``, but will not use it on Windows.
    """
----------
Output: 
 """
    for obj in objects:
        print(obj, file=file, **kwargs)


def print_file(*objects, **kwargs):
    """print_file(*objects, sep=None, end=None, file=None, flush=False)

    Args:
        objects (object): zero or more objects to print
        kwargs: Keyword arguments to print()
    """
    for obj in objects:
        print(obj, **kwargs)


def print_file_lines(file_lines, **kwargs):
    """print_file_lines(*file_lines, sep=None, end=None
----------
Solution: 


    sep = kwargs.get("sep")
    sep = sep if sep is not None else " "
    end = kwargs.get("end")
    end = end if end is not None else "\n"
    file = kwargs.get("file")
    file = file if file is not None else sys.stdout
    flush = bool(kwargs.get("flush", False))

    if is_win:
        _print_windows(objects, sep, end, file, flush)
    else:
        _print_unix(objects, sep, end, file, flush)
--------------------
Prompt: 
def fit_radius_from_potentials(z, SampleFreq, Damping, HistBins=100, show_fig=False):
    """
    Fits the dynamical potential to the Steady 
    State Potential by varying the Radius.
    
    z : ndarray
        Position data
    SampleFreq : float
        frequency at which the position data was 
        sampled
    Damping : float
        value of damping (in radians/second)
    HistBins : int
        number of values at which to evaluate 
        the steady state potential / perform
        the fitting to the dynamical potential

    Returns
    -------
    Radius : float
        Radius of the nanoparticle
    RadiusError : float
        One Standard Deviation Error in the Radius from the Fit
        (doesn't take into account possible error in damping)
    fig : matplotlib.figure.Figure object
        figure showing fitted dynamical potential and stationary potential
    ax : matplotlib.axes.Axes object
        axes for above figure

    """
----------
Output: 
 potential.
    show_fig : bool
        whether to show the figure or not
    """
    if show_fig:
        plt.figure()
        plt.plot(z, Damping, 'r')
        plt.title('Steady State Potential')
        plt.ylabel('Damping')
        plt.xlabel('Hist')
        plt.ylabel('HistBins')
        plt.show()
    return z, Damping, HistBins, z.shape[0]

def fit_position_from_potentials(z, HistBins=100, show
----------
Solution: 

    dt = 1/SampleFreq
    boltzmann=Boltzmann
    temp=300 # why halved??
    density=1800
    SteadyStatePotnl = list(steady_state_potential(z, HistBins=HistBins))
    yoffset=min(SteadyStatePotnl[1])
    SteadyStatePotnl[1] -= yoffset

    SpringPotnlFunc = dynamical_potential(z, dt)
    SpringPotnl = SpringPotnlFunc(z)
    kBT_Gamma = temp*boltzmann*1/Damping
    
    DynamicPotentialFunc = make_dynamical_potential_func(kBT_Gamma, density, SpringPotnlFunc)
    FitSoln = _curve_fit(DynamicPotentialFunc, SteadyStatePotnl[0], SteadyStatePotnl[1], p0 = 50)
    print(FitSoln)
    popt, pcov = FitSoln
    perr = _np.sqrt(_np.diag(pcov))
    Radius, RadiusError = popt[0], perr[0]

    mass=((4/3)*pi*((Radius*10**-9)**3))*density
    yfit=(kBT_Gamma/mass)
    Y = yfit*SpringPotnl
    
    fig, ax = _plt.subplots()
    ax.plot(SteadyStatePotnl[0], SteadyStatePotnl[1], 'bo', label="Steady State Potential")
    _plt.plot(z,Y, 'r-', label="Dynamical Potential")
    ax.legend(loc='best')
    ax.set_ylabel('U ($k_{B} T $ Joules)')
    ax.set_xlabel('Distance (mV)')
    _plt.tight_layout()
    if show_fig == True:
        _plt.show()
    return Radius*1e-9, RadiusError*1e-9, fig, ax
--------------------
Prompt: 
def apply_trans_rot(ampal, translation, angle, axis, point, radians=False):
    """Applies a translation and rotation to an AMPAL object."""
----------
Output: 

    if radians:
        return ampal.apply_trans_rot(point, axis)
    else:
        return ampal.apply_rot(axis, point)


def apply_trans_scale(ampal, scale, axis, point):
    """Applies a scale to an AMPAL object."""
    if scale is None:
        return ampal.apply_trans_scale(point)
    else:
        return scale.apply_trans_scale(axis)


def apply_trans_rot_scale(ampal, scale, angle, radians=False):
    """Applies a scale to
----------
Solution: 

    if not numpy.isclose(angle, 0.0):
        ampal.rotate(angle=angle, axis=axis, point=point, radians=radians)
    ampal.translate(vector=translation)
    return
--------------------
Prompt: 
def __initialize_node(self, attributes_flags=int(Qt.ItemIsSelectable | Qt.ItemIsEnabled)):
        """
        Initializes the node.

        :param attributes_flags: Attributes flags.
        :type attributes_flags: int
        """
----------
Output: 

        self.attributes_flags = attributes_flags
        self.set_selected(True)

    def __set_selected(self, selected):
        """
        Sets the node as selected or unselected.

        :param selected: The node is selected or unselected.
        :type selected: bool
        """
        self.set_selected(selected)

    def __select_node(self, node):
        """
        Selects the node.

        :param node: The node is selected or unselected.
        :type node: QTreeWidgetItem
        """
        self.select_node(node, True)
        self.
----------
Solution: 


        attributes = dir(self.__component)
        for attribute in attributes:
            if attribute == "name":
                continue

            if not "_Profile__{0}".format(attribute) in attributes:
                continue

            value = getattr(self.__component, attribute)
            value = ", ".join(value) if type(value) in (tuple, list) else value
            roles = {Qt.DisplayRole: value,
                     Qt.EditRole: value}
            self[attribute] = umbra.ui.nodes.GraphModelAttribute(attribute, value, roles, attributes_flags)

        self.update_tool_tip()
--------------------
Prompt: 
def _get_next_occurrence(haystack, offset, needles):
        """
        Find next occurence of one of the needles in the haystack

        :return: tuple of (index, needle found)
             or: None if no needle was found"""
----------
Output: 

        for needle in needles:
            if offset >= needle.end and offset < needle.start:
                return needle, needle.end, offset
        return None, None, None

    def _get_next_occurrence_in_range(haystack, start, end):
        """
        Find next occurence of one of the needles in the haystack

        :return: tuple of (index, occurence found)
             or: None if no occurence was found"""
        for needle in haystack.get_needles():
            if start >= need
----------
Solution: 

        # make map of first char to full needle (only works if all needles
        # have different first characters)
        firstcharmap = dict([(n[0], n) for n in needles])
        firstchars = firstcharmap.keys()
        while offset < len(haystack):
            if haystack[offset] in firstchars:
                possible_needle = firstcharmap[haystack[offset]]
                if haystack[offset:offset + len(possible_needle)] == possible_needle:
                    return offset, possible_needle
            offset += 1
        return None
--------------------
Prompt: 
def sequence_length(fasta):
    """
    return a dict of the lengths of sequences in a fasta file
    """
----------
Output: 

    seqs = {}
    for seq_record in SeqIO.parse(fasta, "fasta"):
        seqs[seq_record.id] = len(seq_record.seq)
    return seqs


def get_fasta_records(fasta_file):
    """
    return a list of SeqRecord objects
    """
    records = []
    for record in SeqIO.parse(open(fasta_file, "r"), "fasta"):
        if len(record.seq) > 0:
            records.append(SeqRecord(record.id, record.seq))
    return records


def main
----------
Solution: 

    sequences = SeqIO.parse(fasta, "fasta")
    records = {record.id: len(record) for record in sequences}
    return records
--------------------
Prompt: 
def decode_response(client_message, to_object=None):
    """ Decode response from client message"""
----------
Output: 

    if to_object is None:
        to_object = {}
    if type(to_object) is dict:
        to_object = decode_dict(client_message)
    return to_object


def decode_dict(client_message):
    """ Decode dict from client message"""
    return json.loads(client_message)


def decode_dict_response(to_object):
    """ Decode dict response from client message"""
    return json.loads(to_object)


def get_message_from_json(json_message):
    """ Get message from json message"""
   
----------
Solution: 

    parameters = dict(base=None, increment=None, batch_size=None)
    parameters['base'] = client_message.read_long()
    parameters['increment'] = client_message.read_long()
    parameters['batch_size'] = client_message.read_int()
    return parameters
--------------------
Prompt: 
def sentence(random=random, *args, **kwargs):
    """
    Return a whole sentence

    >>> mock_random.seed(0)
    >>> sentence(random=mock_random)
    "Agatha Incrediblebritches can't wait to smell two chimps in Boatbencheston."

    >>> mock_random.seed(2)
    >>> sentence(random=mock_random, slugify=True)
    'blistersecret-studios-is-the-best-company-in-liveronion'

    """
----------
Output: 

    return random.choice(args, **kwargs)

def get_random_string(length=6, chars=string.ascii_lowercase + string.digits):
    """
    Return a random string

    >>> get_random_string(length=6)
    'Agatha Incrediblebritches can't wait to smell two chimps in Boatbencheston.'

    """
    return ''.join(choice(chars) for _ in range(length))

def get_random_slug(length=6, chars=slugify_string(string.ascii_lowercase
----------
Solution: 

    if 'name' in kwargs and kwargs['name']:
        nm = kwargs(name)
    elif random.choice([True, False, False]):
        nm = name(capitalize=True, random=random)
    else:
        nm = random.choice(people)

    def type_one():
        return "{name} will {verb} {thing}.".format(name=nm,
                                                    verb=verb(random=random),
                                                    thing=random.choice([a_thing(random=random),
                                                                         things(random=random)]))

    def type_two():
        return "{city} is in {country}.".format(city=city(capitalize=True, random=random),
                                                country=country(capitalize=True, random=random))

    def type_three():
        return "{name} can't wait to {verb} {thing} in {city}.".format(name=nm,
                                                                      verb=verb(random=random),
                                                                      thing=a_thing(random=random),
                                                                      city=city(capitalize=True, random=random))

    def type_four():
        return "{name} will head to {company} to buy {thing}.".format(name=nm,
                                                                     company=company(capitalize=True, random=random),
                                                                     thing=a_thing(random=random))


    def type_five():
        return "{company} is the best company in {city}.".format(city=city(capitalize=True, random=random),
                                                                 company=company(capitalize=True, random=random))

    def type_six():
        return "To get to {country}, you need to go to {city}, then drive {direction}.".format(
            country=country(capitalize=True, random=random),
            city=city(capitalize=True, random=random),
            direction=direction(random=random))

    def type_seven():
        return "{name} needs {thing}, badly.".format(name=nm, thing=a_thing(random=random))

    def type_eight():
        return "{verb} {noun}!".format(verb=verb(capitalize=True, random=random), noun=noun(random=random))

    return random.choice([type_one,
                          type_two,
                          type_three,
                          type_four,
                          type_five,
                          type_six,
                          type_seven,
                          type_eight])()
--------------------
Prompt: 
def segment(self, source, language=None):
    """Returns a chunk list from the given sentence.

    Args:
      source (str): Source string to segment.
      language (:obj:`str`, optional): A language code.

    Returns:
      A chunk list. (:obj:`budou.chunk.ChunkList`)

    Raises:
      ValueError: If :obj:`language` is given and it is not included in
                  :obj:`supported_languages`.
    """
----------
Output: 

    if language is not None:
      if not self.supported_languages.has_key(language):
        raise ValueError('Unsupported language: %s' % language)
      return self.supported_languages[language].get_chunk_list()
    else:
      return []

  def __iter__(self):
    """Iterates over the sentence.

    Yields:
      :obj:`budou.chunk.Chunk` instances.

    Raises:
      :obj:`StopIteration`: If :obj:`language` is not given and
                  it is not included in :obj:`supported_languages`.
    """
    if self.
----------
Solution: 

    if language and not language in self.supported_languages:
      raise ValueError(
          'Language {} is not supported by NLAPI segmenter'.format(language))

    chunks = ChunkList()
    results = tinysegmenter.tokenize(source)
    seek = 0
    for word in results:
      word = word.strip()
      if not word:
        continue
      if source[seek: seek + len(word)] != word:
        assert source[seek] == ' '
        assert source[seek + 1: seek + len(word) + 1] == word
        chunks.append(Chunk.space())
        seek += 1

      dependency = None
      if word in _PARTICLES or word in _AUX_VERBS or is_hiragana(word):
        dependency = False

      chunk = Chunk(word, dependency=dependency)
      if chunk.is_punct():
        chunk.dependency = chunk.is_open_punct()
      chunks.append(chunk)
      seek += len(word)
    chunks.resolve_dependencies()
    return chunks
--------------------
Prompt: 
def check_announcements():
    """Check for any announcements from cs50.me, raise Error if so."""
----------
Output: 

    try:
        raw = urllib2.urlopen(announce_url).read()
    except urllib2.HTTPError, e:
        raise Error("Couldn't read announcement: %s" % e)
    if raw.startswith("Error:"):
        raise Error("Couldn't read error: %s" % raw)
    if raw.startswith("Error:"):
        raise Error("Couldn't read error: %s" % raw)
    raise Error("Couldn't read error: %s" % raw)

def check_for_updates():
    """Check for any updates from cs50.me, raise Error
----------
Solution: 

    res = requests.get("https://cs50.me/status/submit50")  # TODO change this to submit50.io!
    if res.status_code == 200 and res.text.strip():
        raise Error(res.text.strip())
--------------------
Prompt: 
def highlightBlock(self, text):
        """Takes a block, applies format to the document.
        according to what's in it.
        """
----------
Output: 

        # TODO: this is a hack to get the line number from the first line of the block
        #       to be able to detect the line number of a block
        #       and then use that to highlight the block
        #       (and the line number of the block)
        #       and use that to highlight the line number of a block
        #       and then use that to highlight the line number of a block
        #       to be able to detect the line number of a block
        #       and then use that to highlight the line number of a block
        #       to be able to detect the line number of a
----------
Solution: 


        # I need to know where in the document we are,
        # because our formatting info is global to
        # the document
        cb = self.currentBlock()
        p = cb.position()

        # The \n is not really needed, but sometimes
        # you are in an empty last block, so your position is
        # **after** the end of the document.
        text = str(self.document().toPlainText()) + '\n'

        # Yes, re-highlight the whole document.
        # There **must** be some optimizacion possibilities
        # but it seems fast enough.
        highlight(text, self.lexer, self.formatter)

        # Just apply the formatting to this block.
        # For titles, it may be necessary to backtrack
        # and format a couple of blocks **earlier**.
        for i in range(len(str(text))):
            try:
                self.setFormat(i, 1, self.formatter.data[p + i])
            except IndexError:
                pass

        # I may need to do something about this being called
        # too quickly.
        self.tstamp = time.time()
--------------------
Prompt: 
def paired_paths(main_path, fmt, formats):
    """Return the list of paired notebooks, given main path, and the list of formats"""
----------
Output: 

    paths = []
    for fmt in formats:
        if fmt.lower() == main_path.lower():
            paths.append(fmt)
    return paths


def get_all_notebooks(path):
    """Return a list of all notebooks in the given directory"""
    return [os.path.join(path, f) for f in os.listdir(path) if f.lower().endswith(fmt)]


def get_notebooks_in_directory(path, formats):
    """Return a list of all notebooks in the given directory"""
    return [os.path.join
----------
Solution: 

    if not formats:
        return [(main_path, {'extension': os.path.splitext(main_path)[1]})]

    formats = long_form_multiple_formats(formats)

    # Is there a format that matches the main path?
    base = base_path(main_path, fmt)
    paths = [full_path(base, fmt) for fmt in formats]

    if main_path not in paths:
        raise InconsistentPath(u"Paired paths '{}' do not include the current notebook path '{}'. "
                               u"Current format is '{}', and paired formats are '{}'."
                               .format("','".join(paths), main_path, short_form_one_format(fmt),
                                       short_form_multiple_formats(formats)))

    if len(paths) > len(set(paths)):
        raise InconsistentPath(u'Duplicate paired paths for this notebook. Please fix jupytext.formats.')

    return list(zip(paths, formats))
--------------------
Prompt: 
def ratio(value, decimal_places=0, failure_string='N/A'):
    """
    Converts a floating point value a X:1 ratio.
    
    Number of decimal places set by the `precision` kwarg. Default is one.
    """
----------
Output: 

    if value is None:
        return None
    return (value / float(decimal_places)) * 100.0

def get_format(format):
    """
    Returns the format string with the given name, using the given format string.
    
    If the given format string is not valid, returns `None`.
    """
    if not format:
        return None
    return format.lower()

def get_format_lazy(format):
    """
    Returns the name of the given format string, but with a `lazy`
    attribute. Default is the name of the `lazy` attribute.
    """
    if
----------
Solution: 

    try:
        f = float(value)
    except ValueError:
        return failure_string
    return _saferound(f, decimal_places) + ':1'
--------------------
Prompt: 
def get_cli_returns(
            self,
            jid,
            minions,
            timeout=None,
            tgt='*',
            tgt_type='glob',
            verbose=False,
            show_jid=False,
            **kwargs):
        """
        Starts a watcher looking at the return data for a specified JID

        :returns: all of the information for the JID
        """
----------
Output: 

        return self.core.start_watch(jid, minions, timeout, **kwargs)

    def get_cli_returns_with_timeout(self, timeout):
        """
        Starts a watcher looking at the return data for a specified JID

        :param timeout: timeout in seconds
        :type timeout: int
        :returns: all of the information for the JID
        """
        return self.core.start_watch(
            self.core.start_return_path,
            minions=self.minions,
            timeout=timeout,
            **self.core.core_args
        )
----------
Solution: 

        if verbose:
            msg = 'Executing job with jid {0}'.format(jid)
            print(msg)
            print('-' * len(msg) + '\n')
        elif show_jid:
            print('jid: {0}'.format(jid))
        if timeout is None:
            timeout = self.opts['timeout']
        fret = {}
        # make sure the minions is a set (since we do set operations on it)
        minions = set(minions)

        found = set()
        # start this before the cache lookup-- in case new stuff comes in
        event_iter = self.get_event_iter_returns(jid, minions, timeout=timeout)

        # get the info from the cache
        ret = self.get_cache_returns(jid)
        if ret != {}:
            found.update(set(ret))
            yield ret

        # if you have all the returns, stop
        if len(found.intersection(minions)) >= len(minions):
            raise StopIteration()

        # otherwise, get them from the event system
        for event in event_iter:
            if event != {}:
                found.update(set(event))
                yield event
            if len(found.intersection(minions)) >= len(minions):
                self._clean_up_subscriptions(jid)
                raise StopIteration()
--------------------
Prompt: 
def get_plural(amount, variants, absence=None):
    """
    Get proper case with value

    @param amount: amount of objects
    @type amount: C{integer types}

    @param variants: variants (forms) of object in such form:
        (1 object, 2 objects, 5 objects).
    @type variants: 3-element C{sequence} of C{unicode}
        or C{unicode} (three variants with delimeter ',')

    @param absence: if amount is zero will return it
    @type absence: C{unicode}

    @return: amount with proper variant
    @rtype: C{unicode}
    """
----------
Output: 

    @rtype: C{integer types}
    """
    if absence is None:
        absence = "zero"
    if absence == "zero":
        return amount
    if amount == 0:
        return 0
    if amount == 1:
        return 1
    if amount == 2:
        return 2
    if amount == 5:
        return 5
    if amount < 0:
        raise ValueError("amount must be non-negative")
    if amount > 1:
        raise ValueError("amount must be greater than 1")
    if amount > 2:
        raise ValueError("amount must be greater than 2")
    if amount > 5
----------
Solution: 

    if amount or absence is None:
        return u"%d %s" % (amount, choose_plural(amount, variants))
    else:
        return absence
--------------------
Prompt: 
def flash(self, partition, timeout_ms=None, info_cb=DEFAULT_MESSAGE_CALLBACK):
    """Flashes the last downloaded file to the given partition.

    Args:
      partition: Partition to flash.
      timeout_ms: Optional timeout in milliseconds to wait for it to finish.
      info_cb: See Download. Usually no messages.

    Returns:
      Response to a download request, normally nothing.
    """
----------
Output: 

    if timeout_ms is None:
      timeout_ms = self.default_message_callback_ms
    if info_cb is None:
      info_cb = self.default_message_callback_cb
    if info_cb is None:
      info_cb = self.default_message_callback_ms
    if partition.size > 0:
      self.size = partition.size
    else:
      self.size = 0
    self.download_start_time = time.time()
    self.download_end_time = time.time()
    self.download_time = time.time()
----------
Solution: 

    return self._simple_command('flash', arg=partition, info_cb=info_cb,
                                timeout_ms=timeout_ms)
--------------------
Prompt: 
def map(requests, stream=False, size=None, exception_handler=None, gtimeout=None):
    """Concurrently converts a list of Requests to Responses.

    :param requests: a collection of Request objects.
    :param stream: If True, the content will not be downloaded immediately.
    :param size: Specifies the number of requests to make at a time. If None, no throttling occurs.
    :param exception_handler: Callback function, called when exception occured. Params: Request, Exception
    :param gtimeout: Gevent joinall timeout in seconds. (Note: unrelated to requests timeout)
    """
----------
Output: 
 to the Gevent API)
    :return: a collection of Response objects.
    """
    if size is None:
        size = requests.size()
    if exception_handler is None:
        exception_handler = lambda req: None
    if not stream:
        return collection_response(requests, exception_handler)
    else:
        return collection_response(requests, exception_handler, gtimeout)


def map_async(requests, stream=False, size=None, exception_handler=None):
    """Concurrently converts a list of Requests to Responses.

    :param requests: a collection of Request
----------
Solution: 


    requests = list(requests)

    pool = Pool(size) if size else None
    jobs = [send(r, pool, stream=stream) for r in requests]
    gevent.joinall(jobs, timeout=gtimeout)

    ret = []

    for request in requests:
        if request.response is not None:
            ret.append(request.response)
        elif exception_handler and hasattr(request, 'exception'):
            ret.append(exception_handler(request, request.exception))
        else:
            ret.append(None)

    return ret
--------------------
Prompt: 
def select_projects(self, *args):
        """Copy the query and add filtering by monitored projects.

        This is only useful if the target project represents a Stackdriver
        account containing the specified monitored projects.

        Examples::

            query = query.select_projects('project-1')
            query = query.select_projects('project-1', 'project-2')

        :type args: tuple
        :param args: Project IDs limiting the resources to be included
            in the query.

        :rtype: :class:`Query`
        :returns: The new query object.
        """
----------
Output: 

        return self._client.select_projects(*args)

    def get_project(self, project_id):
        """Get a specific target project.

        :type project_id: str
        :param project_id: The ID of the target project.

        :rtype: :class:`boto.cloudsearch2.search.Resource`
        :returns: The ID of the target project.
        """
        return self._client.get_project(project_id)

    def get_project_policy(self, project_id):
        """Get the current policy for a specific target project.

        :type project
----------
Solution: 

        new_query = copy.deepcopy(self)
        new_query._filter.projects = args
        return new_query
--------------------
Prompt: 
def encode(self,
               data: mx.sym.Symbol,
               data_length: mx.sym.Symbol,
               seq_len: int) -> Tuple[mx.sym.Symbol, mx.sym.Symbol, int]:
        """
        Encodes data given sequence lengths of individual examples and maximum sequence length.

        :param data: Input data.
        :param data_length: Vector with sequence lengths.
        :param seq_len: Maximum sequence length.
        :return: Encoded versions of input data (data, data_length, seq_len).
        """
----------
Output: 

        if self.is_training:
            return self._encode_training(data, data_length, seq_len)
        elif self.is_inference:
            return self._encode_inference(data, seq_len)
        elif self.is_inference_with_length:
            return self._encode_inference_with_length(data, data_length, seq_len)
        else:
            raise NotImplementedError()

    def _encode_training(self, data: mx.sym.Symbol, seq_len: int) -> Tuple[mx.sym.Symbol, int]:
        """
        Encodes
----------
Solution: 

        if self.layout[0] == 'N':
            data = mx.sym.swapaxes(data=data, dim1=0, dim2=1)
        data = self._encode(data, data_length, seq_len)
        if self.layout[0] == 'N':
            data = mx.sym.swapaxes(data=data, dim1=0, dim2=1)
        return data, data_length, seq_len
--------------------
Prompt: 
def destroy_unit(self, unit):
        """Delete a unit from the cluster

        Args:
            unit (str, Unit): The Unit, or name of the unit to delete

        Returns:
            True: The unit was deleted

        Raises:
            fleet.v1.errors.APIError: Fleet returned a response code >= 400

        """
----------
Output: 

        url = "/api/v1/clusters/{0}/units/{1}".format(self.cluster_id, unit)
        return self.client.delete(url)

    def get_cluster_units(self, cluster):
        """Get a list of all units for a cluster

        Args:
            cluster (str): The name of the cluster

        Returns:
            (list of Unit): The list of all units for the cluster

        Raises:
            fleet.v1.errors.APIError: Fleet returned a response code >= 400

        """
        return self.client.get("/api/v
----------
Solution: 


        # if we are given an object, grab it's name property
        # otherwise, convert to unicode
        if isinstance(unit, Unit):
            unit = unit.name
        else:
            unit = str(unit)

        self._single_request('Units.Delete', unitName=unit)
        return True
--------------------
Prompt: 
def make_or_augment_meta(self, role):
        """
        Create or augment a meta file.
        """
----------
Output: 

        if role == 'all':
            self.log.info("Generating all files for role: %s", role)
            self.gen_all_files(self.config_dir)
        elif role =='meta':
            self.log.info("Generating meta files for role: %s", role)
            self.gen_meta_files(self.config_dir)
        else:
            self.log.info("Unknown meta role: %s", role)

    def gen_all_files(self, dir):
        """
        Generate all files in a directory.
        """
        for filename in os
----------
Solution: 

        if not os.path.exists(self.paths["meta"]):
            utils.create_meta_main(self.paths["meta"], self.config, role, "")
            self.report["state"]["ok_role"] += 1
            self.report["roles"][role]["state"] = "ok"

        # swap values in place to use the config values
        swaps = [
            ("author", self.config["author_name"]),
            ("company", self.config["author_company"]),
            ("license", self.config["license_type"]),
        ]

        (new_meta, _) = utils.swap_yaml_string(self.paths["meta"], swaps)

        # normalize the --- at the top of the file by removing it first
        new_meta = new_meta.replace("---", "")
        new_meta = new_meta.lstrip()

        # augment missing main keys
        augments = [
            ("ansigenome_info", "{}"),
            ("galaxy_info", "{}"),
            ("dependencies", "[]"),
        ]

        new_meta = self.augment_main_keys(augments, new_meta)

        # re-attach the ---
        new_meta = "---\n\n" + new_meta

        travis_path = os.path.join(self.paths["role"], ".travis.yml")
        if os.path.exists(travis_path):
            new_meta = new_meta.replace("travis: False", "travis: True")

        utils.string_to_file(self.paths["meta"], new_meta)
--------------------
Prompt: 
def parse_args():
    """
    Argument parser and validator
    :return: args
    """
----------
Output: 

    parser = argparse.ArgumentParser(description='A simple script to generate a new project')
    parser.add_argument('-p', '--project', help='The name of the project', required=True)
    parser.add_argument('-f', '--file', help='The name of the file', required=True)
    parser.add_argument('-s', '--source', help='The name of the source', required=True)
    parser.add_argument('-t', '--type', help='The type of project', required=True)
    parser.add_argument('-p', '--path', help='The
----------
Solution: 

    parser = argparse.ArgumentParser(description="Converts, and downloads a vm by name from vCenter to OVF in specified"
                                                 " directory, then uploads the image as an AMI. AMI will be uploaded "
                                                 "using specified AWS profile, to specified regions. ")
    parser.add_argument('-i', '--vcenter_host', type=str, required=True, help='Hostname or Ip of vCenter API of VM')
    parser.add_argument('-u', '--vcenter_user', type=str, required=True, help='Username for vCenter authentication')
    parser.add_argument('-p', '--vcenter_pass', type=str, required=True, help='Password for authentication to vCenter API')
    parser.add_argument('-n', '--vm_name', type=str, required=True, help='Name of the VM in vCenter')
    parser.add_argument('-d', '--directory', type=str, default=tempfile.mkdtemp(),
                        help='Directory to save the vmdk temp file (defaults to temp location')
    parser.add_argument('-w', '--vcenter_port', type=str, default='443',
                        help='Port to use for communication to vcenter api. Default is 443')
    args = parser.parse_args()
    validate_args(args)
    return args
--------------------
Prompt: 
def _readPPN(self, fname, sldir):
        """
        Private method that reads in and organizes the .ppn file
        Loads the data of the .ppn file into the variable cols.

        """
----------
Output: 

        # read the.ppn file
        try:
            f = open(fname, 'r')
        except IOError:
            return
        # read the.ppn file header
        try:
            header = f.readline()
        except IOError:
            return
        # read the.ppn file data
        f.seek(0, 2)
        # read the.ppn file data
        for line in f:
            # skip comments
            if line.startswith('#'):
                continue
            # skip blank lines
            if not line.strip():
                continue
            # split the line
            lineSplit = line.split()
            # get
----------
Solution: 

        if sldir.endswith(os.sep):
                    #Making sure fname will be formatted correctly
            fname = str(sldir)+str(fname)
        else:
            fname = str(sldir)+os.sep+str(fname)
            self.sldir+=os.sep
        f=open(fname,'r')
        lines=f.readlines()
        for i in range(len(lines)):
            lines[i]=lines[i].strip()

        cols = ['ISOTP', 'ABUNDANCE_MF'] #These are constant, .ppn files have no header to read from
        for i in range(len(lines)):
            if not lines[i].startswith('H'):
                index = i-1
                break

        return cols, index
--------------------
Prompt: 
def fix_e711(self, result):
        """Fix comparison with None."""
----------
Output: 

        if result is None:
            return
        if result is False:
            return
        if result is None:
            return
        if result is True:
            return
        if result is False:
            assert False, "Unknown comparison type"
        for key, value in result.items():
            if value is None:
                del self.data[key]
            else:
                if key in self.data and self.data[key] == value:
                    del self.data[key]

    def test_e712(self):
        """Test that the 'None' value is replaced with 'None' when setting to None."""
----------
Solution: 

        (line_index, offset, target) = get_index_offset_contents(result,
                                                                 self.source)

        right_offset = offset + 2
        if right_offset >= len(target):
            return []

        left = target[:offset].rstrip()
        center = target[offset:right_offset]
        right = target[right_offset:].lstrip()

        if not right.startswith('None'):
            return []

        if center.strip() == '==':
            new_center = 'is'
        elif center.strip() == '!=':
            new_center = 'is not'
        else:
            return []

        self.source[line_index] = ' '.join([left, new_center, right])
--------------------
Prompt: 
def register_offchain_secret(
        channel_state: NettingChannelState,
        secret: Secret,
        secrethash: SecretHash,
) -> None:
    """This will register the secret and set the lock to the unlocked stated.

    Even though the lock is unlock it is *not* claimed. The capacity will
    increase once the next balance proof is received.
    """
----------
Output: 

    if not isinstance(secret, SecretHash):
        raise TypeError("offchain_secret must be a SecretHash")
    if not isinstance(secrethash, SecretHash):
        raise TypeError("offchain_secrethash must be a SecretHash")

    if secrethash.lock is not None:
        raise ValueError("The lock must be a NettingSecrethash")
    if secrethash.unlock is not None:
        raise ValueError("The unlock must be a NettingSecrethash")

    if not secrethash.is_locked:
        raise ValueError("The lock
----------
Solution: 

    our_state = channel_state.our_state
    partner_state = channel_state.partner_state

    register_secret_endstate(our_state, secret, secrethash)
    register_secret_endstate(partner_state, secret, secrethash)
--------------------
Prompt: 
def next_id(self, subfolder):
        """
        ProjectFileEvent Write to File Method
        """
----------
Output: 

        self.project_file_events.append(subfolder)
        return subfolder

    def write_file(self, subfolder, file_name):
        """
        ProjectFileEvent Write to File
        """
        self.project_file_events.append(subfolder)
        self.project_file_methods.append(file_name)
        return subfolder

    def get_folder(self):
        """
        ProjectFileEvent Get Folder
        """
        return self.project_file_methods

    def get_id(self):
        """
        ProjectFileEvent Get ID
        """
        return self.write_
----------
Solution: 

        evt_sim_folders = self.events.filter(
                        ProjectFileEvent.subfolder
                            .like("{0}_%".format(subfolder))
                    )
        max_id = 0
        num_search = re.compile(r'{0}_(\d+)'.format(subfolder), re.IGNORECASE)
        for prj_event in evt_sim_folders:
            found_num = num_search.findall(prj_event.subfolder)
            if found_num is not None:
                max_id = max(max_id, int(found_num[0]))
        return max_id + 1
--------------------
Prompt: 
def do_directives(self, line):
        """List all directives supported by the bot"""
----------
Output: 

        if line.startswith('#'):
            return
        self.do_list(line)

    def do_list(self, line):
        """List all commands supported by the bot"""
        if line.startswith('PING'):
            self.do_ping(line)
        elif line.startswith('PONG'):
            self.do_pong(line)
        elif line.startswith('JOIN'):
            self.do_join(line)
        elif line.startswith('PART'):
            self.do_part(line)
        elif line.startswith('PRIVMSG'):
            self.do_privmsg(line)
        elif
----------
Solution: 

        for name, cmd in self.adapter.directives.items():
            with colorize('blue'):
                print('bot %s:' % name)
                if cmd.__doc__:
                    for line in cmd.__doc__.split('\n'):
                        print('  %s' % line)
                else:
                    print()
--------------------
Prompt: 
def insert(self, schema, fields, **kwargs):
        """
        Persist d into the db

        schema -- Schema()
        fields -- dict -- the values to persist

        return -- int -- the primary key of the row just inserted
        """
----------
Output: 

        if not self.db.has_key(schema):
            raise KeyError("No such schema: %s" % schema)

        if not self.db[schema].has_key(fields):
            self.db[schema][fields] = {}

        self.db[schema][fields][d_key] = fields_to_int(fields)
        self.db[schema][fields][d_key]["index"] = fields_to_int(fields)
        self.db[schema][fields][d_key]["type"] = "int"

        return self.db[schema][fields][
----------
Solution: 

        r = 0

        with self.connection(**kwargs) as connection:
            kwargs['connection'] = connection
            try:
                with self.transaction(**kwargs):
                    r = self._insert(schema, fields, **kwargs)

            except Exception as e:
                exc_info = sys.exc_info()
                if self.handle_error(schema, e, **kwargs):
                    r = self._insert(schema, fields, **kwargs)
                else:
                    self.raise_error(e, exc_info)

        return r
--------------------
Prompt: 
def i3extract(infile, outfile=None):
    """Main event loop"""
----------
Output: 

    if outfile is None:
        outfile = sys.stdout
    else:
        outfile = open(outfile, "w")
    if not outfile:
        outfile.write(sys.stdout)
    for i in range(1, len(infile)):
        line = infile[i]
        if line.startswith("BEGIN:VCALENDAR"):
            if line.startswith("END:VCALENDAR"):
                break
            outfile.write(line)
        elif line.startswith("BEGIN:VECTOR"):
            if line.startswith("BEGIN:VCALENDAR"):
                outfile.write("BEGIN:VECTOR
----------
Solution: 

    if outfile is None:
        outfile = infile + '.h5'
    tray = I3Tray()
    tray.AddModule('I3Reader', 'i3_reader', filename=infile)
    # tray.AddModule(KeepReconstructed, "event_selector")
    tray.AddModule(ReadEventMeta, 'read_meta')    # grab the event ID
    tray.AddModule(
        Distance,
        "compare_space",
        particle_1="best_FirstDusjOrcaVertexFit_FitResult",
        particle_2="best_SecondDusjOrcaVertexFit_FitResult"
    )
    tray.AddModule(
        TimeDistance,
        "compare_time",
        particle_1="best_FirstDusjOrcaVertexFit_FitResult",
        particle_2="best_SecondDusjOrcaVertexFit_FitResult",
    )
    tray.AddModule(
        ReadRecoParticle,
        'read_particle_first',
        ParticleName='best_FirstDusjOrcaVertexFit_FitResult'
    )
    tray.AddModule(
        ReadRecoParticle,
        'read_particle_second',
        ParticleName='best_SecondDusjOrcaVertexFit_FitResult'
    )
    tray.AddModule(
        ReadRecoParticle,
        'read_particle_proba',
        ParticleName='best_DusjOrcaUsingProbabilitiesFinalFit_FitResult'
    )    # noqa
    tray.AddModule(
        ReadLLHValues,
        'read_llh_chere',
        LLHParamContainer=
        'best_DusjOrcaUsingProbabilitiesFinalFit_FitResult_FinalLLHValues'
    )    # noqa
    tray.AddModule(WriteScalars, 'write_scalars', filename=outfile)
    tray.AddModule('TrashCan', 'dustbin')
    tray.Execute()
    tray.Finish()
--------------------
Prompt: 
def Fold(seglist1, seglist2):
	"""
	An iterator that generates the results of taking the intersection
	of seglist1 with each segment in seglist2 in turn.  In each result,
	the segment start and stop values are adjusted to be with respect
	to the start of the corresponding segment in seglist2.  See also
	the segmentlist_range() function.

	This has use in applications that wish to convert ranges of values
	to ranges relative to epoch boundaries.  Below, a list of time
	intervals in hours is converted to a sequence of daily interval
	lists with times relative to midnight.

	Example:

	>>> from pycbc_glue.segments import *
	>>> x = segmentlist([segment(0, 13), segment(14, 20), segment(22, 36)])
	>>> for y in Fold(x, segmentlist_range(0, 48, 24)): print y
	...
	[segment(0, 13), segment(14, 20), segment(22, 24)]
	[segment(0, 12)]
	"""
----------
Output: 
 seconds.
	"""
	for i in range(0, len(seglist1)):
		if seglist1[i] <= seglist2[i]:
			yield (i, seglist1[i], seglist2[i], seglist2[i] - seglist1[i])
	#
	#
	# The following code is a bit of a hack.  It is not
	# very efficient to use the list comprehension to generate
	# the results of the function:
	#
	#   for i in range(0, len(
----------
Solution: 

	for seg in seglist2:
		yield (seglist1 & segments.segmentlist([seg])).shift(-seg[0])
--------------------
Prompt: 
def initialize(self, io_loop):
        """ Start a Bokeh Server Tornado Application on a given Tornado IOLoop.

        """
----------
Output: 

        self.io_loop = io_loop

    def open(self, url, **kwargs):
        """ Open a Bokeh server.

        """
        self.url = url
        self.io_loop.add_callback(self._open_callback, **kwargs)

    def _open_callback(self, callback, **kwargs):
        """ Callback for when a Bokeh server sends a request.

        """
        if callback is not None:
            self.io_loop.add_callback(callback, **kwargs)

    def _open_error(self, error, url):
        """ Callback for when a
----------
Solution: 

        self._loop = io_loop

        for app_context in self._applications.values():
            app_context._loop = self._loop

        self._clients = set()

        self._stats_job = PeriodicCallback(self._log_stats,
                                           self._stats_log_frequency_milliseconds)

        if self._mem_log_frequency_milliseconds > 0:
            self._mem_job = PeriodicCallback(self._log_mem,
                                             self._mem_log_frequency_milliseconds)
        else:
            self._mem_job = None


        self._cleanup_job = PeriodicCallback(self._cleanup_sessions,
                                             self._check_unused_sessions_milliseconds)

        if self._keep_alive_milliseconds > 0:
            self._ping_job = PeriodicCallback(self._keep_alive, self._keep_alive_milliseconds)
        else:
            self._ping_job = None
--------------------
Prompt: 
def serialize(self, resources):
        """Serialize resource(s) according to json-api spec."""
----------
Output: 

        return json.dumps(resources, cls=JSONEncoder)

    def deserialize(self, data):
        """Deserialize resource(s) according to json-api spec."""
        return json.loads(data)

    def get_resource(self, resource):
        """Get a resource from the resource specification."""
        return self._get_resource(resource, 'data')

    def get_resource_type(self, resource_type):
        """Get a resource type from the resource specification."""
        return self._get_resource_type(resource_type, 'data')

    def _get_resource(self, resource,
----------
Solution: 

        serialized = {
            'meta': {
                'sqlalchemy_jsonapi_version': '4.0.9'
            },
            'jsonapi': {
                'version': '1.0'
            }
        }
        # Determine multiple resources by checking for SQLAlchemy query count.
        if hasattr(resources, 'count'):
            serialized['data'] = []
            for resource in resources:
                serialized['data'].append(
                    self._render_resource(resource))
        else:
            serialized['data'] = self._render_resource(resources)

        return serialized
--------------------
Prompt: 
def changelog(**kwargs):
    """
    Generates the changelog since the last release.
    :raises ImproperConfigurationError: if there is no current version
    """
----------
Output: 

    if 'changelog' not in kwargs:
        raise ImproperConfigurationError('changelog is missing "changelog" section')
    if'version' not in kwargs:
        raise ImproperConfigurationError('version is missing "version" section')
    if 'changelog' not in kwargs:
        raise ImproperConfigurationError('changelog is missing "changelog" section')
    if 'log_file' not in kwargs:
        raise ImproperConfigurationError('log_file is missing "log_file" section')
    if 'log_file_size' not in kwargs:
        raise ImproperConfigurationError('
----------
Solution: 

    current_version = get_current_version()
    debug('changelog got current_version', current_version)

    if current_version is None:
        raise ImproperConfigurationError(
            "Unable to get the current version. "
            "Make sure semantic_release.version_variable "
            "is setup correctly"
        )
    previous_version = get_previous_version(current_version)
    debug('changelog got previous_version', previous_version)

    log = generate_changelog(previous_version, current_version)
    click.echo(markdown_changelog(current_version, log, header=False))

    debug('noop={}, post={}'.format(kwargs.get('noop'), kwargs.get('post')))
    if not kwargs.get('noop') and kwargs.get('post'):
        if check_token():
            owner, name = get_repository_owner_and_name()
            click.echo('Updating changelog')
            post_changelog(
                owner,
                name,
                current_version,
                markdown_changelog(current_version, log, header=False)
            )
        else:
            click.echo(
                click.style('Missing token: cannot post changelog', 'red'), err=True)
--------------------
Prompt: 
def mason_morrow(target, throat_perimeter='throat.perimeter',
                 throat_area='throat.area'):
    r"""
    Mason and Morrow relate the capillary pressure to the shaped factor in a
    similar way to Mortensen but for triangles.

    References
    ----------
    Mason, G. and Morrow, N.R.. Capillary behavior of a perfectly wetting
    liquid in irregular triangular tubes. Journal of Colloid and Interface
    Science, 141(1), pp.262-274 (1991).
    """
----------
Output: 
 pp. 1-13.

    Examples
    --------
    >>> from scipy.spatial import Delaunay
    >>> w = Delaunay(90, [0,0,1], [-1,1,1])
    >>> w.as_matrix()
    array([[  0.,   98.,   98.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.],
           [  0.,   0.,   0.,   0.,   0.],
           [  0.,
----------
Solution: 

    # Only apply to throats with an area
    ts = target.throats()[target[throat_area] <= 0]
    P = target[throat_perimeter]
    A = target[throat_area]
    value = A/(P**2)
    value[ts] = 1/(4*_sp.pi)
    return value
--------------------
Prompt: 
def Add(self, key, help="", default=None, validator=None, converter=None, **kw):
        """
        Add an option.


        @param key: the name of the variable, or a list or tuple of arguments
        @param help: optional help text for the options
        @param default: optional default value
        @param validator: optional function that is called to validate the option's value
        @type validator: Called with (key, value, environment)
        @param converter: optional function that is called to convert the option's value before putting it in the environment.
        """
----------
Output: 

        if isinstance(key, types.ListType):
            for i in key:
                if not i in self.options:
                    self.options[i] = environment.ErrorVariable("Option '%s' not found" % (i))
                if not help:
                    help = self.options[i]
                if not validator:
                    validator = self.options[i][0]
                self.options[i] = environment.EnvironmentVariable(i, converter=converter)
        elif isinstance(key, types.TupleType):
            self.options[key[0]] = key[1]
        else:
            raise
----------
Solution: 


        if SCons.Util.is_List(key) or isinstance(key, tuple):
            self._do_add(*key)
            return

        if not SCons.Util.is_String(key) or \
            not SCons.Environment.is_valid_construction_var(key):
                raise SCons.Errors.UserError("Illegal Variables.Add() key `%s'" % str(key))

        self._do_add(key, help, default, validator, converter)
--------------------
Prompt: 
def query_gfy(self, gfyname):
        """
        Query a gfy name for URLs and more information.
        """
----------
Output: 

        return self.gfy_url(gfyname)

    def gfy_url(self, gfyname):
        """
        Return the URL for a gfy.
        """
        return gfyname.replace('http://', 'https://')

    def gfy_url_for_file(self, gfyname, filename):
        """
        Return the URL for a gfy.
        """
        return self.gfy_url(filename)

    def gfy_url_for_stream(self, gfyname, filename):
        """
        Return the URL for a gfy.
       
----------
Solution: 

        self.check_token()
        
        r = requests.get(QUERY_ENDPOINT + gfyname, headers=self.headers)
        
        response = r.json()
        
        if r.status_code != 200 and not ERROR_KEY in response:
            raise GfycatClientError('Bad response from Gfycat',
                                    r.status_code)
        elif ERROR_KEY in response:
            raise GfycatClientError(response[ERROR_KEY], r.status_code)
        
        return response
--------------------
Prompt: 
def is_sub_dict(sub_dict, dictionary):
    """Legacy filter for determining if a given dict is present."""
----------
Output: 

    return (
        isinstance(sub_dict, dict)
        and all(is_sub_dict(sub_dict, dictionary) for sub_dict in dictionary.values())
    )


def _filter_state(state, dictionary):
    """Filter out the state."""
    if state.state == STATE_ON:
        return _filter_state(dictionary, state.attributes)
    if state.state == STATE_OFF:
        return _filter_state(dictionary, state.attributes)


def _filter_attributes(state, attributes):
    """Filter out the attributes."""
    if state.attributes
----------
Solution: 

    for key in sub_dict.keys():
        if key not in dictionary:
            return False
        if (type(sub_dict[key]) is not dict) and (sub_dict[key] != dictionary[key]):
            return False
        if (type(sub_dict[key]) is dict) and (not is_sub_dict(sub_dict[key], dictionary[key])):
            return False
    return True
--------------------
Prompt: 
def _ext_pillar(minion_id,
                root_dir,
                follow_dir_links,
                debug,
                keep_newline,
                render_default,
                renderer_blacklist,
                renderer_whitelist,
                template):
    """
    Compile pillar data for a single root_dir for the specified minion ID
    """
----------
Output: 

    # Get the path to the minion
    path = os.path.join(root_dir,'minion.txt')

    # Compile the data
    command = [
        'java',
        '-jar',
        path,
        '-m',
        '-cp',
        str(minion_id),
        str(path),
    ]

    # Add the command line arguments
    if debug:
        command.append('-v')
    if minion_id_file:
        command.append(str(minion_id_file))
    if minion_id_file_path:
        command.append(
----------
Solution: 

    log.debug('file_tree: reading %s', root_dir)

    if not os.path.isdir(root_dir):
        log.error(
            'file_tree: root_dir %s does not exist or is not a directory',
            root_dir
        )
        return {}

    if not isinstance(keep_newline, (bool, list)):
        log.error(
            'file_tree: keep_newline must be either True/False or a list '
            'of file globs. Skipping this ext_pillar for root_dir %s',
            root_dir
        )
        return {}

    ngroup_pillar = {}
    nodegroups_dir = os.path.join(root_dir, 'nodegroups')
    if os.path.exists(nodegroups_dir) and __opts__.get('nodegroups'):
        master_ngroups = __opts__['nodegroups']
        ext_pillar_dirs = os.listdir(nodegroups_dir)
        if ext_pillar_dirs:
            for nodegroup in ext_pillar_dirs:
                if (os.path.isdir(nodegroups_dir) and
                        nodegroup in master_ngroups):
                    ckminions = salt.utils.minions.CkMinions(__opts__)
                    _res = ckminions.check_minions(
                        master_ngroups[nodegroup],
                        'compound')
                    match = _res['minions']
                    if minion_id in match:
                        ngroup_dir = os.path.join(
                            nodegroups_dir, six.text_type(nodegroup))
                        ngroup_pillar = salt.utils.dictupdate.merge(ngroup_pillar,
                            _construct_pillar(ngroup_dir,
                                              follow_dir_links,
                                              keep_newline,
                                              render_default,
                                              renderer_blacklist,
                                              renderer_whitelist,
                                              template),
                            strategy='recurse'
                        )
        else:
            if debug is True:
                log.debug(
                    'file_tree: no nodegroups found in file tree directory %s, skipping...',
                    ext_pillar_dirs
                )
    else:
        if debug is True:
            log.debug('file_tree: no nodegroups found in master configuration')

    host_dir = os.path.join(root_dir, 'hosts', minion_id)
    if not os.path.exists(host_dir):
        if debug is True:
            log.debug(
                'file_tree: no pillar data for minion %s found in file tree directory %s',
                minion_id,
                host_dir
            )
        return ngroup_pillar

    if not os.path.isdir(host_dir):
        log.error('file_tree: %s exists, but is not a directory', host_dir)
        return ngroup_pillar

    host_pillar = _construct_pillar(host_dir,
                                    follow_dir_links,
                                    keep_newline,
                                    render_default,
                                    renderer_blacklist,
                                    renderer_whitelist,
                                    template)
    return salt.utils.dictupdate.merge(ngroup_pillar,
                                       host_pillar,
                                       strategy='recurse')
--------------------
Prompt: 
def pkg_tracking(self):
        """Tracking package dependencies
        """
----------
Output: 

        if self.package_dependencies is None:
            self.package_dependencies = []
        self.package_dependencies.append(self.package_name)
        self.package_dependencies.append(self.package_version)
        self.package_dependencies.append(self.package_dependencies_id)
        self.package_dependencies.append(self.package_dependencies_name)
        self.package_dependencies.append(self.package_dependencies_version)
        self.package_name = None
        self.package_version = None

    def __str__(self):
        """String representation of the
----------
Solution: 

        flag = []
        options = [
            "-t",
            "--tracking"
        ]
        additional_options = [
            "--check-deps",
            "--graph=",
            "--case-ins"
        ]
        for arg in self.args[2:]:
            if arg.startswith(additional_options[1]):
                flag.append(arg)
                self.args.remove(arg)
            if arg in additional_options:
                flag.append(arg)
        # clean additional options from args
        for f in flag:
            if f in self.args:
                self.args.remove(f)
        # print usage message if wrong additional option
        for arg in self.args:
            if arg.startswith("--"):
                if arg not in additional_options:
                    usage("")
                    raise SystemExit()
        if (len(self.args) >= 3 and len(self.args) <= 3 and
                self.args[0] in options and
                self.args[1] in self.meta.repositories):
            TrackingDeps(self.args[2], self.args[1], flag).run()
        elif (len(self.args) >= 2 and
                self.args[1] not in self.meta.repositories):
            usage(self.args[1])
        else:
            usage("")
--------------------
Prompt: 
def create_deeper(self,
                      new_t1,
                      new_t2,
                      child_relationship_class,
                      child_relationship_param=None,
                      report_type=None):
        """
        Start a new comparison level and correctly link it to this one.
        :rtype: DiffLevel
        :return: New level
        """
----------
Output: 

        if not child_relationship_param:
            child_relationship_param = {}
        if not child_relationship_class:
            child_relationship_class = {}
        if not child_relationship_param.get('parent_relationship'):
            child_relationship_param['parent_relationship'] = child_relationship_param.get('parent_relationship_param', {})
        if not report_type:
            report_type = 'default'
        if report_type not in self.report_types:
            raise ValueError('Unknown report type: %s' % report_type)
        if report_type == 'default
----------
Solution: 

        level = self.all_down
        result = DiffLevel(
            new_t1, new_t2, down=None, up=level, report_type=report_type)
        level.down = result
        level.auto_generate_child_rel(
            klass=child_relationship_class, param=child_relationship_param)
        return result
--------------------
Prompt: 
def global_id_field(type_name, id_fetcher=None):
    """
    Creates the configuration for an id field on a node, using `to_global_id` to
    construct the ID from the provided typename. The type-specific ID is fetcher
    by calling id_fetcher on the object, or if not provided, by accessing the `id`
    property on the object.
    """
----------
Output: 

    if id_fetcher is None:
        id_fetcher = id_gen.IdGen()
    if not id_fetcher.is_node():
        id_fetcher.set_id(global_id_field_name(type_name, id_fetcher))
    return id_fetcher.to_global_id(type_name)

def global_id_field_name(type_name, id_fetcher=None):
    """
    Creates the configuration for an id field on a node, using `to_global_id` to
    construct the ID from the provided typename. The type-
----------
Solution: 

    return GraphQLField(
        GraphQLNonNull(GraphQLID),
        description='The ID of an object',
        resolver=lambda obj, args, context, info: to_global_id(
            type_name or info.parent_type.name,
            id_fetcher(obj, context, info) if id_fetcher else obj.id
        )
    )
--------------------
Prompt: 
def dir_on_bezier_curve(P=[(0.0, 0.0)], t=0.5):
    """Return direction at t on bezier curve defined by control points P.
List of vectors per pair of dimensions are returned in radians.
E.g. Where X is "right", Y is "up", Z is "in" on a computer screen, and
  returned value is [pi/4, -pi/4], then the vector will be coming out the
  screen over the viewer's right shoulder.
    """
----------
Output: 

    return P[0] * math.pi / 180.0 * math.asin(t), P[1] * math.pi / 180.0 * math.acos(t), P[2] * math.pi / 180.0

def dir_on_bezier_curve_old(P=[(0.0, 0.0)], t=0.5):
    """Return direction at t on bezier curve defined by control points P.
List of radians per pair of control points is returned in radians.
E.g. Where X is "left", Y is
----------
Solution: 

    assert isinstance(P, list)
    assert len(P) > 0
    if not len(P) > 1:
        return None # Points have no gradient.
    for p in P:
        assert isinstance(p, tuple)
        for i in p:
            assert len(p) > 1
            assert isinstance(i, float)
    assert isinstance(t, float)
    assert 0 <= t <= 1
    
    O = len(P) - 1 # Order of curve
    
    # Recurse down the orders calculating the next set of control points until
    #   there are only two left, which is the points on the gradient we want.
    Q = P
    while O > 1:
        Q = [pt_between_pts(Q[l], Q[l+1], t) for l in range(O)]
        O -= 1
    
    assert len(Q) == 2
    # Now that we have the two points in N dimensions, we can reduce to the
    #   gradients on N-1 planes.
    q0 = Q[0]
    q1 = Q[1]
    
    return dir_between_pts(q0, q1)
--------------------
Prompt: 
def datetime_from_ldap(value):
    """Convert a LDAP-style datetime to a Python aware object.

    See https://tools.ietf.org/html/rfc4517#section-3.3.13 for details.

    Args:
        value (str): the datetime to parse
    """
----------
Output: 

    return datetime.datetime.strptime(value, '%Y-%m-%d %H:%M:%S')


def datetime_to_dn(value):
    """Convert a Python aware datetime to a DN.

    See https://tools.ietf.org/html/rfc4517#section-3.3.13 for details.

    Args:
        value (str): the datetime to parse
    """
    return value.strftime('%Y-%m-%d %H:%M:%S')


def dn_to_datetime(dn):
    """Convert a DN to a Python aware object.

    Args:
----------
Solution: 

    if not value:
        return None
    match = LDAP_DATETIME_RE.match(value)
    if not match:
        return None
    groups = match.groupdict()
    if groups['microsecond']:
        groups['microsecond'] = groups['microsecond'].ljust(6, '0')[:6]
    tzinfo = groups.pop('tzinfo')
    if tzinfo == 'Z':
        tzinfo = timezone.utc
    else:
        offset_mins = int(tzinfo[-2:]) if len(tzinfo) == 5 else 0
        offset = 60 * int(tzinfo[1:3]) + offset_mins
        if tzinfo[0] == '-':
            offset = - offset
        tzinfo = timezone.get_fixed_timezone(offset)
    kwargs = {k: int(v) for k, v in groups.items() if v is not None}
    kwargs['tzinfo'] = tzinfo
    return datetime.datetime(**kwargs)
--------------------
Prompt: 
def Setup():
    """
    Called when the code is installed. Sets up directories and downloads
    the K2 catalog.

    """
----------
Output: 

    global _k2_dir
    global _k2_dir_download
    global _k2_dir_download_dir
    global _k2_dir_download_dir_name
    global _k2_dir_download_dir = os.path.join(os.path.dirname(__file__), 'data')
    if not os.path.exists(_k2_dir_dir):
        os.makedirs(_k2_dir_dir)
    if not os.path.exists(_k2_dir_download):
        os.makedirs(_k2_dir_download_dir)
   
----------
Solution: 


    if not os.path.exists(os.path.join(EVEREST_DAT, 'k2', 'cbv')):
        os.makedirs(os.path.join(EVEREST_DAT, 'k2', 'cbv'))
    GetK2Stars(clobber=False)
--------------------
Prompt: 
def solid_angles(self, permutation=None):
        """
        Returns the list of "perfect" solid angles Each edge is given as a
        list of its end vertices coordinates.
        """
----------
Output: 

        if permutation is None:
            return self._solid_angles
        else:
            return self._permutation_solid_angles(permutation)

    def _solid_angles_from_list(self, list_of_solid_angles):
        """
        Returns the list of "perfect" solid angles Each edge is given
        as a list of its end vertices coordinates.
        """
        if permutation is None:
            return list_of_solid_angles
        else:
            return list_of_solid_angles_from

    def _permutation_solid_angles(self, permutation):
        """
        Returns the list of "
----------
Solution: 

        if permutation is None:
            return self._solid_angles
        else:
            return [self._solid_angles[ii] for ii in permutation]
--------------------
Prompt: 
def mavlink_packet(self, m):
        """trigger sends from ATTITUDE packets"""
----------
Output: 

        if m.get_type() == "ATTITUDE":
            self.mav.mav.mission_item_send(
                0, # time_boot_ms
                    1, # target system
                    mavutil.mavlink.MAV_FRAME_GLOBAL_RELATIVE_ALT,
                    0, # target component
                    mavutil.mavlink.MAV_FRAME_GLOBAL_INT, # x
                    gpi.MAVLink_message(0, 0, mavutil.mavlink.MAV_CMD_NAV_FENCE_RETURN_POINT, 0, 0, 0, 0, 0, 0, 0, 0,
----------
Solution: 

        if not self.have_home and m.get_type() == 'GPS_RAW_INT' and m.fix_type >= 3:
            gen_settings.home_lat = m.lat * 1.0e-7
            gen_settings.home_lon = m.lon * 1.0e-7
            self.have_home = True
            if self.pending_start:
                self.start()
        if m.get_type() != 'ATTITUDE':
            return
        t = self.get_time()
        dt = t - self.last_t
        if dt < 0 or dt > 10:
            self.last_t = t
            return
        if dt > 10 or dt < 0.9:
            return
        self.last_t = t
        for a in self.aircraft:
            if not gen_settings.stop:
                a.update(1.0)
                self.pkt_queue.append(a.pickled())
                while len(self.pkt_queue) > len(self.aircraft)*2:
                    self.pkt_queue.pop(0)
                    
        if self.module('map') is not None and not self.menu_added_map:
            self.menu_added_map = True
            self.module('map').add_menu(self.menu)
--------------------
Prompt: 
def _parse_stack_cfg(content):
    """
    Allow top level cfg to be YAML
    """
----------
Output: 

    try:
        return yaml.load(content)
    except yaml.YAMLError as exc:
        raise AnsibleAssertionError(
            "Error while loading stack_params yaml from %s line %s: %s" % (content.strip(), exc.lineno, exc.message)
        )


class AnsibleAssertionError(AnsibleError):
    pass


class AnsibleOptionsError(AnsibleError):
    pass


class AnsibleInternalError(AnsibleError):
    pass


class AnsibleRuntimeError(AnsibleError):
    pass


class AnsibleConnectionFailure(AnsibleRuntimeError):
    pass


class AnsibleFileNotFound(AnsibleRuntimeError):
    pass



----------
Solution: 

    try:
        obj = salt.utils.yaml.safe_load(content)
        if isinstance(obj, list):
            return obj
    except Exception as e:
        pass
    return content.splitlines()
--------------------
Prompt: 
def photo_url(self):
        """获取话题头像图片地址.

        :return: 话题头像url
        :rtype: str
        """
----------
Output: 

        return self._get_attribute(self.photo_url_field, 'url')

    def _get_attribute(self, field, attribute):
        """获取属性.

        :param field: 话题头像field
        :type: str
        :param attribute: 话题头像attribute
        :type: str
        :return: 话题头像attribute
        :rtype: str
        """
        return self.attributes.get(field, {}).get(attribute, None)

    def _get
----------
Solution: 

        img = self.soup.find('a', id='zh-avartar-edit-form').img['src']
        return img.replace('_m', '_r')
--------------------
Prompt: 
def _remove_brackets(x, i):
    """Removes curly brackets surrounding the Cite element at index 'i' in
    the element list 'x'.  It is assumed that the modifier has been
    extracted.  Empty strings are deleted from 'x'."""
----------
Output: 

    if x[i] == '(':
        x = x[i + 1:-1]
    elif x[i] == '[':
        x = x[i + 1:-1].replace(']', '')
    elif x[i] == ']':
        x = x[i + 1:-1].replace(']', '')
    return x


def _parse_function_call_signature(function_name, line):
    """Parses the 'function call' signature.

    Returns a tuple of:
      - The name of the function
      - The arguments to the function
    """
    # Strip comments
    line =
----------
Solution: 


    assert x[i]['t'] == 'Cite'
    assert i > 0 and i < len(x) - 1

    # Check if the surrounding elements are strings
    if not x[i-1]['t'] == x[i+1]['t'] == 'Str':
        return

    # Trim off curly brackets
    if x[i-1]['c'].endswith('{') and x[i+1]['c'].startswith('}'):
        if len(x[i+1]['c']) > 1:
            x[i+1]['c'] = x[i+1]['c'][1:]
        else:
            del x[i+1]

        if len(x[i-1]['c']) > 1:
            x[i-1]['c'] = x[i-1]['c'][:-1]
        else:
            del x[i-1]
--------------------
Prompt: 
def dict_camel_to_snake_case(camel_dict, convert_keys=True, convert_subkeys=False):
    """
    Recursively convert camelCased keys for a camelCased dict into snake_cased keys

    :param camel_dict: Dictionary to convert
    :param convert_keys: Whether the key should be converted
    :param convert_subkeys: Whether to also convert the subkeys, in case they are named properties of the dict
    :return:
    """
----------
Output: 

    for key, value in camel_dict.items():
        if convert_keys:
            key = convert_camel_to_snake_case(key)
            value = camel_to_snake_case(value)
        if convert_subkeys:
            key = convert_camel_to_snake_case(key)
            value = camel_to_snake_case(value)
        if isinstance(value, dict):
            dict_camel_to_snake_case(value, convert_keys=convert_keys, convert_subkeys=convert_subkeys)
        elif isinstance(value, list):
----------
Solution: 


    converted = {}
    for key, value in camel_dict.items():
        if isinstance(value, dict):
            new_value = dict_camel_to_snake_case(value, convert_keys=convert_subkeys,
                                                 convert_subkeys=True)
        elif isinstance(value, list):
            new_value = []
            for subvalue in value:
                new_subvalue = dict_camel_to_snake_case(subvalue, convert_keys=convert_subkeys,
                                                        convert_subkeys=True) \
                    if isinstance(subvalue, dict) else subvalue
                new_value.append(new_subvalue)
        else:
            new_value = value
        new_key = to_snake_case(key) if convert_keys else key
        converted[new_key] = new_value
    return converted
--------------------
Prompt: 
def step_i_run_command(context, command):
    """
    Run a command as subprocess, collect its output and returncode.
    """
----------
Output: 

    if not command:
        return
    if not context.dry_run:
        context.stdout = context.stdout.replace(
            "\r",
            "\n"
            )
        context.stderr = context.stderr.replace(
            "\r",
            "\n"
            )
        context.returncode = context.returncode.replace(
            "\r",
            "\n"
            )
        context.exit_code = context.returncode.replace(
            "\r",
            "\n"
            )
    else:
        context.stdout = command.stdout
        context.stderr = command.stderr
        context.returncode = command.
----------
Solution: 

    command_util.ensure_workdir_exists(context)
    context.command_result = command_shell.run(command, cwd=context.workdir)
    command_util.workdir_save_coverage_files(context.workdir)
    if False and DEBUG:
        print(u"run_command: {0}".format(command))
        print(u"run_command.output {0}".format(context.command_result.output))
--------------------
Prompt: 
def set_style(network_id, ndex_cred=None, template_id=None):
    """Set the style of the network to a given template network's style

    Parameters
    ----------
    network_id : str
        The UUID of the NDEx network whose style is to be changed.
    ndex_cred : dict
        A dictionary of NDEx credentials.
    template_id : Optional[str]
        The UUID of the NDEx network whose style is used on the
        network specified in the first argument.
    """
----------
Output: 

    if ndex_cred is None:
        ndex_cred = {}
    if template_id is not None:
        network_id = str(network_id)
        ndex_cred = ndex_cred.setdefault(template_id, {})
    if network_id not in ndex_cred:
        ndex_cred[network_id] = {}
    if ndex_cred[network_id].get('style'):
        ndex_cred[network_id]['style'] = {}
    if ndex_cred[network_id].get('style'):
        ndex_cred[
----------
Solution: 

    if not template_id:
        template_id = "ea4ea3b7-6903-11e7-961c-0ac135e8bacf"

    server = 'http://public.ndexbio.org'
    username, password = get_default_ndex_cred(ndex_cred)

    source_network = ndex2.create_nice_cx_from_server(username=username,
                                                      password=password,
                                                      uuid=network_id,
                                                      server=server)

    source_network.apply_template(server, template_id)

    source_network.update_to(network_id, server=server, username=username,
                             password=password)
--------------------
Prompt: 
def load_mo(self, state, page_idx):
        """
        Loads a memory object from memory.

        :param page_idx: the index into the page
        :returns: a tuple of the object
        """
----------
Output: 

        return self.memory.load(state, page_idx)

    def load_mo_with_offset(self, page_idx, offset):
        """
        Loads a memory object from memory and offset.

        :param page_idx: the index into the page
        :param offset: the offset into the memory
        :returns: a tuple of the object and the offset
        """
        return self.memory.load(state, page_idx, offset)

    def load_mo_with_name(self, name):
        """
        Loads a memory object from memory and name.

        :param name: the
----------
Solution: 

        mo = self._storage[page_idx-self._page_addr]
        return self._sinkhole if mo is None else mo
--------------------
Prompt: 
def visual_accelerators(self, value):
        """
        Setter for **self.__visual_accelerators** attribute.

        :param value: Attribute value.
        :type value: tuple or list
        """
----------
Output: 


        self.__visual_accelerators = value

    @property
    def is_visible(self):
        """
        Property for **self.__visible** attribute.

        :return: True if visible.
        :rtype: bool
        """

        return self.__visible

    @is_visible.setter
    def is_visible(self, value):
        """
        Setter for **self.__visible** attribute.

        :param value: Attribute value.
        :type value: bool
        """

        self.__visible = value

    @property
    def is_enabled(self):
        """
        Property for **self.__enabled** attribute.
----------
Solution: 


        if value is not None:
            assert type(value) in (tuple, list), "'{0}' attribute: '{1}' type is not 'tuple' or 'list'!".format(
                "visual_accelerators", value)
        self.__visual_accelerators = value
--------------------
Prompt: 
def _unparse_material(material):
    """
    Turn a trimesh material into a COLLADA material.
    """
----------
Output: 

    if material.type == 'trimesh':
        return _parse_trimesh(material)
    elif material.type == 'trimesh_surface':
        return _parse_trimesh_surface(material)
    elif material.type == 'trimesh_volume':
        return _parse_trimesh_volume(material)
    elif material.type == 'trimesh_surface_sphere':
        return _parse_trimesh_surface_sphere(material)
    elif material.type == 'trimesh_surface_sphere_volume':
        return _parse_trimesh_surface_sphere_volume(material)
----------
Solution: 

    # TODO EXPORT TEXTURES
    if isinstance(material, visual.texture.PBRMaterial):
        diffuse = material.baseColorFactor
        if diffuse is not None:
            diffuse = list(diffuse)

        emission = material.emissiveFactor
        if emission is not None:
            emission = [float(emission[0]), float(emission[1]),
                        float(emission[2]), 1.0]

        shininess = material.roughnessFactor
        if shininess is not None:
            shininess = 2.0 / shininess**2 - 2.0

        effect = collada.material.Effect(
            uuid.uuid4().hex, params=[], shadingtype='phong',
            diffuse=diffuse, emission=emission,
            specular=[1.0, 1.0, 1.0], shininess=float(shininess)
        )
        material = collada.material.Material(
            uuid.uuid4().hex, 'pbrmaterial', effect
        )
    else:
        effect = collada.material.Effect(
            uuid.uuid4().hex, params=[], shadingtype='phong'
        )
        material = collada.material.Material(
            uuid.uuid4().hex, 'defaultmaterial', effect
        )
    return material
--------------------
Prompt: 
def compile_schema(fields):
        """
        joins schema fields with base solr schema

        :param list[str] fields: field list
        :return: compiled schema
        :rtype: byte
        """
----------
Output: 

        return bytes(fields)

    def get_schema(self, field_names):
        """
        joins schema fields with base solr schema

        :param list[str] field_names: field list
        :return: schema joiner
        :rtype: byte
        """
        return self.compile_schema(self.fields[field_names])

    def get_schema_name(self, field_names):
        """
        joins schema fields with base solr schema name

        :param list[str] field_names: field list
        :return: schema joiner
        :rtype: byte
        """
        return self
----------
Solution: 

        path = os.path.dirname(os.path.realpath(__file__))
        # path = os.path.dirname(
        #     os.path.abspath(inspect.getfile(inspect.currentframe())))
        with codecs.open("%s/solr_schema_template.xml" % path, 'r', 'utf-8') as fh:
            schema_template = fh.read()

        schema = schema_template.format('\n'.join(fields))
        if six.PY2:
            return schema.encode('utf-8')

        return schema
--------------------
Prompt: 
def from_file(cls, filename):
        """
        Load ACCESS_KEY_ID and SECRET_ACCESS_KEY from csv
        generated by Amazon's IAM.

        >>> ec2.credentials.from_file('credentials.csv')
        """
----------
Output: 

        with open(filename, 'rb') as f:
            reader = csv.reader(f)
            for row in reader:
                if row[0] == 'ACCESS_KEY_ID':
                    key = row[1]
                elif row[0] == 'SECRET_ACCESS_KEY':
                    key = row[1]
                else:
                    raise ValueError('Unknown key: %s' % row[0])
                cls.access_key_id = key.split(':')[1]
                cls.secret_access_key = key.split(':')[2]

    def __init__(self, filename):
        """
----------
Solution: 

        import csv
        with open(filename, 'r') as f:
            reader = csv.DictReader(f)
            row = reader.next()  # Only one row in the file
        try:
            cls.ACCESS_KEY_ID = row['Access Key Id']
            cls.SECRET_ACCESS_KEY = row['Secret Access Key']
        except KeyError:
            raise IOError('Invalid credentials format')
--------------------
Prompt: 
def reset_subscriptions(self, accounts=[], markets=[], objects=[]):
        """Change the subscriptions of a running Notify instance
        """
----------
Output: 

        if not accounts:
            return
        if not markets:
            return
        if not objects:
            return
        if self.is_notify_enabled:
            self.send_notification_list(markets, self.config['notify_list'])
        else:
            self.send_notification_list(accounts, self.config['accounts'])

    def send_notification_list(self, markets, config):
        """Send a notification list to the notifier
        """
        if not self.is_notify_enabled:
            return
        if not config:
            return
        if markets:
            self.notifier.
----------
Solution: 

        self.websocket.reset_subscriptions(
            accounts, self.get_market_ids(markets), objects
        )
--------------------
Prompt: 
def _get_timeout(self, timeout):
        """
        Return the timeout to use
        """
----------
Output: 

        if timeout is None:
            return self.default_timeout
        else:
            return timeout

    def _get_timeout_exception(self, timeout):
        """
        Return the exception to use when timeout is reached
        """
        if timeout is None:
            return self.default_timeout
        else:
            return timeout

    def _get_timeout_exception_type(self, timeout):
        """
        Return the exception type to use when timeout is reached
        """
        if timeout is None:
            return self.default_timeout
        else:
            return timeout

    def _get_timeout_exception_value(self, timeout
----------
Solution: 

        if timeout is None:
            return self.opts['timeout']
        if isinstance(timeout, int):
            return timeout
        if isinstance(timeout, six.string_types):
            try:
                return int(timeout)
            except ValueError:
                return self.opts['timeout']
        # Looks like the timeout is invalid, use config
        return self.opts['timeout']
--------------------
Prompt: 
def expireat(self, key, when):
        """Emulate expireat"""
----------
Output: 

        self.log.debug("Checking if %s is expired" % key)
        if key in self.cache:
            if self.cache[key][0] < when:
                self.cache[key][1] = time.time()
                self.cache[key][2] = self.cache[key][1]
                self.cache[key][3] = self.cache[key][2]
                self.cache[key][4] = self.cache[key][3]
                self.cache[key][5] = time.time()
                return True
            else:
                self
----------
Solution: 

        expire_time = datetime.fromtimestamp(when)
        key = self._encode(key)
        if key in self.redis:
            self.timeouts[key] = expire_time
            return True
        return False
--------------------
Prompt: 
def _find_immediately(self, locator, search_object=None):
        """
        Attempts to immediately find elements on the page without waiting

        @type locator:          webdriverwrapper.support.locator.Locator
        @param locator:         Locator object describing
        @type search_object:    webdriverwrapper.WebElementWrapper
        @param search_object:   Optional WebElement to start search with.  If null, search will be on self.driver


        @return:                Single WebElemetnWrapper if find_all is False,
                                list of WebElementWrappers if find_all is True
        """
----------
Output: 

        @rtype:                 webdriverwrapper.WebElementWrapper
        @return:                Single elementWrapper if find_all is True,
                                WebElementWrappers if find_all is True
        """
        if search_object is None:
            search_object = self.driver

        if not search_object:
            search_object = search_object.find_element(locator)

        if not search_object:
            search_object = search_object.find_element(locator)

        return webethereum.WebElementWrapperList(
            search_object,
            timeout=self.timeout,
            ignored_exceptions=
----------
Solution: 

        search_object = self.driver if search_object is None else search_object
        elements = self.locator_handler.find_by_locator(search_object, locator, True)
        return [WebElementWrapper.WebElementWrapper(self, locator, element) for element in elements]
--------------------
Prompt: 
def restore_initial_state(self):
        """Restores initial cursors and initial active editor."""
----------
Output: 

        self.set_cursor(self.get_cursor_from_pos(0, 0))
        self.set_active_editor(self.get_active_editor())

    def get_cursor_from_pos(self, x, y):
        """Gets the cursor position of the mouse pointer."""
        return self.get_pos_from_widget(x, y)

    def get_pos_from_widget(self, widget):
        """Gets the mouse position of the given widget."""
        return self.get_widget_pos(widget)

    def get_widget_pos(self,
----------
Solution: 

        self.list.clear()
        self.is_visible = False
        widgets = self.widgets_by_path

        if not self.edit.clicked_outside:
            for path in self.initial_cursors:
                cursor = self.initial_cursors[path]
                if path in widgets:
                    self.set_editor_cursor(widgets[path], cursor)

            if self.initial_widget in self.paths_by_widget:
                index = self.paths.index(self.initial_path)
                self.sig_goto_file.emit(index)
--------------------
Prompt: 
def get(self, file):
        """ Check on github if a file exists

        :param file: File to check status of
        :return: File with new information, including blob, or Error
        :rtype: File or self.ProxyError
        """
----------
Output: 

        if not os.path.exists(file):
            return self.error(404, "File not found")
        if not os.path.isfile(file):
            return self.error(404, "File not found")
        if not self.check_file_permission(file, "write"):
            return self.error(403, "Write permission denied")
        if not self.check_file_permission(file, "delete"):
            return self.error(404, "File not found")
        if not self.check_file_permission(file, "upload"):
            return self.error(
----------
Solution: 

        uri = "{api}/repos/{origin}/contents/{path}".format(
            api=self.github_api_url,
            origin=self.origin,
            path=file.path
        )
        params = {
            "ref": file.branch
        }
        data = self.request("GET", uri, params=params)
        # We update the file blob because it exists and we need it for update
        if data.status_code == 200:
            data = json.loads(data.content.decode("utf-8"))
            file.blob = data["sha"]
        elif data.status_code == 404:
            pass
        else:
            decoded_data = json.loads(data.content.decode("utf-8"))
            return self.ProxyError(
                data.status_code, (decoded_data, "message"),
                step="get", context={
                    "uri": uri,
                    "params": params
                }
            )
        return file
--------------------
Prompt: 
def client(self, verifier=None):
        """
        Return the correct client depending on which stage of the OAuth process
        we're in.
        """
----------
Output: 

        if verifier is None:
            verifier = self.get_verifier()
        return OAuthClient(self.consumer_key, self.consumer_secret, verifier)

    def get_verifier(self):
        """
        Return the oauth verifier.
        """
        return self.options.get('oauth_verifier')

    def set_verifier(self, value):
        """
        Set the oauth verifier.
        """
        self.options['oauth_verifier'] = value

    def get_callback_url(self):
        """
        Return the URL to send the user to after they authorize your app.
        """
        return self.
----------
Solution: 

        # We're just starting out and don't have neither request nor access
        # token. Return the standard client
        if not self._request_token and not self._access_token:
            client = oauth.Client(self.consumer, timeout=TIMEOUT)
        
        # We're one step in, we've got the request token and can add that to 
        # the client.
        if self._request_token and not self._access_token:
            if verifier is not None:
                self._request_token.set_verifier(verifier)
            client = oauth.Client(self.consumer, self._request_token, timeout=TIMEOUT)
            
        # Two steps in, we've got an access token and can now properly sign 
        # our client requests with it.
        if self._access_token:
            client = oauth.Client(self.consumer, self._access_token, timeout=TIMEOUT)
        
        return client
--------------------
Prompt: 
def add_filter(self, filter_values):
        """Improve the original one to deal with OR cases."""
----------
Output: 

        if self.filter_values:
            self.filter_values.update(filter_values)
        else:
            self.filter_values = {}
        self.filter_values.update(filter_values)

    def add_filter_value(self, filter_name, filter_values):
        """Add a filter value to the filter name."""
        if filter_name not in self.filter_values:
            self.filter_values[filter_name] = []
        self.filter_values[filter_name].extend(filter_values)

    def get_filter_values(self, filter
----------
Solution: 

        field = self._params['field']
        # Build a `AND` query on values wihtout the OR operator.
        # and a `OR` query for each value containing the OR operator.
        filters = [
            Q('bool', should=[
                Q('term',  **{field: v}) for v in value.split(OR_SEPARATOR)
            ])
            if OR_SEPARATOR in value else
            Q('term',  **{field: value})
            for value in filter_values
        ]
        return Q('bool', must=filters) if len(filters) > 1 else filters[0]
--------------------
Prompt: 
def refreshCatalogs(self):
        """
        It reindexes the modified catalogs but, while cleanAndRebuildCatalogs
        recatalogs all objects in the database, this method only reindexes over
        the already cataloged objects.

        If a metacolumn is added it refreshes the catalog, if only a new index
        is added, it reindexes only those new indexes.
        """
----------
Output: 

        if self.isMetaColumn(self.getCatalogs()):
            self.db.execute("UPDATE catalogs SET metacolumn =? WHERE id =?", (self.getCatalogs(), self.id, self.id))
            self.db.execute("UPDATE objects SET indexid =? WHERE id =?", (self.id, self.id, self.id))
        else:
            self.db.execute("DELETE FROM catalogs WHERE id =?", (self.id, self.id))
            self.db.execute("DELETE FROM objects WHERE indexid =?
----------
Solution: 

        to_refresh = self.refreshcatalog[:]
        to_reindex = self.reindexcatalog.keys()
        to_reindex = to_reindex[:]
        done = []
        # Start reindexing the catalogs with new columns
        for catalog_to_refresh in to_refresh:
            logger.info(
                'Catalog {0} refreshing started'.format(catalog_to_refresh))
            catalog = getToolByName(self.portal, catalog_to_refresh)
            handler = ZLogHandler(self.pgthreshold)
            catalog.refreshCatalog(pghandler=handler)
            logger.info('Catalog {0} refreshed'.format(catalog_to_refresh))
            transaction.commit()
            done.append(catalog_to_refresh)
        # Now the catalogs which only need reindxing
        for catalog_to_reindex in to_reindex:
            if catalog_to_reindex in done:
                continue
            logger.info(
                'Catalog {0} reindexing started'.format(catalog_to_reindex))
            catalog = getToolByName(
                self.portal, catalog_to_reindex)
            indexes = self.reindexcatalog[catalog_to_reindex]
            handler = ZLogHandler(self.pgthreshold)
            catalog.reindexIndex(indexes, None, pghandler=handler)
            logger.info('Catalog {0} reindexed'.format(catalog_to_reindex))
            transaction.commit()
            done.append(catalog_to_reindex)
--------------------
Prompt: 
def rmv_normal(mu, tau, size=1):
    """
    Random multivariate normal variates.
    """
----------
Output: 

    return multivariate_normal(mu, tau, size)


def rmv_uniform(low, high, size=1):
    """
    Randomly sampled from a uniform distribution.
    """
    return rmv_normal(low, high, size)


def rmv_normal(mu, sigma, size=1):
    """
    Randomly sampled from a multivariate normal distribution.
    """
    return multivariate_normal(mu, sigma, size)


def rmv_uniform_like(low, high, size=1):
    """
    Randomly sampled from a uniform distribution.
    """
   
----------
Solution: 


    sig = np.linalg.cholesky(tau)
    mu_size = np.shape(mu)

    if size == 1:
        out = np.random.normal(size=mu_size)
        try:
            flib.dtrsm_wrap(sig, out, 'L', 'T', 'L', 1.)
        except:
            out = np.linalg.solve(sig, out)
        out += mu
        return out
    else:
        if not hasattr(size, '__iter__'):
            size = (size,)
        tot_size = np.prod(size)
        out = np.random.normal(size=(tot_size,) + mu_size)
        for i in xrange(tot_size):
            try:
                flib.dtrsm_wrap(sig, out[i, :], 'L', 'T', 'L', 1.)
            except:
                out[i, :] = np.linalg.solve(sig, out[i,:])
            out[i, :] += mu
        return out.reshape(size + mu_size)
--------------------
Prompt: 
def cache(self):
        """
        Enable in-memory caching (Spark only).
        """
----------
Output: 

        self._jdf.cache()

    @since("2.4.0")
    def unpersist(self):
        """
        Enable un-persistent (Spark only).
        """
        self._jdf.unpersist()

    @since("2.4.0")
    def coalesce(self, numPartitions):
        """
        Return a new :class:`DataFrame` that has exactly `numPartitions` partitions.

        Similar to coalesce defined on an :class:`RDD`, this operation results in a coalesce
        linkage.

        :param numPartitions: the number of partitions to distribute
        """
        return DataFrame(
----------
Solution: 

        if self.mode == 'spark':
            self.values.cache()
            return self
        else:
            notsupported(self.mode)
--------------------
Prompt: 
def _compact_class_repr(obj):
        """ A compact version of __repr__ for each of the steps.
        """
----------
Output: 

        return repr(obj)

    def _get_class_name(self, obj):
        """ A compact version of __repr__ for each of the steps.
        """
        return obj.__class__.__name__

    def _get_class_module(self, obj):
        """ A compact version of __repr__ for each of the steps.
        """
        return obj.__class__.__module__

    def _get_class_code(self, obj):
        """ A compact version of __repr__ for each of the steps.
        """
        return obj.func_code

    def _get_class_docstring(
----------
Solution: 

        dict_str_list = []
        post_repr_string = ""

        # If features are present, then shorten it.
        init_func = obj.__init__
        if _sys.version_info.major == 2:
            init_func = init_func.__func__

        fields = _inspect.getargspec(init_func).args
        fields = fields[1:] # remove self
        if 'features' in fields:
            fields.remove('features')
            features = obj.get("features")
            if features is not None:
                post_repr_string = ' on %s feature(s)' % len(features)
        if 'excluded_features' in fields:
            fields.remove('excluded_features')

        # GLC transformers.
        if issubclass(obj.__class__, _Transformer):
            for attr in fields:
                dict_str_list.append("%s=%s" % (attr, obj.get(attr).__repr__()))

        # Chains
        elif obj.__class__ == TransformerChain:
            _step_classes = list(map(lambda x: x.__class__.__name__, obj.get('steps')))
            _steps = _internal_utils.pretty_print_list(
                                          _step_classes, 'steps', False)
            dict_str_list.append(_steps)

        # For user defined transformers.
        else:
            for attr in fields:
                dict_str_list.append("%s=%s" % (attr, obj.__dict__[attr]))

        return "%s(%s)%s" % (obj.__class__.__name__, ", ".join(dict_str_list),
                             post_repr_string)
--------------------
Prompt: 
def CA_TRACE(residue):
    """
    Reduces protein residues to the alpha carbon:
    CA trace only.
    """
----------
Output: 

    return (residue.id, residue.name)

def CA_CHAIN(chain):
    """
    Reduces a chain:
    CA chain only.
    """
    return chain

def CA_CHAIN_END(chain):
    """
    End of the chain:
    CA chain only.
    """
    return chain

def CA_CHAIN_START(chain):
    """
    Start of the chain:
    CA chain only.
    """
    return chain

def CA_CHAIN_END(chain):
    """
    End of the chain:
    CA chain only.
    """
    return chain
----------
Solution: 


    cg_residue = deepcopy(residue)

    for atom in cg_residue.child_dict.keys():
        if cg_residue[atom].name != 'CA':
            cg_residue.detach_child(cg_residue[atom].name)
    
    return cg_residue
--------------------
Prompt: 
def convert_to_float_list(value):
    """ Converts a comma separate string to a list

    :param value:
        the format must be 1.2,-3.5 (commas with no space)
    :type value:
        String

    :returns:
        List

    :example:
        >>> convert_to_integer_list('003,003,004,004')
        [1.2, -3.5]

    """
----------
Output: 

    if not value:
        return []
    return value.split(',')


def convert_to_int_list_with_commas(value):
    """ Converts a comma separate string to a list

    :param value:
        the format must be 1.2,-3.5 (commas with no space)
    :type value:
        String

    :returns:
        List

    :example:
        >>> convert_to_int_list_with_commas('003,003,004,004')
        [1.2, -3.5]

    """
    if not value:
        return []
   
----------
Solution: 

    if isinstance(value, list) or value is None:
        return value
    else:
        s = re.findall('([-+]?\d*\.\d+|\d+|[-+]?\d+)', value)
        for k, v in enumerate(s):
            try:
                s[k] = float(v)
            except ValueError:
                pass
        return s
--------------------
Prompt: 
def _tobytes(self):
        """Serializes the write buffer into a single string (bytes).

        Returns:
            a string (bytes) object.
        """
----------
Output: 

        return _encode_int(self.write_buffer)

    def _get_write_buffer(self):
        """Retrieves the write buffer into a string.

        Returns:
            a string (bytes) object.
        """
        return _encode_str(self.write_buffer)

    def _get_write_buffer_size(self):
        """Retrieves the write buffer size.

        Returns:
            an integer (or None) that represents the number of bytes
            remaining in the buffer.
        """
        return _encode_int(self.write_buffer_size)

    def _get_write_buffer
----------
Solution: 

        if not self._has_view:
            # fast path optimization
            if len(self._deque) == 0:
                return b""
            elif len(self._deque) == 1:
                # no copy
                return self._deque[0]
            else:
                return b"".join(self._deque)
        else:
            tmp = [x.tobytes() if isinstance(x, memoryview) else x
                   for x in self._deque]
            return b"".join(tmp)
--------------------
Prompt: 
def main(argString=None):
    """The main function of the module.

    :param argString: the options.

    :type argString: list

    These are the steps:

    1. Prints the options.
    2. Checks the number of samples in the ``tfam`` file
       (:py:func:`compute_nb_samples`).
    3. Computes the heterozygosity rate (:py:func:`compute_heterozygosity`).
    4. Saves the heterozygosity data (in ``out.het``).
    5. Plots the heterozygosity rate (:py:func:`plot_heterozygosity`).

    """
----------
Output: 
sity values.
    6. Plots the heterozygosity values.
    7. Plots the heterozygosity values.
    8. Plots the heterozygosity values.
    9. Plots the heterozygosity values.
    10. Plots the heterozygosity values.
    11. Plots the heterozygosity values.
    12. Plots the heterozygosity values.
    13. Plots the heterozygosity values.
    14. Prints the heterozygosity rate.
    15. Plots the hetero
----------
Solution: 

    # Getting and checking the options
    args = parseArgs(argString)
    checkArgs(args)

    # Check the number of samples
    nb_samples = compute_nb_samples(args.tfile)

    # Compute the heterozygosity rate
    heterozygosity, samples = compute_heterozygosity(args.tfile, nb_samples)

    # Save heterozygosity data
    save_heterozygosity(heterozygosity, samples, args.out)

    # Plotting the heterozygosity rate distribution
    plot_heterozygosity(heterozygosity, args)
--------------------
Prompt: 
def write_config(self, access_token):
        """ Write credentials to the config file """
----------
Output: 

        self.access_token = access_token
        self.config.write()

    def get_access_token(self):
        """ Get the access token """
        return self.access_token

    def get_token(self):
        """ Get the current token """
        return self.access_token

    def get_user_id(self):
        """ Get the current user id """
        return self.user_id

    def get_user_name(self):
        """ Get the current user name """
        return self.user_name

    def get_user_id_from_name(self, name
----------
Solution: 

        self.config.add_section('exist')

        # TODO: config is reading 'None' as string during authorization, so clearing this out
        # if no id or secret is set - need to fix this later
        if self.client_id:
            self.config.set('exist', 'client_id', self.client_id)
        else:
            self.config.set('exist', 'client_id', '')

        if self.client_secret:
            self.config.set('exist', 'client_secret', self.client_secret)
        else:
            self.config.set('exist', 'client_secret', '')

        self.config.set('exist', 'access_token', access_token)

        with open(self.config_file, 'w') as cfg:
            self.config.write(cfg)
        print('Credentials written to %s' % self.config_file)
--------------------
Prompt: 
def find_promulgation_date(line):
    """
    >>> find_promulgation_date("Loi  nº 2010-383 du 16 avril 2010 autorisant l'approbation de l'accord entre...")
    '2010-04-16'
    """
----------
Output: 

    if line.startswith("P"):
        return "date"
    elif line.startswith("M"):
        return "date"
    elif line.startswith("A"):
        return "date"
    elif line.startswith("Autorisant"):
        return "date"
    elif line.startswith("Date"):
        return "date"
    else:
        return "date"


def find_promulgation_date_and_time(line):
    """
    >>> find_promulgation_date_and_time("Loi  nº 2010-383 du 16 avril 2010 autorisant l
----------
Solution: 

    line = line.split(' du ')[1]
    return format_date(re.search(r"(\d\d? \w\w\w+ \d\d\d\d)", line).group(1))
--------------------
Prompt: 
def calc_drm(skydir, ltc, event_class, event_types,
             egy_bins, cth_bins, nbin=64):
    """Calculate the detector response matrix."""
----------
Output: 

    # Read in the skydir
    skydir = skydir.replace('skydir', 'ltc')
    # Read in the event_class
    event_class = event_class.replace('event_class', 'event')
    # Read in the event_types
    event_types = event_types.replace('event_types', 'event_type')
    # Read in the egy_bins
    egy_bins = egy_bins.replace('egy_bins', 'bins')
    # Read in the cth_bins
    cth_bins = cth_
----------
Solution: 

    npts = int(np.ceil(128. / bins_per_dec(egy_bins)))
    egy_bins = np.exp(utils.split_bin_edges(np.log(egy_bins), npts))

    etrue_bins = 10**np.linspace(1.0, 6.5, nbin * 5.5 + 1)
    egy = 10**utils.edge_to_center(np.log10(egy_bins))
    egy_width = utils.edge_to_width(egy_bins)
    etrue = 10**utils.edge_to_center(np.log10(etrue_bins))
    edisp = create_avg_edisp(skydir, ltc, event_class, event_types,
                             egy, etrue, cth_bins)
    edisp = edisp * egy_width[:, None, None]
    edisp = sum_bins(edisp, 0, npts)
    return edisp
--------------------
Prompt: 
def ensure_image_is_hex(input_path):
    """Return a path to a hex version of a firmware image.

    If the input file is already in hex format then input_path
    is returned and nothing is done.  If it is not in hex format
    then an SCons action is added to convert it to hex and the
    target output file path is returned.

    A cache is kept so that each file is only converted once.

    Args:
        input_path (str): A path to a firmware image.

    Returns:
        str: The path to a hex version of input_path, this may
            be equal to input_path if it is already in hex format.
    """
----------
Output: 
 will
            be used to cache the results of conversion.
    """
    return SCons.Util.splitext(input_path)[0] + ".hex"


class Tool(object):
    def __init__(self, name, function=None, **kw):
        self.name = name
        if function:
            self.function = function
        for k, v in kw.items():
            setattr(self, k, v)

    def __call__(self, target, source, env):
        if not target:
            raise SCons.Errors.UserError("tool entry for %s not found" % self.name
----------
Solution: 


    family = utilities.get_family('module_settings.json')
    target = family.platform_independent_target()
    build_dir = target.build_dirs()['build']

    if platform.system() == 'Windows':
        env = Environment(tools=['mingw'], ENV=os.environ)
    else:
        env = Environment(tools=['default'], ENV=os.environ)

    input_path = str(input_path)
    image_name = os.path.basename(input_path)

    root, ext = os.path.splitext(image_name)
    if len(ext) == 0:
        raise BuildError("Unknown file format or missing file extension in ensure_image_is_hex", file_name=input_path)

    file_format = ext[1:]

    if file_format == 'hex':
        return input_path

    if file_format == 'elf':
        new_file = os.path.join(build_dir, root + '.hex')

        if new_file not in CONVERTED_HEX_FILES:
            env.Command(new_file, input_path, action=Action("arm-none-eabi-objcopy -O ihex $SOURCE $TARGET",
                                                            "Creating intel hex file from: $SOURCE"))
            CONVERTED_HEX_FILES.add(new_file)

        return new_file

    raise BuildError("Unknown file format extension in ensure_image_is_hex",
                     file_name=input_path, extension=file_format)
--------------------
Prompt: 
def will_print(level=1):
    """Returns True if the current global status of messaging would print a
    message using any of the printing functions in this module.
    """
----------
Output: 

    return _lib.TCOD_PrintMessage(ctypes.byref(ctypes.c_int(level)))

def will_print_with_name(level=1):
    """Returns True if the current global status of messaging would print
    a message using any of the printing functions in this module.
    """
    return _lib.TCOD_PrintWithName(ctypes.byref(ctypes.c_int(level)))

def will_print_with_type(level=1):
    """Returns True if the current global status of messaging would print
    a message using a specified type.
    """
----------
Solution: 

    if level == 1:
        #We only affect printability using the quiet setting.
        return quiet is None or quiet == False
    else:
        return ((isinstance(verbosity, int) and level <= verbosity) or
                (isinstance(verbosity, bool) and verbosity == True))
--------------------
Prompt: 
def render(self, context, instance, placeholder):
        """
        Update the context with plugin's data
        """
----------
Output: 

        context.update({
            'instance': instance,
            'placeholder': placeholder,
            'plugin': instance.plugin,
            'plugin_id': instance.pk,
            'plugin_parent': instance.parent_id,
            'plugin_language': instance.language,
            'plugin_order': instance.plugin.get_children().count(),
            'plugin_parent_id': instance.parent_id,
            'plugin_language_id': instance.language,
            'plugin_order_id': instance.plugin_order_id,
            'plugin_order_language_id': instance.language
----------
Solution: 

        context = super(CMSSelectedEntriesPlugin, self).render(
            context, instance, placeholder)
        context['entries'] = instance.entries.all()
        return context
--------------------
Prompt: 
def entities(self, name_id):
        """ Returns all the entities of assertions for a subject, disregarding
        whether the assertion still is valid or not.

        :param name_id: The subject identifier, a NameID instance
        :return: A possibly empty list of entity identifiers
        """
----------
Output: 

        return self._get_all_entities(self.assertions, name_id)

    def _get_all_entities(self, assertions, name_id):
        """ Returns all the assertions for a subject, disregarding
        whether the assertion still is valid or not.

        :param assertions: The assertions to be returned
        :param name_id: The subject identifier, a NameID instance
        :return: A list of entity identifiers
        """
        if not assertions:
            return []
        return self._get_all_assertions(assertions, name_id)

    def _get_all
----------
Solution: 

        cni = code(name_id)
        return list(self._db[cni].keys())
--------------------
Prompt: 
def match_all_in(self, matches, item):
        """Matches all matches to elements of item."""
----------
Output: 

        for match in matches:
            if isinstance(match, str):
                if not self.match_str(match, item):
                    return False
            elif isinstance(item, str):
                if not self.match_all(match, item):
                    return False
            else:
                raise TypeError("Unknown type of match: %s" % type(item))
        return True

    def match_str(self, match, item):
        """Matches the given string."""
        if isinstance(match, str):
            return self.match_str(match, item)
        elif isinstance(item, str):
            return match
       
----------
Solution: 

        for i, match in enumerate(matches):
            self.match(match, item + "[" + str(i) + "]")
--------------------
Prompt: 
def coord_to_col( self, pos ):
        """
        Return the alignment column index corresponding to coordinate pos.

        pos is relative to the + strand, regardless of the component's strand.

        """
----------
Output: 

        return self.strand_to_index( pos )

    def strand_to_index( self, strand ):
        """
        Return the index of the strand in the given position.

        strand is a strand object.

        """
        return self.strand_to_index_by_name( strand, self.name_to_index )

    def name_to_index_by_name( self, name, pos ):
        """
        Return the alignment column position corresponding to the alignment name.

        pos is the relative position in the strand object, as a string.

        """
       
----------
Solution: 

        start,end = self.get_forward_strand_start(),self.get_forward_strand_end()
        if pos < start or pos > end:
            raise ValueError("Range error: %d not in %d-%d" % ( pos, start, end ))
        if not self.index:
            self.index = list()
            if (self.strand == '-'):
                # nota bene: for - strand self.index[x] maps to one column
                # higher than is actually associated with the position;  thus
                # when slice_by_component() and slice_by_coord() flip the ends,
                # the resulting slice is correct
                for x in range( len(self.text)-1,-1,-1 ):
                    if not self.text[x] == '-':
                        self.index.append( x + 1 )
                self.index.append( 0 )
            else:
                for x in range( len(self.text) ):
                    if not self.text[x] == '-':
                        self.index.append(x)
                self.index.append( len(self.text) )
        x = None
        try:
            x = self.index[ pos - start ]
        except:
            raise Exception("Error in index.")
        return x
--------------------
Prompt: 
def _bin(self, bin):  # type: (str) -> str
        """
        Return path to the given executable.
        """
----------
Output: 

        return os.path.join(self.path, bin)

    def _get_exe(self, name):  # type: (str) -> str
        """
        Return path to the given executable.
        """
        return os.path.join(self.path, name)

    def _get_exe_version(self, name):  # type: (str) -> str
        """
        Return version of the given executable.
        """
        return self._bin(self.get_exe(name))

    def _get_version_from_build(self, build):  # type: (
----------
Solution: 

        bin_path = (self._bin_dir / bin).with_suffix(".exe" if self._is_windows else "")
        if not bin_path.exists():
            return bin

        return str(bin_path)
--------------------
Prompt: 
def init_original_response(self):
        """Get the original response for comparing, confirm is_cookie_necessary"""
----------
Output: 

        return self.original_response

    def test_request_cookie_not_present(self):
        """Test that a missing cookie is not added to the response"""
        self.original_response = self.original_response.replace(
            b'Set-Cookie: foo=bar', b'Set-Cookie: foo=bar')
        self.original_response = self.original_response.replace(
            b'Set-Cookie: foo=bar', b'Set-Cookie: foo=bar')
        self.is_cookie_necessary = self.original_response.replace(
            b'Set
----------
Solution: 

        no_cookie_resp = None
        self.is_cookie_necessary = True
        if 'json' in self.request:
            self.request['data'] = json.dumps(self.request.pop('json')).encode(
                self.encoding)
        r1 = self.req.request(
            retry=self.retry, timeout=self.timeout, **self.request)
        if 'headers' in self.request:
            # test is_cookie_necessary
            cookie = self.request['headers'].get('Cookie', None)
            if cookie:
                new_request = deepcopy(self.request)
                new_request['headers']['Cookie'] = ''
                r2 = self.req.request(
                    retry=self.retry, timeout=self.timeout, **new_request)
                no_cookie_resp = self.ensure_response(r2)
        resp = r1.x
        assert resp, ValueError(
            'original_response should not be failed. %s' % self.request)
        self.original_response = self.ensure_response(r1)
        self.encoding = self.encoding or resp.encoding
        if no_cookie_resp == self.original_response:
            self.ignore['headers'].append('Cookie')
            self.is_cookie_necessary = False
        return self.original_response
--------------------
Prompt: 
def collect_blocks(self):
        """
        Collect the blocks in a list
        """
----------
Output: 

        self.blocks = []
        self.blocks_by_id = {}
        for block in self.blocks:
            self.blocks_by_id[block.id] = block
            self.blocks.append(block)

    def _get_block_by_id(self, block_id):
        """
        Return a block instance for the given block id
        """
        return self.blocks_by_id[block_id]

    def _get_blocks_by_type(self, block_type):
        """
        Return a list of all blocks with the given block type
        """
        return
----------
Solution: 

        if self.mode == 'spark':
            return self.values.tordd().sortByKey().values().collect()

        if self.mode == 'local':
            return self.values.values.flatten().tolist()
--------------------
Prompt: 
def activate_debug():
    """Activate debug logging on console

    This function is useful when playing with python-textops through a python console.
    It is not recommended to use this function in a real application : use standard logging
    functions instead.
    """
----------
Output: 

    global _DEBUG
    _DEBUG = True
    if _DEBUG:
        _DEBUG = False
    else:
        _DEBUG = None

def get_debug_level():
    """Get debug level

    This function is useful when running in a debug mode.
    It is recommended to use this function in a real application : use the
    :func:`logging.DEBUG` logger level.
    """
    return logging.DEBUG

def get_debug_format():
    """Get debug format

    This function is useful when running in a debug mode.
    It is recommended to use this function in a real application : use the
   
----------
Solution: 

    ch = logging.StreamHandler()
    ch.setLevel(logging.DEBUG)
    logger.addHandler(ch)
    logger.setLevel(logging.DEBUG)
--------------------
Prompt: 
def unregister_message_handler(self, target_or_handler):
        """Unregister a mpv script message handler for the given script message target name.

        You can also call the ``unregister_mpv_messages`` function attribute set on the handler function when it is
        registered.
        """
----------
Output: 

        if isinstance(target_or_handler, str):
            target_or_handler = self.get_script_message_handler(target_or_handler)
        if target_or_handler is None:
            raise ValueError("Invalid target name: %s" % target_or_handler)
        if target_or_handler not in self.mpv_scripts:
            raise ValueError("Unknown message target: %s" % target_or_handler)
        self.mpv_scripts[target_or_handler].unregister(target_or_handler)
        if target_or_handler in self
----------
Solution: 

        if isinstance(target_or_handler, str):
            del self._message_handlers[target_or_handler]
        else:
            for key, val in self._message_handlers.items():
                if val == target_or_handler:
                    del self._message_handlers[key]
--------------------
Prompt: 
def save_config(self, cmd="save config", confirm=False, confirm_response=""):
        """Save Config"""
----------
Output: 

        if not confirm:
            return self.write_config(cmd, "config")
        if not confirm_response:
            return self.write_config(cmd, "config", confirm_response="y")
        return self.write_config(cmd, "config", confirm_response=confirm_response)

    def write_config(self, cmd, cmd_args="", confirm=False, confirm_response=""):
        """Write Config"""
        if not confirm:
            return self.write_command(cmd, cmd_args)
        if not confirm_response:
            return self.write_command(cmd,
----------
Solution: 

        return super(ExtremeErsSSH, self).save_config(
            cmd=cmd, confirm=confirm, confirm_response=confirm_response
        )
--------------------
Prompt: 
def do_gather(flist):
    """ Gather all the HDUs from a list of files"""
----------
Output: 

    for f in flist:
        if os.path.isfile(f):
            hdu = fits.open(f)
            hdu_data = hdu[0].data
            hdu_data = hdu_data.reshape(1, hdu_data.shape[0])
            hdu_data = hdu_data.T
            hdu_data = hdu_data.T
            hdu_data = hdu_data.T
            hdu_data = hdu_data.T
            hdu_data = hdu_data.T
            hdu_data
----------
Solution: 

    hlist = []
    nskip = 3
    for fname in flist:
        fin = fits.open(fname)
        if len(hlist) == 0:
            if fin[1].name == 'SKYMAP':
                nskip = 4
            start = 0
        else:
            start = nskip
        for h in fin[start:]:
            hlist.append(h)
    hdulistout = fits.HDUList(hlist)
    return hdulistout
--------------------
Prompt: 
def get_feh(cls, filename):
        """
        example filename: yapsi_w_X0p602357_Z0p027643.dat
        """
----------
Output: 

        with open(filename, 'r') as f:
            return cls.read_header(f)

    @classmethod
    def read_header(cls, file):
        """
        example file: yapsi_w_X0p602357_Z0p027643.dat
        """
        return cls.read_header_from_file(file)

    @classmethod
    def read_header_from_file(cls, file):
        """
        example file: yapsi_w_X0p602357_Z0p027643.dat
        """
        with open(file, 'r
----------
Solution: 

        X,Y,Z = cls._get_XYZ(filename)

        Xsun = 0.703812
        Zsun = 0.016188

        return np.log10((Z/X) / (Zsun/Xsun))
--------------------
Prompt: 
def do_gate(self, gate: Gate):
        """
        Perform a gate.

        :return: ``self`` to support method chaining.
        """
----------
Output: 

        return gate.set_gate_state(self.gate_state)

    def do_set_state(self, state: State):
        """
        Set the state of the device.

        :param state: The new state.
        """
        self.gate_state = state

    def do_set_state_of_device(self, device: Device):
        """
        Set the state of the device.

        :param device: The new state.
        """
        self.device_id = device.id

    def do_set_device_name(self, name: str):
        """
        Set
----------
Solution: 

        gate_matrix, qubit_inds = _get_gate_tensor_and_qubits(gate=gate)
        # Note to developers: you can use either einsum- or tensordot- based functions.
        # tensordot seems a little faster, but feel free to experiment.
        # self.wf = targeted_einsum(gate=gate_matrix, wf=self.wf, wf_target_inds=qubit_inds)
        self.wf = targeted_tensordot(gate=gate_matrix, wf=self.wf, wf_target_inds=qubit_inds)
        return self
--------------------
Prompt: 
def filter_wildcard(names, pattern):
    """
    Return a tuple of strings that match a shell-style wildcard pattern.
    """
----------
Output: 

    if not names:
        return (), (), ()
    if not pattern:
        return ((), (), ())
    if isinstance(names, str):
        names = names.split()
    return (names, (), ()) + tuple(map(re.escape, pattern) + names), ()


def filter_glob(names, pattern):
    """
    Return a tuple of strings that match a shell-style glob pattern.
    """
    if not names:
        return (), (), ()
    if not pattern:
        return ((), (), ())
    if isinstance(names, str):
        names = names.split()
    return (names
----------
Solution: 

    return tuple(name for name in names
                 if fnmatch.fnmatch(name, pattern))
--------------------
Prompt: 
def keyReleaseEvent(self, event):
        """Reimplement Qt method.

        Handle "most recent used" tab behavior,
        When ctrl is released and tab_switcher is visible, tab will be changed.
        """
----------
Output: 

        if self.isVisible():
            self.tab_switcher.keyReleaseEvent(event)

    def mousePressEvent(self, event):
        """Reimplement Qt method.

        Handle "most recent used" tab behavior,
        When ctrl is pressed and tab_switcher is visible, tab will be changed.
        """
        if self.isVisible():
            self.tab_switcher.mousePressEvent(event)

    def mouseReleaseEvent(self, event):
        """Reimplement Qt method.

        Handle "most recent used" tab behavior,
        When ctrl is released and tab_switcher is visible
----------
Solution: 

        if self.isVisible():
            qsc = get_shortcut(context='Editor', name='Go to next file')

            for key in qsc.split('+'):
                key = key.lower()
                if ((key == 'ctrl' and event.key() == Qt.Key_Control) or
                   (key == 'alt' and event.key() == Qt.Key_Alt)):
                        self.item_selected()
        event.accept()
--------------------
Prompt: 
def _add_span_node_ids_to_token_nodes(self):
        """
        Adds to every token node the list of spans (span node IDs) that it
        belongs to.

        TokenNode.spans - a list of `int` ids of `SpanNode`s
        """
----------
Output: 

        self.span_nodes = []
        for span_id in self.span_ids:
            self.span_nodes.append(span_id)

    def _add_span_node_ids(self, span_id):
        """
        Adds to every span node the ID of the span is the index of
        the token.

        TokenNode.spans - a list of `int` ids of `SpanNode`s
        """
        self.span_ids.append(span_id)

    def _add_span_ids_to_token_nodes(self, span_ids):
       
----------
Solution: 

        span_dict = defaultdict(list)
        for span_edge in self._spanning_relation_ids:
            token_node_id = self.edges[span_edge].target
            span_node_id = self.edges[span_edge].source
            span_dict[token_node_id].append(span_node_id)

        for token_node_id in span_dict:
            self.nodes[token_node_id].spans = span_dict[token_node_id]
--------------------
Prompt: 
def get_tuids(self, branch, revision, files):
        """
        GET TUIDS FROM ENDPOINT, AND STORE IN DB
        :param branch: BRANCH TO FIND THE REVISION/FILE
        :param revision: THE REVISION NUNMBER
        :param files: THE FULL PATHS TO THE FILES
        :return: MAP FROM FILENAME TO TUID LIST
        """
----------
Output: 

        if branch =='master':
            return self.get_master_tuids(revision, files)
        elif branch == 'trunk':
            return self.get_trunk_tuids(revision, files)
        elif branch == 'tuids':
            return self.get_tuids(branch, revision, files)
        else:
            raise Exception('Invalid branch: %s' % branch)

    def get_master_tuids(self, branch, files):
        """
        GET TUIDS FROM STARTPOINT, AND FILES IN BRANCH
        :param branch: BRANCH TO FIND THE REVISION
----------
Solution: 


        # SCRUB INPUTS
        revision = revision[:12]
        files = [file.lstrip('/') for file in files]

        with Timer(
            "ask tuid service for {{num}} files at {{revision|left(12)}}",
            {"num": len(files), "revision": revision},
            silent=not DEBUG or not self.enabled
        ):
            response = self.db.query(
                "SELECT file, tuids FROM tuid WHERE revision=" + quote_value(revision) +
                " AND file IN " + quote_list(files)
            )
            found = {file: json2value(tuids) for file, tuids in response.data}

            try:
                remaining = set(files) - set(found.keys())
                new_response = None
                if remaining:
                    request = wrap({
                        "from": "files",
                        "where": {"and": [
                            {"eq": {"revision": revision}},
                            {"in": {"path": remaining}},
                            {"eq": {"branch": branch}}
                        ]},
                        "branch": branch,
                        "meta": {
                            "format": "list",
                            "request_time": Date.now()
                        }
                    })
                    if self.push_queue is not None:
                        if DEBUG:
                            Log.note("record tuid request to SQS: {{timestamp}}", timestamp=request.meta.request_time)
                        self.push_queue.add(request)
                    else:
                        if DEBUG:
                            Log.note("no recorded tuid request")

                    if not self.enabled:
                        return found

                    new_response = http.post_json(
                        self.endpoint,
                        json=request,
                        timeout=self.timeout
                    )

                    if new_response.data and any(r.tuids for r in new_response.data):
                        try:
                            with self.db.transaction() as transaction:


                                command = "INSERT INTO tuid (revision, file, tuids) VALUES " + sql_list(
                                    quote_list((revision, r.path, value2json(r.tuids)))
                                    for r in new_response.data
                                    if r.tuids != None
                                )
                                if not command.endswith(" VALUES "):
                                    transaction.execute(command)
                        except Exception as e:
                            Log.error("can not insert {{data|json}}", data=new_response.data, cause=e)
                self.num_bad_requests = 0

                found.update({r.path: r.tuids for r in new_response.data} if new_response else {})
                return found

            except Exception as e:
                self.num_bad_requests += 1
                if self.enabled:
                    if "502 Bad Gateway" in e:
                        self.enabled = False
                        Log.error("TUID service has problems.", cause=e)
                    elif self.num_bad_requests >= MAX_BAD_REQUESTS:
                        self.enabled = False
                        Log.error("TUID service has problems.", cause=e)
                    else:
                        Log.warning("TUID service has problems.", cause=e)
                        Till(seconds=SLEEP_ON_ERROR).wait()
                return found
--------------------
Prompt: 
def remapOpenCv(im, coords):
    """
    Remap an image using OpenCV. See :func:`remap` for parameters.
    """
----------
Output: 

    if im.ndim == 2:
        return cv2.remap(im, coords, None, cv2.INTER_LINEAR)
    else:
        return cv2.warpAffine(im, coords, (int(img.shape[1] * 0.5), int(img.shape[0] * 0.5)), flags=cv2.INTER_LINEAR)


def remapCv2(coords):
    """
    Remap an image using OpenCV. See :func:`remap` for parameters.
    """
    if coords.ndim == 2:
        return cv2.
----------
Solution: 

    # required for older OpenCV versions
    im = np.require(im, im.dtype, 'C')
    return cv2.remap(im, coords, None, cv2.INTER_LANCZOS4)
--------------------
Prompt: 
def _invert(self, tempdir, catch_output=True, **kwargs):
        """Internal function than runs an inversion using CRTomo.

        Parameters
        ----------
        tempdir : string
            directory which to use as a tomodir
        catch_output : bool, optional
            if True, catch all outputs of the CRTomo call (default: True)
        cores : int, optional
            how many cores to use. (default 2)
        """
----------
Output: 

        # TODO: this is a hack to get around a bug in the C++ code that causes
        #       the test to fail when the test is run on a single core
        #       (e.g. when using multiple cores)
        #       see issue #1074
        if catch_output:
            tempdir = tempfile.mkdtemp()
            try:
                catch_output_args = (tempdir, tempdir, tempdir, tempdir)
                for arg in catch_output_args:
                    if arg is not None:
                        arg = str(arg)
                    if arg is not None:
                        os.environ[arg] =
----------
Solution: 

        nr_cores = kwargs.get('cores', 2)
        print('attempting inversion in directory: {0}'.format(tempdir))
        pwd = os.getcwd()
        os.chdir(tempdir)

        self.save_to_tomodir('.')
        os.chdir('exe')
        binary = CRBin.get('CRTomo')
        print('Using binary: {0}'.format(binary))
        print('calling CRTomo')
        # store env variable
        env_omp = os.environ.get('OMP_NUM_THREADS', '')
        os.environ['OMP_NUM_THREADS'] = '{0}'.format(nr_cores)
        if catch_output:
            subprocess.check_output(
                binary,
                shell=True,
                stderr=subprocess.STDOUT,
            )
        else:
            subprocess.call(
                binary,
                shell=True,
            )
        # reset environment variable
        os.environ['OMP_NUM_THREADS'] = env_omp

        print('finished')

        os.chdir(pwd)
        self.read_inversion_results(tempdir)
--------------------
Prompt: 
def exists(provider, config_location=DEFAULT_CONFIG_DIR):
    """Check whether provider info is already stored """
----------
Output: 

    if not os.path.isdir(config_location):
        return False

    return provider in config.get_all()


def get_provider_name(provider):
    """Return the name of the provider """
    return provider.get_name()


def get_provider_type(provider):
    """Return the type of the provider """
    return provider.get_type()


def get_provider_class(provider):
    """Return the class of the provider """
    return provider.get_class()


def get_provider_type_by_name(provider, name):
   
----------
Solution: 


    config_dir = os.path.join(config_location, NOIPY_CONFIG)
    auth_file = os.path.join(config_dir, provider)
    return os.path.exists(auth_file)
--------------------
Prompt: 
def coarsen(self, windows, func, boundary='exact', side='left'):
        """
        Apply
        """
----------
Output: 

        if not self.is_valid_window(windows):
            raise ValueError("Window must be a valid window")

        if not isinstance(func, np.ndarray):
            func = np.array(func)

        if not isinstance(boundary, str):
            boundary = str(boundary)

        if not isinstance(windows, list):
            windows = [windows]

        if not isinstance(windows[0], list):
            windows = [windows]

        if len(windows) == 1:
            windows = windows[0]

        windows = [self._get_window(w) for w in windows]
        windows =
----------
Solution: 

        windows = {k: v for k, v in windows.items() if k in self.dims}
        if not windows:
            return self.copy()

        reshaped, axes = self._coarsen_reshape(windows, boundary, side)
        if isinstance(func, str):
            name = func
            func = getattr(duck_array_ops, name, None)
            if func is None:
                raise NameError('{} is not a valid method.'.format(name))
        return type(self)(self.dims, func(reshaped, axis=axes), self._attrs)
--------------------
Prompt: 
def firstChild(self):
        """
            firstChild - property, Get the first child block, text or tag.

                @return <str/AdvancedTag/None> - The first child block, or None if no child blocks
        """
----------
Output: 

        return self.get(0)

    def lastChild(self):
        """
            lastChild - property, Get the last child block, text or tag.

                @return <str/AdvancedTag/None> - The last child block, or None if no child blocks
        """
        return self.get(len(self) - 1)

    def nextSibling(self):
        """
            nextSibling - property, Get the next sibling block, text or tag.

                @return <AdvancedTag/None> - The next sibling block, or None if no next blocks
        """
        return self.get(len(self
----------
Solution: 

        blocks = object.__getattribute__(self, 'blocks')
        # First block is empty string for indent, but don't hardcode incase that changes
        if blocks[0] == '':
           firstIdx = 1
        else:
           firstIdx = 0

        if len(blocks) == firstIdx:
            # No first child
            return None

        return blocks[1]
--------------------
Prompt: 
def taskfileinfo_path_data(tfi, role):
    """Return the data for path

    :param tfi: the :class:`jukeboxcore.filesys.TaskFileInfo` holds the data
    :type tfi: :class:`jukeboxcore.filesys.TaskFileInfo`
    :param role: item data role
    :type role: QtCore.Qt.ItemDataRole
    :returns: data for the path
    :rtype: depending on role
    :raises: None
    """
----------
Output: 

    if role == QtCore.Qt.DisplayRole:
        return tfi.path
    elif role == QtCore.Qt.EditRole:
        return tfi.edit_path
    elif role == QtCore.Qt.ToolTipRole:
        return tfi.tooltip_path
    elif role == QtCore.Qt.UserRole:
        return tfi.user_path
    elif role == QtCore.Qt.UserRole + 1:
        return tfi.user_path + 1
    else:
        raise ValueError("Unknown data role")


def taskfileinfo_data_changed(tfi, role):
    """Return the
----------
Solution: 

    if role == QtCore.Qt.DisplayRole or role == QtCore.Qt.EditRole:
        return JB_File(tfi).get_fullpath()
--------------------
Prompt: 
def preserve_cwd(func: Callable) -> Callable:
    """
    Decorator to preserve the current working directory in calls to the
    decorated function.

    Example:

    .. code-block:: python

        @preserve_cwd
        def myfunc():
            os.chdir("/faraway")

        os.chdir("/home")
        myfunc()
        assert os.getcwd() == "/home"
    """
----------
Output: 


    @functools.wraps(func)
    def wrapper(*args: Any, **kwargs: Any) -> Callable:
        with chdir(os.path.abspath(os.path.expanduser(args[0])):
            return func(*args, **kwargs)
    return wrapper


def get_test_config() -> ConfigType:
    """
    Return a ConfigType object with the default settings.
    """

    return Config(
        data_dir=os.path.join(os.path.expanduser("~"), ".local", "share", "test-data"),
        project_dir=os.path.join(
----------
Solution: 

    # http://stackoverflow.com/questions/169070/python-how-do-i-write-a-decorator-that-restores-the-cwd  # noqa
    def decorator(*args_, **kwargs) -> Any:
        cwd = os.getcwd()
        result = func(*args_, **kwargs)
        os.chdir(cwd)
        return result
    return decorator
--------------------
Prompt: 
def add_dos(self, label, dos):
        """
        Adds a dos for plotting.

        Args:
            label:
                label for the DOS. Must be unique.
            dos:
                Dos object
        """
----------
Output: 

        self.dos.append(dos)

    def add_dos_line(self, label, line):
        """
        Adds a single DOS line.

        Args:
            label:
                label for the line. Must be unique.
            line:
                Dos line object
        """
        self.dos_lines.append(self.Line(label, line))

    def add_dos_label(self, label, label_type):
        """
        Adds a DOS label.

        Args:
            label:
                label for the DOS. Must be unique.
            label_type:
                label type for the
----------
Solution: 

        energies = dos.energies - dos.efermi if self.zero_at_efermi \
            else dos.energies
        densities = dos.get_smeared_densities(self.sigma) if self.sigma \
            else dos.densities
        efermi = dos.efermi
        self._doses[label] = {'energies': energies, 'densities': densities,
                              'efermi': efermi}
--------------------
Prompt: 
def shutdown(self):
        """Revert stdin and stdout, close the socket."""
----------
Output: 

        self.close()
        self.sock.close()
        self.sock = None
        self.rfile.close()

    def close(self):
        """Close the socket."""
        self.rfile = None
        self.close_on_exec = True
        self.close_on_exit = True
        self.rfile = None
        self.close_on_exec = False
        self.rfile = None
        self.wfile = None
        self.wfile = None
        self.shutdown_lock.release()

    def get_request(self):
        """Get the request and client address from the socket."""
        self
----------
Solution: 

        sys.stdout = self.old_stdout
        sys.stdin = self.old_stdin
        self.skt.close()
        self.set_continue()
--------------------
Prompt: 
def update(self, item, **kwargs):
        """
        Update the information in this instance.
        
        :param item: a dictionary or a Message instance 
        """
----------
Output: 

        if item is None:
            return
        if isinstance(item, dict):
            self.update_dict(item)
        elif isinstance(item, Message):
            self.update_message(item)
        elif isinstance(item, MessageList):
            self.update_message_list(item)
        elif isinstance(item, MessageListList):
            self.update_message_list_list(item)
        elif isinstance(item, MessageListListList):
            self.update_message_list_list(item)
        else:
            raise TypeError("Invalid type for update: %s" % item)

   
----------
Solution: 

        if isinstance(item, dict):
            self._dict.update(item)
        elif isinstance(item, Message):
            for key, val in item.items():
                self._dict[key] = val
        else:
            raise ValueError("Can't update message using: '%s'" % (item,))
--------------------
Prompt: 
def matches(self, properties):
        """
        Tests if the given criterion matches this LDAP criterion

        :param properties: A dictionary of properties
        :return: True if the properties matches this criterion, else False
        """
----------
Output: 

        return self.get_dn(properties) == self.dn

    def get_dn(self, properties):
        """
        Returns the DN of the LDAP entry

        :param properties: A dictionary of properties
        :return: The DN of the LDAP entry
        """
        return self.ldap_dn_from_properties(properties)

    def get_object_sid(self, properties):
        """
        Returns the SID of the object

        :param properties: A dictionary of properties
        :return: The SID of the object
        """
        return self.get_sid(properties) == self.sid

    def
----------
Solution: 

        try:
            # Use the comparator
            return self.comparator(self.value, properties[self.name])
        except KeyError:
            # Criterion key is not in the properties
            return False
--------------------
Prompt: 
def keep_only_positive_boxes(boxes):
    """
    Given a set of BoxList containing the `labels` field,
    return a set of BoxList for which `labels > 0`.

    Arguments:
        boxes (list of BoxList)
    """
----------
Output: 

    return {box for box in boxes if box.num_boxes > 0 and box.num_boxes <= 1}


def top_k_boxes(boxes):
    """
    Given a set of BoxList containing the `labels` field,
    return a set of BoxList for which `labels < k` - 1.

    Arguments:
        boxes (list of BoxList)
    """
    return {box for box in boxes if box.num_boxes > 0 and box.num_boxes > k}


def bottom_k_boxes(boxes):
    """
    Given a set of BoxList containing
----------
Solution: 

    assert isinstance(boxes, (list, tuple))
    assert isinstance(boxes[0], BoxList)
    assert boxes[0].has_field("labels")
    positive_boxes = []
    positive_inds = []
    num_boxes = 0
    for boxes_per_image in boxes:
        labels = boxes_per_image.get_field("labels")
        inds_mask = labels > 0
        inds = inds_mask.nonzero().squeeze(1)
        positive_boxes.append(boxes_per_image[inds])
        positive_inds.append(inds_mask)
    return positive_boxes, positive_inds
--------------------
Prompt: 
def render_reply(self):
        """
        Create a :class:`~.Data` object equal to the object from which the from
        was created through :meth:`from_xso`, except that the values of the
        fields are exchanged with the values set on the form.

        Fields which have no corresponding form descriptor are left untouched.
        Fields which are accessible through form descriptors, but are not in
        the original :class:`~.Data` are not included in the output.

        This method only works on forms created through :meth:`from_xso`.

        The resulting :class:`~.Data` instance has the :attr:`~.Data.type_` set
        to :attr:`~.DataType.SUBMIT`.
        """
----------
Output: 
:`reply_to` and
        :attr:`from_reply_to` attributes set.

        :return: a new :class:`~.Data` object with the values set on the form
        :rtype: :class:`~.Form`
        """
        return Form(self.data, self.reply_to, self.from_xso, self.data_key, self.data_value)

    def __str__(self):
        """
        Return a string representation of the :class:`~.Data` object.

        :return: a string representation of the :class:`~.Data` object
        :rtype: :class:`~.String
----------
Solution: 


        data = copy.copy(self._recv_xso)
        data.type_ = forms_xso.DataType.SUBMIT
        data.fields = list(self._recv_xso.fields)

        for i, field_xso in enumerate(data.fields):
            if field_xso.var is None:
                continue
            if field_xso.var == "FORM_TYPE":
                continue
            key = fields.descriptor_ns, field_xso.var
            try:
                descriptor = self.DESCRIPTOR_MAP[key]
            except KeyError:
                continue

            bound_field = descriptor.__get__(self, type(self))
            data.fields[i] = bound_field.render(
                use_local_metadata=False
            )

        return data
--------------------
Prompt: 
def tokenize_middle_high_german_words(text):
    """Tokenizes MHG text"""
----------
Output: 

    return [word for word in re.findall(r'\w+', text) if word not in ['M', 'M', 'M', 'M']]


def tokenize_middle_high_german_sents(text):
    """Tokenizes MHG text"""
    return [word for word in re.findall(r'\w+', text) if word not in ['M', 'M', 'M']]


def tokenize_middle_high_german_words(text):
    """Tokenizes MHG text"""
    return [word for word in re.findall(r'\w+', text
----------
Solution: 


    assert isinstance(text, str)
    # As far as I know, hyphens were never used for compounds, so the tokenizer treats all hyphens as line-breaks
    text = re.sub(r'-\n',r'-', text)
    text = re.sub(r'\n', r' ', text)
    text = re.sub(r'(?<=.)(?=[\.\";\,\:\[\]\(\)!&?])',r' ', text)
    text = re.sub(r'(?<=[\.\";\,\:\[\]\(\)!&?])(?=.)',r' ', text)
    text = re.sub(r'\s+',r' ', text)
    text = str.split(text)

    return text
--------------------
Prompt: 
def parse_cli_args():
    """parse args from the CLI and return a dict"""
----------
Output: 

    parser = argparse.ArgumentParser(description='Run a test suite.')
    parser.add_argument('--verbose', '-v', action='store_true',
                        help='Verbose output')
    parser.add_argument('--debug', '-d', action='store_true',
                        help='Debug output')
    parser.add_argument('--no-build', '-n', action='store_true',
                        help='Do not build the test suite')
    parser.add_argument('--profile', '-p', default=None,
                        help='Profile to use')
    parser.add_argument('--with-coverage', action='
----------
Solution: 

    parser = argparse.ArgumentParser(description='2048 in your terminal')
    parser.add_argument('--mode', dest='mode', type=str,
                        default=None, help='colors mode (dark or light)')
    parser.add_argument('--az', dest='azmode', action='store_true',
                        help='Use the letters a-z instead of numbers')
    parser.add_argument('--resume', dest='resume', action='store_true',
                        help='restart the game from where you left')
    parser.add_argument('-v', '--version', action='store_true')
    parser.add_argument('-r', '--rules', action='store_true')
    return vars(parser.parse_args())
--------------------
Prompt: 
def url_join(url, path):
    """ url version of os.path.join
    """
----------
Output: 

    return urlparse.urljoin(url, path)


def url_join_path(url, path):
    """ url version of os.path.join_path
    """
    return os.path.join(url, path)


def url_join_path_native(url, path):
    """ url version of os.path.join_path_native
    """
    return os.path.join_path_native(url, path)


def url_join_path_linux(url, *args):
    """ url version of os.path.join_path_linux
    """
----------
Solution: 

    p = six.moves.urllib.parse.urlparse(url)

    t = None
    if p.path and p.path[-1] == '/':
        if path and path[0] == '/':
            path = path[1:]
        t = ''.join([p.path, path])
    else:
        t = ('' if path and path[0] == '/' else '/').join([p.path, path])

    return six.moves.urllib.parse.urlunparse(
        p[:2]+
        (t,)+ # os.sep is different on windows, don't use it here.
        p[3:]
    )
--------------------
Prompt: 
def defaults(self):
        """Return the defaults, with their values interpolated (with the
        defaults dict itself)

        Mainly to support defaults using values such as %(here)s
        """
----------
Output: 

        return self.defaults

    def __call__(self, value):
        """Return the value, with the appropriate substitutions"""
        if value is None:
            return ''
        return self.defaults.get(value, value)

    def __getitem__(self, key):
        """Return the value, with the appropriate substitutions"""
        if key in self.defaults:
            return self.defaults[key]
        return None

    def __setitem__(self, key, value):
        """Set the value, with the appropriate substitutions"""
        if key in self.defaults:
            self.defaults[key] = value
        else:
           
----------
Solution: 

        defaults = ConfigParser.defaults(self).copy()
        for key, val in iteritems(defaults):
            defaults[key] = self.get('DEFAULT', key) or val
        return defaults
--------------------
Prompt: 
def speak(self, speech, play_behavior=None):
        # type: (str, PlayBehavior) -> 'ResponseFactory'
        """Say the provided speech to the user.

        :param speech: the output speech sent back to the user.
        :type speech: str
        :param play_behavior: attribute to control alexa's speech
            interruption
        :type play_behavior: ask_sdk_model.ui.play_behavior.PlayBehavior
        :return: response factory with partial response being built and
            access from self.response.
        :rtype: ResponseFactory
        """
----------
Output: 

        return ResponseFactory(self.response, speech, play_behavior)

    def speak_async(self, play_behavior=None):
        # type: (str, PlayBehavior) -> 'ResponseFactory'
        """Say the provided playback to the user.

        :param playback: the output speech access from the user.
        :type playback: str
        :return: response factory with asynchronous response being built
            and access from self.response.
        :rtype: ResponseFactory
        """
        return ResponseFactory(self.response, playback, None)

    def speak_async_async(
----------
Solution: 

        ssml = "<speak>{}</speak>".format(self.__trim_outputspeech(
            speech_output=speech))
        self.response.output_speech = SsmlOutputSpeech(
            ssml=ssml, play_behavior=play_behavior)
        return self
--------------------
Prompt: 
def polygon_scale(polygon):
    """
    For a Polygon object, return the diagonal length of the AABB.

    Parameters
    ------------
    polygon: shapely.geometry.Polygon object

    Returns
    ------------
    scale: float, length of AABB diagonal
    """
----------
Output: 

    return polygon.area / 2.0


def polygon_length(polygon):
    """
    For a Polygon object, return the length of the AABB.

    Parameters
    ------------
    polygon: shapely.geometry.Polygon object

    Returns
    ------------
    length: float, diagonal length of the polygon
    """
    return np.linalg.norm(np.cross(polygon.exterior.coords, polygon.interior.coords))


def get_polygon_center(polygon):
    """
    For a Polygon object, return the center of the AABB.

    Parameters
    ------------
    polygon
----------
Solution: 

    extents = np.reshape(polygon.bounds, (2, 2)).ptp(axis=0)
    scale = (extents ** 2).sum() ** .5

    return scale
--------------------
Prompt: 
def check_login(func):
    """检查用户登录状态
    :param func: 需要被检查的函数
    """
----------
Output: 

    @wraps(func)
    def wrapper(*args, **kwargs):
        if not current_user.is_authenticated:
            return redirect(url_for('login'))
        return func(*args, **kwargs)
    return wrapper


@app.route('/logout')
def logout():
    """检查用户登出确认状态
    :param current_user: 用户对象
    """
    logout_user()
    return redirect(url_for('index'))


@app.route('/login', methods=['GET', 'POST'])
def login():
----------
Solution: 


    @wraps(func)
    def wrapper(*args, **kwargs):
        ret = func(*args, **kwargs)
        if type(ret) == requests.Response:
            # 检测结果是否为JSON
            if ret.content[0]!=b'{' and ret.content[0]!=b'[':
                return ret
            try:
                foo = json.loads(ret.content.decode('utf-8'))
                if 'errno' in foo and foo['errno'] == -6:
                    logging.debug(
                            'Offline, deleting cookies file then relogin.')
                    path = '.{0}.cookies'.format(args[0].username)
                    if os.path.exists(path):
                        os.remove(path)
                    args[0]._initiate()
            except:
                raise LoginFailed('User unsigned in.')
        return ret

    return wrapper
--------------------
Prompt: 
def _get_kwarg(self, name, kwargs):
        """
        Helper to get value of a named attribute irrespective of whether it is passed
        with or without "@" prefix.
        """
----------
Output: 

        if name.startswith('@') and name.endswith('@'):
            name = name[1:-1]
        return kwargs.get(name, None)

    def _get_attr_type(self, obj, name):
        """
        Helper to get the type of an attribute.
        """
        if obj is None:
            return None
        return obj.get_type_string()

    def _get_attr_name(self, obj, name):
        """
        Helper to get the name of an attribute.
        """
        if obj is None:
            return None
        return obj.get_name()

    def
----------
Solution: 

        at_name = '@{}'.format(name)

        if name in kwargs:
            if at_name in kwargs:
                raise ValueError('Both {!r} and {!r} specified in kwargs'.format(name, at_name))
            return kwargs[name]

        if at_name in kwargs:
            return kwargs[at_name]

        return not_set
--------------------
Prompt: 
def iter_variants(self):
        """Iterate over the variants within this package, in index order.

        Returns:
            `Variant` iterator.
        """
----------
Output: 

        for variant in self.variants:
            yield variant

    def iter_variants_by_id(self, id):
        """Iterate over the variants within this package, in order.

        Args:
            id (str): Package id.

        Returns:
            `Variant` iterator.
        """
        for variant in self.iter_variants():
            if id == variant.id:
                yield variant

    def iter_variants_by_name(self, name):
        """Iterate over the variants within this package, in name order.

        Args:
            name (str): Package name.

        Returns:
            `Variant
----------
Solution: 

        for variant in self.repository.iter_variants(self.resource):
            yield Variant(variant, context=self.context, parent=self)
--------------------
Prompt: 
def url_view(url_pattern, name=None, priority=None):
    """
    Decorator for registering functional views.
    Meta decorator syntax has to be used in order to accept arguments.

    This decorator does not really do anything that magical:

    This:
    >>> from urljects import U, url_view
    >>> @url_view(U / 'my_view')
    ... def my_view(request)
    ...     pass

    is equivalent to this:
    >>> def my_view(request)
    ...     pass
    >>> my_view.urljects_view = True
    >>> my_view.url = U / 'my_view'
    >>> my_view.url_name = 'my_view'

    Those view are then supposed to be used with ``view_include`` which will
    register all views that have ``urljects_view`` set to ``True``.

    :param url_pattern: regex or URLPattern or anything passable to url()
    :param name: name of the view, __name__ will be used otherwise.
    :param priority: priority of the view, the lower the better
    """
----------
Output: 
 >>> my_view.url_pattern ='my_view'
    >>> my_view.priority ='my_view'
    >>> my_view.url_view(url_pattern='http://example.com/my_view')
   ... def my_view(url, request):
   ...     return request.response.body

    """
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            if url_pattern is not None:
                return url_pattern.format(*args, **kwargs)
            if priority is not None:
                return priority
----------
Solution: 


    def meta_wrapper(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            return func(*args, **kwargs)

        wrapper.urljects_view = True
        wrapper.url = url_pattern
        wrapper.url_name = name or func.__name__
        wrapper.url_priority = priority

        return wrapper
    return meta_wrapper
--------------------
Prompt: 
def _get_tasks_by_queue(self):
        """Return the tasks for this Context, grouped by queue."""
----------
Output: 

        return self.tasks.values()

    def get_task(self, task_id):
        """Return the task with the given task_id."""
        if task_id not in self._get_tasks_by_id:
            raise ValueError("No task with id %s" % task_id)
        return self._get_tasks_by_id[task_id][0]

    def get_task_by_id(self, task_id):
        """Return the task with the given task_id."""
        if task_id not in self._get_tasks_by_id:
----------
Solution: 

        task_map = {}
        _checker = None

        # Ask the persistence engine for an Async to use for checking if the
        # context is complete.
        if self._persistence_engine:
            _checker = self._persistence_engine.context_completion_checker

        for async in self._tasks:
            queue = async.get_queue()
            if _checker:
                async.update_options(_context_checker=_checker)

            task = async.to_task()
            task_map.setdefault(queue, []).append(task)

        return task_map
--------------------
Prompt: 
async def fromURL(
            cls, url, *, credentials=None, insecure=False):
        """Return a `SessionAPI` for a given MAAS instance."""
----------
Output: 

        return cls(
            self._loop,
            loop=self._loop,
            timeout=self._timeout,
            ssl=self._ssl,
            auth=self._auth,
            verify=self._verify,
            caCerts=self._ca_certs,
            loop=self._loop,
            sslContext=self._ssl_context,
            url=url,
            credentials=credentials,
            insecure=insecure)

    @asyncio.coroutine
    def asyncGet(self, request, timeout=None):
        """Return a `SessionAPI` for a given MAAS instance."""
        if timeout is None:
            request
----------
Solution: 

        try:
            description = await helpers.fetch_api_description(
                url, insecure=insecure)
        except helpers.RemoteError as error:
            # For now just re-raise as SessionError.
            raise SessionError(str(error))
        else:
            session = cls(description, credentials)
            session.insecure = insecure
            return session
--------------------
Prompt: 
def _ParseContinue(s):
  """Parses a Unicode continuation field.

  These are of the form '<Name, First>' or '<Name, Last>'.
  Instead of giving an explicit range in a single table entry,
  some Unicode tables use two entries, one for the first
  code value in the range and one for the last.
  The first entry's description is '<Name, First>' instead of 'Name'
  and the second is '<Name, Last>'.

    '<Name, First>' => ('Name', 'First')
    '<Name, Last>' => ('Name', 'Last')
    'Anything else' => ('Anything else', None)

  Args:
    s: continuation field string

  Returns:
    pair: name and ('First', 'Last', or None)
  """
----------
Output: 
 => ('Name', 'Anything else')

  Args:
    s: The Unicode string to parse.

  Returns:
    The parsed Unicode string.
  """
  if s.startswith(u'\u0000'):
    return s[2:]
  else:
    return s


def _ParseControlChar(s):
  """Parses a control character field.

  Args:
    s: The control character to parse.

  Returns:
    The parsed control character.
  """
  if s.startswith(u'\u0000'):
    return s[2:]
  else:
    return s


def _ParseControlWord(s
----------
Solution: 


  match = re.match("<(.*), (First|Last)>", s)
  if match is not None:
    return match.groups()
  return (s, None)
--------------------
Prompt: 
def make_record(level, xref_id, tag, value, sub_records, offset, dialect,
                parser=None):
    """Create Record instance based on parameters.

    :param int level: Record level number.
    :param str xref_id: Record reference ID, possibly empty.
    :param str tag: Tag name.
    :param value: Record value, possibly empty. Value can be None, bytes, or
        string object, if it is bytes then it should be decoded into strings
        before calling freeze(), this is normally done by the parser which
        knows about encodings.
    :param list sub_records: Initial list of subordinate records,
        possibly empty. List can be updated later.
    :param int offset: Record location in a file.
    :param dialect: One of DIALECT_* constants.
    :param parser: Instance of `GedcomReader` class, only needed for
        records whose walue is a pointer.
    :return: Instance of :py:class:`Record` (or one of its subclasses).
    """
----------
Output: 
 str sub_records: Sub-records, possibly empty.
        Sub-records should be a sequence of bytes.
    :param int offset: Offset of record in bytes.
    :param dialect: The dialect to use to interpret record values.
    :param str parser: The parser to use to interpret record values.
    :return: The Record instance.
    :rtype: :class:`Record`
    """
    if parser is None:
        parser = RecordParser()
    return parser.make_record(xref_id, tag, value, sub_records, offset)


def decode_record(xref_id,
----------
Solution: 

    # value can be bytes or string so we check for both, 64 is code for '@'
    if value and len(value) > 2 and \
        ((value[0] == '@' and value[-1] == '@') or
         (value[0] == 64 and value[-1] == 64)):
        # this looks like a <pointer>, make a Pointer record
        klass = Pointer
        rec = klass(parser)
    else:
        klass = _tag_class.get(tag, Record)
        rec = klass()

    rec.level = level
    rec.xref_id = xref_id
    rec.tag = tag
    rec.value = value
    rec.sub_records = sub_records
    rec.offset = offset
    rec.dialect = dialect
    return rec
--------------------
Prompt: 
def _decdeg_distance(pt1, pt2):
    """
    Earth surface distance (in km) between decimal latlong points using
    Haversine approximation.

    http://stackoverflow.com/questions/15736995/
    how-can-i-quickly-estimate-the-distance-between-two-latitude-longitude-
    points
    """
----------
Output: 

    lat1 = pt1[0]
    lat2 = pt2[0]
    lat3 = pt1[1]
    lat4 = pt2[1]
    lat5 = pt1[2]
    lat6 = pt1[3]
    lat7 = pt2[3]
    lat8 = pt2[4]
    lat9 = pt3[0]
    lat10 = pt3[1]
    lat11 = pt3[2]
    lat12 = pt3[3]
    lat13 = pt3[4]
    lat14 = pt4[0]
    lat
----------
Solution: 


    lat1, lon1 = pt1
    lat2, lon2 = pt2

    # Convert decimal degrees to radians
    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])

    # haversine formula
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
    c = 2 * np.arcsin(np.sqrt(a))
    km = 6367 * c

    return km
--------------------
Prompt: 
def setup_top(self):
        """Create top-level elements of the hybrid schema."""
----------
Output: 

        self.top = self.create_element('top')
        self.top.set('name', 'Top')
        self.top.set('type', 'object')
        self.top.append(self.create_element('object', {'name': 'foo'}))
        self.top.append(self.create_element('object', {'name': 'bar'}))
        self.top.append(self.create_element('object', {'name': 'baz'})
        self.top.append(self.create_element('object', {'name': 'quux'})
        self.top.append(
----------
Solution: 

        self.top_grammar = SchemaNode("grammar")
        self.top_grammar.attr = {
            "xmlns": "http://relaxng.org/ns/structure/1.0",
            "datatypeLibrary": "http://www.w3.org/2001/XMLSchema-datatypes"}
        self.tree = SchemaNode("start")
--------------------
Prompt: 
def partitionBy(self, numPartitions, partitionFunc=portable_hash):
        """
        Return a copy of the DStream in which each RDD are partitioned
        using the specified partitioner.
        """
----------
Output: 

        return self.map(lambda x: x.partitioner).filter(lambda x: x > numPartitions).collect()

    def foreachPartition(self, func):
        """
        Apply a function to each RDD in this group and return a result that
        contains all of the elements in each RDD resulting from applying the function
        to each RDD.

        >>> x = sc.parallelize([("a", 1), ("b", 4)])
        >>> def f(iterator):
       ...     for x in iterator:
       ...         yield x
        >>> x.foreachPartition(f)
        Traceback (most recent call last):
           ...
----------
Solution: 

        return self.transform(lambda rdd: rdd.partitionBy(numPartitions, partitionFunc))
--------------------
Prompt: 
def _ModifiedDecoder(wire_type, decode_value, modify_value):
  """Like SimpleDecoder but additionally invokes modify_value on every value
  before storing it.  Usually modify_value is ZigZagDecode.
  """
----------
Output: 


  def InnerDecode(buffer, pos):
    """Inner decode function that adds the new line to the buffer, and
    returns the new line number.
    """

    return wire_type * len(modify_value) + pos

  return InnerDecode


def _ModifiedEncoder(wire_type, modify_value, modify_state):
  """Like _ModifiedDecoder but additionally invokes modify_state on every
  encoder line that matches the modify_value.  Usually modify_value is ZigZagEncode.
  """

  if wire_type == wire_format.WIRETYPE_VARINT:
    modify_value
----------
Solution: 


  # Reusing _SimpleDecoder is slightly slower than copying a bunch of code, but
  # not enough to make a significant difference.

  def InnerDecode(buffer, pos):
    (result, new_pos) = decode_value(buffer, pos)
    return (modify_value(result), new_pos)
  return _SimpleDecoder(wire_type, InnerDecode)
--------------------
Prompt: 
def _create_server(host, port):
    """
    Helper function.  Creates a listening socket on the designated
    host and port.  Modeled on the socket.create_connection()
    function.
    """
----------
Output: 

    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.setblocking(0)
    s.connect((host, port))
    return s


def _create_client(host, port):
    """
    Helper function.  Creates a client socket on the designated
    host and port.  Modeled on the socket.create_connection()
    function.
    """
    c = _create_connection(host, port)
    return c


def _create_connection(host, port):
    """
    Helper function.  Creates a new socket connection to the
----------
Solution: 


    exc = socket.error("getaddrinfo returns an empty list")
    for res in socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM):
        af, socktype, proto, canonname, sa = res
        sock = None
        try:
            # Create the listening socket
            sock = socket.socket(af, socktype, proto)
            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            sock.bind(sa)
            sock.listen(1024)
            return sock

        except socket.error as exc:
            # Clean up after ourselves
            if sock is not None:
                sock.close()

    # Couldn't create a listening socket
    raise exc
--------------------
Prompt: 
def injections_from_cli(opts):
    """Gets injection parameters from the inference file(s).

    Parameters
    ----------
    opts : argparser
        Argparser object that has the command-line objects to parse.

    Returns
    -------
    FieldArray
        Array of the injection parameters from all of the input files given
        by ``opts.input_file``.
    """
----------
Output: 

    parser = ArgumentParser(prog='mx classify',
                            description='Evaluate a trained model.')
    parser.add_argument('--model', '-m', default='imagenet',
                        help='Model to use. Options are:'+
                        'imagenet, imagenet-mlp, imagenet-cnn, imagenet-nn, imagenet-mlp, imagenet-cnn, imagenet-nn, imagenet-mlp, imagenet-cnn, imagenet-nn, imagenet-mlp, imagenet-cnn,
----------
Solution: 

    input_files = opts.input_file
    if isinstance(input_files, str):
        input_files = [input_files]
    injections = None
    # loop over all input files getting the injection files
    for input_file in input_files:
        fp = loadfile(input_file, 'r')
        these_injs = fp.read_injections()
        if injections is None:
            injections = these_injs
        else:
            injections = injections.append(these_injs)
    return injections
--------------------
Prompt: 
def _bigger_than_zero(value: str) -> int:
    """Type evaluator for argparse."""
----------
Output: 

    return int(value) >= 0


def _parse_args() -> argparse.Namespace:
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description='A tool to generate a random password.')
    parser.add_argument(
        '--output', '-o',
        help='The output file path.',
        required=True,
        type=_bigger_than_zero,
        default=None)
    return parser.parse_args()


def _parse_args_and_config() -> argparse.Namespace:
    """Parse command line and config file options."""
    args = _parse
----------
Solution: 

    ivalue = int(value)
    if ivalue < 0:
        raise ArgumentTypeError(
            '{} should be bigger than 0'.format(ivalue)
        )
    return ivalue
--------------------
Prompt: 
def _render_item(self, dstack, key, value = None, **settings):
        """
        Format single tree line.
        """
----------
Output: 

        if value is None:
            value = self.get_default(key, self.tree_default)
        if value is None:
            value = self.get_default(key, self.tree_default)
        if value is None:
            value = self.get_default(key, self.tree_default)
        if value is None:
            value = self.get_default(key, self.tree_default)
        if value is None:
            value = self.get_default(key, self.tree_default)
        if value is None:
            value = self.get_
----------
Solution: 

        cur_depth = len(dstack) - 1

        treeptrn = ''
        s = self._es_text(settings, settings[self.SETTING_TREE_FORMATING])
        for ds in dstack:
            treeptrn += ' ' + self.fmt_text(self.tchar(settings[self.SETTING_TREE_STYLE], cur_depth, *ds), **s) + ''

        strptrn = "{}"
        if value is not None:
            strptrn += ": {}"
        s = self._es_text(settings, settings[self.SETTING_TEXT_FORMATING])
        strptrn = self.fmt_text(strptrn.format(key, value), **s)

        return '{} {}'.format(treeptrn, strptrn)
--------------------
Prompt: 
def run(self):
        """Run parent install, and then save the man file."""
----------
Output: 

        self.save_man_file()
        self.install_man_file()

    def save_man_file(self):
        """Save the man file."""
        man_path = os.path.join(self.install_dir,'share','man','man1')
        man_file = os.path.join(man_path,'man1', '1.0','man1')
        with open(man_file, 'w') as man_file:
            man_file.write(self.man_file_content)

    def install_man_file
----------
Solution: 

        install.run(self)

        # man directory
        if self._custom_man_dir is not None:
            if not os.path.exists(self._custom_man_dir):
                os.makedirs(self._custom_man_dir)
            shutil.copy("man/fades.1", self._custom_man_dir)
--------------------
Prompt: 
def trainGP(self,fast=False,scales0=None,fixed0=None,lambd=None):
        """
        Train the gp
       
        Args:
            fast:       if true and the gp has not been initialized, initializes a kronSum gp
            scales0:	initial variance components params
            fixed0:     initial fixed effect params
        """
----------
Output: 

        if scales0 is None:
            if fast:
                self.gp = kronSumGP(self.data)
            else:
                self.gp = kronSumGP(self.data)
        else:
            self.gp.setScales(scales0)
            self.gp.setFixed(fixed0)
            self.gp.setLambdas(lambd)
            self.data = self.data + self.data.mean()

    def testGP(self,fast=False,scales0=None,fixed0=None,test=False):
        """
        Test the
----------
Solution: 

        assert self.n_terms>0, 'CVarianceDecomposition:: No variance component terms'

        if not self.init:		self.initGP(fast=fast)

        # set lambda
        if lambd!=None:		self.gp.setLambda(lambd)

        # set scales0
        if scales0!=None:
            self.setScales(scales0)
        # init gp params
        self.vd.initGPparams()
        # set fixed0
        if fixed0!=None:
            params = self.gp.getParams()
            params['dataTerm'] = fixed0
            self.gp.setParams(params)

        # LIMIX CVARIANCEDECOMPOSITION TRAINING
        conv =self.vd.trainGP()
        
        self.cache['Sigma']   = None
        self.cache['Hessian'] = None
            
        return conv
--------------------
Prompt: 
def get_relationships_for_destination(self, destination_id=None):
        """Gets a ``RelationshipList`` corresponding to the given peer ``Id``.

        arg:    destination_id (osid.id.Id): a peer ``Id``
        return: (osid.relationship.RelationshipList) - the relationships
        raise:  NullArgument - ``destination_id`` is ``null``
        raise:  OperationFailed - unable to complete request
        raise:  PermissionDenied - authorization failure
        *compliance: mandatory -- This method must be implemented.*

        """
----------
Output: 

        return  # osid.relationship.RelationshipList

    @utilities.arguments_not_none
    def get_relationships_for_destination_on_date(self, source_id=None, destination_id=None, date=None):
        """Gets a ``RelationshipList`` corresponding to the given peer ``Id``
        and effective during the entire given date range inclusive but not
        limited to, or in the future.

        arg:    source_id (osid.id.Id): a peer ``Id``
        arg:    destination_id (osid.id.Id): a peer ``Id
----------
Solution: 

        if destination_id is None:
            raise NullArgument()
        url_path = ('/handcar/services/relationship/families/' +
                    self._catalog_idstr + '/relationships?sourceid=' +
                    str(destination_id))
        return objects.RelationshipList(self._get_request(url_path))
--------------------
Prompt: 
def cli(obj, role, scopes, delete):
    """Add or delete role-to-permission lookup entry."""
----------
Output: 

    if not obj.has_permission(role,'read', 'write'):
        raise PermissionDenied()
    if not obj.has_permission(role, 'write','manage'):
        raise PermissionDenied()
    if delete:
        obj.delete()
    else:
        for scope in scopes:
            if scope =='read':
                continue
            obj.permission(role, 'write', scope)
        for scope in scopes:
            if scope == 'write':
                continue
            obj.permission(role,'manage', scope)


def _get_user_permissions(user):
    """Return a list of permissions for
----------
Solution: 

    client = obj['client']
    if delete:
        client.delete_perm(delete)
    else:
        if not role:
            raise click.UsageError('Missing option "--role".')
        if not scopes:
            raise click.UsageError('Missing option "--scope".')
        try:
            perm = client.create_perm(role, scopes)
        except Exception as e:
            click.echo('ERROR: {}'.format(e))
            sys.exit(1)
        click.echo(perm.id)
--------------------
Prompt: 
def import_(module, objects=None, via=None):
    """
    :param module: py3 compatiable module path
    :param objects: objects want to imported, it should be a list
    :param via: for some py2 module, you should give the import path according the
        objects which you want to imported
    :return: object or module
    """
----------
Output: 

    if objects is None:
        objects = []
    if via is None:
        via = {}
    if isinstance(module, str):
        module = import_(module)
    if isinstance(module, types.ModuleType):
        module = import_(module.__name__)
    if isinstance(module, str):
        module = import_(module)
    if isinstance(module, types.ModuleType):
        module = import_(module.__name__)
    if isinstance(module, types.ModuleType):
        module = import_(module)
    if isinstance(module, types.ModuleType):
        module = import_(module)
    if isinstance(module,
----------
Solution: 

    if PY3:
        mod = __import__(module, fromlist=['*'])
    else:
        path = modules_mapping.get(module)
        if not path:
            raise Exception("Can't find the module %s in mappings." % module)
        if isinstance(path, list):
            if not via:
                raise Exception("You should give a via parameter to enable import from py2.")
            path = via
        mod = __import__(path, fromlist=['*'])

    if objects:
        if not isinstance(objects, (list, tuple)):
            raise Exception("objects parameter should be a list or tuple.")
        r = []
        for x in objects:
            r.append(getattr(mod, x))
        if len(r) > 1:
            return tuple(r)
        else:
            return r[0]
    else:
        return mod
--------------------
Prompt: 
def conditional_http_tween_factory(handler, registry):
    """
    Tween that adds ETag headers and tells Pyramid to enable 
    conditional responses where appropriate.
    """
----------
Output: 

    def conditional_http_tween(request):
        if request.method == 'POST':
            return handler(request, **request.form)
        else:
            return handler(request, **request.args)

    return registry.add_route(
        'tween', '/tween', handler=request_tween_factory,
        methods=['GET', 'HEAD', 'POST'])

def conditional_http_tween_factory_with_headers(handler, registry):
    """
    Tween that adds ETag headers and tells Pyramid to enable 
    conditional responses where appropriate.
    """
    def
----------
Solution: 

    settings = registry.settings if hasattr(registry, 'settings') else {}
    not_cacheble_list = []
    if 'not.cachable.list' in settings:
        not_cacheble_list = settings.get('not.cachable.list').split()

    def conditional_http_tween(request):
        response = handler(request)

        if request.path not in not_cacheble_list:

            # If the Last-Modified header has been set, we want to enable the
            # conditional response processing.
            if response.last_modified is not None:
                response.conditional_response = True

            # We want to only enable the conditional machinery if either we
            # were given an explicit ETag header by the view or we have a
            # buffered response and can generate the ETag header ourself.
            if response.etag is not None:
                response.conditional_response = True
            elif (isinstance(response.app_iter, Sequence) and
                          len(response.app_iter) == 1) and response.body is not None:
                response.conditional_response = True
                response.md5_etag()

        return response

    return conditional_http_tween
--------------------
Prompt: 
def submit_the_only_form(self):
    """
    Look for a form on the page and submit it.

    Asserts if more than one form exists.
    """
----------
Output: 

    for form in self.doc.xpath('//form'):
      if len(form.xpath('input', {'name': 'action', 'type':'submit'})) > 1:
        raise AssertionError('More than one form exists.')
      if len(form.xpath('input', {'name':'submit'})) == 1:
        return form.submit()
    raise AssertionError('No form found.')

  def get_form_by_id(self, id):
    """
    Look for a form on the page and return it.

    Args:
      id: The id of the form to look for.

    Returns:
----------
Solution: 

    form = ElementSelector(world.browser, str('//form'))
    assert form, "Cannot find a form on the page."
    form.submit()
--------------------
Prompt: 
def restart(name, timeout=10):
    """
    Restarts a container

    name
        Container name or ID

    timeout : 10
        Timeout in seconds after which the container will be killed (if it has
        not yet gracefully shut down)


    **RETURN DATA**

    A dictionary will be returned, containing the following keys:

    - ``status`` - A dictionary showing the prior state of the container as
      well as the new state
    - ``result`` - A boolean noting whether or not the action was successful
    - ``restarted`` - If restart was successful, this key will be present and
      will be set to ``True``.


    CLI Examples:

    .. code-block:: bash

        salt myminion docker.restart mycontainer
        salt myminion docker.restart mycontainer timeout=20
    """
----------
Output: 
 to False.
    """
    return _restart(name, timeout)


def stop(name, timeout=10):
    """
    Stops a container

    name
        Container name or ID

    timeout : 10
        Timeout in seconds after which the container will be killed (if it has
        not yet gracefully shut down)


    **RETURN DATA**

    A dictionary will be returned, containing the following keys:

    - ``status`` - A dictionary showing the prior state of the container as
      well as the new state
    - ``result`` - A boolean noting whether or not the action was successful
    - ``restarted``
----------
Solution: 

    ret = _change_state(name, 'restart', 'running', timeout=timeout)
    if ret['result']:
        ret['restarted'] = True
    return ret
--------------------
Prompt: 
def find(self, obj):
        """Returns the index of the given object in the queue, it might be string
        which will be searched inside each task.

        :arg obj: object we are looking

        :return: -1 if the object is not found or else the location of the task
        """
----------
Output: 

        if isinstance(obj, str):
            return self.index(obj)
        elif isinstance(obj, int):
            return self.location(obj)
        else:
            raise TypeError("Can't find object of type %s" % type(obj))

    def __iter__(self):
        """Returns an iterator over the tasks in the queue.

        :return: An iterator over the tasks in the queue
        """
        return self.tasks.__iter__()

    def __len__(self):
        """Returns the number of tasks in the queue.

        :return: -1 if the queue is empty
        """
        return self
----------
Solution: 

        if not self.connected:
            raise ConnectionError('Queue is not connected')

        data = self.rdb.lrange(self._name, 0, -1)
        for i, datum in enumerate(data):
            if datum.find(str(obj)) != -1:
                return i
        return -1
--------------------
Prompt: 
def marginBuy(self, currencyPair, rate, amount, lendingRate=None):
        """Places a margin buy order in a given market. Required POST
        parameters are "currencyPair", "rate", and "amount". You may optionally
         specify a maximum lending rate using the "lendingRate" parameter.
         If successful, the method will return the order number and any trades
         immediately resulting from your order."""
----------
Output: 

        if lendingRate is not None:
            if lendingRate < 0:
                raise ArgumentsRequired(self, "lendingRate must be non-negative")
            if lendingRate > len(currencyPair):
                raise ArgumentsRequired(self, "lendingRate must be less than lendingPair")
            if lendingRate < len(rate):
                raise ArgumentsRequired(self, "lendingRate must be less than lendingPair")
            if lendingRate > len(currencyPair) + 1:
                raise ArgumentsRequired(self, "lendingPair must be less than lendingPair")
            if
----------
Solution: 

        return self._private('marginBuy', currencyPair=currencyPair, rate=rate,
                             amount=amount, lendingRate=lendingRate)
--------------------
Prompt: 
def _build_filename_from_browserstack_json(j):
    """ Build a useful filename for an image from the screenshot json metadata """
----------
Output: 

    return "%s-%s.png" % (j["name"], j["id"])


def _build_filename_from_json(j):
    """ Build a useful filename for an image from the screenshot json metadata """
    return "%s-%s.json" % (j["name"], j["id"])


def _build_filename_from_local_json(j):
    """ Build a useful filename for an image from the screenshot local json metadata """
    return "%s-%s.png" % (j["name"], j["id"])


def _build_filename_from_
----------
Solution: 

    filename = ''
    device = j['device'] if j['device'] else 'Desktop'
    if j['state'] == 'done' and j['image_url']:
        detail = [device, j['os'], j['os_version'],
                  j['browser'], j['browser_version'], '.jpg']
        filename = '_'.join(item.replace(" ", "_") for item in detail if item)
    else:
        print 'screenshot timed out, ignoring this result'
    return filename
--------------------
Prompt: 
def transformer_parsing_base():
  """HParams for parsing on WSJ only."""
----------
Output: 

  hparams = common_hparams.HParams(
      data_dir=os.path.join(hparams.data_dir, 'transformer_data'),
      train_data_pattern=r'train_data/(\d+).txt',
      test_data_pattern=r'test_data/(\d+).txt',
      num_examples=1,
      num_classes=2,
      num_features=3,
      num_classes_per_layer=2,
      num_filters=32,
      filter_size=3,
      num_channels=4,
      num
----------
Solution: 

  hparams = transformer_base()
  hparams.attention_dropout = 0.2
  hparams.layer_prepostprocess_dropout = 0.2
  hparams.max_length = 512
  hparams.learning_rate_warmup_steps = 16000
  hparams.hidden_size = 1024
  hparams.learning_rate = 0.05
  hparams.shared_embedding_and_softmax_weights = False
  return hparams
--------------------
Prompt: 
def parse(self, filename):
        """
        Parses a file into an AstromData structure.

        Args:
          filename: str
            The name of the file whose contents will be parsed.

        Returns:
          data: AstromData
            The file contents extracted into a data structure for programmatic
            access.
        """
----------
Output: 

        with open(filename, 'r') as f:
            contents = f.read()

        return self._parse_data(contents, self._program_name_to_data_type)

    def _parse_data(self, contents, program_name_to_data_type):
      """
      Parses a string into a list of programmatic data structures.

      Args:
        contents: str
            The contents of the file.
        program_name_to_data_type: dict
            A dictionary mapping programmatic data types to the data structure
            that will be parsed.

      Returns:
        data: list
----------
Solution: 

        filehandle = storage.open_vos_or_local(filename, "rb")
        assert filehandle is not None, "Failed to open file {} ".format(filename)
        filestr = filehandle.read()
        filehandle.close()

        assert filestr is not None, "File contents are None"

        observations = self._parse_observation_list(filestr)

        self._parse_observation_headers(filestr, observations)

        sys_header = self._parse_system_header(filestr)

        sources = self._parse_source_data(filestr, observations)

        return AstromData(observations, sys_header, sources, discovery_only=self.discovery_only)
--------------------
Prompt: 
def remove_tag(self, key, value=None):
        """
        Remove a tag from this object.  Removing a tag involves a round-trip
        to the EC2 service.

        :type key: str
        :param key: The key or name of the tag being stored.

        :type value: str
        :param value: An optional value that can be stored with the tag.
                      If a value is provided, it must match the value
                      currently stored in EC2.  If not, the tag will not
                      be removed.  If a value of None is provided, all
                      tags with the specified name will be deleted.
                      NOTE: There is an important distinction between
                      a value of '' and a value of None.
        """
----------
Output: 
 key will be removed.
        """
        params = {'Action': 'RemoveTags',
                  'Key': key,
                  'Value': value}
        return self.get_status('RemoveTags', params)

    # Tag methods

    def get_all_tags(self, filters=None):
        """
        Retrieve all the tags associated with the current instance.

        :type filters: dict
        :param filters: Optional filters that can be used to limit
                        the results returned.  Filters are provided
                        in the form of a dictionary consisting of
                        filter names as the key and filter values
                        as the value.  The set of
----------
Solution: 

        if value:
            tags = {key : value}
        else:
            tags = [key]
        status = self.connection.delete_tags([self.id], tags)
        if key in self.tags:
            del self.tags[key]
--------------------
Prompt: 
def as_tuple(self, value):
        """Utility function which converts lists to tuples."""
----------
Output: 

        if isinstance(value, list):
            return tuple(as_tuple(self, v) for v in value)
        else:
            return value

    def __init__(self, value):
        """Constructor."""
        self.value = value

    def __repr__(self):
        """Return the representation."""
        return repr(self.value)

    def __str__(self):
        """Return the string representation."""
        return str(self.value)

    def __eq__(self, other):
        """Return whether other is equal to self."""
        return self.value == other.value

    def __ne__(self,
----------
Solution: 

        if isinstance(value, list):
            value = tuple(value)
        return value
--------------------
Prompt: 
def movable_items(self):
        """Filter selection

        Filter items of selection that cannot be moved (i.e. are not instances of `Item`) and return the rest.
        """
----------
Output: 

        return self.filter_items(self.selected_items)

    def filter_items(self, items):
        """Filter items

        Filter items of selection that cannot be moved (i.e. are not instances of `Item`) and return the rest.
        """
        if not items:
            return items
        if len(items) == 1:
            return items[0]
        if len(items) > 2:
            return items
        if len(items) == 0:
            return items[0]
        if len(items) == 2:
            return items[1]
        if len(items) >
----------
Solution: 

        view = self.view

        if self._move_name_v:
            yield InMotion(self._item, view)
        else:
            selected_items = set(view.selected_items)
            for item in selected_items:
                if not isinstance(item, Item):
                    continue
                yield InMotion(item, view)
--------------------
Prompt: 
def devices(self, timeout=None):
        """Executes adb devices -l and returns a list of objects describing attached devices.

        :param timeout: optional integer specifying the maximum time in
            seconds for any spawned adb process to complete before
            throwing an ADBTimeoutError.  This timeout is per adb call. The
            total time spent may exceed this value. If it is not
            specified, the value set in the ADBHost constructor is used.
        :returns: an object contain
        :raises: * ADBTimeoutError
                 * ADBError

        The output of adb devices -l ::

            $ adb devices -l
            List of devices attached
            b313b945               device usb:1-7 product:d2vzw model:SCH_I535 device:d2vzw

        is parsed and placed into an object as in

        [{'device_serial': 'b313b945', 'state': 'device', 'product': 'd2vzw',
          'usb': '1-7', 'device': 'd2vzw', 'model': 'SCH_I535' }]

        """
----------
Output: 
 devices -l
            0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
----------
Solution: 

        # b313b945               device usb:1-7 product:d2vzw model:SCH_I535 device:d2vzw
        # from Android system/core/adb/transport.c statename()
        re_device_info = re.compile(r'([^\s]+)\s+(offline|bootloader|device|host|recovery|sideload|no permissions|unauthorized|unknown)')
        devices = []
        lines = self.command_output(["devices", "-l"], timeout=timeout).split('\n')
        for line in lines:
            if line == 'List of devices attached ':
                continue
            match = re_device_info.match(line)
            if match:
                device = {
                    'device_serial': match.group(1),
                    'state': match.group(2)
                }
                remainder = line[match.end(2):].strip()
                if remainder:
                    try:
                        device.update(dict([j.split(':')
                                            for j in remainder.split(' ')]))
                    except ValueError:
                        self._logger.warning('devices: Unable to parse '
                                             'remainder for device %s' % line)
                devices.append(device)
        return devices
--------------------
Prompt: 
def wait_for_ajax_calls_to_complete(self, timeout=5):
        """
        Waits until there are no active or pending ajax requests.

        Raises TimeoutException should silence not be had.

        :param timeout: time to wait for silence (default: 5 seconds)
        :return: None
        """
----------
Output: 

        time.sleep(timeout)
        while True:
            try:
                self.driver.find_element_by_xpath(
                        '//*[@id="ajax-error-message"]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/div[2]/
----------
Solution: 

        from selenium.webdriver.support.ui import WebDriverWait

        WebDriverWait(self.driver, timeout).until(lambda s: s.execute_script("return jQuery.active === 0"))
--------------------
Prompt: 
def is_char_in_pairs(pos_char, pairs):
        """Return True if the charactor is in pairs of brackets or quotes."""
----------
Output: 

        return (pos_char == "(" and pairs[0] == pos_char) or \
                (pos_char == "{" and pairs[0] == pos_char) or \
                (pos_char == "}" and pairs[0] == "]")

    def is_char_in_string(pos_string, string):
        """Return True if the charactor is in string."""
        return (string[0] == pos_string) and \
                (string[1] == pos_string) or \
                (string[1] == "{")

    def is_char_in_
----------
Solution: 

        for pos_left, pos_right in pairs.items():
            if pos_left < pos_char < pos_right:
                return True

        return False
--------------------
Prompt: 
def insert_code(filename, code, save=True, marker='# ATX CODE END'):
    """ Auto append code """
----------
Output: 

    with open(filename, 'w') as f:
        f.write(code)
        if save:
            f.write(f'Saving {filename} code to {code}...')
        if save:
            f.write(f'Saving {filename} code to {code}...')
        if save:
            f.write(f'Saving {filename} code to {code}...')
        if save:
            f.write(f'Saving {filename} code to {code}...')
        if save:
            f.write(f'Saving {filename} code to {code}...
----------
Solution: 

    content = ''
    found = False
    for line in open(filename, 'rb'):
        if not found and line.strip() == marker:
            found = True
            cnt = line.find(marker)
            content += line[:cnt] + code
        content += line
    if not found:
        if not content.endswith('\n'):
            content += '\n'
        content += code + marker + '\n'
    if save:
        with open(filename, 'wb') as f:
            f.write(content)
    return content
--------------------
Prompt: 
def __properties_update(self, properties):
        """
        Internal update of configuration properties. Does not notifies the
        ConfigurationAdmin of this modification.

        :param properties: the new set of properties for this configuration
        :return: True if the properties have been updated, else False
        """
----------
Output: 

        if not self.is_valid_property(properties, "name"):
            raise ValueError("Configuration name is invalid")
        if not self.is_valid_property(properties, "value"):
            raise ValueError("Configuration value is invalid")
        if not self.is_valid_property(properties, "value_type"):
            raise ValueError("Configuration value_type is invalid")
        if not self.is_valid_property(properties, "value_type_name"):
            raise ValueError("Configuration value_type_name is invalid")
        if not self.is_valid_property(properties, "value_
----------
Solution: 

        if not properties:
            # Nothing to do
            return False

        with self.__lock:
            # Make a copy of the properties
            properties = properties.copy()

            # Override properties
            properties[services.CONFIG_PROP_PID] = self.__pid

            if self.__location:
                properties[
                    services.CONFIG_PROP_BUNDLE_LOCATION
                ] = self.__location

            if self.__factory_pid:
                properties[
                    services.CONFIG_PROP_FACTORY_PID
                ] = self.__factory_pid

            # See if new properties are different
            if properties == self.__properties:
                return False

            # Store the copy (before storing data)
            self.__properties = properties
            self.__updated = True

            # Store the data
            # it will cause FileInstall to update this configuration again, but
            # this will ignored because self.__properties has already been
            # saved
            self.__persistence.store(self.__pid, properties)
            return True
--------------------
Prompt: 
def after_invoke(self, coro):
        """A decorator that registers a coroutine as a post-invoke hook.

        A post-invoke hook is called directly after the command is
        called. This makes it a useful function to clean-up database
        connections or any type of clean up required.

        This post-invoke hook takes a sole parameter, a :class:`.Context`.

        See :meth:`.Bot.after_invoke` for more info.

        Parameters
        -----------
        coro: :ref:`coroutine <coroutine>`
            The coroutine to register as the post-invoke hook.

        Raises
        -------
        TypeError
            The coroutine passed is not actually a coroutine.
        """
----------
Output: 

        HTTPException
            The command failed to register.
        """
        self.register_hook(coroutine)

    def after_invoke_error(self, error):
        """A decorator that registers an error as a post-invoke hook.

        A post-invoke hook is called directly after the error is
        called. This makes it a useful function to clean-up database
        connections or any type of clean up required.

        This post-invoke hook takes a sole parameter, a :class:`.Context`.

        See :meth:`.Bot.after_invoke` for more info.

        Parameters
        -----------
        error: :ref:`coroutine
----------
Solution: 

        if not asyncio.iscoroutinefunction(coro):
            raise TypeError('The post-invoke hook must be a coroutine.')

        self._after_invoke = coro
        return coro
--------------------
Prompt: 
def main():
    """
    Example application that shows how to handle attach/detach events generated
    by the USB devices.

    In this case we open the device and listen for messages when it is attached.
    And when it is detached we remove it from our list of monitored devices.
    """
----------
Output: 

    import sys
    import time

    # Create a new instance of the USBDevice class
    usb = USBDevice(0)

    # Open the device
    while True:
        # Wait for a message from the device to be generated
        time.sleep(0.1)
        # Remove the event from the device from the list of monitored devices
        usb.remove_event(usb.get_event_count())

        # Check if we have a message
        if usb.get_message_count() > 0:
            # If we have a message, we can open the device
            if usb.get_message(0):
----------
Solution: 

    try:
        # Start up the detection thread such that handle_attached and handle_detached will
        # be called when devices are attached and detached, respectively.
        USBDevice.start_detection(on_attached=handle_attached, on_detached=handle_detached)

        # Wait for events.
        while True:
            time.sleep(1)

    except Exception as ex:
        print('Exception:', ex)

    finally:
        # Close all devices and stop detection.
        for sn, device in __devices.items():
            device.close()

        USBDevice.stop_detection()
--------------------
Prompt: 
def download_icon_font(icon_font, directory):
    """Download given (implemented) icon font into passed directory"""
----------
Output: 

    if not os.path.exists(directory):
        os.makedirs(directory)
    filename = os.path.join(directory, icon_font)
    if not os.path.exists(filename):
        raise ValueError("Invalid icon font: %s" % icon_font)
    with open(filename, 'wb') as f:
        f.write(urllib2.urlopen(url).read())


def get_icon_font_path(icon_font):
    """Return full path to the icon font"""
    return os.path.join(directory, 'images', icon_font)
----------
Solution: 

    try:
        downloader = AVAILABLE_ICON_FONTS[icon_font]['downloader'](directory)
        downloader.download_files()
        return downloader
    except KeyError:  # pragma: no cover
        raise Exception("We don't support downloading font '{name}'".format(
            name=icon_font)
        )
--------------------
Prompt: 
def start(self, message):
        """
        Manually starts timer with the message.

        :param message:  The display message.
        """
----------
Output: 

        self.message = message
        self.timer = Timer(message.time, message.time)
        self.timer.start()

    def stop(self):
        """
        Stops timer.
        """
        self.timer.stop()

    def __str__(self):
        """
        Returns a string representation of the timer.
        """
        return self.message.format(self.name)

    def __repr__(self):
        """
        Returns a string representation of the timer.
        """
        return self.message.format(self.name)

    def __eq__(self, other):
        """
        Returns true
----------
Solution: 

        self._start = time.clock()
        VSGLogger.info("{0:<20} - Started".format(message))
--------------------
Prompt: 
def serve_websocket(request, port):
    """Start UWSGI websocket loop and proxy."""
----------
Output: 

    # TODO: this is a hack to get around the fact that we can't use the same
    #       socket for the server, but it works.
    #       It's not clear what we want, but it's the best we can do.
    #       We should probably use a timeout here, but that's not a problem.
    #       We should probably use a socket timeout here, but that's not a problem.
    #       We should probably use a socket timeout here, but that's not a problem.
    #       We should probably use a socket timeout here, but that's not a problem.
   
----------
Solution: 

    env = request.environ

    # Send HTTP response 101 Switch Protocol downstream
    uwsgi.websocket_handshake(env['HTTP_SEC_WEBSOCKET_KEY'], env.get('HTTP_ORIGIN', ''))

    # Map the websocket URL to the upstream localhost:4000x Notebook instance
    parts = urlparse(request.url)
    parts = parts._replace(scheme="ws", netloc="localhost:{}".format(port))
    url = urlunparse(parts)

    # Proxy initial connection headers
    headers = [(header, value) for header, value in request.headers.items() if header.lower() in CAPTURE_CONNECT_HEADERS]

    logger.info("Connecting to upstream websockets: %s, headers: %s", url, headers)

    ws = ProxyClient(url, headers=headers)
    ws.connect()

    # TODO: Will complain loudly about already send headers - how to abort?
    return httpexceptions.HTTPOk()
--------------------
Prompt: 
def pack(self):
    """Pack to framed binary message.

    """
----------
Output: 

    packed = []
    packed.append(struct.pack("!B", self.type))
    packed.append(struct.pack("!H", 0)) # placeholder for length at index 2
    packed.append(struct.pack("!L", self.xid)
    packed.append(struct.pack("!H", self.err_type)
    packed.append(struct.pack("!H", self.code)
    packed.append(struct.pack("!H", self.data)
    length = sum([len(x) for x in packed])
    if length > 255:
      self.max_len =
----------
Solution: 

    buf = np.zeros(512, dtype=np.uint8)
    packed_len = self._get_framed(buf, 0, self._copy_payload)
    d = buf[:packed_len]
    return d.tobytes()
--------------------
Prompt: 
def get_task_class(tasks, task_args):
    """Reads in a list of tasks provided by the user,
    loads the appropiate task, and returns two lists,
    pre_stop_tasks and post_stop_tasks
    :param tasks: list of strings locating tasks to load
    :type tasks: list
    :param task_args: list of strings to be used as args
    :type task_args: list
    """
----------
Output: 

    if len(tasks)!= len(task_args):
        raise ValueError("Length of list of tasks must be equal to len(task_args)")
    tasks = [task_args[i] for i in range(0, len(task_args), 2)]
    task_args = [task_args[i] for i in range(1, len(task_args), 2)]
    task_class = []
    for task in tasks:
        if task.startswith("task_"):
            task_class.append(get_task_class(task[5:], task_args))
        elif
----------
Solution: 

    pre_stop_tasks = []
    post_stop_tasks = []
    task_to_task_args = dict(list(zip(tasks, task_args)))
    tasks_classes = [PreStopTask, PostStopTask]

    for func, task_args in task_to_task_args.items():
        for task_class in tasks_classes:
            imported_class = dynamic_import(func, task_class)
            if imported_class:
                if task_class is PreStopTask:
                    pre_stop_tasks.append(imported_class(task_args))
                elif task_class is PostStopTask:
                    post_stop_tasks.append(imported_class(task_args))
                else:
                    print("ERROR: Class is not a type of Pre/Post StopTask:" + func)
                    sys.exit(1)
    return pre_stop_tasks, post_stop_tasks
